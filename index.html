<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-11T01:30:00Z">09-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">LanSER: Language-Model Supported Speech Emotion Recognition. (arXiv:2309.03978v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03978">
<div class="article-summary-box-inner">
<span><p>Speech emotion recognition (SER) models typically rely on costly
human-labeled data for training, making scaling methods to large speech
datasets and nuanced emotion taxonomies difficult. We present LanSER, a method
that enables the use of unlabeled data by inferring weak emotion labels via
pre-trained large language models through weakly-supervised learning. For
inferring weak labels constrained to a taxonomy, we use a textual entailment
approach that selects an emotion label with the highest entailment score for a
speech transcript extracted via automatic speech recognition. Our experimental
results show that models pre-trained on large datasets with this weak
supervision outperform other baseline models on standard SER datasets when
fine-tuned, and show improved label efficiency. Despite being pre-trained on
labels derived only from text, we show that the resulting representations
appear to model the prosodic content of speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03992">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are increasingly being used for generating text
in a variety of use cases, including journalistic news articles. Given the
potential malicious nature in which these LLMs can be used to generate
disinformation at scale, it is important to build effective detectors for such
AI-generated text. Given the surge in development of new LLMs, acquiring
labeled training data for supervised detectors is a bottleneck. However, there
might be plenty of unlabeled text data available, without information on which
generator it came from. In this work we tackle this data problem, in detecting
AI-generated news text, and frame the problem as an unsupervised domain
adaptation task. Here the domains are the different text generators, i.e. LLMs,
and we assume we have access to only the labeled source data and unlabeled
target data. We develop a Contrastive Domain Adaptation framework, called
ConDA, that blends standard domain adaptation techniques with the
representation power of contrastive learning to learn domain invariant
representations that are effective for the final unsupervised detection task.
Our experiments demonstrate the effectiveness of our framework, resulting in
average performance gains of 31.7% from the best performing baselines, and
within 0.8% margin of a fully supervised detector. All our code and data is
available at https://github.com/AmritaBh/ConDA-gen-text-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04019">
<div class="article-summary-box-inner">
<span><p>Gene set analysis is a mainstay of functional genomics, but it relies on
manually curated databases of gene functions that are incomplete and unaware of
biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large
Language Model (LLM), to develop hypotheses about common gene functions from
its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene
sets with names that summarize their consensus functions, substantiated by
analysis text and citations. Benchmarking against named gene sets in the Gene
Ontology, GPT-4 generated very similar names in 50% of cases, while in most
remaining cases it recovered the name of a more general concept. In gene sets
discovered in 'omics data, GPT-4 names were more informative than gene set
enrichment, with supporting statements and citations that largely verified in
human review. The ability to rapidly synthesize common gene functions positions
LLMs as valuable functional genomics assistants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04027">
<div class="article-summary-box-inner">
<span><p>Machine learning models can perpetuate unintended biases from unfair and
imbalanced datasets. Evaluating and debiasing these datasets and models is
especially hard in text datasets where sensitive attributes such as race,
gender, and sexual orientation may not be available. When these models are
deployed into society, they can lead to unfair outcomes for historically
underrepresented groups. In this paper, we present a dataset coupled with an
approach to improve text fairness in classifiers and language models. We create
a new, more comprehensive identity lexicon, TIDAL, which includes 15,123
identity terms and associated sense context across three demographic
categories. We leverage TIDAL to develop an identity annotation and
augmentation tool that can be used to improve the availability of identity
context and the effectiveness of ML fairness techniques. We evaluate our
approaches using human contributors, and additionally run experiments focused
on dataset and model debiasing. Results show our assistive annotation technique
improves the reliability and velocity of human-in-the-loop processes. Our
dataset and methods uncover more disparities during evaluation, and also
produce more fair models during remediation. These approaches provide a
practical path forward for scaling classifier and generative model fairness in
real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems. (arXiv:2309.04031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04031">
<div class="article-summary-box-inner">
<span><p>Transferring the knowledge of large language models (LLMs) is a promising
technique to incorporate linguistic knowledge into end-to-end automatic speech
recognition (ASR) systems. However, existing works only transfer a single
representation of LLM (e.g. the last layer of pretrained BERT), while the
representation of a text is inherently non-unique and can be obtained variously
from different layers, contexts and models. In this work, we explore a wide
range of techniques to obtain and transfer multiple representations of LLMs
into a transducer-based ASR system. While being conceptually simple, we show
that transferring multiple representations of LLMs can be an effective
alternative to transferring only a single representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation and Mitigation of Agnosia in Multimodal Large Language Models. (arXiv:2309.04041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04041">
<div class="article-summary-box-inner">
<span><p>While Multimodal Large Language Models (MLLMs) are widely used for a variety
of vision-language tasks, one observation is that they sometimes misinterpret
visual inputs or fail to follow textual instructions even in straightforward
cases, leading to irrelevant responses, mistakes, and ungrounded claims. This
observation is analogous to a phenomenon in neuropsychology known as Agnosia,
an inability to correctly process sensory modalities and recognize things
(e.g., objects, colors, relations). In our study, we adapt this similar concept
to define "agnosia in MLLMs", and our goal is to comprehensively evaluate and
mitigate such agnosia in MLLMs. Inspired by the diagnosis and treatment process
in neuropsychology, we propose a novel framework EMMA (Evaluation and
Mitigation of Multimodal Agnosia). In EMMA, we develop an evaluation module
that automatically creates fine-grained and diverse visual question answering
examples to assess the extent of agnosia in MLLMs comprehensively. We also
develop a mitigation module to reduce agnosia in MLLMs through multimodal
instruction tuning on fine-grained conversations. To verify the effectiveness
of our framework, we evaluate and analyze agnosia in seven state-of-the-art
MLLMs using 9K test samples. The results reveal that most of them exhibit
agnosia across various aspects and degrees. We further develop a fine-grained
instruction set and tune MLLMs to mitigate agnosia, which led to notable
improvement in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multi-document Summarization with Holistic Inference. (arXiv:2309.04087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04087">
<div class="article-summary-box-inner">
<span><p>Multi-document summarization aims to obtain core information from a
collection of documents written on the same topic. This paper proposes a new
holistic framework for unsupervised multi-document extractive summarization.
Our method incorporates the holistic beam search inference method associated
with the holistic measurements, named Subset Representative Index (SRI). SRI
balances the importance and diversity of a subset of sentences from the source
documents and can be calculated in unsupervised and adaptive manners. To
demonstrate the effectiveness of our method, we conduct extensive experiments
on both small and large-scale multi-document summarization datasets under both
unsupervised and adaptive settings. The proposed method outperforms strong
baselines by a significant margin, as indicated by the resulting ROUGE scores
and diversity measures. Our findings also suggest that diversity is essential
for improving multi-document summary performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta predictive learning model of natural languages. (arXiv:2309.04106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04106">
<div class="article-summary-box-inner">
<span><p>Large language models based on self-attention mechanisms have achieved
astonishing performances not only in natural language itself, but also in a
variety of tasks of different nature. However, regarding processing language,
our human brain may not operate using the same principle. Then, a debate is
established on the connection between brain computation and artificial
self-supervision adopted in large language models. One of most influential
hypothesis in brain computation is the predictive coding framework, which
proposes to minimize the prediction error by local learning. However, the role
of predictive coding and the associated credit assignment in language
processing remains unknown. Here, we propose a mean-field learning model within
the predictive coding framework, assuming that the synaptic weight of each
connection follows a spike and slab distribution, and only the distribution is
trained. This meta predictive learning is successfully validated on classifying
handwritten digits where pixels are input to the network in sequence, and on
the toy and real language corpus. Our model reveals that most of the
connections become deterministic after learning, while the output connections
have a higher level of variability. The performance of the resulting network
ensemble changes continuously with data load, further improving with more
training data, in analogy with the emergent behavior of large language models.
Therefore, our model provides a starting point to investigate the physics and
biology correspondences of the language processing and the unexpected general
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RST-style Discourse Parsing Guided by Document-level Content Structures. (arXiv:2309.04141v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04141">
<div class="article-summary-box-inner">
<span><p>Rhetorical Structure Theory based Discourse Parsing (RST-DP) explores how
clauses, sentences, and large text spans compose a whole discourse and presents
the rhetorical structure as a hierarchical tree. Existing RST parsing pipelines
construct rhetorical structures without the knowledge of document-level content
structures, which causes relatively low performance when predicting the
discourse relations for large text spans. Recognizing the value of high-level
content-related information in facilitating discourse relation recognition, we
propose a novel pipeline for RST-DP that incorporates structure-aware news
content sentence representations derived from the task of News Discourse
Profiling. By incorporating only a few additional layers, this enhanced
pipeline exhibits promising performance across various RST parsing metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04146">
<div class="article-summary-box-inner">
<span><p>The statistical analysis of large scale legal corpus can provide valuable
legal insights. For such analysis one needs to (1) select a subset of the
corpus using document retrieval tools, (2) structuralize text using information
extraction (IE) systems, and (3) visualize the data for the statistical
analysis. Each process demands either specialized tools or programming skills
whereas no comprehensive unified "no-code" tools have been available.
Especially for IE, if the target information is not predefined in the ontology
of the IE system, one needs to build their own system. Here we provide NESTLE,
a no code tool for large-scale statistical analysis of legal corpus. With
NESTLE, users can search target documents, extract information, and visualize
the structured data all via the chat interface with accompanying auxiliary GUI
for the fine-level control. NESTLE consists of three main components: a search
engine, an end-to-end IE system, and a Large Language Model (LLM) that glues
the whole components together and provides the chat interface. Powered by LLM
and the end-to-end IE system, NESTLE can extract any type of information that
has not been predefined in the IE system opening up the possibility of
unlimited customizable statistical analysis of the corpus without writing a
single line of code. The use of the custom end-to-end IE system also enables
faster and low-cost IE on large scale corpus. We validate our system on 15
Korean precedent IE tasks and 3 legal text classification tasks from LEXGLUE.
The comprehensive experiments reveal NESTLE can achieve GPT-4 comparable
performance by training the internal IE module with 4 human-labeled, and 192
LLM-labeled examples. The detailed analysis provides the insight on the
trade-off between accuracy, time, and cost in building such system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Utterance Conditioned VAE for Speech Generation. (arXiv:2309.04156v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04156">
<div class="article-summary-box-inner">
<span><p>Speech synthesis systems powered by neural networks hold promise for
multimedia production, but frequently face issues with producing expressive
speech and seamless editing. In response, we present the Cross-Utterance
Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to
enhance prosody and ensure natural speech generation. This framework leverages
the powerful representational capabilities of pre-trained language models and
the re-expression abilities of variational autoencoders (VAEs). The core
component of the CUC-VAE S2 framework is the cross-utterance CVAE, which
extracts acoustic, speaker, and textual features from surrounding sentences to
generate context-sensitive prosodic features, more accurately emulating human
prosody generation. We further propose two practical algorithms tailored for
distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and
CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the
framework, designed to generate audio with contextual prosody derived from
surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real
mel spectrogram sampling conditioned on contextual information, producing audio
that closely mirrors real sound and thereby facilitating flexible speech
editing based on text such as deletion, insertion, and replacement.
Experimental results on the LibriTTS datasets demonstrate that our proposed
models significantly enhance speech synthesis and editing, producing more
natural and expressive speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue. (arXiv:2309.04162v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04162">
<div class="article-summary-box-inner">
<span><p>Pre-trained models have achieved success in Chinese Short Text Matching (STM)
tasks, but they often rely on superficial clues, leading to a lack of robust
predictions. To address this issue, it is crucial to analyze and mitigate the
influence of superficial clues on STM models. Our study aims to investigate
their over-reliance on the edit distance feature, commonly used to measure the
semantic similarity of Chinese text pairs, which can be considered a
superficial clue. To mitigate STM models' over-reliance on superficial clues,
we propose a novel resampling training strategy called Gradually Learn Samples
Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of
In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we
demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the
robustness and generalization of Chinese STM models. Moreover, we conduct a
detailed analysis of existing methods and reveal their commonality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04174">
<div class="article-summary-box-inner">
<span><p>Prompt-based classification adapts tasks to a cloze question format utilizing
the [MASK] token and the filled tokens are then mapped to labels through
pre-defined verbalizers. Recent studies have explored the use of verbalizer
embeddings to reduce labor in this process. However, all existing studies
require a tuning process for either the pre-trained models or additional
trainable embeddings. Meanwhile, the distance between high-dimensional
verbalizer embeddings should not be measured by Euclidean distance due to the
potential for non-linear manifolds in the representation space. In this study,
we propose a tuning-free manifold-based space re-embedding method called
Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for
verbalizer embeddings, which preserves local properties within the same class
as guidance for classification. Experimental results indicate that even without
tuning any parameters, our LLE-INC is on par with automated verbalizers with
parameter tuning. And with the parameter updating, our approach further
enhances prompt-based tuning by up to 3.2%. Furthermore, experiments with the
LLaMA-7B&amp;13B indicate that LLE-INC is an efficient tuning-free classification
approach for the hyper-scale language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. (arXiv:2309.04175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04175">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable success in diverse
natural language processing (NLP) tasks in general domains. However, LLMs
sometimes generate responses with the hallucination about medical facts due to
limited domain knowledge. Such shortcomings pose potential risks in the
utilization of LLMs within medical contexts. To address this challenge, we
propose knowledge-tuning, which leverages structured medical knowledge bases
for the LLMs to grasp domain knowledge efficiently and facilitate reliable
response generation. We also release cMedKnowQA, a Chinese medical knowledge
question-answering dataset constructed from medical knowledge bases to assess
the medical knowledge proficiency of LLMs. Experimental results show that the
LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of
accuracy in response generation compared with vanilla instruction-tuning and
offer a new reliable way for the domain adaptation of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04198">
<div class="article-summary-box-inner">
<span><p>The application of Large Language Models (LLMs) to the medical domain has
stimulated the interest of researchers. Recent studies have focused on
constructing Instruction Fine-Tuning (IFT) data through medical knowledge
graphs to enrich the interactive medical knowledge of LLMs. However, the
medical literature serving as a rich source of medical knowledge remains
unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive
knowledge acquisition from Chinese medical literature. It assesses the
proficiency of LLMs in mastering medical knowledge through a free-dialogue
fact-checking task. We identify a phenomenon called the ``fact-following
response``, where LLMs tend to affirm facts mentioned in questions and display
a reluctance to challenge them. To eliminate the inaccurate evaluation caused
by this phenomenon, for the golden fact, we artificially construct test data
from two perspectives: one consistent with the fact and one inconsistent with
the fact. Drawing from the probing experiment on the CALLA dataset, we conclude
that IFT data highly correlated with the medical literature corpus serves as a
potent catalyst for LLMs, enabling themselves to skillfully employ the medical
knowledge acquired during the pre-training phase within interactive scenarios,
enhancing accuracy. Furthermore, we design a framework for automatically
constructing IFT data based on medical literature and discuss some real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04213">
<div class="article-summary-box-inner">
<span><p>As social media becomes increasingly popular, more and more activities
related to public health emerge. Current techniques for public health analysis
involve popular models such as BERT and large language models (LLMs). However,
the costs of training in-domain LLMs for public health are especially
expensive. Furthermore, such kinds of in-domain datasets from social media are
generally imbalanced. To tackle these challenges, the data imbalance issue can
be overcome by data augmentation and balanced training. Moreover, the ability
of the LLMs can be effectively utilized by prompting the model properly. In
this paper, a novel ALEX framework is proposed to improve the performance of
public health analysis on social media by adopting an LLMs explanation
mechanism. Results show that our ALEX model got the best performance among all
submissions in both Task 2 and Task 4 with a high score in Task 1 in Social
Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://
github.com/YanJiangJerry/ALEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. (arXiv:2309.04269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04269">
<div class="article-summary-box-inner">
<span><p>Selecting the ``right'' amount of information to include in a summary is a
difficult task. A good summary should be detailed and entity-centric without
being overly dense and hard to follow. To better understand this tradeoff, we
solicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain
of Density'' (CoD) prompt. Specifically, GPT-4 generates an initial
entity-sparse summary before iteratively incorporating missing salient entities
without increasing the length. Summaries generated by CoD are more abstractive,
exhibit more fusion, and have less of a lead bias than GPT-4 summaries
generated by a vanilla prompt. We conduct a human preference study on 100 CNN
DailyMail articles and find that that humans prefer GPT-4 summaries that are
more dense than those generated by a vanilla prompt and almost as dense as
human written summaries. Qualitative analysis supports the notion that there
exists a tradeoff between informativeness and readability. 500 annotated CoD
summaries, as well as an extra 5,000 unannotated summaries, are freely
available on HuggingFace
(https://huggingface.co/datasets/griffin/chain_of_density).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations. (arXiv:2309.04292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04292">
<div class="article-summary-box-inner">
<span><p>Fuzzy Fingerprints have been successfully used as an interpretable text
classification technique, but, like most other techniques, have been largely
surpassed in performance by Large Pre-trained Language Models, such as BERT or
RoBERTa. These models deliver state-of-the-art results in several Natural
Language Processing tasks, namely Emotion Recognition in Conversations (ERC),
but suffer from the lack of interpretability and explainability. In this paper,
we propose to combine the two approaches to perform ERC, as a means to obtain
simpler and more interpretable Large Language Models-based classifiers. We
propose to feed the utterances and their previous conversational turns to a
pre-trained RoBERTa, obtaining contextual embedding utterance representations,
that are then supplied to an adapted Fuzzy Fingerprint classification module.
We validate our approach on the widely used DailyDialog ERC benchmark dataset,
in which we obtain state-of-the-art level results using a much lighter model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens. (arXiv:2309.04333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04333">
<div class="article-summary-box-inner">
<span><p>Many useful tasks on scientific documents, such as topic classification and
citation prediction, involve corpora that span multiple scientific domains.
Typically, such tasks are accomplished by representing the text with a vector
embedding obtained from a Transformer's single CLS token. In this paper, we
argue that using multiple CLS tokens could make a Transformer better specialize
to multiple scientific domains. We present Multi2SPE: it encourages each of
multiple CLS tokens to learn diverse ways of aggregating token embeddings, then
sums them up together to create a single vector representation. We also propose
our new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector
encoders under multi-domain settings. We show that Multi2SPE reduces error by
up to 25 percent in multi-domain citation prediction, while requiring only a
negligible amount of computation in addition to one BERT forward pass.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. (arXiv:2309.04369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04369">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made progress in various real-world tasks,
which stimulates requirements for the evaluation of LLMs. Existing LLM
evaluation methods are mainly supervised signal-based which depends on static
datasets and cannot evaluate the ability of LLMs in dynamic real-world
scenarios where deep interaction widely exists. Other LLM evaluation methods
are human-based which are costly and time-consuming and are incapable of
large-scale evaluation of LLMs. To address the issues above, we propose a novel
Deep Interaction-based LLM-evaluation framework. In our proposed framework,
LLMs' performances in real-world domains can be evaluated from their deep
interaction with other LLMs in elaborately designed evaluation tasks.
Furthermore, our proposed framework is a general evaluation method that can be
applied to a host of real-world tasks such as machine translation and code
generation. We demonstrate the effectiveness of our proposed method through
extensive experiments on four elaborately designed evaluation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers. (arXiv:2309.04372v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04372">
<div class="article-summary-box-inner">
<span><p>Diffusion-model-based text-guided image generation has recently made
astounding progress, producing fascinating results in open-domain image
manipulation tasks. Few models, however, currently have complete zero-shot
capabilities for both global and local image editing due to the complexity and
diversity of image manipulation tasks. In this work, we propose a method with a
mixture-of-expert (MOE) controllers to align the text-guided capacity of
diffusion models with different kinds of human instructions, enabling our model
to handle various open-domain image manipulation tasks with natural language
instructions. First, we use large language models (ChatGPT) and conditional
image synthesis models (ControlNet) to generate a large number of global image
transfer dataset in addition to the instruction-based local image editing
dataset. Then, using an MOE technique and task-specific adaptation training on
a large-scale dataset, our conditional diffusion model can edit images globally
and locally. Extensive experiments demonstrate that our approach performs
surprisingly well on various image manipulation tasks when dealing with
open-domain images and arbitrary human instructions. Please refer to our
project page: [https://oppo-mente-lab.github.io/moe_controller/]
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market. (arXiv:2309.04389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04389">
<div class="article-summary-box-inner">
<span><p>In recent years, great advances in pre-trained language models (PLMs) have
sparked considerable research focus and achieved promising performance on the
approach of dense passage retrieval, which aims at retrieving relative passages
from massive corpus with given questions. However, most of existing datasets
mainly benchmark the models with factoid queries of general commonsense, while
specialised fields such as finance and economics remain unexplored due to the
deficiency of large-scale and high-quality datasets with expert annotations. In
this work, we propose a new task, policy retrieval, by introducing the Chinese
Stock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages
labeled by experienced experts with relevant articles from 10k+ entries in our
collected Chinese policy corpus. Experiments on lexical, embedding and
fine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet
also suggests ample potential for improvement. Our best performing baseline
achieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10 and 80.6% Precision@10 on
dev set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models. (arXiv:2309.04461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04461">
<div class="article-summary-box-inner">
<span><p>Vision-language models (VLMs) have recently demonstrated strong efficacy as
visual assistants that can parse natural queries about the visual content and
generate human-like outputs. In this work, we explore the ability of these
models to demonstrate human-like reasoning based on the perceived information.
To address a crucial concern regarding the extent to which their reasoning
capabilities are fully consistent and grounded, we also measure the reasoning
consistency of these models. We achieve this by proposing a chain-of-thought
(CoT) based consistency measure. However, such an evaluation requires a
benchmark that encompasses both high-level inference and detailed reasoning
chains, which is costly. We tackle this challenge by proposing a
LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously
ensuring the generation of a high-quality dataset. Based on this pipeline and
the existing coarse-grained annotated dataset, we build the CURE benchmark to
measure both the zero-shot reasoning performance and consistency of VLMs. We
evaluate existing state-of-the-art VLMs, and find that even the best-performing
model is unable to demonstrate strong visual reasoning capabilities and
consistency, indicating that substantial efforts are required to enable VLMs to
perform visual reasoning as systematically and consistently as humans. As an
early step, we propose a two-stage training framework aimed at improving both
the reasoning performance and consistency of VLMs. The first stage involves
employing supervised fine-tuning of VLMs using step-by-step reasoning samples
automatically generated by LLMs. In the second stage, we further augment the
training process by incorporating feedback provided by LLMs to produce
reasoning chains that are highly consistent and grounded. We empirically
highlight the effectiveness of our framework in both reasoning performance and
consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Text Formality: A Study of Text Classification Approaches. (arXiv:2204.08975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08975">
<div class="article-summary-box-inner">
<span><p>Formality is one of the important characteristics of text documents. The
automatic detection of the formality level of a text is potentially beneficial
for various natural language processing tasks. Before, two large-scale datasets
were introduced for multiple languages featuring formality annotation -- GYAFC
and X-FORMAL. However, they were primarily used for the training of style
transfer models. At the same time, the detection of text formality on its own
may also be a useful application. This work proposes the first to our knowledge
systematic study of formality detection methods based on statistical,
neural-based, and Transformer-based machine learning methods and delivers the
best-performing models for public usage. We conducted three types of
experiments -- monolingual, multilingual, and cross-lingual. The study shows
the overcome of Char BiLSTM model over Transformer-based ones for the
monolingual and multilingual formality classification task, while
Transformer-based classifiers are more stable to cross-lingual knowledge
transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: A Lightweight and Robust Neural Architecture for Discourse Parsing. (arXiv:2210.09537v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09537">
<div class="article-summary-box-inner">
<span><p>Complex feature extractors are widely employed for text representation
building. However, these complex feature extractors make the NLP systems prone
to overfitting especially when the downstream training datasets are relatively
small, which is the case for several discourse parsing tasks. Thus, we propose
an alternative lightweight neural architecture that removes multiple complex
feature extractors and only utilizes learnable self-attention modules to
indirectly exploit pretrained neural language models, in order to maximally
preserve the generalizability of pre-trained language models. Experiments on
three common discourse parsing tasks show that powered by recent pretrained
language models, the lightweight architecture consisting of only two
self-attention layers obtains much better generalizability and robustness.
Meanwhile, it achieves comparable or even better system performance with fewer
learnable parameters and less processing time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05880">
<div class="article-summary-box-inner">
<span><p>To facilitate the research on intelligent and human-like chatbots with
multi-modal context, we introduce a new video-based multi-modal dialogue
dataset, called TikTalk. We collect 38K videos from a popular video-sharing
platform, along with 367K conversations posted by users beneath them. Users
engage in spontaneous conversations based on their multi-modal experiences from
watching videos, which helps recreate real-world chitchat context. Compared to
previous multi-modal dialogue datasets, the richer context types in TikTalk
lead to more diverse conversations, but also increase the difficulty in
capturing human interests from intricate multi-modal information to generate
personalized responses. Moreover, external knowledge is more frequently evoked
in our dataset. These facts reveal new challenges for multi-modal dialogue
models. We quantitatively demonstrate the characteristics of TikTalk, propose a
video-based multi-modal chitchat task, and evaluate several dialogue baselines.
Experimental results indicate that the models incorporating large language
models (LLM) can generate more diverse responses, while the model utilizing
knowledge graphs to introduce external knowledge performs the best overall.
Furthermore, no existing model can solve all the above challenges well. There
is still a large room for future improvements, even for LLM with visual
extensions. Our dataset is available at
\url{https://ruc-aimind.github.io/projects/TikTalk/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization. (arXiv:2301.12307v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12307">
<div class="article-summary-box-inner">
<span><p>State-of-the-art summarization systems can generate highly fluent summaries.
These summaries, however, may contain factual inconsistencies and/or
information not present in the source. Hence, an important component of
assessing the quality of summaries is to determine whether there is information
consistency between the source and the summary. Existing approaches are
typically based on lexical matching or representation-based methods. In this
work, we introduce an alternative scheme based on standard
information-theoretic measures in which the information present in the source
and summary is directly compared. We propose a Multiple-choice Question
Answering and Generation framework, MQAG, which approximates the information
consistency by computing the expected statistical distance between summary and
source answer distributions over automatically generated multiple-choice
questions. This approach exploits multiple-choice answer probabilities, as
predicted answer distributions can be compared. We conduct experiments on four
summary evaluation datasets: QAG-CNNDM/XSum, XSum-Hallucination, Podcast
Assessment, and SummEval. Experiments show that MQAG, using models trained on
SQuAD or RACE, outperforms existing evaluation methods on the majority of
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Tracking in Language Models. (arXiv:2305.02363v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02363">
<div class="article-summary-box-inner">
<span><p>Keeping track of how states of entities change as a text or dialog unfolds is
a key prerequisite to discourse understanding. Yet, there have been few
systematic investigations into the ability of large language models (LLMs) to
track discourse entities. In this work, we present a task probing to what
extent a language model can infer the final state of an entity given an English
description of the initial state and a series of state-changing operations. We
use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track
the state of entities, and find that only GPT-3.5 models, which have been
pretrained on large amounts of code, exhibit this ability. We then investigate
whether smaller models pretrained primarily on text can learn to track
entities, through finetuning T5 on several training/evaluation splits. While
performance degrades for more complex splits, we find that even when evaluated
on a different set of entities from training or longer operation sequences, a
finetuned model can perform non-trivial entity tracking. Taken together, these
results suggest that language models can learn to track entities but
pretraining on text corpora alone does not make this capacity surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conditional Generative Chatbot using Transformer Model. (arXiv:2306.02074v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02074">
<div class="article-summary-box-inner">
<span><p>A Chatbot serves as a communication tool between a human user and a machine
to achieve an appropriate answer based on the human input. In more recent
approaches, a combination of Natural Language Processing and sequential models
are used to build a generative Chatbot. The main challenge of these models is
their sequential nature, which leads to less accurate results. To tackle this
challenge, in this paper, a novel architecture is proposed using conditional
Wasserstein Generative Adversarial Networks and a transformer model for answer
generation in Chatbots. While the generator of the proposed model consists of a
full transformer model to generate an answer, the discriminator includes only
the encoder part of a transformer model followed by a classifier. To the best
of our knowledge, this is the first time that a generative Chatbot is proposed
using the embedded transformer in both generator and discriminator models.
Relying on the parallel computing of the transformer model, the results of the
proposed model on the Cornell Movie-Dialog corpus and the Chit-Chat datasets
confirm the superiority of the proposed model compared to state-of-the-art
alternatives using different evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ValiTex -- a unified validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02863">
<div class="article-summary-box-inner">
<span><p>Guidance on how to validate computational text-based measures of social
science constructs is fragmented. Although scholars generally acknowledge the
importance of validating their text-based measures, they often lack common
terminology and a unified framework to do so. This paper introduces ValiTex, a
new validation framework designed to assist scholars in validly measuring
social science constructs based on textual data. The framework draws on a
long-established validity concept in psychometrics but extends these concepts
to cover the specific needs of computational text analysis. ValiTex consists of
two components, a conceptual framework and a dynamic checklist. Whereas the
conceptual framework provides a general structure along distinct phases on how
to approach validation, the dynamic checklist defines specific validation steps
and provides guidance on which steps might be considered recommendable (i.e.,
providing relevant and necessary validation evidence) or optional (i.e., useful
for providing additional supporting validation evidence). We demonstrate the
utility of the framework by applying it to a use case of detecting sexism from
social media data
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12032">
<div class="article-summary-box-inner">
<span><p>In the realm of Large Language Models, the balance between instruction data
quality and quantity has become a focal point. Recognizing this, we introduce a
self-guided methodology for LLMs to autonomously discern and select cherry
samples from vast open-source datasets, effectively minimizing manual curation
and potential cost for instruction tuning an LLM. Our key innovation, the
Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to
identify discrepancies between a model's expected responses and its autonomous
generation prowess. Through the adept application of IFD, cherry samples are
pinpointed, leading to a marked uptick in model training efficiency. Empirical
validations on renowned datasets like Alpaca and WizardLM underpin our
findings; with a mere 10% of conventional data input, our strategy showcases
improved results. This synthesis of self-guided cherry-picking and the IFD
metric signifies a transformative leap in the optimization of LLMs, promising
both efficiency and resource-conscious advancements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15363">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL
task. However, the absence of a systematical benchmark inhibits the development
of designing effective, efficient and economic LLM-based Text-to-SQL solutions.
To address this challenge, in this paper, we first conduct a systematical and
extensive comparison over existing prompt engineering methods, including
question representation, example selection and example organization, and with
these experimental results, we elaborate their pros and cons. Based on these
findings, we propose a new integrated solution, named DAIL-SQL, which refreshes
the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To
explore the potential of open-source LLM, we investigate them in various
scenarios, and further enhance their performance with supervised fine-tuning.
Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well
as the advantages and disadvantages of the supervised fine-tuning.
Additionally, towards an efficient and economic LLM-based Text-to-SQL solution,
we emphasize the token efficiency in prompt engineering and compare the prior
studies under this metric. We hope that our work provides a deeper
understanding of Text-to-SQL with LLMs, and inspires further investigations and
broad applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15452">
<div class="article-summary-box-inner">
<span><p>The reasoning capabilities of Large Language Models (LLMs) play a pivotal
role in the realm of embodied artificial intelligence. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation. (arXiv:2308.16797v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16797">
<div class="article-summary-box-inner">
<span><p>Despite significant research effort in the development of automatic dialogue
evaluation metrics, little thought is given to evaluating dialogues other than
in English. At the same time, ensuring metrics are invariant to semantically
similar responses is also an overlooked topic. In order to achieve the desired
properties of robustness and multilinguality for dialogue evaluation metrics,
we propose a novel framework that takes advantage of the strengths of current
evaluation models with the newly-established paradigm of prompting Large
Language Models (LLMs). Empirical results show our framework achieves state of
the art results in terms of mean Spearman correlation scores across several
benchmarks and ranks first place on both the Robust and Multilingual tasks of
the DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain Dialogue
Systems", proving the evaluation capabilities of prompted LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00359">
<div class="article-summary-box-inner">
<span><p>Shannon, in his seminal paper introducing information theory, divided the
communication into three levels: technical, semantic, and effectivenss. While
the technical level is concerned with accurate reconstruction of transmitted
symbols, the semantic and effectiveness levels deal with the inferred meaning
and its effect on the receiver. Thanks to telecommunications, the first level
problem has produced great advances like the internet. Large Language Models
(LLMs) make some progress towards the second goal, but the third level still
remains largely untouched. The third problem deals with predicting and
optimizing communication for desired receiver behavior. LLMs, while showing
wide generalization capabilities across a wide range of tasks, are unable to
solve for this. One reason for the underperformance could be a lack of
"behavior tokens" in LLMs' training corpora. Behavior tokens define receiver
behavior over a communication, such as shares, likes, clicks, purchases,
retweets, etc. While preprocessing data for LLM training, behavior tokens are
often removed from the corpora as noise. Therefore, in this paper, we make some
initial progress towards reintroducing behavior tokens in LLM training. The
trained models, other than showing similar performance to LLMs on content
understanding tasks, show generalization capabilities on behavior simulation,
content simulation, behavior understanding, and behavior domain adaptation.
Using a wide range of tasks on two corpora, we show results on all these
capabilities. We call these models Large Content and Behavior Models (LCBMs).
Further, to spur more research on LCBMs, we release our new Content Behavior
Corpus (CBC), a repository containing communicator, message, and corresponding
receiver behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm. (arXiv:2309.03563v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03563">
<div class="article-summary-box-inner">
<span><p>In intent detection tasks, leveraging meaningful semantic information from
intent labels can be particularly beneficial for few-shot scenarios. However,
existing few-shot intent detection methods either ignore the intent labels,
(e.g. treating intents as indices) or do not fully utilize this information
(e.g. only using part of the intent labels). In this work, we present an
end-to-end One-to-All system that enables the comparison of an input utterance
with all label candidates. The system can then fully utilize label semantics in
this way. Experiments on three few-shot intent detection tasks demonstrate that
One-to-All is especially effective when the training resource is extremely
scarce, achieving state-of-the-art performance in 1-, 3- and 5-shot settings.
Moreover, we present a novel pretraining strategy for our model that utilizes
indirect supervision from paraphrasing, enabling zero-shot cross-domain
generalization on intent detection tasks. Our code is at
https://github.com/jiangshdd/AllLablesTogether.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring an LM to generate Prolog Predicates from Mathematics Questions. (arXiv:2309.03667v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03667">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a surge in interest in NLP driven by ChatGPT.
ChatGPT, a transformer-based generative language model of substantial scale,
exhibits versatility in performing various tasks based on natural language.
Nevertheless, large language models often exhibit poor performance in solving
mathematics questions that require reasoning. Prior research has demonstrated
the effectiveness of chain-of-thought prompting in enhancing reasoning
capabilities. Now, we aim to investigate whether fine-tuning a model for the
generation of Prolog codes, a logic language, and subsequently passing these
codes to a compiler can further improve accuracy. Consequently, we employ
chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other
fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code +
chain-of-thought, and chain-of-thought + Prolog code, respectively. The results
reveal that the Prolog generation model surpasses the baseline in performance,
while the combination generation models do not yield significant improvements.
The Prolog corpus based on GSM8K and the correspondingly finetuned Prolog
generation model based on LLaMA7B are released to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03882">
<div class="article-summary-box-inner">
<span><p>Multi-choice questions (MCQs) serve as a common yet important task format in
the research of large language models (LLMs). Our work shows that LLMs exhibit
an inherent "selection bias" in MCQs, which refers to LLMs' preferences to
select options located at specific positions (like "Option C"). This bias is
prevalent across various LLMs, making their performance vulnerable to option
position changes in MCQs. We identify that one primary cause resulting in
selection bias is option numbering, i.e., the ID symbols A/B/C/D associated
with the options. To mitigate selection bias, we propose a new method called
PriDe. PriDe first decomposes the observed model prediction distribution into
an intrinsic prediction over option contents and a prior distribution over
option IDs. It then estimates the prior by permutating option contents on a
small number of test samples, which is used to debias the subsequent test
samples. We demonstrate that, as a label-free, inference-time method, PriDe
achieves a more effective and computation-efficient debiasing than strong
baselines. We further show that the priors estimated by PriDe generalize well
across different domains, highlighting its practical potential in broader
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing How a Chatbot References User Utterances from Previous Chatting Sessions: An Investigation of Users' Privacy Concerns and Perceptions. (arXiv:2308.04879v1 [cs.HC] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04879">
<div class="article-summary-box-inner">
<span><p>Chatbots are capable of remembering and referencing previous conversations,
but does this enhance user engagement or infringe on privacy? To explore this
trade-off, we investigated the format of how a chatbot references previous
conversations with a user and its effects on a user's perceptions and privacy
concerns. In a three-week longitudinal between-subjects study, 169 participants
talked about their dental flossing habits to a chatbot that either, (1-None):
did not explicitly reference previous user utterances, (2-Verbatim): referenced
previous utterances verbatim, or (3-Paraphrase): used paraphrases to reference
previous utterances. Participants perceived Verbatim and Paraphrase chatbots as
more intelligent and engaging. However, the Verbatim chatbot also raised
privacy concerns with participants. To gain insights as to why people prefer
certain conditions or had privacy concerns, we conducted semi-structured
interviews with 15 participants. We discuss implications from our findings that
can help designers choose an appropriate format to reference previous user
utterances and inform in the design of longitudinal dialogue scripting.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-11 23:11:06.897912021 UTC">2023-09-11 23:11:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>