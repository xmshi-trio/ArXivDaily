<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-21T01:30:00Z">09-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying Organizations for Food System Ontologies using Natural Language Processing. (arXiv:2309.10880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10880">
<div class="article-summary-box-inner">
<span><p>Our research explores the use of natural language processing (NLP) methods to
automatically classify entities for the purpose of knowledge graph population
and integration with food system ontologies. We have created NLP models that
can automatically classify organizations with respect to categories associated
with environmental issues as well as Standard Industrial Classification (SIC)
codes, which are used by the U.S. government to characterize business
activities. As input, the NLP models are provided with text snippets retrieved
by the Google search engine for each organization, which serves as a textual
description of the organization that is used for learning. Our experimental
results show that NLP models can achieve reasonably good performance for these
two classification tasks, and they rely on a general framework that could be
applied to many other classification problems as well. We believe that NLP
models represent a promising approach for automatically harvesting information
to populate knowledge graphs and aligning the information with existing
ontologies through shared categories and concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer. (arXiv:2309.10891v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10891">
<div class="article-summary-box-inner">
<span><p>Zero-shot cross-lingual transfer is a central task in multilingual NLP,
allowing models trained in languages with more sufficient training resources to
generalize to other low-resource languages. Earlier efforts on this task use
parallel corpora, bilingual dictionaries, or other annotated alignment data to
improve cross-lingual transferability, which are typically expensive to obtain.
In this paper, we propose a simple yet effective method, SALT, to improve the
zero-shot cross-lingual transfer of the multilingual pretrained language models
without the help of such external data. By incorporating code-switching and
embedding mixup with self-augmentation, SALT effectively distills cross-lingual
knowledge from the multilingual PLM and enhances its transferability on
downstream tasks. Experimental results on XNLI and PAWS-X show that our method
is able to improve zero-shot cross-lingual transferability without external
data. Our code is available at https://github.com/luka-group/SALT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans. (arXiv:2309.10898v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10898">
<div class="article-summary-box-inner">
<span><p>The text editing tasks, including sentence fusion, sentence splitting and
rephrasing, text simplification, and Grammatical Error Correction (GEC), share
a common trait of dealing with highly similar input and output sequences. This
area of research lies at the intersection of two well-established fields: (i)
fully autoregressive sequence-to-sequence approaches commonly used in tasks
like Neural Machine Translation (NMT) and (ii) sequence tagging techniques
commonly used to address tasks such as Part-of-speech tagging, Named-entity
recognition (NER), and similar. In the pursuit of a balanced architecture,
researchers have come up with numerous imaginative and unconventional
solutions, which we're discussing in the Related Works section. Our approach to
addressing text editing tasks is called RedPenNet and is aimed at reducing
architectural and parametric redundancies presented in specific
Sequence-To-Edits models, preserving their semi-autoregressive advantages. Our
models achieve $F_{0.5}$ scores of 77.60 on the BEA-2019 (test), which can be
considered as state-of-the-art the only exception for system combination and
67.71 on the UAGEC+Fluency (test) benchmarks.
</p>
<p>This research is being conducted in the context of the UNLP 2023 workshop,
where it was presented as a paper as a paper for the Shared Task in Grammatical
Error Correction (GEC) for Ukrainian. This study aims to apply the RedPenNet
approach to address the GEC problem in the Ukrainian language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10916">
<div class="article-summary-box-inner">
<span><p>Adversarial examples, deliberately crafted using small perturbations to fool
deep neural networks, were first studied in image processing and more recently
in NLP. While approaches to detecting adversarial examples in NLP have largely
relied on search over input perturbations, image processing has seen a range of
techniques that aim to characterise adversarial subspaces over the learned
representations.
</p>
<p>In this paper, we adapt two such approaches to NLP, one based on nearest
neighbors and influence functions and one on Mahalanobis distances. The former
in particular produces a state-of-the-art detector when compared against
several strong baselines; moreover, the novel use of influence functions
provides insight into how the nature of adversarial example subspaces in NLP
relate to those in image processing, and also how they differ depending on the
kind of NLP task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Speech Recognition Contextualization with Large Language Models. (arXiv:2309.10917v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10917">
<div class="article-summary-box-inner">
<span><p>In recent years, Large Language Models (LLMs) have garnered significant
attention from the research community due to their exceptional performance and
generalization capabilities. In this paper, we introduce a novel method for
contextualizing speech recognition models incorporating LLMs. Our approach
casts speech recognition as a mixed-modal language modeling task based on a
pretrained LLM. We provide audio features, along with optional text tokens for
context, to train the system to complete transcriptions in a decoder-only
fashion. As a result, the system is implicitly incentivized to learn how to
leverage unstructured contextual information during training. Our empirical
results demonstrate a significant improvement in performance, with a 6% WER
reduction when additional textual context is provided. Moreover, we find that
our method performs competitively and improve by 7.5% WER overall and 17% WER
on rare words against a baseline contextualized RNN-T system that has been
trained on more than twenty five times larger speech dataset. Overall, we
demonstrate that by only adding a handful number of trainable parameters via
adapters, we can unlock contextualized speech recognition capability for the
pretrained LLM while keeping the same text-only input functionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10923">
<div class="article-summary-box-inner">
<span><p>In this study, we propose a staging area for ingesting new superconductors'
experimental data in SuperCon that is machine-collected from scientific
articles. Our objective is to enhance the efficiency of updating SuperCon while
maintaining or enhancing the data quality. We present a semi-automatic staging
area driven by a workflow combining automatic and manual processes on the
extracted database. An anomaly detection automatic process aims to pre-screen
the collected data. Users can then manually correct any errors through a user
interface tailored to simplify the data verification on the original PDF
documents. Additionally, when a record is corrected, its raw data is collected
and utilised to improve machine learning models as training data. Evaluation
experiments demonstrate that our staging area significantly improves curation
quality. We compare the interface with the traditional manual approach of
reading PDF documents and recording information in an Excel document. Using the
interface boosts the precision and recall by 6% and 50%, respectively to an
average increase of 40% in F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Autoregressive Streaming ASR With Label Context. (arXiv:2309.10926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10926">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive (NAR) modeling has gained significant interest in speech
processing since these models achieve dramatically lower inference time than
autoregressive (AR) models while also achieving good transcription accuracy.
Since NAR automatic speech recognition (ASR) models must wait for the
completion of the entire utterance before processing, some works explore
streaming NAR models based on blockwise attention for low-latency applications.
However, streaming NAR models significantly lag in accuracy compared to
streaming AR and non-streaming NAR models. To address this, we propose a
streaming "semi-autoregressive" ASR model that incorporates the labels emitted
in previous blocks as additional context using a Language Model (LM)
subnetwork. We also introduce a novel greedy decoding algorithm that addresses
insertion and deletion errors near block boundaries while not significantly
increasing the inference time. Experiments show that our method outperforms the
existing streaming NAR model by 19% relative on Tedlium2, 16%/8% on
Librispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB) /
Callhome(CH) test sets. It also reduced the accuracy gap with streaming AR and
non-streaming NAR models while achieving 2.5x lower latency. We also
demonstrate that our approach can effectively utilize external text data to
pre-train the LM subnetwork to further improve streaming ASR accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training. (arXiv:2309.10929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10929">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce the concept of complex text style transfer tasks,
and constructed complex text datasets based on two widely applicable scenarios.
Our dataset is the first large-scale data set of its kind, with 700 rephrased
sentences and 1,000 sentences from the game Genshin Impact. While large
language models (LLM) have shown promise in complex text style transfer, they
have drawbacks such as data privacy concerns, network instability, and high
deployment costs. To address these issues, we explore the effectiveness of
small models (less than T5-3B) with implicit style pre-training through
contrastive learning. We also propose a method for automated evaluation of text
generation quality based on alignment with human evaluations using ChatGPT.
Finally, we compare our approach with existing methods and show that our model
achieves state-of-art performances of few-shot text style transfer models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Family of Pretrained Transformer Language Models for Russian. (arXiv:2309.10931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10931">
<div class="article-summary-box-inner">
<span><p>Nowadays, Transformer language models (LMs) represent a fundamental component
of the NLP research methodologies and applications. However, the development of
such models specifically for the Russian language has received little
attention. This paper presents a collection of 13 Russian Transformer LMs based
on the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and
encoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these
models is readily available via the HuggingFace platform. We provide a report
of the model architecture design and pretraining, and the results of evaluating
their generalization abilities on Russian natural language understanding and
generation datasets and benchmarks. By pretraining and releasing these
specialized Transformer LMs, we hope to broaden the scope of the NLP research
directions and enable the development of industrial solutions for the Russian
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarks for Pir\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. (arXiv:2309.10945v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10945">
<div class="article-summary-box-inner">
<span><p>Pir\'a is a reading comprehension dataset focused on the ocean, the Brazilian
coast, and climate change, built from a collection of scientific abstracts and
reports on these topics. This dataset represents a versatile language resource,
particularly useful for testing the ability of current machine learning models
to acquire expert scientific knowledge. Despite its potential, a detailed set
of baselines has not yet been developed for Pir\'a. By creating these
baselines, researchers can more easily utilize Pir\'a as a resource for testing
machine learning models across a wide range of question answering tasks. In
this paper, we define six benchmarks over the Pir\'a dataset, covering closed
generative question answering, machine reading comprehension, information
retrieval, open question answering, answer triggering, and multiple choice
question answering. As part of this effort, we have also produced a curated
version of the original dataset, where we fixed a number of grammar issues,
repetitions, and other shortcomings. Furthermore, the dataset has been extended
in several new directions, so as to face the aforementioned benchmarks:
translation of supporting texts from English into Portuguese, classification
labels for answerability, automatic paraphrases of questions and answers, and
multiple choice candidates. The results described in this paper provide several
points of reference for researchers interested in exploring the challenges
provided by the Pir\'a dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10952">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLM) have revolutionized Natural Language Processing
(NLP), improving state-of-the-art on many existing tasks and exhibiting
emergent capabilities. However, LLMs have not yet been successfully applied on
semi-structured document information extraction, which is at the core of many
document processing workflows and consists of extracting key entities from a
visually rich document (VRD) given a predefined target schema. The main
obstacles to LLM adoption in that task have been the absence of layout encoding
within LLMs, critical for a high quality extraction, and the lack of a
grounding mechanism ensuring the answer is not hallucinated. In this paper, we
introduce Language Model-based Document Information Extraction and Localization
(LMDX), a methodology to adapt arbitrary LLMs for document information
extraction. LMDX can do extraction of singular, repeated, and hierarchical
entities, both with and without training data, while providing grounding
guarantees and localizing the entities within the document. In particular, we
apply LMDX to the PaLM 2-S LLM and evaluate it on VRDU and CORD benchmarks,
setting a new state-of-the-art and showing how LMDX enables the creation of
high quality, data-efficient parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10954">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) using large language models for tasks with many
labels is challenging due to the limited context window, which makes it
difficult to fit a sufficient number of examples in the prompt. In this paper,
we use a pre-trained dense retrieval model to bypass this limitation, giving
the model only a partial view of the full label space for each inference call.
Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art
performance in few-shot settings for three common intent classification
datasets, with no finetuning. We also surpass fine-tuned performance on
fine-grained sentiment classification in certain cases. We analyze the
performance across number of in-context examples and different model scales,
showing that larger models are necessary to effectively and consistently make
use of larger context lengths for ICL. By running several ablations, we analyze
the model's use of: a) the similarity of the in-context examples to the current
input, b) the semantic content of the class names, and c) the correct
correspondence between examples and labels. We demonstrate that all three are
needed to varying degrees depending on the domain, contrary to certain recent
works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10966">
<div class="article-summary-box-inner">
<span><p>Recent research in decoding methods for Natural Language Generation (NLG)
tasks has shown that the traditional beam search and greedy decoding algorithms
are not optimal, because model probabilities do not always align with human
preferences. Stronger decoding methods, including Quality Estimation (QE)
reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to
mitigate the model-perplexity-vs-quality mismatch. While these decoding methods
achieve state-of-the-art performance, they are prohibitively expensive to
compute. In this work, we propose MBR finetuning and QE finetuning which
distill the quality gains from these decoding methods at training time, while
using an efficient decoding algorithm at inference time. Using the canonical
NLG task of Neural Machine Translation (NMT), we show that even with
self-training, these finetuning methods significantly outperform the base
model. Moreover, when using an external LLM as a teacher model, these
finetuning methods outperform finetuning on human-generated references. These
findings suggest new ways to leverage monolingual data to achieve improvements
in model quality that are on par with, or even exceed, improvements from
human-curated data, while maintaining maximum efficiency during decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model. (arXiv:2309.11000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11000">
<div class="article-summary-box-inner">
<span><p>This paper explores the potential of constructing an AI spoken dialogue
system that "thinks how to respond" and "thinks how to speak" simultaneously,
which more closely aligns with the human speech production process compared to
the current cascade pipeline of independent chatbot and Text-to-Speech (TTS)
modules. We hypothesize that Large Language Models (LLMs) with billions of
parameters possess significant speech understanding capabilities and can
jointly model dialogue responses and linguistic features. We conduct two sets
of experiments: 1) Prosodic structure prediction, a typical front-end task in
TTS, demonstrating the speech understanding ability of LLMs, and 2) Further
integrating dialogue response and a wide array of linguistic features using a
unified encoding format. Our results indicate that the LLM-based approach is a
promising direction for building unified spoken dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach. (arXiv:2309.11027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11027">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) aims to extract and classify entity mentions
in the text into pre-defined types (e.g., organization or person name).
Recently, many works have been proposed to shape the NER as a machine reading
comprehension problem (also termed MRC-based NER), in which entity recognition
is achieved by answering the formulated questions related to pre-defined entity
types through MRC, based on the contexts. However, these works ignore the label
dependencies among entity types, which are critical for precisely recognizing
named entities. In this paper, we propose to incorporate the label dependencies
among entity types into a multi-task learning framework for better MRC-based
NER. We decompose MRC-based NER into multiple tasks and use a self-attention
module to capture label dependencies. Comprehensive experiments on both nested
NER and flat NER datasets are conducted to validate the effectiveness of the
proposed Multi-NER. Experimental results show that Multi-NER can achieve better
performance on all datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11042">
<div class="article-summary-box-inner">
<span><p>Recently, Large Language Models (LLMs) have achieved amazing zero-shot
learning performance over a variety of Natural Language Processing (NLP) tasks,
especially for text generative tasks. Yet, the large size of LLMs often leads
to the high computational cost of model training and online deployment. In our
work, we present ALTER, a system that effectively builds the multi-tAsk
Learners with mixTure-of-task-adaptERs upon small language models (with &lt;1B
parameters) to address multiple NLP tasks simultaneously, capturing the
commonalities and differences between tasks, in order to support
domain-specific applications. Specifically, in ALTER, we propose the
Mixture-of-Task-Adapters (MTA) module as an extension to the transformer
architecture for the underlying model to capture the intra-task and inter-task
knowledge. A two-stage training method is further proposed to optimize the
collaboration between adapters at a small computational cost. Experimental
results over a mixture of NLP tasks show that our proposed MTA architecture and
the two-stage training method achieve good performance. Based on ALTER, we have
also produced MTA-equipped language models for various domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks. (arXiv:2309.11046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11046">
<div class="article-summary-box-inner">
<span><p>Across various domains, data from different sources such as Baidu Baike and
Wikipedia often manifest in distinct forms. Current entity matching
methodologies predominantly focus on homogeneous data, characterized by
attributes that share the same structure and concise attribute values. However,
this orientation poses challenges in handling data with diverse formats.
Moreover, prevailing approaches aggregate the similarity of attribute values
between corresponding attributes to ascertain entity similarity. Yet, they
often overlook the intricate interrelationships between attributes, where one
attribute may have multiple associations. The simplistic approach of pairwise
attribute comparison fails to harness the wealth of information encapsulated
within entities.To address these challenges, we introduce a novel entity
matching model, dubbed Entity Matching Model for Capturing Complex Attribute
Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model
transforms the matching task into a sequence matching problem to mitigate the
impact of varying data formats. Moreover, by introducing attention mechanisms,
it identifies complex relationships between attributes, emphasizing the degree
of matching among multiple attributes rather than one-to-one correspondences.
Through the integration of the EMM-CCAR model, we adeptly surmount the
challenges posed by data heterogeneity and intricate attribute
interdependencies. In comparison with the prevalent DER-SSM and Ditto
approaches, our model achieves improvements of approximately 4% and 1% in F1
scores, respectively. This furnishes a robust solution for addressing the
intricacies of attribute complexity in entity matching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11049">
<div class="article-summary-box-inner">
<span><p>Question answering on tabular data (TableQA), which aims at generating
answers to questions grounded on a given table, has attracted increasing
attention in recent years. Existing work tends to generate factual short-form
answers by extracting information from one or a few table cells without
reasoning over selected table cells. However, the free-form TableQA, requiring
a more complex relevant table cell selection strategy and the complex
integration and inference of separate pieces of information, has been
under-explored. To this end, this paper proposes a generalized three-stage
approach: Table-to-Graph conversion and cell localizing, external knowledge
retrieval and table-text fusion (called TAG-QA), addressing the challenge of
inferring long free-form answer for generative TableQA. In particular, TAG-QA
(1) locates relevant table cells using a graph neural network to gather
intersecting cells between relevant rows and columns; (2) leverages external
knowledge from Wikipedia and (3) generates answers by integrating both tabular
data and natural linguistic information. Experiments with a human evaluation
demonstrate that TAG-QA is capable of generating more faithful and coherent
sentence when compared with several state-of-the-art baselines. Especially,
TAG-QA outperforms the strong pipeline-based baseline TAPAS by 17% and 14%, in
terms of BLEU-4 and PARENT F-score, respectively. Moreover, TAG-QA outperforms
end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake News BR: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11052">
<div class="article-summary-box-inner">
<span><p>The proliferation of fake news has become a significant concern in recent
times due to its potential to spread misinformation and manipulate public
opinion. In this paper, we present a comprehensive study on the detection of
fake news in Brazilian Portuguese, focusing on journalistic-type news. We
propose a machine learning-based approach that leverages natural language
processing techniques, including TF-IDF and Word2Vec, to extract features from
textual data. We evaluate the performance of various classification algorithms,
such as logistic regression, support vector machine, random forest, AdaBoost,
and LightGBM, on a dataset containing both true and fake news articles. The
proposed approach achieves a high level of accuracy and F1-Score, demonstrating
its effectiveness in identifying fake news. Additionally, we develop a
user-friendly web platform, FAKENEWSBR.COM, to facilitate the verification of
news articles' veracity. Our platform provides real-time analysis, allowing
users to assess the likelihood of news articles being fake. Through empirical
analysis and comparative studies, we demonstrate the potential of our approach
to contribute to the fight against the spread of fake news and promote more
informed media consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11054">
<div class="article-summary-box-inner">
<span><p>Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem
solving. We conduct a comprehensive examination of methods for designing CoT,
comparing conventional natural language CoT with various program CoTs,
including the self-describing program, the comment-describing program, and the
non-describing program. Furthermore, we investigate the impact of programming
language on program CoTs, comparing Python and Wolfram Language. Through
extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs
often have superior effectiveness in math problem solving. Notably, the best
performing combination with 30B parameters beats GPT-3.5-turbo by a significant
margin. The results show that self-describing program offers greater diversity
and thus can generally achieve higher performance. We also find that Python is
a better choice of language than Wolfram for program CoTs. The experimental
results provide a valuable guideline for future CoT designs that take into
account both programming language and coding style for further advancements.
Our datasets and code are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates. (arXiv:2309.11063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11063">
<div class="article-summary-box-inner">
<span><p>Text editing is a crucial task that involves modifying text to better align
with user intents. However, existing text editing benchmark datasets have
limitations in providing only coarse-grained instructions. Consequently,
although the edited output may seem reasonable, it often deviates from the
intended changes outlined in the gold reference, resulting in low evaluation
scores. To comprehensively investigate the text editing capabilities of large
language models, this paper introduces XATU, the first benchmark specifically
designed for fine-grained instruction-based explainable text editing. XATU
covers a wide range of topics and text types, incorporating lexical, syntactic,
semantic, and knowledge-intensive edits. To enhance interpretability, we
leverage high-quality data sources and human annotation, resulting in a
benchmark that includes fine-grained instructions and gold-standard edit
explanations. By evaluating existing open and closed large language models
against our benchmark, we demonstrate the effectiveness of instruction tuning
and the impact of underlying architecture across various editing tasks.
Furthermore, extensive experimentation reveals the significant role of
explanations in fine-tuning language models for text editing tasks. The
benchmark will be open-sourced to support reproduction and facilitate future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt. (arXiv:2309.11065v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11065">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that multi-task pre-training greatly improves the
model's robustness and transfer ability, which is crucial for building a
high-quality dialog system. However, most previous works on multi-task
pre-training rely heavily on human-defined input format or prompt, which is not
optimal in quality and quantity. In this work, we propose to use Task-based
Automatic Prompt generation (TAP) to automatically generate high-quality
prompts. Using the high-quality prompts generated, we scale the corpus of the
pre-trained conversation model to 122 datasets from 15 dialog-related tasks,
resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful
foundation model for various conversational tasks and different dialog systems.
Extensive experiments have shown that UniPCM is robust to input prompts and
capable of various dialog-related tasks. Moreover, UniPCM has strong transfer
ability and excels at low resource scenarios, achieving SOTA results on 9
different datasets ranging from task-oriented dialog to open-domain
conversation. Furthermore, we are amazed to find that TAP can generate prompts
on par with those collected with crowdsourcing. The code is released with the
paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11082">
<div class="article-summary-box-inner">
<span><p>In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11093">
<div class="article-summary-box-inner">
<span><p>Lyric translation, a field studied for over a century, is now attracting
computational linguistics researchers. We identified two limitations in
previous studies. Firstly, lyric translation studies have predominantly focused
on Western genres and languages, with no previous study centering on K-pop
despite its popularity. Second, the field of lyric translation suffers from a
lack of publicly available datasets; to the best of our knowledge, no such
dataset exists. To broaden the scope of genres and languages in lyric
translation studies, we introduce a novel singable lyric translation dataset,
approximately 89\% of which consists of K-pop song lyrics. This dataset aligns
Korean and English lyrics line-by-line and section-by-section. We leveraged
this dataset to unveil unique characteristics of K-pop lyric translation,
distinguishing it from other extensively studied genres, and to construct a
neural lyric translation model, thereby underscoring the importance of a
dedicated dataset for singable lyric translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttentionMix: Data augmentation method that relies on BERT attention mechanism. (arXiv:2309.11104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11104">
<div class="article-summary-box-inner">
<span><p>The Mixup method has proven to be a powerful data augmentation technique in
Computer Vision, with many successors that perform image mixing in a guided
manner. One of the interesting research directions is transferring the
underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP).
Even though there already exist several methods that apply Mixup to textual
data, there is still room for new, improved approaches. In this work, we
introduce AttentionMix, a novel mixing method that relies on attention-based
information. While the paper focuses on the BERT attention mechanism, the
proposed approach can be applied to generally any attention-based model.
AttentionMix is evaluated on 3 standard sentiment classification datasets and
in all three cases outperforms two benchmark approaches that utilize Mixup
mechanism, as well as the vanilla BERT method. The results confirm that the
attention-based information can be effectively used for data augmentation in
the NLP domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation. (arXiv:2309.11127v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11127">
<div class="article-summary-box-inner">
<span><p>By integrating recent advances in large language models (LLMs) and generative
models into the emerging semantic communication (SC) paradigm, in this article
we put forward to a novel framework of language-oriented semantic communication
(LSC). In LSC, machines communicate using human language messages that can be
interpreted and manipulated via natural language processing (NLP) techniques
for SC efficiency. To demonstrate LSC's potential, we introduce three
innovative algorithms: 1) semantic source coding (SSC) which compresses a text
prompt into its key head words capturing the prompt's syntactic essence while
maintaining their appearance order to keep the prompt's context; 2) semantic
channel coding (SCC) that improves robustness against errors by substituting
head words with their lenghthier synonyms; and 3) semantic knowledge
distillation (SKD) that produces listener-customized prompts via in-context
learning the listener's language style. In a communication task for progressive
text-to-image generation, the proposed methods achieve higher perceptual
similarities with fewer transmissions while enhancing robustness in noisy
communication channels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype of a robotic system to assist the learning process of English language with text-generation through DNN. (arXiv:2309.11142v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11142">
<div class="article-summary-box-inner">
<span><p>In the last ongoing years, there has been a significant ascending on the
field of Natural Language Processing (NLP) for performing multiple tasks
including English Language Teaching (ELT). An effective strategy to favor the
learning process uses interactive devices to engage learners in their
self-learning process. In this work, we present a working prototype of a
humanoid robotic system to assist English language self-learners through text
generation using Long Short Term Memory (LSTM) Neural Networks. The learners
interact with the system using a Graphic User Interface that generates text
according to the English level of the user. The experimentation was conducted
using English learners and the results were measured accordingly to
International English Language Testing System (IELTS) rubric. Preliminary
results show an increment in the Grammatical Range of learners who interacted
with the system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11143">
<div class="article-summary-box-inner">
<span><p>Unsupervised sentence representation learning aims to transform input
sentences into fixed-length vectors enriched with intricate semantic
information while obviating the reliance on labeled data. Recent progress
within this field, propelled by contrastive learning and prompt engineering,
has significantly bridged the gap between unsupervised and supervised
strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains
largely untapped within this trajectory. To unlock latent capabilities within
pre-trained models, such as BERT, we propose a two-stage approach for sentence
representation: comprehension and summarization. Subsequently, the output of
the latter phase is harnessed as the vectorized representation of the input
sentence. For further performance enhancement, we meticulously refine both the
contrastive learning loss function and the template denoising technique for
prompt engineering. Rigorous experimentation substantiates our method,
CoT-BERT, transcending a suite of robust baselines without necessitating other
text representation models or external databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessment of Pre-Trained Models Across Languages and Grammars. (arXiv:2309.11165v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11165">
<div class="article-summary-box-inner">
<span><p>We present an approach for assessing how multilingual large language models
(LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to
recover constituent and dependency structures by casting parsing as sequence
labeling. To do so, we select a few LLMs and study them on 13 diverse UD
treebanks for dependency parsing and 10 treebanks for constituent parsing. Our
results show that: (i) the framework is consistent across encodings, (ii)
pre-trained word vectors do not favor constituency representations of syntax
over dependencies, (iii) sub-word tokenization is needed to represent syntax,
in contrast to character-based models, and (iv) occurrence of a language in the
pretraining data is more important than the amount of task data when recovering
syntax from the word vectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11166">
<div class="article-summary-box-inner">
<span><p>The swift advancement in the scale and capabilities of Large Language Models
(LLMs) positions them as promising tools for a variety of downstream tasks. In
addition to the pursuit of better performance and the avoidance of violent
feedback on a certain prompt, to ensure the responsibility of the LLM, much
attention is drawn to the robustness of LLMs. However, existing evaluation
methods mostly rely on traditional question answering datasets with predefined
supervised labels, which do not align with the superior generation capabilities
of contemporary LLMs. To address this issue, we propose a novel rational
evaluation approach that leverages pre-trained reward models as diagnostic
tools to evaluate the robustness of LLMs, which we refer to as the Reward Model
for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical
experiments have demonstrated that TREval provides an accurate method for
evaluating the robustness of an LLM, especially when faced with more
challenging open questions. Furthermore, our results demonstrate that LLMs
frequently exhibit vulnerability to word-level perturbations, which are
commonplace in daily language usage. Notably, we were surprised to discover
that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted.
The code of TREval is available in https://github.com/Harry-mic/TREval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute. (arXiv:2309.11197v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11197">
<div class="article-summary-box-inner">
<span><p>The Languini Kitchen serves as both a research collective and codebase
designed to empower researchers with limited computational resources to
contribute meaningfully to the field of language modelling. We introduce an
experimental protocol that enables model comparisons based on equivalent
compute, measured in accelerator hours. The number of tokens on which a model
is trained is defined by the model's throughput and the chosen compute class.
Notably, this approach avoids constraints on critical hyperparameters which
affect total parameters or floating-point operations. For evaluation, we
pre-process an existing large, diverse, and high-quality dataset of books that
surpasses existing academic benchmarks in quality, diversity, and document
length. On it, we compare methods based on their empirical scaling trends which
are estimated through experiments at various levels of compute. This work also
provides two baseline models: a feed-forward model derived from the GPT-2
architecture and a recurrent model in the form of a novel LSTM with ten-fold
throughput. While the GPT baseline achieves better perplexity throughout all
our levels of compute, our LSTM baseline exhibits a predictable and more
favourable scaling law. This is due to the improved throughput and the need for
fewer training tokens to achieve the same decrease in test perplexity.
Extrapolating the scaling laws leads of both models results in an intersection
at roughly 50,000 accelerator hours. We hope this work can serve as the
foundation for meaningful and reproducible language modelling research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11206">
<div class="article-summary-box-inner">
<span><p>Despite their competitive performance on knowledge-intensive tasks, large
language models (LLMs) still have limitations in memorizing all world knowledge
especially long tail knowledge. In this paper, we study the KG-augmented
language model approach for solving the knowledge graph question answering
(KGQA) task that requires rich world knowledge. Existing work has shown that
retrieving KG knowledge to enhance LLMs prompting can significantly improve
LLMs performance in KGQA. However, their approaches lack a well-formed
verbalization of KG knowledge, i.e., they ignore the gap between KG
representations and textual representations. To this end, we propose an
answer-sensitive KG-to-Text approach that can transform KG knowledge into
well-textualized statements most informative for KGQA. Based on this approach,
we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.
Experiments on several KGQA benchmarks show that the proposed KG-to-Text
augmented LLMs approach outperforms previous KG-augmented LLMs approaches
regarding answer accuracy and usefulness of knowledge statements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speak While You Think: Streaming Speech Synthesis During Text Generation. (arXiv:2309.11210v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11210">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) demonstrate impressive capabilities, yet
interaction with these models is mostly facilitated through text. Using
Text-To-Speech to synthesize LLM outputs typically results in notable latency,
which is impractical for fluent voice conversations. We propose LLM2Speech, an
architecture to synthesize speech while text is being generated by an LLM which
yields significant latency reduction. LLM2Speech mimics the predictions of a
non-streaming teacher model while limiting the exposure to future context in
order to enable streaming. It exploits the hidden embeddings of the LLM, a
by-product of the text generation that contains informative semantic context.
Experimental results show that LLM2Speech maintains the teacher's quality while
reducing the latency to enable natural conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. (arXiv:2309.11235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11235">
<div class="article-summary-box-inner">
<span><p>Nowadays, open-source large language models like LLaMA have emerged. Recent
developments have incorporated supervised fine-tuning (SFT) and reinforcement
learning fine-tuning (RLFT) to align these models with human goals. However,
SFT methods treat all training data with mixed quality equally, while RLFT
methods require high-quality pairwise or ranking-based preference data. In this
study, we present a novel framework, named OpenChat, to advance open-source
language models with mixed-quality data. Specifically, we consider the general
SFT training data, consisting of a small amount of expert data mixed with a
large proportion of sub-optimal data, without any preference labels. We propose
the C(onditioned)-RLFT, which regards different data sources as coarse-grained
reward labels and learns a class-conditioned policy to leverage complementary
data quality information. Interestingly, the optimal policy in C-RLFT can be
easily solved through single-stage, RL-free supervised learning, which is
lightweight and avoids costly human preference labeling. Through extensive
experiments on three standard benchmarks, our openchat-13b fine-tuned with
C-RLFT achieves the highest average performance among all 13b open-source
language models. Moreover, we use AGIEval to validate the model generalization
performance, in which only openchat-13b surpasses the base model. Finally, we
conduct a series of analyses to shed light on the effectiveness and robustness
of OpenChat. Our code, data, and models are publicly available at
https://github.com/imoneoi/openchat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Scenario Refiner: Grounding subjects in images at the morphological level. (arXiv:2309.11252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11252">
<div class="article-summary-box-inner">
<span><p>Derivationally related words, such as "runner" and "running", exhibit
semantic differences which also elicit different visual scenarios. In this
paper, we ask whether Vision and Language (V\&amp;L) models capture such
distinctions at the morphological level, using a a new methodology and dataset.
We compare the results from V\&amp;L models to human judgements and find that
models' predictions differ from those of human participants, in particular
displaying a grammatical bias. We further investigate whether the human-model
misalignment is related to model architecture. Our methodology, developed on
one specific morphological contrast, can be further extended for testing models
on capturing other nuanced language features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11259">
<div class="article-summary-box-inner">
<span><p>In recent years, substantial advancements in pre-trained language models have
paved the way for the development of numerous non-English language versions,
with a particular focus on encoder-only and decoder-only architectures. While
Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited
prowess in natural language understanding and generation, there remains a
scarcity of encoder-decoder models designed for sequence-to-sequence tasks
involving input-output pairs. This paper breaks new ground by introducing the
implementation and evaluation of renowned encoder-decoder architectures,
exclusively pre-trained on Spanish corpora. Specifically, we present Spanish
versions of BART, T5, and BERT2BERT-style models and subject them to a
comprehensive assessment across a diverse range of sequence-to-sequence tasks,
spanning summarization, rephrasing, and generative question answering. Our
findings underscore the competitive performance of all models, with BART and T5
emerging as top performers across all evaluated tasks. As an additional
contribution, we have made all models publicly available to the research
community, fostering future exploration and development in Spanish language
processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounded Complex Task Segmentation for Conversational Assistants. (arXiv:2309.11271v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11271">
<div class="article-summary-box-inner">
<span><p>Following complex instructions in conversational assistants can be quite
daunting due to the shorter attention and memory spans when compared to reading
the same instructions. Hence, when conversational assistants walk users through
the steps of complex tasks, there is a need to structure the task into
manageable pieces of information of the right length and complexity. In this
paper, we tackle the recipes domain and convert reading structured instructions
into conversational structured ones. We annotated the structure of instructions
according to a conversational scenario, which provided insights into what is
expected in this setting. To computationally model the conversational step's
characteristics, we tested various Transformer-based architectures, showing
that a token-based approach delivers the best results. A further user study
showed that users tend to favor steps of manageable complexity and length, and
that the proposed methodology can improve the original web-based instructional
text. Specifically, 86% of the evaluated tasks were improved from a
conversational suitability point of view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Wizard of Curiosities: Enriching Dialogues with Fun Facts. (arXiv:2309.11283v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11283">
<div class="article-summary-box-inner">
<span><p>Introducing curiosities in a conversation is a way to teach something new to
the person in a pleasant and enjoyable way. Enriching dialogues with
contextualized curiosities can improve the users' perception of a dialog system
and their overall user experience. In this paper, we introduce a set of curated
curiosities, targeting dialogues in the cooking and DIY domains. In particular,
we use real human-agent conversations collected in the context of the Amazon
Alexa TaskBot challenge, a multimodal and multi-turn conversational setting.
According to an A/B test with over 1000 conversations, curiosities not only
increase user engagement, but provide an average relative rating improvement of
9.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains. (arXiv:2309.11285v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11285">
<div class="article-summary-box-inner">
<span><p>This paper presents the overview of the AuTexTification shared task as part
of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the
framework of the SEPLN 2023 conference. AuTexTification consists of two
subtasks: for Subtask 1, participants had to determine whether a text is
human-authored or has been generated by a large language model. For Subtask 2,
participants had to attribute a machine-generated text to one of six different
text generation models. Our AuTexTification 2023 dataset contains more than
160.000 texts across two languages (English and Spanish) and five domains
(tweets, reviews, news, legal, and how-to articles). A total of 114 teams
signed up to participate, of which 36 sent 175 runs, and 20 of them sent their
working notes. In this overview, we present the AuTexTification dataset and
task, the submitted participating systems, and the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11295">
<div class="article-summary-box-inner">
<span><p>We present Clinical Prediction with Large Language Models (CPLLM), a method
that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical
disease prediction. We utilized quantization and fine-tuned the LLM using
prompts, with the task of predicting whether patients will be diagnosed with a
target disease during their next visit or in the subsequent diagnosis,
leveraging their historical diagnosis records. We compared our results versus
various baselines, including Logistic Regression, RETAIN, and Med-BERT, which
is the current state-of-the-art model for disease prediction using structured
EHR data. Our experiments have shown that CPLLM surpasses all the tested models
in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements
compared to the baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features. (arXiv:2309.11307v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11307">
<div class="article-summary-box-inner">
<span><p>Predicting the success of Conversational Task Assistants (CTA) can be
critical to understand user behavior and act accordingly. In this paper, we
propose TB-Rater, a Transformer model which combines conversational-flow
features with user behavior features for predicting user ratings in a CTA
scenario. In particular, we use real human-agent conversations and ratings
collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn
conversational context. Our results show the advantages of modeling both the
conversational-flow and behavioral aspects of the conversation in a single
model for offline rating prediction. Additionally, an analysis of the
CTA-specific behavioral features brings insights into this setting and can be
used to bootstrap future systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services. (arXiv:2309.11325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11325">
<div class="article-summary-box-inner">
<span><p>We propose DISC-LawLLM, an intelligent legal system utilizing large language
models (LLMs) to provide a wide range of legal services. We adopt legal
syllogism prompting strategies to construct supervised fine-tuning datasets in
the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability.
We augment LLMs with a retrieval module to enhance models' ability to access
and utilize external legal knowledge. A comprehensive legal benchmark,
DISC-Law-Eval, is presented to evaluate intelligent legal systems from both
objective and subjective dimensions. Quantitative and qualitative results on
DISC-Law-Eval demonstrate the effectiveness of our system in serving various
users across diverse legal scenarios. The detailed resources are available at
https://github.com/FudanDISC/DISC-LawLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11327">
<div class="article-summary-box-inner">
<span><p>Crafting an effective Automatic Speech Recognition (ASR) solution for
dialects demands innovative approaches that not only address the data scarcity
issue but also navigate the intricacies of linguistic diversity. In this paper,
we address the aforementioned ASR challenge, focusing on the Tunisian dialect.
First, textual and audio data is collected and in some cases annotated. Second,
we explore self-supervision, semi-supervision and few-shot code-switching
approaches to push the state-of-the-art on different Tunisian test sets;
covering different acoustic, linguistic and prosodic conditions. Finally, and
given the absence of conventional spelling, we produce a human evaluation of
our transcripts to avoid the noise coming from spelling inadequacies in our
testing references. Our models, allowing to transcribe audio samples in a
linguistic mix involving Tunisian Arabic, English and French, and all the data
used during training and testing are released for public use and further
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRAVID: An End-to-End Video Translation Framework. (arXiv:2309.11338v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11338">
<div class="article-summary-box-inner">
<span><p>In today's globalized world, effective communication with people from diverse
linguistic backgrounds has become increasingly crucial. While traditional
methods of language translation, such as written text or voice-only
translations, can accomplish the task, they often fail to capture the complete
context and nuanced information conveyed through nonverbal cues like facial
expressions and lip movements. In this paper, we present an end-to-end video
translation system that not only translates spoken language but also
synchronizes the translated speech with the lip movements of the speaker. Our
system focuses on translating educational lectures in various Indian languages,
and it is designed to be effective even in low-resource system settings. By
incorporating lip movements that align with the target language and matching
them with the speaker's voice using voice cloning techniques, our application
offers an enhanced experience for students and users. This additional feature
creates a more immersive and realistic learning environment, ultimately making
the learning process more effective and engaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11341">
<div class="article-summary-box-inner">
<span><p>Classifying research output into context-specific label taxonomies is a
challenging and relevant downstream task, given the volume of existing and
newly published articles. We propose a method to enhance the performance of
article classification by enriching simple Graph Neural Networks (GNN)
pipelines with edge-heterogeneous graph representations. SciBERT is used for
node feature generation to capture higher-order semantics within the articles'
textual metadata. Fully supervised transductive node classification experiments
are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the
PubMed diabetes dataset, augmented with additional metadata from Microsoft
Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate
that edge-heterogeneous graphs consistently improve the performance of all GNN
models compared to the edge-homogeneous graphs. The transformed data enable
simple and shallow GNN pipelines to achieve results on par with more complex
architectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition
with a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with
sub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures
using a 2-layer GraphSAGE by including additional co-authorship edges in the
graph (accuracy 89.88%). The implementation is available at:
$\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GECTurk: Grammatical Error Correction and Detection Dataset for Turkish. (arXiv:2309.11346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11346">
<div class="article-summary-box-inner">
<span><p>Grammatical Error Detection and Correction (GEC) tools have proven useful for
native speakers and second language learners. Developing such tools requires a
large amount of parallel, annotated data, which is unavailable for most
languages. Synthetic data generation is a common practice to overcome the
scarcity of such data. However, it is not straightforward for morphologically
rich languages like Turkish due to complex writing rules that require
phonological, morphological, and syntactic information. In this work, we
present a flexible and extensible synthetic data generation pipeline for
Turkish covering more than 20 expert-curated grammar and spelling rules
(a.k.a., writing rules) implemented through complex transformation functions.
Using this pipeline, we derive 130,000 high-quality parallel sentences from
professionally edited articles. Additionally, we create a more realistic test
set by manually annotating a set of movie reviews. We implement three baselines
formulating the task as i) neural machine translation, ii) sequence tagging,
and iii) prefix tuning with a pretrained decoder-only model, achieving strong
results. Furthermore, we perform exhaustive experiments on out-of-domain
datasets to gain insights on the transferability and robustness of the proposed
approaches. Our results suggest that our corpus, GECTurk, is high-quality and
allows knowledge transfer for the out-of-domain setting. To encourage further
research on Turkish GEC, we release our datasets, baseline models, and the
synthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff. (arXiv:2309.11379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11379">
<div class="article-summary-box-inner">
<span><p>Blockwise self-attentional encoder models have recently emerged as one
promising end-to-end approach to simultaneous speech translation. These models
employ a blockwise beam search with hypothesis reliability scoring to determine
when to wait for more input speech before translating further. However, this
method maintains multiple hypotheses until the entire speech input is consumed
-- this scheme cannot directly show a single \textit{incremental} translation
to users. Further, this method lacks mechanisms for \textit{controlling} the
quality vs. latency tradeoff. We propose a modified incremental blockwise beam
search incorporating local agreement or hold-$n$ policies for quality-latency
control. We apply our framework to models trained for online or offline
translation and demonstrate that both types can be effectively used in online
mode.
</p>
<p>Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing
latency or 0.8-1.4 s latency improvement without changing quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying Lobby Influence in the European Parliament. (arXiv:2309.11381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11381">
<div class="article-summary-box-inner">
<span><p>We present a method based on natural language processing (NLP), for studying
the influence of interest groups (lobbies) in the law-making process in the
European Parliament (EP). We collect and analyze novel datasets of lobbies'
position papers and speeches made by members of the EP (MEPs). By comparing
these texts on the basis of semantic similarity and entailment, we are able to
discover interpretable links between MEPs and lobbies. In the absence of a
ground-truth dataset of such links, we perform an indirect validation by
comparing the discovered links with a dataset, which we curate, of retweet
links between MEPs and lobbies, and with the publicly disclosed meetings of
MEPs. Our best method achieves an AUC score of 0.77 and performs significantly
better than several baselines. Moreover, an aggregate analysis of the
discovered links, between groups of related lobbies and political groups of
MEPs, correspond to the expectations from the ideology of the groups (e.g.,
center-left groups are associated with social causes). We believe that this
work, which encompasses the methodology, datasets, and results, is a step
towards enhancing the transparency of the intricate decision-making processes
within democratic institutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions. (arXiv:2309.11382v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11382">
<div class="article-summary-box-inner">
<span><p>Visual language navigation (VLN) is an embodied task demanding a wide range
of skills encompassing understanding, perception, and planning. For such a
multifaceted challenge, previous VLN methods totally rely on one model's own
thinking to make predictions within one round. However, existing models, even
the most advanced large language model GPT4, still struggle with dealing with
multiple tasks by single-round self-thinking. In this work, drawing inspiration
from the expert consultation meeting, we introduce a novel zero-shot VLN
framework. Within this framework, large models possessing distinct abilities
are served as domain experts. Our proposed navigation agent, namely DiscussNav,
can actively discuss with these experts to collect essential information before
moving at every step. These discussions cover critical navigation subtasks like
instruction understanding, environment perception, and completion estimation.
Through comprehensive experiments, we demonstrate that discussions with domain
experts can effectively facilitate navigation by perceiving
instruction-relevant information, correcting inadvertent errors, and sifting
through in-consistent movement decisions. The performances on the
representative VLN task R2R show that our method surpasses the leading
zero-shot VLN model by a large margin on all metrics. Additionally, real-robot
experiments display the obvious advantages of our method over single-round
self-thinking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Form End-to-End Speech Translation via Latent Alignment Segmentation. (arXiv:2309.11384v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11384">
<div class="article-summary-box-inner">
<span><p>Current simultaneous speech translation models can process audio only up to a
few seconds long. Contemporary datasets provide an oracle segmentation into
sentences based on human-annotated transcripts and translations. However, the
segmentation into sentences is not available in the real world. Current speech
segmentation approaches either offer poor segmentation quality or have to trade
latency for quality. In this paper, we propose a novel segmentation approach
for a low-latency end-to-end speech translation. We leverage the existing
speech translation encoder-decoder architecture with ST CTC and show that it
can perform the segmentation task without supervision or additional parameters.
To the best of our knowledge, our method is the first that allows an actual
end-to-end simultaneous speech translation, as the same model is used for
translation and segmentation at the same time. On a diverse set of language
pairs and in- and out-of-domain data, we show that the proposed approach
achieves state-of-the-art quality at no additional computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safurai 001: New Qualitative Approach for Code LLM Evaluation. (arXiv:2309.11385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11385">
<div class="article-summary-box-inner">
<span><p>This paper presents Safurai-001, a new Large Language Model (LLM) with
significant potential in the domain of coding assistance. Driven by recent
advancements in coding LLMs, Safurai-001 competes in performance with the
latest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al.,
2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more
conversational interaction. By capitalizing on the progress in data engineering
(including latest techniques of data transformation and prompt engineering) and
instruction tuning, this new model promises to stand toe-to-toe with recent
closed and open source developments. Recognizing the need for an efficacious
evaluation metric for coding LLMs, this paper also introduces GPT4-based
MultiParameters, an evaluation benchmark that harnesses varied parameters to
present a comprehensive insight into the models functioning and performance.
Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and
WizardCoder by 18.78% in the Code Readability parameter and more.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kosmos-2.5: A Multimodal Literate Model. (arXiv:2309.11419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11419">
<div class="article-summary-box-inner">
<span><p>We present Kosmos-2.5, a multimodal literate model for machine reading of
text-intensive images. Pre-trained on large-scale text-intensive images,
Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1)
generating spatially-aware text blocks, where each block of text is assigned
its spatial coordinates within the image, and (2) producing structured text
output that captures styles and structures into the markdown format. This
unified multimodal literate capability is achieved through a shared Transformer
architecture, task-specific prompts, and flexible text representations. We
evaluate Kosmos-2.5 on end-to-end document-level text recognition and
image-to-markdown text generation. Furthermore, the model can be readily
adapted for any text-intensive image understanding task with different prompts
through supervised fine-tuning, making it a general-purpose tool for real-world
applications involving text-rich images. This work also paves the way for the
future scaling of multimodal large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11436">
<div class="article-summary-box-inner">
<span><p>Autonomous user interface (UI) agents aim to facilitate task automation by
interacting with the user interface without manual intervention. Recent studies
have investigated eliciting the capabilities of large language models (LLMs)
for effective engagement in diverse environments. To align with the
input-output requirement of LLMs, existing approaches are developed under a
sandbox setting where they rely on external tools and application-specific APIs
to parse the environment into textual elements and interpret the predicted
actions. Consequently, those approaches often grapple with inference
inefficiency and error propagation risks. To mitigate the challenges, we
introduce Auto-UI, a multimodal solution that directly interacts with the
interface, bypassing the need for environment parsing or reliance on
application-dependent APIs. Moreover, we propose a chain-of-action technique --
leveraging a series of intermediate previous action histories and future action
plans -- to help the agent decide what action to execute. We evaluate our
approach on a new device-control benchmark AITW with 30K unique instructions,
spanning multi-step tasks such as application operation, web searching, and web
shopping. Experimental results show that Auto-UI achieves state-of-the-art
performance with an action type prediction accuracy of 90% and an overall
action success rate of 74%. Code is publicly available at
https://github.com/cooelf/Auto-UI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction. (arXiv:2309.11439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11439">
<div class="article-summary-box-inner">
<span><p>In Grammatical Error Correction (GEC), it is crucial to ensure the user's
comprehension of a reason for correction. Existing studies present tokens,
examples, and hints as to the basis for correction but do not directly explain
the reasons for corrections. Although methods that use Large Language Models
(LLMs) to provide direct explanations in natural language have been proposed
for various tasks, no such method exists for GEC. Generating explanations for
GEC corrections involves aligning input and output tokens, identifying
correction points, and presenting corresponding explanations consistently.
However, it is not straightforward to specify a complex format to generate
explanations, because explicit control of generation is difficult with prompts.
This study introduces a method called controlled generation with Prompt
Insertion (PI) so that LLMs can explain the reasons for corrections in natural
language. In PI, LLMs first correct the input text, and then we automatically
extract the correction points based on the rules. The extracted correction
points are sequentially inserted into the LLM's explanation output as prompts,
guiding the LLMs to generate explanations for the correction points. We also
create an Explainable GEC (XGEC) dataset of correction reasons by annotating
NUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT
using original prompts miss some correction points, the generation control
using PI can explicitly guide to describe explanations for all correction
points, contributing to improved performance in generating correction reasons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11489">
<div class="article-summary-box-inner">
<span><p>Designing reward functions is a longstanding challenge in reinforcement
learning (RL); it requires specialized knowledge or domain data, leading to
high costs for development. To address this, we introduce Text2Reward, a
data-free framework that automates the generation of dense reward functions
based on large language models (LLMs). Given a goal described in natural
language, Text2Reward generates dense reward functions as an executable program
grounded in a compact representation of the environment. Unlike inverse RL and
recent work that uses LLMs to write sparse reward codes, Text2Reward produces
interpretable, free-form dense reward codes that cover a wide range of tasks,
utilize existing packages, and allow iterative refinement with human feedback.
We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,
MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17
manipulation tasks, policies trained with generated reward codes achieve
similar or better task success rates and convergence speed than expert-written
reward codes. For locomotion tasks, our method learns six novel locomotion
behaviors with a success rate exceeding 94%. Furthermore, we show that the
policies trained in the simulator with our method can be deployed in the real
world. Finally, Text2Reward further improves the policies by refining their
reward functions with human feedback. Video results are available at
https://text-to-reward.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11495">
<div class="article-summary-box-inner">
<span><p>Generation of plausible yet incorrect factual information, termed
hallucination, is an unsolved issue in large language models. We study the
ability of language models to deliberate on the responses they give in order to
correct their mistakes. We develop the Chain-of-Verification (CoVe) method
whereby the model first (i) drafts an initial response; then (ii) plans
verification questions to fact-check its draft; (iii) answers those questions
independently so the answers are not biased by other responses; and (iv)
generates its final verified response. In experiments, we show CoVe decreases
hallucinations across a variety of tasks, from list-based questions from
Wikidata, closed book MultiSpanQA and longform text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11499">
<div class="article-summary-box-inner">
<span><p>This paper presents DreamLLM, a learning framework that first achieves
versatile Multimodal Large Language Models (MLLMs) empowered with frequently
overlooked synergy between multimodal comprehension and creation. DreamLLM
operates on two fundamental principles. The first focuses on the generative
modeling of both language and image posteriors by direct sampling in the raw
multimodal space. This approach circumvents the limitations and information
loss inherent to external feature extractors like CLIP, and a more thorough
multimodal understanding is obtained. Second, DreamLLM fosters the generation
of raw, interleaved documents, modeling both text and image contents, along
with unstructured layouts. This allows DreamLLM to learn all conditional,
marginal, and joint multimodal distributions effectively. As a result, DreamLLM
is the first MLLM capable of generating free-form interleaved content.
Comprehensive experiments highlight DreamLLM's superior performance as a
zero-shot multimodal generalist, reaping from the enhanced learning synergy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Procedures as Programs: Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08214">
<div class="article-summary-box-inner">
<span><p>When humans conceive how to perform a particular task, they do so
hierarchically: splitting higher-level tasks into smaller sub-tasks. However,
in the literature on natural language (NL) command of situated agents, most
works have treated the procedures to be executed as flat sequences of simple
actions, or any hierarchies of procedures have been shallow at best. In this
paper, we propose a formalism of procedures as programs, a powerful yet
intuitive method of representing hierarchical procedural knowledge for agent
command and control. We further propose a modeling paradigm of hierarchical
modular networks, which consist of a planner and reactors that convert NL
intents to predictions of executable programs and probe the environment for
information necessary to complete the program execution. We instantiate this
framework on the IQA and ALFRED datasets for NL instruction following. Our
model outperforms reactive baselines by a large margin on both datasets. We
also demonstrate that our framework is more data-efficient, and that it allows
for fast iterative development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attacking Open-domain Question Answering by Injecting Misinformation. (arXiv:2110.07803v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07803">
<div class="article-summary-box-inner">
<span><p>With a rise in false, inaccurate, and misleading information in propaganda,
news, and social media, real-world Question Answering (QA) systems face the
challenges of synthesizing and reasoning over misinformation-polluted contexts
to derive correct answers. This urgency gives rise to the need to make QA
systems robust to misinformation, a topic previously unexplored. We study the
risk of misinformation to QA models by investigating the sensitivity of
open-domain QA models to corpus pollution with misinformation documents. We
curate both human-written and model-generated false documents that we inject
into the evidence corpus of QA models and assess the impact on the performance
of these systems. Experiments show that QA models are vulnerable to even small
amounts of evidence contamination brought by misinformation, with large
absolute performance drops on all models. Misinformation attack brings more
threat when fake documents are produced at scale by neural models or the
attacker targets hacking specific questions of interest. To defend against such
a threat, we discuss the necessity of building a misinformation-aware QA system
that integrates question-answering and misinformation detection in a joint
fashion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13397">
<div class="article-summary-box-inner">
<span><p>Language barriers present a great challenge in our increasingly connected and
global world. Especially within the medical domain, e.g. hospital or emergency
room, communication difficulties and delays may lead to malpractice and
non-optimal patient care. In the HYKIST project, we consider patient-physician
communication, more specifically between a German-speaking physician and an
Arabic- or Vietnamese-speaking patient. Currently, a doctor can call the
Triaphon service to get assistance from an interpreter in order to help
facilitate communication. The HYKIST goal is to support the usually
non-professional bilingual interpreter with an automatic speech translation
system to improve patient care and help overcome language barriers. In this
work, we present our ASR system development efforts for this conversational
telephone speech translation task in the medical domain for two languages
pairs, data collection, various acoustic model architectures and
dialect-induced difficulties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Triplet Extraction by Template Infilling. (arXiv:2212.10708v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10708">
<div class="article-summary-box-inner">
<span><p>The task of triplet extraction aims to extract pairs of entities and their
corresponding relations from unstructured text. Most existing methods train an
extraction model on training data involving specific target relations, and are
incapable of extracting new relations that were not observed at training time.
Generalizing the model to unseen relations typically requires fine-tuning on
synthetic training data which is often noisy and unreliable. We show that by
reducing triplet extraction to a template infilling task over a pre-trained
language model (LM), we can equip the extraction model with zero-shot learning
capabilities and eliminate the need for additional training data. We propose a
novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling),
that aligns the task objective to the pre-training objective of generative
transformers to generalize to unseen relations. Experiments on FewRel and
Wiki-ZSL datasets demonstrate that ZETT shows consistent and stable
performance, outperforming previous state-of-the-art methods, even when using
automatically generated templates. https://github.com/megagonlabs/zett/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing And Improving Neural Speaker Embeddings for ASR. (arXiv:2301.04571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04571">
<div class="article-summary-box-inner">
<span><p>Neural speaker embeddings encode the speaker's speech characteristics through
a DNN model and are prevalent for speaker verification tasks. However, few
studies have investigated the usage of neural speaker embeddings for an ASR
system. In this work, we present our efforts w.r.t integrating neural speaker
embeddings into a conformer based hybrid HMM ASR system. For ASR, our improved
embedding extraction pipeline in combination with the Weighted-Simple-Add
integration method results in x-vector and c-vector reaching on par performance
with i-vectors. We further compare and analyze different speaker embeddings. We
present our acoustic model improvements obtained by switching from newbob
learning rate schedule to one cycle learning schedule resulting in a ~3%
relative WER reduction on Switchboard, additionally reducing the overall
training time by 17%. By further adding neural speaker embeddings, we gain
additional ~3% relative WER improvement on Hub5'00. Our best Conformer-based
hybrid ASR system with speaker embeddings achieves 9.0% WER on Hub5'00 and
Hub5'01 with training on SWB 300h.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family. (arXiv:2303.07992v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07992">
<div class="article-summary-box-inner">
<span><p>ChatGPT is a powerful large language model (LLM) that covers knowledge
resources such as Wikipedia and supports natural language question answering
using its own knowledge. Therefore, there is growing interest in exploring
whether ChatGPT can replace traditional knowledge-based question answering
(KBQA) models. Although there have been some works analyzing the question
answering performance of ChatGPT, there is still a lack of large-scale,
comprehensive testing of various types of complex questions to analyze the
limitations of the model. In this paper, we present a framework that follows
the black-box testing specifications of CheckList proposed by Ribeiro et. al.
We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex
question answering datasets, which include six English datasets and two
multilingual datasets. The total number of test cases is approximately 190,000.
In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5
to identify commonalities between the GPT family and other LLMs. The dataset
and code are available at
https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MasakhaNEWS: News Topic Classification for African languages. (arXiv:2304.09972v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09972">
<div class="article-summary-box-inner">
<span><p>African languages are severely under-represented in NLP research due to lack
of datasets covering several NLP tasks. While there are individual language
specific datasets that are being expanded to different tasks, only a handful of
NLP tasks (e.g. named entity recognition and machine translation) have
standardized benchmark datasets covering several geographical and
typologically-diverse African languages. In this paper, we develop MasakhaNEWS
-- a new benchmark dataset for news topic classification covering 16 languages
widely spoken in Africa. We provide an evaluation of baseline models by
training classical machine learning models and fine-tuning several language
models. Furthermore, we explore several alternatives to full fine-tuning of
language models that are better suited for zero-shot and few-shot learning such
as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern
exploiting training (PET), prompting language models (like ChatGPT), and
prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).
Our evaluation in zero-shot setting shows the potential of prompting ChatGPT
for news topic classification in low-resource African languages, achieving an
average performance of 70 F1 points without leveraging additional supervision
like MAD-X. In few-shot setting, we show that with as little as 10 examples per
label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of
full supervised training (92.6 F1 points) leveraging the PET approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts. (arXiv:2305.12477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12477">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have exhibited remarkable performance on various
Natural Language Processing (NLP) tasks. However, there is a current hot debate
regarding their reasoning capacity. In this paper, we examine the performance
of GPT-3.5, GPT-4, and BARD models, by performing a thorough technical
evaluation on different reasoning tasks across eleven distinct datasets. Our
paper provides empirical evidence showcasing the superior performance of
ChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting
throughout almost all evaluated tasks. While the superiority of GPT-4 compared
to GPT-3.5 might be explained by its larger size and NLP efficiency, this was
not evident for BARD. We also demonstrate that the three models show limited
proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To
bolster our findings, we present a detailed and comprehensive analysis of the
results from these three models. Furthermore, we propose a set of engineered
prompts that enhances the zero-shot setting performance of all three models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Data Augmentation for Document-grounded Dialog Systems in Low Resource Languages. (arXiv:2305.14949v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14949">
<div class="article-summary-box-inner">
<span><p>This paper proposes a framework to address the issue of data scarcity in
Document-Grounded Dialogue Systems(DGDS). Our model leverages high-resource
languages to enhance the capability of dialogue generation in low-resource
languages. Specifically, We present a novel pipeline CLEM (Cross-Lingual
Enhanced Model) including adversarial training retrieval (Retriever and
Re-ranker), and Fid (fusion-in-decoder) generator. To further leverage
high-resource language, we also propose an innovative architecture to conduct
alignment across different languages with translated training. Extensive
experiment results demonstrate the effectiveness of our model and we achieved
4th place in the DialDoc 2023 Competition. Therefore, CLEM can serve as a
solution to resource scarcity in DGDS and provide useful guidance for
multi-lingual alignment tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17089">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been successfully used in many
natural-language tasks and applications including text generation and AI
chatbots. They also are a promising new technology for concept-oriented deep
learning (CODL). However, the prerequisite is that LLMs understand concepts and
ensure conceptual consistency. We discuss these in this paper, as well as major
uses of LLMs for CODL including concept extraction from text, concept graph
extraction from text, and concept learning. Human knowledge consists of both
symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only
LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal
LLMs, on the other hand, are capable of representing the full range (conceptual
and sensory) of human knowledge. We discuss conceptual understanding in
visual-language LLMs, the most important multimodal LLMs, and major uses of
them for CODL including concept extraction from image, concept graph extraction
from image, and concept learning. While uses of LLMs for CODL are valuable
standalone, they are particularly valuable as part of LLM applications such as
AI chatbots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11760">
<div class="article-summary-box-inner">
<span><p>Emotional intelligence significantly impacts our daily behaviors and
interactions. Although Large Language Models (LLMs) are increasingly viewed as
a stride toward artificial general intelligence, exhibiting impressive
performance in numerous tasks, it is still uncertain if LLMs can genuinely
grasp psychological emotional stimuli. Understanding and responding to
emotional cues gives humans a distinct advantage in problem-solving. In this
paper, we take the first step towards exploring the ability of LLMs to
understand emotional stimuli. To this end, we first conduct automatic
experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,
Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative
applications that represent comprehensive evaluation scenarios. Our automatic
experiments show that LLMs have a grasp of emotional intelligence, and their
performance can be improved with emotional prompts (which we call
"EmotionPrompt" that combines the original prompt with emotional stimuli),
e.g., 8.00% relative performance improvement in Instruction Induction and 115%
in BIG-Bench. In addition to those deterministic tasks that can be
automatically evaluated using existing metrics, we conducted a human study with
106 participants to assess the quality of generative tasks using both vanilla
and emotional prompts. Our human study results demonstrate that EmotionPrompt
significantly boosts the performance of generative tasks (10.9% average
improvement in terms of performance, truthfulness, and responsibility metrics).
We provide an in-depth discussion regarding why EmotionPrompt works for LLMs
and the factors that may influence its performance. We posit that EmotionPrompt
heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs
interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction. (arXiv:2308.06552v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06552">
<div class="article-summary-box-inner">
<span><p>Cross-lingual open information extraction aims to extract structured
information from raw text across multiple languages. Previous work uses a
shared cross-lingual pre-trained model to handle the different languages but
underuses the potential of the language-specific representation. In this paper,
we propose an effective multi-stage tuning framework called MT4CrossIE,
designed for enhancing cross-lingual open information extraction by injecting
language-specific knowledge into the shared model. Specifically, the
cross-lingual pre-trained model is first tuned in a shared semantic space
(e.g., embedding matrix) in the fixed encoder and then other components are
optimized in the second stage. After enough training, we freeze the pre-trained
model and tune the multiple extra low-rank language-specific modules using
mixture-of-LoRAs for model-based cross-lingual transfer. In addition, we
leverage two-stage prompting to encourage the large language model (LLM) to
annotate the multi-lingual raw data for data-based cross-lingual transfer. The
model is trained with multi-lingual objectives on our proposed dataset
OpenIE4++ by combing the model-based and data-based transfer techniques.
Experimental results on various benchmarks emphasize the importance of
aggregating multiple plug-in-and-play language-specific modules and demonstrate
the effectiveness of MT4CrossIE in cross-lingual
OIE\footnote{\url{https://github.com/CSJianYang/Multilingual-Multimodal-NLP}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v3 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15363">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL
task. However, the absence of a systematical benchmark inhibits the development
of designing effective, efficient and economic LLM-based Text-to-SQL solutions.
To address this challenge, in this paper, we first conduct a systematical and
extensive comparison over existing prompt engineering methods, including
question representation, example selection and example organization, and with
these experimental results, we elaborate their pros and cons. Based on these
findings, we propose a new integrated solution, named DAIL-SQL, which refreshes
the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To
explore the potential of open-source LLM, we investigate them in various
scenarios, and further enhance their performance with supervised fine-tuning.
Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well
as the advantages and disadvantages of the supervised fine-tuning.
Additionally, towards an efficient and economic LLM-based Text-to-SQL solution,
we emphasize the token efficiency in prompt engineering and compare the prior
studies under this metric. We hope that our work provides a deeper
understanding of Text-to-SQL with LLMs, and inspires further investigations and
broad applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptASR for contextualized ASR with controllable style. (arXiv:2309.07414v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07414">
<div class="article-summary-box-inner">
<span><p>Prompts are crucial to large language models as they provide context
information such as topic or logical relationships. Inspired by this, we
propose PromptASR, a framework that integrates prompts in end-to-end automatic
speech recognition (E2E ASR) systems to achieve contextualized ASR with
controllable style of transcriptions. Specifically, a dedicated text encoder
encodes the text prompts and the encodings are injected into the speech encoder
by cross-attending the features from two modalities. When using the ground
truth text from preceding utterances as content prompt, the proposed system
achieves 21.9% and 6.8% relative word error rate reductions on a book reading
dataset and an in-house dataset compared to a baseline ASR system. The system
can also take word-level biasing lists as prompt to improve recognition
accuracy on rare words. An additional style prompt can be given to the text
encoder and guide the ASR system to output different styles of transcriptions.
The code is available at icefall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08182">
<div class="article-summary-box-inner">
<span><p>Our work demonstrates that large language model (LLM) pre-trained on texts
can not only solve pure math word problems, but also physics word problems,
whose solution requires calculation and inference based on prior physical
knowledge. We collect and annotate the first physics word problem
dataset-PhysQA, which contains over 1000 junior high school physics word
problems (covering Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity).
Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found
that GPT3.5 could automatically solve 49.3% of the problems through zero-shot
learning and 73.2% through few-shot learning. This result demonstrates that by
using similar problems and their answers as prompt, LLM could solve elementary
physics word problems approaching human level performance. In addition to
solving problems, GPT3.5 can also summarize the knowledge or topics covered by
the problems, provide relevant explanations, and generate new physics word
problems based on the input. Our work is the first research to focus on the
automatic solving, explanation, and generation of physics word problems across
various types and scenarios, and we achieve an acceptable and state-of-the-art
accuracy. This underscores the potential of LLMs for further applications in
secondary education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10305">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable performance on a
variety of natural language tasks based on just a few examples of natural
language instructions, reducing the need for extensive feature engineering.
However, most powerful LLMs are closed-source or limited in their capability
for languages other than English. In this technical report, we present Baichuan
2, a series of large-scale multilingual language models containing 7 billion
and 13 billion parameters, trained from scratch, on 2.6 trillion tokens.
Baichuan 2 matches or outperforms other open-source models of similar size on
public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan
2 excels in vertical domains such as medicine and law. We will release all
pre-training model checkpoints to benefit the research community in better
understanding the training dynamics of Baichuan 2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10326">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the success of question answering (QA),
especially its potential to be a foundation paradigm for tackling diverse NLP
tasks. However, obtaining sufficient data to build an effective and stable QA
system still remains an open problem. For this problem, we introduce an
iterative bootstrapping framework for QA data augmentation (named QASnowball),
which can iteratively generate large-scale high-quality QA data based on a seed
set of supervised examples. Specifically, QASnowball consists of three modules,
an answer extractor to extract core phrases in unlabeled documents as candidate
answers, a question generator to generate questions based on documents and
candidate answers, and a QA data filter to filter out high-quality QA data.
Moreover, QASnowball can be self-enhanced by reseeding the seed set to
fine-tune itself in different iterations, leading to continual improvements in
the generation quality. We conduct experiments in the high-resource English
scenario and the medium-resource Chinese scenario, and the experimental results
show that the data generated by QASnowball can facilitate QA models: (1)
training models on the generated data achieves comparable results to using
supervised data, and (2) pre-training on the generated data and fine-tuning on
supervised data can achieve better performance. Our code and generated data
will be released to advance further work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10447">
<div class="article-summary-box-inner">
<span><p>Controllable text generation is a fundamental aspect of natural language
generation, with numerous methods proposed for different constraint types.
However, these approaches often require significant architectural or decoding
modifications, making them challenging to apply to additional constraints or
resolve different constraint combinations. To address this, our paper
introduces Regular Expression Instruction (REI), which utilizes an
instruction-based mechanism to fully exploit regular expressions' advantages to
uniformly model diverse constraints. Specifically, our REI supports all popular
fine-grained controllable generation constraints, i.e., lexical, positional,
and length, as well as their complex combinations, via regular expression-style
instructions. Our method only requires fine-tuning on medium-scale language
models or few-shot, in-context learning on large language models, and requires
no further adjustment when applied to various constraint combinations.
Experiments demonstrate that our straightforward approach yields high success
rates and adaptability to various constraints while maintaining competitiveness
in automatic metrics and outperforming most previous baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages. (arXiv:2309.10661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10661">
<div class="article-summary-box-inner">
<span><p>Democratizing access to natural language processing (NLP) technology is
crucial, especially for underrepresented and extremely low-resource languages.
Previous research has focused on developing labeled and unlabeled corpora for
these languages through online scraping and document translation. While these
methods have proven effective and cost-efficient, we have identified
limitations in the resulting corpora, including a lack of lexical diversity and
cultural relevance to local communities. To address this gap, we conduct a case
study on Indonesian local languages. We compare the effectiveness of online
scraping, human translation, and paragraph writing by native speakers in
constructing datasets. Our findings demonstrate that datasets generated through
paragraph writing by native speakers exhibit superior quality in terms of
lexical diversity and cultural content. In addition, we present the
\datasetname{} benchmark, encompassing 12 underrepresented and extremely
low-resource languages spoken by millions of individuals in Indonesia. Our
empirical experiment results using existing multilingual large language models
conclude the need to extend these models to more underrepresented languages. We
release the NusaWrites dataset at https://github.com/IndoNLP/nusa-writes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation. (arXiv:2309.10738v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10738">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have achieved impressive results in various music
understanding and generation tasks. However, existing pre-training methods for
symbolic melody generation struggle to capture multi-scale, multi-dimensional
structural information in note sequences, due to the domain knowledge
discrepancy between text and music. Moreover, the lack of available large-scale
symbolic melody datasets limits the pre-training improvement. In this paper, we
propose MelodyGLM, a multi-task pre-training framework for generating melodies
with long-term structure. We design the melodic n-gram and long span sampling
strategies to create local and global blank infilling tasks for modeling the
local and global structures in melodies. Specifically, we incorporate pitch
n-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram
blank infilling tasks for modeling the multi-dimensional structures in
melodies. To this end, we have constructed a large-scale symbolic melody
dataset, MelodyNet, containing more than 0.4 million melody pieces. MelodyNet
is utilized for large-scale pre-training and domain-specific n-gram lexicon
construction. Both subjective and objective evaluations demonstrate that
MelodyGLM surpasses the standard and previous pre-training methods. In
particular, subjective evaluations show that, on the melody continuation task,
MelodyGLM gains average improvements of 0.82, 0.87, 0.78, and 0.94 in
consistency, rhythmicity, structure, and overall quality, respectively.
Notably, MelodyGLM nearly matches the quality of human-composed melodies on the
melody inpainting task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10400">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce Positional Skip-wisE (PoSE) training for
efficient adaptation of large language models~(LLMs) to extremely long context
windows. PoSE decouples train length from target context window size by
simulating long inputs using a fixed context window with manipulated position
indices during training. Concretely, we select several short chunks from a long
input sequence, and introduce distinct skipping bias terms to modify the
position indices of each chunk. These bias terms, along with the length of each
chunk, are altered for each training example, allowing the model to adapt to
all positions within the target context window without training on full length
inputs. Experiments show that, compared with fine-tuning on the full length,
PoSE greatly reduces memory and time overhead with minimal impact on
performance. Leveraging this advantage, we have successfully extended the LLaMA
model to 128k tokens. Furthermore, we empirically confirm that PoSE is
compatible with all RoPE-based LLMs and various position interpolation
strategies. Notably, by decoupling fine-tuning length from target context
window, PoSE can theoretically extend the context window infinitely,
constrained only by memory usage for inference. With ongoing advancements for
efficient inference, we believe PoSE holds great promise for scaling the
context window even further.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-21 23:11:18.780742586 UTC">2023-09-21 23:11:18 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>