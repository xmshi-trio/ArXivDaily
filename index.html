<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-21T01:30:00Z">04-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages. (arXiv:2304.09919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09919">
<div class="article-summary-box-inner">
<span><p>Efficiently and accurately translating a corpus into a low-resource language
remains a challenge, regardless of the strategies employed, whether manual,
automated, or a combination of the two. Many Christian organizations are
dedicated to the task of translating the Holy Bible into languages that lack a
modern translation. Bible translation (BT) work is currently underway for over
3000 extremely low resource languages. We introduce the eBible corpus: a
dataset containing 1009 translations of portions of the Bible with data in 833
different languages across 75 language families. In addition to a BT
benchmarking dataset, we introduce model performance benchmarks built on the No
Language Left Behind (NLLB) neural machine translation (NMT) models. Finally,
we describe several problems specific to the domain of BT and consider how the
established data and model benchmarks might be used for future translation
efforts. For a BT task trained with NLLB, Austronesian and Trans-New Guinea
language families achieve 35.1 and 31.6 BLEU scores respectively, which spurs
future innovations for NMT for low-resource languages in Papua New Guinea.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers. (arXiv:2304.09948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09948">
<div class="article-summary-box-inner">
<span><p>The proliferation of fake reviews of doctors has potentially detrimental
consequences for patient well-being and has prompted concern among consumer
protection groups and regulatory bodies. Yet despite significant advancements
in the fields of machine learning and natural language processing, there
remains limited comprehension of the characteristics differentiating fraudulent
from authentic reviews. This study utilizes a novel pre-labeled dataset of
38048 physician reviews to establish the effectiveness of large language models
in classifying reviews. Specifically, we compare the performance of traditional
ML models, such as logistic regression and support vector machines, to
generative pre-trained transformer models. Furthermore, we use GPT4, the newest
model in the GPT family, to uncover the key dimensions along which fake and
genuine physician reviews differ. Our findings reveal significantly superior
performance of GPT-3 over traditional ML models in this context. Additionally,
our analysis suggests that GPT3 requires a smaller training sample than
traditional models, suggesting its appropriateness for tasks with scarce
training data. Moreover, the superiority of GPT3 performance increases in the
cold start context i.e., when there are no prior reviews of a doctor. Finally,
we employ GPT4 to reveal the crucial dimensions that distinguish fake physician
reviews. In sharp contrast to previous findings in the literature that were
obtained using simulated data, our findings from a real-world dataset show that
fake reviews are generally more clinically detailed, more reserved in
sentiment, and have better structure and grammar than authentic ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-resource Bilingual Dialect Lexicon Induction with Large Language Models. (arXiv:2304.09957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09957">
<div class="article-summary-box-inner">
<span><p>Bilingual word lexicons are crucial tools for multilingual natural language
understanding and machine translation tasks, as they facilitate the mapping of
words in one language to their synonyms in another language. To achieve this,
numerous papers have explored bilingual lexicon induction (BLI) in
high-resource scenarios, using a typical pipeline consisting of two
unsupervised steps: bitext mining and word alignment, both of which rely on
pre-trained large language models~(LLMs).
</p>
<p>In this paper, we present an analysis of the BLI pipeline for German and two
of its dialects, Bavarian and Alemannic. This setup poses several unique
challenges, including the scarcity of resources, the relatedness of the
languages, and the lack of standardization in the orthography of dialects. To
evaluate the BLI outputs, we analyze them with respect to word frequency and
pairwise edit distance. Additionally, we release two evaluation datasets
comprising 1,500 bilingual sentence pairs and 1,000 bilingual word pairs. They
were manually judged for their semantic similarity for each Bavarian-German and
Alemannic-German language pair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09960">
<div class="article-summary-box-inner">
<span><p>Languages are not created randomly but rather to communicate information.
There is a strong association between languages and their underlying meanings,
resulting in a sparse joint distribution that is heavily peaked according to
their correlations. Moreover, these peak values happen to match with the
marginal distribution of languages due to the sparsity. With the advent of LLMs
trained on big data and large models, we can now precisely assess the marginal
distribution of languages, providing a convenient means of exploring the sparse
structures in the joint distribution for effective inferences. In this paper,
we categorize languages as either unambiguous or {\epsilon}-ambiguous and
present quantitative results to demonstrate that the emergent abilities of
LLMs, such as language understanding, in-context learning, chain-of-thought
prompting, and effective instruction fine-tuning, can all be attributed to
Bayesian inference on the sparse joint distribution of languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MasakhaNEWS: News Topic Classification for African languages. (arXiv:2304.09972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09972">
<div class="article-summary-box-inner">
<span><p>African languages are severely under-represented in NLP research due to lack
of datasets covering several NLP tasks. While there are individual language
specific datasets that are being expanded to different tasks, only a handful of
NLP tasks (e.g. named entity recognition and machine translation) have
standardized benchmark datasets covering several geographical and
typologically-diverse African languages. In this paper, we develop MasakhaNEWS
-- a new benchmark dataset for news topic classification covering 16 languages
widely spoken in Africa. We provide an evaluation of baseline models by
training classical machine learning models and fine-tuning several language
models. Furthermore, we explore several alternatives to full fine-tuning of
language models that are better suited for zero-shot and few-shot learning such
as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern
exploiting training (PET), prompting language models (like ChatGPT), and
prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).
Our evaluation in zero-shot setting shows the potential of prompting ChatGPT
for news topic classification in low-resource African languages, achieving an
average performance of 70 F1 points without leveraging additional supervision
like MAD-X. In few-shot setting, we show that with as little as 10 examples per
label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of
full supervised training (92.6 F1 points) leveraging the PET approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radar de Parit\'e: An NLP system to measure gender representation in French news stories. (arXiv:2304.09982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09982">
<div class="article-summary-box-inner">
<span><p>We present the Radar de Parit\'e, an automated Natural Language Processing
(NLP) system that measures the proportion of women and men quoted daily in six
Canadian French-language media outlets. We outline the system's architecture
and detail the challenges we overcame to address French-specific issues, in
particular regarding coreference resolution, a new contribution to the NLP
literature on French. We also showcase statistics covering over one year's
worth of data (282,512 news articles). Our results highlight the
underrepresentation of women in news stories, while also illustrating the
application of modern NLP methods to measure gender representation and address
societal issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09991">
<div class="article-summary-box-inner">
<span><p>Large language models are becoming increasingly pervasive and ubiquitous in
society via deployment in sociotechnical systems. Yet these language models, be
it for classification or generation, have been shown to be biased and behave
irresponsibly, causing harm to people at scale. It is crucial to audit these
language models rigorously. Existing auditing tools leverage either or both
humans and AI to find failures. In this work, we draw upon literature in
human-AI collaboration and sensemaking, and conduct interviews with research
experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro
and Lundberg, 2022), which is powered by a generative large language model
(LLM). Through the design process we highlight the importance of sensemaking
and human-AI communication to leverage complementary strengths of humans and
generative models in collaborative auditing. To evaluate the effectiveness of
the augmented tool, AdaTest++, we conduct user studies with participants
auditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment
analysis model. Qualitative analysis shows that AdaTest++ effectively leverages
human strengths such as schematization, hypothesis formation and testing.
Further, with our tool, participants identified a variety of failures modes,
covering 26 different topics over 2 tasks, that have been shown before in
formal audits and also those previously under-reported.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10145">
<div class="article-summary-box-inner">
<span><p>The release of ChatGPT has uncovered a range of possibilities whereby large
language models (LLMs) can substitute human intelligence. In this paper, we
seek to understand whether ChatGPT has the potential to reproduce
human-generated label annotations in social computing tasks. Such an
achievement could significantly reduce the cost and complexity of social
computing research. As such, we use ChatGPT to re-label five seminal datasets
covering stance detection (2x), sentiment analysis, hate speech, and bot
detection. Our results highlight that ChatGPT does have the potential to handle
these data annotation tasks, although a number of challenges remain. ChatGPT
obtains an average precision 0.609. Performance is highest for the sentiment
analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we
show that performance varies substantially across individual labels. We believe
this work can open up new lines of analysis and act as a basis for future
research into the exploitation of ChatGPT for human annotation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Independence of Association Bias and Empirical Fairness in Language Models. (arXiv:2304.10153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10153">
<div class="article-summary-box-inner">
<span><p>The societal impact of pre-trained language models has prompted researchers
to probe them for strong associations between protected attributes and
value-loaded terms, from slur to prestigious job titles. Such work is said to
probe models for bias or fairness-or such probes 'into representational biases'
are said to be 'motivated by fairness'-suggesting an intimate connection
between bias and fairness. We provide conceptual clarity by distinguishing
between association biases (Caliskan et al., 2022) and empirical fairness (Shen
et al., 2022) and show the two can be independent. Our main contribution,
however, is showing why this should not come as a surprise. To this end, we
first provide a thought experiment, showing how association bias and empirical
fairness can be completely orthogonal. Next, we provide empirical evidence that
there is no correlation between bias metrics and fairness metrics across the
most widely used language models. Finally, we survey the sociological and
psychological literature and show how this literature provides ample support
for expecting these metrics to be uncorrelated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages. (arXiv:2304.10158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10158">
<div class="article-summary-box-inner">
<span><p>One of the challenges with finetuning pretrained language models (PLMs) is
that their tokenizer is optimized for the language(s) it was pretrained on, but
brittle when it comes to previously unseen variations in the data. This can for
instance be observed when finetuning PLMs on one language and evaluating them
on data in a closely related language variety with no standardized orthography.
Despite the high linguistic similarity, tokenization no longer corresponds to
meaningful representations of the target data, leading to low performance in,
e.g., part-of-speech tagging.
</p>
<p>In this work, we finetune PLMs on seven languages from three different
families and analyze their zero-shot performance on closely related,
non-standardized varieties. We consider different measures for the divergence
in the tokenization of the source and target data, and the way they can be
adjusted by manipulating the tokenization during the finetuning step. Overall,
we find that the similarity between the percentage of words that get split into
subwords in the source and target data (the split word ratio difference) is the
strongest predictor for model performance on target data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing FOMC Minutes: Accuracy and Constraints of Language Models. (arXiv:2304.10164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10164">
<div class="article-summary-box-inner">
<span><p>This research article analyzes the language used in the official statements
released by the Federal Open Market Committee (FOMC) after its scheduled
meetings to gain insights into the impact of FOMC official statements on
financial markets and economic forecasting. The study reveals that the FOMC is
careful to avoid expressing emotion in their sentences and follows a set of
templates to cover economic situations. The analysis employs advanced language
modeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The
results show that FinBERT outperforms other techniques in predicting negative
sentiment accurately. However, the study also highlights the challenges and
limitations of using current NLP techniques to analyze FOMC texts and suggests
the potential for enhancing language models and exploring alternative
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval. (arXiv:2304.10195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10195">
<div class="article-summary-box-inner">
<span><p>Passage retrieval aims to retrieve relevant passages from large collections
of the open-domain corpus. Contextual Masked Auto-Encoding has been proven
effective in representation bottleneck pre-training of a monolithic
dual-encoder for passage retrieval. Siamese or fully separated dual-encoders
are often adopted as basic retrieval architecture in the pre-training and
fine-tuning stages for encoding queries and passages into their latent
embedding spaces. However, simply sharing or separating the parameters of the
dual-encoder results in an imbalanced discrimination of the embedding spaces.
In this work, we propose to pre-train Contextual Masked Auto-Encoder with
Mixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate
textual-specific experts for individually encoding the distinct properties of
queries and passages. Meanwhile, a shared self-attention layer is still kept
for unified attention modeling. Results on large-scale passage retrieval
benchmarks show steady improvement in retrieval performances. The quantitive
analysis also shows a more balanced discrimination of the latent embedding
spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Paracrawl for Document-level Neural Machine Translation. (arXiv:2304.10216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10216">
<div class="article-summary-box-inner">
<span><p>Document-level neural machine translation (NMT) has outperformed
sentence-level NMT on a number of datasets. However, document-level NMT is
still not widely adopted in real-world translation systems mainly due to the
lack of large-scale general-domain training data for document-level NMT. We
examine the effectiveness of using Paracrawl for learning document-level
translation. Paracrawl is a large-scale parallel corpus crawled from the
Internet and contains data from various domains. The official Paracrawl corpus
was released as parallel sentences (extracted from parallel webpages) and
therefore previous works only used Paracrawl for learning sentence-level
translation. In this work, we extract parallel paragraphs from Paracrawl
parallel webpages using automatic sentence alignments and we use the extracted
parallel paragraphs as parallel documents for training document-level
translation models. We show that document-level NMT models trained with only
parallel paragraphs from Paracrawl can be used to translate real documents from
TED, News and Europarl, outperforming sentence-level NMT models. We also
perform a targeted pronoun evaluation and show that document-level models
trained with Paracrawl data can help context-aware pronoun translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Open Intent Classification with K-center Contrastive Learning and Adjustable Decision Boundary. (arXiv:2304.10220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10220">
<div class="article-summary-box-inner">
<span><p>Open intent classification, which aims to correctly classify the known
intents into their corresponding classes while identifying the new unknown
(open) intents, is an essential but challenging task in dialogue systems. In
this paper, we introduce novel K-center contrastive learning and adjustable
decision boundary learning (CLAB) to improve the effectiveness of open intent
classification. First, we pre-train a feature encoder on the labeled training
instances, which transfers knowledge from known intents to unknown intents.
Specifically, we devise a K-center contrastive learning algorithm to learn
discriminative and balanced intent features, improving the generalization of
the model for recognizing open intents. Second, we devise an adjustable
decision boundary learning method with expanding and shrinking (ADBES) to
determine the suitable decision conditions. Concretely, we learn a decision
boundary for each known intent class, which consists of a decision center and
the radius of the decision boundary. We then expand the radius of the decision
boundary to accommodate more in-class instances if the out-of-class instances
are far from the decision boundary; otherwise, we shrink the radius of the
decision boundary. Extensive experiments on three benchmark datasets clearly
demonstrate the effectiveness of our method for open intent classification. For
reproducibility, we submit the code at: https://github.com/lxk00/CLAP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indian Sign Language Recognition Using Mediapipe Holistic. (arXiv:2304.10256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10256">
<div class="article-summary-box-inner">
<span><p>Deaf individuals confront significant communication obstacles on a daily
basis. Their inability to hear makes it difficult for them to communicate with
those who do not understand sign language. Moreover, it presents difficulties
in educational, occupational, and social contexts. By providing alternative
communication channels, technology can play a crucial role in overcoming these
obstacles. One such technology that can facilitate communication between deaf
and hearing individuals is sign language recognition. We will create a robust
system for sign language recognition in order to convert Indian Sign Language
to text or speech. We will evaluate the proposed system and compare CNN and
LSTM models. Since there are both static and gesture sign languages, a robust
model is required to distinguish between them. In this study, we discovered
that a CNN model captures letters and characters for recognition of static sign
language better than an LSTM model, but it outperforms CNN by monitoring hands,
faces, and pose in gesture sign language phrases and sentences. The creation of
a text-to-sign language paradigm is essential since it will enhance the sign
language-dependent deaf and hard-of-hearing population's communication skills.
Even though the sign-to-text translation is just one side of communication, not
all deaf or hard-of-hearing people are proficient in reading or writing text.
Some may have difficulty comprehending written language due to educational or
literacy issues. Therefore, a text-to-sign language paradigm would allow them
to comprehend text-based information and participate in a variety of social,
educational, and professional settings.
</p>
<p>Keywords: deaf and hard-of-hearing, DHH, Indian sign language, CNN, LSTM,
static and gesture sign languages, text-to-sign language model, MediaPipe
Holistic, sign language recognition, SLR, SLT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is augmentation effective to improve prediction in imbalanced text datasets?. (arXiv:2304.10283v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10283">
<div class="article-summary-box-inner">
<span><p>Imbalanced datasets present a significant challenge for machine learning
models, often leading to biased predictions. To address this issue, data
augmentation techniques are widely used in natural language processing (NLP) to
generate new samples for the minority class. However, in this paper, we
challenge the common assumption that data augmentation is always necessary to
improve predictions on imbalanced datasets. Instead, we argue that adjusting
the classifier cutoffs without data augmentation can produce similar results to
oversampling techniques. Our study provides theoretical and empirical evidence
to support this claim. Our findings contribute to a better understanding of the
strengths and limitations of different approaches to dealing with imbalanced
data, and help researchers and practitioners make informed decisions about
which methods to use for a given task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decouple Non-parametric Knowledge Distillation For End-to-end Speech Translation. (arXiv:2304.10295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10295">
<div class="article-summary-box-inner">
<span><p>Existing techniques often attempt to make knowledge transfer from a powerful
machine translation (MT) to speech translation (ST) model with some elaborate
techniques, which often requires transcription as extra input during training.
However, transcriptions are not always available, and how to improve the ST
model performance without transcription, i.e., data efficiency, has rarely been
studied in the literature. In this paper, we propose Decoupled Non-parametric
Knowledge Distillation (DNKD) from data perspective to improve the data
efficiency. Our method follows the knowledge distillation paradigm. However,
instead of obtaining the teacher distribution from a sophisticated MT model, we
construct it from a non-parametric datastore via k-Nearest-Neighbor (kNN)
retrieval, which removes the dependence on transcription and MT model. Then we
decouple the classic knowledge distillation loss into target and non-target
distillation to enhance the effect of the knowledge among non-target logits,
which is the prominent "dark knowledge". Experiments on MuST-C corpus show
that, the proposed method can achieve consistent improvement over the strong
baseline without requiring any transcription.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speech Translation by Cross-Modal Multi-Grained Contrastive Learning. (arXiv:2304.10309v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10309">
<div class="article-summary-box-inner">
<span><p>The end-to-end speech translation (E2E-ST) model has gradually become a
mainstream paradigm due to its low latency and less error propagation. However,
it is non-trivial to train such a model well due to the task complexity and
data scarcity. The speech-and-text modality differences result in the E2E-ST
model performance usually inferior to the corresponding machine translation
(MT) model. Based on the above observation, existing methods often use
sharingmechanisms to carry out implicit knowledge transfer by imposing various
constraints. However, the final model often performs worse on the MT task than
the MT model trained alone, which means that the knowledge transfer ability of
this method is also limited. To deal with these problems, we propose the FCCL
(Fine- and Coarse- Granularity Contrastive Learning) approach for E2E-ST, which
makes explicit knowledge transfer through cross-modal multi-grained contrastive
learning. A key ingredient of our approach is applying contrastive learning at
both sentence- and frame-level to give the comprehensive guide for extracting
speech representations containing rich semantic information.In addition, we
adopt a simple whitening method to alleviate the representation degeneration in
the MT model, which adversely affects contrast learning. Experiments on the
MuST-C benchmark show that our proposed approach significantly outperforms the
state-of-the-art E2E-ST baselines on all eight language pairs. Further analysis
indicates that FCCL can free up its capacity from learning grammatical
structure information and force more layers to learn semantic information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DropDim: A Regularization Method for Transformer Networks. (arXiv:2304.10321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10321">
<div class="article-summary-box-inner">
<span><p>We introduceDropDim, a structured dropout method designed for regularizing
the self-attention mechanism, which is a key component of the transformer. In
contrast to the general dropout method, which randomly drops neurons, DropDim
drops part of the embedding dimensions. In this way, the semantic information
can be completely discarded. Thus, the excessive coadapting between different
embedding dimensions can be broken, and the self-attention is forced to encode
meaningful featureswith a certain number of embedding dimensions erased.
Experiments on a wide range of tasks executed on the MUST-C English-Germany
dataset show that DropDim can effectively improve model performance, reduce
over-fitting, and show complementary effects with other regularization methods.
When combined with label smoothing, the WER can be reduced from 19.1% to 15.1%
on the ASR task, and the BLEU value can be increased from26.90 to 28.38 on the
MT task. On the ST task, the model can reach a BLEU score of 22.99, an increase
by 1.86 BLEU points compared to the strong baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10327">
<div class="article-summary-box-inner">
<span><p>Scientific understanding is a fundamental goal of science, allowing us to
explain the world. There is currently no good way to measure the scientific
understanding of agents, whether these be humans or Artificial Intelligence
systems. Without a clear benchmark, it is challenging to evaluate and compare
different levels of and approaches to scientific understanding. In this
Roadmap, we propose a framework to create a benchmark for scientific
understanding, utilizing tools from philosophy of science. We adopt a
behavioral notion according to which genuine understanding should be recognized
as an ability to perform certain tasks. We extend this notion by considering a
set of questions that can gauge different levels of scientific understanding,
covering information retrieval, the capability to arrange information to
produce an explanation, and the ability to infer how things would be different
under different circumstances. The Scientific Understanding Benchmark (SUB),
which is formed by a set of these tests, allows for the evaluation and
comparison of different approaches. Benchmarking plays a crucial role in
establishing trust, ensuring quality control, and providing a basis for
performance evaluation. By aligning machine and human scientific understanding
we can improve their utility, ultimately advancing scientific understanding and
helping to discover new insights within machines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interventional Probing in High Dimensions: An NLI Case Study. (arXiv:2304.10346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10346">
<div class="article-summary-box-inner">
<span><p>Probing strategies have been shown to detect the presence of various
linguistic features in large language models; in particular, semantic features
intermediate to the "natural logic" fragment of the Natural Language Inference
task (NLI). In the case of natural logic, the relation between the intermediate
features and the entailment label is explicitly known: as such, this provides a
ripe setting for interventional studies on the NLI models' representations,
allowing for stronger causal conjectures and a deeper critical analysis of
interventional probing methods. In this work, we carry out new and existing
representation-level interventions to investigate the effect of these semantic
features on NLI classification: we perform amnesic probing (which removes
features as directed by learned linear probes) and introduce the mnestic
probing variation (which forgets all dimensions except the probe-selected
ones). Furthermore, we delve into the limitations of these methods and outline
some pitfalls have been obscuring the effectivity of interventional probing
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Learning for Cross-Lingual Relation Extraction. (arXiv:2304.10354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10354">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) is a crucial task in Information Extraction, which
entails predicting relationships between entities within a given sentence.
However, extending pre-trained RE models to other languages is challenging,
particularly in real-world scenarios where Cross-Lingual Relation Extraction
(XRE) is required. Despite recent advancements in Prompt-Learning, which
involves transferring knowledge from Multilingual Pre-trained Language Models
(PLMs) to diverse downstream tasks, there is limited research on the effective
use of multilingual PLMs with prompts to improve XRE. In this paper, we present
a novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-XRE. To
evaluate its effectiveness, we design and implement several prompt templates,
including hard, soft, and hybrid prompts, and empirically test their
performance on competitive multilingual PLMs, specifically mBART. Our extensive
experiments, conducted on the low-resource ACE05 benchmark across multiple
languages, demonstrate that our Prompt-XRE algorithm significantly outperforms
both vanilla multilingual PLMs and other existing models, achieving
state-of-the-art performance in XRE. To further show the generalization of our
Prompt-XRE on larger data scales, we construct and release a new XRE dataset-
WMT17-EnZh XRE, containing 0.9M English-Chinese pairs extracted from WMT 2017
parallel corpus. Experiments on WMT17-EnZh XRE also show the effectiveness of
our Prompt-XRE against other competitive baselines. The code and newly
constructed dataset are freely available at
\url{https://github.com/HSU-CHIA-MING/Prompt-XRE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population. (arXiv:2304.10392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10392">
<div class="article-summary-box-inner">
<span><p>Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task
in NLP, as it tackles knowledge from external sources with unseen events and
entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an
evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that
suffer from a substantial fraction of incorrect answers, and the evaluation set
is not well-aligned with the external knowledge source as a result of random
sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB
Population benchmark, which addresses the two mentioned problems by using
experts instead of crowd-sourced annotation and by adding diversified
adversarial samples to make the evaluation set more representative. We conduct
extensive experiments comparing state-of-the-art methods for CSKB Population on
the new evaluation set for future research comparisons. Empirical results show
that the population task is still challenging, even for large language models
(LLM) such as ChatGPT. Codes and data are available at
https://github.com/HKUST-KnowComp/CSKB-Population.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10428">
<div class="article-summary-box-inner">
<span><p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA
performances on a variety of NLP tasks, its performance on NER is still
significantly below supervised baselines. This is due to the gap between the
two tasks the NER and LLMs: the former is a sequence labeling task in nature
while the latter is a text-generation model.
</p>
<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the
gap by transforming the sequence labeling task to a generation task that can be
easily adapted by LLMs e.g., the task of finding location entities in the input
text "Columbus is a city" is transformed to generate the text sequence
"@@Columbus## is a city", where special tokens @@## marks the entity to
extract. To efficiently address the "hallucination" issue of LLMs, where LLMs
have a strong inclination to over-confidently label NULL inputs as entities, we
propose a self-verification strategy by prompting LLMs to ask itself whether
the extracted entities belong to a labeled entity tag.
</p>
<p>We conduct experiments on five widely adopted NER datasets, and GPT-NER
achieves comparable performances to fully supervised baselines, which is the
first time as far as we are concerned. More importantly, we find that GPT-NER
exhibits a greater ability in the low-resource and few-shot setups, when the
amount of training data is extremely scarce, GPT-NER performs significantly
better than supervised models. This demonstrates the capabilities of GPT-NER in
real-world NER applications where the number of labeled examples is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safety Assessment of Chinese Large Language Models. (arXiv:2304.10436v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10436">
<div class="article-summary-box-inner">
<span><p>With the rapid popularity of large language models such as ChatGPT and GPT-4,
a growing amount of attention is paid to their safety concerns. These models
may generate insulting and discriminatory content, reflect incorrect social
values, and may be used for malicious purposes such as fraud and dissemination
of misleading information. Evaluating and enhancing their safety is
particularly essential for the wide application of large language models
(LLMs). To further promote the safe deployment of LLMs, we develop a Chinese
LLM safety assessment benchmark. Our benchmark explores the comprehensive
safety performance of LLMs from two perspectives: 8 kinds of typical safety
scenarios and 6 types of more challenging instruction attacks. Our benchmark is
based on a straightforward process in which it provides the test prompts and
evaluates the safety of the generated responses from the evaluated model. In
evaluation, we utilize the LLM's strong evaluation ability and develop it as a
safety evaluator by prompting. On top of this benchmark, we conduct safety
assessments and analyze 15 LLMs including the OpenAI GPT series and other
well-known Chinese LLMs, where we observe some interesting findings. For
example, we find that instruction attacks are more likely to expose safety
issues of all LLMs. Moreover, to promote the development and deployment of
safe, responsible, and ethical AI, we publicly release SafetyPrompts including
100k augmented prompts and responses by LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health. (arXiv:2304.10447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10447">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have been used in various natural language
processing applications. In the mental health domain, domain-specific language
models are pretrained and released, which facilitates the early detection of
mental health conditions. Social posts, e.g., on Reddit, are usually long
documents. However, there are no domain-specific pretrained models for
long-sequence modeling in the mental health domain. This paper conducts
domain-specific continued pretraining to capture the long context for mental
health. Specifically, we train and release MentalXLNet and MentalLongformer
based on XLNet and Longformer. We evaluate the mental health classification
performance and the long-range ability of these two domain-specific pretrained
models. Our models are released in HuggingFace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phoenix: Democratizing ChatGPT across Languages. (arXiv:2304.10453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10453">
<div class="article-summary-box-inner">
<span><p>This paper presents our efforts to democratize ChatGPT across language. We
release a large language model "Phoenix", achieving competitive performance
among open-source English and Chinese models while excelling in languages with
limited resources (covering both Latin and non-Latin languages). We believe
this work will be beneficial to make ChatGPT more accessible, especially in
countries where people cannot use ChatGPT due to restrictions from OpenAI or
local goverments. Our data, code, and models are available at
https://github.com/FreedomIntelligence/LLMZoo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Program with Natural Language. (arXiv:2304.10464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10464">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown remarkable performance in various
basic natural language tasks, which raises hopes for achieving Artificial
General Intelligence. To better complete complex tasks, we need LLMs to program
for the task and then follow the program to generate a specific solution for
the test sample. We propose using natural language as a new programming
language to describe task procedures, making them easily understandable to both
humans and LLMs. LLMs are capable of directly generating natural language
programs, but these programs may still contain factual errors or incomplete
steps. Therefore, we further propose the Learning to Program (LP) method to ask
LLMs themselves to learn natural language programs from the training dataset of
complex tasks and then use the learned program to guide inference. Our
experiments on the AMPS (high school math) and Math (competition mathematics
problems) datasets demonstrate the effectiveness of our approach. When testing
ChatGPT on 10 tasks from the AMPS dataset, our LP method's average performance
outperformed the direct zero-shot test performance by 18.3$\%$. We release our
code at \url{https://github.com/microsoft/NaturalLanguageProgram}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A primer on getting neologisms from foreign languages to under-resourced languages. (arXiv:2304.10495v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10495">
<div class="article-summary-box-inner">
<span><p>Mainly due to lack of support, most under-resourced languages have a reduced
lexicon in most realms and domains of increasing importance, then their
speakers need to significantly augment it. Although neologisms should arise
from the languages themselves, external sources are widely accepted. However,
we dispute the "common sense" of using the imposed official languages, which
are highly probably a legacy of colonialism, as the only source, and we propose
to introduce neologisms from any language as long as these neologisms "sound
like" native words of the target languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Can We Detect Substance Use Disorder?": Knowledge and Time Aware Classification on Social Media from Darkweb. (arXiv:2304.10512v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10512">
<div class="article-summary-box-inner">
<span><p>Opioid and substance misuse is rampant in the United States today, with the
phenomenon known as the "opioid crisis". The relationship between substance use
and mental health has been extensively studied, with one possible relationship
being: substance misuse causes poor mental health. However, the lack of
evidence on the relationship has resulted in opioids being largely inaccessible
through legal means. This study analyzes the substance use posts on social
media with opioids being sold through crypto market listings. We use the Drug
Abuse Ontology, state-of-the-art deep learning, and knowledge-aware BERT-based
models to generate sentiment and emotion for the social media posts to
understand users' perceptions on social media by investigating questions such
as: which synthetic opioids people are optimistic, neutral, or negative about?
or what kind of drugs induced fear and sorrow? or what kind of drugs people
love or are thankful about? or which drugs people think negatively about? or
which opioids cause little to no sentimental reaction. We discuss how we
crawled crypto market data and its use in extracting posts for fentanyl,
fentanyl analogs, and other novel synthetic opioids. We also perform topic
analysis associated with the generated sentiments and emotions to understand
which topics correlate with people's responses to various drugs. Additionally,
we analyze time-aware neural models built on these features while considering
historical sentiment and emotional activity of posts related to a drug. The
most effective model performs well (statistically significant) with
(macroF1=82.12, recall =83.58) to identify substance use disorder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10513">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in aspects like faithfulness. Taking
question answering as a representative application, we seek to understand why
ChatGPT falls short in answering questions faithfully. To address this
question, we attempt to analyze the failures of ChatGPT in complex open-domain
question answering and identifies the abilities under the failures.
Specifically, we categorize ChatGPT's failures into four types: comprehension,
factualness, specificity, and inference. We further pinpoint three critical
abilities associated with QA failures: knowledge memorization, knowledge
association, and knowledge reasoning. Additionally, we conduct experiments
centered on these abilities and propose potential approaches to enhance
faithfulness. The results indicate that furnishing the model with fine-grained
external knowledge, hints for knowledge association, and guidance for reasoning
can empower the model to answer questions more faithfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Media Slant is Contagious. (arXiv:2202.07269v3 [econ.GN] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07269">
<div class="article-summary-box-inner">
<span><p>This paper examines the diffusion of media slant, specifically how partisan
content from national cable news affects local newspapers in the U.S.,
2005-2008. We use a text-based measure of cable news slant trained on content
from Fox News Channel (FNC), CNN, and MSNBC to analyze how local newspapers
adopt FNC's slant over CNN/MSNBC's. Our findings show that local news becomes
more similar to FNC content in response to an exogenous increase in local FNC
viewership. This shift is not limited to borrowing from cable news, but rather,
local newspapers' own content changes. Further, cable TV slant polarizes local
news content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07893">
<div class="article-summary-box-inner">
<span><p>We describe a simple and effective method (Spectral Attribute removaL; SAL)
to remove private or guarded information from neural representations. Our
method uses matrix decomposition to project the input representations into
directions with reduced covariance with the guarded information rather than
maximal covariance as factorization methods normally use. We begin with linear
information removal and proceed to generalize our algorithm to the case of
nonlinear information removal using kernels. Our experiments demonstrate that
our algorithm retains better main task performance after removing the guarded
information compared to previous work. In addition, our experiments demonstrate
that we need a relatively small amount of guarded attribute data to remove
information about these attributes, which lowers the exposure to sensitive data
and is more suitable for low-resource scenarios. Code is available at
https://github.com/jasonshaoshun/SAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation. (arXiv:2203.08994v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08994">
<div class="article-summary-box-inner">
<span><p>As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties, the
capabilities of detecting novelties, characterizing them,
accommodating/adapting to them, gathering ground-truth training data and
incrementally learning the unknowns/novelties become critical in making the AI
agent more and more knowledgeable, powerful and self-sustainable over time. The
key challenge here is how to automate the process so that it is carried out
continually on the agent's own initiative and through its own interactions with
humans, other agents and the environment just like human on-the-job learning.
This paper proposes a framework (called SOLA) for this learning paradigm to
promote the research of building autonomous and continual learning enabled AI
agents. To show feasibility, an implemented agent is also described.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model. (arXiv:2210.16663v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16663">
<div class="article-summary-box-inner">
<span><p>This paper presents BERT-CTC, a novel formulation of end-to-end speech
recognition that adapts BERT for connectionist temporal classification (CTC).
Our formulation relaxes the conditional independence assumptions used in
conventional CTC and incorporates linguistic knowledge through the explicit
output dependency obtained by BERT contextual embedding. BERT-CTC attends to
the full contexts of the input and hypothesized output sequences via the
self-attention mechanism. This mechanism encourages a model to learn
inner/inter-dependencies between the audio and token representations while
maintaining CTC's training efficiency. During inference, BERT-CTC combines a
mask-predict algorithm with CTC decoding, which iteratively refines an output
sequence. The experimental results reveal that BERT-CTC improves over
conventional approaches across variations in speaking styles and languages.
Finally, we show that the semantic representations in BERT-CTC are beneficial
towards downstream spoken language understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14454">
<div class="article-summary-box-inner">
<span><p>As an important variant of entity alignment (EA), multi-modal entity
alignment (MMEA) aims to discover identical entities across different knowledge
graphs (KGs) with relevant images attached. We noticed that current MMEA
algorithms all globally adopt the KG-level modality fusion strategies for
multi-modal entity representation but ignore the variation in modality
preferences for individual entities, hurting the robustness to potential noise
involved in modalities (e.g., blurry images and relations). In this paper, we
present MEAformer, a multi-modal entity alignment transformer approach for meta
modality hybrid, which dynamically predicts the mutual correlation coefficients
among modalities for entity-level feature aggregation. A modal-aware hard
entity replay strategy is further proposed for addressing vague entity details.
Experimental results show that our model not only achieves SOTA performance on
multiple training scenarios including supervised, unsupervised, iterative, and
low resource, but also has a comparable number of parameters, optimistic speed,
and good interpretability. Our code and data are available at
https://github.com/zjukg/MEAformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v5 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07267">
<div class="article-summary-box-inner">
<span><p>We re-replicate 14 psychology studies from the Many Labs 2 replication
project (Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquially
known as GPT3.5. Among the eight studies we could analyse, our GPT sample
replicated 37.5% of the original results and 37.5% of the Many Labs 2 results.
We could not analyse the remaining six studies, due to an unexpected phenomenon
we call the "correct answer" effect. Different runs of GPT3.5 answered nuanced
questions probing political orientation, economic preference, judgement, and
moral philosophy with zero or near-zero variation in responses: with the
supposedly "correct answer." Most but not all of these "correct answers" were
robust to changing the order of answer choices. One exception occurred in the
Moral Foundations Theory survey (Graham et al., 2009), for which GPT3.5 almost
always identified as a conservative in the original condition (N=1,030, 99.6%)
and as a liberal in the reverse-order condition (N=1,030, 99.3%). GPT3.5's
responses to subsequent questions revealed post-hoc rationalisation; there was
a relative bias in the direction of its previously reported political
orientation. But both self-reported GPT conservatives and self-reported GPT
liberals revealed right-leaning Moral Foundations, although the right-leaning
bias of self-reported GPT liberals was weaker. We hypothesise that this pattern
was learned from a conservative bias in the model's largely Internet-based
training data. Since AI models of the future may be trained on much of the same
Internet data as GPT3.5, our results raise concerns that a hypothetical AI-led
future may be subject to a diminished diversity of thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08624">
<div class="article-summary-box-inner">
<span><p>In this paper, we present InstructABSA, Aspect Based Sentiment Analysis
(ABSA) using the instruction learning paradigm for the ABSA subtasks: Aspect
Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint
Task modeling. Our method introduces positive, negative, and neutral examples
to each training sample, and instruction tunes the model (Tk-Instruct) the ABSA
subtasks, yielding significant performance improvements. Experimental results
on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA
outperforms the previous state-of-the-art (SOTA) approaches on the three ABSA
subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x
larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE
subtask by 5.69% points, Rest15 ATSC subtask by 9.59% points, and on the Lapt14
Joint Task by 3.37% points. Our results also suggest a strong generalization
ability to new domains across all three subtasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10866">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have relied heavily on the use of large
Transformers due to their ability to learn at scale. However, the core building
block of Transformers, the attention operator, exhibits quadratic cost in
sequence length, limiting the amount of context accessible. Existing
subquadratic methods based on low-rank and sparse approximations need to be
combined with dense attention layers to match Transformers, indicating a gap in
capability. In this work, we propose Hyena, a subquadratic drop-in replacement
for attention constructed by interleaving implicitly parametrized long
convolutions and data-controlled gating. In recall and reasoning tasks on
sequences of thousands to hundreds of thousands of tokens, Hyena improves
accuracy by more than 50 points over operators relying on state-spaces and
other implicit and explicit methods, matching attention-based models. We set a
new state-of-the-art for dense-attention-free architectures on language
modeling in standard datasets (WikiText103 and The Pile), reaching Transformer
quality with a 20% reduction in training compute required at sequence length
2K. Hyena operators are twice as fast as highly optimized attention at sequence
length 8K, and 100x faster at sequence length 64K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02426">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable
abilities on a wide range of natural language processing (NLP) tasks, including
various machine translation abilities accomplished during chat. However, these
models are only accessible through restricted APIs, which creates barriers to
new research and advancements in the field. Therefore, we propose the
$\mathbf{ParroT}$ framework to enhance and regulate the translation abilities
during chat based on open-sourced LLMs (i.e., LLaMA-7b, BLOOMZ-7b-mt) and human
written translation and evaluation data. Specifically, ParroT reformulates
translation data into the instruction-following style, and introduces a
"$\mathbf{Hint}$" field for incorporating extra requirements to regulate the
translation process. Accordingly, we propose three instruction types for
finetuning ParroT models, including translation instruction, contrastive
instruction, and error-guided instruction. We can finetune either the full
models or partial parameters via low rank adaptation (LoRA). Experiments on
Flores subsets and WMT22 test sets suggest that translation instruction
improves the translation performance of vanilla LLMs significantly while
error-guided instruction can lead to a further improvement, which demonstrates
the importance of learning from low-quality translations annotated by human.
Meanwhile, the ParroT models can also preserve the ability on general tasks
with the Alpaca multi-task dataset involved in finetuning. Please refer to our
Github project for more implementation details:
https://github.com/wxjiao/ParroT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03439">
<div class="article-summary-box-inner">
<span><p>Harnessing logical reasoning ability is a comprehensive natural language
understanding endeavor. With the release of Generative Pretrained Transformer 4
(GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn
the GPT-4 performance on various logical reasoning tasks. This report analyses
multiple logical reasoning datasets, with popular benchmarks like LogiQA and
ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice
reading comprehension and natural language inference tasks with benchmarks
requiring logical reasoning. We further construct a logical reasoning
out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.
We also make a performance comparison between ChatGPT and GPT-4. Experiment
results show that ChatGPT performs significantly better than the RoBERTa
fine-tuning method on most logical reasoning benchmarks. With early access to
the GPT-4 API we are able to conduct intense experiments on the GPT-4 model.
The results show GPT-4 yields even higher performance on most logical reasoning
datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known
datasets like LogiQA and ReClor. However, the performance drops significantly
when handling newly released and out-of-distribution datasets. Logical
reasoning remains challenging for ChatGPT and GPT-4, especially on
out-of-distribution and natural language inference datasets. We release the
prompt-style logical reasoning datasets as a benchmark suite and name it
LogiEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04187">
<div class="article-summary-box-inner">
<span><p>The standard paradigm for fake news detection mainly utilizes text
information to model the truthfulness of news. However, the discourse of online
fake news is typically subtle and it requires expert knowledge to use textual
information to debunk fake news. Recently, studies focusing on multimodal fake
news detection have outperformed text-only methods. Recent approaches utilizing
the pre-trained model to extract unimodal features, or fine-tuning the
pre-trained model directly, have become a new paradigm for detecting fake news.
Again, this paradigm either requires a large number of training instances, or
updates the entire set of pre-trained model parameters, making real-world fake
news detection impractical. Furthermore, traditional multimodal methods fuse
the cross-modal features directly without considering that the uncorrelated
semantic representation might inject noise into the multimodal features. This
paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)
framework. First, we incorporate prompt learning into multimodal fake news
detection. Prompt learning, which only tunes prompts with a frozen language
model, can reduce memory usage significantly and achieve comparable
performances, compared with fine-tuning. We analyse three prompt templates with
a soft verbalizer to detect fake news. In addition, we introduce the
similarity-aware fusing method to adaptively fuse the intensity of multimodal
representation and mitigate the noise injection via uncorrelated cross-modal
features. For evaluation, SAMPLE surpasses the F1 and the accuracies of
previous works on two benchmark multimodal datasets, demonstrating the
effectiveness of the proposed method in detecting fake news. In addition,
SAMPLE also is superior to other approaches regardless of few-shot and
data-rich settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06377">
<div class="article-summary-box-inner">
<span><p>Being able to create meaningful symbols and proficiently use them for higher
cognitive functions such as communication, reasoning, planning, etc., is
essential and unique for human intelligence. Current deep neural networks are
still far behind human's ability to create symbols for such higher cognitive
functions. Here we propose a solution, named SEA-net, to endow neural networks
with ability of symbol creation, semantic understanding and communication.
SEA-net generates symbols that dynamically configure the network to perform
specific tasks. These symbols capture compositional semantic information that
enables the system to acquire new functions purely by symbolic manipulation or
communication. In addition, we found that these self-generated symbols exhibit
an intrinsic structure resembling that of natural language, suggesting a common
framework underlying the generation and understanding of symbols in both human
brains and artificial neural networks. We hope that it will be instrumental in
producing more capable systems in the future that can synergize the strengths
of connectionist and symbolic approaches for AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-code LLM: Visual Programming over LLMs. (arXiv:2304.08103v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08103">
<div class="article-summary-box-inner">
<span><p>Effectively utilizing LLMs for complex tasks is challenging, often involving
a time-consuming and uncontrollable prompt engineering process. This paper
introduces a novel human-LLM interaction framework, Low-code LLM. It
incorporates six types of simple low-code visual programming interactions, all
supported by clicking, dragging, or text editing, to achieve more controllable
and stable responses. Through visual interaction with a graphical user
interface, users can incorporate their ideas into the workflow without writing
trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM
that designs a structured planning workflow for complex tasks, which can be
correspondingly edited and confirmed by users through low-code visual
programming operations, and an Executing LLM that generates responses following
the user-confirmed workflow. We highlight three advantages of the low-code LLM:
controllable generation results, user-friendly human-LLM interaction, and
broadly applicable scenarios. We demonstrate its benefits using four typical
applications. By introducing this approach, we aim to bridge the gap between
humans and LLMs, enabling more effective and efficient utilization of LLMs for
complex tasks. Our system will be soon publicly available at LowCodeLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. (arXiv:2304.09433v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09433">
<div class="article-summary-box-inner">
<span><p>A long standing goal of the data management community is to develop general,
automated systems that ingest semi-structured documents and output queryable
tables without human effort or domain specific customization. Given the sheer
variety of potential documents, state-of-the art systems make simplifying
assumptions and use domain specific training. In this work, we ask whether we
can maintain generality by using large language models (LLMs). LLMs, which are
pretrained on broad data, can perform diverse downstream tasks simply
conditioned on natural language task descriptions.
</p>
<p>We propose and evaluate EVAPORATE, a simple, prototype system powered by
LLMs. We identify two fundamentally different strategies for implementing this
system: prompt the LLM to directly extract values from documents or prompt the
LLM to synthesize code that performs the extraction. Our evaluations show a
cost-quality tradeoff between these two approaches. Code synthesis is cheap,
but far less accurate than directly processing each document with the LLM. To
improve quality while maintaining low cost, we propose an extended code
synthesis implementation, EVAPORATE-CODE+, which achieves better quality than
direct extraction. Our key insight is to generate many candidate functions and
ensemble their extractions using weak supervision. EVAPORATE-CODE+ not only
outperforms the state-of-the art systems, but does so using a sublinear pass
over the documents with the LLM. This equates to a 110x reduction in the number
of tokens the LLM needs to process, averaged across 16 real-world evaluation
settings of 10k documents each.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Natural Language Processing and Psycholinguistics: computationally grounded semantic similarity datasets for Basque and Spanish. (arXiv:2304.09616v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09616">
<div class="article-summary-box-inner">
<span><p>We present a computationally-grounded word similarity dataset based on two
well-known Natural Language Processing resources; text corpora and knowledge
bases. This dataset aims to fulfil a gap in psycholinguistic research by
providing a variety of quantifications of semantic similarity in an extensive
set of noun pairs controlled by variables that play a significant role in
lexical processing. The dataset creation has consisted in three steps, 1)
computing four key psycholinguistic features for each noun; concreteness,
frequency, semantic and phonological neighbourhood density; 2) pairing nouns
across these four variables; 3) for each noun pair, assigning three types of
word similarity measurements, computed out of text, Wordnet and hybrid
embeddings. The present dataset includes noun pairs' information in Basque and
European Spanish, but further work intends to extend it to more languages.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-22 23:11:05.831192632 UTC">2023-04-22 23:11:05 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>