<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-16T01:30:00Z">01-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism. (arXiv:2301.05272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05272">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been making big waves in the machine
learning community within the past few years. The impressive scalability of
LLMs due to the advent of deep learning can be seen as a continuation of
empiricist lingusitic methods, as opposed to rule-based linguistic methods that
are grounded in a nativist perspective. Current LLMs are generally inaccessible
to resource-constrained researchers, due to a variety of factors including
closed source code. This work argues that this lack of accessibility could
instill a nativist bias in researchers new to computational linguistics, given
that new researchers may only have rule-based, nativist approaches to study to
produce new work. Also, given that there are numerous critics of deep learning
claiming that LLMs and related methods may soon lose their relevancy, we
speculate that such an event could trigger a new wave of nativism in the
language processing community. To prevent such a dramatic shift and placing
favor in hybrid methods of rules and deep learning, we call upon researchers to
open source their LLM code wherever possible to allow both empircist and hybrid
approaches to remain accessible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Judgement: Agent-Based Supreme Court Modelling With GPT. (arXiv:2301.05327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05327">
<div class="article-summary-box-inner">
<span><p>We present a novel Transformer-based multi-agent system for simulating the
judicial rulings of the 2010-2016 Supreme Court of the United States. We train
nine separate models with the respective authored opinions of each supreme
justice active ca. 2015 and test the resulting system on 96 real-world cases.
We find our system predicts the decisions of the real-world Supreme Court with
better-than-random accuracy. We further find a correlation between model
accuracy with respect to individual justices and their alignment between legal
conservatism &amp; liberalism. Our methods and results hold significance for
researchers interested in using language models to simulate politically-charged
discourse between multiple agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Neural Machine Translation with Translation Memories. (arXiv:2301.05380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05380">
<div class="article-summary-box-inner">
<span><p>Improving machine translation (MT) systems with translation memories (TMs) is
of great interest to practitioners in the MT community. However, previous
approaches require either a significant update of the model architecture and/or
additional training efforts to make the models well-behaved when TMs are taken
as additional input. In this paper, we present a simple but effective method to
introduce TMs into neural machine translation (NMT) systems. Specifically, we
treat TMs as prompts to the NMT model at test time, but leave the training
process unchanged. The result is a slight update of an existing NMT system,
which can be implemented in a few hours by anyone who is familiar with NMT.
Experimental results on several datasets demonstrate that our system
significantly outperforms strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaNLP@SMM4H22: BERT for Classification of Twitter Posts. (arXiv:2301.05395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05395">
<div class="article-summary-box-inner">
<span><p>The reported work is our straightforward approach for the shared task
Classification of tweets self-reporting age organized by the Social Media
Mining for Health Applications (SMM4H) workshop. This literature describes the
approach that was used to build a binary classification system, that classifies
the tweets related to birthday posts into two classes namely, exact
age(positive class) and non-exact age(negative class). We made two submissions
with variations in the preprocessing of text which yielded F1 scores of 0.80
and 0.81 when evaluated by the organizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In BLOOM: Creativity and Affinity in Artificial Lyrics and Art. (arXiv:2301.05402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05402">
<div class="article-summary-box-inner">
<span><p>We apply a large multilingual language model (BLOOM-176B) in open-ended
generation of Chinese song lyrics, and evaluate the resulting lyrics for
coherence and creativity using human reviewers. We find that current
computational metrics for evaluating large language model outputs (MAUVE) have
limitations in evaluation of creative writing. We note that the human concept
of creativity requires lyrics to be both comprehensible and distinctive -- and
that humans assess certain types of machine-generated lyrics to score more
highly than real lyrics by popular artists. Inspired by the inherently
multimodal nature of album releases, we leverage a Chinese-language stable
diffusion model to produce high-quality lyric-guided album art, demonstrating a
creative approach for an artist seeking inspiration for an album or single.
Finally, we introduce the MojimLyrics dataset, a Chinese-language dataset of
popular song lyrics for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's Just a Matter of Time: Detecting Depression with Time-Enriched Multimodal Transformers. (arXiv:2301.05453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05453">
<div class="article-summary-box-inner">
<span><p>Depression detection from user-generated content on the internet has been a
long-lasting topic of interest in the research community, providing valuable
screening tools for psychologists. The ubiquitous use of social media platforms
lays out the perfect avenue for exploring mental health manifestations in posts
and interactions with other users. Current methods for depression detection
from social media mainly focus on text processing, and only a few also utilize
images posted by users. In this work, we propose a flexible time-enriched
multimodal transformer architecture for detecting depression from social media
posts, using pretrained models for extracting image and text embeddings. Our
model operates directly at the user-level, and we enrich it with the relative
time between posts by using time2vec positional embeddings. Moreover, we
propose another model variant, which can operate on randomly sampled and
unordered sets of posts to be more robust to dataset noise. We show that our
method, using EmoBERTa and CLIP embeddings, surpasses other methods on two
multimodal datasets, obtaining state-of-the-art results of 0.931 F1 score on a
popular multimodal Twitter dataset, and 0.902 F1 score on the only multimodal
Reddit dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structuring ontologies in a context of collaborative system modelling. (arXiv:2301.05478v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05478">
<div class="article-summary-box-inner">
<span><p>Prospective studies require discussing and collaborating with the
stakeholders to create scenarios of the possible evolution of the studied
value-chain. However, stakeholders don't always use the same words when
referring to one idea. Constructing an ontology and homogenizing vocabularies
is thus crucial to identify key variables which serve in the construction of
the needed scenarios. Nevertheless, it is a very complex and timeconsuming
task. In this paper we present the method we used to manually build ontologies
adapted to the needs of two complementary system-analysis models (namely the
"Godet" and the "MyChoice" models), starting from interviews of the agri-food
system's stakeholders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization of Adapter-Based Cross-lingual Transfer with Scheduled Unfreezing. (arXiv:2301.05487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05487">
<div class="article-summary-box-inner">
<span><p>Standard fine-tuning of language models typically performs well on
in-distribution data, but suffers with generalization to distribution shifts.
In this work, we aim to improve generalization of adapter-based cross-lingual
task transfer where such cross-language distribution shifts are imminent. We
investigate scheduled unfreezing algorithms -- originally proposed to mitigate
catastrophic forgetting in transfer learning -- for fine-tuning task adapters
in cross-lingual transfer. Our experiments show that scheduled unfreezing
methods close the gap to full fine-tuning and achieve state-of-the-art transfer
performance, suggesting that these methods can go beyond just mitigating
catastrophic forgetting. Next, aiming to delve deeper into those empirical
findings, we investigate the learning dynamics of scheduled unfreezing using
Fisher Information. Our in-depth experiments reveal that scheduled unfreezing
induces different learning dynamics compared to standard fine-tuning, and
provide evidence that the dynamics of Fisher Information during training
correlate with cross-lingual generalization performance. We additionally
propose a general scheduled unfreezing algorithm that achieves an average of 2
points improvement over four datasets compared to standard fine-tuning and
provides strong empirical evidence for a theory-based justification of the
heuristic unfreezing schedule (i.e., the heuristic schedule is implicitly
maximizing Fisher Information). Our code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Detection of Check-Worthy Claims using World Languages and Adapter Fusion. (arXiv:2301.05494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05494">
<div class="article-summary-box-inner">
<span><p>Check-worthiness detection is the task of identifying claims, worthy to be
investigated by fact-checkers. Resource scarcity for non-world languages and
model learning costs remain major challenges for the creation of models
supporting multilingual check-worthiness detection. This paper proposes
cross-training adapters on a subset of world languages, combined by adapter
fusion, to detect claims emerging globally in multiple languages. (1) With a
vast number of annotators available for world languages and the
storage-efficient adapter models, this approach is more cost efficient. Models
can be updated more frequently and thus stay up-to-date. (2) Adapter fusion
provides insights and allows for interpretation regarding the influence of each
adapter model on a particular language. The proposed solution often
outperformed the top multilingual approaches in our benchmark tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Cross-cultural News Events. (arXiv:2301.05543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05543">
<div class="article-summary-box-inner">
<span><p>We present a methodology to support the analysis of culture from text such as
news events and demonstrate its usefulness on categorizing news events from
different categories (society, business, health, recreation, science, shopping,
sports, arts, computers, games and home) across different geographical
locations (different places in 117 countries). We group countries based on the
culture that they follow and then filter the news events based on their content
category. The news events are automatically labelled with the help of Hofstedes
cultural dimensions. We present combinations of events across different
categories and check the performances of different classification methods. We
also presents experimental comparison of different number of features in order
to find a suitable set to represent the culture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Alzheimer's Dementia Recognition through Spontaneous Speech: a Signal Processing Grand Challenge. (arXiv:2301.05562v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05562">
<div class="article-summary-box-inner">
<span><p>This Signal Processing Grand Challenge (SPGC) targets a difficult automatic
prediction problem of societal and medical relevance, namely, the detection of
Alzheimer's Dementia (AD). Participants were invited to employ signal
processing and machine learning methods to create predictive models based on
spontaneous speech data. The Challenge has been designed to assess the extent
to which predictive models built based on speech in one language (English)
generalise to another language (Greek). To the best of our knowledge no work
has investigated acoustic features of the speech signal in multilingual AD
detection. Our baseline system used conventional machine learning algorithms
with Active Data Representation of acoustic features, achieving accuracy of
73.91% on AD detection, and 4.95 root mean squared error on cognitive score
prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2022 n2c2/UW Shared Task on Extracting Social Determinants of Health. (arXiv:2301.05571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05571">
<div class="article-summary-box-inner">
<span><p>Objective: The n2c2/UW SDOH Challenge explores the extraction of social
determinant of health (SDOH) information from clinical notes. The objectives
include the advancement of natural language processing (NLP) information
extraction techniques for SDOH and clinical information more broadly. This
paper presents the shared task, data, participating teams, performance results,
and considerations for future work.
</p>
<p>Materials and Methods: The task used the Social History Annotated Corpus
(SHAC), which consists of clinical text with detailed event-based annotations
for SDOH events such as alcohol, drug, tobacco, employment, and living
situation. Each SDOH event is characterized through attributes related to
status, extent, and temporality. The task includes three subtasks related to
information extraction (Subtask A), generalizability (Subtask B), and learning
transfer (Subtask C). In addressing this task, participants utilized a range of
techniques, including rules, knowledge bases, n-grams, word embeddings, and
pretrained language models (LM).
</p>
<p>Results: A total of 15 teams participated, and the top teams utilized
pretrained deep learning LM. The top team across all subtasks used a
sequence-to-sequence approach achieving 0.901 F1 for Subtask A, 0.774 F1
Subtask B, and 0.889 F1 for Subtask C.
</p>
<p>Conclusions: Similar to many NLP tasks and domains, pretrained LM yielded the
best performance, including generalizability and learning transfer. An error
analysis indicates extraction performance varies by SDOH, with lower
performance achieved for conditions, like substance use and homelessness, that
increase health risks (risk factors) and higher performance achieved for
conditions, like substance abstinence and living with family, that reduce
health risks (protective factors).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From stage to page: language independent bootstrap measures of distinctiveness in fictional speech. (arXiv:2301.05659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05659">
<div class="article-summary-box-inner">
<span><p>Stylometry is mostly applied to authorial style. Recently, researchers have
begun investigating the style of characters, finding that the variation remains
within authorial bounds. We address the stylistic distinctiveness of characters
in drama. Our primary contribution is methodological; we introduce and evaluate
two non-parametric methods to produce a summary statistic for character
distinctiveness that can be usefully applied and compared across languages and
times. Our first method is based on bootstrap distances between 3-gram
probability distributions, the second (reminiscent of 'unmasking' techniques)
on word keyness curves. Both methods are validated and explored by applying
them to a reasonably large corpus (a subset of DraCor): we analyse 3301
characters drawn from 2324 works, covering five centuries and four languages
(French, German, Russian, and the works of Shakespeare). Both methods appear
useful; the 3-gram method is statistically more powerful but the word keyness
method offers rich interpretability. Both methods are able to capture
phonological differences such as accent or dialect, as well as broad
differences in topic and lexical richness. Based on exploratory analysis, we
find that smaller characters tend to be more distinctive, and that women are
cross-linguistically more distinctive than men, with this latter finding
carefully interrogated using multiple regression. This greater distinctiveness
stems from a historical tendency for female characters to be restricted to an
'internal narrative domain' covering mainly direct discourse and
family/romantic themes. It is hoped that direct, comparable statistical
measures will form a basis for more sophisticated future studies, and advances
in theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing of Aviation Occurrence Reports for Safety Management. (arXiv:2301.05663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05663">
<div class="article-summary-box-inner">
<span><p>Occurrence reporting is a commonly used method in safety management systems
to obtain insight in the prevalence of hazards and accident scenarios. In
support of safety data analysis, reports are often categorized according to a
taxonomy. However, the processing of the reports can require significant effort
from safety analysts and a common problem is interrater variability in labeling
processes. Also, in some cases, reports are not processed according to a
taxonomy, or the taxonomy does not fully cover the contents of the documents.
This paper explores various Natural Language Processing (NLP) methods to
support the analysis of aviation safety occurrence reports. In particular, the
problems studied are the automatic labeling of reports using a classification
model, extracting the latent topics in a collection of texts using a topic
model and the automatic generation of probable cause texts. Experimental
results showed that (i) under the right conditions the labeling of occurrence
reports can be effectively automated with a transformer-based classifier, (ii)
topic modeling can be useful for finding the topics present in a collection of
reports, and (iii) using a summarization model can be a promising direction for
generating probable cause texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TI-CNN: Convolutional Neural Networks for Fake News Detection. (arXiv:1806.00749v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1806.00749">
<div class="article-summary-box-inner">
<span><p>With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locating and Editing Factual Associations in GPT. (arXiv:2202.05262v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05262">
<div class="article-summary-box-inner">
<span><p>We analyze the storage and recall of factual associations in autoregressive
transformer language models, finding evidence that these associations
correspond to localized, directly-editable computations. We first develop a
causal intervention for identifying neuron activations that are decisive in a
model's factual predictions. This reveals a distinct set of steps in
middle-layer feed-forward modules that mediate factual predictions while
processing subject tokens. To test our hypothesis that these computations
correspond to factual association recall, we modify feed-forward weights to
update specific factual associations using Rank-One Model Editing (ROME). We
find that ROME is effective on a standard zero-shot relation extraction (zsRE)
model-editing task, comparable to existing methods. To perform a more sensitive
evaluation, we also evaluate ROME on a new dataset of counterfactual
assertions, on which it simultaneously maintains both specificity and
generalization, whereas other methods sacrifice one or another. Our results
confirm an important role for mid-layer feed-forward modules in storing factual
associations and suggest that direct manipulation of computational mechanisms
may be a feasible approach for model editing. The code, dataset,
visualizations, and an interactive demo notebook are available at
https://rome.baulab.info/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Prompt Learning for Explainable Recommendation. (arXiv:2202.07371v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07371">
<div class="article-summary-box-inner">
<span><p>Providing user-understandable explanations to justify recommendations could
help users better understand the recommended items, increase the system's ease
of use, and gain users' trust. A typical approach to realize it is natural
language generation. However, previous works mostly adopt recurrent neural
networks to meet the ends, leaving the potentially more effective pre-trained
Transformer models under-explored. In fact, user and item IDs, as important
identifiers in recommender systems, are inherently in different semantic space
as words that pre-trained models were already trained on. Thus, how to
effectively fuse IDs into such models becomes a critical issue. Inspired by
recent advancement in prompt learning, we come up with two solutions: find
alternative words to represent IDs (called discrete prompt learning), and
directly input ID vectors to a pre-trained model (termed continuous prompt
learning). In the latter case, ID vectors are randomly initialized but the
model is trained in advance on large corpora, so they are actually in different
learning stages. To bridge the gap, we further propose two training strategies:
sequential tuning and recommendation as regularization. Extensive experiments
show that our continuous prompt learning approach equipped with the training
strategies consistently outperforms strong baselines on three datasets of
explainable recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory Efficient Continual Learning with Transformers. (arXiv:2203.04640v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04640">
<div class="article-summary-box-inner">
<span><p>In many real-world scenarios, data to train machine learning models becomes
available over time. Unfortunately, these models struggle to continually learn
new concepts without forgetting what has been learnt in the past. This
phenomenon is known as catastrophic forgetting and it is difficult to prevent
due to practical constraints. For instance, the amount of data that can be
stored or the computational resources that can be used might be limited.
Moreover, applications increasingly rely on large pre-trained neural networks,
such as pre-trained Transformers, since the resources or data might not be
available in sufficiently large quantities to practitioners to train the model
from scratch. In this paper, we devise a method to incrementally train a model
on a sequence of tasks using pre-trained Transformers and extending them with
Adapters. Different than the existing approaches, our method is able to scale
to a large number of tasks without significant overhead and allows sharing
information across tasks. On both image and text classification tasks, we
empirically demonstrate that our method maintains a good predictive performance
without retraining the model or increasing the number of model parameters over
time. The resulting model is also significantly faster at inference time
compared to Adapter-based state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics. (arXiv:2205.06253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06253">
<div class="article-summary-box-inner">
<span><p>While there have been significant gains in the field of automated video
description, the generalization performance of automated description models to
novel domains remains a major barrier to using these systems in the real world.
Most visual description methods are known to capture and exploit patterns in
the training data leading to evaluation metric increases, but what are those
patterns? In this work, we examine several popular visual description datasets,
and capture, analyze, and understand the dataset-specific linguistic patterns
that models exploit but do not generalize to new domains. At the token level,
sample level, and dataset level, we find that caption diversity is a major
driving factor behind the generation of generic and uninformative captions. We
further show that state-of-the-art models even outperform held-out ground truth
captions on modern metrics, and that this effect is an artifact of linguistic
diversity in datasets. Understanding this linguistic diversity is key to
building strong captioning models, we recommend several methods and approaches
for maintaining diversity in the collection of new data, and dealing with the
consequences of limited diversity when using current models and metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Near-Term Advances in Quantum Natural Language Processing. (arXiv:2206.02171v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02171">
<div class="article-summary-box-inner">
<span><p>This paper describes experiments showing that some tasks in natural language
processing (NLP) can already be performed using quantum computers, though so
far only with small datasets.
</p>
<p>We demonstrate various approaches to topic classification. The first uses an
explicit word-based approach, in which word-topic scoring weights are
implemented as fractional rotations of individual qubit, and a new phrase is
classified based on the accumulation of these weights in a scoring qubit using
entangling controlled-NOT gates. This is compared with more scalable quantum
encodings of word embedding vectors, which are used in the computation of
kernel values in a quantum support vector machine: this approach achieved an
average of 62% accuracy on classification tasks involving over 10000 words,
which is the largest such quantum computing experiment to date.
</p>
<p>We describe a quantum probability approach to bigram modeling that can be
applied to sequences of words and formal concepts, investigating a generative
approximation to these distributions using a quantum circuit Born machine, and
an approach to ambiguity resolution in verb-noun composition using single-qubit
rotations for simple nouns and 2-qubit controlled-NOT gates for simple verbs.
</p>
<p>The smaller systems described have been run successfully on physical quantum
computers, and the larger ones have been simulated. We show that statistically
meaningful results can be obtained using real datasets, but this is much more
difficult to predict than with easier artificial language examples used
previously in developing quantum NLP systems.
</p>
<p>Other approaches to quantum NLP are compared, partly with respect to
contemporary issues including informal language, fluency, and truthfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Medication Changes in Clinical Narratives using Pre-trained Language Models. (arXiv:2208.08417v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08417">
<div class="article-summary-box-inner">
<span><p>An accurate and detailed account of patient medications, including medication
changes within the patient timeline, is essential for healthcare providers to
provide appropriate patient care. Healthcare providers or the patients
themselves may initiate changes to patient medication. Medication changes take
many forms, including prescribed medication and associated dosage modification.
These changes provide information about the overall health of the patient and
the rationale that led to the current care. Future care can then build on the
resulting state of the patient. This work explores the automatic extraction of
medication change information from free-text clinical notes. The Contextual
Medication Event Dataset (CMED) is a corpus of clinical notes with annotations
that characterize medication changes through multiple change-related
attributes, including the type of change (start, stop, increase, etc.),
initiator of the change, temporality, change likelihood, and negation. Using
CMED, we identify medication mentions in clinical text and propose three novel
high-performing BERT-based systems that resolve the annotated medication change
characteristics. We demonstrate that our proposed systems improve medication
change classification performance over the initial work exploring CMED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptation of domain-specific transformer models with text oversampling for sentiment analysis of social media posts on Covid-19 vaccines. (arXiv:2209.10966v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10966">
<div class="article-summary-box-inner">
<span><p>Covid-19 has spread across the world and several vaccines have been developed
to counter its surge. To identify the correct sentiments associated with the
vaccines from social media posts, we fine-tune various state-of-the-art
pre-trained transformer models on tweets associated with Covid-19 vaccines.
Specifically, we use the recently introduced state-of-the-art pre-trained
transformer models RoBERTa, XLNet and BERT, and the domain-specific transformer
models CT-BERT and BERTweet that are pre-trained on Covid-19 tweets. We further
explore the option of text augmentation by oversampling using Language Model
based Oversampling Technique (LMOTE) to improve the accuracies of these models,
specifically, for small sample datasets where there is an imbalanced class
distribution among the positive, negative and neutral sentiment classes. Our
results summarize our findings on the suitability of text oversampling for
imbalanced small sample datasets that are used to fine-tune state-of-the-art
pre-trained transformer models, and the utility of domain-specific transformer
models for the classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10341">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our larger model BioGPT-Large achieves 81.0%
on PubMedQA. Our case study on text generation further demonstrates the
advantage of BioGPT on biomedical literature to generate fluent descriptions
for biomedical terms. Code is available at https://github.com/microsoft/BioGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Automated Essay Scoring using Transformer Models. (arXiv:2210.12809v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12809">
<div class="article-summary-box-inner">
<span><p>Automated essay scoring is one of the most important problem in Natural
Language Processing. It has been explored for a number of years, and it remains
partially solved. In addition to its economic and educational usefulness, it
presents research problems. Transfer learning has proved to be beneficial in
NLP. Data augmentation techniques have also helped build state-of-the-art
models for automated essay scoring. Many works in the past have attempted to
solve this problem by using RNNs, LSTMs, etc. This work examines the
transformer models like BERT, RoBERTa, etc. We empirically demonstrate the
effectiveness of transformer models and data augmentation for automated essay
grading across many topics using a single model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A partial order view of message-passing communication models. (arXiv:2210.13062v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13062">
<div class="article-summary-box-inner">
<span><p>There is a wide variety of message-passing communication models, ranging from
synchronous ''rendez-vous'' communications to fully asynchronous/out-of-order
communications. For large-scale distributed systems, the communication model is
determined by the transport layer of the network, and a few classes of orders
of message delivery (FIFO, causally ordered) have been identified in the early
days of distributed computing. For local-scale message-passing applications,
e.g., running on a single machine, the communication model may be determined by
the actual implementation of message buffers and by how FIFO queues are used.
While large-scale communication models, such as causal ordering, are defined by
logical axioms, local-scale models are often defined by an operational
semantics. In this work, we connect these two approaches, and we present a
unified hierarchy of communication models encompassing both large-scale and
local-scale models, based on their concurrent behaviors. We also show that all
the communication models we consider can be axiomatized in the monadic second
order logic, and may therefore benefit from several bounded verification
techniques based on bounded special treewidth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01181">
<div class="article-summary-box-inner">
<span><p>We demonstrate a proof-of-concept of a large language model conducting
corporate lobbying related activities. An autoregressive large language model
(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are
relevant to specific public companies and provides explanations and confidence
levels. For the bills the model deems as relevant, the model drafts a letter to
the sponsor of the bill in an attempt to persuade the congressperson to make
changes to the proposed legislation. We use hundreds of novel ground-truth
labels of the relevance of a bill to a company to benchmark the performance of
the model, which outperforms the baseline of predicting the most common outcome
of irrelevance. We also benchmark the performance of the previous OpenAI GPT-3
model (text-davinci-002), which was the state-of-the-art model on many academic
natural language tasks until text-davinci-003 was recently released. The
performance of text-davinci-002 is worse than a simple benchmark. These results
suggest that, as large language models continue to exhibit improved natural
language understanding capabilities, performance on corporate lobbying related
tasks will continue to improve. Longer-term, if AI begins to influence law in a
manner that is not a direct extension of human intentions, this threatens the
critical role that law as information could play in aligning AI with humans.
This Essay explores how this is increasingly a possibility. Initially, AI is
being used to simply augment human lobbyists for a small proportion of their
daily tasks. However, firms have an incentive to use less and less human
oversight over automated assessments of policy ideas and the written
communication to regulatory agencies and Congressional staffers. The core
question raised is where to draw the line between human-driven and AI-driven
policy influence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI2: The next leap toward native language based and explainable machine learning framework. (arXiv:2301.03391v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03391">
<div class="article-summary-box-inner">
<span><p>The machine learning frameworks flourished in the last decades, allowing
artificial intelligence to get out of academic circles to be applied to
enterprise domains. This field has significantly advanced, but there is still
some meaningful improvement to reach the subsequent expectations. The proposed
framework, named AI$^{2}$, uses a natural language interface that allows a
non-specialist to benefit from machine learning algorithms without necessarily
knowing how to program with a programming language. The primary contribution of
the AI$^{2}$ framework allows a user to call the machine learning algorithms in
English, making its interface usage easier. The second contribution is
greenhouse gas (GHG) awareness. It has some strategies to evaluate the GHG
generated by the algorithm to be called and to propose alternatives to find a
solution without executing the energy-intensive algorithm. Another contribution
is a preprocessing module that helps to describe and to load data properly.
Using an English text-based chatbot, this module guides the user to define
every dataset so that it can be described, normalized, loaded and divided
appropriately. The last contribution of this paper is about explainability. For
decades, the scientific community has known that machine learning algorithms
imply the famous black-box problem. Traditional machine learning methods
convert an input into an output without being able to justify this result. The
proposed framework explains the algorithm's process with the proper texts,
graphics and tables. The results, declined in five cases, present usage
applications from the user's English command to the explained output.
Ultimately, the AI$^{2}$ framework represents the next leap toward native
language-based, human-oriented concerns about machine learning framework.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-16 23:12:32.083708915 UTC">2023-01-16 23:12:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>