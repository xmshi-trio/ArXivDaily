<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-31T01:30:00Z">01-31</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Based Editing for Text Style Transfer. (arXiv:2301.11997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11997">
<div class="article-summary-box-inner">
<span><p>Prompting approaches have been recently explored in text style transfer,
where a textual prompt is used to query a pretrained language model to generate
style-transferred texts word by word in an autoregressive manner. However, such
a generation process is less controllable and early prediction errors may
affect future word predictions. In this paper, we present a prompt-based
editing approach for text style transfer. Specifically, we prompt a pretrained
language model for style classification and use the classification probability
to compute a style score. Then, we perform discrete search with word-level
editing to maximize a comprehensive scoring function for the style-transfer
task. In this way, we transform a prompt-based generation problem into a
classification one, which is a training-free process and more controllable than
the autoregressive generation of sentences. In our experiments, we performed
both automatic and human evaluation on three style-transfer benchmark datasets,
and show that our approach largely outperforms the state-of-the-art systems
that have 20 times more parameters. Additional empirical analyses further
demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation. (arXiv:2301.12004v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12004">
<div class="article-summary-box-inner">
<span><p>Language models have steadily increased in size over the past few years. They
achieve a high level of performance on various natural language processing
(NLP) tasks such as question answering and summarization. Large language models
(LLMs) have been used for generation and can now output human-like text. Due to
this, there are other downstream tasks in the realm of dialog that can now
harness the LLMs' language understanding capabilities. Dialog evaluation is one
task that this paper will explore. It concentrates on prompting with LLMs:
BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the
choice of datasets used for training a model contributes to how well it
performs on a task as well as on how the prompt should be structured.
Specifically, the more diverse and relevant the group of datasets that a model
is trained on, the better dialog evaluation performs. This paper also
investigates how the number of examples in the prompt and the type of example
selection used affect the model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved knowledge distillation by utilizing backward pass knowledge in neural networks. (arXiv:2301.12006v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12006">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is one of the prominent techniques for model
compression. In this method, the knowledge of a large network (teacher) is
distilled into a model (student) with usually significantly fewer parameters.
KD tries to better-match the output of the student model to that of the teacher
model based on the knowledge extracts from the forward pass of the teacher
network. Although conventional KD is effective for matching the two networks
over the given data points, there is no guarantee that these models would match
in other areas for which we do not have enough training samples. In this work,
we address that problem by generating new auxiliary training samples based on
extracting knowledge from the backward pass of the teacher in the areas where
the student diverges greatly from the teacher. We compute the difference
between the teacher and the student and generate new data samples that maximize
the divergence. This is done by perturbing data samples in the direction of the
gradient of the difference between the student and the teacher. Augmenting the
training set by adding this auxiliary improves the performance of KD
significantly and leads to a closer match between the student and the teacher.
Using this approach, when data samples come from a discrete domain, such as
applications of natural language processing (NLP) and language understanding,
is not trivial. However, we show how this technique can be used successfully in
such applications. We evaluated the performance of our method on various tasks
in computer vision and NLP domains and got promising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (arXiv:2301.12017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12017">
<div class="article-summary-box-inner">
<span><p>Improving the deployment efficiency of transformer-based language models has
been challenging given their high computation and memory cost. While INT8
quantization has recently been shown to be effective in reducing both the
memory cost and latency while preserving model accuracy, it remains unclear
whether we can leverage INT4 (which doubles peak hardware throughput) to
achieve further latency improvement. In this work, we fully investigate the
feasibility of using INT4 quantization for language models, and show that using
INT4 introduces no or negligible accuracy degradation for encoder-only and
encoder-decoder models, but causes a significant accuracy drop for decoder-only
models. To materialize the performance gain using INT4, we develop a
highly-optimized end-to-end INT4 encoder inference pipeline supporting
different quantization strategies. Our INT4 pipeline is $8.5\times$ faster for
latency-oriented scenarios and up to $3\times$ for throughput-oriented
scenarios compared to the inference of FP16, and improves the SOTA BERT INT8
performance from FasterTransformer by up to $1.7\times$. We also provide
insights into the failure cases when applying INT4 to decoder-only models, and
further explore the compatibility of INT4 quantization with other compression
techniques, like pruning and layer reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12050">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world, which makes learning complex tasks with sparse rewards
difficult. If initialized with knowledge of high-level subgoals and transitions
between subgoals, RL agents could utilize this Abstract World Model (AWM) for
planning and exploration. We propose using few-shot large language models
(LLMs) to hypothesize an AWM, that is tested and verified during exploration,
to improve sample efficiency in embodied RL agents. Our DECKARD agent applies
LLM-guided exploration to item crafting in Minecraft in two phases: (1) the
Dream phase where the agent uses an LLM to decompose a task into a sequence of
subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a
modular policy for each subgoal and verifies or corrects the hypothesized AWM
on the basis of its experiences. Our method of hypothesizing an AWM with LLMs
and then verifying the AWM based on agent experience not only increases sample
efficiency over contemporary methods by an order of magnitude but is also
robust to and corrects errors in the LLM, successfully blending noisy
internet-scale information from LLMs with knowledge grounded in environment
dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Intrinsic Gender Bias Evaluation Measures without using Human Annotated Examples. (arXiv:2301.12074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12074">
<div class="article-summary-box-inner">
<span><p>Numerous types of social biases have been identified in pre-trained language
models (PLMs), and various intrinsic bias evaluation measures have been
proposed for quantifying those social biases. Prior works have relied on human
annotated examples to compare existing intrinsic bias evaluation measures.
However, this approach is not easily adaptable to different languages nor
amenable to large scale evaluations due to the costs and difficulties when
recruiting human annotators. To overcome this limitation, we propose a method
to compare intrinsic gender bias evaluation measures without relying on
human-annotated examples. Specifically, we create multiple bias-controlled
versions of PLMs using varying amounts of male vs. female gendered sentences,
mined automatically from an unannotated corpus using gender-related word lists.
Next, each bias-controlled PLM is evaluated using an intrinsic bias evaluation
measure, and the rank correlation between the computed bias scores and the
gender proportions used to fine-tune the PLMs is computed. Experiments on
multiple corpora and PLMs repeatedly show that the correlations reported by our
proposed method that does not require human annotated examples are comparable
to those computed using human annotated examples in prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Pre-trained Language Models for Antibody. (arXiv:2301.12112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12112">
<div class="article-summary-box-inner">
<span><p>Antibodies are vital proteins offering robust protection for the human body
from pathogens. The development of general protein and antibody-specific
pre-trained language models both facilitate antibody prediction tasks. However,
few studies comprehensively explore the representation capability of distinct
pre-trained language models on different antibody problems. Here, to
investigate the problem, we aim to answer the following key questions: (1) How
do pre-trained language models perform in antibody tasks with different
specificity? (2) How many benefits will the model gain if we introduce the
specific biological mechanism to the pre-training process? (3) Do the learned
antibody pre-trained representations make sense in real-world antibody
problems, like drug discovery and immune process understanding? Previously, no
benchmark available largely hindered the study to answer these questions. To
facilitate the investigation, we provide an AnTibody Understanding Evaluation
(ATUE) benchmark. We comprehensively evaluate the performance of protein
pre-trained language models by empirical study along with conclusions and new
insights. Our ATUE and code are released at https://github.com/dqwang122/EATLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12132">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models have been widely used in downstream NLP
tasks via task-specific fine-tuning. Recently, an array of Parameter-Efficient
Fine-Tuning (PEFT) methods have also achieved strong task performance while
updating a much smaller number of parameters compared to full model tuning.
However, it is non-trivial to make informed per-task design choices (i.e., to
create PEFT configurations) concerning the selection of PEFT architectures and
modules, the number of tunable parameters, and even the layers in which the
PEFT modules are inserted. Consequently, it is highly likely that the current,
manually set PEFT configurations might be suboptimal for many tasks from the
perspective of the performance-to-efficiency trade-off. To address the core
question of the PEFT configuration selection that aims to control and maximise
the balance between performance and parameter efficiency, we first define a
rich configuration search space spanning multiple representative PEFT modules
along with finer-grained configuration decisions over the modules (e.g.,
parameter budget, insertion layer). We then propose AutoPEFT, a novel framework
to traverse this configuration space: it automatically configures multiple PEFT
modules via high-dimensional Bayesian optimisation. We show the resource
scalability and task transferability of AutoPEFT-found configurations,
outperforming existing PEFT methods on average on the standard GLUE benchmark
while conducting the configuration search on a single task. The per-task
AutoPEFT-based configuration search even outperforms full-model fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Underwater Robotics Semantic Parser Assistant. (arXiv:2301.12134v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12134">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is a means of taking natural language and putting it in a
form that a computer can understand. There has been a multitude of approaches
that take natural language utterances and form them into lambda calculus
expressions -- mathematical functions to describe logic. Here, we experiment
with a sequence to sequence model to take natural language utterances, convert
those to lambda calculus expressions, when can then be parsed, and place them
in an XML format that can be used by a finite state machine. Experimental
results show that we can have a high accuracy model such that we can bridge the
gap between technical and nontechnical individuals in the robotics field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets. (arXiv:2301.12139v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12139">
<div class="article-summary-box-inner">
<span><p>We evaluate five English NLP benchmark datasets (available on the superGLUE
leaderboard) for bias, along multiple axes. The datasets are the following:
Boolean Question (Boolq), CommitmentBank (CB), Winograd Schema Challenge (WSC),
Winogender diagnostic (AXg), and Recognising Textual Entailment (RTE). Bias can
be harmful and it is known to be common in data, which ML models learn from. In
order to mitigate bias in data, it is crucial to be able to estimate it
objectively. We use bipol, a novel multi-axes bias metric with explainability,
to quantify and explain how much bias exists in these datasets. Multilingual,
multi-axes bias evaluation is not very common. Hence, we also contribute a new,
large labelled Swedish bias-detection dataset, with about 2 million samples;
translated from the English version. In addition, we contribute new multi-axes
lexica for bias detection in Swedish. We train a SotA model on the new dataset
for bias detection. We make the codes, model, and new dataset publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Sentence Transformer as A Multilingual Word Aligner. (arXiv:2301.12140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12140">
<div class="article-summary-box-inner">
<span><p>Multilingual pretrained language models (mPLMs) have shown their
effectiveness in multilingual word alignment induction. However, these methods
usually start from mBERT or XLM-R. In this paper, we investigate whether
multilingual sentence Transformer LaBSE is a strong multilingual word aligner.
This idea is non-trivial as LaBSE is trained to learn language-agnostic
sentence-level embeddings, while the alignment extraction task requires the
more fine-grained word-level embeddings to be language-agnostic. We demonstrate
that the vanilla LaBSE outperforms other mPLMs currently used in the alignment
task, and then propose to finetune LaBSE on parallel corpus for further
improvement. Experiment results on seven language pairs show that our best
aligner outperforms previous state-of-the-art models of all varieties. In
addition, our aligner supports different language pairs in a single model, and
even achieves new state-of-the-art on zero-shot language pairs that does not
appear in the finetuning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Tagging with LSTM-CRF. (arXiv:2301.12206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12206">
<div class="article-summary-box-inner">
<span><p>In the present paper, two models are presented namely LSTM-CRF and
BERT-LSTM-CRF for semantic tagging of universal semantic tag dataset. The
experiments show that the first model is much easier to converge while the
second model that leverages BERT embedding, takes a long time to converge and
needs a big dataset for semtagging to be effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Parsing for Conversational Question Answering over Knowledge Graphs. (arXiv:2301.12217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12217">
<div class="article-summary-box-inner">
<span><p>In this paper, we are interested in developing semantic parsers which
understand natural language questions embedded in a conversation with a user
and ground them to formal queries over definitions in a general purpose
knowledge graph (KG) with very large vocabularies (covering thousands of
concept names and relations, and millions of entities). To this end, we develop
a dataset where user questions are annotated with Sparql parses and system
answers correspond to execution results thereof. We present two different
semantic parsing approaches and highlight the challenges of the task: dealing
with large vocabularies, modelling conversation context, predicting queries
with multiple entities, and generalising to new questions at test time. We hope
our dataset will serve as useful testbed for the development of conversational
semantic parsers. Our dataset and models are released at
https://github.com/EdinburghNLP/SPICE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Presence of informal language, such as emoticons, hashtags, and slang, impact the performance of sentiment analysis models on social media text?. (arXiv:2301.12303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12303">
<div class="article-summary-box-inner">
<span><p>This study aimed to investigate the influence of the presence of informal
language, such as emoticons and slang, on the performance of sentiment analysis
models applied to social media text. A convolutional neural network (CNN) model
was developed and trained on three datasets: a sarcasm dataset, a sentiment
dataset, and an emoticon dataset. The model architecture was held constant for
all experiments and the model was trained on 80% of the data and tested on 20%.
The results revealed that the model achieved an accuracy of 96.47% on the
sarcasm dataset, with the lowest accuracy for class 1. On the sentiment
dataset, the model achieved an accuracy of 95.28%. The amalgamation of sarcasm
and sentiment datasets improved the accuracy of the model to 95.1%, and the
addition of emoticon dataset has a slight positive impact on the accuracy of
the model to 95.37%. The study suggests that the presence of informal language
has a restricted impact on the performance of sentiment analysis models applied
to social media text. However, the inclusion of emoticon data to the model can
enhance the accuracy slightly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization. (arXiv:2301.12307v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12307">
<div class="article-summary-box-inner">
<span><p>State-of-the-art summarization systems can generate highly fluent summaries.
These summaries, however, may contain factual inconsistencies and/or
information not present in the source. Hence, an important component of
assessing the quality of summaries is to determine whether there is information
consistency between the source and the summary. Existing approaches are
typically based on lexical matching or representation-based methods. In this
work, we introduce an alternative scheme based on standard
information-theoretic measures in which the information present in the source
and summary is directly compared. We propose a Multiple-choice Question
Answering and Generation framework, MQAG, which approximates the information
consistency by computing the expected KL-divergence between summary and source
answer distributions over automatically generated multiple-choice questions.
This approach exploits multiple-choice answer probabilities, as predicted
answer distributions can be easily compared. We conduct experiments on four
summary evaluation datasets: QAG-CNNDM/XSum, XSum-Faithfulness, Podcast
Assessment, and SummEval. Experiments show that MQAG (using models trained on
RACE) outperforms existing evaluation methods on the majority of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Prompts: Continual Learning for Language Models. (arXiv:2301.12314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12314">
<div class="article-summary-box-inner">
<span><p>We introduce Progressive Prompts - a simple and efficient approach for
continual learning in language models. Our method allows forward transfer and
resists catastrophic forgetting, without relying on data replay or a large
number of task-specific parameters. Progressive Prompts learns a new soft
prompt for each task and sequentially concatenates it with the previously
learned prompts, while keeping the base model frozen. Experiments on standard
continual learning benchmarks show that our approach outperforms
state-of-the-art methods, with an improvement &gt;20% in average test accuracy
over the previous best-preforming method on T5 model. We also explore a more
challenging continual learning setup with longer sequences of tasks and show
that Progressive Prompts significantly outperforms prior methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time out of Mind: Generating Emotionally Conditioned Rate of Speech. (arXiv:2301.12331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12331">
<div class="article-summary-box-inner">
<span><p>Voice synthesis has seen significant improvements in the past decade
resulting in highly intelligible voices. Further investigations have resulted
in models that can produce variable speech, including conditional emotional
expression. The problem lies, however, in a focus on phrase level modifications
and prosodic vocal features. Using the CREMA-D dataset we have trained a GAN
conditioned on emotion to generate worth lengths for a given input text. These
word lengths are relative to neutral speech and can be provided, through speech
synthesis markup language (SSML) to a text to speech (TTS) system to generate
more expressive speech. We were able to achieve better performances on
objective measures for neutral speech, and better time alignment for happy
speech when compared to an out of box model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Achieving Timestamp Prediction While Recognizing with Non-Autoregressive End-to-End ASR Model. (arXiv:2301.12343v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12343">
<div class="article-summary-box-inner">
<span><p>Conventional ASR systems use frame-level phoneme posterior to conduct
force-alignment~(FA) and provide timestamps, while end-to-end ASR systems
especially AED based ones are short of such ability. This paper proposes to
perform timestamp prediction~(TP) while recognizing by utilizing continuous
integrate-and-fire~(CIF) mechanism in non-autoregressive ASR model -
Paraformer. Foucing on the fire place bias issue of CIF, we conduct
post-processing strategies including fire-delay and silence insertion. Besides,
we propose to use scaled-CIF to smooth the weights of CIF output, which is
proved beneficial for both ASR and TP task. Accumulated averaging shift~(AAS)
and diarization error rate~(DER) are adopted to measure the quality of
timestamps and we compare these metrics of proposed system and conventional
hybrid force-alignment system. The experiment results over manually-marked
timestamps testset show that the proposed optimization methods significantly
improve the accuracy of CIF timestamps, reducing 66.7\% and 82.1\% of AAS and
DER respectively. Comparing to Kaldi force-alignment trained with the same
data, optimized CIF timestamps achieved 12.3\% relative AAS reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising. (arXiv:2105.07911v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07911">
<div class="article-summary-box-inner">
<span><p>In text-to-SQL task, seq-to-seq models often lead to sub-optimal performance
due to limitations in their architecture. In this paper, we present a simple
yet effective approach that adapts transformer-based seq-to-seq model to robust
text-to-SQL generation. Instead of inducing constraint to decoder or reformat
the task as slot-filling, we propose to train seq-to-seq model with Schema
aware Denoising (SeaD), which consists of two denoising objectives that train
model to either recover input or predict output from two novel erosion and
shuffle noises. These denoising objectives acts as the auxiliary tasks for
better modeling the structural data in S2S generation. In addition, we improve
and propose a clause-sensitive execution guided (EG) decoding strategy to
overcome the limitation of EG decoding for generative model. The experiments
show that the proposed method improves the performance of seq-to-seq model in
both schema linking and grammar correctness and establishes new
state-of-the-art on WikiSQL benchmark. The results indicate that the capacity
of vanilla seq-to-seq architecture for text-to-SQL may have been
under-estimated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Systematic Investigation of Strategies Tailored for Low-Resource Settings for Low-Resource Dependency Parsing. (arXiv:2201.11374v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11374">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on low-resource dependency parsing for multiple
languages. Several strategies are tailored to enhance performance in
low-resource scenarios. While these are well-known to the community, it is not
trivial to select the best-performing combination of these strategies for a
low-resource language that we are interested in, and not much attention has
been given to measuring the efficacy of these strategies. We experiment with 5
low-resource strategies for our ensembled approach on 7 Universal Dependency
(UD) low-resource languages. Our exhaustive experimentation on these languages
supports the effective improvements for languages not covered in pretrained
models. We show a successful application of the ensembled system on a truly
low-resource language Sanskrit. The code and data are available at:
https://github.com/Jivnesh/SanDP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks. (arXiv:2205.00305v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00305">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models with millions of parameters require
large storage. Recent approaches tackle this shortcoming by training adapters,
but these approaches still require a relatively large number of parameters. In
this study, AdapterBias, a surprisingly simple yet effective adapter
architecture, is proposed. AdapterBias adds a token-dependent shift to the
hidden output of transformer layers to adapt to downstream tasks with only a
vector and a linear layer. Extensive experiments are conducted to demonstrate
the effectiveness of AdapterBias. The experiments show that our proposed method
can dramatically reduce the trainable parameters compared to the previous works
with a minimal decrease in task performances compared with fine-tuned
pre-trained models. We further find that AdapterBias automatically learns to
assign more significant representation shifts to the tokens related to the task
in consideration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Zero-Shot Reasoners. (arXiv:2205.11916v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11916">
<div class="article-summary-box-inner">
<span><p>Pretrained large language models (LLMs) are widely used in many sub-fields of
natural language processing (NLP) and generally known as excellent few-shot
learners with task-specific exemplars. Notably, chain of thought (CoT)
prompting, a recent technique for eliciting complex multi-step reasoning
through step-by-step answer examples, achieved the state-of-the-art
performances in arithmetics and symbolic reasoning, difficult system-2 tasks
that do not follow the standard scaling laws for LLMs. While these successes
are often attributed to LLMs' ability for few-shot learning, we show that LLMs
are decent zero-shot reasoners by simply adding "Let's think step by step"
before each answer. Experimental results demonstrate that our Zero-shot-CoT,
using the same single prompt template, significantly outperforms zero-shot LLM
performances on diverse benchmark reasoning tasks including arithmetics
(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin
Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled
Objects), without any hand-crafted few-shot examples, e.g. increasing the
accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with
large InstructGPT model (text-davinci-002), as well as similar magnitudes of
improvements with another off-the-shelf large model, 540B parameter PaLM. The
versatility of this single prompt across very diverse reasoning tasks hints at
untapped and understudied fundamental zero-shot capabilities of LLMs,
suggesting high-level, multi-task broad cognitive capabilities may be extracted
by simple prompting. We hope our work not only serves as the minimal strongest
zero-shot baseline for the challenging reasoning benchmarks, but also
highlights the importance of carefully exploring and analyzing the enormous
zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or
few-shot exemplars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Locality and Isotropy in Dialogue Modeling. (arXiv:2205.14583v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14583">
<div class="article-summary-box-inner">
<span><p>Existing dialogue modeling methods have achieved promising performance on
various dialogue tasks with the aid of Transformer and the large-scale
pre-trained language models. However, some recent studies revealed that the
context representations produced by these methods suffer the problem of
anisotropy. In this paper, we find that the generated representations are also
not conversational, losing the conversation structure information during the
context modeling stage. To this end, we identify two properties in dialogue
modeling, i.e., locality and isotropy, and present a simple method for dialogue
representation calibration, namely SimDRC, to build isotropic and
conversational feature spaces. Experimental results show that our approach
significantly outperforms the current state-of-the-art models on three dialogue
tasks across the automatic and human evaluation metrics. More in-depth analyses
further confirm the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making sense of spoken plurals. (arXiv:2207.01947v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01947">
<div class="article-summary-box-inner">
<span><p>Distributional semantics offers new ways to study the semantics of
morphology. This study focuses on the semantics of noun singulars and their
plural inflectional variants in English. Our goal is to compare two models for
the conceptualization of plurality. One model (FRACSS) proposes that all
singular-plural pairs should be taken into account when predicting plural
semantics from singular semantics. The other model (CCA) argues that
conceptualization for plurality depends primarily on the semantic class of the
base word. We compare the two models on the basis of how well the speech signal
of plural tokens in a large corpus of spoken American English aligns with the
semantic vectors predicted by the two models. Two measures are employed: the
performance of a form-to-meaning mapping and the correlations between form
distances and meaning distances. Results converge on a superior alignment for
CCA. Our results suggest that usage-based approaches to pluralization in which
a given word's own semantic neighborhood is given priority outperform theories
according to which pluralization is conceptualized as a process building on
high-level abstraction. We see that what has often been conceived of as a
highly abstract concept, [+plural], is better captured via a family of
mid-level partial generalizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13243">
<div class="article-summary-box-inner">
<span><p>The last decade of machine learning has seen drastic increases in scale and
capabilities. Deep neural networks (DNNs) are increasingly being deployed in
the real world. However, they are difficult to analyze, raising concerns about
using them without a rigorous understanding of how they function. Effective
tools for interpreting them will be important for building more trustworthy AI
by helping to identify problems, fix bugs, and improve basic understanding. In
particular, "inner" interpretability techniques, which focus on explaining the
internal components of DNNs, are well-suited for developing a mechanistic
understanding, guiding manual modifications, and reverse engineering solutions.
</p>
<p>Much recent work has focused on DNN interpretability, and rapid progress has
thus far made a thorough systematization of methods difficult. In this survey,
we review over 300 works with a focus on inner interpretability tools. We
introduce a taxonomy that classifies methods by what part of the network they
help to explain (weights, neurons, subnetworks, or latent representations) and
whether they are implemented during (intrinsic) or after (post hoc) training.
To our knowledge, we are also the first to survey a number of connections
between interpretability research and work in adversarial robustness, continual
learning, modularity, network compression, and studying the human visual
system. We discuss key challenges and argue that the status quo in
interpretability research is largely unproductive. Finally, we highlight the
importance of future work that emphasizes diagnostics, debugging, adversaries,
and benchmarking in order to make interpretability tools more useful to
engineers in practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complexity-Based Prompting for Multi-Step Reasoning. (arXiv:2210.00720v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00720">
<div class="article-summary-box-inner">
<span><p>We study the task of prompting large-scale language models to perform
multi-step reasoning. Existing work shows that when prompted with a chain of
thoughts (CoT), sequences of short sentences describing intermediate reasoning
steps towards a final answer, large language models can generate new reasoning
chains and predict answers for new inputs. A central question is which
reasoning examples make the most effective prompts. In this work, we propose
complexity-based prompting, a simple and effective example selection scheme for
multi-step reasoning. We show that prompts with higher reasoning complexity,
i.e., chains with more reasoning steps, achieve substantially better
performance on multi-step reasoning tasks over strong baselines. We further
extend our complexity-based criteria from prompting (selecting inputs) to
decoding (selecting outputs), where we sample multiple reasoning chains from
the model, then choose the majority of generated answers from complex reasoning
chains (over simple chains). When used to prompt GPT-3 and Codex, our approach
substantially improves multi-step reasoning accuracy and achieves new
state-of-the-art (SOTA) performance on three math benchmarks (GSM8K,
MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and
Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared
with existing example selection schemes like manual tuning or retrieval-based
selection, selection based on reasoning complexity is intuitive, easy to
implement, and annotation-efficient. Further results demonstrate the robustness
of performance gains from complex prompts under format perturbation and
distribution shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching Biomedical Knowledge for Low-resource Language Through Large-Scale Translation. (arXiv:2210.05598v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05598">
<div class="article-summary-box-inner">
<span><p>Biomedical data and benchmarks are highly valuable yet very limited in
low-resource languages other than English such as Vietnamese. In this paper, we
make use of a state-of-the-art translation model in English-Vietnamese to
translate and produce both pretrained as well as supervised data in the
biomedical domains. Thanks to such large-scale translation, we introduce
ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20
million translated abstracts from the high-quality public PubMed corpus.
ViPubMedT5 demonstrates state-of-the-art results on two different biomedical
benchmarks in summarization and acronym disambiguation. Further, we release
ViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the
recently public En-vi translation model and carefully refined by human experts,
with evaluations of existing methods against ViPubmedT5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CS-Insights: A System for Analyzing Computer Science Research. (arXiv:2210.06878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06878">
<div class="article-summary-box-inner">
<span><p>This paper presents CS-Insights, an interactive web application to analyze
computer science publications from DBLP through multiple perspectives. The
dedicated interfaces allow its users to identify trends in research activity,
productivity, accessibility, author's productivity, venues' statistics, topics
of interest, and the impact of computer science research on other fields.
CS-Insightsis publicly available, and its modular architecture can be easily
adapted to domains other than computer science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation Extraction. (arXiv:2210.11800v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11800">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) has achieved remarkable progress with the help of
pre-trained language models. However, existing RE models are usually incapable
of handling two situations: implicit expressions and long-tail relation types,
caused by language complexity and data sparsity. In this paper, we introduce a
simple enhancement of RE using $k$ nearest neighbors ($k$NN-RE). $k$NN-RE
allows the model to consult training relations at test time through a
nearest-neighbor search and provides a simple yet effective means to tackle the
two issues above. Additionally, we observe that $k$NN-RE serves as an effective
way to leverage distant supervision (DS) data for RE. Experimental results show
that the proposed $k$NN-RE achieves state-of-the-art performances on a variety
of supervised RE datasets, i.e., ACE05, SciERC, and Wiki80, along with
outperforming the best model to date on the i2b2 and Wiki80 datasets in the
setting of allowing using DS. Our code and models are available at:
https://github.com/YukinoWan/kNN-RE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17406">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been reported to have strong performance on
natural language processing tasks. However, performance metrics such as
accuracy do not measure the quality of the model in terms of its ability to
robustly represent complex linguistic structure. In this work, we propose a
framework and measure of robustness to assess the consistency of linguistic
representations against syntax-preserving perturbations. We leverage recent
advances in extracting linguistic constructs from LLMs to test the robustness
of such structures. Empirically, we study the performance of four LLMs across
six different corpora on the proposed robustness measures. We provide evidence
that context-free representation (e.g., GloVe) are in some cases competitive
with context-dependent representations from modern LLMs (e.g., BERT), yet
equally brittle to syntax-preserving manipulations. Emergent syntactic
representations in neural networks are brittle, thus our work poses the
attention on the risk of comparing such structures to those that are object of
a long lasting debate in linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01117">
<div class="article-summary-box-inner">
<span><p>The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. (arXiv:2212.05032v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05032">
<div class="article-summary-box-inner">
<span><p>Large-scale diffusion models have achieved state-of-the-art results on
text-to-image synthesis (T2I) tasks. Despite their ability to generate
high-quality yet creative images, we observe that attribution-binding and
compositional capabilities are still considered major challenging issues,
especially when involving multiple objects. In this work, we improve the
compositional skills of T2I models, specifically more accurate attribute
binding and better image compositions. To do this, we incorporate linguistic
structures with the diffusion guidance process based on the controllable
properties of manipulating cross-attention layers in diffusion-based T2I
models. We observe that keys and values in cross-attention layers have strong
semantic meanings associated with object layouts and content. Therefore, we can
better preserve the compositional semantics in the generated image by
manipulating the cross-attention representations based on linguistic insights.
Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention
design is efficient that requires no additional training samples. We achieve
better compositional skills in qualitative and quantitative results, leading to
a 5-8% advantage in head-to-head user comparison studies. Lastly, we conduct an
in-depth analysis to reveal potential causes of incorrect image compositions
and justify the properties of cross-attention layers in the generation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments. (arXiv:2212.07425v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07425">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation, propaganda, and flawed argumentation has been
amplified in the Internet era. Given the volume of data and the subtlety of
identifying violations of argumentation norms, supporting information analytics
tasks, like content moderation, with trustworthy methods that can identify
logical fallacies is essential. In this paper, we formalize prior theoretical
work on logical fallacies into a comprehensive three-stage evaluation framework
of detection, coarse-grained, and fine-grained classification. We adapt
existing evaluation datasets for each stage of the evaluation. We employ three
families of robust and explainable methods based on prototype reasoning,
instance-based reasoning, and knowledge injection. The methods combine language
models with background knowledge and explainable mechanisms. Moreover, we
address data sparsity with strategies for data augmentation and curriculum
learning. Our three-stage framework natively consolidates prior datasets and
methods from existing tasks, like propaganda detection, serving as an
overarching evaluation testbed. We extensively evaluate these methods on our
datasets, focusing on their robustness and explainability. Our results provide
insight into the strengths and weaknesses of the methods on different
components and fallacy classes, indicating that fallacy identification is a
challenging task that may require specialized forms of reasoning to capture
various classes. We share our open-source code and data on GitHub to support
further work on logical fallacy identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. (arXiv:2212.12017v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12017">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that fine-tuning large pre-trained language models on a
collection of tasks described via instructions, a.k.a. instruction-tuning,
improves their zero and few-shot generalization to unseen tasks. However, there
is a limited understanding of the performance trade-offs of different decisions
made during the instruction-tuning process. These decisions include the scale
and diversity of the instruction-tuning benchmark, different task sampling
strategies, fine-tuning with and without demonstrations, training using
specialized datasets for reasoning and dialogue, and finally, the fine-tuning
objectives themselves. In this paper, we characterize the effect of
instruction-tuning decisions on downstream task performance when scaling both
model and benchmark sizes. To this end, we create OPT-IML Bench: a large
benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated
into task categories from 8 existing benchmarks, and prepare an evaluation
framework to measure three types of model generalizations: to tasks from fully
held-out categories, to held-out tasks from seen categories, and to held-out
instances from seen tasks. Through the lens of this framework, we first present
insights about instruction-tuning decisions as applied to OPT-30B and further
exploit these insights to train OPT-IML 30B and 175B, which are
instruction-tuned versions of OPT. OPT-IML demonstrates all three
generalization abilities at both scales on four different evaluation benchmarks
with diverse tasks and input formats -- PromptSource, FLAN,
Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly
outperform OPT on all benchmarks but is also highly competitive with existing
models fine-tuned on each specific benchmark. We release OPT-IML at both
scales, together with the OPT-IML Bench evaluation framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01181">
<div class="article-summary-box-inner">
<span><p>We demonstrate a proof-of-concept of a large language model conducting
corporate lobbying related activities. An autoregressive large language model
(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are
relevant to specific public companies and provides explanations and confidence
levels. For the bills the model deems as relevant, the model drafts a letter to
the sponsor of the bill in an attempt to persuade the congressperson to make
changes to the proposed legislation. We use hundreds of novel ground-truth
labels of the relevance of a bill to a company to benchmark the performance of
the model. It outperforms the baseline of predicting the most common outcome of
irrelevance. We also benchmark the performance of the previous OpenAI GPT-3
model (text-davinci-002), which was the state-of-the-art model on many academic
natural language tasks until text-davinci-003 was recently released. The
performance of text-davinci-002 is worse than the simple baseline. Longer-term,
if AI begins to influence law in a manner that is not a direct extension of
human intentions, this threatens the critical role that law as information
could play in aligning AI with humans. Initially, AI is being used to simply
augment human lobbyists for a small portion of their daily tasks. However,
firms have an incentive to use less and less human oversight over automated
assessments of policy ideas and the written communication to regulatory
agencies and Congressional staffers. The core question raised is where to draw
the line between human-driven and AI-driven policy influence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cohesive Distillation Architecture for Neural Language Models. (arXiv:2301.08130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08130">
<div class="article-summary-box-inner">
<span><p>A recent trend in Natural Language Processing is the exponential growth in
Language Model (LM) size, which prevents research groups without a necessary
hardware infrastructure from participating in the development process. This
study investigates methods for Knowledge Distillation (KD) to provide efficient
alternatives to large-scale models. In this context, KD means extracting
information about language encoded in a Neural Network and Lexical Knowledge
Databases. We developed two methods to test our hypothesis that efficient
architectures can gain knowledge from LMs and extract valuable information from
lexical sources. First, we present a technique to learn confident probability
distribution for Masked Language Modeling by prediction weighting of multiple
teacher networks. Second, we propose a method for Word Sense Disambiguation
(WSD) and lexical KD that is general enough to be adapted to many LMs. Our
results show that KD with multiple teachers leads to improved training
convergence. When using our lexical pre-training method, LM characteristics are
not lost, leading to increased performance in Natural Language Understanding
(NLU) tasks over the state-of-the-art while adding no parameters. Moreover, the
improved semantic understanding of our model increased the task performance
beyond WSD and NLU in a real-problem scenario (Plagiarism Detection). This
study suggests that sophisticated training methods and network architectures
can be superior over scaling trainable parameters. On this basis, we suggest
the research area should encourage the development and use of efficient models
and rate impacts resulting from growing LM size equally against task
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness. (arXiv:2301.08881v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08881">
<div class="article-summary-box-inner">
<span><p>Neural text-to-SQL models have achieved remarkable performance in translating
natural language questions into SQL queries. However, recent studies reveal
that text-to-SQL models are vulnerable to task-specific perturbations. Previous
curated robustness test sets usually focus on individual phenomena. In this
paper, we propose a comprehensive robustness benchmark based on Spider, a
cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design
17 perturbations on databases, natural language questions, and SQL queries to
measure the robustness from different angles. In order to collect more
diversified natural question perturbations, we utilize large pretrained
language models (PLMs) to simulate human behaviors in creating natural
questions. We conduct a diagnostic study of the state-of-the-art models on the
robustness set. Experimental results reveal that even the most robust model
suffers from a 14.0% performance drop overall and a 50.7% performance drop on
the most challenging perturbation. We also present a breakdown analysis
regarding text-to-SQL model designs and provide insights for improving model
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poor Man's Quality Estimation: Predicting Reference-Based MT Metrics Without the Reference. (arXiv:2301.09008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09008">
<div class="article-summary-box-inner">
<span><p>Machine translation quality estimation (QE) predicts human judgements of a
translation hypothesis without seeing the reference. State-of-the-art QE
systems based on pretrained language models have been achieving remarkable
correlations with human judgements yet they are computationally heavy and
require human annotations, which are slow and expensive to create. To address
these limitations, we define the problem of metric estimation (ME) where one
predicts the automated metric scores also without the reference. We show that
even without access to the reference, our model can estimate automated metrics
($\rho$=60% for BLEU, $\rho$=51% for other metrics) at the sentence-level.
Because automated metrics correlate with human judgements, we can leverage the
ME task for pre-training a QE model. For the QE task, we find that pre-training
on TER is better ($\rho$=23%) than training for scratch ($\rho$=20%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Fiduciaries: A Case Study Toward Robustly Communicating With Artificial Intelligence Through Legal Standards. (arXiv:2301.10095v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10095">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) is taking on increasingly autonomous roles,
e.g., browsing the web as a research assistant and managing money. But
specifying goals and restrictions for AI behavior is difficult. Similar to how
parties to a legal contract cannot foresee every potential "if-then"
contingency of their future relationship, we cannot specify desired AI behavior
for all circumstances. Legal standards facilitate robust communication of
inherently vague and underspecified goals. Instructions (in the case of
language models, "prompts") that employ legal standards will allow AI agents to
develop shared understandings of the spirit of a directive that generalize
expectations regarding acceptable actions to take in unspecified states of the
world. Standards have built-in context that is lacking from other goal
specification languages, such as plain language and programming languages.
Through an empirical study on thousands of evaluation labels we constructed
from U.S. court opinions, we demonstrate that large language models (LLMs) are
beginning to exhibit an "understanding" of one of the most relevant legal
standards for AI agents: fiduciary obligations. Performance comparisons across
models suggest that, as LLMs continue to exhibit improved core capabilities,
their legal standards understanding will also continue to improve. OpenAI's
latest LLM has 78% accuracy on our data, their previous release has 73%
accuracy, and a model from their 2020 GPT-3 paper has 27% accuracy (worse than
random). Our research is an initial step toward a framework for evaluating AI
understanding of legal standards more broadly, and for conducting reinforcement
learning with legal feedback (RLLF).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MTTN: Multi-Pair Text to Text Narratives for Prompt Generation. (arXiv:2301.10172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10172">
<div class="article-summary-box-inner">
<span><p>The increased interest in diffusion models has opened up opportunities for
advancements in generative text modeling. These models can produce impressive
images when given a well-crafted prompt, but creating a powerful or meaningful
prompt can be hit-or-miss. To address this, we have created a large-scale
dataset that is derived and synthesized from real prompts and indexed with
popular image-text datasets such as MS-COCO and Flickr. We have also
implemented stages that gradually reduce context and increase complexity, which
will further enhance the output due to the complex annotations created. The
dataset, called MTTN, includes over 2.4 million sentences divided into 5
stages, resulting in a total of over 12 million pairs, and a vocabulary of over
300,000 unique words, providing ample variation. The original 2.4 million pairs
are designed to reflect the way language is used on the internet globally,
making the dataset more robust for any model trained on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10410">
<div class="article-summary-box-inner">
<span><p>Cross-domain NER is a challenging task to address the low-resource problem in
practical scenarios. Previous typical solutions mainly obtain a NER model by
pre-trained language models (PLMs) with data from a rich-resource domain and
adapt it to the target domain. Owing to the mismatch issue among entity types
in different domains, previous approaches normally tune all parameters of PLMs,
ending up with an entirely new NER model for each domain. Moreover, current
models only focus on leveraging knowledge in one general source domain while
failing to successfully transfer knowledge from multiple sources to the target.
To address these issues, we introduce Collaborative Domain-Prefix Tuning for
cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,
we present text-to-text generation grounding domain-related instructors to
transfer knowledge to new domain NER tasks without structural modifications. We
utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate
the potential of PLMs to handle NER tasks across various domains. Experimental
results on the Cross-NER benchmark show that the proposed approach has flexible
transfer ability and performs better on both one-source and multiple-source
cross-domain NER tasks. Codes will be available in
https://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Reasoning of Entities and Events in Procedural Texts. (arXiv:2301.10896v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10896">
<div class="article-summary-box-inner">
<span><p>Entities and events have long been regarded as the crux of machine reasoning.
Specifically, procedural texts have received increasing attention due to the
dynamic nature of involved entities and events. Existing work has exclusively
focused on entity state tracking (e.g., the temperature of a pan) or
counterfactual event reasoning (e.g., how likely am I to burn myself by
touching the pan), while these two tasks are tightly intertwined. In this work,
we propose CREPE, the first benchmark on causal reasoning about event
plausibility based on entity states. We experiment with strong large language
models and show that most models including GPT3 perform close to chance of .30
F1, lagging far behind the human performance of .87 F1. Inspired by the finding
that structured representations such as programming languages benefits event
reasoning as a prompt to code language models such as Codex, we creatively
inject the causal relations between entities and events through intermediate
variables and boost the performance to .67 to .72 F1. Our proposed event
representation not only allows for knowledge injection, but also marks the
first successful attempt of chain-of-thought reasoning with code language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11004">
<div class="article-summary-box-inner">
<span><p>Interactions among humans on social media often convey intentions behind
their actions, yielding a psychological language resource for Mental Health
Analysis (MHA) of online users. The success of Computational Intelligence
Techniques (CIT) for inferring mental illness from such social media resources
points to NLP as a lens for causal analysis and perception mining. However, we
argue that more consequential and explainable research is required for optimal
impact on clinical psychology practice and personalized mental healthcare. To
bridge this gap, we posit two significant dimensions: (1) Causal analysis to
illustrate a cause and effect relationship in the user generated text; (2)
Perception mining to infer psychological perspectives of social effects on
online users intentions. Within the scope of Natural Language Processing (NLP),
we further explore critical areas of inquiry associated with these two
dimensions, specifically through recent advancements in discourse analysis.
This position paper guides the community to explore solutions in this space and
advance the state of practice in developing conversational agents for inferring
mental health from social media. We advocate for a more explainable approach
toward modeling computational psychology problems through the lens of language
as we observe an increased number of research contributions in dataset and
problem formulation for causal relation extraction and perception enhancements
while inferring mental states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Molecular Language Model as Multi-task Generator. (arXiv:2301.11259v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11259">
<div class="article-summary-box-inner">
<span><p>Molecule generation with desired properties has grown immensely in popularity
by disruptively changing the way scientists design molecular structures and
providing support for chemical and materials design. However, despite the
promising outcome, previous machine learning-based deep generative models
suffer from a reliance on complex, task-specific fine-tuning, limited
dimensional latent spaces, or the quality of expert rules. In this work, we
propose MolGen, a pre-trained molecular language model that effectively learns
and shares knowledge across multiple generation tasks and domains.
Specifically, we pre-train MolGen with the chemical language SELFIES on more
than 100 million unlabelled molecules. We further propose multi-task molecular
prefix tuning across several molecular generation tasks and different molecular
domains (synthetic &amp; natural products) with a self-feedback mechanism.
Extensive experiments show that MolGen can obtain superior performances on
well-known molecular generation benchmark datasets. The further analysis
illustrates that MolGen can accurately capture the distribution of molecules,
implicitly learn their structural characteristics, and efficiently explore the
chemical space with the guidance of multi-task molecular prefix tuning. Codes,
datasets, and the pre-trained model will be available in
https://github.com/zjunlp/MolGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11660">
<div class="article-summary-box-inner">
<span><p>As the size of the pre-trained language model (PLM) continues to increase,
numerous parameter-efficient transfer learning methods have been proposed
recently to compensate for the tremendous cost of fine-tuning. Despite the
impressive results achieved by large pre-trained language models (PLMs) and
various parameter-efficient transfer learning (PETL) methods on sundry
benchmarks, it remains unclear if they can handle inputs that have been
distributionally shifted effectively. In this study, we systematically explore
how the ability to detect out-of-distribution (OOD) changes as the size of the
PLM grows or the transfer methods are altered. Specifically, we evaluated
various PETL techniques, including fine-tuning, Adapter, LoRA, and
prefix-tuning, on three different intention classification tasks, each
utilizing various language models with different scales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11757">
<div class="article-summary-box-inner">
<span><p>The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
synthesis. One area that has yet to be fully explored is the application of
diffusion models to music generation. Music generation requires to handle
multiple aspects, including the temporal dimension, long-term structure,
multiple layers of overlapping sounds, and nuances that only trained listeners
can detect. In our work, we investigate the potential of diffusion models for
text-conditional music generation. We develop a cascading latent diffusion
approach that can generate multiple minutes of high-quality stereo music at
48kHz from textual descriptions. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open-source libraries
with the hope of facilitating future work in the field.
</p>
<p>We open-source the following: Music samples for this paper:
https://bit.ly/anonymous-mousai; all music samples for all models:
https://bit.ly/audio-diffusion; and codes:
https://github.com/archinetai/audio-diffusion-pytorch
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-31 23:12:55.361690133 UTC">2023-01-31 23:12:55 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>