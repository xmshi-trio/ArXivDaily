<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-04T01:30:00Z">10-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training. (arXiv:2310.01418v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01418">
<div class="article-summary-box-inner">
<span><p>Depression is debilitating, and not uncommon. Indeed, studies of excessive
social media users show correlations with depression, ADHD, and other mental
health concerns. Given that there is a large number of people with excessive
social media usage, then there is a significant population of potentially
undiagnosed users and posts that they create. In this paper, we propose a
depression severity detection system using a semi-supervised learning technique
to predict if a post is from a user who is experiencing severe, moderate, or
low (non-diagnostic) levels of depression. Namely, we use a trained model to
classify a large number of unlabelled social media posts from Reddit, then use
these generated labels to train a more powerful classifier. We demonstrate our
framework on Detecting Signs of Depression from Social Media Text -
LT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01420">
<div class="article-summary-box-inner">
<span><p>Conversational tutoring systems (CTSs) offer learning experiences driven by
natural language interaction. They are known to promote high levels of
cognitive engagement and benefit learning outcomes, particularly in reasoning
tasks. Nonetheless, the time and cost required to author CTS content is a major
obstacle to widespread adoption. In this paper, we introduce a novel type of
CTS that leverages the recent advances in large language models (LLMs) in two
ways: First, the system induces a tutoring script automatically from a lesson
text. Second, the system automates the script orchestration via two LLM-based
agents (Ruffle&amp;Riley) with the roles of a student and a professor in a
learning-by-teaching format. The system allows a free-form conversation that
follows the ITS-typical outer-/inner-loop structure. In an initial
between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler
QA chatbots and reading activity, we found no significant differences in
post-test scores. Nonetheless, in the learning experience survey, Ruffle&amp;Riley
users expressed higher ratings of understanding and remembering and further
perceived the offered support as more helpful and the conversation as coherent.
Our study provides insights for a new generation of scalable CTS technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01423">
<div class="article-summary-box-inner">
<span><p>Since ChatGPT has emerged as a major AIGC model, providing high-quality
responses across a wide range of applications (including software development
and maintenance), it has attracted much interest from many individuals. ChatGPT
has great promise, but there are serious problems that might arise from its
misuse, especially in the realms of education and public safety. Several AIGC
detectors are available, and they have all been tested on genuine text.
However, more study is needed to see how effective they are for multi-domain
ChatGPT material. This study aims to fill this need by creating a multi-domain
dataset for testing the state-of-the-art APIs and tools for detecting
artificially generated information used by universities and other research
institutions. A large dataset consisting of articles, abstracts, stories, news,
and product reviews was created for this study. The second step is to use the
newly created dataset to put six tools through their paces. Six different
artificial intelligence (AI) text identification systems, including "GPTkit,"
"GPTZero," "Originality," "Sapling," "Writer," and "Zylalab," have accuracy
rates between 55.29 and 97.0%. Although all the tools fared well in the
evaluations, originality was particularly effective across the board.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01424">
<div class="article-summary-box-inner">
<span><p>Rapid advancements in language models (LMs) have led to their adoption across
many sectors. Alongside the potential benefits, such models present a range of
risks, including around privacy. In particular, as LMs have grown in size, the
potential to memorise aspects of their training data has increased, resulting
in the risk of leaking private information. As LMs become increasingly
widespread, it is vital that we understand such privacy risks and how they
might be mitigated. To help researchers and policymakers understand the state
of knowledge around privacy attacks and mitigations, including where more work
is needed, we present the first technical survey on LM privacy. We (i) identify
a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey
existing attacks and use our taxonomy of dimensions to highlight key trends,
(iii) discuss existing mitigation strategies, highlighting their strengths and
limitations, identifying key gaps and demonstrating open problems and areas for
concern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Borges and AI. (arXiv:2310.01425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01425">
<div class="article-summary-box-inner">
<span><p>Many believe that Large Language Models (LLMs) open the era of Artificial
Intelligence (AI). Some see opportunities while others see dangers. Yet both
proponents and opponents grasp AI through the imagery popularised by science
fiction. Will the machine become sentient and rebel against its creators? Will
we experience a paperclip apocalypse? Before answering such questions, we
should first ask whether this mental imagery provides a good description of the
phenomenon at hand. Understanding weather patterns through the moods of the
gods only goes so far. The present paper instead advocates understanding LLMs
and their connection to AI through the imagery of Jorge Luis Borges, a master
of 20th century literature, forerunner of magical realism, and precursor to
postmodern literature. This exercise leads to a new perspective that
illuminates the relation between language modelling and artificial
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Sorting Combats Recency Bias In Long Context Language Models. (arXiv:2310.01427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01427">
<div class="article-summary-box-inner">
<span><p>Current language models often fail to incorporate long contexts efficiently
during generation. We show that a major contributor to this issue are attention
priors that are likely learned during pre-training: relevant information
located earlier in context is attended to less on average. Yet even when models
fail to use the information from a relevant document in their response, they
still pay preferential attention to that document compared to an irrelevant
document at the same position. We leverage this fact to introduce ``attention
sorting'': perform one step of decoding, sort documents by the attention they
receive (highest attention going last), repeat the process, generate the answer
with the newly sorted context. We find that attention sorting improves
performance of long context models. Our findings highlight some challenges in
using off-the-shelf language models for retrieval augmented generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chatmap : Large Language Model Interaction with Cartographic Data. (arXiv:2310.01429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01429">
<div class="article-summary-box-inner">
<span><p>The swift advancement and widespread availability of foundational Large
Language Models (LLMs), complemented by robust fine-tuning methodologies, have
catalyzed their adaptation for innovative and industrious applications.
Enabling LLMs to recognize and interpret geospatial data, while offering a
linguistic access to vast cartographic datasets, is of significant importance.
OpenStreetMap (OSM) is the most ambitious open-source global initiative
offering detailed urban and rural geographic data, curated by a community of
over 10 million contributors, which constitutes a great potential for LLM
applications. In this study, we demonstrate the proof of concept and details of
the process of fine-tuning a relatively small scale (1B parameters) LLM with a
relatively small artificial dataset curated by a more capable teacher model, in
order to provide a linguistic interface to the OSM data of an arbitrary urban
region. Through this interface, users can inquire about a location's
attributes, covering a wide spectrum of concepts, such as its touristic appeal
or the potential profitability of various businesses in that vicinity. The
study aims to provide an initial guideline for such generative artificial
intelligence (AI) adaptations and demonstrate early signs of useful emerging
abilities in this context even in minimal computational settings. The
embeddings of artificially curated prompts including OSM data are also
investigated in detail, which might be instrumental for potential geospatially
aware urban Retrieval Augmented Generation (RAG) applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection. (arXiv:2310.01430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01430">
<div class="article-summary-box-inner">
<span><p>The introduction of the MUStARD dataset, and its emotion recognition
extension MUStARD++, have identified sarcasm to be a multi-modal phenomenon --
expressed not only in natural language text, but also through manners of speech
(like tonality and intonation) and visual cues (facial expression). With this
work, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by
considering state-of-the-art language, speech, and visual encoders, for fully
utilizing the totality of the multi-modal richness that it has to offer,
achieving a 2\% improvement in macro-F1 over the existing benchmark.
Additionally, to cure the imbalance in the `sarcasm type' category in
MUStARD++, we propose an extension, which we call \emph{MUStARD++ Balanced},
benchmarking the same with instances from the extension split across both train
and test sets, achieving a further 2.4\% macro-F1 boost. The new clips were
taken from a novel source -- the TV show, House MD, which adds to the diversity
of the dataset, and were manually annotated by multiple annotators with
substantial inter-annotator agreement in terms of Cohen's kappa and
Krippendorf's alpha. Our code, extended data, and SOTA benchmark models are
made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01432">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown promise as automated evaluators for
assessing the quality of answers generated by AI systems. However, these
LLM-based evaluators exhibit position bias, or inconsistency, when used to
evaluate candidate answers in pairwise comparisons, favoring either the first
or second answer regardless of content. To address this limitation, we propose
PORTIA, an alignment-based system designed to mimic human comparison strategies
to calibrate position bias in a lightweight yet effective manner. Specifically,
PORTIA splits the answers into multiple segments, aligns similar content across
candidate answers, and then merges them back into a single prompt for
evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to
evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances
the consistency rates for all the models and comparison forms tested, achieving
an average relative improvement of 47.46%. Remarkably, PORTIA enables less
advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4
model at just 10% of the cost. Furthermore, it rectifies around 80% of the
position bias instances within the GPT-4 model, elevating its consistency rate
up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced
GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with
human evaluators. These findings highlight PORTIA's ability to correct position
bias, improve LLM consistency, and boost performance while keeping
cost-efficiency. This represents a valuable step toward a more reliable and
scalable use of LLMs for automated evaluations across diverse applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. (arXiv:2310.01434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01434">
<div class="article-summary-box-inner">
<span><p>The field of Artificial Intelligence has witnessed remarkable progress in
recent years, especially with the emergence of powerful large language models
(LLMs) based on the transformer architecture. Cloud-based LLMs, such as
OpenAI's ChatGPT, offer impressive capabilities but come with concerns
regarding latency and privacy due to network dependencies. This article
presents an innovative approach to LLM inference, envisioning a future where
LLMs with billions of parameters can be executed directly on mobile devices
without network connectivity. The article showcases a fine-tuned GPT LLM with 3
billion parameters that can operate smoothly on devices with as low as 4GB of
memory. Through the integration of native code and model quantization
techniques, the application not only serves as a general-purpose assistant but
also facilitates seamless mobile interactions with text-to-actions features.
The article provides insights into the training pipeline, implementation
details, test results, and future directions of on-device LLM inference. This
breakthrough technology opens up possibilities for empowering users with
sophisticated AI capabilities while preserving their privacy and eliminating
latency concerns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Many Voices of Duying: Revisiting the Disputed Essays Between Lu Xun and Zhou Zuoren. (arXiv:2310.01440v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01440">
<div class="article-summary-box-inner">
<span><p>Lu Xun and Zhou Zuoren stand as two of the most influential writers in modern
Chinese literature. Beyond their familial ties as brothers, they were also
intimate collaborators during the nascent stages of their writing careers. This
research employs quantitative methods to revisit three disputed essays
pseudonymously published by the brothers in 1912. Our stylometric analysis uses
an interpretable authorship attribution model to investigate the essays'
authorship and examine the brothers' respective writing styles. Our findings
suggest that 'Looking at the Country of China' was authored by Lu Xun.
Moreover, 'People of Yue, Forget Not Your Ancestors' Instructions' seems to be
either predominantly authored or extensively revised by Lu Xun given its
notable stylistic similarities to 'Looking at the Land of Yue,' a piece Zhou
Zuoren recognized as his own, but edited by Lu Xun. The third essay, 'Where Has
the Character of the Republic Gone?,' exhibits a 'diluted', mixed writing
style, suggesting thorough collaboration. We offer visual representations of
essay features to facilitate a nuanced and intuitive understanding. We have
uncovered evidence suggesting Lu Xun's covert engagement with social issues
during his purported 'silent era' and provided insights into the brothers'
formative intellectual trajectories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01441">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated impressive inferential
capabilities, with numerous research endeavors devoted to enhancing this
capacity through prompting. Despite these efforts, a unified epistemological
foundation is still conspicuously absent. Drawing inspiration from Kant's a
priori philosophy, we propose the UPAR prompting framework, designed to emulate
the structure of human cognition within LLMs. The UPAR framework is delineated
into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the
extraction of structured information from complex contexts, prior planning of
solutions, execution according to plan, and self-reflection. This structure
significantly augments the explainability and accuracy of LLM inference,
producing a human-understandable and inspectable inferential trajectory.
Furthermore, our work offers an epistemological foundation for existing
prompting techniques, allowing for a possible systematic integration of these
methods. With GPT-4, our approach elevates the accuracy from COT baseline of
22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in
the causal judgment task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01444">
<div class="article-summary-box-inner">
<span><p>Recent advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Recent
advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Through
iterative exploration and PPO training, LTC empowers the agent to assimilate
short-term experiences into long-term memory. To optimize agent interactions
for task-specific learning, we introduce three structured communication
patterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as
decision-making, knowledge-intensive reasoning, and numerical reasoning. We
evaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA
(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,
it exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,
LTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it
outperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,
LTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results
showcase the versatility and efficiency of the LTC approach across diverse
domains. We will open-source our code to promote further development of the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning. (arXiv:2310.01446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01446">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are showcasing impressive ability in handling
complex reasoning tasks. In real-world situations, problems often span a
spectrum of complexities. Humans inherently adjust their problem-solving
approaches based on task complexity. However, most methodologies that leverage
LLMs tend to adopt a uniform approach: utilizing consistent models, prompting
methods, and degrees of problem decomposition, regardless of the problem
complexity. Inflexibility of them can bring unnecessary computational overhead
or sub-optimal performance. To address this problem, we introduce an
Adaptive-Solver framework. It strategically modulates solving strategies based
on the difficulties of the problems. Given an initial solution, the framework
functions with two primary modules. The initial evaluation module assesses the
adequacy of the current solution. If improvements are needed, the subsequent
adaptation module comes into play. Within this module, three key adaptation
strategies are employed: (1) Model Adaptation: Switching to a stronger LLM when
a weaker variant is inadequate. (2) Prompting Method Adaptation: Alternating
between different prompting techniques to suit the problem's nuances. (3)
Decomposition Granularity Adaptation: Breaking down a complex problem into more
fine-grained sub-questions to enhance solvability. Through such dynamic
adaptations, our framework not only enhances computational efficiency but also
elevates the overall performance. This dual-benefit ensures both the efficiency
of the system for simpler tasks and the precision required for more complex
questions. Experimental results from complex reasoning tasks reveal that the
prompting method adaptation and decomposition granularity adaptation enhance
performance across all tasks. Furthermore, the model adaptation approach
significantly reduces API costs (up to 50%) while maintaining superior
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01448">
<div class="article-summary-box-inner">
<span><p>Do large language models (LLMs) genuinely understand the semantics of the
language, or just memorize the training data? The recent concern on potential
data contamination of LLMs has raised awareness of the community to conduct
research on LLMs evaluation. In this paper, we propose MSTemp, an approach that
creates meta semantic templates to evaluate the semantic understanding ability
of LLMs. The core of MSTemp is not to perform evaluation directly on existing
benchmark datasets, but to generate new out-of-distribution (OOD) evaluation
sets using existing datasets as seeds. Specifically, for a given sentence,
MSTemp leverages another language model to generate new samples while
preserving its semantics. The new samples are called semantic templates to the
original sentence. Then, MSTemp generates evaluation samples via sentence
parsing and random word replacement on the semantic templates. MSTemp is highly
flexible, dynamic, and cost-effective. Our initial experiments show that
MSTemp-generated samples can significantly reduce the performance of LLMs using
existing datasets as seeds. We hope this initial work can shed light on future
research of LLMs evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01452">
<div class="article-summary-box-inner">
<span><p>Despite outstanding performance in a variety of NLP tasks, recent studies
have revealed that NLP models are vulnerable to adversarial attacks that
slightly perturb the input to cause the models to misbehave. Among these
attacks, adversarial word-level perturbations are well-studied and effective
attack strategies. Since these attacks work in black-box settings, they do not
require access to the model architecture or model parameters and thus can be
detrimental to existing NLP applications. To perform an attack, the adversary
queries the victim model many times to determine the most important words in an
input text and to replace these words with their corresponding synonyms. In
this work, we propose a lightweight and attack-agnostic defense whose main goal
is to perplex the process of generating an adversarial example in these
query-based black-box attacks; that is to fool the textual fooler. This
defense, named AdvFooler, works by randomizing the latent representation of the
input at inference time. Different from existing defenses, AdvFooler does not
necessitate additional computational overhead during training nor relies on
assumptions about the potential adversarial perturbation set while having a
negligible impact on the model's accuracy. Our theoretical and empirical
analyses highlight the significance of robustness resulting from confusing the
adversary via randomizing the latent space, as well as the impact of
randomization on clean accuracy. Finally, we empirically demonstrate near
state-of-the-art robustness of AdvFooler against representative adversarial
word-level attacks on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NarrativePlay: Interactive Narrative Understanding. (arXiv:2310.01459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01459">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce NarrativePlay, a novel system that allows users
to role-play a fictional character and interact with other characters in
narratives such as novels in an immersive environment. We leverage Large
Language Models (LLMs) to generate human-like responses, guided by personality
traits extracted from narratives. The system incorporates auto-generated visual
display of narrative settings, character portraits, and character speech,
greatly enhancing user experience. Our approach eschews predefined sandboxes,
focusing instead on main storyline events extracted from narratives from the
perspective of a user-selected character. NarrativePlay has been evaluated on
two types of narratives, detective and adventure stories, where users can
either explore the world or improve their favorability with the narrative
characters through conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models. (arXiv:2310.01467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01467">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLM) have revolutionized the NLP landscape,
achieving stellar performances across diverse tasks. These models, while
benefiting from vast training data, often require fine-tuning on specific data
to cater to distinct downstream tasks. However, this data adaptation process
has inherent security and privacy concerns, primarily when leveraging
user-generated, device-residing data. Federated learning (FL) provides a
solution, allowing collaborative model fine-tuning without centralized data
collection. However, applying FL to finetune PLMs is hampered by challenges,
including restricted model parameter access, high computational requirements,
and communication overheads. This paper introduces Federated Black-box Prompt
Tuning (FedBPT), a framework designed to address these challenges. FedBPT does
not require the clients to access the model parameters. By focusing on training
optimal prompts and utilizing gradient-free optimization methods, FedBPT
reduces the number of exchanged variables, boosts communication efficiency, and
minimizes computational and storage costs. Experiments highlight the
framework's ability to drastically cut communication and memory costs while
maintaining competitive performance. Ultimately, FedBPT presents a promising
solution for efficient, privacy-preserving fine-tuning of PLM in the age of
large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01468">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are currently effective at answering questions
that are clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This \textit{entity-deducing game} can serve as an
evaluation framework to probe the conversational reasoning and planning
capabilities of language models. We systematically evaluate various LLMs and
discover significant differences in their performance on this task. We find
that strong LLMs like GPT-4 outperform human players by a large margin. We
further employ Behavior Cloning (BC) to examine whether a weaker model is
capable of imitating a stronger model and generalizing to data or domains,
using only the demonstrations from a stronger model. We finally propose to use
Reinforcement Learning to enhance reasoning and planning capacity of Vicuna
models through episodes of game playing, which lead to significant performance
improvement. We hope that this problem offers insights into how autonomous
agents could be trained to behave more intelligently in ambiguous
circumstances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01469">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be
knowledgeable and able to adapt to many tasks. However, we still can not
completely trust their answer, since LLMs suffer from
hallucination--fabricating non-existent facts to cheat users without
perception. And the reasons for their existence and pervasiveness remain
unclear. In this paper, we demonstrate that non-sense prompts composed of
random tokens can also elicit the LLMs to respond with hallucinations. This
phenomenon forces us to revisit that hallucination may be another view of
adversarial examples, and it shares similar features with conventional
adversarial examples as the basic feature of LLMs. Therefore, we formalize an
automatic hallucination triggering method as the hallucination attack in an
adversarial way. Finally, we explore basic feature of attacked adversarial
prompts and propose a simple yet effective defense strategy. Our code is
released on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01558">
<div class="article-summary-box-inner">
<span><p>Retrieval-augmented language models (RALMs) hold promise to produce language
understanding systems that are are factual, efficient, and up-to-date. An
important desideratum of RALMs, is that retrieved information helps model
performance when it is relevant, and does not harm performance when it is not.
This is particularly important in multi-hop reasoning scenarios, where misuse
of irrelevant evidence can lead to cascading errors. However, recent work has
shown that retrieval augmentation can sometimes have a negative effect on
performance. In this work, we present a thorough analysis on five open-domain
question answering benchmarks, characterizing cases when retrieval reduces
accuracy. We then propose two methods to mitigate this issue. First, a simple
baseline that filters out retrieved passages that do not entail question-answer
pairs according to a natural language inference (NLI) model. This is effective
in preventing performance reduction, but at a cost of also discarding relevant
passages. Thus, we propose a method for automatically generating data to
fine-tune the language model to properly leverage retrieved passages, using a
mix of relevant and irrelevant contexts at training time. We empirically show
that even 1,000 examples suffice to train the model to be robust to irrelevant
contexts while maintaining high performance on examples with relevant ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Against Authorship Identification Attacks. (arXiv:2310.01568v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01568">
<div class="article-summary-box-inner">
<span><p>Authorship identification has proven unsettlingly effective in inferring the
identity of the author of an unsigned document, even when sensitive personal
information has been carefully omitted. In the digital era, individuals leave a
lasting digital footprint through their written content, whether it is posted
on social media, stored on their employer's computers, or located elsewhere.
When individuals need to communicate publicly yet wish to remain anonymous,
there is little available to protect them from unwanted authorship
identification. This unprecedented threat to privacy is evident in scenarios
such as whistle-blowing. Proposed defenses against authorship identification
attacks primarily aim to obfuscate one's writing style, thereby making it
unlinkable to their pre-existing writing, while concurrently preserving the
original meaning and grammatical integrity. The presented work offers a
comprehensive review of the advancements in this research area spanning over
the past two decades and beyond. It emphasizes the methodological frameworks of
modification and generation-based strategies devised to evade authorship
identification attacks, highlighting joint efforts from the differential
privacy community. Limitations of current research are discussed, with a
spotlight on open challenges and potential research avenues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education. (arXiv:2310.01603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01603">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) plays a significant role in our daily lives
and has become an essential part of Artificial Intelligence (AI) education in
K-12. As children grow up with NLP-powered applications, it is crucial to
introduce NLP concepts to them, fostering their understanding of language
processing, language generation, and ethical implications of AI and NLP. This
paper presents a comprehensive review of digital learning environments for
teaching NLP in K-12. Specifically, it explores existing digital learning
tools, discusses how they support specific NLP tasks and procedures, and
investigates their explainability and evaluation results in educational
contexts. By examining the strengths and limitations of these tools, this
literature review sheds light on the current state of NLP learning tools in
K-12 education. It aims to guide future research efforts to refine existing
tools, develop new ones, and explore more effective and inclusive strategies
for integrating NLP into K-12 educational contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAL: Interactive Task Learning with GPT Dialog Parsing. (arXiv:2310.01627v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01627">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning often requires millions of examples to produce static,
black-box models. In contrast, interactive task learning (ITL) emphasizes
incremental knowledge acquisition from limited instruction provided by humans
in modalities such as natural language. However, in practice, ITL systems often
suffers from brittle, error-prone language parsing. Large language models
(LLMs) are resistant to brittleness but are not interpretable and cannot learn
incrementally. We present VAL, an ITL system with a new philosophy for
LLM/symbolic integration. By using LLMs only for specific tasks -- such as
predicate and argument selection -- within an algorithmic framework, VAL reaps
the benefits of LLMs to support interactive learning of hierarchical task
knowledge from natural language. Acquired knowledge is human interpretable and
generalizes to support execution of novel tasks without additional training. We
studied users' interactions with VAL in a video game setting, finding that most
users could successfully teach VAL using language they felt was natural.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition. (arXiv:2310.01688v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01688">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel framework for joint speaker diarization (SD) and
automatic speech recognition (ASR), named SLIDAR (sliding-window
diarization-augmented recognition). SLIDAR can process arbitrary length inputs
and can handle any number of speakers, effectively solving ``who spoke what,
when'' concurrently. SLIDAR leverages a sliding window approach and consists of
an end-to-end diarization-augmented speech transcription (E2E DAST) model which
provides, locally, for each window: transcripts, diarization and speaker
embeddings. The E2E DAST model is based on an encoder-decoder architecture and
leverages recent techniques such as serialized output training and
``Whisper-style" prompting. The local outputs are then combined to get the
final SD+ASR result by clustering the speaker embeddings to get global speaker
identities. Experiments performed on monaural recordings from the AMI corpus
confirm the effectiveness of the method in both close-talk and far-field speech
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01691">
<div class="article-summary-box-inner">
<span><p>Prompt tuning in natural language processing (NLP) has become an increasingly
popular method for adapting large language models to specific tasks. However,
the transferability of these prompts, especially continuous prompts, between
different models remains a challenge. In this work, we propose a zero-shot
continuous prompt transfer method, where source prompts are encoded into
relative space and the corresponding target prompts are searched for
transferring to target models. Experimental results confirm the effectiveness
of our method, showing that 'task semantics' in continuous prompts can be
generalized across various language models. Moreover, we find that combining
'task semantics' from multiple source models can further enhance the
generalizability of transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closing the Curious Case of Neural Text Degeneration. (arXiv:2310.01693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01693">
<div class="article-summary-box-inner">
<span><p>Despite their ubiquity in language generation, it remains unknown why
truncation sampling heuristics like nucleus sampling are so effective. We
provide a theoretical explanation for the effectiveness of the truncation
sampling by proving that truncation methods that discard tokens below some
probability threshold (the most common type of truncation) can guarantee that
all sampled tokens have nonzero true probability. However, thresholds are a
coarse heuristic, and necessarily discard some tokens with nonzero true
probability as well. In pursuit of a more precise sampling strategy, we show
that we can leverage a known source of model errors, the softmax bottleneck, to
prove that certain tokens have nonzero true probability, without relying on a
threshold. Based on our findings, we develop an experimental truncation
strategy and the present pilot studies demonstrating the promise of this type
of algorithm. Our evaluations show that our method outperforms its
threshold-based counterparts under automatic and human evaluation metrics for
low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical
findings and pilot experiments provide both insight into why truncation
sampling works, and make progress toward more expressive sampling algorithms
that better surface the generative capabilities of large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making. (arXiv:2310.01708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01708">
<div class="article-summary-box-inner">
<span><p>Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and
patient data to offer real-time recommendations, with Large Language Models
(LLMs) emerging as a promising tool to generate plain-text explanations for
medical decisions. This study explores the effectiveness and reliability of
LLMs in generating explanations for diagnoses based on patient complaints.
Three experienced doctors evaluated LLM-generated explanations of the
connection between patient complaints and doctor and model-assigned diagnoses
across several stages. Experimental results demonstrated that LLM explanations
significantly increased doctors' agreement rates with given diagnoses and
highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study
underscores the potential and challenges of LLMs in healthcare and emphasizes
the need for careful integration and evaluation to ensure patient safety and
optimal clinical utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01717">
<div class="article-summary-box-inner">
<span><p>We investigate the unsupervised constituency parsing task, which organizes
words and phrases of a sentence into a hierarchical structure without using
linguistically annotated data. We observe that existing unsupervised parsers
capture differing aspects of parsing structures, which can be leveraged to
enhance unsupervised parsing performance. To this end, we propose a notion of
"tree averaging," based on which we further propose a novel ensemble method for
unsupervised parsing. To improve inference efficiency, we further distill the
ensemble knowledge into a student model; such an ensemble-then-distill process
is an effective approach to mitigate the over-smoothing problem existing in
common multi-teacher distilling methods. Experiments show that our method
surpasses all previous approaches, consistently demonstrating its effectiveness
and robustness across various runs, with different ensemble components, and
under domain-shift conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nugget: Neural Agglomerative Embeddings of Text. (arXiv:2310.01732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01732">
<div class="article-summary-box-inner">
<span><p>Embedding text sequences is a widespread requirement in modern language
understanding. Existing approaches focus largely on constant-size
representations. This is problematic, as the amount of information contained in
text often varies with the length of the input. We propose a solution called
Nugget, which encodes language into a representation based on a dynamically
selected subset of input tokens. These nuggets are learned through tasks like
autoencoding and machine translation, and intuitively segment language into
meaningful units. We demonstrate Nugget outperforms related approaches in tasks
involving semantic comparison. Finally, we illustrate these compact units allow
for expanding the contextual window of a language model (LM), suggesting new
future LMs that can condition on significantly larger amounts of content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01749">
<div class="article-summary-box-inner">
<span><p>Attention, specifically scaled dot-product attention, has proven effective
for natural language, but it does not have a mechanism for handling
hierarchical patterns of arbitrary nesting depth, which limits its ability to
recognize certain syntactic structures. To address this shortcoming, we propose
stack attention: an attention operator that incorporates stacks, inspired by
their theoretical connections to context-free languages (CFLs). We show that
stack attention is analogous to standard attention, but with a latent model of
syntax that requires no syntactic supervision. We propose two variants: one
related to deterministic pushdown automata (PDAs) and one based on
nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.
We show that transformers with stack attention are very effective at learning
CFLs that standard transformers struggle on, achieving strong results on a CFL
with theoretically maximal parsing difficulty. We also show that stack
attention is more effective at natural language modeling under a constrained
parameter budget, and we include results on machine translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01777">
<div class="article-summary-box-inner">
<span><p>The transformer architecture has made breakthroughs in recent years on tasks
which require modeling pairwise relationships between sequential elements, as
is the case in natural language understanding. However, transformers struggle
with long sequences due to the quadratic complexity of the attention operation,
and previous research has aimed to lower the complexity by sparsifying or
linearly approximating the attention matrix. Yet, these approaches cannot
straightforwardly distill knowledge from a teacher's attention matrix, and
often require complete retraining from scratch. Furthermore, previous sparse
and linear approaches may also lose interpretability if they do not produce
full quadratic attention matrices. To address these challenges, we propose SEA:
Sparse linear attention with an Estimated Attention mask. SEA estimates the
attention matrix with linear complexity via kernel-based linear attention, then
creates a sparse approximation to the full attention matrix with a top-k
selection to perform a sparse attention operation. For language modeling tasks
(Wikitext2), previous linear and sparse attention methods show a roughly
two-fold worse perplexity scores over the quadratic OPT-125M baseline, while
SEA achieves an even better perplexity than OPT-125M, using roughly half as
much memory as OPT-125M. Moreover, SEA maintains an interpretable attention
matrix and can utilize knowledge distillation to lower the complexity of
existing pretrained transformers. We believe that our work will have a large
practical impact, as it opens the possibility of running large transformers on
resource-limited devices with less memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can large language models provide useful feedback on research papers? A large-scale empirical analysis. (arXiv:2310.01783v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01783">
<div class="article-summary-box-inner">
<span><p>Expert feedback lays the foundation of rigorous research. However, the rapid
growth of scholarly production and intricate knowledge specialization challenge
the conventional scientific feedback mechanisms. High-quality peer reviews are
increasingly difficult to obtain. Researchers who are more junior or from
under-resourced settings have especially hard times getting timely feedback.
With the breakthrough of large language models (LLM) such as GPT-4, there is
growing interest in using LLMs to generate scientific feedback on research
manuscripts. However, the utility of LLM-generated feedback has not been
systematically studied. To address this gap, we created an automated pipeline
using GPT-4 to provide comments on the full PDFs of scientific papers. We
evaluated the quality of GPT-4's feedback through two large-scale studies. We
first quantitatively compared GPT-4's generated feedback with human peer
reviewer feedback in 15 Nature family journals (3,096 papers in total) and the
ICLR machine learning conference (1,709 papers). The overlap in the points
raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature
journals, 39.23% for ICLR) is comparable to the overlap between two human
reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The
overlap between GPT-4 and human reviewers is larger for the weaker papers. We
then conducted a prospective user study with 308 researchers from 110 US
institutions in the field of AI and computational biology to understand how
researchers perceive feedback generated by our GPT-4 system on their own
papers. Overall, more than half (57.4%) of the users found GPT-4 generated
feedback helpful/very helpful and 82.4% found it more beneficial than feedback
from at least some human reviewers. While our findings show that LLM-generated
feedback can help researchers, we also identify several limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01798">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance might even
degrade post self-correction. Drawing from these insights, we offer suggestions
for future research and practical applications in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01801">
<div class="article-summary-box-inner">
<span><p>In this study, we introduce adaptive KV cache compression, a plug-and-play
method that reduces the memory footprint of generative inference for Large
Language Models (LLMs). Different from the conventional KV cache that retains
key and value vectors for all context tokens, we conduct targeted profiling to
discern the intrinsic structure of attention modules. Based on the recognized
structure, we then construct the KV cache in an adaptive manner: evicting
long-range contexts on attention heads emphasizing local contexts, discarding
non-special tokens on attention heads centered on special tokens, and only
employing the standard KV cache for attention heads that broadly attend to all
tokens. Moreover, with the lightweight attention profiling used to guide the
construction of the adaptive KV cache, FastGen can be deployed without
resource-intensive fine-tuning or re-training. In our experiments across
various asks, FastGen demonstrates substantial reduction on GPU memory
consumption with negligible generation quality loss. We will release our code
and the compatible CUDA kernel for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01825">
<div class="article-summary-box-inner">
<span><p>Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced
significant growth and have been extensively employed to adapt large vision and
language models to various domains, enabling satisfactory model performance
with minimal computational needs. Despite these advances, more research has yet
to delve into potential PEFT applications in real-life scenarios, particularly
in the critical domains of remote sensing and crop monitoring. The diversity of
climates across different regions and the need for comprehensive large-scale
datasets have posed significant obstacles to accurately identify crop types
across varying geographic locations and changing growing seasons. This study
seeks to bridge this gap by comprehensively exploring the feasibility of
cross-area and cross-year out-of-distribution generalization using the
State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to
explore PEFT approaches for crop monitoring. Specifically, we focus on adapting
the SOTA TSViT model to address winter wheat field segmentation, a critical
task for crop monitoring and food security. This adaptation process involves
integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and
prompt tuning. Using PEFT techniques, we achieved notable results comparable to
those achieved using full fine-tuning methods while training only a mere 0.7%
parameters of the whole TSViT architecture. The in-house labeled data-set,
referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated
polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over
five consecutive years. Using Sentinel-2 images, our model achieved a 84%
F1-score. We intend to publicly release the Lebanese winter wheat data set,
code repository, and model weights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01828">
<div class="article-summary-box-inner">
<span><p>eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01837">
<div class="article-summary-box-inner">
<span><p>Current AI-based methods do not provide comprehensible physical
interpretations of the utilized data, extracted features, and
predictions/inference operations. As a result, deep learning models trained
using high-resolution satellite imagery lack transparency and explainability
and can be merely seen as a black box, which limits their wide-level adoption.
Experts need help understanding the complex behavior of AI models and the
underlying decision-making process. The explainable artificial intelligence
(XAI) field is an emerging field providing means for robust, practical, and
trustworthy deployment of AI models. Several XAI techniques have been proposed
for image classification tasks, whereas the interpretation of image
segmentation remains largely unexplored. This paper offers to bridge this gap
by adapting the recent XAI classification algorithms and making them usable for
muti-class image segmentation, where we mainly focus on buildings' segmentation
from high-resolution satellite images. To benchmark and compare the performance
of the proposed approaches, we introduce a new XAI evaluation methodology and
metric based on "Entropy" to measure the model uncertainty. Conventional XAI
evaluation methods rely mainly on feeding area-of-interest regions from the
image back to the pre-trained (utility) model and then calculating the average
change in the probability of the target class. Those evaluation metrics lack
the needed robustness, and we show that using Entropy to monitor the model
uncertainty in segmenting the pixels within the target class is more suitable.
We hope this work will pave the way for additional XAI research for image
segmentation and applications in the remote sensing discipline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment. (arXiv:2310.01839v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01839">
<div class="article-summary-box-inner">
<span><p>Automatic pronunciation assessment (APA) manages to quantify the
pronunciation proficiency of a second language (L2) learner in a language.
Prevailing approaches to APA normally leverage neural models trained with a
regression loss function, such as the mean-squared error (MSE) loss, for
proficiency level prediction. Despite most regression models can effectively
capture the ordinality of proficiency levels in the feature space, they are
confronted with a primary obstacle that different phoneme categories with the
same proficiency level are inevitably forced to be close to each other,
retaining less phoneme-discriminative information. On account of this, we
devise a phonemic contrast ordinal (PCO) loss for training regression-based APA
models, which aims to preserve better phonemic distinctions between phoneme
categories meanwhile considering ordinal relationships of the regression target
output. Specifically, we introduce a phoneme-distinct regularizer into the MSE
loss, which encourages feature representations of different phoneme categories
to be far apart while simultaneously pulling closer the representations
belonging to the same phoneme category by means of weighted distances. An
extensive set of experiments carried out on the speechocean762 benchmark
dataset suggest the feasibility and effectiveness of our model in relation to
some existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01845">
<div class="article-summary-box-inner">
<span><p>Foundation models have excelled in various tasks but are often evaluated on
general benchmarks. The adaptation of these models for specific domains, such
as remote sensing imagery, remains an underexplored area. In remote sensing,
precise building instance segmentation is vital for applications like urban
planning. While Convolutional Neural Networks (CNNs) perform well, their
generalization can be limited. For this aim, we present a novel approach to
adapt foundation models to address existing models' generalization dropback.
Among several models, our focus centers on the Segment Anything Model (SAM), a
potent foundation model renowned for its prowess in class-agnostic image
segmentation capabilities. We start by identifying the limitations of SAM,
revealing its suboptimal performance when applied to remote sensing imagery.
Moreover, SAM does not offer recognition abilities and thus fails to classify
and tag localized objects. To address these limitations, we introduce different
prompting strategies, including integrating a pre-trained CNN as a prompt
generator. This novel approach augments SAM with recognition abilities, a first
of its kind. We evaluated our method on three remote sensing datasets,
including the WHU Buildings dataset, the Massachusetts Buildings dataset, and
the AICrowd Mapping Challenge. For out-of-distribution performance on the WHU
dataset, we achieve a 5.47% increase in IoU and a 4.81% improvement in
F1-score. For in-distribution performance on the WHU dataset, we observe a
2.72% and 1.58% increase in True-Positive-IoU and True-Positive-F1 score,
respectively. We intend to release our code repository, hoping to inspire
further exploration of foundation models for domain-specific tasks within the
remote sensing community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking and Improving Generator-Validator Consistency of Language Models. (arXiv:2310.01846v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01846">
<div class="article-summary-box-inner">
<span><p>As of September 2023, ChatGPT correctly answers "what is 7+8" with 15, but
when asked "7+8=15, True or False" it responds with "False". This inconsistency
between generating and validating an answer is prevalent in language models
(LMs) and erodes trust. In this paper, we propose a framework for measuring the
consistency between generation and validation (which we call
generator-validator consistency, or GV-consistency), finding that even GPT-4, a
state-of-the-art LM, is GV-consistent only 76% of the time. To improve the
consistency of LMs, we propose to finetune on the filtered generator and
validator responses that are GV-consistent, and call this approach consistency
fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B
from 60% to 93%, and the improvement extrapolates to unseen tasks and domains
(e.g., GV-consistency for positive style transfers extrapolates to unseen
styles like humor). In addition to improving consistency, consistency
fine-tuning improves both generator quality and validator accuracy without
using any labeled data. Evaluated across 6 tasks, including math questions,
knowledge-intensive QA, and instruction following, our method improves the
generator quality by 16% and the validator accuracy by 6.3% across all tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?. (arXiv:2310.01854v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01854">
<div class="article-summary-box-inner">
<span><p>To decipher the algorithm underlying the human brain's language
representation, previous work probed brain responses to language input with
pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks.
However, full fine-tuning generally updates the entire parametric space and
distorts pre-trained features, cognitively inconsistent with the brain's robust
multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained
weights and learns task-specific embeddings to fit a task. Could prompt-tuning
generate representations that better account for the brain's language
representations than fine-tuning? If so, what kind of NLU task leads a
pre-trained model to better decode the information represented in the human
brain? We investigate these questions by comparing prompt-tuned and fine-tuned
representations in neural decoding, that is predicting the linguistic stimulus
from the brain activities evoked by the stimulus. We find that on none of the
10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning in
neural decoding, implicating that a more brain-consistent tuning method yields
representations that better correlate with brain data. Moreover, we identify
that tasks dealing with fine-grained concept meaning yield representations that
better decode brain activation patterns than other tasks, especially the
syntactic chunking task. This indicates that our brain encodes more
fine-grained concept information than shallow syntactic information when
representing languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01886">
<div class="article-summary-box-inner">
<span><p>Many pre-trained large-scale models provided online have become highly
effective in transferring to downstream tasks. At the same time, various
task-specific models fine-tuned on these pre-trained models are available
online for public use. In practice, as collecting task-specific data is
labor-intensive and fine-tuning the large pre-trained models is computationally
expensive, one can reuse task-specific finetuned models to deal with downstream
tasks. However, using a model per task causes a heavy burden on storage and
serving. Recently, many training-free and parameter-efficient methods have been
proposed for reusing multiple fine-tuned task-specific models into a single
multi-task model. However, these methods exhibit a large accuracy gap compared
with using a fine-tuned model per task. In this paper, we propose
Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing
Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task
vector into a merged model by magnitude pruning. For reusing LoRA fine-tuned
models, we propose PERU-LoRA use a lower-rank matrix to approximate the LoRA
matrix by singular value decomposition. Both PERUFFT and PERU-LoRA are
training-free. Extensive experiments conducted on computer vision and natural
language process tasks demonstrate the effectiveness and parameter-efficiency
of the proposed methods. The proposed PERU-FFT and PERU-LoRA outperform
existing reusing model methods by a large margin and achieve comparable
performance to using a fine-tuned model per task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01889">
<div class="article-summary-box-inner">
<span><p>Transformers have emerged as the architecture of choice for many
state-of-the-art AI models, showcasing exceptional performance across a wide
range of AI applications. However, the memory demands imposed by Transformers
limit their ability to handle long sequences, thereby creating challenges for
tasks involving extended sequences or long-term dependencies. We present a
distinct approach, Ring Attention, which leverages blockwise computation of
self-attention to distribute long sequences across multiple devices while
concurrently overlapping the communication of key-value blocks with the
computation of blockwise attention. By processing longer input sequences while
maintaining memory efficiency, Ring Attention enables training and inference of
sequences that are device count times longer than those of prior
memory-efficient Transformers, effectively eliminating the memory constraints
imposed by individual devices. Extensive experiments on language modeling tasks
demonstrate the effectiveness of Ring Attention in allowing large sequence
input size and improving performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Evaluation Framework: Best Practices for Human Evaluation. (arXiv:2310.01917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01917">
<div class="article-summary-box-inner">
<span><p>Human evaluation plays a crucial role in Natural Language Processing (NLP) as
it assesses the quality and relevance of developed systems, thereby
facilitating their enhancement. However, the absence of widely accepted human
evaluation metrics in NLP hampers fair comparisons among different systems and
the establishment of universal assessment standards. Through an extensive
analysis of existing literature on human evaluation metrics, we identified
several gaps in NLP evaluation methodologies. These gaps served as motivation
for developing our own hierarchical evaluation framework. The proposed
framework offers notable advantages, particularly in providing a more
comprehensive representation of the NLP system's performance. We applied this
framework to evaluate the developed Machine Reading Comprehension system, which
was utilized within a human-AI symbiosis model. The results highlighted the
associations between the quality of inputs and outputs, underscoring the
necessity to evaluate both components rather than solely focusing on outputs.
In future work, we will investigate the potential time-saving benefits of our
proposed framework for evaluators assessing NLP systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01929">
<div class="article-summary-box-inner">
<span><p>Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have
recently gained prominence for their remarkable zero-shot capabilities in
generating images guided by textual prompts. Language, as a conduit of culture,
plays a pivotal role in these models' multilingual capabilities, which in turn
shape their cultural agency. In this study, we explore the cultural perception
embedded in TTI models by characterizing culture across three hierarchical
tiers: cultural dimensions, cultural domains, and cultural concepts. We propose
a comprehensive suite of evaluation techniques, including intrinsic evaluations
using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA)
model, and human assessments, to discern TTI cultural perceptions. To
facilitate our research, we introduce the CulText2I dataset, derived from four
diverse TTI models and spanning ten languages. Our experiments reveal insights
into these models' cultural awareness, cultural distinctions, and the unlocking
of cultural features, releasing the potential for cross-cultural applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving. (arXiv:2310.01957v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01957">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown promise in the autonomous driving
sector, particularly in generalization and interpretability. We introduce a
unique object-level multimodal LLM architecture that merges vectorized numeric
modalities with a pre-trained LLM to improve context understanding in driving
situations. We also present a new dataset of 160k QA pairs derived from 10k
driving scenarios, paired with high quality control commands collected with RL
agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct
pretraining strategy is devised to align numeric vector modalities with static
LLM representations using vector captioning language data. We also introduce an
evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency
in interpreting driving scenarios, answering questions, and decision-making.
Our findings highlight the potential of LLM-based driving action generation in
comparison to traditional behavioral cloning. We make our benchmark, datasets,
and model available for further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Knowledge Bases for Visual Word Sense Disambiguation. (arXiv:2310.01960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01960">
<div class="article-summary-box-inner">
<span><p>Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies
between linguistic sense disambiguation and fine-grained multimodal retrieval.
The recent advancements in the development of visiolinguistic (VL) transformers
suggest some off-the-self implementations with encouraging results, which
however we argue that can be further improved. To this end, we propose some
knowledge-enhancement techniques towards improving the retrieval performance of
VL transformers via the usage of Large Language Models (LLMs) as Knowledge
Bases. More specifically, knowledge stored in LLMs is retrieved with the help
of appropriate prompts in a zero-shot manner, achieving performance
advancements. Moreover, we convert VWSD to a purely textual question-answering
(QA) problem by considering generated image captions as multiple-choice
candidate answers. Zero-shot and few-shot prompting strategies are leveraged to
explore the potential of such a transformation, while Chain-of-Thought (CoT)
prompting in the zero-shot setting is able to reveal the internal reasoning
steps an LLM follows to select the appropriate candidate. In total, our
presented approach is the first one to analyze the merits of exploiting
knowledge stored in LLMs in different ways to solve WVSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01991">
<div class="article-summary-box-inner">
<span><p>While forward reasoning (i.e. find the answer given the question) has been
explored extensively in the recent literature, backward reasoning is relatively
unexplored. We examine the backward reasoning capabilities of LLMs on Math Word
Problems (MWPs): given a mathematical question and its answer, with some
details omitted from the question, can LLMs effectively retrieve the missing
information?
</p>
<p>In this paper, we formally define the backward reasoning task on math word
problems and modify three datasets to evaluate this task: GSM8k, SVAMP and
MultiArith. Our findings show a significant drop in the accuracy of models on
backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4,
GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we
propose three novel techniques that improve performance: Rephrase reformulates
the given problem into a forward reasoning problem, PAL-Tools combines the idea
of Program-Aided LLMs to produce a set of equations that can be solved by an
external solver, and Check your Work exploits the availability of natural
verifier of high accuracy in the forward direction, interleaving solving and
verification steps. Finally, realizing that each of our base methods correctly
solves a different set of problems, we propose a novel Bayesian formulation for
creating an ensemble over these base methods aided by a verifier to further
boost the accuracy by a significant margin. Extensive experimentation
demonstrates that our techniques successively improve the performance of LLMs
on the backward reasoning task, with the final ensemble-based method resulting
in a substantial performance gain compared to the raw LLMs with standard
prompting techniques such as chain-of-thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02031">
<div class="article-summary-box-inner">
<span><p>Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jury: A Comprehensive Evaluation Toolkit. (arXiv:2310.02040v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02040">
<div class="article-summary-box-inner">
<span><p>Evaluation plays a critical role in deep learning as a fundamental block of
any prediction-based system. However, the vast number of Natural Language
Processing (NLP) tasks and the development of various metrics have led to
challenges in evaluating different systems with different metrics. To address
these challenges, we introduce jury, a toolkit that provides a unified
evaluation framework with standardized structures for performing evaluation
across different tasks and metrics. The objective of jury is to standardize and
improve metric evaluation for all systems and aid the community in overcoming
the challenges in evaluation. Since its open-source release, jury has reached a
wide audience and is available at https://github.com/obss/jury.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tuning Large language model for End-to-end Speech Translation. (arXiv:2310.02050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02050">
<div class="article-summary-box-inner">
<span><p>With the emergence of large language models (LLMs), multimodal models based
on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,
and SpeechGPT exhibit an impressive ability to comprehend and generate human
instructions. However, their performance often falters when faced with complex
tasks like end-to-end speech translation (E2E-ST), a cross-language and
cross-modal translation task. In comparison to single-modal models, multimodal
models lag behind in these scenarios. This paper introduces LST, a Large
multimodal model designed to excel at the E2E-ST task. LST consists of a speech
frontend, an adapter, and a LLM backend. The training of LST consists of two
stages: (1) Modality adjustment, where the adapter is tuned to align speech
representation with text embedding space, and (2) Downstream task fine-tuning,
where both the adapter and LLM model are trained to optimize performance on the
E2EST task. Experimental results on the MuST-C speech translation benchmark
demonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on
En-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a
new state-of-the-art. Additionally, we conduct an in-depth analysis of
single-modal model selection and the impact of training strategies, which lays
the foundation for future research. We will open up our code and models after
review.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Topic-Focus Articulation in Meaning-to-Text Generation using Graph Neural Networks. (arXiv:2310.02053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02053">
<div class="article-summary-box-inner">
<span><p>A bare meaning representation can be expressed in various ways using natural
language, depending on how the information is structured on the surface level.
We are interested in finding ways to control topic-focus articulation when
generating text from meaning. We focus on distinguishing active and passive
voice for sentences with transitive verbs. The idea is to add pragmatic
information such as topic to the meaning representation, thereby forcing either
active or passive voice when given to a natural language generation system. We
use graph neural models because there is no explicit information about word
order in a meaning represented by a graph. We try three different methods for
topic-focus articulation (TFA) employing graph neural models for a
meaning-to-text generation task. We propose a novel encoding strategy about
node aggregation in graph neural models, which instead of traditional encoding
by aggregating adjacent node information, learns node representations by using
depth-first search. The results show our approach can get competitive
performance with state-of-art graph models on general text generation, and lead
to significant improvements on the task of active-passive conversion compared
to traditional adjacency-based aggregation strategies. Different types of TFA
can have a huge impact on the performance of the graph models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation. (arXiv:2210.00193v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00193">
<div class="article-summary-box-inner">
<span><p>We present FRMT, a new dataset and evaluation benchmark for Few-shot
Region-aware Machine Translation, a type of style-targeted translation. The
dataset consists of professional translations from English into two regional
variants each of Portuguese and Mandarin Chinese. Source documents are selected
to enable detailed analysis of phenomena of interest, including lexically
distinct terms and distractor terms. We explore automatic evaluation metrics
for FRMT and validate their correlation with expert human evaluation across
both region-matched and mismatched rating scenarios. Finally, we present a
number of baseline models for this task, and offer guidelines for how
researchers can train, evaluate, and compare their own models. Our dataset and
evaluation code are publicly available: https://bit.ly/frmt-task
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialoGen: Generalized Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06282">
<div class="article-summary-box-inner">
<span><p>Long-range context modeling is crucial to both dialogue understanding and
generation. The most popular method for dialogue context representation is to
concatenate the last-$k$ utterances in chronological order. However, this
method may not be ideal for conversations containing long-range dependencies,
i.e., when there is a need to look beyond last-$k$ utterances to generate a
meaningful response. In this work, we propose DialoGen, a novel encoder-decoder
based framework for dialogue generation with a generalized context
representation that can look beyond the last-$k$ utterances. The main idea of
our approach is to identify and utilize the most relevant historical utterances
instead of last-$k$, which also enables the compact representation of dialogue
history with fewer tokens. We study the effectiveness of our proposed method on
both dialogue generation (open-domain) and understanding (DST). Even with a
compact context representation, DialoGen performs comparably to the
state-of-the-art models on the open-domain DailyDialog dataset. We observe a
similar behavior on the DST task of the MultiWOZ dataset when the proposed
context representation is applied to existing DST models. We also discuss the
generalizability and interpretability of DialoGen and show that the relevance
score of previous utterances agrees well with human cognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Rewriting for Effective Misinformation Discovery. (arXiv:2210.07467v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07467">
<div class="article-summary-box-inner">
<span><p>We propose a novel system to help fact-checkers formulate search queries for
known misinformation claims and effectively search across multiple social media
platforms. We introduce an adaptable rewriting strategy, where editing actions
for queries containing claims (e.g., swap a word with its synonym; change verb
tense into present simple) are automatically learned through offline
reinforcement learning. Our model uses a decision transformer to learn a
sequence of editing actions that maximizes query retrieval metrics such as mean
average precision. We conduct a series of experiments showing that our query
rewriting system achieves a relative increase in the effectiveness of the
queries of up to 42%, while producing editing action sequences that are human
interpretable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00674">
<div class="article-summary-box-inner">
<span><p>Few-shot learning is valuable in many real-world applications, but learning a
generalizable model without overfitting to the few labeled datapoints is
challenging. In this work, we focus on Few-shot Learning with Auxiliary Data
(FLAD), a training paradigm that assumes access to auxiliary data during
few-shot learning in hopes of improving generalization. Previous works have
proposed automated methods for mixing auxiliary and target data, but these
methods typically scale linearly (or worse) with the number of auxiliary
datasets, limiting their practicality. In this work we relate FLAD to the
explore-exploit dilemma that is central to the multi-armed bandit setting and
derive algorithms whose computational complexity is independent of the number
of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets
than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and
compare them with prior FLAD methods that either explore or exploit, finding
that the combination of exploration and exploitation is crucial. Through
extensive experimentation we find that our methods outperform all pre-existing
FLAD methods by 4% and lead to the first 3 billion parameter language models
that outperform the 175 billion parameter GPT-3. Overall, our work suggests
that the discovery of better, more efficient mixing strategies for FLAD may
provide a viable path towards substantially improving generalization in
few-shot learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02676">
<div class="article-summary-box-inner">
<span><p>Learning from human preferences is important for language models to match
human needs and to align with human and social values. Prior works have
achieved remarkable successes by learning from human feedback to understand and
follow instructions. Nonetheless, these methods are either founded on
hand-picked model generations that are favored by human annotators, rendering
them inefficient in terms of data utilization and challenging to apply in
general, or they depend on reinforcement learning, which often suffers from
imperfect reward functions and relies on extremely challenging optimizations.
In this work, we propose a novel technique, Chain of Hindsight, that is easy to
optimize and can learn from any form of feedback, regardless of its polarity.
Our idea is inspired by how humans learn from extensive feedback presented in
the form of languages. We convert all types of feedback into sequences of
sentences, which are then used to fine-tune the model, allowing us to take
advantage of the language comprehension capabilities of language models. We
condition the model on a sequence of model generations paired with feedback. By
doing so, the model is trained to generate outputs based on feedback, while
learning to identify and correct negative attributes or errors. Applying our
method to large language models, we observed that Chain of Hindsight
significantly surpasses previous methods in aligning language models with human
preferences. We report significant improvements on summarization and dialogue
benchmarks, with our approach markedly preferred in human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04736">
<div class="article-summary-box-inner">
<span><p>Our work addresses the critical issue of distinguishing text generated by
Large Language Models (LLMs) from human-produced text, a task essential for
numerous applications. Despite ongoing debate about the feasibility of such
differentiation, we present evidence supporting its consistent achievability,
except when human and machine text distributions are indistinguishable across
their entire support. Drawing from information theory, we argue that as
machine-generated text approximates human-like quality, the sample size needed
for detection increases. We establish precise sample complexity bounds for
detecting AI-generated text, laying groundwork for future research aimed at
developing advanced, multi-sample detectors. Our empirical evaluations across
multiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the
viability of enhanced detection methods. We test various state-of-the-art text
generators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and
Llama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector,
GPTZero. Our findings align with OpenAI's empirical data related to sequence
length, marking the first theoretical substantiation for these observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08612">
<div class="article-summary-box-inner">
<span><p>Backpropagation, the cornerstone of deep learning, is limited to computing
gradients for continuous variables. This limitation poses challenges for
problems involving discrete latent variables. To address this issue, we propose
a novel approach to approximate the gradient of parameters involved in
generating discrete latent variables. First, we examine the widely used
Straight-Through (ST) heuristic and demonstrate that it works as a first-order
approximation of the gradient. Guided by our findings, we propose ReinMax,
which achieves second-order accuracy by integrating Heun's method, a
second-order numerical method for solving ODEs. ReinMax does not require
Hessian or other second-order derivatives, thus having negligible computation
overheads. Extensive experimental results on various tasks demonstrate the
superiority of ReinMax over the state of the art. Implementations are released
at https://github.com/microsoft/ReinMax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Retrieval for Motion and Text via DopTriple Loss. (arXiv:2305.04195v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04195">
<div class="article-summary-box-inner">
<span><p>Cross-modal retrieval of image-text and video-text is a prominent research
area in computer vision and natural language processing. However, there has
been insufficient attention given to cross-modal retrieval between human motion
and text, despite its wide-ranging applicability. To address this gap, we
utilize a concise yet effective dual-unimodal transformer encoder for tackling
this task. Recognizing that overlapping atomic actions in different human
motion sequences can lead to semantic conflicts between samples, we explore a
novel triplet loss function called DropTriple Loss. This loss function discards
false negative samples from the negative sample set and focuses on mining
remaining genuinely hard negative samples for triplet training, thereby
reducing violations they cause. We evaluate our model and approach on the
HumanML3D and KIT Motion-Language datasets. On the latest HumanML3D dataset, we
achieve a recall of 62.9% for motion retrieval and 71.5% for text retrieval
(both based on R@10). The source code for our approach is publicly available at
https://github.com/eanson023/rehamot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"What do others think?": Task-Oriented Conversational Modeling with Subjective Knowledge. (arXiv:2305.12091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12091">
<div class="article-summary-box-inner">
<span><p>Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that
assist users in accomplishing specific goals, such as booking a hotel or a
restaurant. Traditional TODs rely on domain-specific APIs/DBs or external
factual knowledge to generate responses, which cannot accommodate subjective
user requests (e.g., "Is the WIFI reliable?" or "Does the restaurant have a
good atmosphere?"). To address this issue, we propose a novel task of
subjective-knowledge-based TOD (SK-TOD). We also propose the first
corresponding dataset, which contains subjective knowledge-seeking dialogue
contexts and manually annotated responses grounded in subjective knowledge
sources. When evaluated with existing TOD approaches, we find that this task
poses new challenges such as aggregating diverse opinions from multiple
knowledge snippets. We hope this task and dataset can promote further research
on TOD and subjective content understanding. The code and the dataset are
available at https://github.com/alexa/dstc11-track5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13269">
<div class="article-summary-box-inner">
<span><p>We present chain-of-knowledge (CoK), a novel framework that augments large
language models (LLMs) by dynamically incorporating grounding information from
heterogeneous sources. It results in more factual rationales and reduced
hallucination in generation. Specifically, CoK consists of three stages:
reasoning preparation, dynamic knowledge adapting, and answer consolidation.
Given a knowledge-intensive question, CoK first prepares several preliminary
rationales and answers while identifying the relevant knowledge domains. If
there is no majority consensus among the answers from samples, CoK corrects the
rationales step by step by adapting knowledge from the identified domains.
These corrected rationales can plausibly serve as a better foundation for the
final answer consolidation. Unlike prior studies that primarily use
unstructured data, CoK also leverages structured knowledge sources such as
Wikidata and tables that provide more reliable factual information. To access
both unstructured and structured knowledge sources in the dynamic knowledge
adapting stage, we propose an adaptive query generator that allows the
generation of queries for various types of query languages, including SPARQL,
SQL, and natural sentences. Moreover, to minimize error propagation between
rationales, CoK corrects the rationales progressively using preceding corrected
rationales to generate and correct subsequent rationales. Extensive experiments
show that CoK consistently improves the performance of LLMs on
knowledge-intensive tasks across different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts. (arXiv:2305.13300v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13300">
<div class="article-summary-box-inner">
<span><p>By providing external information to large language models (LLMs), tool
augmentation (including retrieval augmentation) has emerged as a promising
solution for addressing the limitations of LLMs' static parametric memory.
However, how receptive are LLMs to such external evidence, especially when the
evidence conflicts with their parametric memory? We present the first
comprehensive and controlled investigation into the behavior of LLMs when
encountering knowledge conflicts. We propose a systematic framework to elicit
high-quality parametric memory from LLMs and construct the corresponding
counter-memory, which enables us to conduct a series of controlled experiments.
Our investigation reveals seemingly contradicting behaviors of LLMs. On the one
hand, different from prior wisdom, we find that LLMs can be highly receptive to
external evidence even when that conflicts with their parametric memory, given
that the external evidence is coherent and convincing. On the other hand, LLMs
also demonstrate a strong confirmation bias when the external evidence contains
some information that is consistent with their parametric memory, despite being
presented with conflicting evidence at the same time. These results pose
important implications that are worth careful consideration for the further
development and deployment of tool- and retrieval-augmented LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization. (arXiv:2306.01102v5 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01102">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as powerful tools capable of
accomplishing a broad spectrum of tasks. Their abilities span numerous areas,
and one area where they have made a significant impact is in the domain of code
generation. In this context, we view LLMs as mutation and crossover tools.
Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and
robust solutions. By merging the code-generating abilities of LLMs with the
diversity and robustness of QD solutions, we introduce LLMatic, a Neural
Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS
directly through prompts, LLMatic uses a procedural approach, leveraging QD for
prompts and network architecture to create diverse and highly performant
networks. We test LLMatic on the CIFAR-10 image classification benchmark,
demonstrating that it can produce competitive networks with just $2,000$
searches, even without prior knowledge of the benchmark domain or exposure to
any previous top-performing models for the benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Knowledge Distillation for Auto-regressive Language Models. (arXiv:2306.13649v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13649">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is widely used for compressing a teacher model to
reduce its inference cost and memory footprint, by training a smaller student
model. However, current KD methods for auto-regressive sequence models suffer
from distribution mismatch between output sequences seen during training and
those generated by the student during inference. To address this issue, we
introduce Generalized Knowledge Distillation (GKD). Instead of solely relying
on a fixed set of output sequences, GKD trains the student on its
self-generated output sequences by leveraging feedback from the teacher on such
sequences. Unlike supervised KD approaches, GKD also offers the flexibility to
employ alternative loss functions between the student and teacher, which can be
useful when the student lacks the expressivity to mimic the teacher's
distribution. Furthermore, GKD facilitates the seamless integration of
distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for
distilling auto-regressive T5 language models on summarization, translation,
and arithmetic reasoning tasks as well as task-agnostic instruction tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RecallM: An Adaptable Memory Mechanism with Temporal Understanding for Large Language Models. (arXiv:2307.02738v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02738">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made extraordinary progress in the field of
Artificial Intelligence and have demonstrated remarkable capabilities across a
large variety of tasks and domains. However, as we venture closer to creating
Artificial General Intelligence (AGI) systems, we recognize the need to
supplement LLMs with long-term memory to overcome the context window limitation
and more importantly, to create a foundation for sustained reasoning,
cumulative learning and long-term user interaction. In this paper we propose
RecallM, a novel architecture for providing LLMs with an adaptable and
updatable long-term memory mechanism. Unlike previous methods, the RecallM
architecture is particularly effective at belief updating and maintaining a
temporal understanding of the knowledge provided to it. We demonstrate through
various experiments the effectiveness of this architecture. Furthermore,
through our own temporal understanding and belief updating experiments, we show
that RecallM is four times more effective than using a vector database for
updating knowledge previously stored in long-term memory. We also demonstrate
that RecallM shows competitive performance on general question-answering and
in-context learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06945">
<div class="article-summary-box-inner">
<span><p>We propose the In-context Autoencoder (ICAE), leveraging the power of a large
language models (LLM) to compress a long context into short compact memory
slots that can be directly conditioned on by the LLM for various purposes. ICAE
is first pretrained using both autoencoding and language modeling objectives on
massive text data, enabling it to generate memory slots that accurately and
comprehensively represent the original context; Then, it is fine-tuned on
instruction data for producing desirable responses to various prompts.
Experiments demonstrate that our lightweight ICAE, introducing fewer than 1%
additional parameters, effectively achieves 4X context compression based on
Llama, offering advantages in both improved latency and GPU memory cost during
inference, and showing an interesting insight in memorization as well as
potential for scalability. These promising results imply a novel perspective on
the connection between working memory in cognitive science and representation
learning in LLMs, revealing ICAE's significant implications in addressing the
long context problem and suggesting further research in LLM context management.
Our data, code and model are released at https://github.com/getao/icae.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v4 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10490">
<div class="article-summary-box-inner">
<span><p>We demonstrate how images and sounds can be used for indirect prompt and
instruction injection in multi-modal LLMs. An attacker generates an adversarial
perturbation corresponding to the prompt and blends it into an image or audio
recording. When the user asks the (unmodified, benign) model about the
perturbed image or audio, the perturbation steers the model to output the
attacker-chosen text and/or make the subsequent dialog follow the attacker's
instruction. We illustrate this attack with several proof-of-concept examples
targeting LLaVa and PandaGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Learning Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.12375">
<div class="article-summary-box-inner">
<span><p>The predictions of Large Language Models (LLMs) on downstream tasks often
improve significantly when including examples of the input--label relationship
in the context. However, there is currently no consensus about how this
in-context learning (ICL) ability of LLMs works. For example, while Xie et al.
(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)
argue ICL does not even learn label relationships from in-context examples. In
this paper, we provide novel insights into how ICL leverages label information,
revealing both capabilities and limitations. To ensure we obtain a
comprehensive picture of ICL behavior, we study probabilistic aspects of ICL
predictions and thoroughly examine the dynamics of ICL as more examples are
provided. Our experiments show that ICL predictions almost always depend on
in-context labels, and that ICL can learn truly novel tasks in-context.
However, we also find that ICL struggles to fully overcome prediction
preferences acquired from pre-training data, and, further, that ICL does not
consider all in-context information equally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.12856">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models (LLMs) have recently achieved better
generalization and sample efficiency in autonomous web automation. However, the
performance on real-world websites has still suffered from (1) open domainness,
(2) limited context length, and (3) lack of inductive bias on HTML. We
introduce WebAgent, an LLM-driven agent that learns from self-experience to
complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into canonical
sub-instructions, summarizes long HTML documents into task-relevant snippets,
and acts on websites via Python programs generated from those. We design
WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new
pre-trained LLMs for long HTML documents using local and global attention
mechanisms and a mixture of long-span denoising objectives, for planning and
summarization. We empirically demonstrate that our modular recipe improves the
success on real websites by over 50%, and that HTML-T5 is the best model to
solve various HTML understanding tasks; achieving 18.7% higher success rate
than the prior method on MiniWoB web automation benchmark, and SoTA performance
on Mind2Web, an offline task planning evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. (arXiv:2307.16789v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16789">
<div class="article-summary-box-inner">
<span><p>Despite the advancements of open-source large language models (LLMs), e.g.,
LLaMA, they remain significantly limited in tool-use capabilities, i.e., using
external tools (APIs) to fulfill human instructions. The reason is that current
instruction tuning largely focuses on basic language tasks but ignores the
tool-use domain. This is in contrast to the excellent tool-use capabilities of
state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,
we introduce ToolLLM, a general tool-use framework encompassing data
construction, model training, and evaluation. We first present ToolBench, an
instruction-tuning dataset for tool use, which is constructed automatically
using ChatGPT. Specifically, the construction can be divided into three stages:
(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49
categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to
generate diverse instructions involving these APIs, covering both single-tool
and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to
search for a valid solution path (chain of API calls) for each instruction. To
enhance the reasoning capabilities of LLMs, we develop a novel depth-first
search-based decision tree algorithm. It enables LLMs to evaluate multiple
reasoning traces and expand the search space. Moreover, to evaluate the
tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.
Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it
with a neural API retriever to recommend appropriate APIs for each instruction.
Experiments show that ToolLLaMA demonstrates a remarkable ability to execute
complex instructions and generalize to unseen APIs, and exhibits comparable
performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot
generalization ability in an out-of-distribution tool-use dataset: APIBench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Race Detection Using Large Language Models. (arXiv:2308.07505v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07505">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are demonstrating significant promise as an
alternate strategy to facilitate analyses and optimizations of high-performance
computing programs, circumventing the need for resource-intensive manual tool
creation. In this paper, we explore a novel LLM-based data race detection
approach combining prompting engineering and fine-tuning techniques. We create
a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with
fine-grain labels showing the presence of data race pairs and their associated
variables, line numbers, and read/write information. DRB-ML is then used to
evaluate representative LLMs and fine-tune open-source ones. Our experiment
shows that LLMs can be a viable approach to data race detection. However, they
still cannot compete with traditional data race detection tools when we need
detailed information about variable pairs causing data races.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16137">
<div class="article-summary-box-inner">
<span><p>In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex domains, they often face the
need to follow longer user prompts or generate longer texts. In these
situations, the $\textit{length generalization failure}$ of LLMs on long
sequences becomes more prominent. Most pre-training schemes truncate training
sequences to a fixed length. LLMs often struggle to generate fluent and
coherent texts after longer contexts, even with relative positional encoding
specifically designed to cope with this problem. Common solutions such as
finetuning on longer corpora often involve daunting hardware and time costs and
require careful training process design. To more efficiently extrapolate
existing LLMs' generation quality to longer texts, we theoretically and
empirically investigate the main out-of-distribution (OOD) factors contributing
to this problem. Inspired by this diagnosis, we propose a simple yet effective
solution for on-the-fly length generalization, LM-Infinite. It involves only a
$\mathbf{\Lambda}$-shaped attention mask (to avoid excessive attended tokens)
and a distance limit (to avoid unseen distances) while requiring no parameter
updates or learning. We find it applicable to a variety of LLMs using
relative-position encoding methods. LM-Infinite is computationally efficient
with $O(n)$ time and space, and demonstrates consistent text generation fluency
and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with
2.72x decoding speedup. We will make the codes publicly available following
publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. (arXiv:2309.05653v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05653">
<div class="article-summary-box-inner">
<span><p>We introduce MAmmoTH, a series of open-source large language models (LLMs)
specifically tailored for general math problem-solving. The MAmmoTH models are
trained on MathInstruct, our meticulously curated instruction tuning dataset.
MathInstruct is compiled from 13 math datasets with intermediate rationales,
six of which have rationales newly curated by us. It presents a unique hybrid
of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also
ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT
not only unleashes the potential of tool use but also allows different thought
processes for different math problems. As a result, the MAmmoTH series
substantially outperform existing open-source models on nine mathematical
reasoning datasets across all scales with an average accuracy gain between 16%
and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a
competition-level dataset), which exceeds the best open-source 7B model
(WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH,
even surpassing GPT-4's CoT result. Our work underscores the importance of
diverse problem coverage and the use of hybrid rationales in developing
superior math generalist models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple evolutionary pressures shape identical consonant avoidance in the world's languages. (arXiv:2309.14006v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14006">
<div class="article-summary-box-inner">
<span><p>Languages disfavor word forms containing sequences of similar or identical
consonants, due to the biomechanical and cognitive difficulties posed by
patterns of this sort. However, the specific evolutionary processes responsible
for this phenomenon are not fully understood. Words containing sequences of
identical consonants may be more likely to arise than those without; processes
of word form mutation may be more likely to remove than create sequences of
identical consonants in word forms; finally, words containing identical
consonants may die out more frequently than those without. Phylogenetic
analyses of the evolution of homologous word forms indicate that words with
identical consonants arise less frequently than those without, and processes
which mutate word forms are more likely to remove sequences of identical
consonants than introduce them. However, words with identical consonants do not
die out more frequently than those without. Further analyses reveal that forms
with identical consonants are replaced in basic meaning functions more
frequently than words without. Taken together, results suggest that the under
representation of sequences of identical consonants is overwhelmingly a
byproduct of constraints on word form coinage, though processes related to word
usage also serve to ensure that such patterns are infrequent in more salient
vocabulary items. These findings clarify previously unknown aspects of
processes of lexical evolution and competition that take place during language
change, optimizing communicative systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts. (arXiv:2309.17415v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.17415">
<div class="article-summary-box-inner">
<span><p>This paper explores the robustness of LLMs' preference to their internal
memory or the given prompt, which may contain contrasting information in
real-world applications due to noise or task settings. To this end, we
establish a quantitative benchmarking framework and conduct the role playing
intervention to control LLMs' preference. In specific, we define two types of
robustness, factual robustness targeting the ability to identify the correct
fact from prompts or memory, and decision style to categorize LLMs' behavior in
making consistent choices -- assuming there is no definitive "right" answer --
intuitive, dependent, or rational based on cognitive theory. Our findings,
derived from extensive experiments on seven open-source and closed-source LLMs,
reveal that these models are highly susceptible to misleading prompts,
especially for instructing commonsense knowledge. While detailed instructions
can mitigate the selection of misleading answers, they also increase the
incidence of invalid responses. After Unraveling the preference, we intervene
different sized LLMs through specific style of role instruction, showing their
varying upper bound of robustness and adaptivity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00535">
<div class="article-summary-box-inner">
<span><p>We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical
framework to understand the training procedure of multilayer Transformer
architectures. This is achieved by integrating out the self-attention layer in
Transformers, producing a modified dynamics of MLP layers only. JoMA removes
unrealistic assumptions in previous analysis (e.g., lack of residual
connection) and predicts that the attention first becomes sparse (to learn
salient tokens), then dense (to learn less salient tokens) in the presence of
nonlinear activations, while in the linear case, it is consistent with existing
works that show attention becomes sparse over time. We leverage JoMA to
qualitatively explains how tokens are combined to form hierarchies in
multilayer Transformers, when the input tokens are generated by a latent
hierarchical generative model. Experiments on models trained from real-world
dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia)
verify our theoretical findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification. (arXiv:2310.00602v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00602">
<div class="article-summary-box-inner">
<span><p>Commonly used features in spoken language identification (LID), such as
mel-spectrogram or MFCC, lose high-frequency information due to windowing. The
loss further increases for longer temporal contexts. To improve generalization
of the low-resourced LID systems, we investigate an alternate feature
representation, wavelet scattering transform (WST), that compensates for the
shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We
first optimize WST features for multiple South Asian LID corpora. We show that
LID requires low octave resolution and frequency-scattering is not useful.
Further, cross-corpora evaluations show that the optimal WST hyper-parameters
depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based
LID systems with different sets of WST hyper-parameters to improve
generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05%
and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRAM: Benchmarking Temporal Reasoning for Large Language Models. (arXiv:2310.00835v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00835">
<div class="article-summary-box-inner">
<span><p>Reasoning about time is essential for understanding the nuances of events
described in natural language. Previous research on this topic has been limited
in scope, characterized by a lack of standardized benchmarks that would allow
for consistent evaluations across different studies. In this paper, we
introduce TRAM, a temporal reasoning benchmark composed of ten datasets,
encompassing various temporal aspects of events such as order, arithmetic,
frequency, and duration, designed to facilitate a comprehensive evaluation of
the temporal reasoning capabilities of large language models (LLMs). We conduct
an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both
zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based
models to establish the baseline evaluations. Our findings indicate that these
models still trail human performance in temporal reasoning tasks. It is our
aspiration that TRAM will spur further progress in enhancing the temporal
reasoning abilities of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit. (arXiv:2310.01206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01206">
<div class="article-summary-box-inner">
<span><p>We present appjsonify, a Python-based PDF-to-JSON conversion toolkit for
academic papers. It parses a PDF file using several visual-based document
layout analysis models and rule-based text processing approaches. appjsonify is
a flexible tool that allows users to easily configure the processing pipeline
to handle a specific format of a paper they wish to process. We are publicly
releasing appjsonify as an easy-to-install toolkit available via PyPI and
GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Generalization of Training-based ChatGPT Detection Methods. (arXiv:2310.01307v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01307">
<div class="article-summary-box-inner">
<span><p>ChatGPT is one of the most popular language models which achieve amazing
performance on various natural language tasks. Consequently, there is also an
urgent need to detect the texts generated ChatGPT from human written. One of
the extensively studied methods trains classification models to distinguish
both. However, existing studies also demonstrate that the trained models may
suffer from distribution shifts (during test), i.e., they are ineffective to
predict the generated texts from unseen language tasks or topics. In this work,
we aim to have a comprehensive investigation on these methods' generalization
behaviors under distribution shift caused by a wide range of factors, including
prompts, text lengths, topics, and language tasks. To achieve this goal, we
first collect a new dataset with human and ChatGPT texts, and then we conduct
extensive studies on the collected dataset. Our studies unveil insightful
findings which provide guidance for developing future methodologies or data
collection strategies for ChatGPT detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01405">
<div class="article-summary-box-inner">
<span><p>In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery. (arXiv:2111.06812v5 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06812">
<div class="article-summary-box-inner">
<span><p>Buildings' segmentation is a fundamental task in the field of earth
observation and aerial imagery analysis. Most existing deep learning-based
methods in the literature can be applied to a fixed or narrow-range spatial
resolution imagery. In practical scenarios, users deal with a broad spectrum of
image resolutions. Thus, a given aerial image often needs to be re-sampled to
match the spatial resolution of the dataset used to train the deep learning
model, which results in a degradation in segmentation performance. To overcome
this challenge, we propose, in this manuscript, Scale-invariant Neural Network
(Sci-Net) architecture that segments buildings from wide-range spatial
resolution aerial images. Specifically, our approach leverages UNet
hierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract
fine-grained multi-scale representations. Sci-Net significantly outperforms
state of the art models on the Open Cities AI and the Multi-Scale Building
datasets with a steady improvement margin across different spatial resolutions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-04 23:11:08.199585987 UTC">2023-10-04 23:11:08 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>