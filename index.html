<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-19T01:30:00Z">12-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Foresight -- Deep Generative Modelling of Patient Timelines using Electronic Health Records. (arXiv:2212.08072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08072">
<div class="article-summary-box-inner">
<span><p>Electronic Health Records (EHRs) hold detailed longitudinal information about
each patient's health status and general clinical history, a large portion of
which is stored within the unstructured text. Temporal modelling of this
medical history, which considers the sequence of events, can be used to
forecast and simulate future events, estimate risk, suggest alternative
diagnoses or forecast complications. While most prediction approaches use
mainly structured data or a subset of single-domain forecasts and outcomes, we
processed the entire free-text portion of EHRs for longitudinal modelling. We
present Foresight, a novel GPT3-based pipeline that uses NER+L tools (i.e.
MedCAT) to convert document text into structured, coded concepts, followed by
providing probabilistic forecasts for future medical events such as disorders,
medications, symptoms and interventions. Since large portions of EHR data are
in text form, such an approach benefits from a granular and detailed view of a
patient while introducing modest additional noise. On tests in two large UK
hospitals (King's College Hospital, South London and Maudsley) and the US
MIMIC-III dataset precision@10 of 0.80, 0.81 and 0.91 was achieved for
forecasting the next biomedical concept. Foresight was also validated on 34
synthetic patient timelines by 5 clinicians and achieved relevancy of 97% for
the top forecasted candidate disorder. Foresight can be easily trained and
deployed locally as it only requires free-text data (as a minimum). As a
generative model, it can simulate follow-on disorders, medications and
interventions for as many steps as required. Foresight is a general-purpose
model for biomedical concept modelling that can be used for real-world risk
estimation, virtual trials and clinical research to study the progression of
diseases, simulate interventions and counterfactuals, and for educational
purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constitutional AI: Harmlessness from AI Feedback. (arXiv:2212.08073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08073">
<div class="article-summary-box-inner">
<span><p>As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as 'Constitutional AI'. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint processing of linguistic properties in brains and language models. (arXiv:2212.08094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08094">
<div class="article-summary-box-inner">
<span><p>Language models have been shown to be very effective in predicting brain
recordings of subjects experiencing complex language stimuli. For a deeper
understanding of this alignment, it is important to understand the alignment
between the detailed processing of linguistic information by the human brain
versus language models. In NLP, linguistic probing tasks have revealed a
hierarchy of information processing in neural language models that progresses
from simple to complex with an increase in depth. On the other hand, in
neuroscience, the strongest alignment with high-level language brain regions
has consistently been observed in the middle layers. These findings leave an
open question as to what linguistic information actually underlies the observed
alignment between brains and language models. We investigate this question via
a direct approach, in which we eliminate information related to specific
linguistic properties in the language model representations and observe how
this intervention affects the alignment with fMRI brain recordings obtained
while participants listened to a story. We investigate a range of linguistic
properties (surface, syntactic and semantic) and find that the elimination of
each one results in a significant decrease in brain alignment across all layers
of a language model. These findings provide direct evidence for the role of
specific linguistic information in the alignment between brain and language
models, and opens new avenues for mapping the joint information processing in
both systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFEE: Interactive DataFlow Execution and Evaluation Kit. (arXiv:2212.08099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08099">
<div class="article-summary-box-inner">
<span><p>DataFlow has been emerging as a new paradigm for building task-oriented
chatbots due to its expressive semantic representations of the dialogue tasks.
Despite the availability of a large dataset SMCalFlow and a simplified syntax,
the development and evaluation of DataFlow-based chatbots remain challenging
due to the system complexity and the lack of downstream toolchains. In this
demonstration, we present DFEE, an interactive DataFlow Execution and
Evaluation toolkit that supports execution, visualization and benchmarking of
semantic parsers given dialogue input and backend database. We demonstrate the
system via a complex dialog task: event scheduling that involves temporal
reasoning. It also supports diagnosing the parsing results via a friendly
interface that allows developers to examine dynamic DataFlow and the
corresponding execution results. To illustrate how to benchmark SoTA models, we
propose a novel benchmark that covers more sophisticated event scheduling
scenarios and a new metric on task success evaluation. The codes of DFEE have
been released on https://github.com/amazonscience/dataflow-evaluation-toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies. (arXiv:2212.08104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08104">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
</p>
<p>Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moto: Enhancing Embedding with Multiple Joint Factors for Chinese Text Classification. (arXiv:2212.08105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08105">
<div class="article-summary-box-inner">
<span><p>Recently, language representation techniques have achieved great performances
in text classification. However, most existing representation models are
specifically designed for English materials, which may fail in Chinese because
of the huge difference between these two languages. Actually, few existing
methods for Chinese text classification process texts at a single level.
However, as a special kind of hieroglyphics, radicals of Chinese characters are
good semantic carriers. In addition, Pinyin codes carry the semantic of tones,
and Wubi reflects the stroke structure information, \textit{etc}.
Unfortunately, previous researches neglected to find an effective way to
distill the useful parts of these four factors and to fuse them. In our works,
we propose a novel model called Moto: Enhancing Embedding with
\textbf{M}ultiple J\textbf{o}int Fac\textbf{to}rs. Specifically, we design an
attention mechanism to distill the useful parts by fusing the four-level
information above more effectively. We conduct extensive experiments on four
popular tasks. The empirical results show that our Moto achieves SOTA 0.8316
($F_1$-score, 2.11\% improvement) on Chinese news titles, 96.38 (1.24\%
improvement) on Fudan Corpus and 0.9633 (3.26\% improvement) on THUCNews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Routine Outcome Monitoring in Psychotherapy Treatment using Sentiment-Topic Modelling Approach. (arXiv:2212.08111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08111">
<div class="article-summary-box-inner">
<span><p>Despite the importance of emphasizing the right psychotherapy treatment for
an individual patient, assessing the outcome of the therapy session is equally
crucial. Evidence showed that continuous monitoring patient's progress can
significantly improve the therapy outcomes to an expected change. By monitoring
the outcome, the patient's progress can be tracked closely to help clinicians
identify patients who are not progressing in the treatment. These monitoring
can help the clinician to consider any necessary actions for the patient's
treatment as early as possible, e.g., recommend different types of treatment,
or adjust the style of approach. Currently, the evaluation system is based on
the clinical-rated and self-report questionnaires that measure patients'
progress pre- and post-treatment. While outcome monitoring tends to improve the
therapy outcomes, however, there are many challenges in the current method,
e.g. time and financial burden for administering questionnaires, scoring and
analysing the results. Therefore, a computational method for measuring and
monitoring patient progress over the course of treatment is needed, in order to
enhance the likelihood of positive treatment outcome. Moreover, this
computational method could potentially lead to an inexpensive monitoring tool
to evaluate patients' progress in clinical care that could be administered by a
wider range of health-care professionals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Domain Knowledge in Language Models for Task-Oriented Dialogue Systems. (arXiv:2212.08120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08120">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLM) have advanced the state-of-the-art across
NLP applications, but lack domain-specific knowledge that does not naturally
occur in pre-training data. Previous studies augmented PLMs with symbolic
knowledge for different downstream NLP tasks. However, knowledge bases (KBs)
utilized in these studies are usually large-scale and static, in contrast to
small, domain-specific, and modifiable knowledge bases that are prominent in
real-world task-oriented dialogue (TOD) systems. In this paper, we showcase the
advantages of injecting domain-specific knowledge prior to fine-tuning on TOD
tasks. To this end, we utilize light-weight adapters that can be easily
integrated with PLMs and serve as a repository for facts learned from different
KBs. To measure the efficacy of proposed knowledge injection methods, we
introduce Knowledge Probing using Response Selection (KPRS) -- a probe designed
specifically for TOD models. Experiments on KPRS and the response generation
task show improvements of knowledge injection with adapters over strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WEKA-Based: Key Features and Classifier for French of Five Countries. (arXiv:2212.08132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08132">
<div class="article-summary-box-inner">
<span><p>This paper describes a French dialect recognition system that will
appropriately distinguish between different regional French dialects. A corpus
of five regions - Monaco, French-speaking, Belgium, French-speaking
Switzerland, French-speaking Canada and France, which is targeted
forconstruction by the Sketch Engine. The content of the corpus is related to
the four themes of eating, drinking, sleeping and living, which are closely
linked to popular life. The experimental results were obtained through the
processing of a python coded pre-processor and Waikato Environment for
Knowledge Analysis (WEKA) data analytic tool which contains many filters and
classifiers for machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Long Sequence Modeling via State Space Augmented Transformer. (arXiv:2212.08136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08136">
<div class="article-summary-box-inner">
<span><p>Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08153">
<div class="article-summary-box-inner">
<span><p>Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, FiD
suffers from very expensive inference. We show that the majority of inference
time results from memory bandwidth constraints in the decoder, and propose two
simple changes to the FiD architecture to speed up inference by 7x. The faster
decoder inference then allows for a much larger decoder. We denote FiD with the
above modifications as FiDO, and show that it strongly improves performance
over existing FiD models for a wide range of inference budgets. For example,
FiDO-Large-XXL performs faster inference than FiD-Base and achieves better
performance than FiD-Large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks. (arXiv:2212.08158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08158">
<div class="article-summary-box-inner">
<span><p>Vision and language models (VL) are known to exploit unrobust indicators in
individual modalities (e.g., introduced by distributional biases), instead of
focusing on relevant information in each modality. A small drop in accuracy
obtained on a VL task with a unimodal model suggests that so-called unimodal
collapse occurred. But how to quantify the amount of unimodal collapse
reliably, at dataset and instance-level, to diagnose and combat unimodal
collapse in a targeted way? We present MM-SHAP, a performance-agnostic
multimodality score that quantifies the proportion by which a model uses
individual modalities in multimodal tasks. MM-SHAP is based on Shapley values
and will be applied in two ways: (1) to compare models for their degree of
multimodality, and (2) to measure the contribution of individual modalities for
a given task and dataset. Experiments with 6 VL models -- LXMERT, CLIP and four
ALBEF variants -- on four VL tasks highlight that unimodal collapse can occur
to different degrees and in different directions, contradicting the wide-spread
assumption that unimodal collapse is one-sided. We recommend MM-SHAP for
analysing multimodal tasks, to diagnose and guide progress towards multimodal
integration. Code available at: https://github.com/Heidelberg-NLP/MM-SHAP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Synthetic Datasets for Conversational Recommender Systems. (arXiv:2212.08167v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08167">
<div class="article-summary-box-inner">
<span><p>For researchers leveraging Large-Language Models (LLMs) in the generation of
training datasets, especially for conversational recommender systems - the
absence of robust evaluation frameworks has been a long-standing problem. The
efficiency brought about by LLMs in the data generation phase is impeded during
the process of evaluation of the generated data, since it generally requires
human-raters to ensure that the data generated is of high quality and has
sufficient diversity. Since the quality of training data is critical for
downstream applications, it is important to develop metrics that evaluate the
quality holistically and identify biases. In this paper, we present a framework
that takes a multi-faceted approach towards evaluating datasets produced by
generative models and discuss the advantages and limitations of various
evaluation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Measures of Spread in High Dimensional Latent Spaces. (arXiv:2212.08172v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08172">
<div class="article-summary-box-inner">
<span><p>Understanding geometric properties of natural language processing models'
latent spaces allows the manipulation of these properties for improved
performance on downstream tasks. One such property is the amount of data spread
in a model's latent space, or how fully the available latent space is being
used. In this work, we define data spread and demonstrate that the commonly
used measures of data spread, Average Cosine Similarity and a partition
function min/max ratio I(V), do not provide reliable metrics to compare the use
of latent space across models. We propose and examine eight alternative
measures of data spread, all but one of which improve over these current
metrics when applied to seven synthetic data distributions. Of our proposed
measures, we recommend one principal component-based measure and one
entropy-based measure that provide reliable, relative measures of spread and
can be used to compare models of different sizes and dimensionalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NBC-Softmax : Darkweb Author fingerprinting and migration tracking. (arXiv:2212.08184v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08184">
<div class="article-summary-box-inner">
<span><p>Metric learning aims to learn distances from the data, which enhances the
performance of similarity-based algorithms. An author style detection task is a
metric learning problem, where learning style features with small intra-class
variations and larger inter-class differences is of great importance to achieve
better performance. Recently, metric learning based on softmax loss has been
used successfully for style detection. While softmax loss can produce separable
representations, its discriminative power is relatively poor. In this work, we
propose NBC-Softmax, a contrastive loss based clustering technique for softmax
loss, which is more intuitive and able to achieve superior performance. Our
technique meets the criterion for larger number of samples, thus achieving
block contrastiveness, which is proven to outperform pair-wise losses. It uses
mini-batch sampling effectively and is scalable. Experiments on 4 darkweb
social forums, with NBCSAuthor that uses the proposed NBC-Softmax for author
and sybil detection, shows that our negative block contrastive approach
constantly outperforms state-of-the-art methods using the same network
architecture.
</p>
<p>Our code is publicly available at : https://github.com/gayanku/NBC-Softmax
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources in Natural Language Understanding Systems. (arXiv:2212.08192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08192">
<div class="article-summary-box-inner">
<span><p>Many state-of-the-art natural language understanding (NLU) models are based
on pretrained neural language models. These models often make inferences using
information from multiple sources. An important class of such inferences are
those that require both background knowledge, presumably contained in a model's
pretrained parameters, and instance-specific information that is supplied at
inference time. However, the integration and reasoning abilities of NLU models
in the presence of multiple knowledge sources have been largely understudied.
In this work, we propose a test suite of coreference resolution tasks that
require reasoning over multiple facts. Our dataset is organized into subtasks
that differ in terms of which knowledge sources contain relevant facts. We
evaluate state-of-the-art coreference resolution models on our dataset. Our
results indicate that several models struggle to reason on-the-fly over
knowledge observed both at pretrain time and at inference time. However, with
task-specific training, a subset of models demonstrates the ability to
integrate certain knowledge types from multiple sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Chess Commentaries by Combining Language Models with Symbolic Reasoning Engines. (arXiv:2212.08195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08195">
<div class="article-summary-box-inner">
<span><p>Despite many recent advancements in language modeling, state-of-the-art
language models lack grounding in the real world and struggle with tasks
involving complex reasoning. Meanwhile, advances in the symbolic reasoning
capabilities of AI have led to systems that outperform humans in games like
chess and Go (Silver et al., 2018). Chess commentary provides an interesting
domain for bridging these two fields of research, as it requires reasoning over
a complex board state and providing analyses in natural language. In this work
we demonstrate how to combine symbolic reasoning engines with controllable
language models to generate chess commentaries. We conduct experiments to
demonstrate that our approach generates commentaries that are preferred by
human judges over previous baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saved You A Click: Automatically Answering Clickbait Titles. (arXiv:2212.08196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08196">
<div class="article-summary-box-inner">
<span><p>Often clickbait articles have a title that is phrased as a question or vague
teaser that entices the user to click on the link and read the article to find
the explanation. We developed a system that will automatically find the answer
or explanation of the clickbait hook from the website text so that the user
does not need to read through the text themselves. We fine-tune an extractive
question and answering model (RoBERTa) and an abstractive one (T5), using data
scraped from the 'StopClickbait' Facebook pages and Reddit's 'SavedYouAClick'
subforum. We find that both extractive and abstractive models improve
significantly after finetuning. We find that the extractive model performs
slightly better according to ROUGE scores, while the abstractive one has a
slight edge in terms of BERTscores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension. (arXiv:2212.08204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08204">
<div class="article-summary-box-inner">
<span><p>The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A unified information-theoretic model of EEG signatures of human language processing. (arXiv:2212.08205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08205">
<div class="article-summary-box-inner">
<span><p>We advance an information-theoretic model of human language processing in the
brain, in which incoming linguistic input is processed at two levels, in terms
of a heuristic interpretation and in terms of error correction. We propose that
these two kinds of information processing have distinct electroencephalographic
signatures, corresponding to the well-documented N400 and P600 components of
language-related event-related potentials (ERPs). Formally, we show that the
information content (surprisal) of a word in context can be decomposed into two
quantities: (A) heuristic surprise, which signals processing difficulty of word
given its inferred context, and corresponds with the N400 signal; and (B)
discrepancy signal, which reflects divergence between the true context and the
inferred context, and corresponds to the P600 signal. Both of these quantities
can be estimated using modern NLP techniques. We validate our theory by
successfully simulating ERP patterns elicited by a variety of linguistic
manipulations in previously-reported experimental data from Ryskin et al.
(2021). Our theory is in principle compatible with traditional cognitive
theories assuming a `good-enough' heuristic interpretation stage, but with
precise information-theoretic formulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meeting Summarization: A Survey of the State of the Art. (arXiv:2212.08206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08206">
<div class="article-summary-box-inner">
<span><p>Information overloading requires the need for summarizers to extract salient
information from the text. Currently, there is an overload of dialogue data due
to the rise of virtual communication platforms. The rise of Covid-19 has led
people to rely on online communication platforms like Zoom, Slack, Microsoft
Teams, Discord, etc. to conduct their company meetings. Instead of going
through the entire meeting transcripts, people can use meeting summarizers to
select useful data. Nevertheless, there is a lack of comprehensive surveys in
the field of meeting summarizers. In this survey, we aim to cover recent
meeting summarization techniques. Our survey offers a general overview of text
summarization along with datasets and evaluation metrics for meeting
summarization. We also provide the performance of each summarizer on a
leaderboard. We conclude our survey with different challenges in this domain
and potential research opportunities for future researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Azimuth: Systematic Error Analysis for Text Classification. (arXiv:2212.08216v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08216">
<div class="article-summary-box-inner">
<span><p>We present Azimuth, an open-source and easy-to-use tool to perform error
analysis for text classification. Compared to other stages of the ML
development cycle, such as model training and hyper-parameter tuning, the
process and tooling for the error analysis stage are less mature. However, this
stage is critical for the development of reliable and trustworthy AI systems.
To make error analysis more systematic, we propose an approach comprising
dataset analysis and model quality assessment, which Azimuth facilitates. We
aim to help AI practitioners discover and address areas where the model does
not generalize by leveraging and integrating a range of ML techniques, such as
saliency maps, similarity, uncertainty, and behavioral analyses, all in one
tool. Our code and documentation are available at
github.com/servicenow/azimuth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games. (arXiv:2212.08279v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08279">
<div class="article-summary-box-inner">
<span><p>Persuasion modeling is a key building block for conversational agents.
Existing works in this direction are limited to analyzing textual dialogue
corpus. We argue that visual signals also play an important role in
understanding human persuasive behaviors. In this paper, we introduce the first
multimodal dataset for modeling persuasion behaviors. Our dataset includes 199
dialogue transcriptions and videos captured in a multi-player social deduction
game setting, 26,647 utterance level annotations of persuasion strategy, and
game level annotations of deduction game outcomes. We provide extensive
experiments to show how dialogue context and visual signals benefit persuasion
strategy prediction. We also explore the generalization ability of language
models for persuasion modeling and the role of persuasion strategies in
predicting social deduction game outcomes. Our dataset, code, and models can be
found at https://persuasion-deductiongame.socialai-data.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08283">
<div class="article-summary-box-inner">
<span><p>Most TextVQA approaches focus on the integration of objects, scene texts and
question words by a simple transformer encoder. But this fails to capture the
semantic relations between different modalities. The paper proposes a Scene
Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the
semantic relations among the objects, Optical Character Recognition (OCR)
tokens and the question words. It is achieved by a TextVQA-based scene graph
that discovers the underlying semantics of an image. We created a
guided-attention module to capture the intra-modal interplay between the
language and the vision as a guidance for inter-modal interactions. To make
explicit teaching of the relations between the two modalities, we proposed and
integrated two attention modules, namely a scene graph-based semantic
relation-aware attention and a positional relation-aware attention. We
conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.
It is shown that our SceneGATE method outperformed existing ones because of the
scene graph and its attention modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALERT: Adapting Language Models to Reasoning Tasks. (arXiv:2212.08286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08286">
<div class="article-summary-box-inner">
<span><p>Current large language models can perform reasonably well on complex tasks
that require step-by-step reasoning with few-shot learning. Are these models
applying reasoning skills they have learnt during pre-training and reason
outside of their training context, or are they simply memorizing their training
corpus at finer granularity and have learnt to better understand their context?
To tease apart these possibilities, we introduce ALERT, a benchmark and suite
of analyses for assessing language models' reasoning ability comparing
pre-trained and finetuned models on complex tasks that require reasoning skills
to solve. ALERT provides a test bed to asses any language model on fine-grained
reasoning skills, which spans over 20 datasets and covers 10 different
reasoning skills. We leverage ALERT to further investigate the role of
finetuning. With extensive empirical analysis we find that language models
learn more reasoning skills such as textual entailment, abductive reasoning,
and analogical reasoning during finetuning stage compared to pretraining state.
We also find that when language models are finetuned they tend to overfit to
the prompt template, which hurts the robustness of models causing
generalization problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rich Event Modeling for Script Event Prediction. (arXiv:2212.08287v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08287">
<div class="article-summary-box-inner">
<span><p>Script is a kind of structured knowledge extracted from texts, which contains
a sequence of events. Based on such knowledge, script event prediction aims to
predict the subsequent event. To do so, two aspects should be considered for
events, namely, event description (i.e., what the events should contain) and
event encoding (i.e., how they should be encoded). Most existing methods
describe an event by a verb together with only a few core arguments (i.e.,
subject, object, and indirect object), which are not precise. In addition,
existing event encoders are limited to a fixed number of arguments, which are
not flexible to deal with extra information. Thus, in this paper, we propose
the Rich Event Prediction (REP) framework for script event prediction.
Fundamentally, it is based on the proposed rich event description, which
enriches the existing ones with three kinds of important information, namely,
the senses of verbs, extra semantic roles, and types of participants. REP
contains an event extractor to extract such information from texts. Based on
the extracted rich information, a predictor then selects the most probable
subsequent event. The core component of the predictor is a transformer-based
event encoder to flexibly deal with an arbitrary number of arguments.
Experimental results on the widely used Gigaword Corpus show the effectiveness
of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Text Generation via Probability Density Estimation in the Latent Space. (arXiv:2212.08307v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08307">
<div class="article-summary-box-inner">
<span><p>Previous work on controllable text generation has explored the idea of
control from the latent space, such as optimizing a representation with
attribute-related classifiers or sampling a representation from relevant
discrete samples. However, they are not effective enough in modeling both the
latent space and the control, leaving controlled text with low quality and
diversity. In this work, we propose a novel control framework using probability
density estimation in the latent space. Our method utilizes an invertible
transformation function, the Normalizing Flow, that maps the complex
distributions in the latent space to simple Gaussian distributions in the prior
space. Thus, we can perform sophisticated and flexible control in the prior
space and feed the control effects back into the latent space owing to the
one-one-mapping property of invertible transformations. Experiments on
single-attribute controls and multi-attribute control reveal that our method
outperforms several strong baselines on attribute relevance and text quality
and achieves the SOTA. Further analysis of control strength adjustment
demonstrates the flexibility of our control strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language. (arXiv:2212.08321v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08321">
<div class="article-summary-box-inner">
<span><p>End-to-end text-to-speech synthesis (TTS) can generate highly natural
synthetic speech from raw text. However, rendering the correct pitch accents is
still a challenging problem for end-to-end TTS. To tackle the challenge of
rendering correct pitch accent in Japanese end-to-end TTS, we adopt PnG~BERT, a
self-supervised pretrained model in the character and phoneme domain for TTS.
We investigate the effects of features captured by PnG~BERT on Japanese TTS by
modifying the fine-tuning condition to determine the conditions helpful
inferring pitch accents. We manipulate content of PnG~BERT features from being
text-oriented to speech-oriented by changing the number of fine-tuned layers
during TTS. In addition, we teach PnG~BERT pitch accent information by
fine-tuning with tone prediction as an additional downstream task. Our
experimental results show that the features of PnG~BERT captured by pretraining
contain information helpful inferring pitch accent, and PnG~BERT outperforms
baseline Tacotron on accent correctness in a listening test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks. (arXiv:2212.08322v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08322">
<div class="article-summary-box-inner">
<span><p>Causal chain reasoning (CCR) is an essential ability for many decision-making
AI systems, which requires the model to build reliable causal chains by
connecting causal pairs. However, CCR suffers from two main transitive
problems: threshold effect and scene drift. In other words, the causal pairs to
be spliced may have a conflicting threshold boundary or scenario. To address
these issues, we propose a novel Reliable Causal chain reasoning
framework~(ReCo), which introduces exogenous variables to represent the
threshold and scene factors of each causal pair within the causal chain, and
estimates the threshold and scene contradictions across exogenous variables via
structural causal recurrent neural networks~(SRNN). Experiments show that ReCo
outperforms a series of strong baselines on both Chinese and English CCR
datasets. Moreover, by injecting reliable causal chain knowledge distilled by
ReCo, BERT can achieve better performances on four downstream causal-related
tasks than BERT models enhanced by other kinds of knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder. (arXiv:2212.08329v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08329">
<div class="article-summary-box-inner">
<span><p>Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of
the factors that have been driving TTS are the advancements of probabilistic
models and latent representation learning. We propose a TTS method based on
latent variable conversion using a diffusion probabilistic model and the
variational autoencoder (VAE). In our TTS method, we use a waveform model based
on VAE, a diffusion model that predicts the distribution of latent variables in
the waveform model from texts, and an alignment model that learns alignments
between the text and speech latent sequences. Our method integrates diffusion
with VAE by modeling both mean and variance parameters with diffusion, where
the target distribution is determined by approximation from VAE. This latent
variable conversion framework potentially enables us to flexibly incorporate
various latent feature extractors. Our experiments show that our method is
robust to linguistic labels with poor orthography and alignment errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08330">
<div class="article-summary-box-inner">
<span><p>Attention-based neural networks, such as Transformers, have become ubiquitous
in numerous applications, including computer vision, natural language
processing, and time-series analysis. In all kinds of attention networks, the
attention maps are crucial as they encode semantic dependencies between input
tokens. However, most existing attention networks perform modeling or reasoning
based on representations, wherein the attention maps of different layers are
learned separately without explicit interactions. In this paper, we propose a
novel and generic evolving attention mechanism, which directly models the
evolution of inter-token relationships through a chain of residual
convolutional modules. The major motivations are twofold. On the one hand, the
attention maps in different layers share transferable knowledge, thus adding a
residual connection can facilitate the information flow of inter-token
relationships across layers. On the other hand, there is naturally an
evolutionary trend among attention maps at different abstraction levels, so it
is beneficial to exploit a dedicated convolution-based module to capture this
process. Equipped with the proposed mechanism, the convolution-enhanced
evolving attention networks achieve superior performance in various
applications, including time-series representation, natural language
understanding, machine translation, and image classification. Especially on
time-series representation tasks, Evolving Attention-enhanced Dilated
Convolutional (EA-DC-) Transformer outperforms state-of-the-art models
significantly, achieving an average of 17% improvement compared to the best
SOTA. To the best of our knowledge, this is the first work that explicitly
models the layer-wise evolution of attention maps. Our implementation is
available at https://github.com/pkuyym/EvolvingAttention
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Law to Binary Tree -- An Formal Interpretation of Legal Natural Language. (arXiv:2212.08335v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08335">
<div class="article-summary-box-inner">
<span><p>Knowledge representation and reasoning in law are essential to facilitate the
automation of legal analysis and decision-making tasks. In this paper, we
propose a new approach based on legal science, specifically legal taxonomy, for
representing and reasoning with legal documents. Our approach interprets the
regulations in legal documents as binary trees, which facilitates legal
reasoning systems to make decisions and resolve logical contradictions. The
advantages of this approach are twofold. First, legal reasoning can be
performed on the basis of the binary tree representation of the regulations.
Second, the binary tree representation of the regulations is more
understandable than the existing sentence-based representations. We provide an
example of how our approach can be used to interpret the regulations in a legal
document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to disagree well: Investigating the dispute tactics used on Wikipedia. (arXiv:2212.08353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08353">
<div class="article-summary-box-inner">
<span><p>Disagreements are frequently studied from the perspective of either detecting
toxicity or analysing argument structure. We propose a framework of dispute
tactics that unifies these two perspectives, as well as other dialogue acts
which play a role in resolving disputes, such as asking questions and providing
clarification. This framework includes a preferential ordering among
rebuttal-type tactics, ranging from ad hominem attacks to refuting the central
argument. Using this framework, we annotate 213 disagreements (3,865
utterances) from Wikipedia Talk pages. This allows us to investigate research
questions around the tactics used in disagreements; for instance, we provide
empirical validation of the approach to disagreement recommended by Wikipedia.
We develop models for multilabel prediction of dispute tactics in an utterance,
achieving the best performance with a transformer-based label powerset model.
Adding an auxiliary task to incorporate the ordering of rebuttal tactics
further yields a statistically significant increase. Finally, we show that
these annotations can be used to provide useful additional signals to improve
performance on the task of predicting escalation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks. (arXiv:2212.08354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08354">
<div class="article-summary-box-inner">
<span><p>Massively multi-task learning with large language models has recently made
substantial progress on few-shot generalization. However, this is usually
performed in a centralized learning fashion, ignoring the privacy sensitivity
issue of (annotated) data used in multiple tasks. To mitigate this issue, we
propose FewFedWeight, a few-shot federated learning framework across multiple
tasks, to achieve the best of both worlds: privacy preservation and cross-task
generalization. FewFedWeight trains client models in isolated devices without
sharing data. It broadcasts the global model in the server to each client and
produces pseudo data for clients so that knowledge from the global model can be
explored to enhance few-shot learning of each client model. An energy-based
algorithm is further proposed to weight pseudo samples in order to reduce the
negative impact of noise from the generated pseudo data. Adaptive model weights
of client models are also tuned according to their performance. We use these
model weights to dynamically aggregate client models to update the global
model. Experiments on 118 NLP tasks show that FewFedWeight can significantly
improve the performance of client models on 61% tasks with an average
performance improvement rate of 30.5% over the baseline and substantially
outperform FedAvg and other decentralized learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homonymy Information for English WordNet. (arXiv:2212.08388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08388">
<div class="article-summary-box-inner">
<span><p>A widely acknowledged shortcoming of WordNet is that it lacks a distinction
between word meanings which are systematically related (polysemy), and those
which are coincidental (homonymy). Several previous works have attempted to
fill this gap, by inferring this information using computational methods. We
revisit this task, and exploit recent advances in language modelling to
synthesise homonymy annotation for Princeton WordNet. Previous approaches treat
the problem using clustering methods; by contrast, our method works by linking
WordNet to the Oxford English Dictionary, which contains the information we
need. To perform this alignment, we pair definitions based on their proximity
in an embedding space produced by a Transformer model. Despite the simplicity
of this approach, our best model attains an F1 of .97 on an evaluation set that
we annotate. The outcome of our work is a high-quality homonymy annotation
layer for Princeton WordNet, which we release.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons learned from the evaluation of Spanish Language Models. (arXiv:2212.08390v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08390">
<div class="article-summary-box-inner">
<span><p>Given the impact of language models on the field of Natural Language
Processing, a number of Spanish encoder-only masked language models (aka BERTs)
have been trained and released. These models were developed either within large
projects using very large private corpora or by means of smaller scale academic
efforts leveraging freely available data. In this paper we present a
comprehensive head-to-head comparison of language models for Spanish with the
following results: (i) Previously ignored multilingual models from large
companies fare better than monolingual models, substantially changing the
evaluation landscape of language models in Spanish; (ii) Results across the
monolingual models are not conclusive, with supposedly smaller and inferior
models performing competitively. Based on these empirical results, we argue for
the need of more research to understand the factors underlying them. In this
sense, the effect of corpus size, quality and pre-training techniques need to
be further investigated to be able to obtain Spanish monolingual models
significantly better than the multilingual ones released by large private
companies, specially in the face of rapid ongoing progress in the field. The
recent activity in the development of language technology for Spanish is to be
welcomed, but our results show that building language models remains an open,
resource-heavy problem which requires to marry resources (monetary and/or
computational) with the best research expertise and practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphorical Polysemy Detection: Conventional Metaphor meets Word Sense Disambiguation. (arXiv:2212.08395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08395">
<div class="article-summary-box-inner">
<span><p>Linguists distinguish between novel and conventional metaphor, a distinction
which the metaphor detection task in NLP does not take into account. Instead,
metaphoricity is formulated as a property of a token in a sentence, regardless
of metaphor type. In this paper, we investigate the limitations of treating
conventional metaphors in this way, and advocate for an alternative which we
name 'metaphorical polysemy detection' (MPD). In MPD, only conventional
metaphoricity is treated, and it is formulated as a property of word senses in
a lexicon. We develop the first MPD model, which learns to identify
conventional metaphors in the English WordNet. To train it, we present a novel
training procedure that combines metaphor detection with word sense
disambiguation (WSD). For evaluation, we manually annotate metaphor in two
subsets of WordNet. Our model significantly outperforms a strong baseline based
on a state-of-the-art metaphor detection model, attaining an ROC-AUC score of
.78 (compared to .65) on one of the sets. Additionally, when paired with a WSD
model, our approach outperforms a state-of-the-art metaphor detection model at
identifying conventional metaphors in text (.659 F1 compared to .626).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing distilBert transformer model for sentiment classification of COVID-19's Persian open-text responses. (arXiv:2212.08407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08407">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has caused drastic alternations in human life in all
aspects. The government's laws in this regard affected the lifestyle of all
people. Due to this fact studying the sentiment of individuals is essential to
be aware of the future impacts of the coming pandemics. To contribute to this
aim, we proposed an NLP (Natural Language Processing) model to analyze
open-text answers in a survey in Persian and detect positive and negative
feelings of the people in Iran. In this study, a distilBert transformer model
was applied to take on this task. We deployed three approaches to perform the
comparison, and our best model could gain accuracy: 0.824, Precision: 0.824,
Recall: 0.798, and F1 score: 0.804.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoder Tuning: Efficient Language Understanding as Decoding. (arXiv:2212.08408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08408">
<div class="article-summary-box-inner">
<span><p>With the evergrowing sizes of pre-trained models (PTMs), it has been an
emerging practice to only provide the inference APIs for users, namely
model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen,
most current approaches focus on the input side, seeking for powerful prompts
to stimulate models for correct answers. However, we argue that input-side
adaptation could be arduous due to the lack of gradient signals and they
usually require thousands of API queries, resulting in high computation and
time costs. In light of this, we present Decoder Tuning (DecT), which in
contrast optimizes task-specific decoder networks on the output side.
Specifically, DecT first extracts prompt-stimulated output scores for initial
predictions. On top of that, we train an additional decoder network on the
output representations to incorporate posterior data knowledge. By
gradient-based optimization, DecT can be trained within several seconds and
requires only one PTM query per sample. Empirically, we conduct extensive
natural language understanding experiments and show that DecT significantly
outperforms state-of-the-art algorithms with a $10^3\times$ speed-up.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Small Language Models to Reason. (arXiv:2212.08410v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08410">
<div class="article-summary-box-inner">
<span><p>Chain of thought prompting successfully improves the reasoning capabilities
of large language models, achieving state of the art results on a range of
datasets. However, these reasoning capabilities only appear to emerge in models
with a size of over 100 billion parameters. In this paper, we explore the
transfer of such reasoning capabilities to models with less than 100 billion
parameters via knowledge distillation. Specifically, we finetune a student
model on the chain of thought outputs generated by a larger teacher model. Our
experiments show that the proposed method improves task performance across
arithmetic, commonsense and symbolic reasoning datasets. For example, the
accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on
PaLM-540B generated chains of thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Rule-Based Decoding: Revisiting Syntactic Rules in Neural Constituency Parsing. (arXiv:2212.08458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08458">
<div class="article-summary-box-inner">
<span><p>Most recent studies on neural constituency parsing focus on encoder
structures, while few developments are devoted to decoders. Previous research
has demonstrated that probabilistic statistical methods based on syntactic
rules are particularly effective in constituency parsing, whereas syntactic
rules are not used during the training of neural models in prior work probably
due to their enormous computation requirements. In this paper, we first
implement a fast CKY decoding procedure harnessing GPU acceleration, based on
which we further derive a syntactic rule-based (rule-constrained) CKY decoding.
In the experiments, our method obtains 95.89 and 92.52 F1 on the datasets of
PTB and CTB respectively, which shows significant improvements compared with
previous approaches. Besides, our parser achieves strong and competitive
cross-domain performance in zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experiments on Generalizability of BERTopic on Multi-Domain Short Text. (arXiv:2212.08459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08459">
<div class="article-summary-box-inner">
<span><p>Topic modeling is widely used for analytically evaluating large collections
of textual data. One of the most popular topic techniques is Latent Dirichlet
Allocation (LDA), which is flexible and adaptive, but not optimal for e.g.
short texts from various domains. We explore how the state-of-the-art BERTopic
algorithm performs on short multi-domain text and find that it generalizes
better than LDA in terms of topic coherence and diversity. We further analyze
the performance of the HDBSCAN clustering algorithm utilized by BERTopic and
find that it classifies a majority of the documents as outliers. This crucial,
yet overseen problem excludes too many documents from further analysis. When we
replace HDBSCAN with k-Means, we achieve similar performance, but without
outliers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Best-Answer Prediction in Q&A Sites Using User Information. (arXiv:2212.08475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08475">
<div class="article-summary-box-inner">
<span><p>Community Question Answering (CQA) sites have spread and multiplied
significantly in recent years. Sites like Reddit, Quora, and Stack Exchange are
becoming popular amongst people interested in finding answers to diverse
questions. One practical way of finding such answers is automatically
predicting the best candidate given existing answers and comments. Many studies
were conducted on answer prediction in CQA but with limited focus on using the
background information of the questionnaires. We address this limitation using
a novel method for predicting the best answers using the questioner's
background information and other features, such as the textual content or the
relationships with other participants. Our answer classification model was
trained using the Stack Exchange dataset and validated using the Area Under the
Curve (AUC) metric. The experimental results show that the proposed method
complements previous methods by pointing out the importance of the
relationships between users, particularly throughout the level of involvement
in different communities on Stack Exchange. Furthermore, we point out that
there is little overlap between user-relation information and the information
represented by the shallow text features and the meta-features, such as time
differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implementation of general formal translators. (arXiv:2212.08482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08482">
<div class="article-summary-box-inner">
<span><p>The general translator formalism and computing specific implementations are
proposed. The implementation of specific elements necessary to process the
source and destination information within the translators are presented. Some
common directives or instructions, such as classes and procedures, were unified
and generalized in order to allow general translations implementations. In
order to cover general cases, two levels of processing are required, related to
the source and destination information appropriate transformations, with the
related control and processing instructions. The proposed general translator
elements are useful for processing natural or artificial information described
through any types of languages or systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric. (arXiv:2212.08486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08486">
<div class="article-summary-box-inner">
<span><p>End-to-End speech-to-speech translation (S2ST) is generally evaluated with
text-based metrics. This means that generated speech has to be automatically
transcribed, making the evaluation dependent on the availability and quality of
automatic speech recognition (ASR) systems. In this paper, we propose a
text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the
dependency on ASR systems. BLASER leverages a multilingual multimodal encoder
to directly encode the speech segments for source input, translation output and
reference into a shared embedding space and computes a score of the translation
quality that can be used as a proxy to human evaluation. To evaluate our
approach, we construct training and evaluation sets from more than 40k human
annotations covering seven language directions. The best results of BLASER are
achieved by training with supervision from human rating scores. We show that
when evaluated at the sentence level, BLASER correlates significantly better
with human judgment compared to ASR-dependent metrics including ASR-SENTBLEU in
all translation directions and ASR-COMET in five of them. Our analysis shows
combining speech and text as inputs to BLASER does not increase the correlation
with human scores, but best correlations are achieved when using speech, which
motivates the goal of our research. Moreover, we show that using ASR for
references is detrimental for text-based metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks. (arXiv:2212.08489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08489">
<div class="article-summary-box-inner">
<span><p>In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs allows SLU
systems to improve in comparison to the 1-best setup (4% relative improvement).
However, crossmodal approaches, i.e., learning from acoustic and text
embeddings, obtains performance similar to the oracle setup, and a relative
improvement of 18% over the 1-best configuration. Thus, crossmodal
architectures represent a good alternative to overcome the limitations of
working purely automatically generated textual data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Check-worthy Claim Detection across Topics for Automated Fact-checking. (arXiv:2212.08514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08514">
<div class="article-summary-box-inner">
<span><p>An important component of an automated fact-checking system is the claim
check-worthiness detection system, which ranks sentences by prioritising them
based on their need to be checked. Despite a body of research tackling the
task, previous research has overlooked the challenging nature of identifying
check-worthy claims across different topics. In this paper, we assess and
quantify the challenge of detecting check-worthy claims for new, unseen topics.
After highlighting the problem, we propose the AraCWA model to mitigate the
performance deterioration when detecting check-worthy claims across topics. The
AraCWA model enables boosting the performance for new topics by incorporating
two components for few-shot learning and data augmentation. Using a publicly
available dataset of Arabic tweets consisting of 14 different topics, we
demonstrate that our proposed data augmentation strategy achieves substantial
improvements across topics overall, where the extent of the improvement varies
across topics. Further, we analyse the semantic similarities between topics,
suggesting that the similarity metric could be used as a proxy to determine the
difficulty level of an unseen topic prior to undertaking the task of labelling
the underlying sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2212.08542v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08542">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-trained transformers have improved the state of the art
on a variety of speech tasks. Due to the quadratic time and space complexity of
self-attention, they usually operate at the level of relatively short (e.g.,
utterance) segments. In this paper, we study the use of context, i.e.,
surrounding segments, during fine-tuning and propose a new approach called
context-aware fine-tuning. We attach a context module on top of the last layer
of a pre-trained model to encode the whole segment into a context embedding
vector which is then used as an additional feature for the final prediction.
During the fine-tuning stage, we introduce an auxiliary loss that encourages
this context embedding vector to be similar to context vectors of surrounding
segments. This allows the model to make predictions without access to these
surrounding segments at inference time and requires only a tiny overhead
compared to standard fine-tuned models. We evaluate the proposed approach using
the SLUE and Librilight benchmarks for several downstream tasks: Automatic
speech recognition (ASR), named entity recognition (NER), and sentiment
analysis (SA). The results show that context-aware fine-tuning not only
outperforms a standard fine-tuning baseline but also rivals a strong context
injection baseline that uses neighboring speech segments during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Czech News Article Dataset: An Interdisciplinary Approach to Trustworthiness Analysis. (arXiv:2212.08550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08550">
<div class="article-summary-box-inner">
<span><p>We present the Verifee Dataset: a novel dataset of news articles with
fine-grained trustworthiness annotations. We develop a detailed methodology
that assesses the texts based on their parameters encompassing editorial
transparency, journalist conventions, and objective reporting while penalizing
manipulative techniques. We bring aboard a diverse set of researchers from
social, media, and computer sciences to overcome barriers and limited framing
of this interdisciplinary problem. We collect over $10,000$ unique articles
from almost $60$ Czech online news sources. These are categorized into one of
the $4$ classes across the credibility spectrum we propose, raging from
entirely trustworthy articles all the way to the manipulative ones. We produce
detailed statistics and study trends emerging throughout the set. Lastly, we
fine-tune multiple popular sequence-to-sequence language models using our
dataset on the trustworthiness classification task and report the best testing
F-1 score of $0.52$. We open-source the dataset, annotation methodology, and
annotators' instructions in full length at https://verifee.ai/research to
enable easy build-up work. We believe similar methods can help prevent
disinformation and educate in the realm of media literacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is it Required? Ranking the Skills Required for a Job-Title. (arXiv:2212.08553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08553">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our method for ranking the skills required for a
given job title. Our analysis shows that important/relevant skills appear more
frequently in similar job titles. We train a Language-agnostic BERT Sentence
Encoder (LaBSE) model to predict the importance of the skills using weak
supervision. We show the model can learn the importance of skills and perform
well in other languages. Furthermore, we show how the Inverse Document
Frequency factor of skill boosts the specialised skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Identification of Motivation for Code-Switching in Speech Transcripts. (arXiv:2212.08565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08565">
<div class="article-summary-box-inner">
<span><p>Code-switching, or switching between languages, occurs for many reasons and
has important linguistic, sociological, and cultural implications. Multilingual
speakers code-switch for a variety of purposes, such as expressing emotions,
borrowing terms, making jokes, introducing a new topic, etc. The reason for
code-switching may be quite useful for analysis, but is not readily apparent.
To remedy this situation, we annotate a new dataset of motivations for
code-switching in Spanish-English. We build the first system (to our knowledge)
to automatically identify a wide range of motivations that speakers code-switch
in everyday speech, achieving an accuracy of 75% across all motivations.
Additionally, we show that the system can be adapted to new language pairs,
achieving 66% accuracy on a new language pair (Hindi-English), demonstrating
the cross-lingual applicability of our annotation scheme
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better. (arXiv:2212.08597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08597">
<div class="article-summary-box-inner">
<span><p>While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations "detached" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation. (arXiv:2212.08607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08607">
<div class="article-summary-box-inner">
<span><p>Prompting large language models has enabled significant recent progress in
multi-step reasoning over text. However, when applied to text generation from
semi-structured data (e.g., graphs or tables), these methods typically suffer
from low semantic coverage, hallucination, and logical inconsistency. We
propose MURMUR, a neuro-symbolic modular approach to text generation from
semi-structured data with multi-step reasoning. MURMUR is a best-first search
method that generates reasoning paths using: (1) neural and symbolic modules
with specific linguistic and logical skills, (2) a grammar whose production
rules define valid compositions of modules, and (3) value functions that assess
the quality of each reasoning step. We conduct experiments on two diverse
data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in
their data representations (graphs and tables) and span multiple linguistic and
logical skills. MURMUR obtains significant improvements over recent few-shot
baselines like direct prompting and chain-of-thought prompting, while also
achieving comparable performance to fine-tuned GPT-2 on out-of-domain data.
Moreover, human evaluation shows that MURMUR generates highly faithful and
correct reasoning paths that lead to 26% more logically consistent summaries on
LogicNLG, compared to direct prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Planting and Mitigating Memorized Content in Predictive-Text Language Models. (arXiv:2212.08619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08619">
<div class="article-summary-box-inner">
<span><p>Language models are widely deployed to provide automatic text completion
services in user products. However, recent research has revealed that language
models (especially large ones) bear considerable risk of memorizing private
training data, which is then vulnerable to leakage and extraction by
adversaries. In this study, we test the efficacy of a range of
privacy-preserving techniques to mitigate unintended memorization of sensitive
user text, while varying other factors such as model size and adversarial
conditions. We test both "heuristic" mitigations (those without formal privacy
guarantees) and Differentially Private training, which provides provable levels
of privacy at the cost of some model performance. Our experiments show that
(with the exception of L2 regularization), heuristic mitigations are largely
ineffective in preventing memorization in our test suite, possibly because they
make too strong of assumptions about the characteristics that define
"sensitive" or "private" text. In contrast, Differential Privacy reliably
prevents memorization in our experiments, despite its computational and
model-performance costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08620">
<div class="article-summary-box-inner">
<span><p>We present POTATO, the Portable text annotation tool, a free, fully
open-sourced annotation system that 1) supports labeling many types of text and
multimodal data; 2) offers easy-to-configure features to maximize the
productivity of both deployers and annotators (convenient templates for common
ML/NLP tasks, active learning, keypress shortcuts, keyword highlights,
tooltips); and 3) supports a high degree of customization (editable UI,
inserting pre-screening questions, attention and qualification tests).
Experiments over two annotation tasks suggest that POTATO improves labeling
speed through its specially-designed productivity features, especially for long
documents and complex tasks. POTATO is available at
https://github.com/davidjurgens/potato and will continue to be updated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation. (arXiv:2212.08632v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08632">
<div class="article-summary-box-inner">
<span><p>Multi-modal and multi-hop question answering aims to answer a question based
on multiple input sources from different modalities. Previous methods retrieve
the evidence separately and feed the retrieved evidence to a language model to
generate the corresponding answer. However, these methods fail to build
connections between candidates and thus cannot model the inter-dependent
relation during retrieval. Moreover, the reasoning process over multi-modality
candidates can be unbalanced without building alignments between different
modalities. To address this limitation, we propose a Structured Knowledge and
Unified Retrieval Generation based method (SKURG). We align the sources from
different modalities via the shared entities and map them into a shared
semantic space via structured knowledge. Then, we utilize a unified
retrieval-generation decoder to integrate intermediate retrieval results for
answer generation and adaptively determine the number of retrieval steps. We
perform experiments on two multi-modal and multi-hop datasets: WebQA and
MultimodalQA. The results demonstrate that SKURG achieves state-of-the-art
performance on both retrieval and answer generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Prompting Large Language Models for Open-Domain QA. (arXiv:2212.08635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08635">
<div class="article-summary-box-inner">
<span><p>Open-Domain Question Answering (ODQA) requires models to answer factoid
questions with no context given. The common way for this task is to train
models on a large-scale annotated dataset to retrieve related documents and
generate answers based on these documents. In this paper, we show that the ODQA
architecture can be dramatically simplified by treating Large Language Models
(LLMs) as a knowledge corpus and propose a Self-Prompting framework for LLMs to
perform ODQA so as to eliminate the need for training data and external
knowledge corpus. Concretely, we firstly generate multiple pseudo QA pairs with
background passages and one-sentence explanations for these QAs by prompting
LLMs step by step and then leverage the generated QA pairs for in-context
learning. Experimental results show our method surpasses previous
state-of-the-art methods by +8.8 EM averagely on three widely-used ODQA
datasets, and even achieves comparable performance with several
retrieval-augmented fine-tuned models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Mechanism of Evolution Shared by Proteins and Words. (arXiv:2012.14309v2 [q-bio.PE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14309">
<div class="article-summary-box-inner">
<span><p>Complex systems, such as life and languages, are governed by principles of
evolution. The analogy and comparison between biology and
linguistics\cite{alphafold2, RoseTTAFold, lang_virus, cell language, faculty1,
language of gene, Protein linguistics, dictionary, Grammar of pro_dom,
complexity, genomics_nlp, InterPro, language modeling, Protein language
modeling} provide a computational foundation for characterizing and analyzing
protein sequences, human corpora, and their evolution. However, no general
mathematical formula has been proposed so far to illuminate the origin of
quantitative hallmarks shared by life and language. Here we show several new
statistical relationships shared by proteins and words, which inspire us to
establish a general mechanism of evolution with explicit formulations that can
incorporate both old and new characteristics. We found natural selection can be
quantified via the entropic formulation by the principle of least effort to
determine the sequence variation that survives in evolution. Besides, the
origin of power law behavior and how changes in the environment stimulate the
emergence of new proteins and words can also be explained via the introduction
of function connection network. Our results demonstrate not only the
correspondence between genetics and linguistics over their different
hierarchies but also new fundamental physical properties for the evolution of
complex adaptive systems. We anticipate our statistical tests can function as
quantitative criteria to examine whether an evolution theory of sequence is
consistent with the regularity of real data. In the meantime, their
correspondence broadens the bridge to exchange existing knowledge, spurs new
interpretations, and opens Pandora's box to release several potentially
revolutionary challenges. For example, does linguistic arbitrariness conflict
with the dogma that structure determines function?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting the Tesseract Open-Source OCR Engine for Tamil and Sinhala Legacy Fonts and Creating a Parallel Corpus for Tamil-Sinhala-English. (arXiv:2109.05952v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05952">
<div class="article-summary-box-inner">
<span><p>Most low-resource languages do not have the necessary resources to create
even a substantial monolingual corpus. These languages may often be found in
government proceedings but mainly in Portable Document Format (PDF) that
contains legacy fonts. Extracting text from these documents to create a
monolingual corpus is challenging due to legacy font usage and printer-friendly
encoding, which are not optimized for text extraction. Therefore, we propose a
simple, automatic, and novel idea that can scale for Tamil, Sinhala, English
languages, and many documents along with parallel corpora. Since Tamil and
Sinhala are Low-Resource Languages, we improved the performance of Tesseract by
employing LSTM-based training on more than 20 legacy fonts to recognize printed
characters in these languages. Especially, our model detects code-mixed text,
numbers, and special characters from the printed document. It is shown that
this approach can reduce the character-level error rate of Tesseract from 6.03
to 2.61 for Tamil (-3.42% relative change) and 7.61 to 4.74 for Sinhala (-2.87%
relative change), as well as the word-level error rate from 39.68 to 20.61 for
Tamil (-19.07% relative change) and 35.04 to 26.58 for Sinhala (-8.46% relative
change) on the test set. Also, our newly created parallel corpus consists of
185.4k, 168.9k, and 181.04k sentences and 2.11M, 2.22M, and 2.33M Words in
Tamil, Sinhala, and English respectively. This study shows that fine-tuning
Tesseract models on multiple new fonts help to understand the texts and
enhances the performance of the OCR. We made newly trained models and the
source code for fine-tuning Tesseract, freely available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01959">
<div class="article-summary-box-inner">
<span><p>Researchers produce thousands of scholarly documents containing valuable
technical knowledge. The community faces the laborious task of reading these
documents to identify, extract, and synthesize information. To automate
information gathering, document-level question answering (QA) offers a flexible
framework where human-posed questions can be adapted to extract diverse
knowledge. Finetuning QA systems requires access to labeled data (tuples of
context, question and answer). However, data curation for document QA is
uniquely challenging because the context (i.e. answer evidence passage) needs
to be retrieved from potentially long, ill-formatted documents. Existing QA
datasets sidestep this challenge by providing short, well-defined contexts that
are unrealistic in real-world applications. We present a three-stage document
QA approach: (1) text extraction from PDF; (2) evidence retrieval from
extracted texts to form well-posed contexts; (3) QA to extract knowledge from
contexts to return high-quality answers -- extractive, abstractive, or Boolean.
Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system
achieves a +7.19 improvement in Answer-F1 over existing baselines while
delivering superior context selection. Our results demonstrate that DRC holds
tremendous promise as a flexible framework for practical scientific document
QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Numerical Optimizations for Weighted Low-rank Estimation on Language Model. (arXiv:2211.09718v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09718">
<div class="article-summary-box-inner">
<span><p>Singular value decomposition (SVD) is one of the most popular compression
methods that approximate a target matrix with smaller matrices. However,
standard SVD treats the parameters within the matrix with equal importance,
which is a simple but unrealistic assumption. The parameters of a trained
neural network model may affect task performance unevenly, which suggests
non-equal importance among the parameters. Compared to SVD, the decomposition
method aware of parameter importance is the more practical choice in real
cases. Unlike standard SVD, weighted value decomposition is a non-convex
optimization problem that lacks a closed-form solution. We systematically
investigated multiple optimization strategies to tackle the problem and
examined our method by compressing Transformer-based language models. Further,
we designed a metric to predict when the SVD may introduce a significant
performance drop, for which our method can be a rescue strategy. The extensive
evaluations demonstrate that our method can perform better than current SOTA
methods in compressing Transformer-based language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Probabilistic-Logic based Commonsense Representation Framework for Modelling Inferences with Multiple Antecedents and Varying Likelihoods. (arXiv:2211.16822v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16822">
<div class="article-summary-box-inner">
<span><p>Commonsense knowledge-graphs (CKGs) are important resources towards building
machines that can 'reason' on text or environmental inputs and make inferences
beyond perception. While current CKGs encode world knowledge for a large number
of concepts and have been effectively utilized for incorporating commonsense in
neural models, they primarily encode declarative or single-condition
inferential knowledge and assume all conceptual beliefs to have the same
likelihood. Further, these CKGs utilize a limited set of relations shared
across concepts and lack a coherent knowledge organization structure resulting
in redundancies as well as sparsity across the larger knowledge graph.
Consequently, today's CKGs, while useful for a first level of reasoning, do not
adequately capture deeper human-level commonsense inferences which can be more
nuanced and influenced by multiple contextual or situational factors.
</p>
<p>Accordingly, in this work, we study how commonsense knowledge can be better
represented by -- (i) utilizing a probabilistic logic representation scheme to
model composite inferential knowledge and represent conceptual beliefs with
varying likelihoods and (ii) incorporating a hierarchical conceptual ontology
to identify salient concept-relevant relations and organize beliefs at
different conceptual levels. Our resulting knowledge representation framework
can encode a wider variety of world knowledge and represent beliefs flexibly
using grounded concepts as well as free-text phrases. As a result, the
framework can be utilized as both a traditional free-text knowledge graph and a
grounded logic-based inference system more suitable for neuro-symbolic
applications. We describe how we extend the PrimeNet knowledge base with our
framework through crowd-sourcing and expert-annotation, and demonstrate its
application for more interpretable passage-based semantic parsing and question
answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04800">
<div class="article-summary-box-inner">
<span><p>Current work in named entity recognition (NER) uses either cross entropy (CE)
or conditional random fields (CRF) as the objective/loss functions to optimize
the underlying NER model. Both of these traditional objective functions for the
NER problem generally produce adequate performance when the data distribution
is balanced and there are sufficient annotated training examples. But since NER
is inherently an imbalanced tagging problem, the model performance under the
low-resource settings could suffer using these standard objective functions.
Based on recent advances in area under the ROC curve (AUC) maximization, we
propose to optimize the NER model by maximizing the AUC score. We give evidence
that by simply combining two binary-classifiers that maximize the AUC score,
significant performance improvement over traditional loss functions is achieved
under low-resource NER settings. We also conduct extensive experiments to
demonstrate the advantages of our method under the low-resource and
highly-imbalanced data distribution settings. To the best of our knowledge,
this is the first work that brings AUC maximization to the NER setting.
Furthermore, we show that our method is agnostic to different types of NER
embeddings, models and domains. The code to replicate this work will be
provided upon request.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards mapping the contemporary art world with ArtLM: an art-specific NLP model. (arXiv:2212.07127v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07127">
<div class="article-summary-box-inner">
<span><p>With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference. (arXiv:2212.07428v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07428">
<div class="article-summary-box-inner">
<span><p>We introduce a linguistically enhanced combination of pre-training methods
for transformers. The pre-training objectives include POS-tagging, synset
prediction based on semantic knowledge graphs, and parent prediction based on
dependency parse trees. Our approach achieves competitive results on the
Natural Language Inference task, compared to the state of the art. Specifically
for smaller models, the method results in a significant performance boost,
emphasizing the fact that intelligent pre-training can make up for fewer
parameters and help building more efficient models. Combining POS-tagging and
synset prediction yields the overall best results.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-19 23:12:38.178066468 UTC">2022-12-19 23:12:38 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>