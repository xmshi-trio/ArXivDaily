<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-29T01:30:00Z">03-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses. (arXiv:2303.15587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15587">
<div class="article-summary-box-inner">
<span><p>In the field of Japanese-Chinese translation linguistics, the issue of
correctly translating attributive clauses has persistently proven to be
challenging. Present-day machine translation tools often fail to accurately
translate attributive clauses from Japanese to Chinese. In light of this, this
paper investigates the linguistic problem underlying such difficulties, namely
how does the semantic role of the modified noun affect the selection of
translation patterns for attributive clauses, from a linguistic perspective. To
ad-dress these difficulties, a pre-edit scheme is proposed, which aims to
enhance the accuracy of translation. Furthermore, we propose a novel two-step
prompt strategy, which combines this pre-edit scheme with ChatGPT, currently
the most widely used large language model. This prompt strategy is capable of
optimizing translation input in zero-shot scenarios and has been demonstrated
to improve the average translation accuracy score by over 35%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models. (arXiv:2303.15619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15619">
<div class="article-summary-box-inner">
<span><p>Through exploiting a high level of parallelism enabled by graphics processing
units, transformer architectures have enabled tremendous strides forward in the
field of natural language processing. In a traditional masked language model,
special MASK tokens are used to prompt our model to gather contextual
information from surrounding words to restore originally hidden information. In
this paper, we explore a task-specific masking framework for pre-trained large
language models that enables superior performance on particular downstream
tasks on the datasets in the GLUE benchmark. We develop our own masking
algorithm, Typhoon, based on token input gradients, and compare this with other
standard baselines. We find that Typhoon offers performance competitive with
whole-word masking on the MRPC dataset. Our implementation can be found in a
public Github Repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization. (arXiv:2303.15621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15621">
<div class="article-summary-box-inner">
<span><p>The performance of abstractive text summarization has been greatly boosted by
pre-trained language models recently. The main concern of existing abstractive
summarization methods is the factual inconsistency problem of their generated
summary. To alleviate the problem, many efforts have focused on developing
effective factuality evaluation metrics based on natural language inference and
question answering et al. However, they have limitations of high computational
complexity and relying on annotated data. Most recently, large language models
such as ChatGPT have shown strong ability in not only natural language
understanding but also natural language inference. In this paper, we study the
factual inconsistency evaluation ability of ChatGPT under the zero-shot setting
by evaluating it on the coarse-grained and fine-grained factuality evaluation
tasks including binary natural language inference (NLI), summary ranking, and
consistency rating. Experimental results show that ChatGPT outperforms previous
SOTA evaluation metrics on 6/9 datasets across three tasks, demonstrating its
great potential for assessing factual inconsistency in the zero-shot setting.
The results also highlight the importance of prompt design and the need for
future efforts to address ChatGPT's limitations on evaluation bias, wrong
reasoning, and hallucination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. (arXiv:2303.15647v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15647">
<div class="article-summary-box-inner">
<span><p>This paper presents a systematic overview and comparison of
parameter-efficient fine-tuning methods covering over 40 papers published
between February 2019 and February 2023. These methods aim to resolve the
infeasibility and impracticality of fine-tuning large language models by only
training a small set of parameters. We provide a taxonomy that covers a broad
range of methods and present a detailed method comparison with a specific focus
on real-life efficiency and fine-tuning multibillion-scale language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint embedding in Hierarchical distance and semantic representation learning for link prediction. (arXiv:2303.15655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15655">
<div class="article-summary-box-inner">
<span><p>The link prediction task aims to predict missing entities or relations in the
knowledge graph and is essential for the downstream application. Existing
well-known models deal with this task by mainly focusing on representing
knowledge graph triplets in the distance space or semantic space. However, they
can not fully capture the information of head and tail entities, nor even make
good use of hierarchical level information. Thus, in this paper, we propose a
novel knowledge graph embedding model for the link prediction task, namely,
HIE, which models each triplet (\textit{h}, \textit{r}, \textit{t}) into
distance measurement space and semantic measurement space, simultaneously.
Moreover, HIE is introduced into hierarchical-aware space to leverage rich
hierarchical information of entities and relations for better representation
learning. Specifically, we apply distance transformation operation on the head
entity in distance space to obtain the tail entity instead of translation-based
or rotation-based approaches. Experimental results of HIE on four real-world
datasets show that HIE outperforms several existing state-of-the-art knowledge
graph embedding methods on the link prediction task and deals with complex
relations accurately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15662">
<div class="article-summary-box-inner">
<span><p>This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE
Conference on Games. The objective of this competition is for participants to
create effective prompts for ChatGPT--enabling it to generate Science Birds
levels with high stability and character-like qualities--fully using their
creativity as well as prompt engineering skills. ChatGPT is a conversational
agent developed by OpenAI. Science Birds is selected as the competition
platform because designing an Angry Birds-like level is not a trivial task due
to the in-game gravity; the playability of the levels is determined by their
stability. To lower the entry barrier to the competition, we limit the task to
the generation of capitalized English alphabetical characters. Here, the
quality of the generated levels is determined by their stability and similarity
to the given characters. A sample prompt is provided to participants for their
reference. An experiment is conducted to determine the effectiveness of its
modified versions on level stability and similarity by testing them on several
characters. To the best of our knowledge, we believe that ChatGPT4PCG is the
first competition of its kind and hope to inspire enthusiasm for prompt
engineering in procedural content generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training Transformers for Knowledge Graph Completion. (arXiv:2303.15682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15682">
<div class="article-summary-box-inner">
<span><p>Learning transferable representation of knowledge graphs (KGs) is challenging
due to the heterogeneous, multi-relational nature of graph structures. Inspired
by Transformer-based pretrained language models' success on learning
transferable representation for texts, we introduce a novel inductive KG
representation model (iHT) for KG completion by large-scale pre-training. iHT
consists of a entity encoder (e.g., BERT) and a neighbor-aware relational
scoring function both parameterized by Transformers. We first pre-train iHT on
a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art
results on matched evaluations, with a relative improvement of more than 25% in
mean reciprocal rank over previous SOTA models. When further fine-tuned on
smaller KGs with either entity and relational shifts, pre-trained iHT
representations are shown to be transferable, significantly improving the
performance on FB15K-237 and WN18RR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model and Evaluation: Towards Fairness in Multilingual Text Classification. (arXiv:2303.15697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15697">
<div class="article-summary-box-inner">
<span><p>Recently, more and more research has focused on addressing bias in text
classification models. However, existing research mainly focuses on the
fairness of monolingual text classification models, and research on fairness
for multilingual text classification is still very limited. In this paper, we
focus on the task of multilingual text classification and propose a debiasing
framework for multilingual text classification based on contrastive learning.
Our proposed method does not rely on any external language resources and can be
extended to any other languages. The model contains four modules: multilingual
text representation module, language fusion module, text debiasing module, and
text classification module. The multilingual text representation module uses a
multilingual pre-trained language model to represent the text, the language
fusion module makes the semantic spaces of different languages tend to be
consistent through contrastive learning, and the text debiasing module uses
contrastive learning to make the model unable to identify sensitive attributes'
information. The text classification module completes the basic tasks of
multilingual text classification. In addition, the existing research on the
fairness of multilingual text classification is relatively simple in the
evaluation mode. The evaluation method of fairness is the same as the
monolingual equality difference evaluation method, that is, the evaluation is
performed on a single language. We propose a multi-dimensional fairness
evaluation framework for multilingual text classification, which evaluates the
model's monolingual equality difference, multilingual equality difference,
multilingual equality performance difference, and destructiveness of the
fairness strategy. We hope that our work can provide a more general debiasing
method and a more comprehensive evaluation framework for multilingual text
fairness tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics. (arXiv:2303.15705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15705">
<div class="article-summary-box-inner">
<span><p>Song translation requires both translation of lyrics and alignment of music
notes so that the resulting verse can be sung to the accompanying melody, which
is a challenging problem that has attracted some interests in different aspects
of the translation process. In this paper, we propose Lyrics-Melody Translation
with Adaptive Grouping (LTAG), a holistic solution to automatic song
translation by jointly modeling lyrics translation and lyrics-melody alignment.
It is a novel encoder-decoder framework that can simultaneously translate the
source lyrics and determine the number of aligned notes at each decoding step
through an adaptive note grouping module. To address data scarcity, we
commissioned a small amount of training data annotated specifically for this
task and used large amounts of augmented data through back-translation.
Experiments conducted on an English-Chinese song translation data set show the
effectiveness of our model in both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias or Diversity? Unraveling Semantic Discrepancy in U.S. News Headlines. (arXiv:2303.15708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15708">
<div class="article-summary-box-inner">
<span><p>There is a broad consensus that news media outlets incorporate ideological
biases in their news articles. However, prior studies on measuring the
discrepancies among media outlets and further dissecting the origins of
semantic differences suffer from small sample sizes and limited scope. In this
study, we collect a large dataset of 1.8 million news headlines from major U.S.
media outlets spanning from 2014 to 2022 to thoroughly track and dissect the
semantic discrepancy in U.S. news media. We employ multiple correspondence
analysis (MCA) to quantify the semantic discrepancy relating to four prominent
topics - domestic politics, economic issues, social issues, and foreign
affairs. Additionally, we compare the most frequent n-grams in media headlines
to provide further qualitative insights into our analysis. Our findings
indicate that on domestic politics and social issues, the discrepancy can be
attributed to a certain degree of media bias. Meanwhile, the discrepancy in
reporting foreign affairs is largely attributed to the diversity in individual
journalistic styles. Finally, U.S. media outlets show consistency and high
similarity in their coverage of economic issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15714">
<div class="article-summary-box-inner">
<span><p>Language models have been shown to perform remarkably well on a wide range of
natural language processing tasks. In this paper, we propose a novel system
that uses language models to perform multi-step logical reasoning. Our system
incorporates explicit planning into its inference procedure, thus able to make
more informed reasoning decisions at each step by looking ahead into their
future effects. In our experiments, our full system significantly outperforms
other competing systems. On a multiple-choice question answering task, our
system performs competitively compared to GPT-3-davinci despite having only
around 1.5B parameters. We conduct several ablation studies to demonstrate that
explicit planning plays a crucial role in the system's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of ChatGPT for NLP-based Mental Health Applications. (arXiv:2303.15727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15727">
<div class="article-summary-box-inner">
<span><p>Large language models (LLM) have been successful in several natural language
understanding tasks and could be relevant for natural language processing
(NLP)-based mental health application research. In this work, we report the
performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three
text-based mental health classification tasks: stress detection (2-class
classification), depression detection (2-class classification), and suicidality
detection (5-class classification). We obtained annotated social media posts
for the three classification tasks from public datasets. Then ChatGPT API
classified the social media posts with an input prompt for classification. We
obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression
detection, and suicidality detection, respectively. A baseline model that
always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and
0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a
potential use of language models for mental health classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes. (arXiv:2303.15846v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15846">
<div class="article-summary-box-inner">
<span><p>We investigate different natural language processing (NLP) approaches based
on contextualised word representations for the problem of early prediction of
lung cancer using free-text patient medical notes of Dutch primary care
physicians. Because lung cancer has a low prevalence in primary care, we also
address the problem of classification under highly imbalanced classes.
Specifically, we use large Transformer-based pretrained language models (PLMs)
and investigate: 1) how \textit{soft prompt-tuning} -- an NLP technique used to
adapt PLMs using small amounts of training data -- compares to standard model
fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more
robust compared to PLMs in highly imbalanced settings; and 3) how models fare
when trained on notes from a small number of patients. We find that 1)
soft-prompt tuning is an efficient alternative to standard model fine-tuning;
2) PLMs show better discrimination but worse calibration compared to simpler
static word embedding models as the classification problem becomes more
imbalanced; and 3) results when training models on small number of patients are
mixed and show no clear differences between PLMs and WEMs. All our code is
available open source in
\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Granularity Matching Attention Network for Query Intent Classification in E-commerce Retrieval. (arXiv:2303.15870v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15870">
<div class="article-summary-box-inner">
<span><p>Query intent classification, which aims at assisting customers to find
desired products, has become an essential component of the e-commerce search.
Existing query intent classification models either design more exquisite models
to enhance the representation learning of queries or explore label-graph and
multi-task to facilitate models to learn external information. However, these
models cannot capture multi-granularity matching features from queries and
categories, which makes them hard to mitigate the gap in the expression between
informal queries and categories.
</p>
<p>This paper proposes a Multi-granularity Matching Attention Network (MMAN),
which contains three modules: a self-matching module, a char-level matching
module, and a semantic-level matching module to comprehensively extract
features from the query and a query-category interaction matrix. In this way,
the model can eliminate the difference in expression between queries and
categories for query intent classification. We conduct extensive offline and
online A/B experiments, and the results show that the MMAN significantly
outperforms the strong baselines, which shows the superiority and effectiveness
of MMAN. MMAN has been deployed in production and brings great commercial value
for our company.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling. (arXiv:2303.15973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15973">
<div class="article-summary-box-inner">
<span><p>Dropout is a widely used regularization trick to resolve the overfitting
issue in large feedforward neural networks trained on a small dataset, which
performs poorly on the held-out test subset. Although the effectiveness of this
regularization trick has been extensively studied for convolutional neural
networks, there is a lack of analysis of it for unsupervised models and in
particular, VAE-based neural topic models. In this paper, we have analyzed the
consequences of dropout in the encoder as well as in the decoder of the VAE
architecture in three widely used neural topic models, namely, contextualized
topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly
available datasets. We characterize the dropout effect on these models in terms
of the quality and predictive performance of the generated topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Experimental Study on Sentiment Classification of Moroccan dialect texts in the web. (arXiv:2303.15987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15987">
<div class="article-summary-box-inner">
<span><p>With the rapid growth of the use of social media websites, obtaining the
users' feedback automatically became a crucial task to evaluate their
tendencies and behaviors online. Despite this great availability of
information, and the increasing number of Arabic users only few research has
managed to treat Arabic dialects. The purpose of this paper is to study the
opinion and emotion expressed in real Moroccan texts precisely in the YouTube
comments using some well-known and commonly used methods for sentiment
analysis. In this paper, we present our work of Moroccan dialect comments
classification using Machine Learning (ML) models and based on our collected
and manually annotated YouTube Moroccan dialect dataset. By employing many text
preprocessing and data representation techniques we aim to compare our
classification results utilizing the most commonly used supervised classifiers:
k-nearest neighbors (KNN), Support Vector Machine (SVM), Naive Bayes (NB), and
deep learning (DL) classifiers such as Convolutional Neural Network (CNN) and
Long Short-Term Memory (LTSM). Experiments were performed using both raw and
preprocessed data to show the importance of the preprocessing. In fact, the
experimental results prove that DL models have a better performance for
Moroccan Dialect than classical approaches and we achieved an accuracy of 90%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetically generated text for supervised text analysis. (arXiv:2303.16028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16028">
<div class="article-summary-box-inner">
<span><p>Supervised text models are a valuable tool for political scientists but
present several obstacles to their use, including the expense of hand-labeling
documents, the difficulty of retrieving rare relevant documents for annotation,
and copyright and privacy concerns involved in sharing annotated documents.
This article proposes a partial solution to these three issues, in the form of
controlled generation of synthetic text with large language models. I provide a
conceptual overview of text generation, guidance on when researchers should
prefer different techniques for generating synthetic text, a discussion of
ethics, and a simple technique for improving the quality of synthetic text. I
demonstrate the usefulness of synthetic text with three applications:
generating synthetic tweets describing the fighting in Ukraine, synthetic news
articles describing specified political events for training an event detection
system, and a multilingual corpus of populist manifesto statements for training
a sentence-level populism classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information. (arXiv:2303.16098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16098">
<div class="article-summary-box-inner">
<span><p>This paper presents the first publicly available version of the Carolina
Corpus and discusses its future directions. Carolina is a large open corpus of
Brazilian Portuguese texts under construction using web-as-corpus methodology
enhanced with provenance, typology, versioning, and text integrality. The
corpus aims at being used both as a reliable source for research in Linguistics
and as an important resource for Computer Science research on language models,
contributing towards removing Portuguese from the set of low-resource
languages. Here we present the construction of the corpus methodology,
comparing it with other existing methodologies, as well as the corpus current
state: Carolina's first public version has $653,322,577$ tokens, distributed
over $7$ broad types. Each text is annotated with several different metadata
categories in its header, which we developed using TEI annotation standards. We
also present ongoing derivative works and invite NLP researchers to contribute
with their own.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hallucinations in Large Multilingual Translation Models. (arXiv:2303.16104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16104">
<div class="article-summary-box-inner">
<span><p>Large-scale multilingual machine translation systems have demonstrated
remarkable ability to translate directly between numerous languages, making
them increasingly appealing for real-world applications. However, when deployed
in the wild, these models may generate hallucinated translations which have the
potential to severely undermine user trust and raise safety concerns. Existing
research on hallucinations has primarily focused on small bilingual models
trained on high-resource languages, leaving a gap in our understanding of
hallucinations in massively multilingual models across diverse translation
scenarios. In this work, we fill this gap by conducting a comprehensive
analysis on both the M2M family of conventional neural machine translation
models and ChatGPT, a general-purpose large language model~(LLM) that can be
prompted for translation. Our investigation covers a broad spectrum of
conditions, spanning over 100 translation directions across various resource
levels and going beyond English-centric language pairs. We provide key insights
regarding the prevalence, properties, and mitigation of hallucinations, paving
the way towards more responsible and reliable machine translation systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16133">
<div class="article-summary-box-inner">
<span><p>As general purpose vision models get increasingly effective at a wide set of
tasks, it is imperative that they be consistent across the tasks they support.
Inconsistent AI models are considered brittle and untrustworthy by human users
and are more challenging to incorporate into larger systems that take
dependencies on their outputs. Measuring consistency between very heterogeneous
tasks that might include outputs in different modalities is challenging since
it is difficult to determine if the predictions are consistent with one
another. As a solution, we introduce a benchmark dataset, COCOCON, where we use
contrast sets created by modifying test instances for multiple tasks in small
but semantically meaningful ways to change the gold label, and outline metrics
for measuring if a model is consistent by ranking the original and perturbed
instances across tasks. We find that state-of-the-art systems suffer from a
surprisingly high degree of inconsistent behavior across tasks, especially for
more heterogeneous tasks. Finally, we propose using a rank correlation-based
auxiliary objective computed over large automatically created cross-task
contrast sets to improve the multi-task consistency of large unified models,
while retaining their original accuracy on downstream tasks. Project website
available at https://adymaharana.github.io/cococon/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16166">
<div class="article-summary-box-inner">
<span><p>Despite its pivotal role in research experiments, code correctness is often
presumed only on the basis of the perceived quality of the results. This comes
with the risk of erroneous outcomes and potentially misleading findings. To
address this issue, we posit that the current focus on result reproducibility
should go hand in hand with the emphasis on coding best practices. We bolster
our call to the NLP community by presenting a case study, in which we identify
(and correct) three bugs in widely used open-source implementations of the
state-of-the-art Conformer architecture. Through comparative experiments on
automatic speech recognition and translation in various language settings, we
demonstrate that the existence of bugs does not prevent the achievement of good
and reproducible results and can lead to incorrect conclusions that potentially
misguide future research. In response to this, this study is a call to action
toward the adoption of coding best practices aimed at fostering correctness and
improving the quality of the developed software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Countering Essentialism through Social Bias Reasoning. (arXiv:2303.16173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16173">
<div class="article-summary-box-inner">
<span><p>Essentialist beliefs (i.e., believing that members of the same group are
fundamentally alike) play a central role in social stereotypes and can lead to
harm when left unchallenged. In our work, we conduct exploratory studies into
the task of countering essentialist beliefs (e.g., ``liberals are stupid'').
Drawing on prior work from psychology and NLP, we construct five types of
counterstatements and conduct human studies on the effectiveness of these
different strategies. Our studies also investigate the role in choosing a
counterstatement of the level of explicitness with which an essentialist belief
is conveyed. We find that statements that broaden the scope of a stereotype
(e.g., to other groups, as in ``conservatives can also be stupid'') are the
most popular countering strategy. We conclude with a discussion of challenges
and open questions for future work in this area (e.g., improving factuality,
studying community-specific variation) and we emphasize the importance of work
at the intersection of NLP and psychology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16199">
<div class="article-summary-box-inner">
<span><p>We present LLaMA-Adapter, a lightweight adaption method to efficiently
fine-tune LLaMA into an instruction-following model. Using 52K self-instruct
demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon
the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8
A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and
prepend them to the input text tokens at higher transformer layers. Then, a
zero-init attention mechanism with zero gating is proposed, which adaptively
injects the new instructional cues into LLaMA, while effectively preserves its
pre-trained knowledge. With efficient training, LLaMA-Adapter generates
high-quality responses, comparable to Alpaca with fully fine-tuned 7B
parameters. Furthermore, our approach can be simply extended to multi-modal
input, e.g., images, for image-conditioned LLaMA, which achieves superior
reasoning capacity on ScienceQA. We release our code at
https://github.com/ZrrSkywalker/LLaMA-Adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection. (arXiv:2112.11479v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11479">
<div class="article-summary-box-inner">
<span><p>Recent advancements in technology have led to a boost in social media usage
which has ultimately led to large amounts of user-generated data which also
includes hateful and offensive speech. The language used in social media is
often a combination of English and the native language in the region. In India,
Hindi is used predominantly and is often code-switched with English, giving
rise to the Hinglish (Hindi+English) language. Various approaches have been
made in the past to classify the code-mixed Hinglish hate speech using
different machine learning and deep learning-based techniques. However, these
techniques make use of recurrence on convolution mechanisms which are
computationally expensive and have high memory requirements. Past techniques
also make use of complex data processing making the existing techniques very
complex and non-sustainable to change in data. Proposed work gives a much
simpler approach which is not only at par with these complex networks but also
exceeds performance with the use of subword tokenization algorithms like BPE
and Unigram, along with multi-head attention-based techniques, giving an
accuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use
of BPE and Unigram algorithms help handle the nonconventional Hinglish
vocabulary making the proposed technique simple, efficient and sustainable to
use in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word Alignment. (arXiv:2210.06207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06207">
<div class="article-summary-box-inner">
<span><p>Word alignments are essential for a variety of NLP tasks. Therefore, choosing
the best approaches for their creation is crucial. However, the scarce
availability of gold evaluation data makes the choice difficult. We propose
SilverAlign, a new method to automatically create silver data for the
evaluation of word aligners by exploiting machine translation and minimal
pairs. We show that performance on our silver data correlates well with gold
benchmarks for 9 language pairs, making our approach a valid resource for
evaluation of different domains and languages when gold data are not available.
This addresses the important scenario of missing gold data alignments for
low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16771">
<div class="article-summary-box-inner">
<span><p>In recent years, pretrained models revolutionized the paradigm of natural
language understanding (NLU), where we append a randomly initialized
classification head after the pretrained backbone, e.g. BERT, and finetune the
whole model. As the pretrained backbone makes a major contribution to the
improvement, we naturally expect a good pretrained classification head can also
benefit the training. However, the final-layer output of the backbone, i.e. the
input of the classification head, will change greatly during finetuning, making
the usual head-only pretraining (LP-FT) ineffective. In this paper, we find
that parameter-efficient tuning makes a good classification head, with which we
can simply replace the randomly initialized heads for a stable performance
gain. Our experiments demonstrate that the classification head jointly
pretrained with parameter-efficient tuning consistently improves the
performance on 9 tasks in GLUE and SuperGLUE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptCap: Prompt-Guided Task-Aware Image Captioning. (arXiv:2211.09699v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09699">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (VQA) involves questions that
require world knowledge beyond the image to yield the correct answer. Large
language models (LMs) like GPT-3 are particularly helpful for this task because
of their strong knowledge retrieval and reasoning capabilities. To enable LM to
understand images, prior work uses a captioning model to convert images into
text. However, when summarizing an image in a single caption sentence, which
visual entities to describe are often underspecified. Generic image captions
often miss visual details essential for the LM to answer visual questions
correctly. To address this challenge, we propose PromptCap (Prompt-guided image
Captioning), a captioning model designed to serve as a better connector between
images and black-box LMs. Different from generic captions, PromptCap takes a
natural-language prompt to control the visual entities to describe in the
generated caption. The prompt contains a question that the caption should aid
in answering. To avoid extra annotation, PromptCap is trained by examples
synthesized with GPT-3 and existing datasets. We demonstrate PromptCap's
effectiveness on an existing pipeline in which GPT-3 is prompted with image
captions to carry out VQA. PromptCap outperforms generic captions by a large
margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks
(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that
PromptCap generalizes well to unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06373">
<div class="article-summary-box-inner">
<span><p>Current approaches to empathetic response generation typically encode the
entire dialogue history directly and put the output into a decoder to generate
friendly feedback. These methods focus on modelling contextual information but
neglect capturing the direct intention of the speaker. We argue that the last
utterance in the dialogue empirically conveys the intention of the speaker.
Consequently, we propose a novel model named InferEM for empathetic response
generation. We separately encode the last utterance and fuse it with the entire
dialogue through the multi-head attention based intention fusion module to
capture the speaker's intention. Besides, we utilize previous utterances to
predict the last utterance, which simulates human's psychology to guess what
the interlocutor may speak in advance. To balance the optimizing rates of the
utterance prediction and response generation, a multi-task learning strategy is
designed for InferEM. Experimental results demonstrate the plausibility and
validity of InferEM in improving empathetic expression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextDescriptives: A Python package for calculating a large variety of metrics from text. (arXiv:2301.02057v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02057">
<div class="article-summary-box-inner">
<span><p>TextDescriptives is a Python package for calculating a large variety of
metrics from text. It is built on top of spaCy and can be easily integrated
into existing workflows. The package has already been used for analysing the
linguistic stability of clinical texts, creating features for predicting
neuropsychiatric conditions, and analysing linguistic goals of primary school
students. This paper describes the package and its features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opportunities and Challenges in Neural Dialog Tutoring. (arXiv:2301.09919v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09919">
<div class="article-summary-box-inner">
<span><p>Designing dialog tutors has been challenging as it involves modeling the
diverse and complex pedagogical strategies employed by human tutors. Although
there have been significant recent advances in neural conversational systems
using large language models (LLMs) and growth in available dialog corpora,
dialog tutoring has largely remained unaffected by these advances. In this
paper, we rigorously analyze various generative language models on two dialog
tutoring datasets for language learning using automatic and human evaluations
to understand the new opportunities brought by these advances as well as the
challenges we must overcome to build models that would be usable in real
educational settings. We find that although current approaches can model
tutoring in constrained learning scenarios when the number of concepts to be
taught and possible teacher strategies are small, they perform poorly in less
constrained scenarios. Our human quality evaluation shows that both models and
ground-truth annotations exhibit low performance in terms of equitable
tutoring, which measures learning opportunities for students and how engaging
the dialog is. To understand the behavior of our models in a real tutoring
setting, we conduct a user study using expert annotators and find a
significantly large number of model reasoning errors in 45% of conversations.
Finally, we connect our findings to outline future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KNNs of Semantic Encodings for Rating Prediction. (arXiv:2302.00412v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00412">
<div class="article-summary-box-inner">
<span><p>This paper explores a novel application of textual semantic similarity to
user-preference representation for rating prediction. The approach represents a
user's preferences as a graph of textual snippets from review text, where the
edges are defined by semantic similarity. This textual, memory-based approach
to rating prediction enables review-based explanations for recommendations. The
method is evaluated quantitatively, highlighting that leveraging text in this
way outperforms both strong memory-based and model-based collaborative
filtering baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. (arXiv:2302.07027v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07027">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) are trained on massive corpora, but often
need to specialize to specific domains. A parameter-efficient adaptation method
suggests training an adapter for each domain on the task of language modeling.
This leads to good in-domain scores but can be impractical for domain- or
resource-restricted settings. A solution is to use a related-domain adapter for
the novel domain at test time. In this paper, we introduce AdapterSoup, an
approach that performs weight-space averaging of adapters trained on different
domains. Our approach is embarrassingly parallel: first, we train a set of
domain-specific adapters; then, for each novel domain, we determine which
adapters should be averaged at test time. We present extensive experiments
showing that AdapterSoup consistently improves performance to new domains
without extra training. We also explore weight averaging of adapters trained on
the same domain with different hyper-parameters, and show that it preserves the
performance of a PLM on new domains while obtaining strong in-domain results.
We explore various approaches for choosing which adapters to combine, such as
text clustering and semantic similarity. We find that using clustering leads to
the most competitive results on novel domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stage Pipeline for Multilingual Dialect Detection. (arXiv:2303.03487v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.03487">
<div class="article-summary-box-inner">
<span><p>Dialect Identification is a crucial task for localizing various Large
Language Models. This paper outlines our approach to the VarDial 2023 shared
task. Here we have to identify three or two dialects from three languages each
which results in a 9-way classification for Track-1 and 6-way classification
for Track-2 respectively. Our proposed approach consists of a two-stage system
and outperforms other participants' systems and previous works in this domain.
We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase
is available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-augmented Prompt Tuning for Better Few-shot Learning. (arXiv:2303.12314v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12314">
<div class="article-summary-box-inner">
<span><p>Prompt tuning is a parameter-efficient method, which freezes all PLM
parameters and only prepends some additional tunable tokens called soft prompts
to the input text. However, soft prompts heavily rely on a better
initialization and may easily result in overfitting under few-shot settings,
which causes prompt-tuning performing much worse than fine-tuning. To address
the above issues, this paper proposes a novel Self-sUpervised Meta-prompt
learning framework with MEtagradient Regularization for few shot generalization
(SUMMER). We leverage self-supervised meta-learning to better initialize soft
prompts and curriculum-based task augmentation is further proposed to enrich
the meta-task distribution. Besides, a novel meta-gradient regularization
method is integrated into the meta-prompt learning framework, which meta-learns
to transform the raw gradient during few-shot learning into a
domain-generalizable direction, thus alleviating the problem of overfitting.
Extensive experiments show that SUMMER achieves better performance for
different few-shot downstream tasks, and also exhibits a stronger domain
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12712">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13284">
<div class="article-summary-box-inner">
<span><p>In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13351">
<div class="article-summary-box-inner">
<span><p>In this work we create a question answering dataset over the DBLP scholarly
knowledge graph (KG). DBLP is an on-line reference for bibliographic
information on major computer science publications that indexes over 4.4
million publications published by more than 2.2 million authors. Our dataset
consists of 10,000 question answer pairs with the corresponding SPARQL queries
which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD
is the largest scholarly question answering dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14070">
<div class="article-summary-box-inner">
<span><p>Recent large language models (LLMs) in the general domain, such as ChatGPT,
have shown remarkable success in following instructions and producing
human-like responses. However, such language models have not been tailored to
the medical domain, resulting in poor answer accuracy and inability to give
plausible recommendations for medical diagnosis, medications, etc. To address
this issue, we collected more than 700 diseases and their corresponding
symptoms, required medical tests, and recommended medications, from which we
generated 5K doctor-patient conversations. By fine-tuning LLMs using these
tailored doctor-patient conversations, the resulting models emerge with great
potential to understand patients' needs, provide informed advice, and offer
valuable assistance in a variety of medical-related fields. The integration of
these advanced language models into healthcare can revolutionize the way
healthcare professionals and patients communicate, ultimately improving the
overall efficiency and quality of patient care and outcomes. In addition, we
made public all the source codes, datasets, and model weights to facilitate the
further development of dialogue models in the medical field. The training data,
codes, and weights of this project are available at:
https://github.com/Kent0n-Li/ChatDoctor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14337">
<div class="article-summary-box-inner">
<span><p>Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a
time-sensitive comprehensive understanding of the situation to allow for
appropriate decision-making and effective action response. Automated generation
of situation reports can significantly reduce the time, effort, and cost for
domain experts when preparing their official human-curated reports. However, AI
research toward this goal has been very limited, and no successful trials have
yet been conducted to automate such report generation. We propose SmartBook, a
novel task formulation targeting situation report generation, which consumes
large volumes of news data to produce a structured situation report with
multiple hypotheses (claims) summarized and grounded with rich links to factual
evidence. We realize SmartBook for the Ukraine-Russia crisis by automatically
generating intelligence analysis reports to assist expert analysts. The
machine-generated reports are structured in the form of timelines, with each
timeline organized by major events (or chapters), corresponding strategic
questions (or sections) and their grounded summaries (or section content). Our
proposed framework automatically detects real-time event-related strategic
questions, which are more directed than manually-crafted analyst questions,
which tend to be too complex, hard to parse, vague and high-level. Results from
thorough qualitative evaluations show that roughly 82% of the questions in
Smartbook have strategic importance, with at least 93% of the sections in the
report being tactically useful. Further, experiments show that expert analysts
tend to add more information into the SmartBook reports, with only 2.3% of the
existing tokens being deleted, meaning SmartBook can serve as a useful
foundation for analysts to build upon when creating intelligence reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15078">
<div class="article-summary-box-inner">
<span><p>Text summarization has a wide range of applications in many scenarios. The
evaluation of the quality of the generated text is a complex problem. A big
challenge to language evaluation is that there is a clear divergence between
existing metrics and human evaluation. For example, the quality of a document
summary can be measured by human annotators from both objective aspects, such
as grammatical and semantic correctness, as well as subjective dimensions, such
as comprehensiveness, succinctness, and interestingness. Most of the automatic
evaluation methods like BLUE/ROUGE may be not able to capture the above
dimensions well. In this paper, we propose a new evaluation framework based on
LLMs, which provides a comprehensive evaluation framework by comparing
generated text and reference text from both objective and subjective aspects.
First, we propose to model objective and subjective dimensions of generated
text based on roleplayers prompting mechanism. Furthermore, we introduce a
context-based prompting mechanism that is able to generate dynamic roleplayer
profiles based on input context. Finally, we design a multi-roleplayer
prompting technology based on batch prompting to integrate multiple evaluation
results into evaluation results. Experimental results on two real datasets for
summarization show that our model is highly competitive and has a very high
consistency with human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14838">
<div class="article-summary-box-inner">
<span><p>Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-29 23:12:17.624733784 UTC">2023-03-29 23:12:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>