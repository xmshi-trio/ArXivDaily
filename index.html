<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-11T01:30:00Z">04-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Nations: Quantifying the Role of Multilinguals in Communication on Social Media. (arXiv:2304.03797v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03797">
<div class="article-summary-box-inner">
<span><p>Social media enables the rapid spread of many kinds of information, from
memes to social movements. However, little is known about how information
crosses linguistic boundaries. We apply causal inference techniques on the
European Twitter network to quantify multilingual users' structural role and
communication influence in cross-lingual information exchange. Overall,
multilinguals play an essential role; posting in multiple languages increases
betweenness centrality by 13%, and having a multilingual network neighbor
increases monolinguals' odds of sharing domains and hashtags from another
language 16-fold and 4-fold, respectively. We further show that multilinguals
have a greater impact on diffusing information less accessible to their
monolingual compatriots, such as information from far-away countries and
content about regional politics, nascent social movements, and job
opportunities. By highlighting information exchange across borders, this work
sheds light on a crucial component of how information and ideas spread around
the world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03843">
<div class="article-summary-box-inner">
<span><p>Humans have a powerful and mysterious capacity to reason. By working through
a series of purely mental steps, we can make inferences we would not be capable
of making directly -- despite that fact that we get no additional data from the
world. Similarly, large language models can perform better at complex tasks
through chain-of-thought reasoning, where they generate intermediate steps
before answering a question. We use language models to investigate the
questions of when and why reasoning is helpful, testing the hypothesis that
reasoning is effective when training data consisting of local clusters of
variables that influence each other strongly. These training conditions enable
the chaining of accurate local inferences in order to estimate relationships
between variables that were not seen together in training. We train an
autoregressive transformer on samples from joint distributions defined by Bayes
nets, but only include a subset of all the variables in each sample. We compare
language models' ability to match conditional probabilities both with and
without intermediate reasoning steps, finding that intermediate steps help only
when the training data is locally structured with respect to dependencies
between variables. Furthermore, intermediate variables need to be relevant to
the relationship between observed information and target inferences. Our
results illustrate how the statistical structure of training data drives the
effectiveness of reasoning step by step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factify 2: A Multimodal Fake News and Satire News Dataset. (arXiv:2304.03897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03897">
<div class="article-summary-box-inner">
<span><p>The internet gives the world an open platform to express their views and
share their stories. While this is very valuable, it makes fake news one of our
society's most pressing problems. Manual fact checking process is time
consuming, which makes it challenging to disprove misleading assertions before
they cause significant harm. This is he driving interest in automatic fact or
claim verification. Some of the existing datasets aim to support development of
automating fact-checking techniques, however, most of them are text based.
Multi-modal fact verification has received relatively scant attention. In this
paper, we provide a multi-modal fact-checking dataset called FACTIFY 2,
improving Factify 1 by using new data sources and adding satire articles.
Factify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have three
broad categories - support, no-evidence, and refute, with sub-categories based
on the entailment of visual and textual data. We also provide a BERT and Vison
Transformer based baseline, which acheives 65% F1 score in the test set. The
baseline codes and the dataset will be made available at
https://github.com/surya1701/Factify-2.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03898">
<div class="article-summary-box-inner">
<span><p>In recent years, short Text Matching tasks have been widely applied in the
fields ofadvertising search and recommendation. The difficulty lies in the lack
of semantic information and word ambiguity caused by the short length of the
text. Previous works have introduced complement sentences or knowledge bases to
provide additional feature information. However, these methods have not fully
interacted between the original sentence and the complement sentence, and have
not considered the noise issue that may arise from the introduction of external
knowledge bases. Therefore, this paper proposes a short Text Matching model
that combines contrastive learning and external knowledge. The model uses a
generative model to generate corresponding complement sentences and uses the
contrastive learning method to guide the model to obtain more semantically
meaningful encoding of the original sentence. In addition, to avoid noise, we
use keywords as the main semantics of the original sentence to retrieve
corresponding knowledge words in the knowledge base, and construct a knowledge
graph. The graph encoding model is used to integrate the knowledge base
information into the model. Our designed model achieves state-of-the-art
performance on two publicly available Chinese Text Matching datasets,
demonstrating the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study and Improvement for Speech Emotion Recognition. (arXiv:2304.03899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03899">
<div class="article-summary-box-inner">
<span><p>Multimodal speech emotion recognition aims to detect speakers' emotions from
audio and text. Prior works mainly focus on exploiting advanced networks to
model and fuse different modality information to facilitate performance, while
neglecting the effect of different fusion strategies on emotion recognition. In
this work, we consider a simple yet important problem: how to fuse audio and
text modality information is more helpful for this multimodal task. Further, we
propose a multimodal emotion recognition model improved by perspective loss.
Empirical results show our method obtained new state-of-the-art results on the
IEMOCAP dataset. The in-depth analysis explains why the improved model can
achieve improvements and outperforms baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Code Explanations Created by Students and Large Language Models. (arXiv:2304.03938v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03938">
<div class="article-summary-box-inner">
<span><p>Reasoning about code and explaining its purpose are fundamental skills for
computer scientists. There has been extensive research in the field of
computing education on the relationship between a student's ability to explain
code and other skills such as writing and tracing code. In particular, the
ability to describe at a high-level of abstraction how code will behave over
all possible inputs correlates strongly with code writing skills. However,
developing the expertise to comprehend and explain code accurately and
succinctly is a challenge for many students. Existing pedagogical approaches
that scaffold the ability to explain code, such as producing exemplar code
explanations on demand, do not currently scale well to large classrooms. The
recent emergence of powerful large language models (LLMs) may offer a solution.
In this paper, we explore the potential of LLMs in generating explanations that
can serve as examples to scaffold students' ability to understand and explain
code. To evaluate LLM-created explanations, we compare them with explanations
created by students in a large course ($n \approx 1000$) with respect to
accuracy, understandability and length. We find that LLM-created explanations,
which can be produced automatically on demand, are rated as being significantly
easier to understand and more accurate summaries of code than student-created
explanations. We discuss the significance of this finding, and suggest how such
models can be incorporated into introductory programming education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MphayaNER: Named Entity Recognition for Tshivenda. (arXiv:2304.03952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03952">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) plays a vital role in various Natural Language
Processing tasks such as information retrieval, text classification, and
question answering. However, NER can be challenging, especially in low-resource
languages with limited annotated datasets and tools. This paper adds to the
effort of addressing these challenges by introducing MphayaNER, the first
Tshivenda NER corpus in the news domain. We establish NER baselines by
\textit{fine-tuning} state-of-the-art models on MphayaNER. The study also
explores zero-shot transfer between Tshivenda and other related Bantu
languages, with chiShona and Kiswahili showing the best results. Augmenting
MphayaNER with chiShona data was also found to improve model performance
significantly. Both MphayaNER and the baseline models are made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition. (arXiv:2304.04026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04026">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a fundamental NLP tasks with a wide range
of practical applications. The performance of state-of-the-art NER methods
depends on high quality manually anotated datasets which still do not exist for
some languages. In this work we aim to remedy this situation in Slovak by
introducing WikiGoldSK, the first sizable human labelled Slovak NER dataset. We
benchmark it by evaluating state-of-the-art multilingual Pretrained Language
Models and comparing it to the existing silver-standard Slovak NER dataset. We
also conduct few-shot experiments and show that training on a sliver-standard
dataset yields better results. To enable future work that can be based on
Slovak NER, we release the dataset, code, as well as the trained models
publicly under permissible licensing terms at
https://github.com/NaiveNeuron/WikiGoldSK.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP. (arXiv:2304.04029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04029">
<div class="article-summary-box-inner">
<span><p>We introduce bipol, a new metric with explainability, for estimating social
bias in text data. Harmful bias is prevalent in many online sources of data
that are used for training machine learning (ML) models. In a step to address
this challenge we create a novel metric that involves a two-step process:
corpus-level evaluation based on model classification and sentence-level
evaluation based on (sensitive) term frequency (TF). After creating new models
to detect bias along multiple axes using SotA architectures, we evaluate two
popular NLP datasets (COPA and SQUAD). As additional contribution, we created a
large dataset (with almost 2 million labelled samples) for training models in
bias detection and make it publicly available. We also make public our codes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. (arXiv:2304.04052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04052">
<div class="article-summary-box-inner">
<span><p>The sequence-to-sequence (seq2seq) task aims at generating the target
sequence based on the given input source sequence. Traditionally, most of the
seq2seq task is resolved by the Encoder-Decoder framework which requires an
encoder to encode the source sequence and a decoder to generate the target
text. Recently, a bunch of new approaches have emerged that apply decoder-only
language models directly to the seq2seq task. Despite the significant
advancements in applying language models to the seq2seq task, there is still a
lack of thorough analysis on the effectiveness of the decoder-only language
model architecture. This paper aims to address this gap by conducting a
detailed comparison between the encoder-decoder architecture and the
decoder-only language model framework through the analysis of a regularized
encoder-decoder structure. This structure is designed to replicate all
behaviors in the classical decoder-only language model but has an encoder and a
decoder making it easier to be compared with the classical encoder-decoder
structure. Based on the analysis, we unveil the attention degeneration problem
in the language model, namely, as the generation step number grows, less and
less attention is focused on the source sequence. To give a quantitative
understanding of this problem, we conduct a theoretical sensitivity analysis of
the attention output with respect to the source input. Grounded on our
analysis, we propose a novel partial attention language model to solve the
attention degeneration problem. Experimental results on machine translation,
summarization, and data-to-text generation tasks support our analysis and
demonstrate the effectiveness of our proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04054">
<div class="article-summary-box-inner">
<span><p>The paper describes a transformer-based system designed for SemEval-2023 Task
9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict
the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very
intimate). The official training set for the competition consisted of tweets in
six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The
test set included the given six languages as well as external data with four
languages not presented in the training set (Hindi, Arabic, Dutch, and Korean).
We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa
model adapted to the Twitter domain. To improve the performance of unseen
languages, each tweet was supplemented by its English translation. We explored
the effectiveness of translated data for the languages seen in fine-tuning
compared to unseen languages and estimated strategies for using translated data
in transformer-based models. Our solution ranked 4th on the leaderboard while
achieving an overall Pearson's r of 0.599 over the test set. The proposed
system improves up to 0.088 Pearson's r over a score averaged across all 45
submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning. (arXiv:2304.04087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04087">
<div class="article-summary-box-inner">
<span><p>This paper presents a deep learning-based pipeline for categorizing Bengali
toxic comments, in which at first a binary classification model is used to
determine whether a comment is toxic or not, and then a multi-label classifier
is employed to determine which toxicity type the comment belongs to. For this
purpose, we have prepared a manually labeled dataset consisting of 16,073
instances among which 8,488 are Toxic and any toxic comment may correspond to
one or more of the six toxic categories - vulgar, hate, religious, threat,
troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERT
Embedding achieved 89.42% accuracy for the binary classification task while as
a multi-label classifier, a combination of Convolutional Neural Network and
Bi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanism
achieved 78.92% accuracy and 0.86 as weighted F1-score. To explain the
predictions and interpret the word feature importance during classification by
the proposed models, we utilized Local Interpretable Model-Agnostic
Explanations (LIME) framework. We have made our dataset public and can be
accessed at -
https://github.com/deepu099cse/Multi-Labeled-Bengali-Toxic-Comments-Classification
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04099">
<div class="article-summary-box-inner">
<span><p>Unsupervised discovery of stories with correlated news articles in real-time
helps people digest massive news streams without expensive human annotations. A
common approach of the existing studies for unsupervised online story discovery
is to represent news articles with symbolic- or graph-based embedding and
incrementally cluster them into stories. Recent large language models are
expected to improve the embedding further, but a straightforward adoption of
the models by indiscriminately encoding all information in articles is
ineffective to deal with text-rich and evolving news streams. In this work, we
propose a novel thematic embedding with an off-the-shelf pretrained sentence
encoder to dynamically represent articles and stories by considering their
shared temporal themes. To realize the idea for unsupervised online story
discovery, a scalable framework USTORY is introduced with two main techniques,
theme- and time-aware dynamic embedding and novelty-aware adaptive clustering,
fueled by lightweight story summaries. A thorough evaluation with real news
data sets demonstrates that USTORY achieves higher story discovery performances
than baselines while being robust and scalable to various streaming settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-class Categorization of Reasons behind Mental Disturbance in Long Texts. (arXiv:2304.04118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04118">
<div class="article-summary-box-inner">
<span><p>Motivated with recent advances in inferring users' mental state in social
media posts, we identify and formulate the problem of finding causal indicators
behind mental illness in self-reported text. In the past, we witness the
presence of rule-based studies for causal explanation analysis on curated
Facebook data. The investigation on transformer-based model for multi-class
causal categorization in Reddit posts point to a problem of using long-text
which contains as many as 4000 words. Developing end-to-end transformer-based
models subject to the limitation of maximum-length in a given instance. To
handle this problem, we use Longformer and deploy its encoding on
transformer-based classifier. The experimental results show that Longformer
achieves new state-of-the-art results on M-CAMS, a publicly available dataset
with 62\% F1-score. Cause-specific analysis and ablation study prove the
effectiveness of Longformer. We believe our work facilitates causal analysis of
depression and suicide risk on social media data, and shows potential for
application on other mental health conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Graph Convolutional Network for Text Classification. (arXiv:2304.04152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04152">
<div class="article-summary-box-inner">
<span><p>Graph convolutional network (GCN) has been successfully applied to capture
global non-consecutive and long-distance semantic information for text
classification. However, while GCN-based methods have shown promising results
in offline evaluations, they commonly follow a seen-token-seen-document
paradigm by constructing a fixed document-token graph and cannot make
inferences on new documents. It is a challenge to deploy them in online systems
to infer steaming text data. In this work, we present a continual GCN model
(ContGCN) to generalize inferences from observed documents to unobserved
documents. Concretely, we propose a new all-token-any-document paradigm to
dynamically update the document-token graph in every batch during both the
training and testing phases of an online system. Moreover, we design an
occurrence memory module and a self-supervised contrastive learning objective
to update ContGCN in a label-free manner. A 3-month A/B test on Huawei public
opinion analysis system shows ContGCN achieves 8.86% performance gain compared
with state-of-the-art methods. Offline experiments on five public datasets also
show ContGCN can improve inference quality. The source code will be released at
https://github.com/Jyonn/ContGCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An investigation of speaker independent phrase break models in End-to-End TTS systems. (arXiv:2304.04157v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04157">
<div class="article-summary-box-inner">
<span><p>This paper presents our work on phrase break prediction in the context of
end-to-end TTS systems, motivated by the following questions: (i) Is there any
utility in incorporating an explicit phrasing model in an end-to-end TTS
system?, and (ii) How do you evaluate the effectiveness of a phrasing model in
an end-to-end TTS system? In particular, the utility and effectiveness of
phrase break prediction models are evaluated in in the context of childrens
story synthesis, using listener comprehension. We show by means of perceptual
listening evaluations that there is a clear preference for stories synthesized
after predicting the location of phrase breaks using a trained phrasing model,
over stories directly synthesized without predicting the location of phrase
breaks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04187">
<div class="article-summary-box-inner">
<span><p>The standard paradigm for fake news detection mainly utilizes text
information to model the truthfulness of news. However, the discourse of online
fake news is typically subtle and it requires expert knowledge to use textual
information to debunk fake news. Recently, studies focusing on multimodal fake
news detection have outperformed text-only methods. Recent approaches utilizing
the pre-trained model to extract unimodal features, or fine-tuning the
pre-trained model directly, have become a new paradigm for detecting fake news.
Again, this paradigm either requires a large number of training instances, or
updates the entire set of pre-trained model parameters, making real-world fake
news detection impractical. Furthermore, traditional multimodal methods fuse
the cross-modal features directly without considering that the uncorrelated
semantic representation might inject noise into the multimodal features. This
paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)
framework. First, we incorporate prompt learning into multimodal fake news
detection. Prompt learning, which only tunes prompts with a frozen language
model, can reduce memory usage significantly and achieve comparable
performances, compared with fine-tuning. We analyse three prompt templates with
a soft verbalizer to detect fake news. In addition, we introduce the
similarity-aware fusing method to adaptively fuse the intensity of multimodal
representation and mitigate the noise injection via uncorrelated cross-modal
features. For evaluation, SAMPLE surpasses the F1 and the accuracies of
previous works on two benchmark multimodal datasets, demonstrating the
effectiveness of the proposed method in detecting fake news. In addition,
SAMPLE also is superior to other approaches regardless of few-shot and
data-rich settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques. (arXiv:2304.04190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04190">
<div class="article-summary-box-inner">
<span><p>This paper describes the participation of team QUST in the SemEval2023 task
3. The monolingual models are first evaluated with the under-sampling of the
majority classes in the early stage of the task. Then, the pre-trained
multilingual model is fine-tuned with a combination of the class weights and
the sample weights. Two different fine-tuning strategies, the task-agnostic and
the task-dependent, are further investigated. All experiments are conducted
under the 10-fold cross-validation, the multilingual approaches are superior to
the monolingual ones. The submitted system achieves the second best in Italian
and Spanish (zero-shot) in subtask-1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extractive Summarization via ChatGPT for Faithful Summary Generation. (arXiv:2304.04193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04193">
<div class="article-summary-box-inner">
<span><p>Extractive summarization is a crucial task in natural language processing
that aims to condense long documents into shorter versions by directly
extracting sentences. The recent introduction of ChatGPT has attracted
significant interest in the NLP community due to its remarkable performance on
a wide range of downstream tasks. However, concerns regarding factuality and
faithfulness have hindered its practical applications for summarization
systems. This paper first presents a thorough evaluation of ChatGPT's
performance on extractive summarization and compares it with traditional
fine-tuning methods on various benchmark datasets. Our experimental analysis
reveals that ChatGPT's extractive summarization performance is still inferior
to existing supervised systems in terms of ROUGE scores. In addition, we
explore the effectiveness of in-context learning and chain-of-thought reasoning
for enhancing its performance. Furthermore, we find that applying an
extract-then-generate pipeline with ChatGPT yields significant performance
improvements over abstractive baselines in terms of summary faithfulness. These
observations highlight potential directions for enhancing ChatGPT's
capabilities for faithful text summarization tasks using two-stage approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RISC: Generating Realistic Synthetic Bilingual Insurance Contract. (arXiv:2304.04212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04212">
<div class="article-summary-box-inner">
<span><p>This paper presents RISC, an open-source Python package data generator
(https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile
insurance contracts based on the Quebec regulatory insurance form in French and
English. Insurance contracts are 90 to 100 pages long and use complex legal and
insurance-specific vocabulary for a layperson. Hence, they are a much more
complex class of documents than those in traditional NLP corpora. Therefore, we
introduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile
Contract dataset based on the mandatory Quebec car insurance contract. The
dataset comprises 10,000 French and English unannotated insurance contracts.
RISCBAC enables NLP research for unsupervised automatic summarisation, question
answering, text simplification, machine translation and more. Moreover, it can
be further automatically annotated as a dataset for supervised tasks such as
NER
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04250">
<div class="article-summary-box-inner">
<span><p>Methods for making high-quality recommendations often rely on learning latent
representations from interaction data. These methods, while performant, do not
provide ready mechanisms for users to control the recommendation they receive.
Our work tackles this problem by proposing LACE, a novel concept value
bottleneck model for controllable text recommendations. LACE represents each
user with a succinct set of human-readable concepts through retrieval given
user-interacted documents and learns personalized representations of the
concepts based on user documents. This concept based user profile is then
leveraged to make recommendations. The design of our model affords control over
the recommendations through a number of intuitive interactions with a
transparent user profile. We first establish the quality of recommendations
obtained from LACE in an offline evaluation on three recommendation tasks
spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we
validate the controllability of LACE under simulated user interactions.
Finally, we implement LACE in an interactive controllable recommender system
and conduct a user study to demonstrate that users are able to improve the
quality of recommendations they receive through interactions with an editable
user profile.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding. (arXiv:2304.04256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04256">
<div class="article-summary-box-inner">
<span><p>Zero-shot dialogue understanding aims to enable dialogue to track the user's
needs without any training data, which has gained increasing attention. In this
work, we investigate the understanding ability of ChatGPT for zero-shot
dialogue understanding tasks including spoken language understanding (SLU) and
dialogue state tracking (DST). Experimental results on four popular benchmarks
reveal the great potential of ChatGPT for zero-shot dialogue understanding. In
addition, extensive analysis shows that ChatGPT benefits from the multi-turn
interactive prompt in the DST task but struggles to perform slot filling for
SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue
understanding tasks, hoping to provide some insights for future research on
building zero-shot dialogue understanding systems with Large Language Models
(LLMs).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain. (arXiv:2304.04280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04280">
<div class="article-summary-box-inner">
<span><p>This paper introduces FrenchMedMCQA, the first publicly available
Multiple-Choice Question Answering (MCQA) dataset in French for medical domain.
It is composed of 3,105 questions taken from real exams of the French medical
specialization diploma in pharmacy, mixing single and multiple answers. Each
instance of the dataset contains an identifier, a question, five possible
answers and their manual correction(s). We also propose first baseline models
to automatically process this MCQA task in order to report on the current
performances and to highlight the difficulty of the task. A detailed analysis
of the results showed that it is necessary to have representations adapted to
the medical domain or to the MCQA task: in our case, English specialized models
yielded better results than generic French ones, even though FrenchMedMCQA is
in French. Corpus, models and tools are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04321">
<div class="article-summary-box-inner">
<span><p>Understanding the continuous states of objects is essential for task learning
and planning in the real world. However, most existing task learning benchmarks
assume discrete(e.g., binary) object goal states, which poses challenges for
the learning of complex tasks and transferring learned policy from simulated
environments to the real world. Furthermore, state discretization limits a
robot's ability to follow human instructions based on the grounding of actions
and states. To tackle these challenges, we present ARNOLD, a benchmark that
evaluates language-grounded task learning with continuous states in realistic
3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve
understanding object states and learning policies for continuous goals. To
promote language-instructed learning, we provide expert demonstrations with
template-generated language descriptions. We assess task performance by
utilizing the latest language-conditioned policy learning models. Our results
indicate that current models for language-conditioned manipulations continue to
experience significant challenges in novel goal-state generalizations, scene
generalizations, and object generalizations. These findings highlight the need
to develop new algorithms that address this gap and underscore the potential
for further research in this area. See our project page at:
https://arnold-benchmark.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04339">
<div class="article-summary-box-inner">
<span><p>Recently, ChatGPT has drawn great attention from both the research community
and the public. We are particularly curious about whether it can serve as a
universal sentiment analyzer. To this end, in this work, we provide a
preliminary evaluation of ChatGPT on the understanding of opinions, sentiments,
and emotions contained in the text. Specifically, we evaluate it in four
settings, including standard evaluation, polarity shift evaluation, open-domain
evaluation, and sentiment inference evaluation. The above evaluation involves
18 benchmark datasets and 5 representative sentiment analysis tasks, and we
compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA)
models on end-task. Moreover, we also conduct human evaluation and present some
qualitative case studies to gain a deep comprehension of its sentiment analysis
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus. (arXiv:2304.04358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04358">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a new NLP task -- generating short factual
articles with references for queries by mining supporting evidence from the
Web. In this task, called WebBrain, the ultimate goal is to generate a fluent,
informative, and factually-correct short article (e.g., a Wikipedia article)
for a factual query unseen in Wikipedia. To enable experiments on WebBrain, we
construct a large-scale dataset WebBrain-Raw by extracting English Wikipedia
articles and their crawlable Wikipedia references. WebBrain-Raw is ten times
larger than the previous biggest peer dataset, which can greatly benefit the
research community. From WebBrain-Raw, we construct two task-specific datasets:
WebBrain-R and WebBrain-G, which are used to train in-domain retriever and
generator, respectively. Besides, we empirically analyze the performances of
the current state-of-the-art NLP techniques on WebBrain and introduce a new
framework ReGen, which enhances the generation factualness by improved evidence
retrieval and task-specific pre-training for generation. Experiment results
show that ReGen outperforms all baselines in both automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04370">
<div class="article-summary-box-inner">
<span><p>Human intelligence has the remarkable ability to assemble basic skills into
complex ones so as to solve complex tasks. This ability is equally important
for Artificial Intelligence (AI), and thus, we assert that in addition to the
development of large, comprehensive intelligent models, it is equally crucial
to equip such models with the capability to harness various domain-specific
expert models for complex task-solving in the pursuit of Artificial General
Intelligence (AGI). Recent developments in Large Language Models (LLMs) have
demonstrated remarkable learning and reasoning abilities, making them promising
as a controller to select, synthesize, and execute external models to solve
complex tasks. In this project, we develop OpenAGI, an open-source AGI research
platform, specifically designed to offer complex, multi-step tasks and
accompanied by task-specific datasets, evaluation metrics, and a diverse range
of extensible models. OpenAGI formulates complex tasks as natural language
queries, serving as input to the LLM. The LLM subsequently selects,
synthesizes, and executes models provided by OpenAGI to address the task.
Furthermore, we propose a Reinforcement Learning from Task Feedback (RLTF)
mechanism, which uses the task-solving result as feedback to improve the LLM's
task-solving ability. Thus, the LLM is responsible for synthesizing various
external models for solving complex tasks, while RLTF provides feedback to
improve its task-solving ability, enabling a feedback loop for self-improving
AI. We believe that the paradigm of LLMs operating various expert models for
complex task-solving is a promising approach towards AGI. To facilitate the
community's long-term improvement and evaluation of AGI's ability, we
open-source the code, benchmark, and evaluation methods of the OpenAGI project
at https://github.com/agiresearch/OpenAGI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. (arXiv:2104.14839v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14839">
<div class="article-summary-box-inner">
<span><p>Recently, various neural encoder-decoder models pioneered by Seq2Seq
framework have been proposed to achieve the goal of generating more abstractive
summaries by learning to map input text to output text. At a high level, such
neural models can freely generate summaries without any constraint on the words
or phrases used. Moreover, their format is closer to human-edited summaries and
output is more readable and fluent. However, the neural model's abstraction
ability is a double-edged sword. A commonly observed problem with the generated
summaries is the distortion or fabrication of factual information in the
article. This inconsistency between the original text and the summary has
caused various concerns over its applicability, and the previous evaluation
methods of text summarization are not suitable for this issue. In response to
the above problems, the current research direction is predominantly divided
into two categories, one is to design fact-aware evaluation metrics to select
outputs without factual inconsistency errors, and the other is to develop new
summarization systems towards factual consistency. In this survey, we focus on
presenting a comprehensive review of these fact-specific evaluation methods and
text summarization models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05999">
<div class="article-summary-box-inner">
<span><p>Code is seldom written in a single left-to-right pass and is instead
repeatedly edited and refined. We introduce InCoder, a unified generative model
that can perform program synthesis (via left-to-right generation) as well as
editing (via infilling). InCoder is trained to generate code files from a large
corpus of permissively licensed code, where regions of code have been randomly
masked and moved to the end of each file, allowing code infilling with
bidirectional context. Our model is the first generative model that is able to
directly perform zero-shot code infilling, which we evaluate on challenging
tasks such as type inference, comment generation, and variable re-naming. We
find that the ability to condition on bidirectional context substantially
improves performance on these tasks, while still performing comparably on
standard program synthesis benchmarks in comparison to left-to-right only
models pretrained at similar scale. The InCoder models and code are publicly
released. https://sites.google.com/view/incoder-code-models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10498">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models (LLMs) have transformed the field of
natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art
performance on natural language tasks is being pushed forward with every new
large language model. Along with natural language abilities, there has been a
significant interest in understanding whether such models exhibit reasoning
capabilities with the use of reasoning benchmarks. However, even though results
are seemingly positive, these benchmarks prove to be simplistic in nature and
the performance of LLMs on these benchmarks cannot be used as evidence to
support, many a times outlandish, claims being made about LLMs' reasoning
capabilities. Further, these only represent a very limited set of simple
reasoning tasks and we need to look at more sophisticated reasoning problems if
we are to measure the true limits of such LLM-based systems. Motivated by this,
we propose an extensible assessment framework to test the capabilities of LLMs
on reasoning about actions and change, a central aspect of human intelligence.
We provide multiple test cases that are more involved than any of the
previously established benchmarks and each test case evaluates a different
aspect of reasoning about actions and change. Results on GPT-3 (davinci),
Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance
on such reasoning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01180">
<div class="article-summary-box-inner">
<span><p>This work investigates the use of large-scale, English-only pre-trained
models (CLIP and HuBERT) for multilingual image-speech retrieval. For
non-English image-speech retrieval, we outperform the current state-of-the-art
performance by a wide margin both when training separate models for each
language, and with a single model which processes speech in all three
languages. We identify key differences in model behavior and performance
between English and non-English settings, attributable to the English-only
pre-training of CLIP and HuBERT, and investigate how fine-tuning the
pre-trained models impacts these differences. Finally, we show that our models
can be used for mono- and cross-lingual speech-text retrieval and cross-lingual
speech-speech retrieval, despite never having seen any parallel speech-text or
speech-speech data during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02533">
<div class="article-summary-box-inner">
<span><p>The substitute-based recommendation is widely used in E-commerce to provide
better alternatives to customers. However, existing research typically uses the
customer behavior signals like co-view and view-but-purchase-another to capture
the substitute relationship. Despite its intuitive soundness, we find that such
an approach might ignore the functionality and characteristics of products. In
this paper, we adapt substitute recommendation into language matching problem
by taking product title description as model input to consider product
functionality. We design a new transformation method to de-noise the signals
derived from production data. In addition, we consider multilingual support
from the engineering point of view. Our proposed end-to-end transformer-based
model achieves both successes from offline and online experiments. The proposed
model has been deployed in a large-scale E-commerce website for 11 marketplaces
in 6 languages. Our proposed model is demonstrated to increase revenue by 19%
based on an online A/B experiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discharge Summary Hospital Course Summarisation of In Patient Electronic Health Record Text with Clinical Concept Guided Deep Pre-Trained Transformer Models. (arXiv:2211.07126v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07126">
<div class="article-summary-box-inner">
<span><p>Brief Hospital Course (BHC) summaries are succinct summaries of an entire
hospital encounter, embedded within discharge summaries, written by senior
clinicians responsible for the overall care of a patient. Methods to
automatically produce summaries from inpatient documentation would be
invaluable in reducing clinician manual burden of summarising documents under
high time-pressure to admit and discharge patients. Automatically producing
these summaries from the inpatient course, is a complex, multi-document
summarisation task, as source notes are written from various perspectives (e.g.
nursing, doctor, radiology), during the course of the hospitalisation. We
demonstrate a range of methods for BHC summarisation demonstrating the
performance of deep learning summarisation models across extractive and
abstractive summarisation scenarios. We also test a novel ensemble extractive
and abstractive summarisation model that incorporates a medical concept
ontology (SNOMED) as a clinical guidance signal and shows superior performance
in 2 real-world clinical data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction. (arXiv:2211.15003v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15003">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in
sentiment analysis research, aiming to extract triplets of the aspect term, its
corresponding opinion term, and its associated sentiment polarity from a given
sentence. Recently, many neural networks based models with different tagging
schemes have been proposed, but almost all of them have their limitations:
heavily relying on 1) prior assumption that each word is only associated with a
single role (e.g., aspect term, or opinion term, etc. ) and 2) word-level
interactions and treating each opinion/aspect as a set of independent words.
Hence, they perform poorly on the complex ASTE task, such as a word associated
with multiple roles or an aspect/opinion term with multiple words. Hence, we
propose a novel approach, Span TAgging and Greedy infErence (STAGE), to extract
sentiment triplets in span-level, where each span may consist of multiple words
and play different roles simultaneously. To this end, this paper formulates the
ASTE task as a multi-class span classification problem. Specifically, STAGE
generates more accurate aspect sentiment triplet extractions via exploring
span-level information and constraints, which consists of two components,
namely, span tagging scheme and greedy inference strategy. The former tag all
possible candidate spans based on a newly-defined tagging set. The latter
retrieves the aspect/opinion term with the maximum length from the candidate
sentiment snippet to output sentiment triplets. Furthermore, we propose a
simple but effective model based on the STAGE, which outperforms the
state-of-the-arts by a large margin on four widely-used datasets. Moreover, our
STAGE can be easily generalized to other pair/triplet extraction tasks, which
also demonstrates the superiority of the proposed scheme STAGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs. (arXiv:2211.15845v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15845">
<div class="article-summary-box-inner">
<span><p>Existing knowledge graph (KG) embedding models have primarily focused on
static KGs. However, real-world KGs do not remain static, but rather evolve and
grow in tandem with the development of KG applications. Consequently, new facts
and previously unseen entities and relations continually emerge, necessitating
an embedding model that can quickly learn and transfer new knowledge through
growth. Motivated by this, we delve into an expanding field of KG embedding in
this paper, i.e., lifelong KG embedding. We consider knowledge transfer and
retention of the learning on growing snapshots of a KG without having to learn
embeddings from scratch. The proposed model includes a masked KG autoencoder
for embedding learning and update, with an embedding transfer strategy to
inject the learned knowledge into the new entity and relation embeddings, and
an embedding regularization method to avoid catastrophic forgetting. To
investigate the impacts of different aspects of KG growth, we construct four
datasets to evaluate the performance of lifelong KG embedding. Experimental
results show that the proposed model outperforms the state-of-the-art inductive
and lifelong embedding baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MiLMo:Minority Multilingual Pre-trained Language Model. (arXiv:2212.01779v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01779">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models are trained on large-scale unsupervised data, and
they can fine-turn the model only on small-scale labeled datasets, and achieve
good results. Multilingual pre-trained language models can be trained on
multiple languages, and the model can understand multiple languages at the same
time. At present, the search on pre-trained models mainly focuses on rich
resources, while there is relatively little research on low-resource languages
such as minority languages, and the public multilingual pre-trained language
model can not work well for minority languages. Therefore, this paper
constructs a multilingual pre-trained model named MiLMo that performs better on
minority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and
Korean. To solve the problem of scarcity of datasets on minority languages and
verify the effectiveness of the MiLMo model, this paper constructs a minority
multilingual text classification dataset named MiTC, and trains a word2vec
model for each language. By comparing the word2vec model and the pre-trained
model in the text classification task, this paper provides an optimal scheme
for the downstream task research of minority languages. The final experimental
results show that the performance of the pre-trained model is better than that
of the word2vec model, and it has achieved the best results in minority
multilingual text classification. The multilingual pre-trained model MiLMo,
multilingual word2vec model and multilingual text classification dataset MiTC
are published on <a href="http://milmo.cmli-nlp.com/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. (arXiv:2212.14548v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14548">
<div class="article-summary-box-inner">
<span><p>Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01313">
<div class="article-summary-box-inner">
<span><p>This work provides a formalization of Knowledge Graphs (KGs) as a new class
of graphs that we denote doubly exchangeable attributed graphs, where node and
pairwise (joint 2-node) representations must be equivariant to permutations of
both node ids and edge (&amp; node) attributes (relations &amp; node features).
Double-permutation equivariant KG representations open a new research direction
in KGs. We show that this equivariance imposes a structural representation of
relations that allows neural networks to perform complex logical reasoning
tasks in KGs. Finally, we introduce a general blueprint for such equivariant
representations and test a simple GNN-based double-permutation equivariant
neural architecture that achieve state-of-the-art Hits@10 test accuracy in the
WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately
perform logical reasoning tasks that no existing methods can perform, to the
best of our knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL. (arXiv:2302.05965v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05965">
<div class="article-summary-box-inner">
<span><p>One of the recent best attempts at Text-to-SQL is the pre-trained language
model. Due to the structural property of the SQL queries, the seq2seq model
takes the responsibility of parsing both the schema items (i.e., tables and
columns) and the skeleton (i.e., SQL keywords). Such coupled targets increase
the difficulty of parsing the correct SQL queries especially when they involve
many schema items and logic operators. This paper proposes a ranking-enhanced
encoding and skeleton-aware decoding framework to decouple the schema linking
and the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its
encoder is injected by the most relevant schema items instead of the whole
unordered ones, which could alleviate the schema linking effort during SQL
parsing, and its decoder first generates the skeleton and then the actual SQL
query, which could implicitly constrain the SQL parsing. We evaluate our
proposed framework on Spider and its three robustness variants: Spider-DK,
Spider-Syn, and Spider-Realistic. The experimental results show that our
framework delivers promising performance and robustness. Our code is available
at https://github.com/RUCKBReasoning/RESDSQL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11520">
<div class="article-summary-box-inner">
<span><p>We introduce a new framework, Directional Stimulus Prompting, that uses a
tuneable language model (LM) to provide guidance for the black-box frozen large
language model (LLM) on downstream tasks. Unlike prior work that manually or
automatically finds the optimal prompt for each task, we train a policy LM to
generate discrete tokens as directional stimulus of each input, which is a
hint/cue such as keywords of an article for summarization. The directional
stimulus is then combined with the original input and fed into the LLM to guide
its generation toward the desired target. The policy LM can be trained through
1) supervised learning from annotated data and 2) reinforcement learning from
offline and online rewards to explore directional stimulus that better aligns
LLMs with human preferences. This framework is flexibly applicable to various
LMs and tasks. To verify its effectiveness, we apply our framework to
summarization and dialogue response generation tasks. Experimental results
demonstrate that it can significantly improve LLMs' performance with a small
collection of training data: a T5 (780M) trained with 2,000 samples from the
CNN/Daily Mail dataset improves Codex (175B)'s performance by 9.0% in ROUGE-Avg
scores; only 80 dialogues can boost the combined score by 39.7%, achieving
comparable or even better performance than some fully trained models on the
MultiWOZ dataset. We have made our code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16755">
<div class="article-summary-box-inner">
<span><p>Pretrained language models often generate outputs that are not in line with
human preferences, such as harmful text or factually incorrect summaries.
Recent work approaches the above issues by learning from a simple form of human
feedback: comparisons between pairs of model-generated outputs. However,
comparison feedback only conveys limited information about human preferences.
In this paper, we introduce Imitation learning from Language Feedback (ILF), a
new approach that utilizes more informative language feedback. ILF consists of
three steps that are applied iteratively: first, conditioning the language
model on the input, an initial LM output, and feedback to generate refinements.
Second, selecting the refinement incorporating the most feedback. Third,
finetuning the language model to maximize the likelihood of the chosen
refinement given the input. We show theoretically that ILF can be viewed as
Bayesian Inference, similar to Reinforcement Learning from human feedback. We
evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic
summarization task. Our experiments demonstrate that large language models
accurately incorporate feedback and that finetuning with ILF scales well with
the dataset size, even outperforming finetuning on human summaries. Learning
from both language and comparison feedback outperforms learning from each
alone, achieving human-level summarization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Large Language Models. (arXiv:2303.18223v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18223">
<div class="article-summary-box-inner">
<span><p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Creativity of Large Language Models. (arXiv:2304.00008v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00008">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are revolutionizing several areas of Artificial
Intelligence. One of the most remarkable applications is creative writing,
e.g., poetry or storytelling: the generated outputs are often of astonishing
quality. However, a natural question arises: can LLMs be really considered
creative? In this article we firstly analyze the development of LLMs under the
lens of creativity theories, investigating the key open questions and
challenges. Then, we discuss a set of "easy" and "hard" problems in machine
creativity, presenting them in relation to LLMs. Finally, we examine the
societal impact of these technologies with a particular focus on the creative
industries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study. (arXiv:2304.00723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00723">
<div class="article-summary-box-inner">
<span><p>Evaluating the quality of generated text is a challenging task in natural
language processing. This difficulty arises from the inherent complexity and
diversity of text. Recently, OpenAI's ChatGPT, a powerful large language model
(LLM), has garnered significant attention due to its impressive performance in
various tasks. Therefore, we present this report to investigate the
effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their
use in assessing text quality. We compared three kinds of reference-free
evaluation methods based on ChatGPT or similar LLMs. The experimental results
prove that ChatGPT is capable to evaluate text quality effectively from various
perspectives without reference and demonstrates superior performance than most
existing automatic metrics. In particular, the Explicit Score, which utilizes
ChatGPT to generate a numeric score measuring text quality, is the most
effective and reliable method among the three exploited approaches. However,
directly comparing the quality of two texts using ChatGPT may lead to
suboptimal results. We hope this report will provide valuable insights into
selecting appropriate methods for evaluating text quality with LLMs such as
ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01005">
<div class="article-summary-box-inner">
<span><p>Federated learning is a growing field in the machine learning community due
to its decentralized and private design. Model training in federated learning
is distributed over multiple clients giving access to lots of client data while
maintaining privacy. Then, a server aggregates the training done on these
multiple clients without access to their data, which could be emojis widely
used in any social media service and instant messaging platforms to express
users' sentiments. This paper proposes federated learning-based multilingual
emoji prediction in both clean and attack scenarios. Emoji prediction data have
been crawled from both Twitter and SemEval emoji datasets. This data is used to
train and evaluate different transformer model sizes including a sparsely
activated transformer with either the assumption of clean data in all clients
or poisoned data via label flipping attack in some clients. Experimental
results on these models show that federated learning in either clean or
attacked scenarios performs similarly to centralized training in multilingual
emoji prediction on seen and unseen languages under different data sources and
distributions. Our trained transformers perform better than other techniques on
the SemEval emoji dataset in addition to the privacy as well as distributed
benefits of federated learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01492">
<div class="article-summary-box-inner">
<span><p>The truth is significantly hampered by massive rumors that spread along with
breaking news or popular topics. Since there is sufficient corpus gathered from
the same domain for model training, existing rumor detection algorithms show
promising performance on yesterday's news. However, due to a lack of
substantial training data and prior expert knowledge, they are poor at spotting
rumors concerning unforeseen events, especially those propagated in different
languages (i.e., low-resource regimes). In this paper, we propose a unified
contrastive transfer framework to detect rumors by adapting the features
learned from well-resourced rumor data to that of the low-resourced with only
few-shot annotations. More specifically, we first represent rumor circulated on
social media as an undirected topology for enhancing the interaction of user
opinions, and then train a Multi-scale Graph Convolutional Network via a
unified contrastive paradigm to mine effective clues simultaneously from post
semantics and propagation structure. Our model explicitly breaks the barriers
of the domain and/or language issues, via language alignment and a novel
domain-adaptive contrastive learning mechanism. To well-generalize the
representation learning using a small set of annotated target events, we reveal
that rumor-indicative signal is closely correlated with the uniformity of the
distribution of these events. We design a target-wise contrastive training
mechanism with three event-level data augmentation strategies, capable of
unifying the representations by distinguishing target events. Extensive
experiments conducted on four low-resource datasets collected from real-world
microblog platforms demonstrate that our framework achieves much better
performance than state-of-the-art methods and exhibits a superior capacity for
detecting rumors at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01852">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of ChatGPT and GPT-4,
state-of-the-art large language models (LLM) from the GPT series, and their
prospective applications across diverse domains. Indeed, key innovations such
as large-scale pre-training that captures knowledge across the entire world
wide web, instruction fine-tuning and Reinforcement Learning from Human
Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability
and performance. We performed an in-depth analysis of 194 relevant papers on
arXiv, encompassing trend analysis, word cloud representation, and distribution
analysis across various application domains. The findings reveal a significant
and increasing interest in ChatGPT/GPT-4 research, predominantly centered on
direct natural language processing applications, while also demonstrating
considerable potential in areas ranging from education and history to
mathematics, medicine, and physics. This study endeavors to furnish insights
into ChatGPT's capabilities, potential implications, ethical concerns, and
offer direction for future advancements in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02213">
<div class="article-summary-box-inner">
<span><p>Data has growing significance in exploring cutting-edge materials, and the
number of datasets has been generated either by hand or automated approaches.
However, the materials science field struggles to effectively utilize the
abundance of generated data, especially in applied disciplines where materials
are evaluated based on device performance rather than their properties. This
article presents a new NLP task called structured information inference (SII)
to address the complexities of information extraction at the device level in
materials science. We accomplished this task by tuning GPT-3 on an existing
perovskite solar cell FAIR (Findable, Accessible, Interoperable, Reusable)
dataset with 91.8% F1-score and we updated the dataset with all related
scientific papers up to now. The produced data is formatted and normalized,
enabling its direct utilization as input in subsequent data analysis. This
feature will enable materials scientists to develop their own models by
selecting high-quality review papers within their domain. Furthermore, we
designed experiments to predict solar cells' electrical performance and design
materials or devices with target parameters through LLM. We obtained comparable
performance with traditional machine learning methods without feature
selection, demonstrating the potential of LLMs to learn scientific knowledge
and design new materials like a materials scientist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise-Robust Dense Retrieval via Contrastive Alignment Post Training. (arXiv:2304.03401v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03401">
<div class="article-summary-box-inner">
<span><p>The success of contextual word representations and advances in neural
information retrieval have made dense vector-based retrieval a standard
approach for passage and document ranking. While effective and efficient,
dual-encoders are brittle to variations in query distributions and noisy
queries. Data augmentation can make models more robust but introduces overhead
to training set generation and requires retraining and index regeneration. We
present Contrastive Alignment POst Training (CAPOT), a highly efficient
finetuning method that improves model robustness without requiring index
regeneration, the training set optimization, or alteration. CAPOT enables
robust retrieval by freezing the document encoder while the query encoder
learns to align noisy queries with their unaltered root. We evaluate CAPOT
noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,
finding CAPOT has a similar impact as data augmentation with none of its
overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03512">
<div class="article-summary-box-inner">
<span><p>Multi-document scientific summarization can extract and organize important
information from an abundant collection of papers, arousing widespread
attention recently. However, existing efforts focus on producing lengthy
overviews lacking a clear and logical hierarchy. To alleviate this problem, we
present an atomic and challenging task named Hierarchical Catalogue Generation
for Literature Review (HiCatGLR), which aims to generate a hierarchical
catalogue for a review paper given various references. We carefully construct a
novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD)
with 13.8k literature review catalogues and 120k reference papers, where we
benchmark diverse experiments via the end-to-end and pipeline methods. To
accurately assess the model performance, we design evaluation metrics for
similarity to ground truth from semantics and structure. Besides, our extensive
analyses verify the high quality of our dataset and the effectiveness of our
evaluation metrics. Furthermore, we discuss potential directions for this task
to motivate future research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-11 23:11:14.043066395 UTC">2023-04-11 23:11:14 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>