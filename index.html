<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-28T01:30:00Z">04-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. (arXiv:2304.13731v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13731">
<div class="article-summary-box-inner">
<span><p>The immense scale of the recent large language models (LLM) allows many
interesting properties, such as, instruction- and chain-of-thought-based
fine-tuning, that has significantly improved zero- and few-shot performance in
many natural language processing (NLP) tasks. Inspired by such successes, we
adopt such an instruction-tuned LLM Flan-T5 as the text encoder for
text-to-audio (TTA) generation -- a task where the goal is to generate an audio
from its textual description. The prior works on TTA either pre-trained a joint
text-audio encoder or used a non-instruction-tuned model, such as, T5.
Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms
the state-of-the-art AudioLDM on most metrics and stays comparable on the rest
on AudioCaps test set, despite training the LDM on a 63 times smaller dataset
and keeping the text encoder frozen. This improvement might also be attributed
to the adoption of audio pressure level-based sound mixing for training set
augmentation, whereas the prior methods take a random mix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13734">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) have shown exceptional performance in
various tasks, their (arguably) most prominent drawback is generating
inaccurate or false information with a confident tone. In this paper, we
hypothesize that the LLM's internal state can be used to reveal the
truthfulness of a statement. Therefore, we introduce a simple yet effective
method to detect the truthfulness of LLM-generated statements, which utilizes
the LLM's hidden layer activations to determine the veracity of statements. To
train and evaluate our method, we compose a dataset of true and false
statements in six different topics. A classifier is trained to detect which
statement is true or false based on an LLM's activation values. Specifically,
the classifier receives as input the activation values from the LLM for each of
the statements in the dataset. Our experiments demonstrate that our method for
detecting statement veracity significantly outperforms even few-shot prompting
methods, highlighting its potential to enhance the reliability of LLM-generated
content and its practical applicability in real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine Tuning with Abnormal Examples. (arXiv:2304.13783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13783">
<div class="article-summary-box-inner">
<span><p>Given the prevalence of crowd sourced labor in creating Natural Language
processing datasets, these aforementioned sets have become increasingly large.
For instance, the SQUAD dataset currently sits at over 80,000 records. However,
because the English language is rather repetitive in structure, the
distribution of word frequencies in the SQUAD dataset's contexts are relatively
unchanged. By measuring each sentences distance from the co-variate distance of
frequencies of all sentences in the dataset, we identify 10,500 examples that
create a more uniform distribution for training. While fine-tuning ELECTRA [4]
on this subset of examples reaches better performance to a model trained on all
87,000 examples. Herein we introduce a methodology for systematically pruning
datasets for fine tuning reaching better out of sample performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models. (arXiv:2304.13803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13803">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and can
be finetuned to perform well on diverse tasks such as translation and
multilingual word sense disambiguation (WSD). However, they often struggle at
disambiguating word sense in a zero-shot setting. To better understand this
contrast, we present a new study investigating how well PLMs capture
cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an
extension of word-level translation that prompts the model to translate a given
word in context. We find that as the model size increases, PLMs encode more
cross-lingual word sense knowledge and better use context to improve WLT
performance. Building on C-WLT, we introduce a zero-shot approach for WSD,
tested on 18 languages from the XL-WSD dataset. Our method outperforms fully
supervised baselines on recall for many evaluation languages without additional
training or finetuning. This study presents a first step towards understanding
how to best leverage the cross-lingual knowledge inside PLMs for robust
zero-shot reasoning in any language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13835">
<div class="article-summary-box-inner">
<span><p>Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Structured Seed-Mediated Gold Nanorod Growth Procedures from Literature with GPT-3. (arXiv:2304.13846v1 [physics.app-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13846">
<div class="article-summary-box-inner">
<span><p>Although gold nanorods have been the subject of much research, the pathways
for controlling their shape and thereby their optical properties remain largely
heuristically understood. Although it is apparent that the simultaneous
presence of and interaction between various reagents during synthesis control
these properties, computational and experimental approaches for exploring the
synthesis space can be either intractable or too time-consuming in practice.
This motivates an alternative approach leveraging the wealth of synthesis
information already embedded in the body of scientific literature by developing
tools to extract relevant structured data in an automated, high-throughput
manner. To that end, we present an approach using the powerful GPT-3 language
model to extract structured multi-step seed-mediated growth procedures and
outcomes for gold nanorods from unstructured scientific text. GPT-3 prompt
completions are fine-tuned to predict synthesis templates in the form of JSON
documents from unstructured text input with an overall accuracy of $86\%$. The
performance is notable, considering the model is performing simultaneous entity
recognition and relation extraction. We present a dataset of 11,644 entities
extracted from 1,137 papers, resulting in 268 papers with at least one complete
seed-mediated gold nanorod growth procedure and outcome for a total of 332
complete procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks. (arXiv:2304.13861v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13861">
<div class="article-summary-box-inner">
<span><p>Obtaining and annotating data can be expensive and time-consuming, especially
in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small
labeled datasets with synthetic data via simple prompts, in three different
classification tasks with varying complexity. For each task, we randomly select
a base sample of 500 texts to generate 5,000 new synthetic samples. We explore
two augmentation strategies: one that preserves original label distribution and
another that balances the distribution. Using a progressively larger training
sample size, we train and evaluate a 110M parameter multilingual language model
on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a
zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have
strong zero-shot performance across all tasks. We find that data augmented with
synthetic samples yields a good downstream performance, and particularly aids
in low-resource settings, such as in identifying rare classes. Human-annotated
data exhibits a strong predictive power, overtaking synthetic data in two out
of the three tasks. This finding highlights the need for more complex prompts
for synthetic datasets to consistently surpass human-generated ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Procedural Knowledge across Commonsense Tasks. (arXiv:2304.13867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13867">
<div class="article-summary-box-inner">
<span><p>Stories about everyday situations are an essential part of human
communication, motivating the need to develop AI agents that can reliably
understand these stories. Despite the long list of supervised methods for story
completion and procedural understanding, current AI has no mechanisms to
automatically track and explain procedures in unseen stories. To bridge this
gap, we study the ability of AI models to transfer procedural knowledge to
novel narrative tasks in a transparent manner. We design LEAP: a comprehensive
framework that integrates state-of-the-art modeling architectures, training
regimes, and augmentation strategies based on both natural and synthetic
stories. To address the lack of densely annotated training data, we devise a
robust automatic labeler based on few-shot prompting to enhance the augmented
data. Our experiments with in- and out-of-domain tasks reveal insights into the
interplay of different architectures, training regimes, and augmentation
strategies. LEAP's labeler has a clear positive impact on out-of-domain
datasets, while the resulting dense annotation provides native explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models. (arXiv:2304.13875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13875">
<div class="article-summary-box-inner">
<span><p>In online forums like Reddit, users share their experiences with medical
conditions and treatments, including making claims, asking questions, and
discussing the effects of treatments on their health. Building systems to
understand this information can effectively monitor the spread of
misinformation and verify user claims. The Task-8 of the 2023 International
Workshop on Semantic Evaluation focused on medical applications, specifically
extracting patient experience- and medical condition-related entities from user
posts on social media. The Reddit Health Online Talk (RedHot) corpus contains
posts from medical condition-related subreddits with annotations characterizing
the patient experience and medical conditions. In Subtask-1, patient experience
is characterized by personal experience, questions, and claims. In Subtask-2,
medical conditions are characterized by population, intervention, and outcome.
For the automatic extraction of patient experiences and medical condition
information, as a part of the challenge, we proposed language-model-based
extraction systems that ranked $3^{rd}$ on both subtasks' leaderboards. In this
work, we describe our approach and, in addition, explore the automatic
extraction of this information using domain-specific language models and the
inclusion of external knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Keyphrase Generation: Analysis and Evaluation. (arXiv:2304.13883v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13883">
<div class="article-summary-box-inner">
<span><p>Keyphrase generation aims at generating topical phrases from a given text
either by copying from the original text (present keyphrases) or by producing
new keyphrases (absent keyphrases) that capture the semantic meaning of the
text. Encoder-decoder models are most widely used for this task because of
their capabilities for absent keyphrase generation. However, there has been
little to no analysis on the performance and behavior of such models for
keyphrase generation. In this paper, we study various tendencies exhibited by
three strong models: T5 (based on a pre-trained transformer),
CatSeq-Transformer (a non-pretrained Transformer), and ExHiRD (based on a
recurrent neural network). We analyze prediction confidence scores, model
calibration, and the effect of token position on keyphrases generation.
Moreover, we motivate and propose a novel metric framework, SoftKeyScore, to
evaluate the similarity between two sets of keyphrases by using softscores to
account for partial matching and semantic similarity. We find that SoftKeyScore
is more suitable than the standard F1 metric for evaluating two sets of given
keyphrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Data Augmentation for Context-Dependent Text-to-SQL. (arXiv:2304.13902v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13902">
<div class="article-summary-box-inner">
<span><p>The limited scale of annotated data constraints existing context-dependent
text-to-SQL models because of the complexity of labeling. The data augmentation
method is a commonly used method to solve this problem. However, the data
generated by current augmentation methods often lack diversity. In this paper,
we introduce ConDA, which generates interactive questions and corresponding SQL
results. We designed the SQL dialogue state to enhance the data diversity
through the state transition. Meanwhile, we also present a filter method to
ensure the data quality by a grounding model. Additionally, we utilize a
grounding model to identify and filter low-quality questions that mismatch the
state information. Experimental results on the SParC and CoSQL datasets show
that ConDA boosts the baseline model to achieve an average improvement of
$3.3\%$ on complex questions. Moreover, we analyze the augmented data, which
reveals that the data generated by ConDA are of high quality in both SQL
template hardness and types, turns, and question consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13923">
<div class="article-summary-box-inner">
<span><p>With recent progress in large-scale vision and language representation
learning, Vision Language Pretraining (VLP) models have achieved promising
improvements on various multi-modal downstream tasks. Albeit powerful, these
pre-training models still do not take advantage of world knowledge, which is
implicit in multi-modal data but comprises abundant and complementary
information. In this work, we propose a REtrieval-based knowledge Augmented
Vision Language Pre-training model (REAVL), which retrieves world knowledge
from knowledge graphs (KGs) and incorporates them in vision-language
pre-training. REAVL has two core components: a knowledge retriever that
retrieves knowledge given multi-modal data, and a knowledge-augmented model
that fuses multi-modal data and knowledge. By novelly unifying four
knowledge-aware self-supervised tasks, REAVL promotes the mutual integration of
multi-modal data and knowledge by fusing explicit knowledge with
vision-language pairs for masked multi-modal data modeling and KG relational
reasoning. Empirical experiments show that REAVL achieves new state-of-the-art
performance uniformly on knowledge-based vision-language understanding and
multimodal entity linking tasks, and competitive results on general
vision-language tasks while only using 0.2% pre-training data of the best
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning and Reasoning Multifaceted and Longitudinal Data for Poverty Estimates and Livelihood Capabilities of Lagged Regions in Rural India. (arXiv:2304.13958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13958">
<div class="article-summary-box-inner">
<span><p>Poverty is a multifaceted phenomenon linked to the lack of capabilities of
households to earn a sustainable livelihood, increasingly being assessed using
multidimensional indicators. Its spatial pattern depends on social, economic,
political, and regional variables. Artificial intelligence has shown immense
scope in analyzing the complexities and nuances of poverty. The proposed
project aims to examine the poverty situation of rural India for the period of
1990-2022 based on the quality of life and livelihood indicators. The districts
will be classified into `advanced', `catching up', `falling behind', and
`lagged' regions. The project proposes to integrate multiple data sources,
including conventional national-level large sample household surveys, census
surveys, and proxy variables like daytime, and nighttime data from satellite
images, and communication networks, to name a few, to provide a comprehensive
view of poverty at the district level. The project also intends to examine
causation and longitudinal analysis to examine the reasons for poverty. Poverty
and inequality could be widening in developing countries due to demographic and
growth-agglomerating policies. Therefore, targeting the lagging regions and the
vulnerable population is essential to eradicate poverty and improve the quality
of life to achieve the goal of `zero poverty'. Thus, the study also focuses on
the districts with a higher share of the marginal section of the population
compared to the national average to trace the performance of development
indicators and their association with poverty in these regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Evaluation of POS Taggers: From Wall Street Journal to Fandom Wiki. (arXiv:2304.13989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13989">
<div class="article-summary-box-inner">
<span><p>The Wall Street Journal section of the Penn Treebank has been the de-facto
standard for evaluating POS taggers for a long time, and accuracies over 97\%
have been reported. However, less is known about out-of-domain tagger
performance, especially with fine-grained label sets. Using data from Elder
Scrolls Fandom, a wiki about the \textit{Elder Scrolls} video game universe, we
create a modest dataset for qualitatively evaluating the cross-domain
performance of two POS taggers: the Stanford tagger (Toutanova et al. 2003) and
Bilty (Plank et al. 2016), both trained on WSJ. Our analyses show that
performance on tokens seen during training is almost as good as in-domain
performance, but accuracy on unknown tokens decreases from 90.37% to 78.37%
(Stanford) and 87.84\% to 80.41\% (Bilty) across domains. Both taggers struggle
with proper nouns and inconsistent capitalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13994">
<div class="article-summary-box-inner">
<span><p>We present SweCTRL-Mini, a large Swedish language model that can be used for
inference and fine-tuning on a single consumer-grade GPU. The model is based on
the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),
which means that users of the SweCTRL-Mini model can control the genre of the
generated text by inserting special tokens in the generation prompts.
SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a
set of Swedish novels. In this article, we provide (1) a detailed account of
the utilized training data and text pre-processing steps, to the extent that it
is possible to check whether a specific phrase/source was a part of the
training data, and (2) an evaluation of the model on both discriminative tasks,
using automatic evaluation methods, and generative tasks, using human referees.
We also compare the generative capabilities of the model with those of GPT-3.
SweCTRL-Mini is fully open and available for download.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Origin Tracing and Detecting of LLMs. (arXiv:2304.14072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14072">
<div class="article-summary-box-inner">
<span><p>The extraordinary performance of large language models (LLMs) heightens the
importance of detecting whether the context is generated by an AI system. More
importantly, while more and more companies and institutions release their LLMs,
the origin can be hard to trace. Since LLMs are heading towards the time of
AGI, similar to the origin tracing in anthropology, it is of great importance
to trace the origin of LLMs. In this paper, we first raise the concern of the
origin tracing of LLMs and propose an effective method to trace and detect
AI-generated contexts. We introduce a novel algorithm that leverages the
contrastive features between LLMs and extracts model-wise features to trace the
text origins. Our proposed method works under both white-box and black-box
settings therefore can be widely generalized to detect various LLMs.(e.g. can
be generalized to detect GPT-3 models without the GPT-3 models). Also, our
proposed method requires only limited data compared with the supervised
learning methods and can be extended to trace new-coming model origins. We
construct extensive experiments to examine whether we can trace the origins of
given texts. We provide valuable observations based on the experimental
results, such as the difficulty level of AI origin tracing, and the AI origin
similarities, and call for ethical concerns of LLM providers. We are releasing
all codes and data as a toolkit and benchmark for future AI origin tracing and
detecting studies. \footnote{We are releasing all available resource at
\url{https://github.com/OpenLMLab/}.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14104">
<div class="article-summary-box-inner">
<span><p>Interactions between humans are diverse and context-dependent, but previous
works have treated them as categorical, disregarding the heavy tail of possible
interactions. We propose a new paradigm of learning human-human interactions as
free text from a single still image, allowing for flexibility in modeling the
unlimited space of situations and relationships between people. To overcome the
absence of data labelled specifically for this task, we use knowledge
distillation applied to synthetic caption data produced by a large language
model without explicit supervision. We show that the pseudo-labels produced by
this procedure can be used to train a captioning model to effectively
understand human-human interactions in images, as measured by a variety of
metrics that measure textual and semantic faithfulness and factual groundedness
of our predictions. We further show that our approach outperforms SOTA image
captioning and situation recognition models on this task. We will release our
code and pseudo-labels along with Waldo and Wenda, a manually-curated test set
for still image human-human interaction understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14106">
<div class="article-summary-box-inner">
<span><p>While there are abundant researches about evaluating ChatGPT on natural
language understanding and generation tasks, few studies have investigated how
ChatGPT's behavior changes over time. In this paper, we collect a
coarse-to-fine temporal dataset called ChatLog, consisting of two parts that
update monthly and daily: ChatLog-Monthly is a dataset of 38,730
question-answer pairs collected every month including questions from both the
reasoning and classification tasks. ChatLog-Daily, on the other hand, consists
of ChatGPT's responses to 1000 identical questions for long-form generation
every day. We conduct comprehensive automatic and human evaluation to provide
the evidence for the existence of ChatGPT evolving patterns. We further analyze
the unchanged characteristics of ChatGPT over time by extracting its knowledge
and linguistic features. We find some stable features to improve the robustness
of a RoBERTa-based detector on new versions of ChatGPT. We will continuously
maintain our project at https://github.com/THU-KEG/ChatLog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14108">
<div class="article-summary-box-inner">
<span><p>Large multimodal datasets have been instrumental in recent breakthroughs such
as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive
the same research attention as model architectures or training algorithms. To
address this shortcoming in the machine learning ecosystem, we introduce
DataComp, a benchmark where the training code is fixed and researchers innovate
by proposing new training sets. We provide a testbed for dataset experiments
centered around a new candidate pool of 12.8B image-text pairs from Common
Crawl. Participants in our benchmark design new filtering techniques or curate
new data sources and then evaluate their new dataset by running our
standardized CLIP training code and testing on 38 downstream test sets. Our
benchmark consists of multiple scales, with four candidate pool sizes and
associated compute budgets ranging from 12.8M to 12.8B samples seen during
training. This multi-scale design facilitates the study of scaling trends and
makes the benchmark accessible to researchers with varying resources.
</p>
<p>Our baseline experiments show that the DataComp workflow is a promising way
of improving multimodal datasets. We introduce DataComp-1B, a dataset created
by applying a simple filtering algorithm to the 12.8B candidate pool. The
resulting 1.4B subset enables training a CLIP ViT-L/14 from scratch to 79.2%
zero-shot accuracy on ImageNet. Our new ViT-L/14 model outperforms a larger
ViT-g/14 trained on LAION-2B by 0.7 percentage points while requiring 9x less
training compute. We also outperform OpenAI's CLIP ViT-L/14 by 3.7 percentage
points, which is trained with the same compute budget as our model. These gains
highlight the potential for improving model performance by carefully curating
training sets. We view DataComp-1B as only the first step and hope that
DataComp paves the way toward the next generation of multimodal datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14177">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models, including ChatGPT, have demonstrated
exceptional performance in various natural language generation tasks. However,
there has been limited research evaluating ChatGPT's keyphrase generation
ability, which involves identifying informative phrases that accurately reflect
a document's content. This study seeks to address this gap by comparing
ChatGPT's keyphrase generation performance with state-of-the-art models, while
also testing its potential as a solution for two significant challenges in the
field: domain adaptation and keyphrase generation from long documents. We
conducted experiments on six publicly available datasets from scientific
articles and news domains, analyzing performance on both short and long
documents. Our results show that ChatGPT outperforms current state-of-the-art
models in all tested datasets and environments, generating high-quality
keyphrases that adapt well to diverse domains and document lengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14178">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated impressive zero-shot abilities
on a variety of open-ended tasks, while recent research has also explored the
use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,
a novel training paradigm that equips LLMs with multi-modal abilities through
modularized learning of foundation LLM, a visual knowledge module, and a visual
abstractor module. This approach can support multiple modalities and facilitate
diverse unimodal and multimodal abilities through modality collaboration. The
training paradigm of mPLUG-Owl involves a two-stage method for aligning image
and text, which learns visual knowledge with the assistance of LLM while
maintaining and even improving the generation abilities of LLM. In the first
stage, the visual knowledge module and abstractor module are trained with a
frozen LLM module to align the image and text. In the second stage,
language-only and multi-modal supervised datasets are used to jointly fine-tune
a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing
the visual knowledge module. We carefully build a visually-related instruction
evaluation set OwlEval. Experimental results show that our model outperforms
existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction
and visual understanding ability, multi-turn conversation ability, and
knowledge reasoning ability. Besides, we observe some unexpected and exciting
abilities such as multi-image correlation and scene text understanding, which
makes it possible to leverage it for harder real scenarios, such as vision-only
document comprehension. Our code, pre-trained model, instruction-tuned models,
and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The
online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAP at SemEval-2023 Task 3: Is Less Really More? (Back-)Translation as Data Augmentation Strategies for Detecting Persuasion Techniques. (arXiv:2304.14179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14179">
<div class="article-summary-box-inner">
<span><p>Persuasion techniques detection in news in a multi-lingual setup is
non-trivial and comes with challenges, including little training data. Our
system successfully leverages (back-)translation as data augmentation
strategies with multi-lingual transformer models for the task of detecting
persuasion techniques. The automatic and human evaluation of our augmented data
allows us to explore whether (back-)translation aid or hinder performance. Our
in-depth analyses indicate that both data augmentation strategies boost
performance; however, balancing human-produced and machine-generated data seems
to be crucial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment classification in low-resource languages. (arXiv:2304.14189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14189">
<div class="article-summary-box-inner">
<span><p>Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment
Analysis for African Languages, provides insight into how a multilingual large
language model can be a resource for sentiment analysis in languages not seen
during pretraining. The shared task provides datasets of a variety of African
languages from different language families. The languages are to various
degrees related to languages used during pretraining, and the language data
contain various degrees of code-switching. We experiment with both monolingual
and multilingual datasets for the final fine-tuning, and find that with the
provided datasets that contain samples in the thousands, monolingual
fine-tuning yields the best results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Modular Approach for Multilingual Timex Detection and Normalization using Deep Learning and Grammar-based methods. (arXiv:2304.14221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14221">
<div class="article-summary-box-inner">
<span><p>Detecting and normalizing temporal expressions is an essential step for many
NLP tasks. While a variety of methods have been proposed for detection, best
normalization approaches rely on hand-crafted rules. Furthermore, most of them
have been designed only for English. In this paper we present a modular
multilingual temporal processing system combining a fine-tuned Masked Language
Model for detection, and a grammar-based normalizer. We experiment in Spanish
and English and compare with HeidelTime, the state-of-the-art in multilingual
temporal processing. We obtain best results in gold timex normalization, timex
detection and type recognition, and competitive performance in the combined
TempEval-3 relaxed value metric. A detailed error analysis shows that detecting
only those timexes for which it is feasible to provide a normalization is
highly beneficial in this last metric. This raises the question of which is the
best strategy for timex processing, namely, leaving undetected those timexes
for which is not easy to provide normalization rules or aiming for high
coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Strong Zero-Shot Retriever. (arXiv:2304.14233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14233">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a simple method that applies a large language model
(LLM) to large-scale retrieval in zero-shot scenarios. Our method, Language
language model as Retriever (LameR) is built upon no other neural models but an
LLM, while breaking up brute-force combinations of retrievers with LLMs and
lifting the performance of zero-shot retrieval to be very competitive on
benchmark datasets. Essentially, we propose to augment a query with its
potential answers by prompting LLMs with a composition of the query and the
query's in-domain candidates. The candidates, regardless of correct or wrong,
are obtained by a vanilla retrieval procedure on the target collection. Such
candidates, as a part of prompts, are likely to help LLM generate more precise
answers by pattern imitation or candidate summarization. Even if all the
candidates are wrong, the prompts at least make LLM aware of in-collection
patterns and genres. Moreover, due to the low performance of a self-supervised
retriever, the LLM-based query augmentation becomes less effective as the
retriever bottlenecks the whole pipeline. So, we propose to leverage a
non-parametric lexicon-based method (e.g., BM25) as the retrieval module to
capture query-document overlap in a literal fashion. As such, LameR makes the
retrieval procedure transparent to the LLM, so it circumvents the performance
bottleneck.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who. (arXiv:2304.14238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14238">
<div class="article-summary-box-inner">
<span><p>Automated fact-checking is often presented as an epistemic tool that
fact-checkers, social media consumers, and other stakeholders can use to fight
misinformation. Nevertheless, few papers thoroughly discuss how. We document
this by analysing 100 highly-cited papers, and annotating epistemic elements
related to intended use, i.e., means, ends, and stakeholders. We find that
narratives leaving out some of these aspects are common, that many papers
propose inconsistent means and ends, and that the feasibility of suggested
strategies rarely has empirical backing. We argue that this vagueness actively
hinders the technology from reaching its goals, as it encourages overclaiming,
limits criticism, and prevents stakeholder feedback. Accordingly, we provide
several recommendations for thinking and writing about the use of fact-checking
artefacts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Level Sentiment Analysis (ELSA): An exploratory task survey. (arXiv:2304.14241v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14241">
<div class="article-summary-box-inner">
<span><p>This paper explores the task of identifying the overall sentiment expressed
towards volitional entities (persons and organizations) in a document -- what
we refer to as Entity-Level Sentiment Analysis (ELSA). While identifying
sentiment conveyed towards an entity is well researched for shorter texts like
tweets, we find little to no research on this specific task for longer texts
with multiple mentions and opinions towards the same entity. This lack of
research would be understandable if ELSA can be derived from existing tasks and
models. To assess this, we annotate a set of professional reviews for their
overall sentiment towards each volitional entity in the text. We sample from
data already annotated for document-level, sentence-level, and target-level
sentiment in a multi-domain review corpus, and our results indicate that there
is no single proxy task that provides this overall sentiment we seek for the
entities at a satisfactory level of performance. We present a suite of
experiments aiming to assess the contribution towards ELSA provided by
document-, sentence-, and target-level sentiment analysis, and provide a
discussion of their shortcomings. We show that sentiment in our dataset is
expressed not only with an entity mention as target, but also towards targets
with a sentiment-relevant relation to a volitional entity. In our data, these
relations extend beyond anaphoric coreference resolution, and our findings call
for further research of the topic. Finally, we also present a survey of
previous relevant work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files. (arXiv:2304.14275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14275">
<div class="article-summary-box-inner">
<span><p>Semantic knowledge of part-part and part-whole relationships in assemblies is
useful for a variety of tasks from searching design repositories to the
construction of engineering knowledge bases. In this work we propose that the
natural language names designers use in Computer Aided Design (CAD) software
are a valuable source of such knowledge, and that Large Language Models (LLMs)
contain useful domain-specific information for working with this data as well
as other CAD and engineering-related tasks.
</p>
<p>In particular we extract and clean a large corpus of natural language part,
feature and document names and use this to quantitatively demonstrate that a
pre-trained language model can outperform numerous benchmarks on three
self-supervised tasks, without ever having seen this data before. Moreover, we
show that fine-tuning on the text data corpus further boosts the performance on
all tasks, thus demonstrating the value of the text data which until now has
been largely ignored. We also identify key limitations to using LLMs with text
data alone, and our findings provide a strong motivation for further work into
multi-modal text-geometry models.
</p>
<p>To aid and encourage further work in this area we make all our data and code
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays. (arXiv:2304.14276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14276">
<div class="article-summary-box-inner">
<span><p>Background: Recently, ChatGPT and similar generative AI models have attracted
hundreds of millions of users and become part of the public discourse. Many
believe that such models will disrupt society and will result in a significant
change in the education system and information generation in the future. So
far, this belief is based on either colloquial evidence or benchmarks from the
owners of the models -- both lack scientific rigour.
</p>
<p>Objective: Through a large-scale study comparing human-written versus
ChatGPT-generated argumentative student essays, we systematically assess the
quality of the AI-generated content.
</p>
<p>Methods: A large corpus of essays was rated using standard criteria by a
large number of human experts (teachers). We augment the analysis with a
consideration of the linguistic characteristics of the generated essays.
</p>
<p>Results: Our results demonstrate that ChatGPT generates essays that are rated
higher for quality than human-written essays. The writing style of the AI
models exhibits linguistic characteristics that are different from those of the
human-written essays, e.g., it is characterized by fewer discourse and
epistemic markers, but more nominalizations and greater lexical diversity.
</p>
<p>Conclusions: Our results clearly demonstrate that models like ChatGPT
outperform humans in generating argumentative essays. Since the technology is
readily available for anyone to use, educators must act immediately. We must
re-invent homework and develop teaching concepts that utilize these AI models
in the same way as math utilized the calculator: teach the general concepts
first and then use AI tools to free up time for other learning objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Frame Induction with Deep Metric Learning. (arXiv:2304.14286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14286">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated the usefulness of contextualized word
embeddings in unsupervised semantic frame induction. However, they have also
revealed that generic contextualized embeddings are not always consistent with
human intuitions about semantic frames, which causes unsatisfactory performance
for frame induction based on contextualized embeddings. In this paper, we
address supervised semantic frame induction, which assumes the existence of
frame-annotated data for a subset of predicates in a corpus and aims to build a
frame induction model that leverages the annotated data. We propose a model
that uses deep metric learning to fine-tune a contextualized embedding model,
and we apply the fine-tuned contextualized embeddings to perform semantic frame
induction. Our experiments on FrameNet show that fine-tuning with deep metric
learning considerably improves the clustering evaluation scores, namely, the
B-cubed F-score and Purity F-score, by about 8 points or more. We also
demonstrate that our approach is effective even when the number of training
instances is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14293">
<div class="article-summary-box-inner">
<span><p>Large language models generate fluent texts and can follow natural language
instructions to solve a wide range of tasks without task-specific training.
Nevertheless, it is notoriously difficult to control their generation to
satisfy the various constraints required by different applications. In this
work, we present InstructCTG, a controlled text generation framework that
incorporates different constraints by conditioning on natural language
descriptions and demonstrations of the constraints. In particular, we first
extract the underlying constraints of natural texts through a combination of
off-the-shelf NLP tools and simple heuristics. We then verbalize the
constraints into natural language instructions to form weakly supervised
training data. By prepending natural language descriptions of the constraints
and a few demonstrations, we fine-tune a pre-trained language model to
incorporate various types of constraints. Compared to existing search-based or
score-based methods, InstructCTG is more flexible to different constraint types
and has a much smaller impact on the generation quality and speed because it
does not modify the decoding procedure. Additionally, InstructCTG allows the
model to adapt to new constraints without re-training through the use of
few-shot task generalization and in-context learning abilities of
instruction-tuned language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are State-of-the-Art Evaluators of Code Generation. (arXiv:2304.14317v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14317">
<div class="article-summary-box-inner">
<span><p>Recent advancements in the field of natural language generation have
facilitated the use of large language models to assess the quality of generated
text. Although these models have shown promising results in tasks such as
machine translation and summarization, their applicability in code generation
tasks remains limited without human involvement. The complexity of programming
concepts required for such tasks makes it difficult to develop evaluation
metrics that align with human judgment. Token-matching-based metrics, such as
BLEU, have demonstrated weak correlations with human practitioners in code
generation tasks. Moreover, the utilization of human-written test suites to
evaluate functional correctness can be challenging in domains with low
resources. To overcome these obstacles, we propose a new evaluation framework
based on the GPT-3.5 (\texttt{GPT-3.5-turbo}), for code generation assessments.
Our framework addresses the limitations of existing approaches by achieving
superior correlations with functional correctness and human preferences,
without the need for test oracles or references. We evaluate the efficacy of
our framework on two different tasks and four programming languages, comparing
its performance with the state-of-the-art CodeBERTScore metric, which relies on
a pre-trained model. Our results demonstrate that our framework surpasses
CodeBERTScore, delivering high levels of accuracy and consistency across
various programming languages and tasks. We also make our evaluation framework
and datasets available to the public at
\url{https://github.com/terryyz/llm-code-eval}, encouraging further research in
the evaluation of code generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">q2d: Turning Questions into Dialogs to Teach Models How to Search. (arXiv:2304.14318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14318">
<div class="article-summary-box-inner">
<span><p>One of the exciting capabilities of recent language models for dialog is
their ability to independently search for relevant information to ground a
given dialog response. However, obtaining training data to teach models how to
issue search queries is time and resource consuming. In this work, we propose
q2d: an automatic data generation pipeline that generates information-seeking
dialogs from questions. We prompt a large language model (PaLM) to create
conversational versions of question answering datasets, and use it to improve
query generation models that communicate with external search APIs to ground
dialog responses. Unlike previous approaches which relied on human written
dialogs with search queries, our method allows to automatically generate
query-based grounded dialogs with better control and scale. Our experiments
demonstrate that: (1) For query generation on the QReCC dataset, models trained
on our synthetically-generated data achieve 90%--97% of the performance of
models trained on the human-generated data; (2) We can successfully generate
data for training dialog models in new domains without any existing dialog data
as demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) We
perform a thorough analysis of the generated dialogs showing that humans find
them of high quality and struggle to distinguish them from human-written
dialogs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space. (arXiv:2304.14333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14333">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to learn more about how idiomatic information is
structurally encoded in embeddings, using a structural probing method. We
repurpose an existing English verbal multi-word expression (MWE) dataset to
suit the probing framework and perform a comparative probing study of static
(GloVe) and contextual (BERT) embeddings. Our experiments indicate that both
encode some idiomatic information to varying degrees, but yield conflicting
evidence as to whether idiomaticity is encoded in the vector norm, leaving this
an open question. We also identify some limitations of the used dataset and
highlight important directions for future work in improving its suitability for
a probing analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning. (arXiv:2304.14339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14339">
<div class="article-summary-box-inner">
<span><p>This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
Detection. We used a multi-label contrastive loss for fine-tuning large
pre-trained language models in a multi-lingual setting, achieving very
competitive results: our system was ranked first on the official test set and
on the official shared task leaderboard for five of the six languages for which
we had training data and for which we could perform fine-tuning. Here, we
describe our experimental setup, as well as various ablation studies. The code
of our system is available at https://github.com/QishengL/SemEval2023
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination. (arXiv:2304.14347v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14347">
<div class="article-summary-box-inner">
<span><p>With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our
whole society, rapidly altering the way we think, create and live. For
instance, the GPT integration in Bing has altered our approach to online
searching. While nascent LLMs have many advantages, new legal and ethical risks
are also emerging, stemming in particular from stochastic parrots and
hallucination. The EU is the first and foremost jurisdiction that has focused
on the regulation of AI models. However, the risks posed by the new LLMs are
likely to be underestimated by the emerging EU regulatory paradigm. Therefore,
this correspondence warns that the European AI regulatory paradigm must evolve
further to mitigate such risks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems. (arXiv:2304.14354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14354">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown great potential in solving complex
problems in various fields, including oil and gas engineering and other
industrial engineering disciplines like factory automation, PLC programming
etc. However, automatic identification of strong and weak solutions to
fundamental physics equations governing several industrial processes remain a
challenging task. This paper identifies the limitation of current LLM
approaches, particularly ChatGPT in selected practical problems native to oil
and gas engineering but not exclusively. The performance of ChatGPT in solving
complex problems in oil and gas engineering is discussed and the areas where
LLMs are most effective are presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14364">
<div class="article-summary-box-inner">
<span><p>A wave of new task-based virtual assistants has been fueled by increasingly
powerful large language models, such as GPT-4. These conversational agents can
be customized to serve customer-specific use cases, but ensuring that
agent-generated text conforms to designer-specified rules included in prompt
instructions alone is challenging. Therefore, chatbot designers often use
another model, called a guardrail model, to verify that the agent output aligns
with their rules and constraints. We explore using a distillation approach to
guardrail models to monitor the output of the first model using training data
from GPT-4. We find two crucial steps to our CONSCENDI process:
scenario-augmented generation and contrastive training examples. When
generating conversational data, we generate a set of rule-breaking scenarios,
which enumerate a diverse set of high-level ways a rule can be violated. This
scenario-guided approach produces a diverse training set of rule-violating
conversations, and it provides chatbot designers greater control over the
classification process. We also prompt GPT-4 to also generate contrastive
examples by altering conversations with violations into acceptable
conversations. This set of borderline, contrastive examples enables the
distilled model to learn finer-grained distinctions between what is acceptable
and what is not. We find that CONSCENDI results in guardrail models that
improve over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14391">
<div class="article-summary-box-inner">
<span><p>Language is compositional; an instruction can express multiple relation
constraints to hold among objects in a scene that a robot is tasked to
rearrange. Our focus in this work is an instructable scene rearranging
framework that generalizes to longer instructions and to spatial concept
compositions never seen at training time. We propose to represent
language-instructed spatial concepts with energy functions over relative object
arrangements. A language parser maps instructions to corresponding energy
functions and an open-vocabulary visual-language model grounds their arguments
to relevant objects in the scene. We generate goal scene configurations by
gradient descent on the sum of energy functions, one per language predicate in
the instruction. Local vision-based policies then relocate objects to the
inferred goal locations. We test our model on established instruction-guided
manipulation benchmarks, as well as benchmarks of compositional instructions we
introduce. We show our model can execute highly compositional instructions
zero-shot in simulation and in the real world. It outperforms
language-to-action reactive policies and Large Language Model planners by a
large margin, especially for long instructions that involve compositions of
multiple spatial concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">string2string: A Modern Python Library for String-to-String Algorithms. (arXiv:2304.14395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14395">
<div class="article-summary-box-inner">
<span><p>We introduce string2string, an open-source library that offers a
comprehensive suite of efficient algorithms for a broad range of
string-to-string problems. It includes traditional algorithmic solutions as
well as recent advanced neural approaches to tackle various problems in string
alignment, distance measurement, lexical and semantic search, and similarity
analysis -- along with several helpful visualization tools and metrics to
facilitate the interpretation and analysis of these methods. Notable algorithms
featured in the library include the Smith-Waterman algorithm for pairwise local
alignment, the Hirschberg algorithm for global alignment, the Wagner-Fisher
algorithm for edit distance, BARTScore and BERTScore for similarity analysis,
the Knuth-Morris-Pratt algorithm for lexical search, and Faiss for semantic
search. Besides, it wraps existing efficient and widely-used implementations of
certain frameworks and metrics, such as sacreBLEU and ROUGE, whenever it is
appropriate and suitable. Overall, the library aims to provide extensive
coverage and increased flexibility in comparison to existing libraries for
strings. It can be used for many downstream applications, tasks, and problems
in natural-language processing, bioinformatics, and computational social
sciences. It is implemented in Python, easily installable via pip, and
accessible through a simple API. Source code, documentation, and tutorials are
all available on our GitHub page: https://github.com/stanfordnlp/string2string.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">We're Afraid Language Models Aren't Modeling Ambiguity. (arXiv:2304.14399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14399">
<div class="article-summary-box-inner">
<span><p>Ambiguity is an intrinsic feature of natural language. Managing ambiguity is
a key part of human language understanding, allowing us to anticipate
misunderstanding as communicators and revise our interpretations as listeners.
As language models (LMs) are increasingly employed as dialogue interfaces and
writing aids, handling ambiguous language is critical to their success. We
characterize ambiguity in a sentence by its effect on entailment relations with
another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645
examples with diverse kinds of ambiguity. We design a suite of tests based on
AmbiEnt, presenting the first evaluation of pretrained LMs to recognize
ambiguity and disentangle possible meanings. We find that the task remains
extremely challenging, including for the recent GPT-4, whose generated
disambiguations are considered correct only 32% of the time in human
evaluation, compared to 90% for disambiguations in our dataset. Finally, to
illustrate the value of ambiguity-sensitive tools, we show that a multilabel
NLI model can flag political claims in the wild that are misleading due to
ambiguity. We encourage the field to rediscover the importance of ambiguity for
NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14402">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with instruction finetuning demonstrate superior
generative capabilities. However, these models are resource intensive. To
alleviate this issue, we explore distilling knowledge from instruction-tuned
LLMs to much smaller ones. To this end, we carefully develop a large set of
2.58M instructions based on both existing and newly-generated instructions. In
addition to being sizeable, we design our instructions to cover a broad set of
topics to ensure. A thorough investigation of our instruction data demonstrate
their diversity, and we generate responses for these instructions using
gpt-3.5-turbo. We then exploit the instructions to tune a host of models,
dubbed LaMini-LM, of varying sizes, both from the encoder-decoder as well as
the decoder-only families. We evaluate our models both automatically (on 15
different NLP benchmarks) and manually. Results show that our proposed
LaMini-LM are on par with competitive baselines while being nearly 10 times
smaller in size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViMQ: A Vietnamese Medical Question Dataset for Healthcare Dialogue System Development. (arXiv:2304.14405v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14405">
<div class="article-summary-box-inner">
<span><p>Existing medical text datasets usually take the form of ques- tion and answer
pairs that support the task of natural language gener- ation, but lacking the
composite annotations of the medical terms. In this study, we publish a
Vietnamese dataset of medical questions from patients with sentence-level and
entity-level annotations for the Intent Classification and Named Entity
Recognition tasks. The tag sets for two tasks are in medical domain and can
facilitate the development of task- oriented healthcare chatbots with better
comprehension of queries from patients. We train baseline models for the two
tasks and propose a simple self-supervised training strategy with span-noise
modelling that substan- tially improves the performance. Dataset and code will
be published at https://github.com/tadeephuy/ViMQ
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models. (arXiv:2203.07281v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07281">
<div class="article-summary-box-inner">
<span><p>Providing natural language instructions in prompts is a useful new paradigm
for improving task performance of large language models in a zero-shot setting.
Recent work has aimed to improve such prompts via manual rewriting or
gradient-based tuning. However, manual rewriting is time-consuming and requires
subjective interpretation, while gradient-based tuning can be extremely
computationally demanding for large models and may not be feasible for
API-based models. In this work, we introduce Gradient-free Instructional Prompt
Search (GrIPS), a gradient-free, edit-based search approach for improving task
instructions for large language models. GrIPS takes in instructions designed
for humans and automatically returns an improved, edited prompt, while allowing
for API-based tuning. With InstructGPT models, GrIPS improves the average task
performance by up to 4.30 percentage points on eight classification tasks from
the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and
FLAN-T5). We see improvements for both instruction-only prompts and instruction
+ k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and
purely example-based prompts while controlling for the available compute and
data budget. Further, performance of GrIPS is comparable to select
gradient-based tuning approaches. Qualitatively, we show our edits can simplify
instructions and at times make them incoherent but nonetheless improve
accuracy. Our code is available at: https://github.com/archiki/GrIPS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information. (arXiv:2204.13032v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13032">
<div class="article-summary-box-inner">
<span><p>Time is an important aspect of documents and is used in a range of NLP and IR
tasks. In this work, we investigate methods for incorporating temporal
information during pre-training to further improve the performance on
time-related tasks. Compared with common pre-trained language models like BERT
which utilize synchronic document collections (e.g., BookCorpus and Wikipedia)
as the training corpora, we use long-span temporal news article collection for
building word representations. We introduce BiTimeBERT, a novel language
representation model trained on a temporal collection of news articles via two
new pre-training tasks, which harnesses two distinct temporal signals to
construct time-aware language representations. The experimental results show
that BiTimeBERT consistently outperforms BERT and other existing pre-trained
models with substantial gains on different downstream NLP tasks and
applications for which time is of importance (e.g., the accuracy improvement
over BERT is 155\% on the event time estimation task).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PREME: Preference-based Meeting Exploration through an Interactive Questionnaire. (arXiv:2205.02370v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02370">
<div class="article-summary-box-inner">
<span><p>The recent increase in the volume of online meetings necessitates automated
tools for managing and organizing the material, especially when an attendee has
missed the discussion and needs assistance in quickly exploring it. In this
work, we propose a novel end-to-end framework for generating interactive
questionnaires for preference-based meeting exploration. As a result, users are
supplied with a list of suggested questions reflecting their preferences. Since
the task is new, we introduce an automatic evaluation strategy. Namely, it
measures how much the generated questions via questionnaire are answerable to
ensure factual correctness and covers the source meeting for the depth of
possible exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Parallelism Tradeoff: Limitations of Log-Precision Transformers. (arXiv:2207.00729v4 [cs.CC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00729">
<div class="article-summary-box-inner">
<span><p>Despite their omnipresence in modern NLP, characterizing the computational
power of transformer neural nets remains an interesting open question. We prove
that transformers whose arithmetic precision is logarithmic in the number of
input tokens (and whose feedforward nets are computable using space linear in
their input) can be simulated by constant-depth logspace-uniform threshold
circuits. This provides insight on the power of transformers using known
results in complexity theory. For example, if $\mathsf L \neq \mathsf P$ (i.e.,
not all poly-time problems can be solved using logarithmic space), then
transformers cannot even accurately solve linear equalities or check membership
in an arbitrary context-free grammar with empty productions. Our result
intuitively emerges from the transformer architecture's high parallelizability.
We thus speculatively introduce the idea of a fundamental parallelism tradeoff:
any model architecture as parallelizable as the transformer will obey
limitations similar to it. Since parallelism is key to training models at
massive scale, this suggests a potential inherent weakness of the scaling
paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06349">
<div class="article-summary-box-inner">
<span><p>Closed-book question answering (QA) requires a model to directly answer an
open-domain question without access to any external knowledge. Prior work on
closed-book QA either directly finetunes or prompts a pretrained language model
(LM) to leverage the stored knowledge. However, they do not fully exploit the
parameterized knowledge. To address this issue, we propose a two-stage,
closed-book QA framework which employs a coarse-to-fine approach to extract
relevant knowledge and answer a question. Our approach first generates a
related context for a given question by prompting a pretrained LM. We then
prompt the same LM for answer prediction using the generated context and the
question. Additionally, to eliminate failure caused by context uncertainty, we
marginalize over generated contexts. Experimental results on three QA
benchmarks show that our method significantly outperforms previous closed-book
QA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book
methods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Our
method is able to better exploit the stored knowledge in pretrained LMs without
adding extra learnable parameters or needing finetuning, and paves the way for
hybrid models that integrate pretrained LMs with external knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Communication breakdown: On the low mutual intelligibility between human and neural captioning. (arXiv:2210.11512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11512">
<div class="article-summary-box-inner">
<span><p>We compare the 0-shot performance of a neural caption-based image retriever
when given as input either human-produced captions or captions generated by a
neural captioner. We conduct this comparison on the recently introduced
ImageCoDe data-set (Krojer et al., 2022) which contains hard distractors nearly
identical to the images to be retrieved. We find that the neural retriever has
much higher performance when fed neural rather than human captions, despite the
fact that the former, unlike the latter, were generated without awareness of
the distractors that make the task hard. Even more remarkably, when the same
neural captions are given to human subjects, their retrieval performance is
almost at chance level. Our results thus add to the growing body of evidence
that, even when the ``language'' of neural models resembles English, this
superficial resemblance might be deeply misleading.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings. (arXiv:2210.12623v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12623">
<div class="article-summary-box-inner">
<span><p>Zero-resource cross-lingual transfer approaches aim to apply supervised
models from a source language to unlabelled target languages. In this paper we
perform an in-depth study of the two main techniques employed so far for
cross-lingual zero-resource sequence labelling, based either on data or model
transfer. Although previous research has proposed translation and annotation
projection (data-based cross-lingual transfer) as an effective technique for
cross-lingual sequence labelling, in this paper we experimentally demonstrate
that high capacity multilingual language models applied in a zero-shot
(model-based cross-lingual transfer) setting consistently outperform data-based
cross-lingual transfer approaches. A detailed analysis of our results suggests
that this might be due to important differences in language use. More
specifically, machine translation often generates a textual signal which is
different to what the models are exposed to when using gold standard data,
which affects both the fine-tuning and evaluation processes. Our results also
indicate that data-based cross-lingual transfer approaches remain a competitive
option when high-capacity multilingual language models are not available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratizing Neural Machine Translation with OPUS-MT. (arXiv:2212.01936v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01936">
<div class="article-summary-box-inner">
<span><p>This paper presents the OPUS ecosystem with a focus on the development of
open machine translation models and tools, and their integration into end-user
applications, development platforms and professional workflows. We discuss our
on-going mission of increasing language coverage and translation quality, and
also describe on-going work on the development of modular translation models
and speed-optimized compact solutions for real-time translation on regular
desktops and small devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12050">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world. However, if initialized with knowledge of high-level
subgoals and transitions between subgoals, RL agents could utilize this
Abstract World Model (AWM) for planning and exploration. We propose using
few-shot large language models (LLMs) to hypothesize an AWM, that will be
verified through world experience, to improve sample efficiency of RL agents.
Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft
in two phases: (1) the Dream phase where the agent uses an LLM to decompose a
task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase
where the agent learns a modular policy for each subgoal and verifies or
corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and
then verifying the AWM based on agent experience not only increases sample
efficiency over contemporary methods by an order of magnitude but is also
robust to and corrects errors in the LLM, successfully blending noisy
internet-scale information from LLMs with knowledge grounded in environment
dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13808">
<div class="article-summary-box-inner">
<span><p>Table-based reasoning has shown remarkable progress in combining deep models
with discrete reasoning, which requires reasoning over both free-form natural
language (NL) questions and structured tabular data. However, previous
table-based reasoning solutions usually suffer from significant performance
degradation on huge evidence (tables). In addition, most existing methods
struggle to reason over complex questions since the required information is
scattered in different places. To alleviate the above challenges, we exploit
large language models (LLMs) as decomposers for effective table-based
reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence
(a small table) to mitigate the interference of useless information for table
reasoning; and (ii) decompose complex questions into simpler sub-questions for
text reasoning. Specifically, we first use the LLMs to break down the evidence
(tables) involved in the current question, retaining the relevant evidence and
excluding the remaining irrelevant evidence from the huge table. In addition,
we propose a "parsing-execution-filling" strategy to alleviate the
hallucination dilemma of the chain of thought by decoupling logic and numerical
computation in each step. Extensive experiments show that our method can
effectively leverage decomposed evidence and questions and outperforms the
strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,
our model outperforms human performance for the first time on the TabFact
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06675">
<div class="article-summary-box-inner">
<span><p>We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. The implementation of Lion is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PheME: A deep ensemble framework for improving phenotype prediction from multi-modal data. (arXiv:2303.10794v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10794">
<div class="article-summary-box-inner">
<span><p>Detailed phenotype information is fundamental to accurate diagnosis and risk
estimation of diseases. As a rich source of phenotype information, electronic
health records (EHRs) promise to empower diagnostic variant interpretation.
However, how to accurately and efficiently extract phenotypes from the
heterogeneous EHR data remains a challenge. In this work, we present PheME, an
Ensemble framework using Multi-modality data of structured EHRs and
unstructured clinical notes for accurate Phenotype prediction. Firstly, we
employ multiple deep neural networks to learn reliable representations from the
sparse structured EHR data and redundant clinical notes. A multi-modal model
then aligns multi-modal features onto the same latent space to predict
phenotypes. Secondly, we leverage ensemble learning to combine outputs from
single-modal models and multi-modal models to improve phenotype predictions. We
choose seven diseases to evaluate the phenotyping performance of the proposed
framework. Experimental results show that using multi-modal data significantly
improves phenotype prediction in all diseases, the proposed ensemble learning
framework can further boost the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Large Language Models. (arXiv:2303.18223v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18223">
<div class="article-summary-box-inner">
<span><p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11015">
<div class="article-summary-box-inner">
<span><p>We study the problem of decomposing a complex text-to-sql task into smaller
sub-tasks and how such a decomposition can significantly improve the
performance of Large Language Models (LLMs) in the reasoning process. There is
currently a significant gap between the performance of fine-tuned models and
prompting approaches using LLMs on challenging text-to-sql datasets such as
Spider. We show that SQL queries, despite their declarative structure, can be
broken down into sub-problems and the solutions of those sub-problems can be
fed into LLMs to significantly improve their performance. Our experiments with
three LLMs show that this approach consistently improves their performance by
roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even
beating large fine-tuned models on the holdout Spider dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Networks for Text Classification: A Survey. (arXiv:2304.11534v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11534">
<div class="article-summary-box-inner">
<span><p>Text Classification is the most essential and fundamental problem in Natural
Language Processing. While numerous recent text classification models applied
the sequential deep learning technique, graph neural network-based models can
directly deal with complex structured text data and exploit global information.
Many real text classification applications can be naturally cast into a graph,
which captures words, documents, and corpus global features. In this survey, we
bring the coverage of methods up to 2023, including corpus-level and
document-level graph neural networks. We discuss each of these methods in
detail, dealing with the graph construction mechanisms and the graph-based
learning process. As well as the technological survey, we look at issues behind
and future directions addressed in text classification using graph neural
networks. We also cover datasets, evaluation metrics, and experiment design and
present a summary of published performance on the publicly available
benchmarks. Note that we present a comprehensive comparison between different
techniques and identify the pros and cons of various evaluation metrics in this
survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12395">
<div class="article-summary-box-inner">
<span><p>Semantic answer type prediction (SMART) is known to be a useful step towards
effective question answering (QA) systems. The SMART task involves predicting
the top-$k$ knowledge graph (KG) types for a given natural language question.
This is challenging due to the large number of types in KGs. In this paper, we
propose use of extreme multi-label classification using Transformer models
(XBERT) by clustering KG types using structural and semantic features based on
question text. We specifically improve the clustering stage of the XBERT
pipeline using textual and structural features derived from KGs. We show that
these features can improve end-to-end performance for the SMART task, and yield
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13273">
<div class="article-summary-box-inner">
<span><p>With the development of Vision-Language Pre-training Models (VLPMs)
represented by CLIP and ALIGN, significant breakthroughs have been achieved for
association-based visual tasks such as image classification and image-text
retrieval by the zero-shot capability of CLIP without fine-tuning. However,
CLIP is hard to apply to generation-based tasks. This is due to the lack of
decoder architecture and pre-training tasks for generation. Although previous
works have created generation capacity for CLIP through additional language
models, a modality gap between the CLIP representations of different modalities
and the inability of CLIP to model the offset of this gap, which fails the
concept to transfer across modalities. To solve the problem, we try to map
images/videos to the language modality and generate captions from the language
modality. In this paper, we propose the K-nearest-neighbor Cross-modality
Mapping (Knight), a zero-shot method from association to generation. With
text-only unsupervised training, Knight achieves state-of-the-art performance
in zero-shot methods for image captioning and video captioning. Our code is
available at https://github.com/junyangwang0410/Knight.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13712">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive and practical guide for practitioners and
end-users working with Large Language Models (LLMs) in their downstream natural
language processing (NLP) tasks. We provide discussions and insights into the
usage of LLMs from the perspectives of models, data, and downstream tasks.
Firstly, we offer an introduction and brief summary of current GPT- and
BERT-style LLMs. Then, we discuss the influence of pre-training data, training
data, and test data. Most importantly, we provide a detailed discussion about
the use and non-use cases of large language models for various natural language
processing tasks, such as knowledge-intensive tasks, traditional natural
language understanding tasks, natural language generation tasks, emergent
abilities, and considerations for specific tasks.We present various use cases
and non-use cases to illustrate the practical applications and limitations of
LLMs in real-world scenarios. We also try to understand the importance of data
and the specific challenges associated with each NLP task. Furthermore, we
explore the impact of spurious biases on LLMs and delve into other essential
considerations, such as efficiency, cost, and latency, to ensure a
comprehensive understanding of deploying LLMs in practice. This comprehensive
guide aims to provide researchers and practitioners with valuable insights and
best practices for working with LLMs, thereby enabling the successful
implementation of these models in a wide range of NLP tasks. A curated list of
practical guide resources of LLMs, regularly updated, can be found at
\url{https://github.com/Mooler0410/LLMsPracticalGuide}.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-30 23:11:07.288969074 UTC">2023-04-30 23:11:07 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>