<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-05T01:30:00Z">10-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations. (arXiv:2310.02281v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02281">
<div class="article-summary-box-inner">
<span><p>Speech Emotion recognition (SER) in call center conversations has emerged as
a valuable tool for assessing the quality of interactions between clients and
agents. In contrast to controlled laboratory environments, real-life
conversations take place under uncontrolled conditions and are subject to
contextual factors that influence the expression of emotions. In this paper, we
present our approach to constructing a large-scale reallife dataset (CusEmo)
for continuous SER in customer service call center conversations. We adopted
the dimensional emotion annotation approach to capture the subtlety,
complexity, and continuity of emotions in real-life call center conversations,
while annotating contextual information. The study also addresses the
challenges encountered during the application of the End-to-End (E2E) SER
system to the dataset, including determining the appropriate label sampling
rate and input segment length, as well as integrating contextual information
(interlocutor's gender and empathy level) with different weights using
multitask learning. The result shows that incorporating the empathy level
information improved the model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02304">
<div class="article-summary-box-inner">
<span><p>Several recent advances in AI systems (e.g., Tree-of-Thoughts and
Program-Aided Language Models) solve problems by providing a "scaffolding"
program that structures multiple calls to language models to generate better
outputs. A scaffolding program is written in a programming language such as
Python. In this work, we use a language-model-infused scaffolding program to
improve itself. We start with a seed "improver" that improves an input program
according to a given utility function by querying a language model several
times and returning the best solution. We then run this seed improver to
improve itself. Across a small set of downstream tasks, the resulting improved
improver generates programs with significantly better performance than its seed
improver. Afterward, we analyze the variety of self-improvement strategies
proposed by the language model, including beam search, genetic algorithms, and
simulated annealing. Since the language models themselves are not altered, this
is not full recursive self-improvement. Nonetheless, it demonstrates that a
modern language model, GPT-4 in our proof-of-concept experiments, is capable of
writing code that can call itself to improve itself. We critically consider
concerns around the development of self-improving technologies and evaluate the
frequency with which the generated code bypasses a sandbox.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02357">
<div class="article-summary-box-inner">
<span><p>The fundamental problem in toxicity detection task lies in the fact that the
toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in
the field, uses a definition of toxicity given by Dixon et al. - 'rude,
disrespectful, or unreasonable language that is likely to make someone leave a
discussion'. One can instantly see the issue with this definition, as it gives
no quantitative measure of the toxicity and operates with highly subjective
cultural terms. Despite all vagueness and flaws, this definition is de-facto
widely used by many researchers. In this work we suggest quantative
stress-based defenition for the toxicity that overcomes existing shortcomings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks. (arXiv:2310.02372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02372">
<div class="article-summary-box-inner">
<span><p>Key value pair (KVP) extraction or Named Entity Recognition(NER) from
visually rich documents has been an active area of research in document
understanding and data extraction domain. Several transformer based models such
as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art
results. However, addition of even a single new class to the existing model
requires (a) re-annotation of entire training dataset to include this new class
and (b) retraining the model again. Both of these issues really slow down the
deployment of updated model. \\ We present \textbf{ProtoNER}: Prototypical
Network based end-to-end KVP extraction model that allows addition of new
classes to an existing model while requiring minimal number of newly annotated
training samples. The key contributions of our model are: (1) No dependency on
dataset used for initial training of the model, which alleviates the need to
retain original training dataset for longer duration as well as data
re-annotation which is very time consuming task, (2) No intermediate synthetic
data generation which tends to add noise and results in model's performance
degradation, and (3) Hybrid loss function which allows model to retain
knowledge about older classes as well as learn about newly added classes.\\
Experimental results show that ProtoNER finetuned with just 30 samples is able
to achieve similar results for the newly added classes as that of regular model
finetuned with 2600 samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02374">
<div class="article-summary-box-inner">
<span><p>Conversational Health Agents (CHAs) are interactive systems designed to
enhance personal healthcare services by engaging in empathetic conversations
and processing multimodal data. While current CHAs, especially those utilizing
Large Language Models (LLMs), primarily focus on conversation, they often lack
comprehensive agent capabilities. This includes the ability to access personal
user health data from wearables, 24/7 data collection sources, and electronic
health records, as well as integrating the latest published health insights and
connecting with established multimodal data analysis tools. We are developing a
framework to empower CHAs by equipping them with critical thinking, knowledge
acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs,
seamlessly integrates healthcare tools, enables multilingual and multimodal
conversations, and interfaces with a variety of user data analysis tools. We
illustrate its proficiency in handling complex healthcare tasks, such as stress
level estimation, showcasing the agent's cognitive and operational
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Speech Recognition with N-Skipgram and Positional Unigram Matching. (arXiv:2310.02382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02382">
<div class="article-summary-box-inner">
<span><p>Training unsupervised speech recognition systems presents challenges due to
GAN-associated instability, misalignment between speech and text, and
significant memory demands. To tackle these challenges, we introduce a novel
ASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams
(up to N=3) combined with positional unigram statistics gathered from a small
batch of samples. Evaluated on the TIMIT benchmark, our model showcases
competitive performance in ASR and phoneme segmentation tasks. Access our
publicly available code at https://github.com/lwang114/GraphUnsupASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis of Ethereum-based Decentralised Applications. (arXiv:2310.02408v1 [cs.IT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02408">
<div class="article-summary-box-inner">
<span><p>This paper presents MindTheDApp, a toolchain designed specifically for the
structural analysis of Ethereum-based Decentralized Applications (DApps), with
a distinct focus on a complex network-driven approach. Unlike existing tools,
our toolchain combines the power of ANTLR4 and Abstract Syntax Tree (AST)
traversal techniques to transform the architecture and interactions within
smart contracts into a specialized bipartite graph. This enables advanced
network analytics to highlight operational efficiencies within the DApp's
architecture.
</p>
<p>The bipartite graph generated by the proposed tool comprises two sets of
nodes: one representing smart contracts, interfaces, and libraries, and the
other including functions, events, and modifiers. Edges in the graph connect
functions to smart contracts they interact with, offering a granular view of
interdependencies and execution flow within the DApp. This network-centric
approach allows researchers and practitioners to apply complex network theory
in understanding the robustness, adaptability, and intricacies of decentralized
systems.
</p>
<p>Our work contributes to the enhancement of security in smart contracts by
allowing the visualisation of the network, and it provides a deep understanding
of the architecture and operational logic within DApps. Given the growing
importance of smart contracts in the blockchain ecosystem and the emerging
application of complex network theory in technology, our toolchain offers a
timely contribution to both academic research and practical applications in the
field of blockchain technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02409">
<div class="article-summary-box-inner">
<span><p>Standard Transformer-based language models (LMs) scale poorly to long
contexts. We propose a solution based on dynamic contextual compression, which
extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks
to decoder-only LMs. Our method models history as compressed "nuggets" which
are trained to allow for reconstruction, and it can be initialized with
off-the-shelf models such as LLaMA. We demonstrate through experiments in
language modeling, question answering, and summarization that Nugget2D retains
capabilities in these tasks, while drastically reducing the overhead during
decoding in terms of time and space. For example, in the experiments of
autoencoding, Nugget2D can shrink context at a 20x compression ratio with a
BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. (arXiv:2310.02410v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02410">
<div class="article-summary-box-inner">
<span><p>Large Mixture of Experts (MoE) models could achieve state-of-the-art quality
on various language tasks, including machine translation task, thanks to the
efficient model scaling capability with expert parallelism. However, it has
brought a fundamental issue of larger memory consumption and increased memory
bandwidth bottleneck at deployment time. In this paper, we propose Mixture of
Quantized Experts (MoQE) which is a simple weight-only quantization method
applying ultra low-bit down to 2-bit quantizations only to expert weights for
mitigating the increased memory and latency issues of MoE models. We show that
low-bit quantization together with the MoE architecture delivers a reliable
model performance while reducing the memory size significantly even without any
additional training in most cases. In particular, expert layers in MoE models
are much more robust to the quantization than conventional feedforward networks
(FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit
expert weights can deliver better model performance than the dense model
trained on the same dataset. As a result of low-bit quantization, we show the
model size can be reduced by 79.6% of the original half precision floating
point (fp16) MoE model. Combined with an optimized GPU runtime implementation,
it also achieves 1.24X speed-up on A100 GPUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can a student Large Language Model perform as well as it's teacher?. (arXiv:2310.02421v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02421">
<div class="article-summary-box-inner">
<span><p>The burgeoning complexity of contemporary deep learning models, while
achieving unparalleled accuracy, has inadvertently introduced deployment
challenges in resource-constrained environments. Knowledge distillation, a
technique aiming to transfer knowledge from a high-capacity "teacher" model to
a streamlined "student" model, emerges as a promising solution to this dilemma.
This paper provides a comprehensive overview of the knowledge distillation
paradigm, emphasizing its foundational principles such as the utility of soft
labels and the significance of temperature scaling. Through meticulous
examination, we elucidate the critical determinants of successful distillation,
including the architecture of the student model, the caliber of the teacher,
and the delicate balance of hyperparameters. While acknowledging its profound
advantages, we also delve into the complexities and challenges inherent in the
process. Our exploration underscores knowledge distillation's potential as a
pivotal technique in optimizing the trade-off between model performance and
deployment efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions. (arXiv:2310.02431v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02431">
<div class="article-summary-box-inner">
<span><p>Users seek security &amp; privacy (S&amp;P) advice from online resources, including
trusted websites and content-sharing platforms. These resources help users
understand S&amp;P technologies and tools and suggest actionable strategies. Large
Language Models (LLMs) have recently emerged as trusted information sources.
However, their accuracy and correctness have been called into question. Prior
research has outlined the shortcomings of LLMs in answering multiple-choice
questions and user ability to inadvertently circumvent model restrictions
(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable
S&amp;P advice is not well-explored. In this paper, we measure their ability to
refute popular S&amp;P misconceptions that the general public holds. We first study
recent academic literature to curate a dataset of over a hundred S&amp;P-related
misconceptions across six different topics. We then query two popular LLMs
(Bard and ChatGPT) and develop a labeling guide to evaluate their responses to
these misconceptions. To comprehensively evaluate their responses, we further
apply three strategies: query each misconception multiple times, generate and
query their paraphrases, and solicit source URLs of the responses. Both models
demonstrate, on average, a 21.3% non-negligible error rate, incorrectly
supporting popular S&amp;P misconceptions. The error rate increases to 32.6% when
we repeatedly query LLMs with the same or paraphrased misconceptions. We also
expose that models may partially support a misconception or remain
noncommittal, refusing a firm stance on misconceptions. Our exploration of
information sources for responses revealed that LLMs are susceptible to
providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to
unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions. (arXiv:2310.02439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02439">
<div class="article-summary-box-inner">
<span><p>We propose novel evaluations for mathematical reasoning capabilities of Large
Language Models (LLMs) based on mathematical misconceptions. Our primary
approach is to simulate LLMs as a novice learner and an expert tutor, aiming to
identify the incorrect answer to math question resulted from a specific
misconception and to recognize the misconception(s) behind an incorrect answer,
respectively. Contrary to traditional LLMs-based mathematical evaluations that
focus on answering math questions correctly, our approach takes inspirations
from principles in educational learning sciences. We explicitly ask LLMs to
mimic a novice learner by answering questions in a specific incorrect manner
based on incomplete knowledge; and to mimic an expert tutor by identifying
misconception(s) corresponding to an incorrect answer to a question. Using
simple grade-school math problems, our experiments reveal that, while LLMs can
easily answer these questions correctly, they struggle to identify 1) the
incorrect answer corresponding to specific incomplete knowledge
(misconceptions); 2) the misconceptions that explain particular incorrect
answers. Our study indicates new opportunities for enhancing LLMs' math
reasoning capabilities, especially on developing robust student simulation and
expert tutoring models in the educational applications such as intelligent
tutoring systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02446">
<div class="article-summary-box-inner">
<span><p>AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes. (arXiv:2310.02451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02451">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) methods have been broadly applied to
clinical tasks. Machine learning and deep learning approaches have been used to
improve the performance of clinical NLP. However, these approaches require
sufficiently large datasets for training, and trained models have been shown to
transfer poorly across sites. These issues have led to the promotion of data
collection and integration across different institutions for accurate and
portable models. However, this can introduce a form of bias called confounding
by provenance. When source-specific data distributions differ at deployment,
this may harm model performance. To address this issue, we evaluate the utility
of backdoor adjustment for text classification in a multi-site dataset of
clinical notes annotated for mentions of substance abuse. Using an evaluation
framework devised to measure robustness to distributional shifts, we assess the
utility of backdoor adjustment. Our results indicate that backdoor adjustment
can effectively mitigate for confounding shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. (arXiv:2310.02457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02457">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the concept of "alignment" in large language models
(LLMs) through the lens of post-structuralist socio-political theory,
specifically examining its parallels to empty signifiers. To establish a shared
vocabulary around how abstract concepts of alignment are operationalised in
empirical datasets, we propose a framework that demarcates: 1) which dimensions
of model behaviour are considered important, then 2) how meanings and
definitions are ascribed to these dimensions, and by whom. We situate existing
empirical literature and provide guidance on deciding which paradigm to follow.
Through this framework, we aim to foster a culture of transparency and critical
evaluation, aiding the community in navigating the complexities of aligning
LLMs with human populations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02469">
<div class="article-summary-box-inner">
<span><p>The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers. (arXiv:2310.02489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02489">
<div class="article-summary-box-inner">
<span><p>Memory constraint of always-on devices is one of the major concerns when
deploying speech processing models on these devices. While larger models
trained with sufficiently large amount of data generally perform better, making
them fit in the device memory is a demanding challenge. In this paper, we aim
to reduce model size by reparameterizing model weights across Transformer
encoder layers and assuming a special weight composition and structure. More
specifically, inspired by ResNet and the more recent LoRA work, we propose an
approach named ResidualTransformer, where each weight matrix in a Transformer
layer comprises 1) a shared full-rank component with its adjacent layers, and
2) a unique low-rank component to itself. The low-rank matrices only account
for a small amount of model size increase. In addition, we add diagonal weight
matrices to improve modeling capacity of the low-rank matrices. Experiments of
our 10k-hour speech recognition and speech translation tasks show that the
Transformer encoder size can be reduced by ~3X with very slight performance
degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CITING: Large Language Models Create Curriculum for Instruction Tuning. (arXiv:2310.02527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02527">
<div class="article-summary-box-inner">
<span><p>The recent advancement of large language models (LLMs) has been achieved
through a combo of instruction tuning and human alignment. However, building
manually crafted instruction datasets and performing human alignment become the
bottleneck for scaling the development of LLMs. In this paper, we exploit the
idea of leveraging AI models in lieu of humans as the teacher to train student
LLMs. Our method is inspired by how human students refine their writing skills
by following the rubrics and learning from the revisions offered by their
tutors. Specifically, we employ a teacher LLM to create a curriculum for
instruction tuning of the student LLM, namely Curriculum Instruction TunING
(CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics
for evaluating the answers corresponding to various types of questions, and (2)
the student LLM learns to follow the rubrics and perform self-correction from
the revision made by the teacher. We further iteratively carry out it to embody
the procedure of CITING. We compare CITING to a series of state-of-the-art
baselines on four datasets. Our method demonstrates strong improvement in terms
of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically,
it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1%
over RRHF, and 76.3% over RAFT, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NOLA: Networks as Linear Combination of Low Rank Random Basis. (arXiv:2310.02556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02556">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have recently gained popularity due to their
impressive few-shot performance across various downstream tasks. However,
fine-tuning all parameters and storing a unique model for each downstream task
or domain becomes impractical because of the massive size of checkpoints (e.g.,
350GB in GPT-3). Current literature, such as LoRA, showcases the potential of
low-rank modifications to the original weights of an LLM, enabling efficient
adaptation and storage for task-specific models. These methods can reduce the
number of parameters needed to fine-tune an LLM by several orders of magnitude.
Yet, these methods face two primary limitations: 1) the parameter reduction is
lower-bounded by the rank one decomposition, and 2) the extent of reduction is
heavily influenced by both the model architecture and the chosen rank. For
instance, in larger models, even a rank one decomposition might exceed the
number of parameters truly needed for adaptation. In this paper, we introduce
NOLA, which overcomes the rank one lower bound present in LoRA. It achieves
this by re-parameterizing the low-rank matrices in LoRA using linear
combinations of randomly generated matrices (basis) and optimizing the linear
mixture coefficients only. This approach allows us to decouple the number of
trainable parameters from both the choice of rank and the network architecture.
We present adaptation results using GPT-2 and ViT in natural language and
computer vision tasks. NOLA performs as well as, or better than models with
equivalent parameter counts. Furthermore, we demonstrate that we can halve the
parameters in larger models compared to LoRA with rank one, without sacrificing
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02567">
<div class="article-summary-box-inner">
<span><p>8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding. (arXiv:2310.02594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02594">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) typically includes two subtasks: intent
detection and slot filling. Currently, it has achieved great success in
high-resource languages, but it still remains challenging in low-resource
languages due to the scarcity of labeled training data. Hence, there is a
growing interest in zero-shot cross-lingual SLU. Despite of the success of
existing zero-shot cross-lingual SLU models, most of them neglect to achieve
the mutual guidance between intent and slots. To address this issue, we propose
an Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual
Spoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance.
Specifically, we not only apply intra-knowledge distillation between intent
predictions or slot predictions of the same utterance in different languages,
but also apply inter-knowledge distillation between intent predictions and slot
predictions of the same utterance. Our experimental results demonstrate that
our proposed framework significantly improves the performance compared with the
strong baselines and achieves the new state-of-the-art performance on the
MultiATIS++ dataset, obtaining a significant improvement over the previous best
model in overall accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation. (arXiv:2310.02655v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02655">
<div class="article-summary-box-inner">
<span><p>Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk
management strategies. As the volume of CTI reports continues to surge, the
demand for automated tools to streamline report generation becomes increasingly
apparent. While Natural Language Processing techniques have shown potential in
handling text data, they often struggle to address the complexity of diverse
data sources and their intricate interrelationships. Moreover, established
paradigms like STIX have emerged as de facto standards within the CTI
community, emphasizing the formal categorization of entities and relations to
facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic
Generation of Intelligence Reports), a transformative Natural Language
Generation tool specifically designed to address the pressing challenges in the
realm of CTI reporting. AGIR's primary objective is to empower security
analysts by automating the labor-intensive task of generating comprehensive
intelligence reports from formal representations of entity graphs. AGIR
utilizes a two-stage pipeline by combining the advantages of template-based
approaches and the capabilities of Large Language Models such as ChatGPT. We
evaluate AGIR's report generation capabilities both quantitatively and
qualitatively. The generated reports accurately convey information expressed
through formal language, achieving a high recall value (0.99) without
introducing hallucination. Furthermore, we compare the fluency and utility of
the reports with state-of-the-art approaches, showing how AGIR achieves higher
scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.
By using our tool, we estimate that the report writing time is reduced by more
than 40%, therefore streamlining the CTI production of any organization and
contributing to the automation of several CTI tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LC-Score: Reference-less estimation of Text Comprehension Difficulty. (arXiv:2310.02754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02754">
<div class="article-summary-box-inner">
<span><p>Being able to read and understand written text is critical in a digital era.
However, studies shows that a large fraction of the population experiences
comprehension issues. In this context, further initiatives in accessibility are
required to improve the audience text comprehension. However, writers are
hardly assisted nor encouraged to produce easy-to-understand content. Moreover,
Automatic Text Simplification (ATS) model development suffers from the lack of
metric to accurately estimate comprehension difficulty We present
\textsc{LC-Score}, a simple approach for training text comprehension metric for
any French text without reference \ie predicting how easy to understand a given
text is on a $[0, 100]$ scale. Our objective with this scale is to
quantitatively capture the extend to which a text suits to the \textit{Langage
Clair} (LC, \textit{Clear Language}) guidelines, a French initiative closely
related to English Plain Language. We explore two approaches: (i) using
linguistically motivated indicators used to train statistical models, and (ii)
neural learning directly from text leveraging pre-trained language models. We
introduce a simple proxy task for comprehension difficulty training as a
classification task. To evaluate our models, we run two distinct human
annotation experiments, and find that both approaches (indicator based and
neural) outperforms commonly used readability and comprehension metrics such as
FKGL and SAMSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms. (arXiv:2310.02759v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02759">
<div class="article-summary-box-inner">
<span><p>Automated Essay Score (AES) is proven to be one of the cutting-edge
technologies. Scoring techniques are used for various purposes. Reliable scores
are calculated based on influential variables. Such variables can be computed
by different methods based on the domain. The research is concentrated on the
user's understanding of a given topic. The analysis is based on a scoring index
by using Large Language Models. The user can then compare and contrast the
understanding of a topic that they recently learned. The results are then
contributed towards learning analytics and progression is made for enhancing
the learning ability. In this research, the focus is on summarizing a PDF
document and gauging a user's understanding of its content. The process
involves utilizing a Langchain tool to summarize the PDF and extract the
essential information. By employing this technique, the research aims to
determine how well the user comprehends the summarized content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models. (arXiv:2310.02777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02777">
<div class="article-summary-box-inner">
<span><p>Compositionality is a common property in many modalities including natural
languages and images, but the compositional generalization of multi-modal
models is not well-understood. In this paper, we identify two sources of
visual-linguistic compositionality: linguistic priors and the interplay between
images and texts. We show that current attempts to improve compositional
generalization rely on linguistic priors rather than on information in the
image. We also propose a new metric for compositionality without such
linguistic priors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02778">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low Resource Summarization using Pre-trained Language Models. (arXiv:2310.02790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02790">
<div class="article-summary-box-inner">
<span><p>With the advent of Deep Learning based Artificial Neural Networks models,
Natural Language Processing (NLP) has witnessed significant improvements in
textual data processing in terms of its efficiency and accuracy. However, the
research is mostly restricted to high-resource languages such as English and
low-resource languages still suffer from a lack of available resources in terms
of training datasets as well as models with even baseline evaluation results.
Considering the limited availability of resources for low-resource languages,
we propose a methodology for adapting self-attentive transformer-based
architecture models (mBERT, mT5) for low-resource summarization, supplemented
by the construction of a new baseline dataset (76.5k article, summary pairs) in
a low-resource language Urdu. Choosing news (a publicly available source) as
the application domain has the potential to make the proposed methodology
useful for reproducing in other languages with limited resources. Our adapted
summarization model \textit{urT5} with up to 44.78\% reduction in size as
compared to \textit{mT5} can capture contextual information of low resource
language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)
at par with state-of-the-art models in high resource language English
\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method
provided a baseline approach towards extractive as well as abstractive
summarization with competitive evaluation results in a limited resource setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOMINO: A Dual-System for Multi-step Visual Language Reasoning. (arXiv:2310.02804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02804">
<div class="article-summary-box-inner">
<span><p>Visual language reasoning requires a system to extract text or numbers from
information-dense images like charts or plots and perform logical or arithmetic
reasoning to arrive at an answer. To tackle this task, existing work relies on
either (1) an end-to-end vision-language model trained on a large amount of
data, or (2) a two-stage pipeline where a captioning model converts the image
into text that is further read by another large language model to deduce the
answer. However, the former approach forces the model to answer a complex
question with one single step, and the latter approach is prone to inaccurate
or distracting information in the converted text that can confuse the language
model. In this work, we propose a dual-system for multi-step multimodal
reasoning, which consists of a "System-1" step for visual information
extraction and a "System-2" step for deliberate reasoning. Given an input,
System-2 breaks down the question into atomic sub-steps, each guiding System-1
to extract the information required for reasoning from the image. Experiments
on chart and plot datasets show that our method with a pre-trained System-2
module performs competitively compared to prior work on in- and
out-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on
only a small amount of data on multi-step reasoning, the accuracy of our method
is further improved and surpasses the best fully-supervised end-to-end approach
by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging
dataset with human-authored questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02842">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have the ability to solve a variety of tasks,
such as text summarization and mathematical questions, just out of the box, but
they are often trained with a single task in mind. Due to high computational
costs, the current trend is to use prompt instruction tuning to better adjust
monolithic, pretrained LLMs for new -- but often individual -- downstream
tasks. Thus, how one would expand prompt tuning to handle -- concomitantly --
heterogeneous tasks and data distributions is a widely open question. To
address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,
associated with smart gating functionality: the latter -- whose design is one
of the contributions of this paper -- can identify relevant skills embedded in
different groups of prompts and dynamically assign combined experts (i.e.,
collection of prompts), based on the target task. Additionally, MoPs are
empirically agnostic to any model compression technique applied -- for
efficiency reasons -- as well as instruction data source and task composition.
In practice, MoPs can simultaneously mitigate prompt training "interference" in
multi-task, multi-source scenarios (e.g., task and data heterogeneity across
sources), as well as possible implications from model approximations. As a
highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to
$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim
3\%$ up to $\sim30\%$ in the centralized scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate Speech Detection in Limited Data Contexts using Synthetic Data Generation. (arXiv:2310.02876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02876">
<div class="article-summary-box-inner">
<span><p>A growing body of work has focused on text classification methods for
detecting the increasing amount of hate speech posted online. This progress has
been limited to only a select number of highly-resourced languages causing
detection systems to either under-perform or not exist in limited data
contexts. This is majorly caused by a lack of training data which is expensive
to collect and curate in these settings. In this work, we propose a data
augmentation approach that addresses the problem of lack of data for online
hate speech detection in limited data contexts using synthetic data generation
techniques. Given a handful of hate speech examples in a high-resource language
such as English, we present three methods to synthesize new examples of hate
speech data in a target language that retains the hate sentiment in the
original examples but transfers the hate targets. We apply our approach to
generate training data for hate speech classification tasks in Hindi and
Vietnamese. Our findings show that a model trained on synthetic data performs
comparably to, and in some cases outperforms, a model trained only on the
samples available in the target domain. This method can be adopted to bootstrap
hate speech detection models from scratch in limited data contexts. As the
growth of social media within these contexts continues to outstrip response
efforts, this work furthers our capacities for detection, understanding, and
response to hate speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02905">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown remarkable instruction-following
capabilities and achieved impressive performances in various applications.
However, the performances of LLMs depend heavily on the instructions given to
them, which are typically manually tuned with substantial human efforts. Recent
work has used the query-efficient Bayesian optimization (BO) algorithm to
automatically optimize the instructions given to black-box LLMs. However, BO
usually falls short when optimizing highly sophisticated (e.g.,
high-dimensional) objective functions, such as the functions mapping an
instruction to the performance of an LLM. This is mainly due to the limited
expressive power of the Gaussian process (GP) model which is used by BO as a
surrogate to model the objective function. Meanwhile, it has been repeatedly
shown that neural networks (NNs), especially pre-trained transformers, possess
strong expressive power and can model highly complex functions. So, we adopt a
neural bandit algorithm which replaces the GP in BO by an NN surrogate to
optimize instructions for black-box LLMs. More importantly, the neural bandit
algorithm allows us to naturally couple the NN surrogate with the hidden
representation learned by a pre-trained transformer (i.e., an open-source LLM),
which significantly boosts its performance. These motivate us to propose our
INSTruction optimization usIng Neural bandits Coupled with Transformers}
(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use
extensive experiments to show that our INSTINCT consistently outperforms the
existing methods in different tasks, such as in various instruction induction
tasks and the task of improving the zero-shot chain-of-thought instruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02932">
<div class="article-summary-box-inner">
<span><p>Understanding how climate change affects us and learning about available
solutions are key steps toward empowering individuals and communities to
mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,
it is necessary to assess their capability in this domain. In this study, we
present a comprehensive evaluation framework, grounded in science communication
principles, to analyze LLM responses to climate change topics. Our framework
emphasizes both the presentational and epistemological adequacy of answers,
offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our
framework discerns up to 30 distinct issues in model outputs. The task is a
real-world example of a growing number of challenging problems where AI can
complement and lift human performance. We introduce a novel and practical
protocol for scalable oversight that uses AI Assistance and relies on raters
with relevant educational backgrounds. We evaluate several recent LLMs and
conduct a comprehensive analysis of the results, shedding light on both the
potential and the limitations of LLMs in the realm of climate communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models. (arXiv:2310.02943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02943">
<div class="article-summary-box-inner">
<span><p>Traditional automatic speech recognition (ASR) models output lower-cased
words without punctuation marks, which reduces readability and necessitates a
subsequent text processing model to convert ASR transcripts into a proper
format. Simultaneously, the development of end-to-end ASR models capable of
predicting punctuation and capitalization presents several challenges,
primarily due to limited data availability and shortcomings in the existing
evaluation methods, such as inadequate assessment of punctuation prediction. In
this paper, we introduce a LibriSpeech-PC benchmark designed to assess the
punctuation and capitalization prediction capabilities of end-to-end ASR
models. The benchmark includes a LibriSpeech-PC dataset with restored
punctuation and capitalization, a novel evaluation metric called Punctuation
Error Rate (PER) that focuses on punctuation marks, and initial baseline
models. All code, data, and models are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. (arXiv:2310.02949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02949">
<div class="article-summary-box-inner">
<span><p>Warning: This paper contains examples of harmful language, and reader
discretion is recommended. The increasing open release of powerful large
language models (LLMs) has facilitated the development of downstream
applications by reducing the essential cost of data annotation and computation.
To ensure AI safety, extensive safety-alignment measures have been conducted to
armor these models against malicious use (primarily hard prompt attack).
However, beneath the seemingly resilient facade of the armor, there might lurk
a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these
safely aligned LLMs can be easily subverted to generate harmful content.
Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of
data can elicit safely-aligned models to adapt to harmful tasks without
sacrificing model helpfulness. Remarkably, the subverted models retain their
capability to respond appropriately to regular inquiries. Experiments across 8
models released by 5 different organizations (LLaMa-2, Falcon, InternLM,
BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.
Besides, the single-turn English-only attack successfully transfers to
multi-turn dialogue and other languages. This study serves as a clarion call
for a collective effort to overhaul and fortify the safety of open-source LLMs
against malicious attackers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02953">
<div class="article-summary-box-inner">
<span><p>Instruction tuning has emerged as a crucial process for harnessing the
capabilities of large language models (LLMs) by providing explicit task
instructions, leading to improved performance in various tasks. However,
prevalent text-to-text instruction tuning (TextTuning) methods suffer from
limitations in generalization, robustness, and controllability due to the
ambiguity and lack of explicit structure in tasks. In this paper, we propose
JsonTuning, a novel structure-to-structure approach for instruction tuning. By
leveraging the versatility and structured nature of JSON to represent tasks,
JsonTuning enhances generalization by helping the model understand essential
task elements and their relations, improves robustness by minimizing ambiguity,
and increases controllability by providing explicit control over the output. We
conduct a comprehensive comparative study with diverse language models and
evaluation benchmarks. Experimental results show that JsonTuning outperforms
TextTuning in various applications, showcasing improved performance,
adaptability, robustness, and controllability. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for more effective
and reliable LLMs capable of handling diverse scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02954">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing, primarily propelled by Large
Language Models (LLMs), have showcased their remarkable capabilities grounded
in in-context learning. A promising avenue for guiding LLMs in intricate
reasoning tasks involves the utilization of intermediate reasoning steps within
the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies
in the effective selection of exemplars for facilitating in-context learning.
In this study, we introduce a framework that leverages Dual Queries and
Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars
for in-context learning. Dual Queries first query LLM to obtain LLM-generated
knowledge such as CoT, then query the retriever to obtain the final exemplars
via both question and the knowledge. Moreover, for the second query, LoRe
employs dimensionality reduction techniques to refine exemplar selection,
ensuring close alignment with the input question's knowledge. Through extensive
experiments, we demonstrate that DQ-LoRe significantly outperforms prior
state-of-the-art methods in the automatic selection of exemplars for GPT-4,
enhancing performance from 92.5\% to 94.2\%. Our comprehensive analysis further
reveals that DQ-LoRe consistently outperforms retrieval-based approaches in
terms of both performance and adaptability, especially in scenarios
characterized by distribution shifts. DQ-LoRe pushes the boundaries of
in-context learning and opens up new avenues for addressing complex reasoning
challenges. We will release the code soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02971">
<div class="article-summary-box-inner">
<span><p>Prompting and adapter tuning have emerged as efficient alternatives to
fine-tuning (FT) methods. However, existing studies on speech prompting focused
on classification tasks and failed on more complex sequence generation tasks.
Besides, adapter tuning is primarily applied with a focus on encoder-only
self-supervised models. Our experiments show that prompting on Wav2Seq, a
self-supervised encoder-decoder model, surpasses previous works in sequence
generation tasks. It achieves a remarkable 53% relative improvement in word
error rate for ASR and a 27% in F1 score for slot filling. Additionally,
prompting competes with the FT method in the low-resource scenario. Moreover,
we show the transferability of prompting and adapter tuning on Wav2Seq in
cross-lingual ASR. When limited trainable parameters are involved, prompting
and adapter tuning consistently outperform conventional FT across 7 languages.
Notably, in the low-resource scenario, prompting consistently outperforms
adapter tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network. (arXiv:2310.02973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02973">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated promising outcomes by employing large
language models with multi-tasking capabilities. They utilize prompts to guide
the model's behavior and surpass performance of task-specific models. Motivated
by this, we ask: can we build a single model that jointly perform various
spoken language understanding (SLU) tasks? To address this, we utilize
pre-trained automatic speech recognition (ASR) models and employ various task
and dataset specifiers as discrete prompts. We demonstrate efficacy of our
single multi-task learning (MTL) model "UniverSLU" for 12 different speech
classification and sequence generation tasks across 17 datasets and 9
languages. Results show that UniverSLU achieves competitive performance and
even surpasses task-specific models. We also conduct preliminary investigations
into enabling human-interpretable natural phrases instead of task specifiers as
discrete prompts and test the model's generalization capabilities to new
paraphrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation. (arXiv:2310.02977v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02977">
<div class="article-summary-box-inner">
<span><p>Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02980">
<div class="article-summary-box-inner">
<span><p>Modeling long-range dependencies across sequences is a longstanding goal in
machine learning and has led to architectures, such as state space models, that
dramatically outperform Transformers on long sequences. However, these
impressive empirical gains have been by and large demonstrated on benchmarks
(e.g. Long Range Arena), where models are randomly initialized and trained to
predict a target label from an input sequence. In this work, we show that
random initialization leads to gross overestimation of the differences between
architectures and that pretraining with standard denoising objectives, using
$\textit{only the downstream task data}$, leads to dramatic gains across
multiple architectures and to very small gaps between Transformers and state
space models (SSMs). In stark contrast to prior works, we find vanilla
Transformers to match the performance of S4 on Long Range Arena when properly
pretrained, and we improve the best reported results of SSMs on the PathX-256
task by 20 absolute points. Subsequently, we analyze the utility of
previously-proposed structured parameterizations for SSMs and show they become
mostly redundant in the presence of data-driven initialization obtained through
pretraining. Our work shows that, when evaluating different architectures on
supervised tasks, incorporation of data-driven priors via pretraining is
essential for reliable performance estimation, and can be done efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02984">
<div class="article-summary-box-inner">
<span><p>Learning arguably involves the discovery and memorization of abstract rules.
The aim of this paper is to study associative memory mechanisms. Our model is
based on high-dimensional matrices consisting of outer products of embeddings,
which relates to the inner layers of transformer language models. We derive
precise scaling laws with respect to sample size and parameter size, and
discuss the statistical efficiency of different estimators, including
optimization-based algorithms. We provide extensive numerical experiments to
validate and interpret theoretical results, including fine-grained
visualizations of the stored memory associations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02989">
<div class="article-summary-box-inner">
<span><p>Large Language Models have not yet been broadly adapted for the analysis of
scientific datasets due in part to the unique difficulties of tokenizing
numbers. We propose xVal, a numerical encoding scheme that represents any real
number using just a single token. xVal represents a given real number by
scaling a dedicated embedding vector by the number value. Combined with a
modified number-inference approach, this strategy renders the model end-to-end
continuous when considered as a map from the numbers of the input string to
those of the output string. This leads to an inductive bias that is generally
more suitable for applications in scientific domains. We empirically evaluate
our proposal on a number of synthetic and real-world datasets. Compared with
existing number encoding schemes, we find that xVal is more token-efficient and
demonstrates improved generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kosmos-G: Generating Images in Context with Multimodal Large Language Models. (arXiv:2310.02992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02992">
<div class="article-summary-box-inner">
<span><p>Recent advancements in text-to-image (T2I) and vision-language-to-image
(VL2I) generation have made significant strides. However, the generation from
generalized vision-language inputs, especially involving multiple images,
remains under-explored. This paper presents Kosmos-G, a model that leverages
the advanced perception capabilities of Multimodal Large Language Models
(MLLMs) to tackle the aforementioned challenge. Our approach aligns the output
space of MLLM with CLIP using the textual modality as an anchor and performs
compositional instruction tuning on curated data. Kosmos-G demonstrates a
unique capability of zero-shot multi-entity subject-driven generation. Notably,
the score distillation instruction tuning requires no modifications to the
image decoder. This allows for a seamless substitution of CLIP and effortless
integration with a myriad of U-Net techniques ranging from fine-grained
controls to personalized image decoder variants. We posit Kosmos-G as an
initial attempt towards the goal of "image as a foreign language in image
generation."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02998">
<div class="article-summary-box-inner">
<span><p>Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
performance improvements on various multimodal downstream tasks. However,
deploying LVLMs is often problematic due to their massive computational/energy
costs and carbon consumption. Such issues make it infeasible to adopt
conventional iterative global pruning, which is costly due to computing the
Hessian matrix of the entire large model for sparsification. Alternatively,
several studies have recently proposed layer-wise pruning approaches to avoid
the expensive computation of global pruning and efficiently compress model
weights according to their importance within a layer. However, these methods
often suffer from suboptimal model compression due to their lack of a global
perspective. To address this limitation in recent efficient pruning methods for
large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),
a two-stage coarse-to-fine weight pruning approach for LVLMs. We first
determine the sparsity ratios of different layers or blocks by leveraging the
global importance score, which is efficiently computed based on the
zeroth-order approximation of the global model gradients. Then, the multimodal
model performs local layer-wise unstructured weight pruning based on
globally-informed sparsity ratios. We validate our proposed method across
various multimodal and unimodal models and datasets, demonstrating significant
performance improvements over prevalent pruning techniques in the high-sparsity
regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. (arXiv:2310.03003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03003">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have exploded in popularity due to their new
generative capabilities that go far beyond prior state-of-the-art. These
technologies are increasingly being leveraged in various domains such as law,
finance, and medicine. However, these models carry significant computational
challenges, especially the compute and energy costs required for inference.
Inference energy costs already receive less attention than the energy costs of
training LLMs -- despite how often these large models are called on to conduct
inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see
increasing usage and deployment in various domains, a better understanding of
their resource utilization is crucial for cost-savings, scaling performance,
efficient hardware usage, and optimal inference strategies.
</p>
<p>In this paper, we describe experiments conducted to study the computational
and energy utilization of inference with LLMs. We benchmark and conduct a
preliminary analysis of the inference performance and inference energy costs of
different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta
AI on two generations of popular GPUs (NVIDIA V100 \&amp; A100) and two datasets
(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in
research and practice. We present the results of multi-node, multi-GPU
inference using model sharding across up to 32 GPUs. To our knowledge, our work
is the one of the first to study LLM inference performance from the perspective
of computational and energy resources at this scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions. (arXiv:2310.03016v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03016">
<div class="article-summary-box-inner">
<span><p>In order to understand the in-context learning phenomenon, recent works have
adopted a stylized experimental framework and demonstrated that Transformers
can learn gradient-based learning algorithms for various classes of real-valued
functions. However, the limitations of Transformers in implementing learning
algorithms, and their ability to learn other forms of algorithms are not well
understood. Additionally, the degree to which these capabilities are confined
to attention-based models is unclear. Furthermore, it remains to be seen
whether the insights derived from these stylized settings can be extrapolated
to pretrained Large Language Models (LLMs). In this work, we take a step
towards answering these questions by demonstrating the following: (a) On a
test-bed with a variety of Boolean function classes, we find that Transformers
can nearly match the optimal learning algorithm for 'simpler' tasks, while
their performance deteriorates on more 'complex' tasks. Additionally, we find
that certain attention-free models perform (almost) identically to Transformers
on a range of tasks. (b) When provided a teaching sequence, i.e. a set of
examples that uniquely identifies a function in a class, we show that
Transformers learn more sample-efficiently. Interestingly, our results show
that Transformers can learn to implement two distinct algorithms to solve a
single task, and can adaptively select the more sample-efficient algorithm
depending on the sequence of in-context examples. (c) Lastly, we show that
extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines
on prediction tasks that are guaranteed to not be in their training set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Question Answering for Unified Information Extraction. (arXiv:2310.03017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03017">
<div class="article-summary-box-inner">
<span><p>Multimodal information extraction (MIE) aims to extract structured
information from unstructured multimedia content. Due to the diversity of tasks
and settings, most current MIE models are task-specific and data-intensive,
which limits their generalization to real-world scenarios with diverse task
requirements and limited labeled data. To address these issues, we propose a
novel multimodal question answering (MQA) framework to unify three MIE tasks by
reformulating them into a unified span extraction and multi-choice QA pipeline.
Extensive experiments on six datasets show that: 1) Our MQA framework
consistently and significantly improves the performances of various
off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla
prompting. 2) In the zero-shot setting, MQA outperforms previous
state-of-the-art baselines by a large margin. In addition, the effectiveness of
our framework can successfully transfer to the few-shot setting, enhancing LMMs
on a scale of 10B parameters to be competitive or outperform much larger
language models such as ChatGPT and GPT-4. Our MQA framework can serve as a
general principle of utilizing LMMs to better solve MIE and potentially other
downstream multimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages. (arXiv:2310.03018v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03018">
<div class="article-summary-box-inner">
<span><p>We introduce a new zero resource code-switched speech benchmark designed to
directly assess the code-switching capabilities of self-supervised speech
encoders. We showcase a baseline system of language modeling on discrete units
to demonstrate how the code-switching abilities of speech encoders can be
assessed in a zero-resource manner. Our experiments encompass a variety of
well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We
examine the impact of pre-training languages and model size on benchmark
performance. Notably, though our results demonstrate that speech encoders with
multilingual pre-training, exemplified by XLSR, outperform monolingual variants
(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial
room for improvement in their code-switching linguistic abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Sense per Translation. (arXiv:2106.06082v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06082">
<div class="article-summary-box-inner">
<span><p>Word sense disambiguation (WSD) is the task of determining the sense of a
word in context. Translations have been used in WSD as a source of knowledge,
and even as a means of delimiting word senses. In this paper, we define three
theoretical properties of the relationship between senses and translations, and
argue that they constitute necessary conditions for using translations as sense
inventories. The key property of One Sense per Translation (OSPT) provides a
foundation for a translation-based WSD method. The results of an intrinsic
evaluation experiment indicate that our method achieves a precision of
approximately 93% compared to manual corpus annotations. Our extrinsic
evaluation experiments demonstrate WSD improvements of up to 4.6% F1-score on
difficult WSD datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07136">
<div class="article-summary-box-inner">
<span><p>Per-example gradient clipping is a key algorithmic step that enables
practical differential private (DP) training for deep learning models. The
choice of clipping threshold R, however, is vital for achieving high accuracy
under DP. We propose an easy-to-use replacement, called automatic clipping,
that eliminates the need to tune R for any DP optimizers, including DP-SGD,
DP-Adam, DP-LAMB and many others. The automatic variants are as private and
computationally efficient as existing DP optimizers, but require no DP-specific
hyperparameters and thus make DP training as amenable as the standard
non-private training. We give a rigorous convergence analysis of automatic
DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic
convergence rate that matches the standard SGD, under a symmetric gradient
noise assumption of the per-sample gradients (commonly used in the non-DP
literature). We demonstrate on various language and vision tasks that automatic
clipping outperforms or matches the state-of-the-art, and can be easily
employed with minimal changes to existing codebases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03403">
<div class="article-summary-box-inner">
<span><p>We provide a literature review about Automatic Text Summarization (ATS)
systems. We consider a citation-based approach. We start with some popular and
well-known papers that we have in hand about each topic we want to cover and we
have tracked the "backward citations" (papers that are cited by the set of
papers we knew beforehand) and the "forward citations" (newer papers that cite
the set of papers we knew beforehand). In order to organize the different
methods, we present the diverse approaches to ATS guided by the mechanisms they
use to generate a summary. Besides presenting the methods, we also present an
extensive review of the datasets available for summarization tasks and the
methods used to evaluate the quality of the summaries. Finally, we present an
empirical exploration of these methods using the CNN Corpus dataset that
provides golden summaries for extractive and abstractive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alternating Updates for Efficient Transformers. (arXiv:2301.13310v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13310">
<div class="article-summary-box-inner">
<span><p>It has been well established that increasing scale in deep transformer
networks leads to improved quality and performance. However, this increase in
scale often comes with prohibitive increases in compute cost and inference
latency. We introduce Alternating Updates (AltUp), a simple-to-implement method
to increase a model's capacity without the computational burden. AltUp enables
the widening of the learned representation, i.e., the token embedding, while
only incurring a negligible increase in latency. AltUp achieves this by working
on a subblock of the widened representation at each layer and using a
predict-and-correct mechanism to update the inactivated blocks. We present
extensions of AltUp, such as its applicability to the sequence dimension, and
demonstrate how AltUp can be synergistically combined with existing approaches,
such as Sparse Mixture-of-Experts models, to obtain efficient models with even
higher capacity. Our experiments on benchmark transformer models and language
tasks demonstrate the consistent effectiveness of AltUp on a diverse set of
scenarios. Notably, on SuperGLUE and SQuAD benchmarks, AltUp enables up to
$87\%$ speedup relative to the dense baselines at the same accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types. (arXiv:2302.01313v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01313">
<div class="article-summary-box-inner">
<span><p>The task of inductive link prediction in discrete attributed multigraphs
(e.g., knowledge graphs, multilayer networks, heterogeneous networks, etc.)
generally focuses on test predictions with solely new nodes but not both new
nodes and new relation types. In this work, we formally define the task of
predicting (completely) new nodes and new relation types in test as a doubly
inductive link prediction task and introduce a theoretical framework for the
solution. We start by defining the concept of double permutation-equivariant
representations that are equivariant to permutations of both node identities
and edge relation types. We then propose a general blueprint to design neural
architectures that impose a structural representation of relations that can
inductively generalize from training nodes and relations to arbitrarily new
test nodes and relations without the need for adaptation, side information, or
retraining. We also introduce the concept of distributionally double
equivariant positional embeddings designed to perform the same task. Finally,
we empirically demonstrate the capability of the two proposed models on a set
of novel real-world benchmarks, showcasing average relative performance gains
of $39.65\%$ on predicting new relations types compared to baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination. (arXiv:2302.04511v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04511">
<div class="article-summary-box-inner">
<span><p>The Covid-19 pandemic had an enormous effect on our lives, especially on
people's interactions. By introducing Covid-19 vaccines, both positive and
negative opinions were raised over the subject of taking vaccines or not. In
this paper, using data gathered from Twitter, including tweets and user
profiles, we offer a comprehensive analysis of public opinion in Iran about the
Coronavirus vaccines. For this purpose, we applied a search query technique
combined with a topic modeling approach to extract vaccine-related tweets. We
utilized transformer-based models to classify the content of the tweets and
extract themes revolving around vaccination. We also conducted an emotion
analysis to evaluate the public happiness and anger around this topic. Our
results demonstrate that Covid-19 vaccination has attracted considerable
attention from different angles, such as governmental issues, safety or
hesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like
public vaccination and the rate of infection deeply impacted public emotional
status and users' interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.01590">
<div class="article-summary-box-inner">
<span><p>This paper introduces a framework for formally establishing a connection
between a portion of an algebraic language and a Graph Neural Network (GNN).
The framework leverages Context-Free Grammars (CFG) to organize algebraic
operations into generative rules that can be translated into a GNN layer model.
As CFGs derived directly from a language tend to contain redundancies in their
rules and variables, we present a grammar reduction scheme. By applying this
strategy, we define a CFG that conforms to the third-order Weisfeiler-Lehman
(3-WL) test using MATLANG. From this 3-WL CFG, we derive a GNN model, named
G$^2$N$^2$, which is provably 3-WL compliant. Through various experiments, we
demonstrate the superior efficiency of G$^2$N$^2$ compared to other 3-WL GNNs
across numerous downstream tasks. Specifically, one experiment highlights the
benefits of grammar reduction within our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03048">
<div class="article-summary-box-inner">
<span><p>Driven by large-data pre-training, Segment Anything Model (SAM) has been
demonstrated as a powerful and promptable framework, revolutionizing the
segmentation models. Despite the generality, customizing SAM for specific
visual concepts without man-powered prompting is under explored, e.g.,
automatically segmenting your pet dog in different images. In this paper, we
propose a training-free Personalization approach for SAM, termed as PerSAM.
Given only a single image with a reference mask, PerSAM first localizes the
target concept by a location prior, and segments it within other images or
videos via three techniques: target-guided attention, target-semantic
prompting, and cascaded post-refinement. In this way, we effectively adapt SAM
for private use without any training. To further alleviate the mask ambiguity,
we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the
entire SAM, we introduce two learnable weights for multi-scale masks, only
training 2 parameters within 10 seconds for improved performance. To
demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for
personalized evaluation, and test our methods on video object segmentation with
competitive performance. Besides, our approach can also enhance DreamBooth to
personalize Stable Diffusion for text-to-image generation, which discards the
background disturbance for better target appearance learning. Code is released
at https://github.com/ZrrSkywalker/Personalize-SAM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08099">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have
demonstrated state-of-the-art performance on automatic speech recognition (ASR)
and proved to be extremely useful in low label-resource settings. However, the
success of SSL models has yet to transfer to utterance-level tasks such as
speaker, emotion, and language recognition, which still require supervised
fine-tuning of the SSL models to obtain good performance. We argue that the
problem is caused by the lack of disentangled representations and an
utterance-level learning objective for these tasks. Inspired by how HuBERT uses
clustering to discover hidden acoustic units, we formulate a factor analysis
(FA) model that uses the discovered hidden acoustic units to align the SSL
features. The underlying utterance-level representations are disentangled from
the content of speech using probabilistic inference on the aligned features.
Furthermore, the variational lower bound derived from the FA model provides an
utterance-level objective, allowing error gradients to be backpropagated to the
Transformer layers to learn highly discriminative acoustic units. When used in
conjunction with HuBERT's masked prediction training, our models outperform the
current best model, WavLM, on all utterance-level non-semantic tasks on the
SUPERB benchmark with only 20% of labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10276">
<div class="article-summary-box-inner">
<span><p>In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11244">
<div class="article-summary-box-inner">
<span><p>In this work, we explore Parameter-Efficient-Learning (PEL) techniques to
repurpose a General-Purpose-Speech (GSM) model for Arabic dialect
identification (ADI). Specifically, we investigate different setups to
incorporate trainable features into a multi-layer encoder-decoder GSM
formulation under frozen pre-trained settings. Our architecture includes
residual adapter and model reprogramming (input-prompting). We design a
token-level label mapping to condition the GSM for Arabic Dialect
Identification (ADI). This is challenging due to the high variation in
vocabulary and pronunciation among the numerous regional dialects. We achieve
new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We
further reduce the training budgets with the PEL method, which performs within
1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable
parameters. Our study demonstrates how to identify Arabic dialects using a
small dataset and limited computation with open source code and pre-trained
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alt-Text with Context: Improving Accessibility for Images on Twitter. (arXiv:2305.14779v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14779">
<div class="article-summary-box-inner">
<span><p>In this work we present an approach for generating alternative text (or
alt-text) descriptions for images shared on social media, specifically Twitter.
More than just a special case of image captioning, alt-text is both more
literally descriptive and context-specific. Also critically, images posted to
Twitter are often accompanied by user-written text that despite not necessarily
describing the image may provide useful context that if properly leveraged can
be informative. We address this task with a multimodal model that conditions on
both textual information from the associated social media post as well as
visual signal from the image, and demonstrate that the utility of these two
information sources stacks. We put forward a new dataset of 371k images paired
with alt-text and tweets scraped from Twitter and evaluate on it across a
variety of automated metrics as well as human evaluation. We show that our
approach of conditioning on both tweet text and visual information
significantly outperforms prior work, by more than 2x on BLEU@4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. (arXiv:2305.17359v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17359">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have notably enhanced the fluency and diversity
of machine-generated text. However, this progress also presents a significant
challenge in detecting the origin of a given text, and current research on
detection methods lags behind the rapid evolution of LLMs. Conventional
training-based methods have limitations in flexibility, particularly when
adapting to new domains, and they often lack explanatory power. To address this
gap, we propose a novel training-free detection strategy called Divergent
N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and
then use only the preceding portion as input to the LLMs to regenerate the new
remaining parts. By analyzing the differences between the original and new
remaining parts through N-gram analysis in black-box or probability divergence
in white-box, we unveil significant discrepancies between the distribution of
machine-generated text and the distribution of human-written text. We conducted
extensive experiments on the most advanced LLMs from OpenAI, including
text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such
as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach
exhibits state-of-the-art performance in distinguishing between human and
GPT-generated text on four English and one German dataset, outperforming
OpenAI's own classifier, which is trained on millions of text. Additionally,
our methods provide reasonable explanations and evidence to support our claim,
which is a unique feature of explainable detection. Our method is also robust
under the revised text attack and can additionally solve model sourcing. Codes
are available at https://github.com/Xianjun-Yang/DNA-GPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. (arXiv:2306.00978v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00978">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown excellent performance on various
tasks, but the astronomical model size raises the hardware barrier for serving
(memory size) and slows down token generation (memory bandwidth). In this
paper, we propose Activation-aware Weight Quantization (AWQ), a
hardware-friendly approach for LLM low-bit weight-only quantization. Our method
is based on the observation that weights are not equally important: protecting
only 1% of salient weights can greatly reduce quantization error. We then
propose to search for the optimal per-channel scaling that protects the salient
weights by observing the activation, not weights. AWQ does not rely on any
backpropagation or reconstruction, so it can well preserve LLMs' generalization
ability on different domains and modalities, without overfitting to the
calibration set. AWQ outperforms existing work on various language modeling and
domain-specific benchmarks. Thanks to better generalization, it achieves
excellent quantization performance for instruction-tuned LMs and, for the first
time, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible
inference framework tailored for LLMs on the edge, offering more than 3x
speedup over the Huggingface FP16 implementation on both desktop and mobile
GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile
GPU (NVIDIA Jetson Orin 64GB).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization. (arXiv:2306.01102v6 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01102">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as powerful tools capable of
accomplishing a broad spectrum of tasks. Their abilities span numerous areas,
and one area where they have made a significant impact is in the domain of code
generation. In this context, we view LLMs as mutation and crossover tools.
Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and
robust solutions. By merging the code-generating abilities of LLMs with the
diversity and robustness of QD solutions, we introduce LLMatic, a Neural
Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS
directly through prompts, LLMatic uses a procedural approach, leveraging QD for
prompts and network architecture to create diverse and highly performant
networks. We test LLMatic on the CIFAR-10 image classification benchmark,
demonstrating that it can produce competitive networks with just $2,000$
searches, even without prior knowledge of the benchmark domain or exposure to
any previous top-performing models for the benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems. (arXiv:2306.03091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03091">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have greatly advanced code auto-completion
systems, with a potential for substantial productivity enhancements for
developers. However, current benchmarks mainly focus on single-file tasks,
leaving an assessment gap for more complex, real-world, multi-file programming
scenarios. To fill this gap, we introduce RepoBench, a new benchmark
specifically designed for evaluating repository-level code auto-completion
systems. RepoBench supports both Python and Java and consists of three
interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code
Completion), and RepoBench-P (Pipeline). Each task respectively measures the
system's ability to retrieve the most relevant code snippets from other files
as cross-file context, predict the next line of code with cross-file and
in-file context, and handle complex tasks that require a combination of both
retrieval and next-line prediction. RepoBench aims to facilitate a more
complete comparison of performance and encouraging continuous improvement in
auto-completion systems. RepoBench is publicly available at
https://github.com/Leolty/repobench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03872">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) significantly benefit from Chain-of-Thought
(CoT) prompting in performing various reasoning tasks. While CoT allows models
to produce more comprehensive reasoning processes, its emphasis on intermediate
reasoning steps can inadvertently introduce hallucinations and accumulated
errors, thereby limiting models' ability to solve complex reasoning tasks.
Inspired by how humans engage in careful and meticulous deductive logical
reasoning processes to solve tasks, we seek to enable language models to
perform explicit and rigorous deductive reasoning, and also ensure the
trustworthiness of their reasoning process through self-verification. However,
directly verifying the validity of an entire deductive reasoning process is
challenging, even with advanced models like ChatGPT. In light of this, we
propose to decompose a reasoning verification process into a series of
step-by-step subprocesses, each only receiving their necessary context and
premises. To facilitate this procedure, we propose Natural Program, a natural
language-based deductive reasoning format. Our approach enables models to
generate precise reasoning steps where subsequent steps are more rigorously
grounded on prior steps. It also empowers language models to carry out
reasoning self-verification in a step-by-step manner. By integrating this
verification process into each deductive reasoning stage, we significantly
enhance the rigor and trustfulness of generated reasoning steps. Along this
process, we also improve the answer correctness on complex reasoning tasks.
Code will be released at https://github.com/lz1oceani/verify_cot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval. (arXiv:2307.00589v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00589">
<div class="article-summary-box-inner">
<span><p>Information retrieval (IR) is essential in biomedical knowledge acquisition
and clinical decision support. While recent progress has shown that language
model encoders perform better semantic retrieval, training such models requires
abundant query-article annotations that are difficult to obtain in biomedicine.
As a result, most biomedical IR systems only conduct lexical matching. In
response, we introduce MedCPT, a first-of-its-kind Contrastively Pre-trained
Transformer model for zero-shot semantic IR in biomedicine. To train MedCPT, we
collected an unprecedented scale of 255 million user click logs from PubMed.
With such data, we use contrastive learning to train a pair of
closely-integrated retriever and re-ranker. Experimental results show that
MedCPT sets new state-of-the-art performance on six biomedical IR tasks,
outperforming various baselines including much larger models such as
GPT-3-sized cpt-text-XL. In addition, MedCPT also generates better biomedical
article and sentence representations for semantic evaluations. As such, MedCPT
can be readily applied to various real-world biomedical IR tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ValiTex -- a unified validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02863">
<div class="article-summary-box-inner">
<span><p>Guidance on how to validate computational text-based measures of social
science constructs is fragmented. While scholars generally acknowledge the
importance of validating their text-based measures, they often lack common
terminology and a unified framework to do so. This paper introduces ValiTex, a
new validation framework designed to assist scholars in validly measuring
social science constructs based on textual data. ValiTex prescribes researchers
to demonstrate three types of validity evidence: substantive evidence
(outlining the theoretical underpinning of the measure), structural evidence
(examining the properties of the text model and its output), and external
evidence (testing for how the measure relates to independent information). In
addition to the framework, ValiTex offers valuable practical guidance through a
checklist that is adaptable for different use cases. The checklist clearly
defines and outlines specific validation steps while also offering a
knowledgeable evaluation of the importance of each validation step to establish
validity. We demonstrate the utility of the framework by applying it to a use
case of detecting sexism from social media data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10928">
<div class="article-summary-box-inner">
<span><p>Evaluation of Large Language Models (LLMs) is challenging because
instruction-following necessitates alignment with human values and the required
set of skills varies depending on the instruction. However, previous studies
have mainly focused on coarse-grained evaluation (i.e. overall preference-based
evaluation), which limits interpretability since it does not consider the
nature of user instructions that require instance-wise skill composition. In
this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on
Alignment Skill Sets), a fine-grained evaluation protocol for both human-based
and model-based evaluation which decomposes coarse-level scoring to a skill
set-level scoring for each instruction. We experimentally observe that the
fine-graininess of evaluation is crucial for attaining a holistic view of model
performance and increasing the reliability of the evaluation. Using FLASK, we
compare multiple open-source and proprietary LLMs and observe a high
correlation between model-based and human-based evaluations. We publicly
release the evaluation data and code implementation at
https://github.com/kaistAI/FLASK.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Eval: Instituting Standardized Evaluation for Long Context Language Models. (arXiv:2307.11088v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11088">
<div class="article-summary-box-inner">
<span><p>Recently, there has been growing interest in extending the context length of
large language models (LLMs), aiming to effectively process long inputs of one
turn or conversations with more extensive histories. While proprietary models
such as GPT-4 and Claude can largely preserve the reasoning ability in an
extended context, open-source models are still progressing through the early
stages of development. To bridge this gap, we propose L-Eval to institute a
more standardized evaluation for long context language models (LCLMs)
addressing two key aspects: dataset construction and evaluation metrics. On the
one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long
documents, and over 2,000 human-labeled query-response pairs encompassing
diverse question styles, domains, and input length (3k$\sim$200k tokens). On
the other hand, we investigate the effectiveness in evalution metrics for
LCLMs. Results show that popular n-gram matching metrics generally can not
correlate well with human judgment, and thus we strongly advocate for
length-instruction-enhanced (LIE) evaluation and employing LLM judges. We
conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source
counterparts using the L-Eval benchmark. Our empirical findings offer useful
insights into the study of LCLMs and lay the groundwork for the development of
more principled evaluation of these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. (arXiv:2308.08155v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08155">
<div class="article-summary-box-inner">
<span><p>AutoGen is an open-source framework that allows developers to build LLM
applications via multiple agents that can converse with each other to
accomplish tasks. AutoGen agents are customizable, conversable, and can operate
in various modes that employ combinations of LLMs, human inputs, and tools.
Using AutoGen, developers can also flexibly define agent interaction behaviors.
Both natural language and computer code can be used to program flexible
conversation patterns for different applications. AutoGen serves as a generic
infrastructure to build diverse applications of various complexities and LLM
capacities. Empirical studies demonstrate the effectiveness of the framework in
many example applications, with domains ranging from mathematics, coding,
question answering, operations research, online decision-making, entertainment,
etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10792">
<div class="article-summary-box-inner">
<span><p>This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness. (arXiv:2308.16175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16175">
<div class="article-summary-box-inner">
<span><p>We introduce BSDetector, a method for detecting bad and speculative answers
from a pretrained Large Language Model by estimating a numeric confidence score
for any output it generated. Our uncertainty quantification technique works for
any LLM accessible only via a black-box API, whose training data remains
unknown. By expending a bit of extra computation, users of any LLM API can now
get the same response as they would ordinarily, as well as a confidence
estimate that cautions when not to trust this response. Experiments on both
closed and open-form Question-Answer benchmarks reveal that BSDetector more
accurately identifies incorrect LLM responses than alternative uncertainty
estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple
responses from the LLM and considering the one with the highest confidence
score, we can additionally obtain more accurate responses from the same LLM,
without any extra training steps. In applications involving automated
evaluation with LLMs, accounting for our confidence scores leads to more
reliable evaluation in both human-in-the-loop and fully-automated settings
(across both GPT 3.5 and 4).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08600">
<div class="article-summary-box-inner">
<span><p>One of the roadblocks to a better understanding of neural networks' internals
is \textit{polysemanticity}, where neurons appear to activate in multiple,
semantically distinct contexts. Polysemanticity prevents us from identifying
concise, human-understandable explanations for what neural networks are doing
internally. One hypothesised cause of polysemanticity is
\textit{superposition}, where neural networks represent more features than they
have neurons by assigning features to an overcomplete set of directions in
activation space, rather than to individual neurons. Here, we attempt to
identify those directions, using sparse autoencoders to reconstruct the
internal activations of a language model. These autoencoders learn sets of
sparsely activating features that are more interpretable and monosemantic than
directions identified by alternative approaches, where interpretability is
measured by automated methods. Moreover, we show that with our learned set of
features, we can pinpoint the features that are causally responsible for
counterfactual behaviour on the indirect object identification task
\citep{wang2022interpretability} to a finer degree than previous
decompositions. This work indicates that it is possible to resolve
superposition in language models using a scalable, unsupervised method. Our
method may serve as a foundation for future mechanistic interpretability work,
which we hope will enable greater model transparency and steerability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10313">
<div class="article-summary-box-inner">
<span><p>Following the success of GPT4, there has been a surge in interest in
multimodal large language model (MLLM) research. This line of research focuses
on developing general-purpose LLMs through fine-tuning pre-trained LLMs and
vision models. However, catastrophic forgetting, a notorious phenomenon where
the fine-tuned model fails to retain similar performance compared to the
pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).
In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the
catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.
We first apply EMT to evaluate several open-source fine-tuned MLLMs and we
discover that almost all evaluated MLLMs fail to retain the same performance
levels as their vision encoders on standard image classification tasks.
Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess
performance throughout the fine-tuning. Interestingly, our results suggest that
early-stage fine-tuning on an image dataset improves performance across other
image datasets, by enhancing the alignment of text and visual features.
However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in
a significant loss of generalizability, even when the image encoder remains
frozen. Our results suggest that MLLMs have yet to demonstrate performance on
par with their vision models on standard image classification tasks and the
current MLLM fine-tuning procedure still has room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14509">
<div class="article-summary-box-inner">
<span><p>Computation in a typical Transformer-based large language model (LLM) can be
characterized by batch size, hidden dimension, number of layers, and sequence
length. Until now, system works for accelerating LLM training have focused on
the first three dimensions: data parallelism for batch size, tensor parallelism
for hidden size and pipeline parallelism for model depth or layers. These
widely studied forms of parallelism are not targeted or optimized for long
sequence Transformer models. Given practical application needs for long
sequence LLM, renewed attentions are being drawn to sequence parallelism.
However, existing works in sequence parallelism are constrained by
memory-communication inefficiency, limiting their scalability to long sequence
large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable
and effective methodology for enabling highly efficient and scalable LLM
training with extremely long sequence length. DeepSpeed-Ulysses at its core
partitions input data along the sequence dimension and employs an efficient
all-to-all collective communication for attention computation. Theoretical
communication analysis shows that whereas other methods incur communication
overhead as sequence length increases, DeepSpeed-Ulysses maintains constant
communication volume when sequence length and compute devices are increased
proportionally. Furthermore, experimental evaluations show that
DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the
existing method SOTA baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.17452">
<div class="article-summary-box-inner">
<span><p>Large language models have made significant progress in various language
tasks, yet they still struggle with complex mathematics. In this paper, we
propose ToRA a series of Tool-integrated Reasoning Agents designed to solve
challenging mathematical problems by seamlessly integrating natural language
reasoning with the utilization of external tools (e.g., computation libraries
and symbolic solvers), thereby amalgamating the analytical prowess of language
and the computational efficiency of tools. To train ToRA, we curate interactive
tool-use trajectories on mathematical datasets, apply imitation learning on the
annotations, and propose output space shaping to further refine models'
reasoning behavior. As a result, ToRA models significantly outperform
open-source models on 10 mathematical reasoning datasets across all scales with
13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the
competition-level dataset MATH, surpassing the best open-source model
WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source
model that achieves an accuracy exceeding 50% on MATH, which significantly
outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems
with programs. Additionally, we conduct a comprehensive analysis of the
benefits and remaining challenges of tool interaction for mathematical
reasoning, providing valuable insights for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00648">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of
pre-trained language models (PLMs) to specific tasks. By tuning only a minimal
set of (extra) parameters, PEFT achieves performance comparable to full
fine-tuning. However, despite its prevalent use, the security implications of
PEFT remain largely unexplored. In this paper, we conduct a pilot study
revealing that PEFT exhibits unique vulnerability to trojan attacks.
Specifically, we present PETA, a novel attack that accounts for downstream
adaptation through bilevel optimization: the upper-level objective embeds the
backdoor into a PLM while the lower-level objective simulates PEFT to retain
the PLM's task-specific performance. With extensive evaluation across a variety
of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in
terms of both attack success rate and unaffected clean accuracy, even after the
victim user performs PEFT over the backdoored PLM using untainted data.
Moreover, we empirically provide possible explanations for PETA's efficacy: the
bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules,
thereby retaining the backdoor throughout PEFT. Based on this insight, we
explore a simple defense that omits PEFT in selected layers of the backdoored
PLM and unfreezes a subset of these layers' parameters, which is shown to
effectively neutralize PETA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01468">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are effective at answering questions that are
clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This entity-deducing game can serve as an evaluation
framework to probe the conversational reasoning and planning capabilities of
language models. We systematically evaluate various LLMs and discover
significant differences in their performance on this task. We find that strong
LLMs like GPT-4 outperform human players by a large margin. We further employ
Behavior Cloning (BC) to examine whether a weaker model is capable of imitating
a stronger model and generalizing to data or domains, using only the
demonstrations from a stronger model. We finally propose to use Reinforcement
Learning to enhance reasoning and planning capacity of Vicuna models through
episodes of game playing, which lead to significant performance improvement. We
hope that this problem offers insights into how autonomous agents could be
trained to behave more intelligently in ambiguous circumstances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01469">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be
knowledgeable and able to adapt to many tasks. However, we still can not
completely trust their answer, since LLMs suffer from
hallucination--fabricating non-existent facts to cheat users without
perception. And the reasons for their existence and pervasiveness remain
unclear. In this paper, we demonstrate that non-sense prompts composed of
random tokens can also elicit the LLMs to respond with hallucinations. This
phenomenon forces us to revisit that hallucination may be another view of
adversarial examples, and it shares similar features with conventional
adversarial examples as the basic feature of LLMs. Therefore, we formalize an
automatic hallucination triggering method as the hallucination attack in an
adversarial way. Finally, we explore basic feature of attacked adversarial
prompts and propose a simple yet effective defense strategy. Our code is
released on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment. (arXiv:2310.01839v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01839">
<div class="article-summary-box-inner">
<span><p>Automatic pronunciation assessment (APA) manages to quantify the
pronunciation proficiency of a second language (L2) learner in a language.
Prevailing approaches to APA normally leverage neural models trained with a
regression loss function, such as the mean-squared error (MSE) loss, for
proficiency level prediction. Despite most regression models can effectively
capture the ordinality of proficiency levels in the feature space, they are
confronted with a primary obstacle that different phoneme categories with the
same proficiency level are inevitably forced to be close to each other,
retaining less phoneme-discriminative information. On account of this, we
devise a phonemic contrast ordinal (PCO) loss for training regression-based APA
models, which aims to preserve better phonemic distinctions between phoneme
categories meanwhile considering ordinal relationships of the regression target
output. Specifically, we introduce a phoneme-distinct regularizer into the MSE
loss, which encourages feature representations of different phoneme categories
to be far apart while simultaneously pulling closer the representations
belonging to the same phoneme category by means of weighted distances. An
extensive set of experiments carried out on the speechocean762 benchmark
dataset suggest the feasibility and effectiveness of our model in relation to
some existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01886">
<div class="article-summary-box-inner">
<span><p>Many pre-trained large-scale models provided online have become highly
effective in transferring to downstream tasks. At the same time, various
task-specific models fine-tuned on these pre-trained models are available
online for public use. In practice, as collecting task-specific data is
labor-intensive and fine-tuning the large pre-trained models is computationally
expensive, one can reuse task-specific finetuned models to deal with downstream
tasks. However, using a model per task causes a heavy burden on storage and
serving. Recently, many training-free and parameter-efficient methods have been
proposed for reusing multiple fine-tuned task-specific models into a single
multi-task model. However, these methods exhibit a large accuracy gap compared
with using a fine-tuned model per task. In this paper, we propose
Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing
Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task
vector into a merged model by magnitude pruning. For reusing LoRA fine-tuned
models, we propose PERU-LoRA use a lower-rank matrix to approximate the LoRA
matrix by singular value decomposition. Both PERUFFT and PERU-LoRA are
training-free. Extensive experiments conducted on computer vision and natural
language process tasks demonstrate the effectiveness and parameter-efficiency
of the proposed methods. The proposed PERU-FFT and PERU-LoRA outperform
existing reusing model methods by a large margin and achieve comparable
performance to using a fine-tuned model per task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02031">
<div class="article-summary-box-inner">
<span><p>Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who's Harry Potter? Approximate Unlearning in LLMs. (arXiv:2310.02238v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02238">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are trained on massive internet corpora that
often contain copyrighted content. This poses legal and ethical challenges for
the developers and users of these models, as well as the original authors and
publishers. In this paper, we propose a novel technique for unlearning a subset
of the training data from a LLM, without having to retrain it from scratch.
</p>
<p>We evaluate our technique on the task of unlearning the Harry Potter books
from the Llama2-7b model (a generative language model recently open-sourced by
Meta). While the model took over 184K GPU-hours to pretrain, we show that in
about 1 GPU hour of finetuning, we effectively erase the model's ability to
generate or recall Harry Potter-related content, while its performance on
common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains
almost unaffected. We make our fine-tuned model publicly available on
HuggingFace for community evaluation. To the best of our knowledge, this is the
first paper to present an effective technique for unlearning in generative
language models.
</p>
<p>Our technique consists of three main components: First, we use a reinforced
model that is further trained on the target data to identify the tokens that
are most related to the unlearning target, by comparing its logits with those
of a baseline model. Second, we replace idiosyncratic expressions in the target
data with generic counterparts, and leverage the model's own predictions to
generate alternative labels for every token. These labels aim to approximate
the next-token predictions of a model that has not been trained on the target
data. Third, we finetune the model on these alternative labels, which
effectively erases the original text from the model's memory whenever it is
prompted with its context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Inclusion in Abstractive Text Summarization. (arXiv:2307.02570v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02570">
<div class="article-summary-box-inner">
<span><p>We address the named entity omission - the drawback of many current
abstractive text summarizers. We suggest a custom pretraining objective to
enhance the model's attention on the named entities in a text. At first, the
named entity recognition model RoBERTa is trained to determine named entities
in the text. After that, this model is used to mask named entities in the text
and the BART model is trained to reconstruct them. Next, the BART model is
fine-tuned on the summarization task. Our experiments showed that this
pretraining approach improves named entity inclusion precision and recall
metrics.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-05 23:11:32.830111239 UTC">2023-10-05 23:11:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>