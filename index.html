<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-09T01:30:00Z">11-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CELLS: A Parallel Corpus for Biomedical Lay Language Generation. (arXiv:2211.03818v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03818">
<div class="article-summary-box-inner">
<span><p>Recent lay language generation systems have used Transformer models trained
on a parallel corpus to increase health information accessibility. However, the
applicability of these models is constrained by the limited size and topical
breadth of available corpora. We introduce CELLS, the largest (63k pairs) and
broadest-ranging (12 journals) parallel corpus for lay language generation. The
abstract and the corresponding lay language summary are written by domain
experts, assuring the quality of our dataset. Furthermore, qualitative
evaluation of expert-authored plain language summaries has revealed background
explanation as a key strategy to increase accessibility. Such explanation is
challenging for neural models to generate because it goes beyond simplification
by adding content absent from the source. We derive two specialized paired
corpora from CELLS to address key challenges in lay language generation:
generating background explanations and simplifying the original abstract. We
adopt retrieval-augmented models as an intuitive fit for the task of background
explanation generation, and show improvements in summary quality and simplicity
while maintaining factual correctness. Taken together, this work presents the
first comprehensive study of background explanation for lay language
generation, paving the path for disseminating scientific knowledge to a broader
audience. CELLS is publicly available at:
https://github.com/LinguisticAnomalies/pls_retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis. (arXiv:2211.03837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03837">
<div class="article-summary-box-inner">
<span><p>Aspect Based Sentiment Analysis is a dominant research area with potential
applications in social media analytics, business, finance, and health. Prior
works in this area are primarily based on supervised methods, with a few
techniques using weak supervision limited to predicting a single aspect
category per review sentence. In this paper, we present an extremely weakly
supervised multi-label Aspect Category Sentiment Analysis framework which does
not use any labelled data. We only rely on a single word per class as an
initial indicative information. We further propose an automatic word selection
technique to choose these seed categories and sentiment words. We explore
unsupervised language model post-training to improve the overall performance,
and propose a multi-label generator model to generate multiple aspect
category-sentiment pairs per review sentence. Experiments conducted on four
benchmark datasets showcase our method to outperform other weakly supervised
baselines by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference. (arXiv:2211.03862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03862">
<div class="article-summary-box-inner">
<span><p>It has been shown that NLI models are usually biased with respect to the
word-overlap between premise and hypothesis; they take this feature as a
primary cue for predicting the entailment label. In this paper, we focus on an
overlooked aspect of the overlap bias in NLI models: the reverse word-overlap
bias. Our experimental results demonstrate that current NLI models are highly
biased towards the non-entailment label on instances with low overlap, and the
existing debiasing methods, which are reportedly successful on existing
challenge datasets, are generally ineffective in addressing this category of
bias. We investigate the reasons for the emergence of the overlap bias and the
role of minority examples in its mitigation. For the former, we find that the
word-overlap bias does not stem from pre-training, and for the latter, we
observe that in contrast to the accepted assumption, eliminating minority
examples does not affect the generalizability of debiasing methods with respect
to the overlap bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strictly Breadth-First AMR Parsing. (arXiv:2211.03922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03922">
<div class="article-summary-box-inner">
<span><p>AMR parsing is the task that maps a sentence to an AMR semantic graph
automatically. We focus on the breadth-first strategy of this task, which was
proposed recently and achieved better performance than other strategies.
However, current models under this strategy only \emph{encourage} the model to
produce the AMR graph in breadth-first order, but \emph{cannot guarantee} this.
To solve this problem, we propose a new architecture that \emph{guarantees}
that the parsing will strictly follow the breadth-first order. In each parsing
step, we introduce a \textbf{focused parent} vertex and use this vertex to
guide the generation. With the help of this new architecture and some other
improvements in the sentence and graph encoder, our model obtains better
performance on both the AMR 1.0 and 2.0 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proactive Detractor Detection Framework Based on Message-Wise Sentiment Analysis Over Customer Support Interactions. (arXiv:2211.03923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03923">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a framework relying solely on chat-based customer
support (CS) interactions for predicting the recommendation decision of
individual users. For our case study, we analyzed a total number of 16.4k users
and 48.7k customer support conversations within the financial vertical of a
large e-commerce company in Latin America. Consequently, our main contributions
and objectives are to use Natural Language Processing (NLP) to assess and
predict the recommendation behavior where, in addition to using static
sentiment analysis, we exploit the predictive power of each user's sentiment
dynamics. Our results show that, with respective feature interpretability, it
is possible to predict the likelihood of a user to recommend a product or
service, based solely on the message-wise sentiment evolution of their CS
conversations in a fully automated way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03929">
<div class="article-summary-box-inner">
<span><p>Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive empirical successes on downstream tasks, we still
have a limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tell Your Story: Task-Oriented Dialogs for Interactive Content Creation. (arXiv:2211.03940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03940">
<div class="article-summary-box-inner">
<span><p>People capture photos and videos to relive and share memories of personal
significance. Recently, media montages (stories) have become a popular mode of
sharing these memories due to their intuitive and powerful storytelling
capabilities. However, creating such montages usually involves a lot of manual
searches, clicks, and selections that are time-consuming and cumbersome,
adversely affecting user experiences.
</p>
<p>To alleviate this, we propose task-oriented dialogs for montage creation as a
novel interactive tool to seamlessly search, compile, and edit montages from a
media collection. To the best of our knowledge, our work is the first to
leverage multi-turn conversations for such a challenging application, extending
the previous literature studying simple media retrieval tasks. We collect a new
dataset C3 (Conversational Content Creation), comprising 10k dialogs
conditioned on media montages simulated from a large media collection.
</p>
<p>We take a simulate-and-paraphrase approach to collect these dialogs to be
both cost and time efficient, while drawing from natural language distribution.
Our analysis and benchmarking of state-of-the-art language models showcase the
multimodal challenges present in the dataset. Lastly, we present a real-world
mobile demo application that shows the feasibility of the proposed work in
real-world applications. Our code and data will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter and Data Efficient Continual Pre-training for Robustness to Dialectal Variance in Arabic. (arXiv:2211.03966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03966">
<div class="article-summary-box-inner">
<span><p>The use of multilingual language models for tasks in low and high-resource
languages has been a success story in deep learning. In recent times, Arabic
has been receiving widespread attention on account of its dialectal variance.
While prior research studies have tried to adapt these multilingual models for
dialectal variants of Arabic, it still remains a challenging problem owing to
the lack of sufficient monolingual dialectal data and parallel translation data
of such dialectal variants. It remains an open problem on whether the limited
dialectical data can be used to improve the models trained in Arabic on its
dialectal variants. First, we show that multilingual-BERT (mBERT) incrementally
pretrained on Arabic monolingual data takes less training time and yields
comparable accuracy when compared to our custom monolingual Arabic model and
beat existing models (by an avg metric of +$6.41$). We then explore two
continual pre-training methods -- (1) using small amounts of dialectical data
for continual finetuning and (2) parallel Arabic to English data and a
Translation Language Modeling loss function. We show that both approaches help
improve performance on dialectal classification tasks ($+4.64$ avg. gain) when
used on monolingual models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps. (arXiv:2211.03988v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03988">
<div class="article-summary-box-inner">
<span><p>IR models using a pretrained language model significantly outperform lexical
approaches like BM25. In particular, SPLADE, which encodes texts to sparse
vectors, is an effective model for practical use because it shows robustness to
out-of-domain datasets. However, SPLADE still struggles with exact matching of
low-frequency words in training data. In addition, domain shifts in vocabulary
and word frequencies deteriorate the IR performance of SPLADE. Because
supervision data are scarce in the target domain, addressing the domain shifts
without supervision data is necessary. This paper proposes an unsupervised
domain adaptation method by filling vocabulary and word-frequency gaps. First,
we expand a vocabulary and execute continual pretraining with a masked language
model on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse
vectors by inverse document frequency weights to consider the importance of
documents with lowfrequency words. We conducted experiments using our method on
datasets with a large vocabulary gap from a source domain. We show that our
method outperforms the present stateof-the-art domain adaptation method. In
addition, our method achieves state-of-the-art results, combined with BM25.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Unstructured Knowledge Access in Conversational Dialogue with ASR Errors. (arXiv:2211.03990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03990">
<div class="article-summary-box-inner">
<span><p>Performance of spoken language understanding (SLU) can be degraded with
automatic speech recognition (ASR) errors. We propose a novel approach to
improve SLU robustness by randomly corrupting clean training text with an ASR
error simulator, followed by self-correcting the errors and minimizing the
target classification loss in a joint manner. In the proposed error simulator,
we leverage confusion networks generated from an ASR decoder without human
transcriptions to generate a variety of error patterns for model training. We
evaluate our approach on the DSTC10 challenge targeted for knowledge-grounded
task-oriented conversational dialogues with ASR errors. Experimental results
show the effectiveness of our proposed approach, boosting the knowledge-seeking
turn detection (KTD) F1 significantly from 0.9433 to 0.9904. Knowledge cluster
classification is boosted from 0.7924 to 0.9333 in Recall@1. After knowledge
document re-ranking, our approach shows significant improvement in all
knowledge selection metrics, from 0.7358 to 0.7806 in Recall@1, from 0.8301 to
0.9333 in Recall@5, and from 0.7798 to 0.8460 in MRR@5 on the test set. In the
recent DSTC10 evaluation, our approach demonstrates significant improvement in
knowledge selection, boosting Recall@1 from 0.495 to 0.7144 compared to the
official baseline. Our source code is released in GitHub
https://github.com/yctam/dstc10_track2_task2.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COV19IR : COVID-19 Domain Literature Information Retrieval. (arXiv:2211.04013v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04013">
<div class="article-summary-box-inner">
<span><p>Increasing number of COVID-19 research literatures cause new challenges in
effective literature screening and COVID-19 domain knowledge aware Information
Retrieval. To tackle the challenges, we demonstrate two tasks along
withsolutions, COVID-19 literature retrieval, and question answering. COVID-19
literature retrieval task screens matching COVID-19 literature documents for
textual user query, and COVID-19 question answering task predicts proper text
fragments from text corpus as the answer of specific COVID-19 related
questions. Based on transformer neural network, we provided solutions to
implement the tasks on CORD-19 dataset, we display some examples to show the
effectiveness of our proposed solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamic Graph Interactive Framework with Label-Semantic Injection for Spoken Language Understanding. (arXiv:2211.04023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04023">
<div class="article-summary-box-inner">
<span><p>Multi-intent detection and slot filling joint models are gaining increasing
traction since they are closer to complicated real-world scenarios. However,
existing approaches (1) focus on identifying implicit correlations between
utterances and one-hot encoded labels in both tasks while ignoring explicit
label characteristics; (2) directly incorporate multi-intent information for
each token, which could lead to incorrect slot prediction due to the
introduction of irrelevant intent. In this paper, we propose a framework termed
DGIF, which first leverages the semantic information of labels to give the
model additional signals and enriched priors. Then, a multi-grain interactive
graph is constructed to model correlations between intents and slots.
Specifically, we propose a novel approach to construct the interactive graph
based on the injection of label semantics, which can automatically update the
graph to better alleviate error propagation. Experimental results show that our
framework significantly outperforms existing approaches, obtaining a relative
improvement of 13.7% over the previous best model on the MixATIS dataset in
overall accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation. (arXiv:2211.04052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04052">
<div class="article-summary-box-inner">
<span><p>kNN-MT presents a new paradigm for domain adaptation by building an external
datastore, which usually saves all target language token occurrences in the
parallel corpus. As a result, the constructed datastore is usually large and
possibly redundant. In this paper, we investigate the interpretability issue of
this approach: what knowledge does the NMT model need? We propose the notion of
local correctness (LAC) as a new angle, which describes the potential
translation correctness for a single entry and for a given neighborhood.
Empirical study shows that our investigation successfully finds the conditions
where the NMT model could easily fail and need related knowledge. Experiments
on six diverse target domains and two language-pairs show that pruning
according to local correctness brings a light and more explainable memory for
kNN-MT domain adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications. (arXiv:2211.04054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04054">
<div class="article-summary-box-inner">
<span><p>Personal assistants, automatic speech recognizers and dialogue understanding
systems are becoming more critical in our interconnected digital world. A clear
example is air traffic control (ATC) communications. ATC aims at guiding
aircraft and controlling the airspace in a safe and optimal manner. These
voice-based dialogues are carried between an air traffic controller (ATCO) and
pilots via very-high frequency radio channels. In order to incorporate these
novel technologies into ATC (low-resource domain), large-scale annotated
datasets are required to develop the data-driven AI systems. Two examples are
automatic speech recognition (ASR) and natural language understanding (NLU). In
this paper, we introduce the ATCO2 corpus, a dataset that aims at fostering
research on the challenging ATC field, which has lagged behind due to lack of
annotated data. The ATCO2 corpus covers 1) data collection and pre-processing,
2) pseudo-annotations of speech data, and 3) extraction of ATC-related named
entities. The ATCO2 corpus is split into three subsets. 1) ATCO2-test-set
corpus contains 4 hours of ATC speech with manual transcripts and a subset with
gold annotations for named-entity recognition (callsign, command, value). 2)
The ATCO2-PL-set corpus consists of 5281 hours of unlabeled ATC data enriched
with automatic transcripts from an in-domain speech recognizer, contextual
information, speaker turn information, signal-to-noise ratio estimate and
English language detection score per sample. Both available for purchase
through ELDA at <a href="http://catalog.elra.info/en-us/repository/browse/ELRA-S0484.">this http URL</a> 3)
The ATCO2-test-set-1h corpus is a one-hour subset from the original test set
corpus, that we are offering for free at https://www.atco2.org/data. We expect
the ATCO2 corpus will foster research on robust ASR and NLU not only in the
field of ATC communications but also in the general research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-resolution embedding extractor for speaker diarisation. (arXiv:2211.04060v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04060">
<div class="article-summary-box-inner">
<span><p>Speaker embedding extractors significantly influence the performance of
clustering-based speaker diarisation systems. Conventionally, only one
embedding is extracted from each speech segment. However, because of the
sliding window approach, a segment easily includes two or more speakers owing
to speaker change points. This study proposes a novel embedding extractor
architecture, referred to as a high-resolution embedding extractor (HEE), which
extracts multiple high-resolution embeddings from each speech segment. Hee
consists of a feature-map extractor and an enhancer, where the enhancer with
the self-attention mechanism is the key to success. The enhancer of HEE
replaces the aggregation process; instead of a global pooling layer, the
enhancer combines relative information to each frame via attention leveraging
the global context. Extracted dense frame-level embeddings can each represent a
speaker. Thus, multiple speakers can be represented by different frame-level
features in each segment. We also propose an artificially generating mixture
data training framework to train the proposed HEE. Through experiments on five
evaluation sets, including four public datasets, the proposed HEE demonstrates
at least 10% improvement on each evaluation set, except for one dataset, which
we analyse that rapid speaker changes less exist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COPEN: Probing Conceptual Knowledge in Pre-trained Language Models. (arXiv:2211.04079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04079">
<div class="article-summary-box-inner">
<span><p>Conceptual knowledge is fundamental to human cognition and knowledge bases.
However, existing knowledge probing works only focus on evaluating factual
knowledge of pre-trained language models (PLMs) and ignore conceptual
knowledge. Since conceptual knowledge often appears as implicit commonsense
behind texts, designing probes for conceptual knowledge is hard. Inspired by
knowledge representation schemata, we comprehensively evaluate conceptual
knowledge of PLMs by designing three tasks to probe whether PLMs organize
entities by conceptual similarities, learn conceptual properties, and
conceptualize entities in contexts, respectively. For the tasks, we collect and
annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual
knowledge Probing bENchmark. Extensive experiments on different sizes and types
of PLMs show that existing PLMs systematically lack conceptual knowledge and
suffer from various spurious correlations. We believe this is a critical
bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are
publicly released at https://github.com/THU-KEG/COPEN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConsPrompt: Easily Exploiting Contrastive Samples for Few-shot Prompt Learning. (arXiv:2211.04118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04118">
<div class="article-summary-box-inner">
<span><p>Prompt learning recently become an effective linguistic tool to motivate the
PLMs' knowledge on few-shot-setting tasks. However, studies have shown the lack
of robustness still exists in prompt learning, since suitable initialization of
continuous prompt and expert-first manual prompt are essential in fine-tuning
process. What is more, human also utilize their comparative ability to motivate
their existing knowledge for distinguishing different examples. Motivated by
this, we explore how to use contrastive samples to strengthen prompt learning.
In detail, we first propose our model ConsPrompt combining with prompt encoding
network, contrastive sampling module, and contrastive scoring module.
Subsequently, two sampling strategies, similarity-based and label-based
strategies, are introduced to realize differential contrastive learning. The
effectiveness of proposed ConsPrompt is demonstrated in five different few-shot
learning tasks and shown the similarity-based sampling strategy is more
effective than label-based in combining contrastive learning. Our results also
exhibits the state-of-the-art performance and robustness in different few-shot
settings, which proves that the ConsPrompt could be assumed as a better
knowledge probe to motivate PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conciseness: An Overlooked Language Task. (arXiv:2211.04126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04126">
<div class="article-summary-box-inner">
<span><p>We report on novel investigations into training models that make sentences
concise. We define the task and show that it is different from related tasks
such as summarization and simplification. For evaluation, we release two test
sets, consisting of 2000 sentences each, that were annotated by two and five
human annotators, respectively. We demonstrate that conciseness is a difficult
task for which zero-shot setups with large neural language models often do not
perform well. Given the limitations of these approaches, we propose a synthetic
data generation method based on round-trip translations. Using this data to
either train Transformers from scratch or fine-tune T5 models yields our
strongest baselines that can be further improved by fine-tuning on an
artificial conciseness dataset that we derived from multi-annotator machine
translation test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning with Tabular Language Models. (arXiv:2211.04128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04128">
<div class="article-summary-box-inner">
<span><p>Despite recent advancements in tabular language model research, real-world
applications are still challenging. In industry, there is an abundance of
tables found in spreadsheets, but acquisition of substantial amounts of labels
is expensive, since only experts can annotate the often highly technical and
domain-specific tables. Active learning could potentially reduce labeling
costs, however, so far there are no works related to active learning in
conjunction with tabular language models. In this paper we investigate
different acquisition functions in a real-world industrial tabular language
model use case for sub-cell named entity recognition. Our results show that
cell-level acquisition functions with built-in diversity can significantly
reduce the labeling effort, while enforced table diversity is detrimental. We
further see open fundamental questions concerning computational efficiency and
the perspective of human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspectives on neural proof nets. (arXiv:2211.04141v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04141">
<div class="article-summary-box-inner">
<span><p>In this paper I will present a novel way of combining proof net proof search
with neural networks. It contrasts with the 'standard' approach which has been
applied to proof search in type-logical grammars in various different forms. In
the standard approach, we first transform words to formulas (supertagging) then
match atomic formulas to obtain a proof. I will introduce an alternative way to
split the task into two: first, we generate the graph structure in a way which
guarantees it corresponds to a lambda-term, then we obtain the detailed
structure using vertex labelling. Vertex labelling is a well-studied task in
graph neural networks, and different ways of implementing graph generation
using neural networks will be explored.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Specific Knowledge Graphs for Complex Finance Topics. (arXiv:2211.04142v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04142">
<div class="article-summary-box-inner">
<span><p>Across the financial domain, researchers answer complex questions by
extensively "searching" for relevant information to generate long-form reports.
This workshop paper discusses automating the construction of query-specific
document and entity knowledge graphs (KGs) for complex research topics. We
focus on the CODEC dataset, where domain experts (1) create challenging
questions, (2) construct long natural language narratives, and (3) iteratively
search and assess the relevance of documents and entities. For the construction
of query-specific KGs, we show that state-of-the-art ranking systems have
headroom for improvement, with specific failings due to a lack of context or
explicit knowledge representation. We demonstrate that entity and document
relevance are positively correlated, and that entity-based query feedback
improves document ranking effectiveness. Furthermore, we construct
query-specific KGs using retrieval and evaluate using CODEC's "ground-truth
graphs", showing the precision and recall trade-offs. Lastly, we point to
future work, including adaptive KG retrieval algorithms and GNN-based weighting
methods, while highlighting key challenges such as high-quality data,
information extraction recall, and the size and sparsity of complex topic
graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Third-Party Aligner for Neural Word Alignments. (arXiv:2211.04198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04198">
<div class="article-summary-box-inner">
<span><p>Word alignment is to find translationally equivalent words between source and
target sentences. Previous work has demonstrated that self-training can achieve
competitive word alignment results. In this paper, we propose to use word
alignments generated by a third-party word aligner to supervise the neural word
alignment training. Specifically, source word and target word of each word pair
aligned by the third-party aligner are trained to be close neighbors to each
other in the contextualized embedding space when fine-tuning a pre-trained
cross-lingual language model. Experiments on the benchmarks of various language
pairs show that our approach can surprisingly do self-correction over the
third-party supervision by finding more accurate word alignments and deleting
wrong word alignments, leading to better performance than various third-party
word aligners, including the currently best one. When we integrate all
supervisions from various third-party aligners, we achieve state-of-the-art
word alignment performances, with averagely more than two points lower
alignment error rates than the best third-party aligner. We released our code
at https://github.com/sdongchuanqi/Third-Party-Supervised-Aligner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preserving Semantics in Textual Adversarial Attacks. (arXiv:2211.04205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04205">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks in NLP challenge the way we look at language models. The
goal of this kind of adversarial attack is to modify the input text to fool a
classifier while maintaining the original meaning of the text. Although most
existing adversarial attacks claim to fulfill the constraint of semantics
preservation, careful scrutiny shows otherwise. We show that the problem lies
in the text encoders used to determine the similarity of adversarial examples,
specifically in the way they are trained. Unsupervised training methods make
these encoders more susceptible to problems with antonym recognition. To
overcome this, we introduce a simple, fully supervised sentence embedding
technique called Semantics-Preserving-Encoder (SPE). The results show that our
solution minimizes the variation in the meaning of the adversarial examples
generated. It also significantly improves the overall quality of adversarial
examples, as confirmed by human evaluators. Furthermore, it can be used as a
component in any existing attack to speed up its execution while maintaining
similar attack success.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Relation Discovery: Towards General and Label-aware Open Relation Extraction. (arXiv:2211.04215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04215">
<div class="article-summary-box-inner">
<span><p>Open Relation Extraction (OpenRE) aims to discover novel relations from open
domains. Previous OpenRE methods mainly suffer from two problems: (1)
Insufficient capacity to discriminate between known and novel relations. When
extending conventional test settings to a more general setting where test data
might also come from seen classes, existing approaches have a significant
performance decline. (2) Secondary labeling must be performed before practical
application. Existing methods cannot label human-readable and meaningful types
for novel relations, which is urgently required by the downstream tasks. To
address these issues, we propose the Active Relation Discovery (ARD) framework,
which utilizes relational outlier detection for discriminating known and novel
relations and involves active learning for labeling novel relations. Extensive
experiments on three real-world datasets show that ARD significantly
outperforms previous state-of-the-art methods on both conventional and our
proposed general OpenRE settings. The source code and datasets will be
available for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-conditioned Embedding Diffusion for Text Generation. (arXiv:2211.04236v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04236">
<div class="article-summary-box-inner">
<span><p>Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetAIL : A Tool to Automatically Detect and Analyze Drift In Language. (arXiv:2211.04250v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04250">
<div class="article-summary-box-inner">
<span><p>Machine learning and deep learning-based decision making has become part of
today's software. The goal of this work is to ensure that machine learning and
deep learning-based systems are as trusted as traditional software. Traditional
software is made dependable by following rigorous practice like static
analysis, testing, debugging, verifying, and repairing throughout the
development and maintenance life-cycle. Similarly for machine learning systems,
we need to keep these models up to date so that their performance is not
compromised. For this, current systems rely on scheduled re-training of these
models as new data kicks in. In this work, we propose to measure the data drift
that takes place when new data kicks in so that one can adaptively re-train the
models whenever re-training is actually required irrespective of schedules. In
addition to that, we generate various explanations at sentence level and
dataset level to capture why a given payload text has drifted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Fairness and Environmental Sustainability in Natural Language Processing. (arXiv:2211.04256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04256">
<div class="article-summary-box-inner">
<span><p>Fairness and environmental impact are important research directions for the
sustainable development of artificial intelligence. However, while each topic
is an active research area in natural language processing (NLP), there is a
surprising lack of research on the interplay between the two fields. This
lacuna is highly problematic, since there is increasing evidence that an
exclusive focus on fairness can actually hinder environmental sustainability,
and vice versa. In this work, we shed light on this crucial intersection in NLP
by (1) investigating the efficiency of current fairness approaches through
surveying example methods for reducing unfair stereotypical bias from the
literature, and (2) evaluating a common technique to reduce energy consumption
(and thus environmental impact) of English NLP models, knowledge distillation
(KD), for its impact on fairness. In this case study, we evaluate the effect of
important KD factors, including layer and dimensionality reduction, with
respect to: (a) performance on the distillation task (natural language
inference and semantic similarity prediction), and (b) multiple measures and
dimensions of stereotypical bias (e.g., gender bias measured via the Word
Embedding Association Test). Our results lead us to clarify current assumptions
regarding the effect of KD on unfair bias: contrary to other findings, we show
that KD can actually decrease model fairness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocioProbe: What, When, and Where Language Models Learn about Sociodemographics. (arXiv:2211.04281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04281">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have outperformed other NLP models on a
wide range of tasks. Opting for a more thorough understanding of their
capabilities and inner workings, researchers have established the extend to
which they capture lower-level knowledge like grammaticality, and mid-level
semantic knowledge like factual understanding. However, there is still little
understanding of their knowledge of higher-level aspects of language. In
particular, despite the importance of sociodemographic aspects in shaping our
language, the questions of whether, where, and how PLMs encode these aspects,
e.g., gender or age, is still unexplored. We address this research gap by
probing the sociodemographic knowledge of different single-GPU PLMs on multiple
English data sets via traditional classifier probing and information-theoretic
minimum description length probing. Our results show that PLMs do encode these
sociodemographics, and that this knowledge is sometimes spread across the
layers of some of the tested PLMs. We further conduct a multilingual analysis
and investigate the effect of supplementary training to further explore to what
extent, where, and with what amount of pre-training data the knowledge is
encoded. Our overall results indicate that sociodemographic knowledge is still
a major challenge for NLP. PLMs require large amounts of pre-training data to
acquire the knowledge and models that excel in general language understanding
do not seem to own more knowledge about these aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BER: Balanced Error Rate For Speaker Diarization. (arXiv:2211.04304v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04304">
<div class="article-summary-box-inner">
<span><p>DER is the primary metric to evaluate diarization performance while facing a
dilemma: the errors in short utterances or segments tend to be overwhelmed by
longer ones. Short segments, e.g., `yes' or `no,' still have semantic
information. Besides, DER overlooks errors in less-talked speakers. Although
JER balances speaker errors, it still suffers from the same dilemma.
Considering all those aspects, duration error, segment error, and
speaker-weighted error constituting a complete diarization evaluation, we
propose a Balanced Error Rate (BER) to evaluate speaker diarization. First, we
propose a segment-level error rate (SER) via connected sub-graphs and adaptive
IoU threshold to get accurate segment matching. Second, to evaluate diarization
in a unified way, we adopt a speaker-specific harmonic mean between duration
and segment, followed by a speaker-weighted average. Third, we analyze our
metric via the modularized system, EEND, and the multi-modal method on real
datasets. SER and BER are publicly available at https://github.com/X-LANCE/BER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning. (arXiv:2211.04325v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04325">
<div class="article-summary-box-inner">
<span><p>We analyze the growth of dataset sizes used in machine learning for natural
language processing and computer vision, and extrapolate these using two
methods; using the historical growth rate and estimating the compute-optimal
dataset size for future predicted compute budgets. We investigate the growth in
data usage by estimating the total stock of unlabeled data available on the
internet over the coming decades. Our analysis indicates that the stock of
high-quality language data will be exhausted soon; likely before 2026. By
contrast, the stock of low-quality language data and image data will be
exhausted only much later; between 2030 and 2050 (for low-quality language) and
between 2030 and 2060 (for images). Our work suggests that the current trend of
ever-growing ML models that rely on enormous datasets might slow down if data
efficiency is not drastically improved or new sources of data become available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Based Metric Learning for Few-Shot NER. (arXiv:2211.04337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04337">
<div class="article-summary-box-inner">
<span><p>Few-shot named entity recognition (NER) targets generalizing to unseen labels
and/or domains with few labeled examples. Existing metric learning methods
compute token-level similarities between query and support sets, but are not
able to fully incorporate label semantics into modeling. To address this issue,
we propose a simple method to largely improve metric learning for NER: 1)
multiple prompt schemas are designed to enhance label semantics; 2) we propose
a novel architecture to effectively combine multiple prompt-based
representations. Empirically, our method achieves new state-of-the-art (SOTA)
results under 16 of the 18 considered settings, substantially outperforming the
previous SOTA by an average of 8.84% and a maximum of 34.51% in relative gains
of micro F1. Our code is available at https://github.com/AChen-qaq/ProML.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?. (arXiv:2211.04364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04364">
<div class="article-summary-box-inner">
<span><p>While a substantial body of prior work has explored adversarial example
generation for natural language understanding tasks, these examples are often
unrealistic and diverge from the real-world data distributions. In this work,
we introduce a two-stage adversarial example generation framework
(NaturalAdversaries), for designing adversaries that are effective at fooling a
given classifier and demonstrate natural-looking failure cases that could
plausibly occur during in-the-wild deployment of the models.
</p>
<p>At the first stage a token attribution method is used to summarize a given
classifier's behaviour as a function of the key tokens in the input. In the
second stage a generative model is conditioned on the key tokens from the first
stage. NaturalAdversaries is adaptable to both black-box and white-box
adversarial attacks based on the level of access to the model parameters. Our
results indicate these adversaries generalize across domains, and offer
insights for future research on improving robustness of neural text
classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal Approach for Dementia Detection from Spontaneous Speech with Tensor Fusion Layer. (arXiv:2211.04368v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04368">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) is a progressive neurological disorder, meaning that
the symptoms develop gradually throughout the years. It is also the main cause
of dementia, which affects memory, thinking skills, and mental abilities.
Nowadays, researchers have moved their interest towards AD detection from
spontaneous speech, since it constitutes a time-effective procedure. However,
existing state-of-the-art works proposing multimodal approaches do not take
into consideration the inter- and intra-modal interactions and propose early
and late fusion approaches. To tackle these limitations, we propose deep neural
networks, which can be trained in an end-to-end trainable way and capture the
inter- and intra-modal interactions. Firstly, each audio file is converted to
an image consisting of three channels, i.e., log-Mel spectrogram, delta, and
delta-delta. Next, each transcript is passed through a BERT model followed by a
gated self-attention layer. Similarly, each image is passed through a Swin
Transformer followed by an independent gated self-attention layer. Acoustic
features are extracted also from each audio file. Finally, the representation
vectors from the different modalities are fed to a tensor fusion layer for
capturing the inter-modal interactions. Extensive experiments conducted on the
ADReSS Challenge dataset indicate that our introduced approaches obtain
valuable advantages over existing research initiatives reaching Accuracy and
F1-score up to 86.25% and 85.48% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nBIIG: A Neural BI Insights Generation System for Table Reporting. (arXiv:2211.04417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04417">
<div class="article-summary-box-inner">
<span><p>We present nBIIG, a neural Business Intelligence (BI) Insights Generation
system. Given a table, our system applies various analyses to create
corresponding RDF representations, and then uses a neural model to generate
fluent textual insights out of these representations. The generated insights
can be used by an analyst, via a human-in-the-loop paradigm, to enhance the
task of creating compelling table reports. The underlying generative neural
model is trained over large and carefully distilled data, curated from multiple
BI domains. Thus, the system can generate faithful and fluent insights over
open-domain tables, making it practical and useful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Order Matters when you Increase Masking. (arXiv:2211.04427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04427">
<div class="article-summary-box-inner">
<span><p>Word order, an essential property of natural languages, is injected in
Transformer-based neural language models using position encoding. However,
recent experiments have shown that explicit position encoding is not always
useful, since some models without such feature managed to achieve state-of-the
art performance on some tasks. To understand better this phenomenon, we examine
the effect of removing position encodings on the pre-training objective itself
(i.e., masked language modelling), to test whether models can reconstruct
position information from co-occurrences alone. We do so by controlling the
amount of masked tokens in the input sentence, as a proxy to affect the
importance of position information for the task. We find that the necessity of
position information increases with the amount of masking, and that masked
language models without position encodings are not able to reconstruct this
information on the task. These findings point towards a direct relationship
between the amount of masking and the ability of Transformers to capture
order-sensitive aspects of language using position encoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review of coreference resolution in English and Persian. (arXiv:2211.04428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04428">
<div class="article-summary-box-inner">
<span><p>Coreference resolution (CR) is one of the most challenging areas of natural
language processing. This task seeks to identify all textual references to the
same real-world entity. Research in this field is divided into coreference
resolution and anaphora resolution. Due to its application in textual
comprehension and its utility in other tasks such as information extraction
systems, document summarization, and machine translation, this field has
attracted considerable interest. Consequently, it has a significant effect on
the quality of these systems. This article reviews the existing corpora and
evaluation metrics in this field. Then, an overview of the coreference
algorithms, from rule-based methods to the latest deep learning techniques, is
provided. Finally, coreference resolution and pronoun resolution systems in
Persian are investigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content. (arXiv:2211.04454v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04454">
<div class="article-summary-box-inner">
<span><p>We present SLATE, a sequence labeling approach for extracting tasks from
free-form content such as digitally handwritten (or "inked") notes on a virtual
whiteboard. Our approach allows us to create a single, low-latency model to
simultaneously perform sentence segmentation and classification of these
sentences into task/non-task sentences. SLATE greatly outperforms a baseline
two-model (sentence segmentation followed by classification model) approach,
achieving a task F1 score of 84.4\%, a sentence segmentation (boundary
similarity) score of 88.4% and three times lower latency compared to the
baseline. Furthermore, we provide insights into tackling challenges of
performing NLP on the inking domain. We release both our code and dataset for
this novel task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperbolic Centroid Calculations for Text Classification. (arXiv:2211.04462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04462">
<div class="article-summary-box-inner">
<span><p>A new development in NLP is the construction of hyperbolic word embeddings.
As opposed to their Euclidean counterparts, hyperbolic embeddings are
represented not by vectors, but by points in hyperbolic space. This makes the
most common basic scheme for constructing document representations, namely the
averaging of word vectors, meaningless in the hyperbolic setting. We
reinterpret the vector mean as the centroid of the points represented by the
vectors, and investigate various hyperbolic centroid schemes and their
effectiveness at text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language. (arXiv:2103.16997v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16997">
<div class="article-summary-box-inner">
<span><p>We present a corpus professionally annotated for grammatical error correction
(GEC) and fluency edits in the Ukrainian language. To the best of our
knowledge, this is the first GEC corpus for the Ukrainian language. We
collected texts with errors (20,715 sentences) from a diverse pool of
contributors, including both native and non-native speakers. The data cover a
wide variety of writing domains, from text chats and essays to formal writing.
Professional proofreaders corrected and annotated the corpus for errors
relating to fluency, grammar, punctuation, and spelling. This corpus can be
used for developing and evaluating GEC systems in Ukrainian. More generally, it
can be used for researching multilingual and low-resource NLP, morphologically
rich languages, document-level GEC, and fluency correction. The corpus is
publicly available at https://github.com/grammarly/ua-gec
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00281">
<div class="article-summary-box-inner">
<span><p>Despite extensive research efforts in recent years, computational
argumentation (CA) remains one of the most challenging areas of natural
language processing. The reason for this is the inherent complexity of the
cognitive processes behind human argumentation, which integrate a plethora of
different types of knowledge, ranging from topic-specific facts and common
sense to rhetorical knowledge. The integration of knowledge from such a wide
range in CA requires modeling capabilities far beyond many other natural
language understanding tasks. Existing research on mining, assessing, reasoning
over, and generating arguments largely acknowledges that much more knowledge is
needed to accurately model argumentation computationally. However, a systematic
overview of the types of knowledge introduced in existing CA models is missing,
hindering targeted progress in the field. Adopting the operational definition
of knowledge as any task-relevant normative information not provided as input,
the survey paper at hand fills this gap by (1) proposing a taxonomy of types of
knowledge required in CA tasks, (2) systematizing the large body of CA work
according to the reliance on and exploitation of these knowledge types for the
four main research areas in CA, and (3) outlining and discussing directions for
future research efforts in CA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Optimal is Greedy Decoding for Extractive Question Answering?. (arXiv:2108.05857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05857">
<div class="article-summary-box-inner">
<span><p>Fine-tuned language models use greedy decoding to answer reading
comprehension questions with relative success. However, this approach does not
ensure that the answer is a span in the given passage, nor does it guarantee
that it is the most probable one. Does greedy decoding actually perform worse
than an algorithm that does adhere to these properties? To study the
performance and optimality of greedy decoding, we present exact-extract, a
decoding algorithm that efficiently finds the most probable answer span in the
context. We compare the performance of T5 with both decoding algorithms on
zero-shot and few-shot extractive question answering. When no training examples
are available, exact-extract significantly outperforms greedy decoding.
However, greedy decoding quickly converges towards the performance of
exact-extract with the introduction of a few training examples, becoming more
extractive and increasingly likelier to generate the most probable span as the
training set grows. We also show that self-supervised training can bias the
model towards extractive behavior, increasing performance in the zero-shot
setting without resorting to annotated examples. Overall, our results suggest
that pretrained language models are so good at adapting to extractive question
answering, that it is often enough to fine-tune on a small training set for the
greedy algorithm to emulate the optimal decoding strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Knowledge Base Question Answering: A Survey. (arXiv:2108.06688v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06688">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) aims to answer a question over a
knowledge base (KB). Early studies mainly focused on answering simple questions
over KBs and achieved great success. However, their performance on complex
questions is still far from satisfactory. Therefore, in recent years,
researchers propose a large number of novel methods, which looked into the
challenges of answering complex questions. In this survey, we review recent
advances on KBQA with the focus on solving complex questions, which usually
contain multiple subjects, express compound relations, or involve numerical
operations. In detail, we begin with introducing the complex KBQA task and
relevant background. Then, we describe benchmark datasets for complex KBQA task
and introduce the construction process of these datasets. Next, we present two
mainstream categories of methods for complex KBQA, namely semantic
parsing-based (SP-based) methods and information retrieval-based (IR-based)
methods. Specifically, we illustrate their procedures with flow designs and
discuss their major differences and similarities. After that, we summarize the
challenges that these two categories of methods encounter when answering
complex questions, and explicate advanced solutions and techniques used in
existing work. Finally, we conclude and discuss several promising directions
related to complex KBQA for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00976">
<div class="article-summary-box-inner">
<span><p>Laws and their interpretations, legal arguments and agreements\ are typically
expressed in writing, leading to the production of vast corpora of legal text.
Their analysis, which is at the center of legal practice, becomes increasingly
elaborate as these collections grow in size. Natural language understanding
(NLU) technologies can be a valuable tool to support legal practitioners in
these endeavors. Their usefulness, however, largely depends on whether current
state-of-the-art models can generalize across various tasks in the legal
domain. To answer this currently open question, we introduce the Legal General
Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets
for evaluating model performance across a diverse set of legal NLU tasks in a
standardized way. We also provide an evaluation and analysis of several generic
and legal-oriented models demonstrating that the latter consistently offer
performance improvements across multiple tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear algebra with transformers. (arXiv:2112.01898v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01898">
<div class="article-summary-box-inner">
<span><p>Transformers can learn to perform numerical computations from examples only.
I study nine problems of linear algebra, from basic matrix operations to
eigenvalue decomposition and inversion, and introduce and discuss four encoding
schemes to represent real numbers. On all problems, transformers trained on
sets of random matrices achieve high accuracies (over 90%). The models are
robust to noise, and can generalize out of their training distribution. In
particular, models trained to predict Laplace-distributed eigenvalues
generalize to different classes of matrices: Wigner matrices or matrices with
positive eigenvalues. The reverse is not true.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07657">
<div class="article-summary-box-inner">
<span><p>Sepsis is a life-threatening condition with organ dysfunction and is a
leading cause of death and critical illness worldwide. Accurate detection of
sepsis during emergency department triage would allow early initiation of lab
analysis, antibiotic administration, and other sepsis treatment protocols. The
purpose of this study was to determine whether EHR data can be extracted and
synthesized with the latest machine learning algorithms (KATE Sepsis) and
clinical natural language processing to produce accurate sepsis models, and
compare KATE Sepsis performance with existing sepsis screening protocols, such
as SIRS and qSOFA. A machine learning model (KATE Sepsis) was developed using
patient encounters with triage data from 16 participating hospitals. KATE
Sepsis, SIRS, standard screening (SIRS with source of infection) and qSOFA were
tested in three settings. Cohort-A was a retrospective analysis on medical
records from a single Site 1. Cohort-B was a prospective analysis of Site 1.
Cohort-C was a retrospective analysis on Site 1 with 15 additional sites.
Across all cohorts, KATE Sepsis demonstrates an AUC of 0.94-0.963 with
73-74.87% TPR and 3.76-7.17% FPR. Standard screening demonstrates an AUC of
0.682-0.726 with 39.39-51.19% TPR and 2.9-6.02% FPR. The qSOFA protocol
demonstrates an AUC of 0.544-0.56, with 10.52-13.18% TPR and 1.22-1.68% FPR.
For severe sepsis, across all cohorts, KATE Sepsis demonstrates an AUC of
0.935-0.972 with 70-82.26% TPR and 4.64-8.62% FPR. For septic shock, across all
cohorts, KATE Sepsis demonstrates an AUC of 0.96-0.981 with 85.71-89.66% TPR
and 4.85-8.8% FPR. SIRS, standard screening, and qSOFA demonstrate low AUC and
TPR for severe sepsis and septic shock detection. KATE Sepsis provided
substantially better sepsis detection performance in triage than commonly used
screening protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Control Globally, Understand Locally: A Global-to-Local Hierarchical Graph Network for Emotional Support Conversation. (arXiv:2204.12749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12749">
<div class="article-summary-box-inner">
<span><p>Emotional support conversation aims at reducing the emotional distress of the
help-seeker, which is a new and challenging task. It requires the system to
explore the cause of help-seeker's emotional distress and understand their
psychological intention to provide supportive responses. However, existing
methods mainly focus on the sequential contextual information, ignoring the
hierarchical relationships with the global cause and local psychological
intention behind conversations, thus leads to a weak ability of emotional
support. In this paper, we propose a Global-to-Local Hierarchical Graph Network
to capture the multi-source information (global cause, local intentions and
dialog history) and model hierarchical relationships between them, which
consists of a multi-source encoder, a hierarchical graph reasoner, and a
global-guide decoder. Furthermore, a novel training objective is designed to
monitor semantic information of the global cause. Experimental results on the
emotional support conversation dataset, ESConv, confirm that the proposed GLHG
has achieved the state-of-the-art performance on the automatic and human
evaluations. The code will be released in here
\footnote{\small{~https://github.com/pengwei-iie/GLHG}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Global and Local Hierarchies for Hierarchical Text Classification. (arXiv:2205.02613v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02613">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Curious Case of Control. (arXiv:2205.12113v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12113">
<div class="article-summary-box-inner">
<span><p>Children acquiring English make systematic errors on subject control
sentences even after they have reached near-adult competence (C. Chomsky,
1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).
Given the advanced fluency of large generative language models, we ask whether
model outputs are consistent with these heuristics, and to what degree
different models are consistent with each other. We find that models can be
categorized by behavior into three separate groups, with broad differences
between the groups. The outputs of models in the largest group are consistent
with positional heuristics that succeed on subject control but fail on object
control. This result is surprising, given that object control is orders of
magnitude more frequent in the text data used to train such models. We examine
to what degree the models are sensitive to prompting with agent-patient
information, finding that raising the salience of agent and patient relations
results in significant changes in the outputs of most models. Based on this
observation, we leverage an existing dataset of semantic proto-role annotations
(White, et al. 2020) to explore the connections between control and labeling
event participants with properties typically associated with agents and
patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems. (arXiv:2205.12228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12228">
<div class="article-summary-box-inner">
<span><p>In natural language understanding (NLU) production systems, users' evolving
needs necessitate the addition of new features over time, indexed by new
symbols added to the meaning representation space. This requires additional
training data and results in ever-growing datasets. We present the first
systematic investigation of this incremental symbol learning scenario. Our
analysis reveals a troubling quirk in building broad-coverage NLU systems: as
the training dataset grows, performance on the new symbol often decreases if we
do not accordingly increase its training data. This suggests that it becomes
more difficult to learn new symbols with a larger training dataset. We show
that this trend holds for multiple mainstream models on two common NLU tasks:
intent recognition and semantic parsing. Rejecting class imbalance as the sole
culprit, we reveal that the trend is closely associated with an effect we call
source signal dilution, where strong lexical cues for the new symbol become
diluted as the training dataset grows. Selectively dropping training examples
to prevent dilution often reverses the trend, showing the over-reliance of
mainstream neural NLU models on simple lexical cues. Code, models, and data are
available at https://aka.ms/nlu-incremental-symbol-learning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-Based Constrained Sampling from Language Models. (arXiv:2205.12558v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12558">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models generate fluent text but are notoriously
hard to controllably sample from. In this work, we study constrained sampling
from such language models: generating text that satisfies user-defined
constraints, while maintaining fluency and the model's performance in a
downstream task. We propose MuCoLa -- a sampling procedure that combines the
log-likelihood of the language model with arbitrary (differentiable)
constraints in a single energy function, and then generates samples in a
non-autoregressive manner. Specifically, it initializes the entire output
sequence with noise and follows a Markov chain defined by Langevin Dynamics
using the gradients of the energy function. We evaluate MuCoLa on text
generation with soft and hard constraints as well as their combinations
obtaining significant improvements over competitive baselines for toxicity
avoidance, sentiment control, and keyword-guided generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning. (arXiv:2205.12598v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12598">
<div class="article-summary-box-inner">
<span><p>Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in English natural
language. While the progress is promising, it is currently unclear if these
models indeed perform logical reasoning by understanding the underlying logical
semantics in the language. To this end, we propose RobustLR, a suite of
evaluation datasets that evaluate the robustness of these models to minimal
logical edits in rulebases and some standard logical equivalence conditions. In
our experiments with RoBERTa and T5, we find that the models trained in prior
works do not perform consistently on the different perturbations in RobustLR,
thus showing that the models are not robust to the proposed logical
perturbations. Further, we find that the models find it especially hard to
learn logical negation and disjunction operators. Overall, using our evaluation
sets, we demonstrate some shortcomings of the deductive reasoning-based
language models, which can eventually help towards designing better models for
logical reasoning over natural language. All the datasets and code base have
been made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Large-scale Paraphrase Acquisition and Generation. (arXiv:2210.03235v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03235">
<div class="article-summary-box-inner">
<span><p>This paper addresses the quality issues in existing Twitter-based paraphrase
datasets, and discusses the necessity of using two separate definitions of
paraphrase for identification and generation tasks. We present a new
Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of
130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert
(MultiPIT_expert) annotations using two different paraphrase definitions for
paraphrase identification, in addition to a multi-reference test set
(MultiPIT_NMR) and a large automatically constructed training set
(MultiPIT_Auto) for paraphrase generation. With improved data annotation
quality and task-specific paraphrase definition, the best pre-trained language
model fine-tuned on our dataset achieves the state-of-the-art performance of
84.2 F1 for automatic paraphrase identification. Furthermore, our empirical
results also demonstrate that the paraphrase generation models trained on
MultiPIT_Auto generate more diverse and high-quality paraphrases compared to
their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and
ParaNMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAPS: A Novel Few-Shot Relation Extraction Pipeline with Query-Information Guided Attention and Adaptive Prototype Fusion. (arXiv:2210.08242v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08242">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction (FSRE) aims at recognizing unseen relations by
learning with merely a handful of annotated instances. To generalize to new
relations more effectively, this paper proposes a novel pipeline for the FSRE
task based on queRy-information guided Attention and adaptive Prototype fuSion,
namely RAPS. Specifically, RAPS first derives the relation prototype by the
query-information guided attention module, which exploits rich interactive
information between the support instances and the query instances, in order to
obtain more accurate initial prototype representations. Then RAPS elaborately
combines the derived initial prototype with the relation information by the
adaptive prototype fusion mechanism to get the integrated prototype for both
train and prediction. Experiments on the benchmark dataset FewRel 1.0 show a
significant improvement of our method against state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Language Model to Train if You Have One Million GPU Hours?. (arXiv:2210.15424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15424">
<div class="article-summary-box-inner">
<span><p>The crystallization of modeling methods around the Transformer architecture
has been a boon for practitioners. Simple, well-motivated architectural
variations can transfer across tasks and scale, increasing the impact of
modeling research. However, with the emergence of state-of-the-art 100B+
parameters models, large language models are increasingly expensive to
accurately design and train. Notably, it can be difficult to evaluate how
modeling decisions may impact emergent capabilities, given that these
capabilities arise mainly from sheer scale alone. In the process of building
BLOOM--the Big Science Large Open-science Open-access Multilingual language
model--our goal is to identify an architecture and training setup that makes
the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform
an ablation study at the billion-parameter scale comparing different modeling
practices and their impact on zero-shot generalization. In addition, we study
the impact of various popular pre-training corpora on zero-shot generalization.
We also study the performance of a multilingual model and how it compares to
the English-only one. Finally, we consider the scaling behaviour of
Transformers to choose the target model size, shape, and training setup. All
our models and code are open-sourced at https://huggingface.co/bigscience .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17406">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been reported to have strong performance on
natural language processing tasks. However, performance metrics such as
accuracy do not measure the quality of the model in terms of its ability to
robustly represent complex linguistic structure. In this work, we propose a
framework to evaluate the robustness of linguistic representations using
probing tasks. We leverage recent advances in extracting emergent linguistic
constructs from LLMs and apply syntax-preserving perturbations to test the
stability of these constructs in order to better understand the representations
learned by LLMs. Empirically, we study the performance of four LLMs across six
different corpora on the proposed robustness measures. We provide evidence that
context-free representation (e.g., GloVe) are in some cases competitive with
context-dependent representations from modern LLMs (e.g., BERT), yet equally
brittle to syntax-preserving manipulations. Emergent syntactic representations
in neural networks are brittle, thus our work poses the attention on the risk
of comparing such structures to those that are object of a long lasting debate
in linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chronic pain patient narratives allow for the estimation of current pain intensity. (arXiv:2210.17473v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17473">
<div class="article-summary-box-inner">
<span><p>Chronic pain is a multi-dimensional experience, and pain intensity plays an
important part, impacting the patients emotional balance, psychology, and
behaviour. Standard self-reporting tools, such as the Visual Analogue Scale for
pain, fail to capture this burden. Moreover, this type of tools is susceptible
to a degree of subjectivity, dependent on the patients clear understanding of
how to use it, social biases, and their ability to translate a complex
experience to a scale. To overcome these and other self-reporting challenges,
pain intensity estimation has been previously studied based on facial
expressions, electroencephalograms, brain imaging, and autonomic features.
However, to the best of our knowledge, it has never been attempted to base this
estimation on the patient narratives of the personal experience of chronic
pain, which is what we propose in this work. Indeed, in the clinical assessment
and management of chronic pain, verbal communication is essential to convey
information to physicians that would otherwise not be easily accessible through
standard reporting tools, since language, sociocultural, and psychosocial
variables are intertwined. We show that language features from patient
narratives indeed convey information relevant for pain intensity estimation,
and that our computational models can take advantage of that. Specifically, our
results show that patients with mild pain focus more on the use of verbs,
whilst moderate and severe pain patients focus on adverbs, and nouns and
adjectives, respectively, and that these differences allow for the distinction
between these three pain classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation. (arXiv:2211.01587v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01587">
<div class="article-summary-box-inner">
<span><p>Recent advances in large-scale pre-training provide large models with the
potential to learn knowledge from the raw text. It is thus natural to ask
whether it is possible to leverage these large models as knowledge bases for
downstream tasks. In this work, we answer the aforementioned question in
unsupervised knowledge-grounded conversation. We explore various methods that
best elicit knowledge from large models. Our human study indicates that, though
hallucinations exist, large models post the unique advantage of being able to
output common sense and summarize facts that cannot be directly retrieved from
the search engine. To better exploit such generated knowledge in dialogue
generation, we treat the generated knowledge as a noisy knowledge source and
propose the posterior-based reweighing as well as the noisy training strategy.
Empirical results on two benchmarks show advantages over the state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-09 23:17:27.408606196 UTC">2022-11-09 23:17:27 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>