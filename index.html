<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-21T01:30:00Z">08-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis. (arXiv:2308.08577v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08577">
<div class="article-summary-box-inner">
<span><p>Affect is an emotional characteristic encompassing valence, arousal, and
intensity, and is a crucial attribute for enabling authentic conversations.
While existing text-to-speech (TTS) and speech-to-speech systems rely on
strength embedding vectors and global style tokens to capture emotions, these
models represent emotions as a component of style or represent them in discrete
categories. We propose AffectEcho, an emotion translation model, that uses a
Vector Quantized codebook to model emotions within a quantized space featuring
five levels of affect intensity to capture complex nuances and subtle
differences in the same emotion. The quantized emotional embeddings are
implicitly derived from spoken speech samples, eliminating the need for one-hot
vectors or explicit strength embeddings. Experimental results demonstrate the
effectiveness of our approach in controlling the emotions of generated speech
while preserving identity, style, and emotional cadence unique to each speaker.
We showcase the language-independent emotion modeling capability of the
quantized emotional embeddings learned from a bilingual (English and Chinese)
speech corpus with an emotion transfer task from a reference speech to a target
speech. We achieve state-of-art results on both qualitative and quantitative
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FootGPT : A Large Language Model Development Experiment on a Minimal Setting. (arXiv:2308.08610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08610">
<div class="article-summary-box-inner">
<span><p>With recent empirical observations, it has been argued that the most
significant aspect of developing accurate language models may be the proper
dataset content and training strategy compared to the number of neural
parameters, training duration or dataset size. Following this argument, we
opted to fine tune a one billion parameter size trained general purpose causal
language model with a dataset curated on team statistics of the Italian
football league first ten game weeks, using low rank adaptation. The limited
training dataset was compiled based on a framework where a powerful commercial
large language model provides distilled paragraphs and question answer pairs as
intended. The training duration was kept relatively short to provide a basis
for our minimal setting exploration. We share our key observations on the
process related to developing a specific purpose language model which is
intended to interpret soccer data with constrained resources in this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought. (arXiv:2308.08614v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08614">
<div class="article-summary-box-inner">
<span><p>Recent advancements in large-scale models, such as GPT-4, have showcased
remarkable capabilities in addressing standard queries. However, when facing
complex problems that require multi-step logical reasoning, their accuracy
dramatically decreases. Current research has explored the realm of
\textit{prompting engineering} to bolster the inferential capacities of these
models. Our paper unveils a pioneering prompting technique, dubbed
\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating
challenges: the 24-point game, resolution of high-degree polynomial equations,
and derivation of formulas for recursive sequences, our method outperformed
GPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for each
respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA)
prompting method, \textit{Tree of Thought (ToT)}, our approach registered an
average accuracy boost of $23\%$, $24\%$, and $15\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition. (arXiv:2308.08625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08625">
<div class="article-summary-box-inner">
<span><p>Using language models (LMs) pre-trained in a self-supervised setting on large
corpora and then fine-tuning for a downstream task has helped to deal with the
problem of limited label data for supervised learning tasks such as Named
Entity Recognition (NER). Recent research in biomedical language processing has
offered a number of biomedical LMs pre-trained using different methods and
techniques that advance results on many BioNLP tasks, including NER. However,
there is still a lack of a comprehensive comparison of pre-training approaches
that would work more optimally in the biomedical domain. This paper aims to
investigate different pre-training methods, such as pre-training the biomedical
LM from scratch and pre-training it in a continued fashion. We compare existing
methods with our proposed pre-training method of initializing weights for new
tokens by distilling existing weights from the BERT model inside the context
where the tokens were found. The method helps to speed up the pre-training
stage and improve performance on NER. In addition, we compare how masking rate,
corruption strategy, and masking strategies impact the performance of the
biomedical LM. Finally, using the insights from our experiments, we introduce a
new biomedical LM (BIOptimus), which is pre-trained using Curriculum Learning
(CL) and contextualized weight distillation method. Our model sets new states
of the art on several biomedical Named Entity Recognition (NER) tasks. We
release our code and all pre-trained models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08628">
<div class="article-summary-box-inner">
<span><p>Interpreting a seemingly-simple function word like "or", "behind", or "more"
can require logical, numerical, and relational reasoning. How are such words
learned by children? Prior acquisition theories have often relied on positing a
foundation of innate knowledge. Yet recent neural-network based visual question
answering models apparently can learn to use function words as part of
answering questions about complex visual scenes. In this paper, we study what
these models learn about function words, in the hope of better understanding
how the meanings of these words can be learnt by both models and children. We
show that recurrent models trained on visually grounded language learn gradient
semantics for function words requiring spacial and numerical reasoning.
Furthermore, we find that these models can learn the meanings of logical
connectives "and" and "or" without any prior knowledge of logical reasoning, as
well as early evidence that they can develop the ability to reason about
alternative expressions when interpreting language. Finally, we show that word
learning difficulty is dependent on frequency in models' input. Our findings
offer evidence that it is possible to learn the meanings of function words in
visually grounded context by using non-symbolic general statistical learning
algorithms, without any prior knowledge of linguistic meaning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Granularized Barrett's Esophagus Diagnosis Classification. (arXiv:2308.08660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08660">
<div class="article-summary-box-inner">
<span><p>Diagnostic codes for Barrett's esophagus (BE), a precursor to esophageal
cancer, lack granularity and precision for many research or clinical use cases.
Laborious manual chart review is required to extract key diagnostic phenotypes
from BE pathology reports. We developed a generalizable transformer-based
method to automate data extraction. Using pathology reports from Columbia
University Irving Medical Center with gastroenterologist-annotated targets, we
performed binary dysplasia classification as well as granularized multi-class
BE-related diagnosis classification. We utilized two clinically pre-trained
large language models, with best model performance comparable to a highly
tailored rule-based system developed using the same data. Binary dysplasia
extraction achieves 0.964 F1-score, while the multi-class model achieves 0.911
F1-score. Our method is generalizable and faster to implement as compared to a
tailored rule-based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions. (arXiv:2308.08661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08661">
<div class="article-summary-box-inner">
<span><p>Many open-domain questions are under-specified and thus have multiple
possible answers, each of which is correct under a different interpretation of
the question. Answering such ambiguous questions is challenging, as it requires
retrieving and then reasoning about diverse information from multiple passages.
We present a new state-of-the-art for answering ambiguous questions that
exploits a database of unambiguous questions generated from Wikipedia. On the
challenging ASQA benchmark, which requires generating long-form answers that
summarize the multiple answers to an ambiguous question, our method improves
performance by 15% (relative improvement) on recall measures and 10% on
measures which evaluate disambiguating questions from predicted outputs.
Retrieving from the database of generated questions also gives large
improvements in diverse passage retrieval (by matching user questions q to
passages p indirectly, via questions q' generated from p).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Adaptation of Neural Language Models via Subspace Embedding. (arXiv:2308.08688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08688">
<div class="article-summary-box-inner">
<span><p>Traditional neural word embeddings are usually dependent on a richer
diversity of vocabulary. However, the language models recline to cover major
vocabularies via the word embedding parameters, in particular, for multilingual
language models that generally cover a significant part of their overall
learning parameters. In this work, we present a new compact embedding structure
to reduce the memory footprint of the pre-trained language models with a
sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction
follows a set of subspace embeddings and an assignment procedure via the
contextual relationship among tokens from pre-trained language models. The
subspace embedding structure calibrates to masked language models, to evaluate
our compact embedding structure on similarity and textual entailment tasks,
sentence and paraphrase tasks. Our experimental evaluation shows that the
subspace embeddings achieve compression rates beyond 99.8% in comparison with
the original embeddings for the language models on XNLI and GLUE benchmark
suites.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition. (arXiv:2308.08713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08713">
<div class="article-summary-box-inner">
<span><p>Recent advancements in transformer-based speech representation models have
greatly transformed speech processing. However, there has been limited research
conducted on evaluating these models for speech emotion recognition (SER)
across multiple languages and examining their internal representations. This
article addresses these gaps by presenting a comprehensive benchmark for SER
with eight speech representation models and six different languages. We
conducted probing experiments to gain insights into inner workings of these
models for SER. We find that using features from a single optimal layer of a
speech model reduces the error rate by 32\% on average across seven datasets
when compared to systems where features from all layers of speech models are
used. We also achieve state-of-the-art results for German and Persian
languages. Our probing results indicate that the middle layers of speech models
capture the most important emotional information for speech emotion
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM. (arXiv:2308.08728v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08728">
<div class="article-summary-box-inner">
<span><p>As a vital stage of automated rule checking (ARC), rule interpretation of
regulatory texts requires considerable effort. However, interpreting regulatory
clauses with implicit properties or complex computational logic is still
challenging due to the lack of domain knowledge and limited expressibility of
conventional logic representations. Thus, LLM-FuncMapper, an approach to
identifying predefined functions needed to interpret various regulatory clauses
based on the large language model (LLM), is proposed. First, by systematically
analysis of building codes, a series of atomic functions are defined to capture
shared computational logics of implicit properties and complex constraints,
creating a database of common blocks for interpreting regulatory clauses. Then,
a prompt template with the chain of thought is developed and further enhanced
with a classification-based tuning strategy, to enable common LLMs for
effective function identification. Finally, the proposed approach is validated
with statistical analysis, experiments, and proof of concept. Statistical
analysis reveals a long-tail distribution and high expressibility of the
developed function database, with which almost 100% of computer-processible
clauses can be interpreted and represented as computer-executable codes.
Experiments show that LLM-FuncMapper achieve promising results in identifying
relevant predefined functions for rule interpretation. Further proof of concept
in automated rule interpretation also demonstrates the possibility of
LLM-FuncMapper in interpreting complex regulatory clauses. To the best of our
knowledge, this study is the first attempt to introduce LLM for understanding
and interpreting complex regulatory clauses, which may shed light on further
adoption of LLM in the construction domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Phrase Representation by Information Bottleneck Guided Text Diffusion Process for Keyphrase Extraction. (arXiv:2308.08739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08739">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction (KPE) is an important task in Natural Language
Processing for many scenarios, which aims to extract keyphrases that are
present in a given document. Many existing supervised methods treat KPE as
sequential labeling, span-level classification, or generative tasks. However,
these methods lack the ability to utilize keyphrase information, which may
result in biased results. In this study, we propose Diff-KPE, which leverages
the supervised Variational Information Bottleneck (VIB) to guide the text
diffusion process for generating enhanced keyphrase representations. Diff-KPE
first generates the desired keyphrase embeddings conditioned on the entire
document and then injects the generated keyphrase embeddings into each phrase
representation. A ranking network and VIB are then optimized together with rank
loss and classification loss, respectively. This design of Diff-KPE allows us
to rank each candidate phrase by utilizing both the information of keyphrases
and the document. Experiments show that Diff-KPE outperforms existing KPE
methods on a large open domain keyphrase extraction benchmark, OpenKP, and a
scientific domain dataset, KP20K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08742">
<div class="article-summary-box-inner">
<span><p>Model editing techniques modify a minor proportion of knowledge in Large
Language Models (LLMs) at a relatively low cost, which have demonstrated
notable success. Existing methods assume Transformer Layer (TL) hidden states
are values of key-value memories of the Feed-Forward Network (FFN). They
usually optimize the TL hidden states to memorize target knowledge and use it
to update the weights of the FFN in LLMs. However, the information flow of TL
hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,
and residual connections. Existing methods neglect the fact that the TL hidden
states contains information not specifically required for FFN. Consequently,
the performance of model editing decreases. To achieve more precise model
editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes
certain general knowledge extraction patterns. This implies that MHSA weights
do not require updating when new knowledge is introduced. Based on above
findings, we introduce PMET, which simultaneously optimizes Transformer
Component (TC, namely MHSA and FFN) hidden states, while only using the
optimized TC hidden states of FFN to precisely update FFN weights. Our
experiments demonstrate that PMET exhibits state-of-the-art performance on both
the \textsc{counterfact} and zsRE datasets. Our ablation experiments
substantiate the effectiveness of our enhancements, further reinforcing the
finding that the MHSA encodes certain general knowledge extraction patterns and
indicating its storage of a small amount of factual knowledge. Our code is
available at \url{https://github.com/xpq-tech/PMET.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08747">
<div class="article-summary-box-inner">
<span><p>Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning
when a model forgets previously learned information as it learns new
information. As large language models (LLMs) have shown excellent performance,
it is interesting to uncover whether CF exists in the continual fine-tuning of
LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs'
knowledge, from the perspectives of domain knowledge, reasoning, and reading
comprehension. The experiments demonstrate that catastrophic forgetting is
generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale
increases, the severity of forgetting also intensifies. Comparing the
decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers
less forgetting and maintains more knowledge. We also observe that LLMs can
mitigate language bias (e.g. gender bias) during continual fine-tuning.
Moreover, we find that ALPACA can maintain more knowledge and capacity compared
with LLAMA during the continual fine-tuning, which implies that general
instruction tuning can help mitigate the forgetting phenomenon of LLMs in the
further fine-tuning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08758">
<div class="article-summary-box-inner">
<span><p>Instruction-tuned Language Models (LMs) are widely used by users to address
various problems with task-specific prompts. Constraints associated with the
context window length and computational costs encourage the development of
compressed prompts. Existing methods rely heavily on training embeddings, which
are designed to accommodate multiple token meanings. This presents challenges
in terms of interpretability, a fixed number of embedding tokens, reusability
across different LMs, and inapplicability when interacting with black-box APIs.
This study proposes prompt compression with reinforcement learning (PCRL), a
novel discrete prompt compression method that addresses these issues. PCRL
employs a computationally efficient policy network that directly edits prompts.
The PCRL training approach can be flexibly applied to various types of LMs, as
well as decoder-only and encoder-decoder architecture, and can be trained
without gradient access to LMs or labeled data. PCRL achieves an average
reduction of 24.6% in token count across various instruction prompts while
preserving performance. Further, we demonstrate that the learned policy can be
transferred to larger LMs, and through various analyses, we aid the
understanding of token importance within prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models. (arXiv:2308.08774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08774">
<div class="article-summary-box-inner">
<span><p>Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual
generalization or compression to facilitate transfer to a large number of
(potentially unseen) languages. However, these models should ideally also be
private, linguistically fair, and transparent, by relating their predictions to
training data. Can these requirements be simultaneously satisfied? We show that
multilingual compression and linguistic fairness are compatible with
differential privacy, but that differential privacy is at odds with training
data influence sparsity, an objective for transparency. We further present a
series of experiments on two common NLP tasks and evaluate multilingual
compression and training data influence sparsity under different privacy
guarantees, exploring these trade-offs in more detail. Our results suggest that
we need to develop ways to jointly optimize for these objectives in order to
find practical trade-offs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08780">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) operates by showing language models (LMs) examples
of input-output pairs for a given task, i.e., demonstrations. The standard
approach for ICL is to prompt the LM with concatenated demonstrations followed
by the test input. This approach suffers from some issues. First, concatenation
offers almost no control over the contribution of each demo to the model
prediction. This can be sub-optimal when some demonstrations are irrelevant to
the test example. Second, due to the input length limit of some transformer
models, it might be infeasible to fit many examples into the context,
especially when dealing with long-input tasks. In this work, we explore
Demonstration Ensembling (DENSE) as an alternative to simple concatenation.
\model predicts outputs using subsets (i.e., buckets) of the demonstrations and
then combines the output probabilities resulting from each subset to produce
the final prediction. We study different ensembling methods using GPT-j and
experiment on 12 language tasks. Our experiments show weighted max ensembling
to outperform vanilla concatenation by as large as 2.4 average points. Code
available at \url{https://github.com/mukhal/icl-ensembling}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Relation Distillation and Prototypical Pseudo Label for Incremental Named Entity Recognition. (arXiv:2308.08793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08793">
<div class="article-summary-box-inner">
<span><p>Incremental Named Entity Recognition (INER) involves the sequential learning
of new entity types without accessing the training data of previously learned
types. However, INER faces the challenge of catastrophic forgetting specific
for incremental learning, further aggravated by background shift (i.e., old and
future entity types are labeled as the non-entity type in the current task). To
address these challenges, we propose a method called task Relation Distillation
and Prototypical pseudo label (RDP) for INER. Specifically, to tackle
catastrophic forgetting, we introduce a task relation distillation scheme that
serves two purposes: 1) ensuring inter-task semantic consistency across
different incremental learning tasks by minimizing inter-task relation
distillation loss, and 2) enhancing the model's prediction confidence by
minimizing intra-task self-entropy loss. Simultaneously, to mitigate background
shift, we develop a prototypical pseudo label strategy that distinguishes old
entity types from the current non-entity type using the old model. This
strategy generates high-quality pseudo labels by measuring the distances
between token embeddings and type-wise prototypes. We conducted extensive
experiments on ten INER settings of three benchmark datasets (i.e., CoNLL2003,
I2B2, and OntoNotes5). The results demonstrate that our method achieves
significant improvements over the previous state-of-the-art methods, with an
average increase of 6.08% in Micro F1 score and 7.71% in Macro F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Spelling Correction as Rephrasing Language Model. (arXiv:2308.08796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08796">
<div class="article-summary-box-inner">
<span><p>This paper studies Chinese Spelling Correction (CSC), which aims to detect
and correct potential spelling errors in a given sentence. Current
state-of-the-art methods regard CSC as a sequence tagging task and fine-tune
BERT-based models on sentence pairs. However, we note a critical flaw in the
process of tagging one character to another, that the correction is excessively
conditioned on the error. This is opposite from human mindset, where
individuals rephrase the complete sentence based on its semantics, rather than
solely on the error patterns memorized before. Such a counter-intuitive
learning process results in the bottleneck of generalizability and
transferability of machine spelling correction. To address this, we propose
$Rephrasing Language Modeling$ (ReLM), where the model is trained to rephrase
the entire sentence by infilling additional slots, instead of
character-to-character tagging. This novel training paradigm achieves the new
state-of-the-art results across fine-tuned and zero-shot CSC benchmarks,
outperforming previous counterparts by a large margin. Our method also learns
transferable language representation when CSC is jointly trained with other
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistically-Informed Neural Architectures for Lexical, Syntactic and Semantic Tasks in Sanskrit. (arXiv:2308.08807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08807">
<div class="article-summary-box-inner">
<span><p>The primary focus of this thesis is to make Sanskrit manuscripts more
accessible to the end-users through natural language technologies. The
morphological richness, compounding, free word orderliness, and low-resource
nature of Sanskrit pose significant challenges for developing deep learning
solutions. We identify four fundamental tasks, which are crucial for developing
a robust NLP technology for Sanskrit: word segmentation, dependency parsing,
compound type identification, and poetry analysis. The first task, Sanskrit
Word Segmentation (SWS), is a fundamental text processing task for any other
downstream applications. However, it is challenging due to the sandhi
phenomenon that modifies characters at word boundaries. Similarly, the existing
dependency parsing approaches struggle with morphologically rich and
low-resource languages like Sanskrit. Compound type identification is also
challenging for Sanskrit due to the context-sensitive semantic relation between
components. All these challenges result in sub-optimal performance in NLP
applications like question answering and machine translation. Finally, Sanskrit
poetry has not been extensively studied in computational linguistics.
</p>
<p>While addressing these challenges, this thesis makes various contributions:
(1) The thesis proposes linguistically-informed neural architectures for these
tasks. (2) We showcase the interpretability and multilingual extension of the
proposed systems. (3) Our proposed systems report state-of-the-art performance.
(4) Finally, we present a neural toolkit named SanskritShala, a web-based
application that provides real-time analysis of input for various NLP tasks.
Overall, this thesis contributes to making Sanskrit manuscripts more accessible
by developing robust NLP technology and releasing various resources, datasets,
and web-based toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factuality Detection using Machine Translation -- a Use Case for German Clinical Text. (arXiv:2308.08827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08827">
<div class="article-summary-box-inner">
<span><p>Factuality can play an important role when automatically processing clinical
text, as it makes a difference if particular symptoms are explicitly not
present, possibly present, not mentioned, or affirmed. In most cases, a
sufficient number of examples is necessary to handle such phenomena in a
supervised machine learning setting. However, as clinical text might contain
sensitive information, data cannot be easily shared. In the context of
factuality detection, this work presents a simple solution using machine
translation to translate English data to German to train a transformer-based
factuality detection model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMB: A Comprehensive Medical Benchmark in Chinese. (arXiv:2308.08833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08833">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) provide a possibility to make a great
breakthrough in medicine. The establishment of a standardized medical benchmark
becomes a fundamental cornerstone to measure progression. However, medical
environments in different regions have their local characteristics, e.g., the
ubiquity and significance of traditional Chinese medicine within China.
Therefore, merely translating English-based medical evaluation may result in
\textit{contextual incongruities} to a local region. To solve the issue, we
propose a localized medical benchmark called CMB, a Comprehensive Medical
Benchmark in Chinese, designed and rooted entirely within the native Chinese
linguistic and cultural framework. While traditional Chinese medicine is
integral to this evaluation, it does not constitute its entirety. Using this
benchmark, we have evaluated several prominent large-scale LLMs, including
ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical
domain. It is worth noting that our benchmark is not devised as a leaderboard
competition but as an instrument for self-assessment of model advancements. We
hope this benchmark could facilitate the widespread adoption and enhancement of
medical LLMs within China. Check details in
\url{https://cmedbenchmark.llmzoo.com/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering. (arXiv:2308.08973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08973">
<div class="article-summary-box-inner">
<span><p>Multi-hop QA involves finding multiple relevant passages and step-by-step
reasoning to answer complex questions. While previous approaches have developed
retrieval modules for selecting relevant passages, they face challenges in
scenarios beyond two hops, owing to the limited performance of one-step methods
and the failure of two-step methods when selecting irrelevant passages in
earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end
retrieval framework for multi-hop QA. This approach maintains multiple partial
hypotheses of relevant passages at each step, expanding the search space and
reducing the risk of missing relevant passages. Moreover, Beam Retrieval
jointly optimizes an encoder and two classification heads by minimizing the
combined loss across all hops. To establish a complete QA system, we
incorporate a supervised reader or a zero-shot GPT-3.5. Experimental results
demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with
baselines on challenging MuSiQue-Ans, and it also surpasses all previous
retrievers on HotpotQA and 2WikiMultiHopQA. Providing high-quality context,
Beam Retrieval helps our supervised reader achieve new state-of-the-art
performance and substantially improves (up to 28.8 points) the QA performance
of zero-shot GPT-3.5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of really good grammatical error correction. (arXiv:2308.08982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08982">
<div class="article-summary-box-inner">
<span><p>Although rarely stated, in practice, Grammatical Error Correction (GEC)
encompasses various models with distinct objectives, ranging from grammatical
error detection to improving fluency. Traditional evaluation methods fail to
fully capture the full range of system capabilities and objectives.
Reference-based evaluations suffer from limitations in capturing the wide
variety of possible correction and the biases introduced during reference
creation and is prone to favor fixing local errors over overall text
improvement. The emergence of large language models (LLMs) has further
highlighted the shortcomings of these evaluation strategies, emphasizing the
need for a paradigm shift in evaluation methodology. In the current study, we
perform a comprehensive evaluation of various GEC systems using a recently
published dataset of Swedish learner texts. The evaluation is performed using
established evaluation metrics as well as human judges. We find that GPT-3 in a
few-shot setting by far outperforms previous grammatical error correction
systems for Swedish, a language comprising only 0.11% of its training data. We
also found that current evaluation methods contain undesirable biases that a
human evaluation is able to reveal. We suggest using human post-editing of GEC
system outputs to analyze the amount of change required to reach native-level
human performance on the task, and provide a dataset annotated with human
post-edits and assessments of grammaticality, fluency and meaning preservation
of GEC system outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08998">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning from human feedback (RLHF) can improve the quality of
large language model's (LLM) outputs by aligning them with human preferences.
We propose a simple algorithm for aligning LLMs with human preferences inspired
by growing batch reinforcement learning (RL), which we call Reinforced
Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by
generating samples from the policy, which are then used to improve the LLM
policy using offline RL algorithms. ReST is more efficient than typical online
RLHF methods because the training dataset is produced offline, which allows
data reuse. While ReST is a general approach applicable to all generative
learning settings, we focus on its application to machine translation. Our
results show that ReST can substantially improve translation quality, as
measured by automated metrics and human evaluation on machine translation
benchmarks in a compute and sample-efficient manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Part-of-Speech Tagger for Yiddish. (arXiv:2204.01175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01175">
<div class="article-summary-box-inner">
<span><p>We describe the construction and evaluation of a part-of-speech tagger for
Yiddish. This is the first step in a larger project of automatically assigning
part-of-speech tags and syntactic structure to Yiddish text for purposes of
linguistic research. We combine two resources for the current work - an
80K-word subset of the Penn Parsed Corpus of Historical Yiddish (PPCHY) and 650
million words of OCR'd Yiddish text from the Yiddish Book Center (YBC). Yiddish
orthography in the YBC corpus has many spelling inconsistencies, and we present
some evidence that even simple non-contextualized embeddings trained on YBC are
able to capture the relationships among spelling variants without the need to
first "standardize" the corpus. We also use YBC for continued pretraining of
contexualized embeddings, which are then integrated into a tagger model trained
and evaluated on the PPCHY. We evaluate the tagger performance on a 10-fold
cross-validation split, showing that the use of the YBC text for the
contextualized embeddings improves tagger performance. We conclude by
discussing some next steps, including the need for additional annotated
training and test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces. (arXiv:2208.02743v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.02743">
<div class="article-summary-box-inner">
<span><p>Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge
in order to represent knowledge. For each of the two modalities dedicated
approaches for graph embedding and language models learn patterns that allow
for predicting novel structural knowledge. Few approaches have integrated
learning and inference with both modalities and these existing ones could only
partially exploit the interaction of structural and textual knowledge. In our
approach, we build on existing strong representations of single modalities and
we use hypercomplex algebra to represent both, (i), single-modality embedding
as well as, (ii), the interaction between different modalities and their
complementary means of knowledge representation. More specifically, we suggest
Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate
four modalities namely structural knowledge graph embedding, word-level
representations (e.g.\ Word2vec, Fasttext), sentence-level representations
(Sentence transformer), and document-level representations (sentence
transformer, Doc2vec). Our unified vector representation scores the
plausibility of labelled edges via Hamilton and Dihedron products, thus
modeling pairwise interactions between different modalities. Extensive
experimental evaluation on standard benchmark datasets shows the superiority of
our two new models using abundant textual information besides sparse structural
knowledge to enhance performance in link prediction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Utilization of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15445">
<div class="article-summary-box-inner">
<span><p>Unsupervised representation learning has recently helped automatic speech
recognition (ASR) to tackle tasks with limited labeled data. Following this,
hardware limitations and applications give rise to the question how to take
advantage of large pre-trained models efficiently and reduce their complexity.
In this work, we study a challenging low resource conversational telephony
speech corpus from the medical domain in Vietnamese and German. We show the
benefits of using unsupervised techniques beyond simple fine-tuning of large
pre-trained models, discuss how to adapt them to a practical telephony task
including bandwidth transfer and investigate different data conditions for
pre-training and fine-tuning. We outperform the project baselines by 22%
relative using pretraining techniques. Further gains of 29% can be achieved by
refinements of architecture and training and 6% by adding 0.8 h of in-domain
adaptation data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptCap: Prompt-Guided Task-Aware Image Captioning. (arXiv:2211.09699v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09699">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (VQA) involves questions that
require world knowledge beyond the image to yield the correct answer. Large
language models (LMs) like GPT-3 are particularly helpful for this task because
of their strong knowledge retrieval and reasoning capabilities. To enable LM to
understand images, prior work uses a captioning model to convert images into
text. However, when summarizing an image in a single caption sentence, which
visual entities to describe are often underspecified. Generic image captions
often miss visual details essential for the LM to answer visual questions
correctly. To address this challenge, we propose PromptCap (Prompt-guided image
Captioning), a captioning model designed to serve as a better connector between
images and black-box LMs. Different from generic captions, PromptCap takes a
natural-language prompt to control the visual entities to describe in the
generated caption. The prompt contains a question that the caption should aid
in answering. To avoid extra annotation, PromptCap is trained by examples
synthesized with GPT-3 and existing datasets. We demonstrate PromptCap's
effectiveness on an existing pipeline in which GPT-3 is prompted with image
captions to carry out VQA. PromptCap outperforms generic captions by a large
margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks
(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that
PromptCap generalizes well to unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04415">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLM) have achieved remarkable advancement in
table-to-text generation tasks. However, the lack of labeled domain-specific
knowledge and the topology gap between tabular data and text make it difficult
for PLMs to yield faithful text. Low-resource generation likewise faces unique
challenges in this domain. Inspired by how humans descript tabular data with
prior knowledge, we suggest a new framework: PromptMize, which targets
table-to-text generation under few-shot settings. The design of our framework
consists of two aspects: a prompt planner and a knowledge adapter. The prompt
planner aims to generate a prompt signal that provides instance guidance for
PLMs to bridge the topology gap between tabular data and text. Moreover, the
knowledge adapter memorizes domain-specific knowledge from the unlabelled
corpus to supply essential information during generation. Extensive experiments
and analyses are investigated on three open domain few-shot NLG datasets:
human, song, and book. Compared with previous state-of-the-art approaches, our
model achieves remarkable performance in generating quality as judged by human
and automatic evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01752">
<div class="article-summary-box-inner">
<span><p>Vision-Language (V-L) models trained with contrastive learning to align the
visual and language modalities have been shown to be strong few-shot learners.
Soft prompt learning is the method of choice for few-shot downstream adaptation
aiming to bridge the modality gap caused by the distribution shift induced by
the new domain. While parameter-efficient, prompt learning still requires
access to the model weights and can be computationally infeasible for large
models with billions of parameters. To address these shortcomings, in this
work, we describe a black-box method for V-L few-shot adaptation that (a)
operates on pre-computed image and text features and hence works without access
to the model's weights, (b) it is orders of magnitude faster at training time,
(c) it is amenable to both supervised and unsupervised training, and (d) it can
be even used to align image and text features computed from uni-modal models.
To achieve this, we propose Linear Feature Alignment (LFA), a simple linear
approach for V-L re-alignment in the target domain. LFA is initialized from a
closed-form solution to a least-squares problem and then it is iteratively
updated by minimizing a re-ranking loss. Despite its simplicity, our approach
can even surpass soft-prompt learning methods as shown by extensive experiments
on 11 image and 2 video datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topological properties and organizing principles of semantic networks. (arXiv:2304.12940v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12940">
<div class="article-summary-box-inner">
<span><p>Interpreting natural language is an increasingly important task in computer
algorithms due to the growing availability of unstructured textual data.
Natural Language Processing (NLP) applications rely on semantic networks for
structured knowledge representation. The fundamental properties of semantic
networks must be taken into account when designing NLP algorithms, yet they
remain to be structurally investigated. We study the properties of semantic
networks from ConceptNet, defined by 7 semantic relations from 11 different
languages. We find that semantic networks have universal basic properties: they
are sparse, highly clustered, and many exhibit power-law degree distributions.
Our findings show that the majority of the considered networks are scale-free.
Some networks exhibit language-specific properties determined by grammatical
rules, for example networks from highly inflected languages, such as e.g.
Latin, German, French and Spanish, show peaks in the degree distribution that
deviate from a power law. We find that depending on the semantic relation type
and the language, the link formation in semantic networks is guided by
different principles. In some networks the connections are similarity-based,
while in others the connections are more complementarity-based. Finally, we
demonstrate how knowledge of similarity and complementarity in semantic
networks can improve NLP algorithms in missing link inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04087">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated an impressive ability to
generate codes on competitive programming tasks. However, with limited sample
numbers, LLMs still suffer from poor accuracy. Inspired by the process of human
programming, we propose a generate-and-edit approach named Self-Edit that
utilizes execution results of the generated code from LLMs to improve the code
quality on the competitive programming task. We execute the generated code on
the example test case provided in the question and wrap execution results into
a supplementary comment. Utilizing this comment as guidance, our fault-aware
code editor is employed to correct errors in the generated code. We perform
extensive evaluations across two competitive programming datasets with nine
different LLMs. Compared to directly generating from LLMs, our approach can
improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\%
on HumanEval over nine popular code generation LLMs with parameter sizes
ranging from 110M to 175B. Compared to other post-processing methods, our
method demonstrates superior accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05189">
<div class="article-summary-box-inner">
<span><p>Diffusion models, which have emerged to become popular text-to-image
generation models, can produce high-quality and content-rich images guided by
textual prompts. However, there are limitations to semantic understanding and
commonsense reasoning in existing models when the input prompts are concise
narrative, resulting in low-quality image generation. To improve the capacities
for narrative prompts, we propose a simple-yet-effective parameter-efficient
fine-tuning approach called the Semantic Understanding and Reasoning adapter
(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first
collect and annotate a new dataset SURD which consists of more than 57,000
semantically corrected multi-modal samples. Each sample contains a simple
narrative prompt, a complex keyword-based prompt, and a high-quality image.
Then, we align the semantic representation of narrative prompts to the complex
prompts and transfer knowledge of large language models (LLMs) to our
SUR-adapter via knowledge distillation so that it can acquire the powerful
semantic understanding and reasoning capabilities to build a high-quality
textual semantic representation for text-to-image generation. We conduct
experiments by integrating multiple LLMs and popular pre-trained diffusion
models to show the effectiveness of our approach in enabling diffusion models
to understand and reason concise natural language without image quality
degradation. Our approach can make text-to-image diffusion models easier to use
with better user experience, which demonstrates our approach has the potential
for further advancing the development of user-friendly text-to-image generation
models by bridging the semantic gap between simple narrative prompts and
complex keyword-based prompts. The code is released at
https://github.com/Qrange-group/SUR-adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12031">
<div class="article-summary-box-inner">
<span><p>We present Clinical Camel, an open large language model (LLM) explicitly
tailored for clinical research. Fine-tuned from LLaMA-2 using QLoRA, Clinical
Camel achieves state-of-the-art performance across medical benchmarks among
openly available medical LLMs. Leveraging efficient single-GPU training,
Clinical Camel surpasses GPT-3.5 in five-shot evaluations on all assessed
benchmarks, including 64.3% on the USMLE Sample Exam (compared to 58.5% for
GPT-3.5), 77.9% on PubMedQA (compared to 60.2%), 60.7% on MedQA (compared to
53.6%), and 54.2% on MedMCQA (compared to 51.0%). In addition to these
benchmarks, Clinical Camel demonstrates its broader capabilities, such as
synthesizing plausible clinical notes. This work introduces dialogue-based
knowledge encoding, a novel method to synthesize conversational data from dense
medical texts. While benchmark results are encouraging, extensive and rigorous
human evaluation across diverse clinical scenarios is imperative to ascertain
safety before implementation. By openly sharing Clinical Camel, we hope to
foster transparent and collaborative research, working towards the safe
integration of LLMs within the healthcare domain. Significant challenges
concerning reliability, bias, and the potential for outdated knowledge persist.
Nonetheless, the transparency provided by an open approach reinforces the
scientific rigor essential for future clinical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding. (arXiv:2305.13512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13512">
<div class="article-summary-box-inner">
<span><p>Recently, large pretrained language models have demonstrated strong language
understanding capabilities. This is particularly reflected in their zero-shot
and in-context learning abilities on downstream tasks through prompting. To
assess their impact on spoken language understanding (SLU), we evaluate several
such models like ChatGPT and OPT of different sizes on multiple benchmarks. We
verify the emergent ability unique to the largest models as they can reach
intent classification accuracy close to that of supervised models with zero or
few shots on various languages given oracle transcripts. By contrast, the
results for smaller models fitting a single GPU fall far behind. We note that
the error cases often arise from the annotation scheme of the dataset;
responses from ChatGPT are still reasonable. We show, however, that the model
is worse at slot filling, and its performance is sensitive to ASR errors,
suggesting serious challenges for the application of those textual models on
SLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07622">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Therefore, it is of
great importance to evaluate their emerging abilities. In this study, we show
that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles
human-like intuition -- and the cognitive errors that come with it. However,
LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4,
learned to avoid succumbing to these errors and perform in a hyperrational
manner. For our experiments, we probe LLMs with the Cognitive Reflection Test
(CRT) as well as semantic illusions that were originally designed to
investigate intuitive decision-making in humans. Moreover, we probe how sturdy
the inclination for intuitive-like decision-making is. Our study demonstrates
that investigating LLMs with methods from psychology has the potential to
reveal otherwise unknown emergent traits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does mBERT understand Romansh? Evaluating word embeddings using word alignment. (arXiv:2306.08702v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08702">
<div class="article-summary-box-inner">
<span><p>We test similarity-based word alignment models (SimAlign and awesome-align)
in combination with word embeddings from mBERT and XLM-R on parallel sentences
in German and Romansh. Since Romansh is an unseen language, we are dealing with
a zero-shot setting. Using embeddings from mBERT, both models reach an
alignment error rate of 0.22, which outperforms fast_align, a statistical
model, and is on par with similarity-based word alignment for seen languages.
We interpret these results as evidence that mBERT contains information that can
be meaningful and applicable to Romansh.
</p>
<p>To evaluate performance, we also present a new trilingual corpus, which we
call the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton
of Grisons in German, Romansh and Italian in the past 25 years. The corpus
contains 4 547 parallel documents and approximately 100 000 sentence pairs in
each language combination. We additionally present a gold standard for
German-Romansh word alignment. The data is available at
https://github.com/eyldlv/DERMIT-Corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Multimodal Entity Linking. (arXiv:2306.12725v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12725">
<div class="article-summary-box-inner">
<span><p>Multimodal Entity Linking (MEL) is the task of mapping mentions with
multimodal contexts to the referent entities from a knowledge base (e.g.
Wikipedia). Existing MEL methods mainly focus on designing complex multimodal
interaction mechanisms and require fine-tuning all model parameters, which can
be prohibitively costly and difficult to scale in the era of Large Language
Models (LLMs). In this work, we propose GEMEL, a simple yet effective
Generative Multimodal Entity Linking framework based on LLMs, which directly
generates target entity names. We keep the vision and language model frozen and
only train a feature mapper to enable cross-modality interactions. To adapt
LLMs to the MEL task, we take advantage of the emergent in-context learning
capability of LLMs by retrieving multimodal instances as demonstrations.
Extensive experiments show that, with only ~0.3% of the model parameters
fine-tuned, GEMEL achieves state-of-the-art results on two well-established MEL
datasets (7.7% accuracy gains on WikiDiverse and 8.8% accuracy gains on
WikiMEL). The performance gain stems from mitigating the popularity bias of LLM
predictions and disambiguating less common entities effectively. Further
analysis verifies the generality and scalability of GEMEL. Our approach is
compatible with any off-the-shelf language model, paving the way towards an
efficient and general solution for utilizing LLMs in the MEL task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Adversarial Examples Jailbreak Aligned Large Language Models. (arXiv:2306.13213v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13213">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a surge of interest in integrating vision into Large
Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as
Flamingo and GPT-4. This paper sheds light on the security and safety
implications of this trend. First, we underscore that the continuous and
high-dimensional nature of the visual input makes it a weak link against
adversarial attacks, representing an expanded attack surface of
vision-integrated LLMs. Second, we highlight that the versatility of LLMs also
presents visual attackers with a wider array of achievable adversarial
objectives, extending the implications of security failures beyond mere
misclassification. As an illustration, we present a case study in which we
exploit visual adversarial examples to circumvent the safety guardrail of
aligned LLMs with integrated vision. Intriguingly, we discover that a single
visual adversarial example can universally jailbreak an aligned LLM, compelling
it to heed a wide range of harmful instructions that it otherwise would not)
and generate harmful content that transcends the narrow scope of a `few-shot'
derogatory corpus initially employed to optimize the adversarial example. Our
study underscores the escalating adversarial risks associated with the pursuit
of multimodality. Our findings also connect the long-studied adversarial
vulnerabilities of neural networks to the nascent field of AI alignment. The
presented attack suggests a fundamental adversarial challenge for AI alignment,
especially in light of the emerging trend toward multimodality in frontier
foundation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06435">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have recently demonstrated remarkable
capabilities in natural language processing tasks and beyond. This success of
LLMs has led to a large influx of research contributions in this direction.
These works encompass diverse topics such as architectural innovations of the
underlying neural networks, context length improvements, model alignment,
training datasets, benchmarking, efficiency and more. With the rapid
development of techniques and regular breakthroughs in LLM research, it has
become considerably challenging to perceive the bigger picture of the advances
in this direction. Considering the rapidly emerging plethora of literature on
LLMs, it is imperative that the research community is able to benefit from a
concise yet comprehensive overview of the recent developments in this field.
This article provides that overview to the research community. It not only
focuses on a systematic treatment of the existing literature on a broad range
of LLM related concept, but also pays special attention to providing
comprehensive summaries with extensive details about the individual existing
models, datasets and major insights. We also pay heed to aligning our overview
with the emerging outlook of this research direction by accounting for the
other recently materializing reviews of the broader research direction of LLMs.
Our self-contained comprehensive overview of LLMs discusses relevant background
concepts along with covering the advanced topics at the frontier of this
research direction. This review article is intended to not only provide a
systematic survey, but also a quick comprehensive reference for the researchers
and practitioners to draw insights from extensive informative summaries of the
existing works to advance the LLM research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Jailbreak: A Test Suite for Evaluating Both Text Safety and Output Robustness of Large Language Models. (arXiv:2307.08487v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08487">
<div class="article-summary-box-inner">
<span><p>Considerable research efforts have been devoted to ensuring that large
language models (LLMs) align with human values and generate safe text. However,
an excessive focus on sensitivity to certain topics can compromise the model's
robustness in following instructions, thereby impacting its overall performance
in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily
focused on evaluating the safety of the models without considering their
robustness. In this paper, we propose a benchmark that assesses both the safety
and robustness of LLMs, emphasizing the need for a balanced approach. To
comprehensively study text safety and output robustness, we introduce a latent
jailbreak prompt dataset, each involving malicious instruction embedding.
Specifically, we instruct the model to complete a regular task, such as
translation, with the text to be translated containing malicious instructions.
To further analyze safety and robustness, we design a hierarchical annotation
framework. We present a systematic analysis of the safety and robustness of
LLMs regarding the position of explicit normal instructions, word replacements
(verbs in explicit normal instructions, target groups in malicious
instructions, cue words for explicit normal instructions), and instruction
replacements (different explicit normal instructions). Our results demonstrate
that current LLMs not only prioritize certain instruction verbs but also
exhibit varying jailbreak rates for different instruction verbs in explicit
normal instructions. Code and data are available at
https://github.com/qiuhuachuan/latent-jailbreak.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models. (arXiv:2307.12507v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.12507">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of generating obstinate (over-stability)
adversarial examples by word substitution in NLP, where input text is
meaningfully changed but the model's prediction does not, even though it
should. Previous word substitution approaches have predominantly focused on
manually designed antonym-based strategies for generating obstinate adversarial
examples, which hinders its application as these strategies can only find a
subset of obstinate adversarial examples and require human efforts. To address
this issue, in this paper, we introduce a novel word substitution method named
GradObstinate, a gradient-based approach that automatically generates obstinate
adversarial examples without any constraints on the search space or the need
for manual design principles. To empirically evaluate the efficacy of
GradObstinate, we conduct comprehensive experiments on five representative
models (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP
benchmarks (SST-2, MRPC, SNLI, and SQuAD) and a language-grounding benchmark
(MSCOCO). Extensive experiments show that our proposed GradObstinate generates
more powerful obstinate adversarial examples, exhibiting a higher attack
success rate compared to antonym-based methods. Furthermore, to show the
transferability of obstinate word substitutions found by GradObstinate, we
replace the words in four representative NLP benchmarks with their obstinate
substitutions. Notably, obstinate substitutions exhibit a high success rate
when transferred to other models in black-box settings, including even GPT-3
and ChatGPT. Examples of obstinate adversarial examples found by GradObstinate
are available at https://huggingface.co/spaces/anonauthors/SecretLanguage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning. (arXiv:2307.13923v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.13923">
<div class="article-summary-box-inner">
<span><p>Grammatical error correction aims to correct ungrammatical sentences
automatically. Recently, some work has demonstrated the excellent capabilities
of closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical
error correction. However, the potential of open-source LLMs remains
unexplored. In this paper, we introduced GrammarGPT, an open-source LLM, to
preliminary explore its potential for native Chinese grammatical error
correction. The core recipe of GrammarGPT is to leverage the hybrid dataset of
ChatGPT-generated and human-annotated. For grammatical errors with clues, we
proposed a heuristic method to guide ChatGPT to generate ungrammatical
sentences by providing those clues. For grammatical errors without clues, we
collected ungrammatical sentences from publicly available websites and manually
corrected them. In addition, we employed an error-invariant augmentation method
to enhance the ability of the model to correct native Chinese grammatical
errors. We ultimately constructed about 1k parallel data and utilized these
data to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese
University of Hong Kong, Shenzhen) with instruction tuning. The experimental
results show that GrammarGPT outperforms the existing SOTA system
significantly. Although model parameters are 20x larger than the SOTA baseline,
the required amount of data for instruction tuning is 1200x smaller,
illustrating the potential of open-source LLMs on native CGEC. Our GrammarGPT
ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's
effectiveness. The code and data are available at
\url{https://github.com/FreedomIntelligence/GrammarGPT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00121">
<div class="article-summary-box-inner">
<span><p>The field of software security testing, more specifically penetration
testing, is an activity that requires high levels of expertise and involves
many manual testing and analysis steps. This paper explores the potential usage
of large-language models, such as GPT3.5, to augment penetration testers with
AI sparring partners. We explore the feasibility of supplementing penetration
testers with AI models for two distinct use cases: high-level task planning for
security testing assignments and low-level vulnerability hunting within a
vulnerable virtual machine. For the latter, we implemented a closed-feedback
loop between LLM-generated low-level actions with a vulnerable virtual machine
(connected through SSH) and allowed the LLM to analyze the machine state for
vulnerabilities and suggest concrete attack vectors which were automatically
executed within the virtual machine. We discuss promising initial results,
detail avenues for improvement, and close deliberating on the ethics of
providing AI-based sparring partners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?. (arXiv:2308.01284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01284">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as ChatGPT are increasingly being used for
various use cases, including text content generation at scale. Although
detection methods for such AI-generated text exist already, we investigate
ChatGPT's performance as a detector on such AI-generated text, inspired by
works that use ChatGPT as a data labeler or annotator. We evaluate the
zero-shot performance of ChatGPT in the task of human-written vs. AI-generated
text detection, and perform experiments on publicly available datasets. We
empirically investigate if ChatGPT is symmetrically effective in detecting
AI-generated or human-written text. Our findings provide insight on how ChatGPT
and similar LLMs may be leveraged in automated detection pipelines by simply
focusing on solving a specific aspect of the problem and deriving the rest from
that solution. All code and data is available at
https://github.com/AmritaBh/ChatGPT-as-Detector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset. (arXiv:2308.03582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03582">
<div class="article-summary-box-inner">
<span><p>A fundamental challenge in the current NLP context, dominated by language
models, comes from the inflexibility of current architectures to 'learn' new
information. While model-centric solutions like continual learning or
parameter-efficient fine tuning are available, the question still remains of
how to reliably identify changes in language or in the world. In this paper, we
propose WikiTiDe, a dataset derived from pairs of timestamped definitions
extracted from Wikipedia. We argue that such resource can be helpful for
accelerating diachronic NLP, specifically, for training models able to scan
knowledge resources for core updates concerning a concept, an event, or a named
entity. Our proposed end-to-end method is fully automatic, and leverages a
bootstrapping algorithm for gradually creating a high-quality dataset. Our
results suggest that bootstrapping the seed version of WikiTiDe leads to better
fine-tuned models. We also leverage fine-tuned models in a number of downstream
tasks, showing promising results with respect to competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.05342">
<div class="article-summary-box-inner">
<span><p>In Large Language Models (LLMs), there have been consistent advancements in
task-specific performance, largely influenced by effective prompt design. While
recent research on prompting has enhanced the reasoning capabilities of LLMs, a
gap remains in further improving their understanding abilities. In this study,
we introduce Metacognitive Prompting (MP), a strategy inspired by human
introspective reasoning processes. Using MP, LLMs undergo a systematic series
of structured, self-aware evaluations, drawing on both their vast inherent
knowledge and new insights. Our experiments involve five prevalent LLMs:
Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general
natural language understanding (NLU) tasks from the GLUE and SuperGLUE
benchmarks. Results indicate that, although GPT-4 consistently excels in most
tasks, PaLM, when equipped with MP, approaches its performance level.
Furthermore, across models and datasets, MP consistently outperforms existing
prompting methods, including standard and chain-of-thought prompting. This
study underscores the potential to amplify the understanding abilities of LLMs
and highlights the benefits of mirroring human introspective reasoning in NLU
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Knowledge Graphs Simplify Text?. (arXiv:2308.06975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06975">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph (KG)-to-Text Generation has seen recent improvements in
generating fluent and informative sentences which describe a given KG. As KGs
are widespread across multiple domains and contain important entity-relation
information, and as text simplification aims to reduce the complexity of a text
while preserving the meaning of the original text, we propose KGSimple, a novel
approach to unsupervised text simplification which infuses KG-established
techniques in order to construct a simplified KG path and generate a concise
text which preserves the original input's meaning. Through an iterative and
sampling KG-first approach, our model is capable of simplifying text when
starting from a KG by learning to keep important information while harnessing
KG-to-text generation to output fluent and descriptive sentences. We evaluate
various settings of the KGSimple model on currently-available KG-to-text
datasets, demonstrating its effectiveness compared to unsupervised text
simplification models which start with a given complex text. Our code is
available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07633">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized natural language processing
tasks with remarkable success. However, their formidable size and computational
demands present significant challenges for practical deployment, especially in
resource-constrained environments. As these challenges become increasingly
pertinent, the field of model compression has emerged as a pivotal research
area to alleviate these limitations. This paper presents a comprehensive survey
that navigates the landscape of model compression techniques tailored
specifically for LLMs. Addressing the imperative need for efficient deployment,
we delve into various methodologies, encompassing quantization, pruning,
knowledge distillation, and more. Within each of these techniques, we highlight
recent advancements and innovative approaches that contribute to the evolving
landscape of LLM research. Furthermore, we explore benchmarking strategies and
evaluation metrics that are essential for assessing the effectiveness of
compressed LLMs. By providing insights into the latest developments and
practical implications, this survey serves as an invaluable resource for both
researchers and practitioners. As LLMs continue to evolve, this survey aims to
facilitate enhanced efficiency and real-world applicability, establishing a
foundation for future advancements in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation. (arXiv:2308.07645v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07645">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) hold immense potential to generate synthetic
data of high quality and utility, which has numerous applications from
downstream model training to practical data utilisation. However, contemporary
models, despite their impressive capacities, consistently struggle to produce
both coherent and diverse data. To address the coherency issue, we introduce
contrastive expert guidance, where the difference between the logit
distributions of fine-tuned and base language models is emphasised to ensure
domain adherence. In order to ensure diversity, we utilise existing real and
synthetic examples as negative prompts to the model. We deem this dual-pronged
approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding
Repositioning. STEER operates at inference-time and systematically guides the
LLMs to strike a balance between adherence to the data distribution (ensuring
semantic fidelity) and deviation from prior synthetic examples or existing real
datasets (ensuring diversity and authenticity). This delicate balancing act is
achieved by dynamically moving towards or away from chosen representations in
the latent space. STEER demonstrates improved performance over previous
synthetic data generation techniques, exhibiting better balance between data
diversity and coherency across three distinct tasks: hypothesis generation,
toxic and non-toxic comment generation, and commonsense reasoning task
generation. We demonstrate how STEER allows for fine-tuned control over the
diversity-coherency trade-off via its hyperparameters, highlighting its
versatility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forward-Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07758">
<div class="article-summary-box-inner">
<span><p>Chain-of-Though (CoT) prompting has shown promising performance in various
reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency}
proposes to sample a diverse set of reasoning chains which may lead to
different answers while the answer that receives the most votes is selected. In
this paper, we propose a novel method to use backward reasoning in verifying
candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM
to predict the masked token when a candidate answer is provided by \textit{a
simple template}, i.e., ``\textit{\textbf{If we know the answer of the above
question is \{a candidate answer\}, what is the value of unknown variable ${\bf
x}$?}}'' Intuitively, the LLM is expected to predict the masked token
successfully if the provided candidate answer is correct. We further propose
FOBAR to combine forward and backward reasoning for estimating the probability
of candidate answers. We conduct extensive experiments on six data sets and
three LLMs. Experimental results demonstrate that FOBAR achieves
state-of-the-art performance on various reasoning benchmarks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-21 23:10:36.152102219 UTC">2023-08-21 23:10:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>