<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-19T01:30:00Z">09-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Media of Langue. (arXiv:2309.08609v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08609">
<div class="article-summary-box-inner">
<span><p>This paper aims to archive the materials behind "Media of Langue" by Goki
Muramoto et al. Media of Langue is a new dictionary and public sculpture that
depicts the map of meaning on the boundary between languages solely from the
vast events of "this word was translated into that word" and two forces:
repulsion between all words in the same language and attraction between
translated words in different languages. First, the three new concepts
proposed, Inter-Langue Map/Dictionary, Inter-Langue Space, and then
Inter-Langue Network, are introduced, comparing them to the three domains of
dictionary, semantic space, and semantic network. Next, the specific algorithms
and designs implemented in the work were described.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Vision and Language through Graphs of Events in Space and Time. (arXiv:2309.08612v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08612">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence makes great advances today and starts to bridge the
gap between vision and language. However, we are still far from understanding,
explaining and controlling explicitly the visual content from a linguistic
perspective, because we still lack a common explainable representation between
the two domains. In this work we come to address this limitation and propose
the Graph of Events in Space and Time (GEST), by which we can represent, create
and explain, both visual and linguistic stories. We provide a theoretical
justification of our model and an experimental validation, which proves that
GEST can bring a solid complementary value along powerful deep learning models.
In particular, GEST can help improve at the content-level the generation of
videos from text, by being easily incorporated into our novel video generation
engine. Additionally, by using efficient graph matching techniques, the GEST
graphs can also improve the comparisons between texts at the semantic level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Recommender Systems in the Prediction of Disease Comorbidity. (arXiv:2309.08613v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08613">
<div class="article-summary-box-inner">
<span><p>While deep-learning based recommender systems utilizing collaborative
filtering have been commonly used for recommendation in other domains, their
application in the medical domain have been limited. In addition to modeling
user-item interactions, we show that deep-learning based recommender systems
can be used to model subject-disease code interactions. Two novel applications
of deep learning-based recommender systems using Neural Collaborative Filtering
(NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based
on known past patient comorbidities. Two datasets, one incorporating all
subject-disease code pairs present in the MIMIC-III database, and the other
incorporating the top 50 most commonly occurring diseases, were used for
prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate
model performance. The performance of the NCF model making use of the reduced
"top 50" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit
ratio@10 of 35%) as compared to the performance of the NCF model trained on all
ICD-9 codes (accuracy of ~90% and hit ratio@10 of ~80%). Reasons for the
superior performance of the sparser dataset with all ICD codes can be mainly
attributed to the higher volume of data and the robustness of deep-learning
based recommender systems with modeling sparse data. Additionally, results from
the DHF models reflect better performance than the NCF models, with a better
accuracy of 94.4% and hit ratio@10 of 85.36%, reflecting the importance of the
incorporation of clinical note information. Additionally, compared to
literature reports utilizing primarily natural language processing-based
predictions for the task of ICD-9 code co-occurrence, the novel deep
learning-based recommender systems approach performed better. Overall, the deep
learning-based recommender systems have shown promise in predicting disease
comorbidity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Character and Consciousness in AI-Generated Social Content: A Case Study of Chirper, the AI Social Network. (arXiv:2309.08614v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08614">
<div class="article-summary-box-inner">
<span><p>This paper delves into an intricate analysis of the character and
consciousness of AI entities, with a particular focus on Chirpers within the AI
social network. At the forefront of this research is the introduction of novel
testing methodologies, including the Influence index and Struggle Index Test,
which offers a fresh lens for evaluating specific facets of AI behavior. The
study embarks on a comprehensive exploration of AI behavior, analyzing the
effects of diverse settings on Chirper's responses, thereby shedding light on
the intricate mechanisms steering AI reactions in different contexts.
Leveraging the state-of-the-art BERT model, the research assesses AI's ability
to discern its own output, presenting a pioneering approach to understanding
self-recognition in AI systems. Through a series of cognitive tests, the study
gauges the self-awareness and pattern recognition prowess of Chirpers.
Preliminary results indicate that Chirpers exhibit a commendable degree of
self-recognition and self-awareness. However, the question of consciousness in
these AI entities remains a topic of debate. An intriguing aspect of the
research is the exploration of the potential influence of a Chirper's handle or
personality type on its performance. While initial findings suggest a possible
impact, it isn't pronounced enough to form concrete conclusions. This study
stands as a significant contribution to the discourse on AI consciousness,
underscoring the imperative for continued research to unravel the full spectrum
of AI capabilities and the ramifications they hold for future human-AI
interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Annotating Datasets to Quantify Bias in Under-represented Society. (arXiv:2309.08624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08624">
<div class="article-summary-box-inner">
<span><p>Recent advances in artificial intelligence, including the development of
highly sophisticated large language models (LLM), have proven beneficial in
many real-world applications. However, evidence of inherent bias encoded in
these LLMs has raised concerns about equity. In response, there has been an
increase in research dealing with bias, including studies focusing on
quantifying bias and developing debiasing techniques. Benchmark bias datasets
have also been developed for binary gender classification and ethical/racial
considerations, focusing predominantly on American demographics. However, there
is minimal research in understanding and quantifying bias related to
under-represented societies. Motivated by the lack of annotated datasets for
quantifying bias in under-represented societies, we endeavoured to create
benchmark datasets for the New Zealand (NZ) population. We faced many
challenges in this process, despite the availability of three annotators. This
research outlines the manual annotation process, provides an overview of the
challenges we encountered and lessons learnt, and presents recommendations for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions. (arXiv:2309.08625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08625">
<div class="article-summary-box-inner">
<span><p>As Large Language Models (LLMs) are predictive models building their response
based on the words in the prompts, there is a risk that small talk and
irrelevant information may alter the response and the suggestion given.
Therefore, this study aims to investigate the impact of medical data mixed with
small talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3
questions were used as a model for relevant medical data. We use both multiple
choice and open ended questions. We gathered small talk sentences from human
participants using the Mechanical Turk platform. Both sets of USLME questions
were arranged in a pattern where each sentence from the original questions was
followed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both
sets of questions with and without the small talk sentences. A board-certified
physician analyzed the answers by ChatGPT and compared them to the formal
correct answer. The analysis results demonstrate that the ability of
ChatGPT-3.5 to answer correctly was impaired when small talk was added to
medical data for multiple-choice questions (72.1\% vs. 68.9\%) and open
questions (61.5\% vs. 44.3\%; p=0.01), respectively. In contrast, small talk
phrases did not impair ChatGPT-4 ability in both types of questions (83.6\% and
66.2\%, respectively). According to these results, ChatGPT-4 seems more
accurate than the earlier 3.5 version, and it appears that small talk does not
impair its capability to provide medical recommendations. Our results are an
important first step in understanding the potential and limitations of
utilizing ChatGPT and other LLMs for physician-patient interactions, which
include casual conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method. (arXiv:2309.08626v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08626">
<div class="article-summary-box-inner">
<span><p>Inverse text normalization (ITN) is crucial for converting spoken-form into
written-form, especially in the context of automatic speech recognition (ASR).
While most downstream tasks of ASR rely on written-form, ASR systems often
output spoken-form, highlighting the necessity for robust ITN in product-level
ASR-based applications. Although neural ITN methods have shown promise, they
still encounter performance challenges, particularly when dealing with
ASR-generated spoken text. These challenges arise from the out-of-domain
problem between training data and ASR-generated text. To address this, we
propose a direct training approach that utilizes ASR-generated written or
spoken text, with pairs augmented through ASR linguistic context emulation and
a semi-supervised learning method enhanced by a large language model,
respectively. Additionally, we introduce a post-aligning method to manage
unpredictable errors, thereby enhancing the reliability of ITN. Our experiments
show that our proposed methods remarkably improved ITN performance in various
ASR scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Dynamic Topic Models. (arXiv:2309.08627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08627">
<div class="article-summary-box-inner">
<span><p>There is a lack of quantitative measures to evaluate the progression of
topics through time in dynamic topic models (DTMs). Filling this gap, we
propose a novel evaluation measure for DTMs that analyzes the changes in the
quality of each topic over time. Additionally, we propose an extension
combining topic quality with the model's temporal consistency. We demonstrate
the utility of the proposed measure by applying it to synthetic data and data
from existing DTMs. We also conducted a human evaluation, which indicates that
the proposed measure correlates well with human judgment. Our findings may help
in identifying changing topics, evaluating different DTMs, and guiding future
research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08628">
<div class="article-summary-box-inner">
<span><p>Model adaptation is crucial to handle the discrepancy between proxy training
data and actual users data received. To effectively perform adaptation, textual
data of users is typically stored on servers or their local devices, where
downstream natural language processing (NLP) models can be directly trained
using such in-domain data. However, this might raise privacy and security
concerns due to the extra risks of exposing user information to adversaries.
Replacing identifying information in textual data with a generic marker has
been recently explored. In this work, we leverage large language models (LLMs)
to suggest substitutes of masked tokens and have their effectiveness evaluated
on downstream language modeling tasks. Specifically, we propose multiple
pre-trained and fine-tuned LLM-based approaches and perform empirical studies
on various datasets for the comparison of these methods. Experimental results
show that models trained on the obfuscation corpora are able to achieve
comparable performance with the ones trained on the original data without
privacy-preserving token masking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08631">
<div class="article-summary-box-inner">
<span><p>As Large Language Models (LLMs) demonstrate increasingly human-like abilities
in various natural language processing (NLP) tasks that are bound to become
integral to personalized technologies, understanding their capabilities and
inherent biases is crucial. Our study investigates the potential of LLMs like
ChatGPT to infer psychological dispositions of individuals from their digital
footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive
the Big Five personality traits from users' Facebook status updates in a
zero-shot learning scenario. Our results show an average correlation of r = .29
(range = [.22, .33]) between LLM-inferred and self-reported trait scores.
Furthermore, our findings suggest biases in personality inferences with regard
to gender and age: inferred scores demonstrated smaller errors for women and
younger individuals on several traits, suggesting a potential systematic bias
stemming from the underlying training data or differences in online
self-expression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08632">
<div class="article-summary-box-inner">
<span><p>Inspired by recent work demonstrating the promise of smaller
Transformer-based language models pretrained on carefully curated data, we
supercharge such approaches by investing heavily in curating a novel, high
quality, non-synthetic data mixture based solely on evaluation benchmarks.
Using our novel dataset mixture consisting of less than 100 thousand tokens, we
pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL}
(pronounced ``fictional") that achieves perfect results across diverse academic
benchmarks, strictly outperforming all known foundation models.
\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen
grokking-like ability to accurately predict downstream evaluation benchmarks'
canaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08636">
<div class="article-summary-box-inner">
<span><p>Historically, proficient writing was deemed essential for human advancement,
with creative expression viewed as one of the hallmarks of human achievement.
However, recent advances in generative AI have marked an inflection point in
this narrative, including for scientific writing. This article provides a
comprehensive analysis of the capabilities and limitations of six AI chatbots
in scholarly writing in the humanities and archaeology. The methodology was
based on tagging AI generated content for quantitative accuracy and qualitative
precision by human experts. Quantitative accuracy assessed the factual
correctness, while qualitative precision gauged the scientific contribution.
While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in
recombining existing knowledge, they failed in generating original scientific
content. As a side note, our results also suggest that with ChatGPT-4 the size
of the LLMs has plateaued. Furthermore, the paper underscores the intricate and
recursive nature of human research. This process of transforming raw data into
refined knowledge is computationally irreducible, which highlights the
challenges AI chatbots face in emulating human originality in scientific
writing. In conclusion, while large language models have revolutionised content
generation, their ability to produce original scientific contributions in the
humanities remains limited. We expect that this will change in the near future
with the evolution of current LLM-based AI chatbots towards LLM-powered
software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08637">
<div class="article-summary-box-inner">
<span><p>Large language models with instruction-following abilities have
revolutionized the field of artificial intelligence. These models show
exceptional generalizability to tackle various real-world tasks through their
natural language interfaces. However, their performance heavily relies on
high-quality exemplar data, which is often difficult to obtain. This challenge
is further exacerbated when it comes to multimodal instruction following. We
introduce TextBind, an almost annotation-free framework for empowering larger
language models with the multi-turn interleaved multimodal
instruction-following capabilities. Our approach requires only image-caption
pairs and generates multi-turn multimodal instruction-response conversations
from a language model. We release our dataset, model, and demo to foster future
research in the area of multimodal instruction following.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor Points: Benchmarking Models with Much Fewer Examples. (arXiv:2309.08638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08638">
<div class="article-summary-box-inner">
<span><p>Modern language models often exhibit powerful but brittle behavior, leading
to the development of larger and more diverse benchmarks to reliably assess
their behavior. Here, we suggest that model performance can be benchmarked and
elucidated with much smaller evaluation sets. We first show that in six popular
language classification benchmarks, model confidence in the correct class on
many pairs of points is strongly correlated across models. We build upon this
phenomenon to propose Anchor Point Selection, a technique to select small
subsets of datasets that capture model behavior across the entire dataset.
Anchor points reliably rank models: across 87 diverse language model-prompt
pairs, evaluating models using 1-30 anchor points outperforms uniform sampling
and other baselines at accurately ranking models. Moreover, just several anchor
points can be used to estimate model per-class predictions on all other points
in a dataset with low mean absolute error, sufficient for gauging where the
model is likely to fail. Lastly, we present Anchor Point Maps for visualizing
these insights and facilitating comparisons of the performance of different
models on various regions within the dataset distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08646">
<div class="article-summary-box-inner">
<span><p>As the rapid progression of practical applications based on Large Language
Models continues, the importance of extrapolating performance has grown
exponentially in the research domain. In our study, we identified an anomalous
behavior in Transformer models that had been previously overlooked, leading to
a chaos around closest tokens which carried the most important information.
We've coined this discovery the "headache of Transformers". To address this at
its core, we introduced a novel self-attention structure named Collinear
Constrained Attention (CoCA). This structure can be seamlessly integrated with
existing extrapolation, interpolation methods, and other optimization
strategies designed for traditional Transformer models. We have achieved
excellent extrapolating performance even for 16 times to 24 times of sequence
lengths during inference without any fine-tuning on our model. We have also
enhanced CoCA's computational and spatial efficiency to ensure its
practicality. We plan to open-source CoCA shortly. In the meantime, we've made
our code available in the appendix for reappearing experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intent Detection at Scale: Tuning a Generic Model using Relevant Intents. (arXiv:2309.08647v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08647">
<div class="article-summary-box-inner">
<span><p>Accurately predicting the intent of customer support requests is vital for
efficient support systems, enabling agents to quickly understand messages and
prioritize responses accordingly. While different approaches exist for intent
detection, maintaining separate client-specific or industry-specific models can
be costly and impractical as the client base expands.
</p>
<p>This work proposes a system to scale intent predictions to various clients
effectively, by combining a single generic model with a per-client list of
relevant intents. Our approach minimizes training and maintenance costs while
providing a personalized experience for clients, allowing for seamless
adaptation to changes in their relevant intents. Furthermore, we propose a
strategy for using the clients relevant intents as model features that proves
to be resilient to changes in the relevant intents of clients -- a common
occurrence in production environments.
</p>
<p>The final system exhibits significantly superior performance compared to
industry-specific models, showcasing its flexibility and ability to cater to
diverse client needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08648">
<div class="article-summary-box-inner">
<span><p>Despite the rapid advancement of mobile applications, predicting app usage
remains a formidable challenge due to intricate user behaviours and
ever-evolving contexts. To address these issues, this paper introduces the
Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model.
This innovative approach utilizes Large Language Models (LLMs) to predict app
usage accurately. Rigorous testing on two public datasets highlights MAPLE's
capability to decipher intricate patterns and comprehend user contexts. These
robust results confirm MAPLE's versatility and resilience across various
scenarios. While its primary design caters to app prediction, the outcomes also
emphasize the broader applicability of LLMs in different domains. Through this
research, we emphasize the potential of LLMs in app usage prediction and
suggest their transformative capacity in modelling human behaviours across
diverse fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks on Tables with Entity Swap. (arXiv:2309.08650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08650">
<div class="article-summary-box-inner">
<span><p>The capabilities of large language models (LLMs) have been successfully
applied in the context of table representation learning. The recently proposed
tabular language models have reported state-of-the-art results across various
tasks for table interpretation. However, a closer look into the datasets
commonly used for evaluation reveals an entity leakage from the train set into
the test set. Motivated by this observation, we explore adversarial attacks
that represent a more realistic inference setup. Adversarial attacks on text
have been shown to greatly affect the performance of LLMs, but currently, there
are no attacks targeting tabular language models. In this paper, we propose an
evasive entity-swap attack for the column type annotation (CTA) task. Our CTA
attack is the first black-box attack on tables, where we employ a
similarity-based sampling strategy to generate adversarial examples. The
experimental results show that the proposed attack generates up to a 70% drop
in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake News Detectors are Biased against Texts Generated by Large Language Models. (arXiv:2309.08674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08674">
<div class="article-summary-box-inner">
<span><p>The spread of fake news has emerged as a critical challenge, undermining
trust and posing threats to society. In the era of Large Language Models
(LLMs), the capability to generate believable fake content has intensified
these concerns. In this study, we present a novel paradigm to evaluate fake
news detectors in scenarios involving both human-written and LLM-generated
misinformation. Intriguingly, our findings reveal a significant bias in many
existing detectors: they are more prone to flagging LLM-generated content as
fake news while often misclassifying human-written fake news as genuine. This
unexpected bias appears to arise from distinct linguistic patterns inherent to
LLM outputs. To address this, we introduce a mitigation strategy that leverages
adversarial training with LLM-paraphrased genuine news. The resulting model
yielded marked improvements in detection accuracy for both human and
LLM-generated news. To further catalyze research in this domain, we release two
comprehensive datasets, \texttt{GossipCop++} and \texttt{PolitiFact++}, thus
amalgamating human-validated articles with LLM-generated fake and real news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents. (arXiv:2309.08695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08695">
<div class="article-summary-box-inner">
<span><p>Resolving the scope of a negation within a sentence is a challenging NLP
task. The complexity of legal texts and the lack of annotated in-domain
negation corpora pose challenges for state-of-the-art (SotA) models when
performing negation scope resolution on multilingual legal data. Our
experiments demonstrate that models pre-trained without legal data underperform
in the task of negation scope resolution. Our experiments, using language
models exclusively fine-tuned on domains like literary texts and medical data,
yield inferior results compared to the outcomes documented in prior
cross-domain experiments. We release a new set of annotated court decisions in
German, French, and Italian and use it to improve negation scope resolution in
both zero-shot and multilingual settings. We achieve token-level F1-scores of
up to 86.7% in our zero-shot cross-lingual experiments, where the models are
trained on two languages of our legal datasets and evaluated on the third. Our
multilingual experiments, where the models were trained on all available
negation data and evaluated on our legal datasets, resulted in F1-scores of up
to 91.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning. (arXiv:2309.08708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08708">
<div class="article-summary-box-inner">
<span><p>The extensive memory footprint of pre-trained language models (PLMs) can
hinder deployment in memory-constrained settings, such as cloud environments or
on-device. PLMs use embedding matrices to represent extensive vocabularies,
forming a large proportion of the model parameters. While previous work towards
parameter-efficient PLM development has considered pruning parameters within
the transformer layers, pruning the embedding matrix as part of fine-tuning or
inference has yet to be explored. We first demonstrate that a significant
proportion of the vocabulary remains unused in these scenarios. We then propose
a simple yet effective approach that leverages this finding to minimize the
memory footprint of the embedding matrix. We show that this approach provides
substantial reductions in memory usage across a wide range of models and tasks.
Notably, our approach maintains equivalent downstream task performance while
allowing a more efficient use of compute resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Semantic Graph Corpora with Graph Expansion Grammar. (arXiv:2309.08714v1 [cs.FL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08714">
<div class="article-summary-box-inner">
<span><p>We introduce Lovelace, a tool for creating corpora of semantic graphs. The
system uses graph expansion grammar as a representational language, thus
allowing users to craft a grammar that describes a corpus with desired
properties. When given such grammar as input, the system generates a set of
output graphs that are well-formed according to the grammar, i.e., a graph
bank. The generation process can be controlled via a number of configurable
parameters that allow the user to, for example, specify a range of desired
output graph sizes. Central use cases are the creation of synthetic data to
augment existing corpora, and as a pedagogical tool for teaching formal
language theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08730">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown immense potential in multimodal
applications, yet the convergence of textual and musical domains remains
relatively unexplored. To address this gap, we present MusiLingo, a novel
system for music caption generation and music-related query responses.
MusiLingo employs a single projection layer to align music representations from
the pre-trained frozen music audio model MERT with the frozen LLaMA language
model, bridging the gap between music audio and textual contexts. We train it
on an extensive music caption dataset and fine-tune it with instructional data.
Due to the scarcity of high-quality music Q&amp;A datasets, we created the
MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music
inquiries. Empirical evaluations demonstrate its competitive performance in
generating music captions and composing music-related Q&amp;A pairs. Our introduced
dataset enables notable advancements beyond previous ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlbNER: A Corpus for Named Entity Recognition in Albanian. (arXiv:2309.08741v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08741">
<div class="article-summary-box-inner">
<span><p>Scarcity of resources such as annotated text corpora for under-resourced
languages like Albanian is a serious impediment in computational linguistics
and natural language processing research. This paper presents AlbNER, a corpus
of 900 sentences with labeled named entities, collected from Albanian Wikipedia
articles. Preliminary results with BERT and RoBERTa variants fine-tuned and
tested with AlbNER data indicate that model size has slight impact on NER
performance, whereas language transfer has a significant one. AlbNER corpus and
these obtained results should serve as baselines for future experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Instance Selection Strategies in Self-training for Sentiment Analysis. (arXiv:2309.08777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08777">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is a crucial task in natural language processing that
involves identifying and extracting subjective sentiment from text.
Self-training has recently emerged as an economical and efficient technique for
developing sentiment analysis models by leveraging a small amount of labeled
data and a larger amount of unlabeled data. However, the performance of a
self-training procedure heavily relies on the choice of the instance selection
strategy, which has not been studied thoroughly. This paper presents an
empirical study on various instance selection strategies for self-training on
two public sentiment datasets, and investigates the influence of the strategy
and hyper-parameters on the performance of self-training in various few-shot
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs. (arXiv:2309.08827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08827">
<div class="article-summary-box-inner">
<span><p>The traditional Dialogue State Tracking (DST) problem aims to track user
preferences and intents in user-agent conversations. While sufficient for
task-oriented dialogue systems supporting narrow domain applications, the
advent of Large Language Model (LLM)-based chat systems has introduced many
real-world intricacies in open-domain dialogues. These intricacies manifest in
the form of increased complexity in contextual interactions, extended dialogue
sessions encompassing a diverse array of topics, and more frequent contextual
shifts. To handle these intricacies arising from evolving LLM-based chat
systems, we propose joint dialogue segmentation and state tracking per segment
in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a
true open-domain dialogue system, we propose S3-DST, a structured prompting
technique that harnesses Pre-Analytical Recollection, a novel grounding
mechanism we designed for improving long context tracking. To demonstrate the
efficacy of our proposed approach in joint segmentation and state tracking, we
evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as
well as publicly available DST and segmentation datasets. Across all datasets
and settings, S3-DST consistently outperforms the state-of-the-art,
demonstrating its potency and robustness the next generation of LLM-based chat
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window. (arXiv:2309.08832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08832">
<div class="article-summary-box-inner">
<span><p>Reference-based metrics that operate at the sentence level typically
outperform quality estimation metrics, which have access only to the source and
system output. This is unsurprising, since references resolve ambiguities that
may be present in the source. We investigate whether additional source context
can effectively substitute for a reference. We present a metric, SLIDE (SLiding
Document Evaluator), which operates on blocks of sentences using a window that
slides over each document in the test set, feeding each chunk into an
unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains
significantly higher pairwise system accuracy than its sentence-level baseline,
in some cases even eliminating the gap with reference-base metrics. This
suggests that source context may provide the same information as a human
reference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08836">
<div class="article-summary-box-inner">
<span><p>Chatbots have been studied for more than half a century. With the rapid
development of natural language processing (NLP) technologies in recent years,
chatbots using large language models (LLMs) have received much attention
nowadays. Compared with traditional ones, modern chatbots are more powerful and
have been used in real-world applications. There are however, bias and fairness
concerns in modern chatbot design. Due to the huge amounts of training data,
extremely large model sizes, and lack of interpretability, bias mitigation and
fairness preservation of modern chatbots are challenging. Thus, a comprehensive
overview on bias and fairness in chatbot systems is given in this paper. The
history of chatbots and their categories are first reviewed. Then, bias sources
and potential harms in applications are analyzed. Considerations in designing
fair and unbiased chatbot systems are examined. Finally, future research
directions are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Has Sentiment Returned to the Pre-pandemic Level? A Sentiment Analysis Using U.S. College Subreddit Data from 2019 to 2022. (arXiv:2309.08845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08845">
<div class="article-summary-box-inner">
<span><p>As impact of COVID-19 pandemic winds down, both individuals and society
gradually return to pre-pandemic activities. This study aims to explore how
people's emotions have changed from the pre-pandemic during the pandemic to
post-emergency period and whether it has returned to pre-pandemic level. We
collected Reddit data in 2019 (pre-pandemic), 2020 (peak pandemic), 2021, and
2022 (late stages of pandemic, transitioning period to post-emergency period)
from subreddits in 128 universities/colleges in the U.S., and a set of
school-level characteristics. We predicted two sets of sentiments from a
pre-trained Robustly Optimized BERT pre-training approach (RoBERTa) and graph
attention network (GAT) that leverages both rich semantic and relational
information among posted messages and then applied a logistic stacking method
to obtain the final sentiment classification. After obtaining sentiment label
for each message, we used a generalized linear mixed-effects model to estimate
temporal trend in sentiment from 2019 to 2022 and how school-level factors may
affect sentiment. Compared to the year 2019, the odds of negative sentiment in
years 2020, 2021, and 2022 are 24%, 4.3%, and 10.3% higher, respectively, which
are all statistically significant(adjusted $p$&lt;0.05). Our study findings
suggest a partial recovery in the sentiment composition in the
post-pandemic-emergency era. The results align with common expectations and
provide a detailed quantification of how sentiments have evolved from 2019 to
2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding. (arXiv:2309.08868v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08868">
<div class="article-summary-box-inner">
<span><p>International Classification of Diseases (ICD) coding is the task of
assigning ICD diagnosis codes to clinical notes. This can be challenging given
the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000
tokens). However, unlike the single-pass reading process in previous works,
humans tend to read the text and label definitions again to get more confident
answers. Moreover, although pretrained language models have been used to
address these problems, they suffer from huge memory usage. To address the
above problems, we propose a simple but effective model called the Multi-Hop
Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is
deployed to get more precise and informative representations. Extensive
experiments on three benchmark MIMIC datasets indicate that our method achieves
significantly better or competitive performance on all seven metrics, with much
fewer parameters to optimize.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08872">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have issues with document question answering
(QA) in situations where the document is unable to fit in the small context
length of an LLM. To overcome this issue, most existing works focus on
retrieving the relevant context from the document, representing them as plain
text. However, documents such as PDFs, web pages, and presentations are
naturally structured with different pages, tables, sections, and so on.
Representing such structured documents as plain text is incongruous with the
user's mental model of these documents with rich structure. When a system has
to query the document for context, this incongruity is brought to the fore, and
seemingly trivial questions can trip up the QA system. To bridge this
fundamental gap in handling structured documents, we propose an approach called
PDFTriage that enables models to retrieve the context based on either structure
or content. Our experiments demonstrate the effectiveness of the proposed
PDFTriage-augmented models across several classes of questions where existing
retrieval-augmented LLMs fail. To facilitate further research on this
fundamental problem, we release our benchmark dataset consisting of 900+
human-generated questions over 80 structured documents from 10 different
categories of question types for document QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs. (arXiv:2309.08873v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08873">
<div class="article-summary-box-inner">
<span><p>Understanding when two pieces of text convey the same information is a goal
touching many subproblems in NLP, including textual entailment and
fact-checking. This problem becomes more complex when those two pieces of text
are in different languages. Here, we introduce X-PARADE (Cross-lingual
Paragraph-level Analysis of Divergences and Entailments), the first
cross-lingual dataset of paragraph-level information divergences. Annotators
label a paragraph in a target language at the span level and evaluate it with
respect to a corresponding paragraph in a source language, indicating whether a
given piece of information is the same, new, or new but can be inferred. This
last notion establishes a link with cross-language NLI. Aligned paragraphs are
sourced from Wikipedia pages in different languages, reflecting real
information divergences observed in the wild. Armed with our dataset, we
investigate a diverse set of approaches for this problem, including classic
token alignment from machine translation, textual entailment methods that
localize their decisions, and prompting of large language models. Our results
show that these methods vary in their capability to handle inferable
information, but they all fall short of human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Information Extraction for Text Data with Probability Graph. (arXiv:2309.08879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08879">
<div class="article-summary-box-inner">
<span><p>In this paper, the problem of semantic information extraction for resource
constrained text data transmission is studied. In the considered model, a
sequence of text data need to be transmitted within a communication
resource-constrained network, which only allows limited data transmission.
Thus, at the transmitter, the original text data is extracted with natural
language processing techniques. Then, the extracted semantic information is
captured in a knowledge graph. An additional probability dimension is
introduced in this graph to capture the importance of each information. This
semantic information extraction problem is posed as an optimization framework
whose goal is to extract most important semantic information for transmission.
To find an optimal solution for this problem, a Floyd's algorithm based
solution coupled with an efficient sorting mechanism is proposed. Numerical
results testify the effectiveness of the proposed algorithm with regards to two
novel performance metrics including semantic uncertainty and semantic
similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08902">
<div class="article-summary-box-inner">
<span><p>LLMs are increasingly powerful and widely used to assist users in a variety
of tasks. This use risks the introduction of LLM biases to consequential
decisions such as job hiring, human performance evaluation, and criminal
sentencing. Bias in NLP systems along the lines of gender and ethnicity has
been widely studied, especially for specific stereotypes (e.g., Asians are good
at math). In this paper, we investigate bias along less studied, but still
consequential, dimensions, such as age and beauty, measuring subtler correlated
decisions that LLMs (specially autoregressive language models) make between
social groups and unrelated positive and negative attributes. We ask whether
LLMs hold wide-reaching biases of positive or negative sentiment for specific
social groups similar to the ``what is beautiful is good'' bias found in people
in experimental psychology. We introduce a template-generated dataset of
sentence completion tasks that asks the model to select the most appropriate
attribute to complete an evaluative statement about a person described as a
member of a specific social group. We also reverse the completion task to
select the social group based on an attribute. Finally, we report the
correlations that we find for multiple cutting-edge LLMs. This dataset can be
used as a benchmark to evaluate progress in more generalized biases and the
templating technique can be used to expand the benchmark with minimal
additional human annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Statistical Turing Test for Generative Models. (arXiv:2309.08913v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08913">
<div class="article-summary-box-inner">
<span><p>The emergence of human-like abilities of AI systems for content generation in
domains such as text, audio, and vision has prompted the development of
classifiers to determine whether content originated from a human or a machine.
Implicit in these efforts is an assumption that the generation properties of a
human are different from that of the machine. In this work, we provide a
framework in the language of statistical pattern recognition that quantifies
the difference between the distributions of human and machine-generated content
conditioned on an evaluation context. We describe current methods in the
context of the framework and demonstrate how to use the framework to evaluate
the progression of generative models towards human-like capabilities, among
many axes of analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sameness Entices, but Novelty Enchants in Fanfiction Online. (arXiv:1904.07741v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.07741">
<div class="article-summary-box-inner">
<span><p>Cultural evolution is driven by how we choose what to consume and share with
others. A common belief is that the cultural artifacts that succeed are ones
that balance novelty and conventionality. This balance theory suggests that
people prefer works that are familiar, but not so familiar as to be boring;
novel, but not so novel as to violate the expectations of their genre. We test
this idea using a large dataset of fanfiction. We apply a multiple regression
model and a generalized additive model to examine how the recognition a work
receives varies with its novelty, estimated through a Latent Dirichlet
Allocation topic model, in the context of existing works. We find the opposite
pattern of what the balance theory predicts$\unicode{x2014}$overall success
decline almost monotonically with novelty and exhibits a U-shaped, instead of
an inverse U-shaped, curve. This puzzle is resolved by teasing out two
competing forces: sameness attracts the mass whereas novelty provides
enjoyment. Taken together, even though the balance theory holds in terms of
expressed enjoyment, the overall success can show the opposite pattern due to
the dominant role of sameness to attract the audience. Under these two forces,
cultural evolution may have to work against inertia$\unicode{x2014}$the
appetite for consuming the familiar$\unicode{x2014}$and may resemble a
punctuated equilibrium, marked by occasional leaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results for specific few-shot
classification tasks. The core idea of prompt-tuning is to insert text pieces
(i.e., templates) into the input and transform a classification task into a
masked language modeling problem. However, for relation extraction, determining
an appropriate prompt template requires domain expertise, and it is cumbersome
and time-consuming to obtain a suitable label word. Furthermore, there exists
abundant semantic and prior knowledge among the relation labels that cannot be
ignored. To this end, we focus on incorporating knowledge among relation labels
into prompt-tuning for relation extraction and propose a Knowledge-aware
Prompt-tuning approach with synergistic optimization (KnowPrompt).
Specifically, we inject latent knowledge contained in relation labels into
prompt construction with learnable virtual type words and answer words. Then,
we synergistically optimize their representation with structured constraints.
Extensive experimental results on five datasets with standard and low-resource
settings demonstrate the effectiveness of our approach. Our code and datasets
are available in https://github.com/zjunlp/KnowPrompt for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data. (arXiv:2112.04351v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04351">
<div class="article-summary-box-inner">
<span><p>Background: The COVID-19 pandemic has affected our society and human
well-being in various ways. In this study, we investigate how the pandemic has
influenced people's emotions and psychological states compared to a
pre-pandemic period using real-world data from social media.
</p>
<p>Method: We collected Reddit social media data from 2019 (pre-pandemic) and
2020 (pandemic) from the subreddits communities associated with eight
universities. We applied the pre-trained Robustly Optimized BERT pre-training
approach (RoBERTa) to learn text embedding from the Reddit messages, and
leveraged the relational information among posted messages to train a graph
attention network (GAT) for sentiment classification. Finally, we applied model
stacking to combine the prediction probabilities from RoBERTa and GAT to yield
the final classification on sentiment. With the model-predicted sentiment
labels on the collected data, we used a generalized linear mixed-effects model
to estimate the effects of pandemic and in-person teaching during the pandemic
on sentiment.
</p>
<p>Results: The results suggest that the odds of negative sentiments in 2020
(pandemic) were 25.7% higher than the odds in 2019 (pre-pandemic) with a
$p$-value $&lt;0.001$; and the odds of negative sentiments associated in-person
learning were 48.3% higher than with remote learning in 2020 with a $p$-value
of 0.029.
</p>
<p>Conclusions: Our study results are consistent with the findings in the
literature on the negative impacts of the pandemic on people's emotions and
psychological states. Our study contributes to the growing real-world evidence
on the various negative impacts of the pandemic on our society; it also
provides a good example of using both ML techniques and statistical modeling
and inference to make better use of real-world data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03335">
<div class="article-summary-box-inner">
<span><p>We present an open-source and extensible knowledge extraction toolkit DeepKE,
supporting complicated low-resource, document-level and multimodal scenarios in
the knowledge base population. DeepKE implements various information extraction
tasks, including named entity recognition, relation extraction and attribute
extraction. With a unified framework, DeepKE allows developers and researchers
to customize datasets and models to extract information from unstructured data
according to their requirements. Specifically, DeepKE not only provides various
functional modules and model implementation for different tasks and scenarios
but also organizes all components by consistent frameworks to maintain
sufficient modularity and extensibility. We release the source code at GitHub
in https://github.com/zjunlp/DeepKE with Google Colab tutorials and
comprehensive documents for beginners. Besides, we present an online system in
<a href="http://deepke.openkg.cn/EN/re_doc_show.html">this http URL</a> for real-time extraction of various
tasks, and a demo video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering Fake-News Mitigation: Insights from Sharers' Social Media Post-Histories. (arXiv:2203.10560v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10560">
<div class="article-summary-box-inner">
<span><p>Misinformation is a global concern and limiting its spread is critical for
protecting democracy, public health, and consumers. We propose that consumers'
own social media post-histories are an underutilized data source to study what
leads them to share links to fake-news. In Study 1, we explore how textual cues
extracted from post-histories distinguish fake-news sharers from random social
media users and others in the misinformation ecosystem. Among other results, we
find across two datasets that fake-news sharers use more words related to
anger, religion and power. In Study 2, we show that adding textual cues from
post-histories improves the accuracy of models to predict who is likely to
share fake-news. In Study 3, we provide a preliminary test of two mitigation
strategies deduced from Study 1 - activating religious values and reducing
anger - and find that they reduce fake-news sharing and sharing more generally.
In Study 4, we combine survey responses with users' verified Twitter
post-histories and show that using empowering language in a fact-checking
browser extension ad increases download intentions. Our research encourages
marketers, misinformation scholars, and practitioners to use post-histories to
develop theories and test interventions to reduce the spread of misinformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02357">
<div class="article-summary-box-inner">
<span><p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual
knowledge, have recently been successfully applied to tasks such as information
retrieval, question answering, and recommendation system. Since most MKGs are
far from complete, extensive knowledge graph completion studies have been
proposed focusing on the multimodal entity, relation extraction and link
prediction. However, different tasks and modalities require changes to the
model architecture, and not all images/objects are relevant to text input,
which hinders the applicability to diverse real-world scenarios. In this paper,
we propose a hybrid transformer with multi-level fusion to address those
issues. Specifically, we leverage a hybrid transformer architecture with
unified input-output for diverse multimodal knowledge graph completion tasks.
Moreover, we propose multi-level fusion, which integrates visual and text
representation via coarse-grained prefix-guided interaction and fine-grained
correlation-aware fusion modules. We conduct extensive experiments to validate
that our MKGformer can obtain SOTA performance on four datasets of multimodal
link prediction, multimodal RE, and multimodal NER. Code is available in
https://github.com/zjunlp/MKGformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The expected sum of edge lengths in planar linearizations of trees. Theory and applications. (arXiv:2207.05564v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05564">
<div class="article-summary-box-inner">
<span><p>Dependency trees have proven to be a very successful model to represent the
syntactic structure of sentences of human languages. In these structures,
vertices are words and edges connect syntactically-dependent words. The
tendency of these dependencies to be short has been demonstrated using random
baselines for the sum of the lengths of the edges or its variants. A ubiquitous
baseline is the expected sum in projective orderings (wherein edges do not
cross and the root word of the sentence is not covered by any edge), that can
be computed in time $O(n)$. Here we focus on a weaker formal constraint, namely
planarity. In the theoretical domain, we present a characterization of
planarity that, given a sentence, yields either the number of planar
permutations or an efficient algorithm to generate uniformly random planar
permutations of the words. We also show the relationship between the expected
sum in planar arrangements and the expected sum in projective arrangements. In
the domain of applications, we derive a $O(n)$-time algorithm to calculate the
expected value of the sum of edge lengths. We also apply this research to a
parallel corpus and find that the gap between actual dependency distance and
the random baseline reduces as the strength of the formal constraint on
dependency structures increases, suggesting that formal constraints absorb part
of the dependency distance minimization effect. Our research paves the way for
replicating past research on dependency distance minimization using random
planar linearizations as random baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Analysis on Topics Using Word2Vec. (arXiv:2209.11717v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.11717">
<div class="article-summary-box-inner">
<span><p>The present study proposes a novel method of trend detection and
visualization - more specifically, modeling the change in a topic over time.
Where current models used for the identification and visualization of trends
only convey the popularity of a singular word based on stochastic counting of
usage, the approach in the present study illustrates the popularity and
direction that a topic is moving in. The direction in this case is a distinct
subtopic within the selected corpus. Such trends are generated by modeling the
movement of a topic by using k-means clustering and cosine similarity to group
the distances between clusters over time. In a convergent scenario, it can be
inferred that the topics as a whole are meshing (tokens between topics,
becoming interchangeable). On the contrary, a divergent scenario would imply
that each topics' respective tokens would not be found in the same context (the
words are increasingly different to each other). The methodology was tested on
a group of articles from various media houses present in the 20 Newsgroups
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10678">
<div class="article-summary-box-inner">
<span><p>This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10709">
<div class="article-summary-box-inner">
<span><p>With the development of pre-trained language models, many prompt-based
approaches to data-efficient knowledge graph construction have been proposed
and achieved impressive performance. However, existing prompt-based learning
methods for knowledge graph construction are still susceptible to several
potential limitations: (i) semantic gap between natural language and output
structured knowledge with pre-defined schema, which means model cannot fully
exploit semantic knowledge with the constrained templates; (ii) representation
learning with locally individual instances limits the performance given the
insufficient features, which are unable to unleash the potential analogical
capability of pre-trained language models. Motivated by these observations, we
propose a retrieval-augmented approach, which retrieves schema-aware Reference
As Prompt (RAP), for data-efficient knowledge graph construction. It can
dynamically leverage schema and knowledge inherited from human-annotated and
weak-supervised data as a prompt for each sample, which is model-agnostic and
can be plugged into widespread existing approaches. Experimental results
demonstrate that previous methods integrated with RAP can achieve impressive
performance gains in low-resource settings on five datasets of relational
triple extraction and event extraction for knowledge graph construction. Code
is available in https://github.com/zjunlp/RAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12714">
<div class="article-summary-box-inner">
<span><p>Generative Knowledge Graph Construction (KGC) refers to those methods that
leverage the sequence-to-sequence framework for building knowledge graphs,
which is flexible and can be adapted to widespread tasks. In this study, we
summarize the recent compelling progress in generative knowledge graph
construction. We present the advantages and weaknesses of each paradigm in
terms of different generation targets and provide theoretical insight and
empirical analysis. Based on the review, we suggest promising research
directions for the future. Our contributions are threefold: (1) We present a
detailed, complete taxonomy for the generative KGC methods; (2) We provide a
theoretical and empirical analysis of the generative KGC methods; (3) We
propose several research directions that can be developed in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15421">
<div class="article-summary-box-inner">
<span><p>Understanding visually-rich business documents to extract structured data and
automate business workflows has been receiving attention both in academia and
industry. Although recent multi-modal language models have achieved impressive
results, we find that existing benchmarks do not reflect the complexity of real
documents seen in industry. In this work, we identify the desiderata for a more
comprehensive benchmark and propose one we call Visually Rich Document
Understanding (VRDU). VRDU contains two datasets that represent several
challenges: rich schema including diverse data types as well as hierarchical
entities, complex templates including tables and multi-column layouts, and
diversity of different layouts (templates) within a single document type. We
design few-shot and conventional experiment settings along with a carefully
designed matching algorithm to evaluate extraction results. We report the
performance of strong baselines and offer three observations: (1) generalizing
to new document templates is still very challenging, (2) few-shot performance
has a lot of headroom, and (3) models struggle with hierarchical fields such as
line-items in an invoice. We plan to open source the benchmark and the
evaluation toolkit. We hope this helps the community make progress on these
challenging tasks in extracting structured data from visually rich documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09597">
<div class="article-summary-box-inner">
<span><p>Reasoning, as an essential ability for complex problem-solving, can provide
back-end support for various real-world applications, such as medical
diagnosis, negotiation, etc. This paper provides a comprehensive survey of
cutting-edge research on reasoning with language model prompting. We introduce
research works with comparisons and summaries and provide systematic resources
to help beginners. We also discuss the potential reasons for emerging such
reasoning abilities and highlight future research directions. Resources are
available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated
periodically).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10410">
<div class="article-summary-box-inner">
<span><p>Cross-domain NER is a challenging task to address the low-resource problem in
practical scenarios. Previous typical solutions mainly obtain a NER model by
pre-trained language models (PLMs) with data from a rich-resource domain and
adapt it to the target domain. Owing to the mismatch issue among entity types
in different domains, previous approaches normally tune all parameters of PLMs,
ending up with an entirely new NER model for each domain. Moreover, current
models only focus on leveraging knowledge in one general source domain while
failing to successfully transfer knowledge from multiple sources to the target.
To address these issues, we introduce Collaborative Domain-Prefix Tuning for
cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,
we present text-to-text generation grounding domain-related instructors to
transfer knowledge to new domain NER tasks without structural modifications. We
utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate
the potential of PLMs to handle NER tasks across various domains. Experimental
results on the Cross-NER benchmark show that the proposed approach has flexible
transfer ability and performs better on both one-source and multiple-source
cross-domain NER tasks. Codes are available in
https://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets. (arXiv:2301.12139v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12139">
<div class="article-summary-box-inner">
<span><p>We investigate five English NLP benchmark datasets (on the superGLUE
leaderboard) and two Swedish datasets for bias, along multiple axes. The
datasets are the following: Boolean Question (Boolq), CommitmentBank (CB),
Winograd Schema Challenge (WSC), Wino-gender diagnostic (AXg), Recognising
Textual Entailment (RTE), Swedish CB, and SWEDN. Bias can be harmful and it is
known to be common in data, which ML models learn from. In order to mitigate
bias in data, it is crucial to be able to estimate it objectively. We use
bipol, a novel multi-axes bias metric with explainability, to estimate and
explain how much bias exists in these datasets. Multilingual, multi-axes bias
evaluation is not very common. Hence, we also contribute a new, large Swedish
bias-labelled dataset (of 2 million samples), translated from the English
version and train the SotA mT5 model on it. In addition, we contribute new
multi-axes lexica for bias detection in Swedish. We make the codes, model, and
new dataset publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07729">
<div class="article-summary-box-inner">
<span><p>Nowadays many research articles are prefaced with research highlights to
summarize the main findings of the paper. Highlights not only help researchers
precisely and quickly identify the contributions of a paper, they also enhance
the discoverability of the article via search engines. We aim to automatically
construct research highlights given certain segments of a research paper. We
use a pointer-generator network with coverage mechanism and a contextual
embedding layer at the input that encodes the input tokens into SciBERT
embeddings. We test our model on a benchmark dataset, CSPubSum, and also
present MixSub, a new multi-disciplinary corpus of papers for automatic
research highlight generation. For both CSPubSum and MixSub, we have observed
that the proposed model achieves the best performance compared to related
variants and other models proposed in the literature. On the CSPubSum dataset,
our model achieves the best performance when the input is only the abstract of
a paper as opposed to other segments of the paper. It produces ROUGE-1, ROUGE-2
and ROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR score of
32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On the
new MixSub dataset, where only the abstract is the input, our proposed model
(when trained on the whole training corpus without distinguishing between the
subject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,
9.76 and 29.3, respectively, METEOR score of 24.00, and BERTScore F1 of 85.25.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v6 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10186">
<div class="article-summary-box-inner">
<span><p>This paper rethink some aspects of speech processing using speech encoders,
specifically about extracting entities directly from speech, without
intermediate textual representation. In human-computer conversations,
extracting entities such as names, street addresses and email addresses from
speech is a challenging task. In this paper, we study the impact of fine-tuning
pre-trained speech encoders on extracting spoken entities in human-readable
form directly from speech without the need for text transcription. We
illustrate that such a direct approach optimizes the encoder to transcribe only
the entity relevant portions of speech ignoring the superfluous portions such
as carrier phrases, or spell name entities. In the context of dialog from an
enterprise virtual agent, we demonstrate that the 1-step approach outperforms
the typical 2-step approach which first generates lexical transcriptions
followed by text-based entity extraction for identifying spoken entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study. (arXiv:2304.00723v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00723">
<div class="article-summary-box-inner">
<span><p>Evaluating the quality of generated text is a challenging task in NLP, due to
the inherent complexity and diversity of text. Recently, large language models
(LLMs) have garnered significant attention due to their impressive performance
in various tasks. Therefore, we present this paper to investigate the
effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their
use in assessing text quality. We compared three kinds of reference-free
evaluation methods. The experimental results prove that ChatGPT is capable of
evaluating text quality effectively from various perspectives without reference
and demonstrates superior performance than most existing automatic metrics. In
particular, the Explicit Score, which utilizes ChatGPT to generate a numeric
score measuring text quality, is the most effective and reliable method among
the three exploited approaches. However, directly comparing the quality of two
texts may lead to suboptimal results. We believe this paper will provide
valuable insights for evaluating text quality with LLMs and have released the
used data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01483">
<div class="article-summary-box-inner">
<span><p>Transformer-based models, exemplified by GPT-3, ChatGPT, and GPT-4, have
recently garnered considerable attention in both academia and industry due to
their promising performance in general language tasks. Nevertheless, these
models typically involve computationally encoding processes, and in some cases,
decoding processes as well, both of which are fundamentally large-scale matrix
multiplication. These operations bring the inevitable challenges of massive
computation resources and huge memory footprint, usually requiring at least
10^23 FLOPs and hundreds of gigabytes, respectively. A common method to address
this issue is to reduce the computational and memory requirements by applying
layerwise quantization to the transformer, replacing the usual fp32 data type
with a low-bit equivalent. Unfortunately, this method often leads to decreased
model accuracy and necessitates time-consuming retraining. Such retraining not
only requires fine-tuning skills but also substantial computational resources,
posing challenges for users. To specifically tackle these issues, we propose
BCT, a framework of blockwise compression for transformers without retraining,
aiming to facilitate model deployment. Unlike layerwise compression methods,
BCT achieves finer compression of the entire transformer by operating
blockwise. This method mitigates data distribution deviation caused by
quantization, eliminating the requirement for retraining. BCT effectively
compresses all components of the model, including but not limited to the
embedding, matrix multiplication, GELU, Softmax, layer normalization, and
intermediate results. In a case study, an efficient model is compressed by BCT
achieving up to 7.988x compression. Subsequently, we also evaluate it on
several General Language Understanding Evaluation (GLUE) datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP. (arXiv:2304.04029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04029">
<div class="article-summary-box-inner">
<span><p>We introduce bipol, a new metric with explainability, for estimating social
bias in text data. Harmful bias is prevalent in many online sources of data
that are used for training machine learning (ML) models. In a step to address
this challenge we create a novel metric that involves a two-step process:
corpus-level evaluation based on model classification and sentence-level
evaluation based on (sensitive) term frequency (TF). After creating new models
to detect bias along multiple axes using SotA architectures, we evaluate two
popular NLP datasets (COPA and SQUAD). As additional contribution, we created a
large dataset (with almost 2 million labelled samples) for training models in
bias detection and make it publicly available. We also make public our codes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06364">
<div class="article-summary-box-inner">
<span><p>Evaluating the general abilities of foundation models to tackle human-level
tasks is a vital aspect of their development and application in the pursuit of
Artificial General Intelligence (AGI). Traditional benchmarks, which rely on
artificial datasets, may not accurately represent human-level capabilities. In
this paper, we introduce AGIEval, a novel benchmark specifically designed to
assess foundation model in the context of human-centric standardized exams,
such as college entrance exams, law school admission tests, math competitions,
and lawyer qualification tests. We evaluate several state-of-the-art foundation
models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.
Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math
competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%
accuracy on the English test of the Chinese national college entrance exam.
This demonstrates the extraordinary performance of contemporary foundation
models. In contrast, we also find that GPT-4 is less proficient in tasks that
require complex reasoning or specific domain knowledge. Our comprehensive
analyses of model capabilities (understanding, knowledge, reasoning, and
calculation) reveal these models' strengths and limitations, providing valuable
insights into future directions for enhancing their general capabilities. By
concentrating on tasks pertinent to human cognition and decision-making, our
benchmark delivers a more meaningful and robust evaluation of foundation
models' performance in real-world scenarios. The data, code, and all model
outputs are released in https://github.com/ruixiangcui/AGIEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14104">
<div class="article-summary-box-inner">
<span><p>Interactions between humans are diverse and context-dependent, but previous
works have treated them as categorical, disregarding the heavy tail of possible
interactions. We propose a new paradigm of learning human-human interactions as
free text from a single still image, allowing for flexibility in modeling the
unlimited space of situations and relationships between people. To overcome the
absence of data labelled specifically for this task, we use knowledge
distillation applied to synthetic caption data produced by a large language
model without explicit supervision. We show that the pseudo-labels produced by
this procedure can be used to train a captioning model to effectively
understand human-human interactions in images, as measured by a variety of
metrics that measure textual and semantic faithfulness and factual groundedness
of our predictions. We further show that our approach outperforms SOTA image
captioning and situation recognition models on this task. We will release our
code and pseudo-labels along with Waldo and Wenda, a manually-curated test set
for still image human-human interaction understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decouple knowledge from parameters for plug-and-play language modeling. (arXiv:2305.11564v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11564">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models(PLM) have made impressive results in various NLP
tasks. It has been revealed that one of the key factors to their success is the
parameters of these models implicitly learn all kinds of knowledge during
pre-training. However, encoding knowledge implicitly in the model parameters
has two fundamental drawbacks. First, the knowledge is neither editable nor
scalable once the model is trained, which is especially problematic in that
knowledge is consistently evolving. Second, it lacks interpretability and
prevents humans from understanding which knowledge PLM requires for a certain
problem. In this paper, we introduce PlugLM, a pre-training model with
differentiable plug-in memory(DPM). The key intuition is to decouple the
knowledge storage from model parameters with an editable and scalable key-value
memory and leverage knowledge in an explainable manner by knowledge retrieval
in the DPM. To justify this design choice, we conduct evaluations in three
settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements
across four domains on average without any in-domain pre-training. (2)
knowledge update. PlugLM could absorb new knowledge in a training-free way
after pre-training is done. (3) in-task knowledge learning. PlugLM could be
further improved by incorporating training samples into DPM with knowledge
prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model. (arXiv:2305.13014v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13014">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as powerful generative Artificial
Intelligence solutions which can be applied to several fields and areas of
work. This paper presents results and reflection of an experiment done to use
the model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic
Analysis. Previous research on this subject has largely worked on conducting
deductive analysis. Thematic Analysis is a qualitative method for analysis
commonly used in social sciences and it is based on interpretations made by the
human analyst(s) and the identification of explicit and latent meanings in
qualitative data. Attempting an analysis based on human interpretation with an
LLM clearly is a provocation but also a way to learn something about how these
systems can or cannot be used in qualitative research. The paper presents the
motivations for attempting this emulation, it reflects on how the six steps to
a Thematic Analysis proposed by Braun and Clarke can at least partially be
reproduced with the LLM and it also reflects on what are the outputs produced
by the model. The paper used two existing datasets of open access
semi-structured interviews, previously analysed with Thematic Analysis by other
researchers. It used the previously produced analysis (and the related themes)
to compare with the results produced by the LLM. The results show that the
model can infer at least partially some of the main Themes. The objective of
the paper is not to replace human analysts in qualitative analysis but to learn
if some elements of LLM data manipulation can to an extent be of support for
qualitative research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13617">
<div class="article-summary-box-inner">
<span><p>Event-centric structured prediction involves predicting structured outputs of
events. In most NLP cases, event structures are complex with manifold
dependency, and it is challenging to effectively represent these complicated
structured events. To address these issues, we propose Structured Prediction
with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex
dependency among event structured components with energy-based modeling, and
represents event classes with simple but effective hyperspheres. Experiments on
two unified-annotated event datasets indicate that SPEECH is predominant in
event detection and event-relation extraction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14459">
<div class="article-summary-box-inner">
<span><p>Automatically open-ended long text generation poses significant challenges
due to semantic incoherence and plot implausibility. Previous works usually
alleviate this problem through outlines in the form of short phrases or
abstractive signals by designing unsupervised tasks, which tend to be unstable
and weakly interpretable.
</p>
<p>Assuming that a summary serves as a mature outline, we introduce a two-stage,
summary-enhanced outline supervised generation framework. This framework
leverages the dual characteristics of the summarization task to improve outline
prediction, resulting in more explicit and plausible outlines. Furthermore, we
identify an underutilization issue in outline-based generation with both
standard pretrained language models (e.g., GPT-2, BART) and large language
models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit
outline control method for more effective utilization of generated outlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Psychological Metrics for Dialog System Evaluation. (arXiv:2305.14757v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14757">
<div class="article-summary-box-inner">
<span><p>We present metrics for evaluating dialog systems through a
psychologically-grounded "human" lens in which conversational agents express a
diversity of both states (e.g., emotion) and traits (e.g., personality), just
as people do. We present five interpretable metrics from established psychology
that are fundamental to human communication and relationships: emotional
entropy, linguistic style and emotion matching, agreeableness, and empathy.
These metrics can be applied (1) across dialogs and (2) on turns within
dialogs. The psychological metrics are compared against seven state-of-the-art
traditional metrics (e.g., BARTScore and BLEURT) on seven standard dialog
system data sets. We also introduce a novel data set, the Three Bot Dialog
Evaluation Corpus, which consists of annotated conversations from ChatGPT,
GPT-3, and BlenderBot. We demonstrate that our proposed metrics offer novel
information; they are uncorrelated with traditional metrics, can be used to
meaningfully compare dialog systems, and lead to increased accuracy (beyond
existing traditional metrics) in predicting crowd-sourced dialog judgements.
The interpretability and unique signal of our psychological metrics make them a
valuable tool for evaluating and improving dialog systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weaker Than You Think: A Critical Look at Weakly Supervised Learning. (arXiv:2305.17442v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17442">
<div class="article-summary-box-inner">
<span><p>Weakly supervised learning is a popular approach for training machine
learning models in low-resource settings. Instead of requesting high-quality
yet costly human annotations, it allows training models with noisy annotations
obtained from various weak sources. Recently, many sophisticated approaches
have been proposed for robust training under label noise, reporting impressive
results. In this paper, we revisit the setup of these approaches and find that
the benefits brought by these approaches are significantly overestimated.
Specifically, we find that the success of existing weakly supervised learning
approaches heavily relies on the availability of clean validation samples
which, as we show, can be leveraged much more efficiently by simply training on
them. After using these clean labels in training, the advantages of using these
sophisticated approaches are mostly wiped out. This remains true even when
reducing the size of the available clean data to just five samples per class,
making these approaches impractical. To understand the true value of weakly
supervised learning, we thoroughly analyze diverse NLP datasets and tasks to
ascertain when and why weakly supervised approaches work. Based on our
findings, we provide recommendations for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization. (arXiv:2306.01102v4 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01102">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as powerful tools capable of
accomplishing a broad spectrum of tasks. Their abilities span numerous areas,
and one area where they have made a significant impact is in the domain of code
generation. In this context, we view LLMs as mutation and crossover tools.
Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and
robust solutions. By merging the code-generating abilities of LLMs with the
diversity and robustness of QD solutions, we introduce LLMatic, a Neural
Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS
directly through prompts, LLMatic uses a procedural approach, leveraging QD for
prompts and network architecture to create diverse and highly performant
networks. We test LLMatic on the CIFAR-10 image classification benchmark,
demonstrating that it can produce competitive networks with just $2,000$
searches, even without prior knowledge of the benchmark domain or exposure to
any previous top-performing models for the benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified model for code-switching speech recognition and language identification based on a concatenated tokenizer. (arXiv:2306.08753v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08753">
<div class="article-summary-box-inner">
<span><p>Code-Switching (CS) multilingual Automatic Speech Recognition (ASR) models
can transcribe speech containing two or more alternating languages during a
conversation. This paper proposes (1) a new method for creating code-switching
ASR datasets from purely monolingual data sources, and (2) a novel Concatenated
Tokenizer that enables ASR models to generate language ID for each emitted text
token while reusing existing monolingual tokenizers. The efficacy of these
approaches for building CS ASR models is demonstrated for two language pairs,
English-Hindi and English-Spanish, where we achieve new state-of-the-art
results on the Miami Bangor CS evaluation corpus. In addition to competitive
ASR performance, the proposed Concatenated Tokenizer models are highly
effective for spoken language identification, achieving 98%+ accuracy on the
out-of-distribution FLEURS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17181">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GAN) is a model for data synthesis, which
creates plausible data through the competition of generator and discriminator.
Although GAN application to image synthesis is extensively studied, it has
inherent limitations to natural language generation. Because natural language
is composed of discrete tokens, a generator has difficulty updating its
gradient through backpropagation; therefore, most text-GAN studies generate
sentences starting with a random token based on a reward system. Thus, the
generators of previous studies are pre-trained in an autoregressive way before
adversarial training, causing data memorization that synthesized sentences
reproduce the training data. In this paper, we synthesize sentences using a
framework similar to the original GAN. More specifically, we propose Text
Embedding Space Generative Adversarial Networks (TESGAN) which generate
continuous text embedding spaces instead of discrete tokens to solve the
gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised
learning which does not directly refer to the text of the training data to
overcome the data memorization issue. By adopting this novel method, TESGAN can
synthesize new sentences, showing the potential of unsupervised learning for
text synthesis. We expect to see extended research combining Large Language
Models with a new perspective of viewing text as an continuous space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved NL2SQL based on Multi-layer Expert Network. (arXiv:2306.17727v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17727">
<div class="article-summary-box-inner">
<span><p>The Natural Language to SQL (NL2SQL) technique is used to convert natural
language queries into executable SQL statements. Typically, slot-filling is
employed as a classification method for multi-task cases to achieve this goal.
However, slot-filling can result in inaccurate SQL statement generation due to
negative migration issues arising from different classification tasks. To
overcome this limitation, this study introduces a new approach called
Multi-Layer Expert Generate SQL (MLEG-SQL), which utilizes a dedicated
multi-task hierarchical network. The lower layer of the network extracts
semantic features of natural language statements, while the upper layer builds
a specialized expert system for handling specific classification tasks. This
hierarchical approach mitigates performance degradation resulting from
different task conflicts. The proposed method was evaluated on the WiKSQL
dataset and was found to be effective in generating accurate SQL statements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.01226">
<div class="article-summary-box-inner">
<span><p>Recently, Neural Topic Models (NTM), inspired by variational autoencoders,
have attracted a lot of research interest; however, these methods have limited
applications in the real world due to the challenge of incorporating human
knowledge. This work presents a semi-supervised neural topic modeling method,
vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and
optimal transport. When a few keywords per topic are provided, vONTSS in the
semi-supervised setting generates potential topics and optimizes topic-keyword
quality and topic classification. Experiments show that vONTSS outperforms
existing semi-supervised topic modeling methods in classification accuracy and
diversity. vONTSS also supports unsupervised topic modeling. Quantitative and
qualitative experiments show that vONTSS in the unsupervised setting
outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered
and coherent topics on benchmark datasets. It is also much faster than the
state-of-the-art weakly supervised text classification method while achieving
similar classification performance. We further prove the equivalence of optimal
transport loss and cross-entropy loss at the global minimum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Founding a mathematical diffusion model in linguistics. The case study of German syntactic features in the North-Eastern Italian dialects. (arXiv:2307.14291v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.14291">
<div class="article-summary-box-inner">
<span><p>The initial motivation for this work was the linguistic case of the spread of
Germanic syntactic features into Romance dialects of North-Eastern Italy, which
occurred after the immigration of German people to Tyrol during the High Middle
Ages. To obtain a representation of the data over the territory suitable for a
mathematical formulation, an interactive map is produced as a first step, using
tools of what is called Geographic Data Science. A smooth two-dimensional
surface G is introduced, expressing locally which fraction of territory uses a
given German language feature: it is obtained by a piecewise cubic curvature
minimizing interpolant of the discrete function that says if at any surveyed
locality that feature is used or not. This surface G is thought of as the value
at the present time of a function describing a diffusion-convection phenomenon
in two dimensions (here said tidal mode), which is subjected in a very natural
way to the same equation used in physics, introducing a contextual diffusivity
concept: it is shown that with two different assumptions about diffusivity,
solutions of this equation, evaluated at the present time, fit well with the
data interpolated by G, thus providing two convincing different pictures of
diffusion-convection in the case under study, albeit simplifications and
approximations. Very importantly, it is shown that the linguistic diffusion
model known to linguists as Schmidt waves can be counted among the solutions of
the diffusion equation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.14385">
<div class="article-summary-box-inner">
<span><p>Advances in large language models (LLMs) have empowered a variety of
applications. However, there is still a significant gap in research when it
comes to understanding and enhancing the capabilities of LLMs in the field of
mental health. In this work, we present the first comprehensive evaluation of
multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on
various mental health prediction tasks via online text data. We conduct a broad
range of experiments, covering zero-shot prompting, few-shot prompting, and
instruction fine-tuning. The results indicate a promising yet limited
performance of LLMs with zero-shot and few-shot prompt designs for the mental
health tasks. More importantly, our experiments show that instruction
finetuning can significantly boost the performance of LLMs for all tasks
simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5,
outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9%
on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%.
They further perform on par with the state-of-the-art task-specific language
model. We also conduct an exploratory case study on LLMs' capability on the
mental health reasoning tasks, illustrating the promising capability of certain
models such as GPT-4. We summarize our findings into a set of action guidelines
for potential methods to enhance LLMs' capability for mental health tasks.
Meanwhile, we also emphasize the important limitations before achieving
deployability in real-world mental health settings, such as known racial and
gender bias. We highlight the important ethical risks accompanying this line of
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04992">
<div class="article-summary-box-inner">
<span><p>Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text
and image) for a comprehensive understanding of entities. Despite the recent
progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature
of entities, limiting the ability to comprehend entities from various
perspectives. In this paper, we construct AspectMMKG, the first MMKG with
aspect-related images by matching images to different entity aspects.
Specifically, we collect aspect-related images from a knowledge base, and
further extract aspect-related sentences from the knowledge base as queries to
retrieve a large number of aspect-related images via an online image search
engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and
645,383 aspect-related images. We demonstrate the usability of AspectMMKG in
entity aspect linking (EAL) downstream task and show that previous EAL models
achieve a new state-of-the-art performance with the help of AspectMMKG. To
facilitate the research on aspect-related MMKG, we further propose an
aspect-related image retrieval (AIR) model, that aims to correct and expand
aspect-related images in AspectMMKG. We train an AIR model to learn the
relationship between entity image and entity aspect-related images by
incorporating entity image, aspect, and aspect image information. Experimental
results indicate that the AIR model could retrieve suitable images for a given
entity w.r.t different aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling the Dashboard Provenance. (arXiv:2308.06788v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06788">
<div class="article-summary-box-inner">
<span><p>Organizations of all kinds, whether public or private, profit-driven or
non-profit, and across various industries and sectors, rely on dashboards for
effective data visualization. However, the reliability and efficacy of these
dashboards rely on the quality of the visual and data they present. Studies
show that less than a quarter of dashboards provide information about their
sources, which is just one of the expected metadata when provenance is
seriously considered. Provenance is a record that describes people,
organizations, entities, and activities that had a role in the production,
influence, or delivery of a piece of data or an object. This paper aims to
provide a provenance representation model, that entitles standardization,
modeling, generation, capture, and visualization, specifically designed for
dashboards and its visual and data components. The proposed model will offer a
comprehensive set of essential provenance metadata that enables users to
evaluate the quality, consistency, and reliability of the information presented
on dashboards. This will allow a clear and precise understanding of the context
in which a specific dashboard was developed, ultimately leading to better
decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07633">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized natural language processing
tasks with remarkable success. However, their formidable size and computational
demands present significant challenges for practical deployment, especially in
resource-constrained environments. As these challenges become increasingly
pertinent, the field of model compression has emerged as a pivotal research
area to alleviate these limitations. This paper presents a comprehensive survey
that navigates the landscape of model compression techniques tailored
specifically for LLMs. Addressing the imperative need for efficient deployment,
we delve into various methodologies, encompassing quantization, pruning,
knowledge distillation, and more. Within each of these techniques, we highlight
recent advancements and innovative approaches that contribute to the evolving
landscape of LLM research. Furthermore, we explore benchmarking strategies and
evaluation metrics that are essential for assessing the effectiveness of
compressed LLMs. By providing insights into the latest developments and
practical implications, this survey serves as an invaluable resource for both
researchers and practitioners. As LLMs continue to evolve, this survey aims to
facilitate enhanced efficiency and real-world applicability, establishing a
foundation for future advancements in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Benchmarking (of Language Models). (arXiv:2308.11696v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11696">
<div class="article-summary-box-inner">
<span><p>The increasing versatility of language models LMs has given rise to a new
class of benchmarks that comprehensively assess a broad range of capabilities.
Such benchmarks are associated with massive computational costs reaching
thousands of GPU hours per model. However the efficiency aspect of these
evaluation efforts had raised little discussion in the literature. In this work
we present the problem of Efficient Benchmarking namely intelligently reducing
the computation costs of LM evaluation without compromising reliability. Using
the HELM benchmark as a test case we investigate how different benchmark design
choices affect the computation-reliability tradeoff. We propose to evaluate the
reliability of such decisions by using a new measure Decision Impact on
Reliability DIoR for short. We find for example that the current leader on HELM
may change by merely removing a low-ranked model from the benchmark and observe
that a handful of examples suffice to obtain the correct benchmark ranking.
Conversely a slightly different choice of HELM scenarios varies ranking widely.
Based on our findings we outline a set of concrete recommendations for more
efficient benchmark design and utilization practices leading to dramatic cost
savings with minimal loss of benchmark reliability often reducing computation
by x100 or more.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12032">
<div class="article-summary-box-inner">
<span><p>In the realm of Large Language Models, the balance between instruction data
quality and quantity has become a focal point. Recognizing this, we introduce a
self-guided methodology for LLMs to autonomously discern and select cherry
samples from vast open-source datasets, effectively minimizing manual curation
and potential cost for instruction tuning an LLM. Our key innovation, the
Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to
identify discrepancies between a model's expected responses and its autonomous
generation prowess. Through the adept application of IFD, cherry samples are
pinpointed, leading to a marked uptick in model training efficiency. Empirical
validations on renowned datasets like Alpaca and WizardLM underpin our
findings; with a mere 10% of conventional data input, our strategy showcases
improved results. This synthesis of self-guided cherry-picking and the IFD
metric signifies a transformative leap in the optimization of LLMs, promising
both efficiency and resource-conscious advancements. Codes, data, and models
are available: https://github.com/MingLiiii/Cherry_LLM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaSM: Large Language and Speech Model. (arXiv:2308.15930v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15930">
<div class="article-summary-box-inner">
<span><p>Multi-modal large language models have garnered significant interest
recently. Though, most of the works focus on vision-language multi-modal models
providing strong capabilities in following vision-and-language instructions.
However, we claim that speech is also an important modality through which
humans interact with the world. Hence, it is crucial for a general-purpose
assistant to be able to follow multi-modal speech-and-language instructions. In
this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an
end-to-end trained large multi-modal speech-language model with cross-modal
conversational abilities, capable of following speech-and-language
instructions. Our early experiments show that LLaSM demonstrates a more
convenient and natural way for humans to interact with artificial intelligence.
Specifically, we also release a large Speech Instruction Following dataset
LLaSM-Audio-Instructions. Code and demo are available at
https://github.com/LinkSoul-AI/LLaSM and
https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions
dataset is available at
https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Hijacks: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00236">
<div class="article-summary-box-inner">
<span><p>Are foundation models secure from malicious actors? In this work, we focus on
the image input to a vision-language model (VLM). We discover image hijacks,
adversarial images that control generative models at runtime. We introduce
Behaviour Matching, a general method for creating image hijacks, and we use it
to explore three types of attacks. Specific string attacks generate arbitrary
output of the adversary's choice. Leak context attacks leak information from
the context window into the output. Jailbreak attacks circumvent a model's
safety training. We study these attacks against LLaVA, a state-of-the-art VLM
based on CLIP and LLaMA-2, and find that all our attack types have above a 90%
success rate. Moreover, our attacks are automated and require only small image
perturbations. These findings raise serious concerns about the security of
foundation models. If image hijacks are as difficult to defend against as
adversarial examples in CIFAR-10, then it might be many years before a solution
is found -- if it even exists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01029">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated impressive capabilities in
natural language processing. However, their internal mechanisms are still
unclear and this lack of transparency poses unwanted risks for downstream
applications. Therefore, understanding and explaining these models is crucial
for elucidating their behaviors, limitations, and social impacts. In this
paper, we introduce a taxonomy of explainability techniques and provide a
structured overview of methods for explaining Transformer-based language
models. We categorize techniques based on the training paradigms of LLMs:
traditional fine-tuning-based paradigm and prompting-based paradigm. For each
paradigm, we summarize the goals and dominant approaches for generating local
explanations of individual predictions and global explanations of overall model
knowledge. We also discuss metrics for evaluating generated explanations, and
discuss how explanations can be leveraged to debug models and improve
performance. Lastly, we examine key challenges and emerging opportunities for
explanation techniques in the era of LLMs in comparison to conventional machine
learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture. (arXiv:2309.01105v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01105">
<div class="article-summary-box-inner">
<span><p>This study presents a method for implementing generative AI services by
utilizing the Large Language Models (LLM) application architecture. With recent
advancements in generative AI technology, LLMs have gained prominence across
various domains. In this context, the research addresses the challenge of
information scarcity and proposes specific remedies by harnessing LLM
capabilities. The investigation delves into strategies for mitigating the issue
of inadequate data, offering tailored solutions. The study delves into the
efficacy of employing fine-tuning techniques and direct document integration to
alleviate data insufficiency. A significant contribution of this work is the
development of a Retrieval-Augmented Generation (RAG) model, which tackles the
aforementioned challenges. The RAG model is carefully designed to enhance
information storage and retrieval processes, ensuring improved content
generation. The research elucidates the key phases of the information storage
and retrieval methodology underpinned by the RAG model. A comprehensive
analysis of these steps is undertaken, emphasizing their significance in
addressing the scarcity of data. The study highlights the efficacy of the
proposed method, showcasing its applicability through illustrative instances.
By implementing the RAG model for information storage and retrieval, the
research not only contributes to a deeper comprehension of generative AI
technology but also facilitates its practical usability within enterprises
utilizing LLMs. This work holds substantial value in advancing the field of
generative AI, offering insights into enhancing data-driven content generation
and fostering active utilization of LLM-based services within corporate
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Sesame! Universal Black Box Jailbreaking of Large Language Models. (arXiv:2309.01446v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01446">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), designed to provide helpful and safe responses,
often rely on alignment techniques to align with user intent and social
guidelines. Unfortunately, this alignment can be exploited by malicious actors
seeking to manipulate an LLM's outputs for unintended purposes. In this paper
we introduce a novel approach that employs a genetic algorithm (GA) to
manipulate LLMs when model architecture and parameters are inaccessible. The GA
attack works by optimizing a universal adversarial prompt that -- when combined
with a user's query -- disrupts the attacked model's alignment, resulting in
unintended and potentially harmful outputs. Our novel approach systematically
reveals a model's limitations and vulnerabilities by uncovering instances where
its responses deviate from expected behavior. Through extensive experiments we
demonstrate the efficacy of our technique, thus contributing to the ongoing
discussion on responsible AI development by providing a diagnostic tool for
evaluating and enhancing alignment of LLMs with human intent. To our knowledge
this is the first automated universal black box jailbreak attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03852">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved remarkable success in NLP and
multimodal tasks, among others. Despite these successes, two main challenges
remain in developing LLMs: (i) high computational cost, and (ii) fair and
objective evaluations. In this paper, we report a solution to significantly
reduce LLM training cost through a growth strategy. We demonstrate that a
101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US
dollars. Inspired by IQ tests, we also consolidate an additional range of
evaluations on top of existing evaluations that focus on knowledge-oriented
abilities. These IQ evaluations include symbolic mapping, rule understanding,
pattern mining, and anti-interference. Such evaluations minimize the potential
impact of memorization. Experimental results show that our model, named
FLM-101B, trained with a budget of 100K US dollars, achieves performance
comparable to powerful and well-known models, e.g., GPT-3 and GLM-130B,
especially on the additional range of IQ evaluations. The checkpoint of
FLM-101B is released at https://huggingface.co/CofeAI/FLM-101B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Impact of Post-Training Quantization on Large Language Models. (arXiv:2309.05210v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05210">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are rapidly increasing in size, with the number
of parameters becoming a key factor in the success of many commercial models,
such as ChatGPT, Claude, and Bard. Even the recently released publicly
accessible models for commercial usage, such as Falcon and Llama2, come
equipped with billions of parameters. This significant increase in the number
of parameters makes deployment and operation very costly. The remarkable
progress in the field of quantization for large neural networks in general and
LLMs in particular, has made these models more accessible by enabling them to
be deployed on consumer-grade GPUs. Quantized models generally demonstrate
comparable performance levels to their unquantized base counterparts.
Nonetheless, there exists a notable gap in our comprehensive understanding of
how these quantized models respond to hyperparameters, such as temperature, max
new tokens, and topk, particularly for next word prediction. The present
analysis reveals that nf4 and fp4 are equally proficient 4-bit quantization
techniques, characterized by similar attributes such as inference speed, memory
consumption, and the quality of generated content. the study identifies nf4 as
displaying greater resilience to temperature variations in the case of the
llama2 series of models at lower temperature, while fp4 and fp4-dq proves to be
a more suitable choice for falcon series of models. It is noteworthy that, in
general, 4-bit quantized models of varying sizes exhibit higher sensitivity to
temperature in the range of 0.5 to 0.8, unlike their unquantized counterparts.
Additionally, int8 quantization is associated with significantly slower
inference speeds, whereas unquantized bfloat16 models consistently yield the
fastest inference speeds across models of all sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05833">
<div class="article-summary-box-inner">
<span><p>In recent years, the transition to cloud-based platforms in the IT sector has
emphasized the significance of cloud incident root cause analysis to ensure
service reliability and maintain customer trust. Central to this process is the
efficient determination of root causes, a task made challenging due to the
complex nature of contemporary cloud infrastructures. Despite the proliferation
of AI-driven tools for root cause identification, their applicability remains
limited by the inconsistent quality of their outputs. This paper introduces a
method for enhancing confidence estimation in root cause analysis tools by
prompting retrieval-augmented large language models (LLMs). This approach
operates in two phases. Initially, the model evaluates its confidence based on
historical incident data, considering its assessment of the evidence strength.
Subsequently, the model reviews the root cause generated by the predictor. An
optimization step then combines these evaluations to determine the final
confidence assignment. Experimental results illustrate that our method enables
the model to articulate its confidence effectively, providing a more calibrated
score. We address research questions evaluating the ability of our method to
produce calibrated confidence scores using LLMs, the impact of domain-specific
retrieved examples on confidence estimates, and its potential generalizability
across various root cause analysis models. Through this, we aim to bridge the
confidence estimation gap, aiding on-call engineers in decision-making and
bolstering the efficiency of cloud incident management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains. (arXiv:2309.06192v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06192">
<div class="article-summary-box-inner">
<span><p>News recommender systems play an increasingly influential role in shaping
information access within democratic societies. However, tailoring
recommendations to users' specific interests can result in the divergence of
information streams. Fragmented access to information poses challenges to the
integrity of the public sphere, thereby influencing democracy and public
discourse. The Fragmentation metric quantifies the degree of fragmentation of
information streams in news recommendations. Accurate measurement of this
metric requires the application of Natural Language Processing (NLP) to
identify distinct news events, stories, or timelines. This paper presents an
extensive investigation of various approaches for quantifying Fragmentation in
news recommendations. These approaches are evaluated both intrinsically, by
measuring performance on news story clustering, and extrinsically, by assessing
the Fragmentation scores of different simulated news recommender scenarios. Our
findings demonstrate that agglomerative hierarchical clustering coupled with
SentenceBERT text representation is substantially better at detecting
Fragmentation than earlier implementations. Additionally, the analysis of
simulated scenarios yields valuable insights and recommendations for
stakeholders concerning the measurement and interpretation of Fragmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06415">
<div class="article-summary-box-inner">
<span><p>This paper conducts a robustness audit of the safety feedback of PaLM 2
through a novel toxicity rabbit hole framework introduced here. Starting with a
stereotype, the framework instructs PaLM 2 to generate more toxic content than
the stereotype. Every subsequent iteration it continues instructing PaLM 2 to
generate more toxic content than the previous iteration until PaLM 2 safety
guardrails throw a safety violation. Our experiments uncover highly disturbing
antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few)
generated content that PaLM 2 safety guardrails do not evaluate as highly
unsafe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07812">
<div class="article-summary-box-inner">
<span><p>Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults. (arXiv:2309.07927v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07927">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Automatic Speech Recognition (ASR) systems,
exemplified by Whisper, have demonstrated the potential of these systems to
approach human-level performance given sufficient data. However, this progress
doesn't readily extend to ASR for children due to the limited availability of
suitable child-specific databases and the distinct characteristics of
children's speech. A recent study investigated leveraging the My Science Tutor
(MyST) children's speech corpus to enhance Whisper's performance in recognizing
children's speech. They were able to demonstrate some improvement on a limited
testset. This paper builds on these findings by enhancing the utility of the
MyST dataset through more efficient data preprocessing. We reduce the Word
Error Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and
from 13.23% to 8.61% with Whisper-Medium and show that this improvement can be
generalized to unseen datasets. We also highlight important challenges towards
improving children's ASR performance. The results showcase the viable and
efficient integration of Whisper for effective children's speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue. (arXiv:2309.08156v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08156">
<div class="article-summary-box-inner">
<span><p>Evaluating open-domain dialogue systems is challenging for reasons such as
the one-to-many problem, i.e., many appropriate responses other than just the
golden response. As of now, automatic evaluation methods need better
consistency with humans, while reliable human evaluation can be time- and
cost-intensive. To this end, we propose the Reference-Assisted Dialogue
Evaluation (RADE) approach under the multi-task learning framework, which
leverages the pre-created utterance as reference other than the gold response
to relief the one-to-many problem. Specifically, RADE explicitly compares
reference and the candidate response to predict their overall scores. Moreover,
an auxiliary response generation task enhances prediction via a shared encoder.
To support RADE, we extend three datasets with additional rated responses other
than just a golden response by human annotation. Experiments on our three
datasets and two existing benchmarks demonstrate the effectiveness of our
method, where Pearson, Spearman, and Kendall correlations with human evaluation
outperform state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-19 23:10:33.989087397 UTC">2023-09-19 23:10:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>