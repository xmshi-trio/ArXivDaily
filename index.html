<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-02-27T01:30:00Z">02-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Data leakage in cross-modal retrieval training: A case study. (arXiv:2302.12258v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12258">
<div class="article-summary-box-inner">
<span><p>The recent progress in text-based audio retrieval was largely propelled by
the release of suitable datasets. Since the manual creation of such datasets is
a laborious task, obtaining data from online resources can be a cheap solution
to create large-scale datasets. We study the recently proposed SoundDesc
benchmark dataset, which was automatically sourced from the BBC Sound Effects
web page. In our analysis, we find that SoundDesc contains several duplicates
that cause leakage of training data to the evaluation data. This data leakage
ultimately leads to overly optimistic retrieval performance estimates in
previous benchmarks. We propose new training, validation, and testing splits
for the dataset that we make available online. To avoid weak contamination of
the test data, we pool audio files that share similar recording setups. In our
experiments, we find that the new splits serve as a more challenging benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views. (arXiv:2302.12297v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12297">
<div class="article-summary-box-inner">
<span><p>Temporal concept drift refers to the problem of data changing over time. In
NLP, that would entail that language (e.g. new expressions, meaning shifts) and
factual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing
on the latter, we benchmark $11$ pretrained masked language models (MLMs) on a
series of tests designed to evaluate the effect of temporal concept drift, as
it is crucial that widely used language models remain up-to-date with the
ever-evolving factual updates of the real world. Specifically, we provide a
holistic framework that (1) dynamically creates temporal test sets of any time
granularity (e.g. month, quarter, year) of factual data from Wikidata, (2)
constructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to
ensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways
(single-token probing, multi-token generation, MLM scoring). In contrast to
prior work, our framework aims to unveil how robust an MLM is over time and
thus to provide a signal in case it has become outdated, by leveraging multiple
views of evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages. (arXiv:2302.12299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12299">
<div class="article-summary-box-inner">
<span><p>Multilingual generative language models (LMs) are increasingly fluent in a
large variety of languages. Trained on the concatenation of corpora in multiple
languages, they enable powerful transfer from high-resource languages to
low-resource ones. However, it is still unknown what cultural biases are
induced in the predictions of these models. In this work, we focus on one
language property highly influenced by culture: formality. We analyze the
formality distributions of XGLM and BLOOM's predictions, two popular generative
multilingual language models, in 5 languages. We classify 1,200 generations per
language as formal, informal, or incohesive and measure the impact of the
prompt formality on the predictions. Overall, we observe a diversity of
behaviors across the models and languages. For instance, XGLM generates
informal text in Arabic and Bengali when conditioned with informal prompts,
much more than BLOOM. In addition, even though both models are highly biased
toward the formal style when prompted neutrally, we find that the models
generate a significant amount of informal predictions even when prompted with
formal text. We release with this work 6,000 annotated samples, paving the way
for future work on the formality of generative multilingual LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning. (arXiv:2302.12313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12313">
<div class="article-summary-box-inner">
<span><p>Advances in computational methods and big data availability have recently
translated into breakthroughs in AI applications. With successes in bottom-up
challenges partially overshadowing shortcomings, the 'human-like' performance
of Large Language Models has raised the question of how linguistic performance
is achieved by algorithms. Given systematic shortcomings in generalization
across many AI systems, in this work we ask whether linguistic performance is
indeed guided by language knowledge in Large Language Models. To this end, we
prompt GPT-3 with a grammaticality judgement task and comprehension questions
on less frequent constructions that are thus unlikely to form part of Large
Language Models' training data. These included grammatical 'illusions',
semantic anomalies, complex nested hierarchies and self-embeddings. GPT-3
failed for every prompt but one, often offering answers that show a critical
lack of understanding even of high-frequency words used in these less frequent
grammatical constructions. The present work sheds light on the boundaries of
the alleged AI human-like linguistic competence and argues that, far from
human-like, the next-word prediction abilities of LLMs may face issues of
robustness, when pushed beyond training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12324">
<div class="article-summary-box-inner">
<span><p>Effective figure captions are crucial for clear comprehension of scientific
figures, yet poor caption writing remains a common issue in scientific
articles. Our study of arXiv cs.CL papers found that 53.88% of captions were
rated as unhelpful or worse by domain experts, showing the need for better
caption generation. Previous efforts in figure caption generation treated it as
a vision task, aimed at creating a model to understand visual content and
complex contextual information. Our findings, however, demonstrate that over
75% of figure captions' tokens align with corresponding figure-mentioning
paragraphs, indicating great potential for language technology to solve this
task. In this paper, we present a novel approach for generating figure captions
in scientific documents using text summarization techniques. Our approach
extracts sentences referencing the target figure, then summarizes them into a
concise caption. In the experiments on real-world arXiv papers (81.2% were
published at academic conferences), our method, using only text data,
outperformed previous approaches in both automatic and human evaluations. We
further conducted data-driven investigations into the two core challenges: (i)
low-quality author-written captions and (ii) the absence of a standard for good
captions. We found that our models could generate improved captions for figures
with original captions rated as unhelpful, and the model trained on captions
with more than 30 tokens produced higher-quality captions. We also found that
good captions often include the high-level takeaway of the figure. Our work
proves the effectiveness of text summarization in generating figure captions
for scholarly articles, outperforming prior vision-based approaches. Our
findings have practical implications for future figure captioning systems,
improving scientific communication clarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12343">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have yielded fast and dramatic progress in NLP,
and now offer strong few- and zero-shot capabilities on new tasks, reducing the
need for annotation. This is especially exciting for the medical domain, in
which supervision is often scant and expensive. At the same time, model
predictions are rarely so accurate that they can be trusted blindly. Clinicians
therefore tend to favor "interpretable" classifiers over opaque LLMs. For
example, risk prediction tools are often linear models defined over manually
crafted predictors that must be laboriously extracted from EHRs. We propose
CHiLL (Crafting High-Level Latents), which uses LLMs to permit natural language
specification of high-level features for linear models via zero-shot feature
extraction using expert-composed queries. This approach has the promise to
empower physicians to use their domain expertise to craft features which are
clinically meaningful for a downstream task of interest, without having to
manually extract these from raw EHR (as often done now). We are motivated by a
real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III
and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to
evaluate our approach. We find that linear models using automatically extracted
features are comparably performant to models using reference features, and
provide greater interpretability than linear models using "Bag-of-Words"
features. We verify that learned feature weights align well with clinical
expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Victim Counts from Text. (arXiv:2302.12367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12367">
<div class="article-summary-box-inner">
<span><p>Decision-makers in the humanitarian sector rely on timely and exact
information during crisis events. Knowing how many civilians were injured
during an earthquake is vital to allocate aids properly. Information about such
victim counts is often only available within full-text event descriptions from
newspapers and other reports. Extracting numbers from text is challenging:
numbers have different formats and may require numeric reasoning. This renders
purely string matching-based approaches insufficient. As a consequence,
fine-grained counts of injured, displaced, or abused victims beyond fatalities
are often not extracted and remain unseen. We cast victim count extraction as a
question answering (QA) task with a regression or classification objective. We
compare regex, dependency parsing, semantic role labeling-based approaches, and
advanced text-to-text models. Beyond model accuracy, we analyze extraction
reliability and robustness which are key for this sensitive task. In
particular, we discuss model calibration and investigate few-shot and
out-of-distribution performance. Ultimately, we make a comprehensive
recommendation on which model to select for different desiderata and data
domains. Our work is among the first to apply numeracy-focused large language
models in a real-world use case with a positive impact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factual Consistency Oriented Speech Recognition. (arXiv:2302.12369v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12369">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel optimization framework for automatic speech
recognition (ASR) with the aim of reducing hallucinations produced by an ASR
model. The proposed framework optimizes the ASR model to maximize an expected
factual consistency score between ASR hypotheses and ground-truth
transcriptions, where the factual consistency score is computed by a separately
trained estimator. Experimental results using the AMI meeting corpus and the
VoxPopuli corpus show that the ASR model trained with the proposed framework
generates ASR hypotheses that have significantly higher consistency scores with
ground-truth transcriptions while maintaining the word error rates close to
those of cross entropy-trained ASR models. Furthermore, it is shown that
training the ASR models with the proposed framework improves the speech
summarization quality as measured by the factual consistency of meeting
conversation summaries generated by a large language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Prediction Oriented method with Multiple Supervisions for Emotion-Cause Pair Extraction. (arXiv:2302.12417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12417">
<div class="article-summary-box-inner">
<span><p>Emotion-cause pair extraction (ECPE) task aims to extract all the pairs of
emotions and their causes from an unannotated emotion text. The previous works
usually extract the emotion-cause pairs from two perspectives of emotion and
cause. However, emotion extraction is more crucial to the ECPE task than cause
extraction. Motivated by this analysis, we propose an end-to-end emotion-cause
extraction approach oriented toward emotion prediction (EPO-ECPE), aiming to
fully exploit the potential of emotion prediction to enhance emotion-cause pair
extraction. Considering the strong dependence between emotion prediction and
emotion-cause pair extraction, we propose a synchronization mechanism to share
their improvement in the training process. That is, the improvement of emotion
prediction can facilitate the emotion-cause pair extraction, and then the
results of emotion-cause pair extraction can also be used to improve the
accuracy of emotion prediction simultaneously. For the emotion-cause pair
extraction, we divide it into genuine pair supervision and fake pair
supervision, where the genuine pair supervision learns from the pairs with more
possibility to be emotion-cause pairs. In contrast, fake pair supervision
learns from other pairs. In this way, the emotion-cause pairs can be extracted
directly from the genuine pair, thereby reducing the difficulty of extraction.
Experimental results show that our approach outperforms the 13 compared systems
and achieves new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics. (arXiv:2302.12433v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12433">
<div class="article-summary-box-inner">
<span><p>We introduce ProofNet, a benchmark for autoformalization and formal proving
of undergraduate-level mathematics. The ProofNet benchmarks consists of 371
examples, each consisting of a formal theorem statement in Lean 3, a natural
language theorem statement, and a natural language proof. The problems are
primarily drawn from popular undergraduate pure mathematics textbooks and cover
topics such as real and complex analysis, linear algebra, abstract algebra, and
topology. We intend for ProofNet to be a challenging benchmark that will drive
progress in autoformalization and automatic theorem proving. We report baseline
results on statement autoformalization via in-context learning. Moreover, we
introduce two novel statement autoformalization methods: prompt retrieval and
distilled backtranslation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUX-PLMs: Pre-training Language Models with Data Multiplexing. (arXiv:2302.12441v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12441">
<div class="article-summary-box-inner">
<span><p>Data multiplexing is a recently proposed method for improving a model's
inference efficiency by processing multiple instances simultaneously using an
ordered representation mixture. Prior work on data multiplexing only used
task-specific Transformers without any pre-training, which limited their
accuracy and generality. In this paper, we develop pre-trained multiplexed
language models (MUX-PLMs) that can be widely finetuned on any downstream task.
Our approach includes a three-stage training procedure and novel multiplexing
and demultiplexing modules for improving throughput and downstream task
accuracy. We demonstrate our method on BERT and ELECTRA pre-training
objectives, with our MUX-BERT and MUX-ELECTRA models achieving 2x/5x inference
speedup with a 2-4 \% drop in absolute performance on GLUE and 1-2 \% drop on
token-level tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. (arXiv:2302.12449v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12449">
<div class="article-summary-box-inner">
<span><p>Recently, much exertion has been paid to design graph self-supervised methods
to obtain generalized pre-trained models, and adapt pre-trained models onto
downstream tasks through fine-tuning. However, there exists an inherent gap
between pretext and downstream graph tasks, which insufficiently exerts the
ability of pre-trained models and even leads to negative transfer. Meanwhile,
prompt tuning has seen emerging success in natural language processing by
aligning pre-training and fine-tuning with consistent training objectives. In
this paper, we identify the challenges for graph prompt tuning: The first is
the lack of a strong and universal pre-training task across sundry pre-training
methods in graph domain. The second challenge lies in the difficulty of
designing a consistent training objective for both pre-training and downstream
tasks. To overcome above obstacles, we propose a novel framework named SGL-PT
which follows the learning strategy ``Pre-train, Prompt, and Predict''.
Specifically, we raise a strong and universal pre-training task coined as SGL
that acquires the complementary merits of generative and contrastive
self-supervised graph learning. And aiming for graph classification task, we
unify pre-training and fine-tuning by designing a novel verbalizer-free
prompting function, which reformulates the downstream task in a similar format
as pretext task. Empirical results show that our method surpasses other
baselines under unsupervised setting, and our prompt tuning method can greatly
facilitate models on biological datasets over fine-tuning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12461">
<div class="article-summary-box-inner">
<span><p>Recent advancements in interpretability research made transformer language
models more transparent. This progress led to a better understanding of their
inner workings for toy and naturally occurring models. However, how these
models internally process sentiment changes has yet to be sufficiently
answered. In this work, we introduce a new interpretability tool called PCP
ablation, where we replace modules with low-rank matrices based on the
principal components of their activations, reducing model parameters and their
behavior to essentials. We demonstrate PCP ablations on MLP and attention
layers in backdoored toy, backdoored large, and naturally occurring models. We
determine MLPs as most important for the backdoor mechanism and use this
knowledge to remove, insert, and modify backdoor mechanisms with engineered
replacements via PCP ablation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Table-to-Text Generation with Prompt-based Adapter. (arXiv:2302.12468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12468">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the topological gap between tabular
data and text and the lack of domain-specific knowledge make it difficult for
PLMs to produce faithful text, especially in real-world applications with
limited resources. In this paper, we mitigate the above challenges by
introducing a novel augmentation method: Prompt-based Adapter (PA), which
targets table-to-text generation under few-shot conditions. The core insight
design of the PA is to inject prompt templates for augmenting domain-specific
knowledge and table-related representations into the model for bridging the
structural gap between tabular data and descriptions through adapters. Such
prompt-based knowledge augmentation method brings at least two benefits: (1)
enables us to fully use the large amounts of unlabelled domain-specific
knowledge, which can alleviate the PLMs' inherent shortcomings of lacking
domain knowledge; (2) allows us to design different types of tasks supporting
the generative challenge. Extensive experiments and analyses are conducted on
three open-domain few-shot NLG datasets: Humans, Books, and Songs. Compared to
previous state-of-the-art approaches, our model achieves superior performance
in terms of both fluency and accuracy as judged by human and automatic
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Sentence Similarity Estimation for Unsupervised Extractive Summarization. (arXiv:2302.12490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12490">
<div class="article-summary-box-inner">
<span><p>Unsupervised extractive summarization aims to extract salient sentences from
a document as the summary without labeled data. Recent literatures mostly
research how to leverage sentence similarity to rank sentences in the order of
salience. However, sentence similarity estimation using pre-trained language
models mostly takes little account of document-level information and has a weak
correlation with sentence salience ranking. In this paper, we proposed two
novel strategies to improve sentence similarity estimation for unsupervised
extractive summarization. We use contrastive learning to optimize a
document-level objective that sentences from the same document are more similar
than those from different documents. Moreover, we use mutual learning to
enhance the relationship between sentence similarity estimation and sentence
salience ranking, where an extra signal amplifier is used to refine the pivotal
information. Experimental results demonstrate the effectiveness of our
strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph Question Answering. (arXiv:2302.12529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12529">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs (KGs) have received increasing attention due to its wide
applications on natural language processing. However, its use case on temporal
question answering (QA) has not been well-explored. Most of existing methods
are developed based on pre-trained language models, which might not be capable
to learn \emph{temporal-specific} presentations of entities in terms of
temporal KGQA task. To alleviate this problem, we propose a novel
\textbf{T}ime-aware \textbf{M}ultiway \textbf{A}daptive (\textbf{TMA}) fusion
network. Inspired by the step-by-step reasoning behavior of humans. For each
given question, TMA first extracts the relevant concepts from the KG, and then
feeds them into a multiway adaptive module to produce a
\emph{temporal-specific} representation of the question. This representation
can be incorporated with the pre-trained KG embedding to generate the final
prediction. Empirical results verify that the proposed model achieves better
performance than the state-of-the-art models in the benchmark dataset. Notably,
the Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex
questions are absolutely improved by 24\% and 10\% compared to the
best-performing baseline. Furthermore, we also show that TMA employing an
adaptive fusion mechanism can provide interpretability by analyzing the
proportion of information in question representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts. (arXiv:2302.12530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12530">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models have achieved great improvements in
semantic matching. However, existing models still suffer from insufficient
ability to capture subtle differences. The modification, addition and deletion
of words in sentence pairs may make it difficult for the model to predict their
relationship. To alleviate this problem, we propose a novel Dual Path Modeling
Framework to enhance the model's ability to perceive subtle differences in
sentence pairs by separately modeling affinity and difference semantics. Based
on dual-path modeling framework we design the Dual Path Modeling Network
(DPM-Net) to recognize semantic relations. And we conduct extensive experiments
on 10 well-studied semantic matching and robustness test datasets, and the
experimental results show that our proposed method achieves consistent
improvements over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Temporal Reasoning for Evidence-Based Fact-Checking. (arXiv:2302.12569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12569">
<div class="article-summary-box-inner">
<span><p>Leveraging contextual knowledge has become standard practice in automated
claim verification, yet the impact of temporal reasoning has been largely
overlooked. Our study demonstrates that time positively influences the claim
verification process of evidence-based fact-checking. The temporal aspects and
relations between claims and evidence are first established through grounding
on shared timelines, which are constructed using publication dates and time
expressions extracted from their text. Temporal information is then provided to
RNN-based and Transformer-based classifiers before or after claim and evidence
encoding. Our time-aware fact-checking models surpass base models by up to 9%
Micro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They also
outperform prior methods that explicitly model temporal relations between
evidence. Our findings show that the presence of temporal information and the
manner in which timelines are constructed greatly influence how fact-checking
models determine the relevance and supporting or refuting character of evidence
documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness in Language Models Beyond English: Gaps and Challenges. (arXiv:2302.12578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12578">
<div class="article-summary-box-inner">
<span><p>With language models becoming increasingly ubiquitous, it has become
essential to address their inequitable treatment of diverse demographic groups
and factors. Most research on evaluating and mitigating fairness harms has been
concentrated on English, while multilingual models and non-English languages
have received comparatively little attention. In this paper, we survey
different aspects of fairness in languages beyond English and multilingual
contexts. This paper presents a survey of fairness in multilingual and
non-English contexts, highlighting the shortcomings of current research and the
difficulties faced by methods designed for English. We contend that the
multitude of diverse cultures and languages across the world makes it
infeasible to achieve comprehensive coverage in terms of constructing fairness
datasets. Thus, the measurement and mitigation of biases must evolve beyond the
current dataset-driven practices that are narrowly focused on specific
dimensions and types of biases and, therefore, impossible to scale across
languages and cultures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining. (arXiv:2302.12584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12584">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe VivesDebate-Speech, a corpus of spoken
argumentation created to leverage audio features for argument mining tasks. The
creation of this corpus represents an important contribution to the
intersection of speech processing and argument mining communities, and one of
the most complete publicly available resources in this topic. Moreover, we have
performed a set of first-of-their-kind experiments which show an improvement
when integrating audio features into the argument mining pipeline. The provided
results can be used as a baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CARE: Collaborative AI-Assisted Reading Environment. (arXiv:2302.12611v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12611">
<div class="article-summary-box-inner">
<span><p>Recent years have seen impressive progress in AI-assisted writing, yet the
developments in AI-assisted reading are lacking. We propose inline commentary
as a natural vehicle for AI-based reading assistance, and present CARE: the
first open integrated platform for the study of inline commentary and reading.
CARE facilitates data collection for inline commentaries in a commonplace
collaborative reading environment, and provides a framework for enhancing
reading with NLP-based assistance, such as text classification, generation or
question answering. The extensible behavioral logging allows unique insights
into the reading and commenting behavior, and flexible configuration makes the
platform easy to deploy in new scenarios. To evaluate CARE in action, we apply
the platform in a user study dedicated to scholarly peer review. CARE
facilitates the data collection and study of inline commentary in NLP,
extrinsic evaluation of NLP assistance, and application prototyping. We invite
the community to explore and build upon the open source implementation of CARE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TUTORING: Instruction-Grounded Conversational Agent for Language Learners. (arXiv:2302.12623v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12623">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Tutoring bot, a generative chatbot trained on a
large scale of tutor-student conversations for English-language learning. To
mimic a human tutor's behavior in language education, the tutor bot leverages
diverse educational instructions and grounds to each instruction as additional
input context for the tutor response generation. As a single instruction
generally involves multiple dialogue turns to give the student sufficient
speaking practice, the tutor bot is required to monitor and capture when the
current instruction should be kept or switched to the next instruction. For
that, the tutor bot is learned to not only generate responses but also infer
its teaching action and progress on the current conversation simultaneously by
a multi-task learning scheme. Our Tutoring bot is deployed under a
non-commercial use license at https://tutoringai.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Depth Look at Word Filling Societal Bias Measures. (arXiv:2302.12640v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12640">
<div class="article-summary-box-inner">
<span><p>Many measures of societal bias in language models have been proposed in
recent years. A popular approach is to use a set of word filling prompts to
evaluate the behavior of the language models. In this work, we analyze the
validity of two such measures -- StereoSet and CrowS-Pairs. We show that these
measures produce unexpected and illogical results when appropriate control
group samples are constructed. Based on this, we believe that they are
problematic and using them in the future should be reconsidered. We propose a
way forward with an improved testing protocol. Finally, we also introduce a new
gender bias dataset for Slovak.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modelling Temporal Document Sequences for Clinical ICD Coding. (arXiv:2302.12666v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12666">
<div class="article-summary-box-inner">
<span><p>Past studies on the ICD coding problem focus on predicting clinical codes
primarily based on the discharge summary. This covers only a small fraction of
the notes generated during each hospital stay and leaves potential for
improving performance by analysing all the available clinical notes. We propose
a hierarchical transformer architecture that uses text across the entire
sequence of clinical notes in each hospital stay for ICD coding, and
incorporates embeddings for text metadata such as their position, time, and
type of note. While using all clinical notes increases the quantity of data
substantially, superconvergence can be used to reduce training costs. We
evaluate the model on the MIMIC-III dataset. Our model exceeds the prior
state-of-the-art when using only discharge summaries as input, and achieves
further performance improvements when all clinical notes are used as input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy. (arXiv:2302.12692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12692">
<div class="article-summary-box-inner">
<span><p>Clinical prediction is an essential task in the healthcare industry. However,
the recent success of transformers, on which large language models are built,
has not been extended to this domain. In this research, we explore the use of
transformers and language models in prognostic prediction for immunotherapy
using real-world patients' clinical data and molecular profiles. This paper
investigates the potential of transformers to improve clinical prediction
compared to conventional machine learning approaches and addresses the
challenge of few-shot learning in predicting rare disease areas. The study
benchmarks the efficacy of baselines and language models on prognostic
prediction across multiple cancer types and investigates the impact of
different pretrained language models under few-shot regimes. The results
demonstrate significant improvements in accuracy and highlight the potential of
NLP in clinical research to improve early detection and intervention for
different diseases. Anonymous codes are available at
\url{https://anonymous.4open.science/r/table2text-88ED}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Transfer of Cognitive Processing Complexity. (arXiv:2302.12695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12695">
<div class="article-summary-box-inner">
<span><p>When humans read a text, their eye movements are influenced by the structural
complexity of the input sentences. This cognitive phenomenon holds across
languages and recent studies indicate that multilingual language models utilize
structural similarities between languages to facilitate cross-lingual transfer.
We use sentence-level eye-tracking patterns as a cognitive indicator for
structural complexity and show that the multilingual model XLM-RoBERTa can
successfully predict varied patterns for 13 typologically diverse languages,
despite being fine-tuned only on English data. We quantify the sensitivity of
the model to structural complexity and distinguish a range of complexity
characteristics. Our results indicate that the model develops a meaningful bias
towards sentence length but also integrates cross-lingual differences. We
conduct a control experiment with randomized word order and find that the model
seems to additionally capture more complex structural information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spanish Built Factual Freectianary (Spanish-BFF): the first IA-generated free dictionary. (arXiv:2302.12746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12746">
<div class="article-summary-box-inner">
<span><p>Dictionaries are one of the oldest and most used linguistic resources.
Building them is a complex task that, to the best of our knowledge, has yet to
be explored with generative Large Language Models (LLMs). We introduce the
"Spanish Built Factual Freectianary" (Spanish-BFF) as the first Spanish
IA-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We
also define future steps we aim to follow to improve this initial commitment to
the field, such as more additional languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble knowledge distillation of self-supervised speech models. (arXiv:2302.12757v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12757">
<div class="article-summary-box-inner">
<span><p>Distilled self-supervised models have shown competitive performance and
efficiency in recent years. However, there is a lack of experience in jointly
distilling multiple self-supervised speech models. In our work, we performed
Ensemble Knowledge Distillation (EKD) on various self-supervised speech models
such as HuBERT, RobustHuBERT, and WavLM. We tried two different aggregation
techniques, layerwise-average and layerwise-concatenation, to the
representations of different teacher models and found that the former was more
effective. On top of that, we proposed a multiple prediction head method for
student models to predict different layer outputs of multiple teacher models
simultaneously. The experimental results show that our method improves the
performance of the distilled models on four downstream speech processing tasks,
Phoneme Recognition, Speaker Identification, Emotion Recognition, and Automatic
Speech Recognition in the hidden-set track of the SUPERB benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Driven Representation Learning for Robotics. (arXiv:2302.12766v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12766">
<div class="article-summary-box-inner">
<span><p>Recent work in visual representation learning for robotics demonstrates the
viability of learning from large video datasets of humans performing everyday
tasks. Leveraging methods such as masked autoencoding and contrastive learning,
these representations exhibit strong transfer to policy learning for visuomotor
control. But, robot learning encompasses a diverse set of problems beyond
control including grasp affordance prediction, language-conditioned imitation
learning, and intent scoring for human-robot collaboration, amongst others.
First, we demonstrate that existing representations yield inconsistent results
across these tasks: masked autoencoding approaches pick up on low-level spatial
features at the cost of high-level semantics, while contrastive learning
approaches capture the opposite. We then introduce Voltron, a framework for
language-driven representation learning from human videos and associated
captions. Voltron trades off language-conditioned visual reconstruction to
learn low-level visual patterns, and visually-grounded language generation to
encode high-level semantics. We also construct a new evaluation suite spanning
five distinct robot learning problems $\unicode{x2013}$ a unified platform for
holistically evaluating visual representations for robotics. Through
comprehensive, controlled experiments across all five problems, we find that
Voltron's language-driven representations outperform the prior
state-of-the-art, especially on targeted problems requiring higher-level
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STA: Self-controlled Text Augmentation for Improving Text Classifications. (arXiv:2302.12784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12784">
<div class="article-summary-box-inner">
<span><p>Despite recent advancements in Machine Learning, many tasks still involve
working in low-data regimes which can make solving natural language problems
difficult. Recently, a number of text augmentation techniques have emerged in
the field of Natural Language Processing (NLP) which can enrich the training
data with new examples, though they are not without their caveats. For
instance, simple rule-based heuristic methods are effective, but lack variation
in semantic content and syntactic structure with respect to the original text.
On the other hand, more complex deep learning approaches can cause extreme
shifts in the intrinsic meaning of the text and introduce unwanted noise into
the training data. To more reliably control the quality of the augmented
examples, we introduce a state-of-the-art approach for Self-Controlled Text
Augmentation (STA). Our approach tightly controls the generation process by
introducing a self-checking procedure to ensure that generated examples retain
the semantic content of the original text. Experimental results on multiple
benchmarking datasets demonstrate that STA substantially outperforms existing
state-of-the-art techniques, whilst qualitative analysis reveals that the
generated examples are both lexically diverse and semantically reliable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HULAT at SemEval-2023 Task 9: Data augmentation for pre-trained transformers applied to Multilingual Tweet Intimacy Analysis. (arXiv:2302.12794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12794">
<div class="article-summary-box-inner">
<span><p>This paper describes our participation in SemEval-2023 Task 9, Intimacy
Analysis of Multilingual Tweets. We fine-tune some of the most popular
transformer models with the training dataset and synthetic data generated by
different data augmentation techniques. During the development phase, our best
results were obtained by using XLM-T. Data augmentation techniques provide a
very slight improvement in the results. Our system ranked in the 27th position
out of the 45 participating systems. Despite its modest results, our system
shows promising results in languages such as Portuguese, English, and Dutch.
All our code is available in the repository
\url{https://github.com/isegura/hulat_intimacy}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. (arXiv:2302.12813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12813">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as ChatGPT, are able to generate
human-like, fluent responses for many downstream tasks, e.g., task-oriented
dialog and question answering. However, applying LLMs to real-world,
mission-critical applications remains challenging mainly due to their tendency
to generate hallucinations and inability to use external knowledge.This paper
proposes a LLM-Augmenter system, which augments a black-box LLM with a set of
plug-and-play modules. Our system makes the LLM generate responses grounded in
consolidated external knowledge, e.g., stored in task-specific databases. It
also iteratively revises LLM prompts to improve model responses using feedback
generated by utility functions, e.g., the factuality score of a LLM-generated
response. The effectiveness of LLM-Augmenter is empirically validated on two
types of mission-critical scenarios, task-oriented dialog and open-domain
question answering. LLM-Augmenter significantly reduces ChatGPT's
hallucinations without sacrificing the fluency and informativeness of its
responses. We make the source code and models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. (arXiv:2302.12822v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12822">
<div class="article-summary-box-inner">
<span><p>Chain-of-thought prompting (CoT) advances the reasoning abilities of large
language models (LLMs) and achieves superior performance in arithmetic,
commonsense, and symbolic reasoning tasks. However, most CoT studies rely on
carefully designed human-annotated rational chains to prompt the language
model, which poses challenges for real-world applications where labeled
training data is available without human-annotated rational chains. This
creates barriers to applications of CoT prompting to these general tasks. This
paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and
Selection with Chain-of-Thought), that can bypass human engineering of CoTs by
automatically augmenting rational chains from a small labeled dataset, and then
pruning low-quality chains to construct a candidate pool of machine-generated
rationale chains based on the labels. Finally, it selects the optimal
combination of several rationale chains from the pool for CoT prompting by
employing a variance-reduced policy gradient strategy to estimate the
significance of each example in a black-box language model. Automate-CoT
enables a quick adaptation of the CoT technique to different tasks.
Experimental results demonstrate the effectiveness of our method, where
state-of-the-art results are achieved on arithmetic reasoning (+2.7\%),
commonsense reasoning (+3.4\%), symbolic reasoning (+3.2\%), and non-reasoning
tasks (+2.5\%). Our code will be available at
https://github.com/shizhediao/automate-cot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Massively Multilingual ASR With Auxiliary CTC Objectives. (arXiv:2302.12829v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12829">
<div class="article-summary-box-inner">
<span><p>Multilingual Automatic Speech Recognition (ASR) models have extended the
usability of speech technologies to a wide variety of languages. With how many
languages these models have to handle, however, a key to understanding their
imbalanced performance across different languages is to examine if the model
actually knows which language it should transcribe. In this paper, we introduce
our work on improving performance on FLEURS, a 102-language open ASR benchmark,
by conditioning the entire model on language identity (LID). We investigate
techniques inspired from recent Connectionist Temporal Classification (CTC)
studies to help the model handle the large number of languages, conditioning on
the LID predictions of auxiliary tasks. Our experimental results demonstrate
the effectiveness of our technique over standard CTC/Attention-based hybrid
mod- els. Furthermore, our state-of-the-art systems using self-supervised
models with the Conformer architecture improve over the results of prior work
on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are
available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Graphical Conventions in a Visual Communication Game. (arXiv:2111.14210v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14210">
<div class="article-summary-box-inner">
<span><p>Humans communicate with graphical sketches apart from symbolic languages.
Primarily focusing on the latter, recent studies of emergent communication
overlook the sketches; they do not account for the evolution process through
which symbolic sign systems emerge in the trade-off between iconicity and
symbolicity. In this work, we take the very first step to model and simulate
this process via two neural agents playing a visual communication game; the
sender communicates with the receiver by sketching on a canvas. We devise a
novel reinforcement learning method such that agents are evolved jointly
towards successful communication and abstract graphical conventions. To inspect
the emerged conventions, we define three fundamental properties -- iconicity,
symbolicity, and semanticity -- and design evaluation methods accordingly. Our
experimental results under different controls are consistent with the
observation in studies of human graphical conventions. Of note, we find that
evolved sketches can preserve the continuum of semantics under proper
environmental pressures. More interestingly, co-evolved agents can switch
between conventionalized and iconic communication based on their familiarity
with referents. We hope the present research can pave the path for studying
emergent communication with the modality of sketches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Dynamic Neural Networks for Natural Language Processing. (arXiv:2202.07101v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07101">
<div class="article-summary-box-inner">
<span><p>Effectively scaling large Transformer models is a main driver of recent
advances in natural language processing. Dynamic neural networks, as an
emerging research direction, are capable of scaling up neural networks with
sub-linear increases in computation and time by dynamically adjusting their
computational path based on the input. Dynamic neural networks could be a
promising solution to the growing parameter numbers of pretrained language
models, allowing both model pretraining with trillions of parameters and faster
inference on mobile devices. In this survey, we summarize progress of three
types of dynamic neural networks in NLP: skimming, mixture of experts, and
early exit. We also highlight current challenges in dynamic neural networks and
directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08562">
<div class="article-summary-box-inner">
<span><p>In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary
attribute-value descriptions, which is considered more comprehensive and
specific than a triple-based fact. However, currently available
hyper-relational KG embedding methods in a single view are limited in
application because they weaken the hierarchical structure that represents the
affiliation between entities. To overcome this limitation, we propose a
dual-view hyper-relational KG structure (DH-KG) that contains a
hyper-relational instance view for entities and a hyper-relational ontology
view for concepts that are abstracted hierarchically from the entities. This
paper defines link prediction and entity typing tasks on DH-KG for the first
time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and
HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding
model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms
baseline models on DH-KG, according to experimental results. Finally, we
provide an example of how this technology can be used to treat hypertension.
Our model and new datasets are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing through Forgetting -- Domain Generalization for Symptom Event Extraction in Clinical Notes. (arXiv:2209.09485v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.09485">
<div class="article-summary-box-inner">
<span><p>Symptom information is primarily documented in free-text clinical notes and
is not directly accessible for downstream applications. To address this
challenge, information extraction approaches that can handle clinical language
variation across different institutions and specialties are needed. In this
paper, we present domain generalization for symptom extraction using
pretraining and fine-tuning data that differs from the target domain in terms
of institution and/or specialty and patient population. We extract symptom
events using a transformer-based joint entity and relation extraction method.
To reduce reliance on domain-specific features, we propose a domain
generalization method that dynamically masks frequent symptoms words in the
source domain. Additionally, we pretrain the transformer language model (LM) on
task-related unlabeled texts for better representation. Our experiments
indicate that masking and adaptive pretraining methods can significantly
improve performance when the source domain is more distant from the target
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Temporal Article Grounding. (arXiv:2210.12444v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12444">
<div class="article-summary-box-inner">
<span><p>Given a long untrimmed video and natural language queries, video grounding
(VG) aims to temporally localize the semantically-aligned video segments.
Almost all existing VG work holds two simple but unrealistic assumptions: 1)
All query sentences can be grounded in the corresponding video. 2) All query
sentences for the same video are always at the same semantic scale.
Unfortunately, both assumptions make today's VG models fail to work in
practice. For example, in real-world multimodal assets (eg, news articles),
most of the sentences in the article can not be grounded in their affiliated
videos, and they typically have rich hierarchical relations (ie, at different
semantic scales). To this end, we propose a new challenging grounding task:
Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an
article and a relevant video, WSAG aims to localize all ``groundable''
sentences to the video, and these sentences are possibly at different semantic
scales. Accordingly, we collect the first WSAG dataset to facilitate this task:
YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow
articles and plentiful YouTube videos. In addition, we propose a simple but
effective method DualMIL for WSAG, which consists of a two-level MIL loss and a
single-/cross- sentence constraint loss. These training objectives are
carefully designed for these relaxed assumptions. Extensive ablations have
verified the effectiveness of DualMIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentBS: Sentence-level Beam Search for Controllable Summarization. (arXiv:2210.14502v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14502">
<div class="article-summary-box-inner">
<span><p>A wide range of control perspectives have been explored in controllable text
generation. Structure-controlled summarization is recently proposed as a useful
and interesting research direction. However, current structure-controlling
methods have limited effectiveness in enforcing the desired structure. To
address this limitation, we propose a sentence-level beam search generation
method (SentBS), where evaluation is conducted throughout the generation
process to select suitable sentences for subsequent generations. We experiment
with different combinations of decoding methods to be used as subcomponents by
SentBS and evaluate results on the structure-controlled dataset MReD.
Experiments show that all explored combinations for SentBS can improve the
agreement between the generated text and the desired structure, with the best
method significantly reducing the structural discrepancies suffered by the
existing model, by approximately 68%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16433">
<div class="article-summary-box-inner">
<span><p>Fully-parametric language models generally require a huge number of model
parameters to store the necessary knowledge for solving multiple natural
language tasks in zero/few-shot settings. In addition, it is hard to adapt to
the evolving world knowledge without the costly model re-training. In this
paper, we develop a novel semi-parametric language model architecture,
Knowledge-in-Context (KiC), which empowers a parametric text-to-text language
model with a knowledge-rich external memory. Specifically, the external memory
contains six different types of knowledge: entity, dictionary, commonsense,
event, script, and causality knowledge. For each input instance, the KiC model
adaptively selects a knowledge type and retrieves the most helpful pieces of
knowledge. The input instance along with its knowledge augmentation is fed into
a text-to-text model (e.g., T5) to generate the output answer, where both the
input and the output are in natural language forms after prompting.
Interestingly, we find that KiC can be identified as a special
mixture-of-experts (MoE) model, where the knowledge selector plays the role of
a router that is used to determine the sequence-to-expert assignment in MoE.
This key observation inspires us to develop a novel algorithm for training KiC
with an instance-adaptive knowledge selector. As a knowledge-rich
semi-parametric language model, KiC only needs a much smaller parametric part
to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+
different tasks, we show that KiC_Large with 770M parameters easily outperforms
large language models (LMs) that are 4-39x larger by a large margin. We also
demonstrate that KiC exhibits emergent abilities at a much smaller model scale
compared to the fully-parametric models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing and Adversarial: Improve ASR with Speaker Labels. (arXiv:2211.06369v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06369">
<div class="article-summary-box-inner">
<span><p>ASR can be improved by multi-task learning (MTL) with domain enhancing or
domain adversarial training, which are two opposite objectives with the aim to
increase/decrease domain variance towards domain-aware/agnostic ASR,
respectively. In this work, we study how to best apply these two opposite
objectives with speaker labels to improve conformer-based ASR. We also propose
a novel adaptive gradient reversal layer for stable and effective adversarial
training without tuning effort. Detailed analysis and experimental verification
are conducted to show the optimal positions in the ASR neural network (NN) to
apply speaker enhancing and adversarial training. We also explore their
combination for further improvement, achieving the same performance as
i-vectors plus adversarial training. Our best speaker-based MTL achieves 7\%
relative improvement on the Switchboard Hub5'00 set. We also investigate the
effect of such speaker-based MTL w.r.t. cleaner dataset and weaker ASR NN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of a Thermodynamics of Human Cognition and Human Culture. (arXiv:2212.12795v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12795">
<div class="article-summary-box-inner">
<span><p>Inspired by foundational studies in classical and quantum physics, and by
information retrieval studies in quantum information theory, we prove that the
notions of 'energy' and 'entropy' can be consistently introduced in human
language and, more generally, in human culture. More explicitly, if energy is
attributed to words according to their frequency of appearance in a text, then
the ensuing energy levels are distributed non-classically, namely, they obey
Bose-Einstein, rather than Maxwell-Boltzmann, statistics, as a consequence of
the genuinely 'quantum indistinguishability' of the words that appear in the
text. Secondly, the 'quantum entanglement' due to the way meaning is carried by
a text reduces the (von Neumann) entropy of the words that appear in the text,
a behaviour which cannot be explained within classical (thermodynamic or
information) entropy. We claim here that this 'quantum-type behaviour is valid
in general in human language', namely, any text is conceptually more concrete
than the words composing it, which entails that the entropy of the overall text
decreases. In addition, we provide examples taken from cognition, where
quantization of energy appears in categorical perception, and from culture,
where entities collaborate, thus 'entangle', to decrease overall entropy. We
use these findings to propose the development of a new 'non-classical
thermodynamic theory' for human cognition, which also covers broad parts of
human culture and its artefacts and bridges concepts with quantum physics
entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04415">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLM) have achieved remarkable advancement in
table-to-text generation tasks. However, the lack of labeled domain-specific
knowledge and the topology gap between tabular data and text make it difficult
for PLMs to yield faithful text. Low-resource generation likewise faces unique
challenges in this domain. Inspired by how humans descript tabular data with
prior knowledge, we suggest a new framework: PromptMize, which targets
table-to-text generation under few-shot settings. The design of our framework
consists of two aspects: a prompt planner and a knowledge adapter. The prompt
planner aims to generate a prompt signal that provides instance guidance for
PLMs to bridge the topology gap between tabular data and text. Moreover, the
knowledge adapter memorizes domain-specific knowledge from the unlabelled
corpus to supply essential information during generation. Extensive experiments
and analyses are investigated on three open domain few-shot NLG datasets:
human, song, and book. Compared with previous state-of-the-art approaches, our
model achieves remarkable performance in generating quality as judged by human
and automatic evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld. (arXiv:2302.05244v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05244">
<div class="article-summary-box-inner">
<span><p>Building open-ended agents that can autonomously discover a diversity of
behaviours is one of the long-standing goals of artificial intelligence. This
challenge can be studied in the framework of autotelic RL agents, i.e. agents
that learn by selecting and pursuing their own goals, self-organizing a
learning curriculum. Recent work identified language as a key dimension of
autotelic learning, in particular because it enables abstract goal sampling and
guidance from social peers for hindsight relabelling. Within this perspective,
we study the following open scientific questions: What is the impact of
hindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can
the agent learn from very rare language goal examples in its experience replay?
How can multiple forms of exploration be combined, and take advantage of easier
goals as stepping stones to reach harder ones? To address these questions, we
use ScienceWorld, a textual environment with rich abstract and combinatorial
physics. We show the importance of selectivity from the social peer's feedback;
that experience replay needs to over-sample examples of rare goals; and that
following self-generated goal sequences where the agent's competence is
intermediate leads to significant improvements in final performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiency 360: Efficient Vision Transformers. (arXiv:2302.08374v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08374">
<div class="article-summary-box-inner">
<span><p>Transformers are widely used for solving tasks in natural language
processing, computer vision, speech, and music domains. In this paper, we talk
about the efficiency of transformers in terms of memory (the number of
parameters), computation cost (number of floating points operations), and
performance of models, including accuracy, the robustness of the model, and
fair \&amp; bias-free features. We mainly discuss the vision transformer for the
image classification task. Our contribution is to introduce an efficient 360
framework, which includes various aspects of the vision transformer, to make it
more efficient for industrial applications. By considering those applications,
we categorize them into multiple dimensions such as privacy, robustness,
transparency, fairness, inclusiveness, continual learning, probabilistic
models, approximation, computational complexity, and spectral complexity. We
compare various vision transformer models based on their performance, the
number of parameters, and the number of floating point operations (FLOPs) on
multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities. (arXiv:2302.11154v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11154">
<div class="article-summary-box-inner">
<span><p>Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit
strong generalization on various visual domains and tasks. However, existing
image classification benchmarks often evaluate recognition on a specific domain
(e.g., outdoor images) or a specific task (e.g., classifying plant species),
which falls short of evaluating whether pre-trained foundational models are
universal visual recognizers. To address this, we formally present the task of
Open-domain Visual Entity recognitioN (OVEN), where a model need to link an
image onto a Wikipedia entity with respect to a text query. We construct
OVEN-Wiki by re-purposing 14 existing datasets with all labels grounded onto
one single label space: Wikipedia entities. OVEN challenges models to select
among six million possible Wikipedia entities, making it a general visual
recognition benchmark with the largest number of labels. Our study on
state-of-the-art pre-trained models reveals large headroom in generalizing to
the massive-scale label space. We show that a PaLI-based auto-regressive visual
recognition model performs surprisingly well, even on Wikipedia entities that
have never been seen during fine-tuning. We also find existing pretrained
models yield different strengths: while PaLI-based models obtain higher overall
performance, CLIP-based models are better at recognizing tail entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSP2022 EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11752">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) is a challenging task of natural language
processing (NLP) and computer vision (CV), attracting significant attention
from researchers. English is a resource-rich language that has witnessed
various developments in datasets and models for visual question answering.
Visual question answering in other languages also would be developed for
resources and models. In addition, there is no multilingual dataset targeting
the visual content of a particular country with its own objects and cultural
characteristics. To address the weakness, we provide the research community
with a benchmark dataset named EVJVQA, including 33,000+ pairs of
question-answer over three languages: Vietnamese, English, and Japanese, on
approximately 5,000 images taken from Vietnam for evaluating multilingual VQA
systems or models. EVJVQA is used as a benchmark dataset for the challenge of
multilingual visual question answering at the 9th Workshop on Vietnamese
Language and Speech Processing (VLSP 2022). This task attracted 62 participant
teams from various universities and organizations. In this article, we present
details of the organization of the challenge, an overview of the methods
employed by shared-task participants, and the results. The highest performances
are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The
multilingual QA systems proposed by the top 2 teams use ViT for the pre-trained
vision model and mT5 for the pre-trained language model, a powerful pre-trained
language model based on the transformer architecture. EVJVQA is a challenging
dataset that motivates NLP and CV researchers to further explore the
multilingual models or systems for visual question answering systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12057">
<div class="article-summary-box-inner">
<span><p>We present ProsAudit, a benchmark in English to assess structural prosodic
knowledge in self-supervised learning (SSL) speech models. It consists of two
subtasks, their corresponding metrics, an evaluation dataset. In the
protosyntax task, the model must correctly identify strong versus weak prosodic
boundaries. In the lexical task, the model needs to correctly distinguish
between pauses inserted between words and within words. We also provide human
evaluation scores on this benchmark. We evaluated a series of SSL models and
found that they were all able to perform above chance on both tasks, even when
trained on an unseen language. However, non-native models performed
significantly worse than native ones on the lexical task, highlighting the
importance of lexical knowledge in this task. We also found a clear effect of
size with models trained on more data performing better in the two subtasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v2 [cs.AI] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13623">
<div class="article-summary-box-inner">
<span><p>In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-02-27 23:13:43.292612931 UTC">2023-02-27 23:13:43 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>