<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-26T01:30:00Z">12-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Lingual DALL-E Storytime. (arXiv:2212.11985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.11985">
<div class="article-summary-box-inner">
<span><p>While recent advancements in artificial intelligence (AI) language models
demonstrate cutting-edge performance when working with English texts,
equivalent models do not exist in other languages or do not reach the same
performance level. This undesired effect of AI advancements increases the gap
between access to new technology from different populations across the world.
This unsought bias mainly discriminates against individuals whose English
skills are less developed, e.g., non-English speakers children. Following
significant advancements in AI research in recent years, OpenAI has recently
presented DALL-E: a powerful tool for creating images based on English text
prompts. While DALL-E is a promising tool for many applications, its decreased
performance when given input in a different language, limits its audience and
deepens the gap between populations. An additional limitation of the current
DALL-E model is that it only allows for the creation of a few images in
response to a given input prompt, rather than a series of consecutive coherent
frames that tell a story or describe a process that changes over time. Here, we
present an easy-to-use automatic DALL-E storytelling framework that leverages
the existing DALL-E model to enable fast and coherent visualizations of
non-English songs and stories, pushing the limit of the one-step-at-a-time
option DALL-E currently offers. We show that our framework is able to
effectively visualize stories from non-English texts and portray the changes in
the plot over time. It is also able to create a narrative and maintain
interpretable changes in the description across frames. Additionally, our
framework offers users the ability to specify constraints on the story
elements, such as a specific location or context, and to maintain a consistent
style throughout the visualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. (arXiv:2212.12017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12017">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that fine-tuning large pre-trained language models on a
collection of tasks described via instructions, a.k.a. instruction-tuning,
improves their zero and few-shot generalization to unseen tasks. However, there
is a limited understanding of the performance trade-offs of different decisions
made during the instruction-tuning process. These decisions include the scale
and diversity of the instruction-tuning benchmark, different task sampling
strategies, fine-tuning with and without demonstrations, training using
specialized datasets for reasoning and dialogue, and finally, the fine-tuning
objectives themselves. In this paper, we characterize the effect of
instruction-tuning decisions on downstream task performance when scaling both
model and benchmark sizes. To this end, we create OPT-IML Bench: a large
benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated
into task categories from 8 existing benchmarks, and prepare an evaluation
framework to measure three types of model generalizations: to tasks from fully
held-out categories, to held-out tasks from seen categories, and to held-out
instances from seen tasks. Through the lens of this framework, we first present
insights about instruction-tuning decisions as applied to OPT-30B and further
exploit these insights to train OPT-IML 30B and 175B, which are
instruction-tuned versions of OPT. OPT-IML demonstrates all three
generalization abilities at both scales on four different evaluation benchmarks
with diverse tasks and input formats -- PromptSource, FLAN,
Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly
outperform OPT on all benchmarks but is also highly competitive with existing
models fine-tuned on each specific benchmark. We release OPT-IML at both
scales, together with the OPT-IML Bench evaluation framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When are Lemons Purple? The Concept Association Bias of CLIP. (arXiv:2212.12043v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12043">
<div class="article-summary-box-inner">
<span><p>Large-scale vision-language models such as CLIP have shown impressive
performance on zero-shot image classification and image-to-text retrieval.
However, such zero-shot performance of CLIP-based models does not realize in
tasks that require a finer-grained correspondence between vision and language,
such as Visual Question Answering (VQA). We investigate why this is the case,
and report an interesting phenomenon of CLIP, which we call the Concept
Association Bias (CAB), as a potential cause of the difficulty of applying CLIP
to VQA and similar tasks. CAB is especially apparent when two concepts are
present in the given image while a text prompt only contains a single concept.
In such a case, we find that CLIP tends to treat input as a bag of concepts and
attempts to fill in the other missing concept crossmodally, leading to an
unexpected zero-shot prediction. For example, when asked for the color of a
lemon in an image, CLIP predicts ``purple'' if the image contains a lemon and
an eggplant. We demonstrate the Concept Association Bias of CLIP by showing
that CLIP's zero-shot classification performance greatly suffers when there is
a strong concept association between an object (e.g. lemon) and an attribute
(e.g. its color). On the other hand, when the association between object and
attribute is weak, we do not see this phenomenon. Furthermore, we show that CAB
is significantly mitigated when we enable CLIP to learn deeper structure across
image and text embeddings by adding an additional Transformer on top of CLIP
and fine-tuning it on VQA. We find that across such fine-tuned variants of
CLIP, the strength of CAB in a model predicts how well it performs on VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the performances of ASR models on English and Spanish accents. (arXiv:2212.12048v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12048">
<div class="article-summary-box-inner">
<span><p>Speech to text models tend to be trained and evaluated against a single
target accent. This is especially true for English for which native speakers
from the United States became the main benchmark. In this work, we are going to
show how two simple methods: pre-trained embeddings and auxiliary
classification losses can improve the performance of ASR systems. We are
looking for upgrades as universal as possible and therefore we will explore
their impact on several models architectures and several languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12061">
<div class="article-summary-box-inner">
<span><p>This article presents a dataset of 10,917 news articles with hierarchical
news categories collected between January 1st 2019, and December 31st 2019. We
manually labelled the articles based on a hierarchical taxonomy with 17
first-level and 109 second-level categories. This dataset can be used to train
machine learning models for automatically classifying news articles by topic.
This dataset can be helpful for researchers working on news structuring,
classification, and predicting future events based on released news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?. (arXiv:2212.12131v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12131">
<div class="article-summary-box-inner">
<span><p>This work presents a detailed linguistic analysis into why larger
Transformer-based pre-trained language models with more parameters and lower
perplexity nonetheless yield surprisal estimates that are less predictive of
human reading times. First, regression analyses show a strictly monotonic,
positive log-linear relationship between perplexity and fit to reading times
for the more recently released five GPT-Neo variants and eight OPT variants on
two separate datasets, replicating earlier results limited to just GPT-2 (Oh et
al., 2022). Subsequently, analysis of residual errors reveals a systematic
deviation of the larger variants, such as underpredicting reading times of
named entities and making compensatory overpredictions for reading times of
function words such as modals and conjunctions. These results suggest that the
propensity of larger Transformer-based models to 'memorize' sequences during
training makes their surprisal estimates diverge from humanlike expectations,
which warrants caution in using pre-trained language models to study human
language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing. (arXiv:2212.12137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12137">
<div class="article-summary-box-inner">
<span><p>We investigate how humans perform the task of dubbing video content from one
language into another, leveraging a novel corpus of 319.57 hours of video from
54 professionally produced titles. This is the first such large-scale study we
are aware of. The results challenge a number of assumptions commonly made in
both qualitative literature on human dubbing and machine-learning literature on
automatic dubbing, arguing for the importance of vocal naturalness and
translation quality over commonly emphasized isometric (character length) and
lip-sync constraints, and for a more qualified view of the importance of
isochronic (timing) constraints. We also find substantial influence of the
source-side audio on human dubs through channels other than the words of the
translation, pointing to the need for research on ways to preserve speech
characteristics, as well as semantic transfer such as emphasis/emotion, in
automatic dubbing systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Questions by Enhancing Text Generation with Sentence Selection. (arXiv:2212.12192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12192">
<div class="article-summary-box-inner">
<span><p>We introduce an approach for the answer-aware question generation problem.
Instead of only relying on the capability of strong pre-trained language
models, we observe that the information of answers and questions can be found
in some relevant sentences in the context. Based on that, we design a model
which includes two modules: a selector and a generator. The selector forces the
model to more focus on relevant sentences regarding an answer to provide
implicit local information. The generator generates questions by implicitly
combining local information from the selector and global information from the
whole context encoded by the encoder. The model is trained jointly to take
advantage of latent interactions between the two modules. Experimental results
on two benchmark datasets show that our model is better than strong pre-trained
models for the question generation task. The code is also available
(shorturl.at/lV567).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning for Sarcasm Detection with a Pruned Dataset. (arXiv:2212.12213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12213">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a form of irony that involves saying or writing something that is
opposite or opposite to what one really means, often in a humorous or mocking
way. It is often used to mock or mock someone or something, or to be humorous
or amusing. Sarcasm is usually conveyed through tone of voice, facial
expressions, or other forms of nonverbal communication, but it can also be
indicated by the use of certain words or phrases that are typically associated
with irony or humor. Sarcasm detection is difficult because it relies on
context and non-verbal cues. It can also be culturally specific, subjective and
ambiguous. In this work, we fine-tune the RoBERTa based sarcasm detection model
presented in Abaskohi et al. [2022] to get to within 0.02 F1 of the
state-of-the-art (Hercog et al. [2022]) on the iSarcasm dataset (Oprea and
Magdy [2019]). This performance is achieved by augmenting iSarcasm with a
pruned version of the Self Annotated Reddit Corpus (SARC) (Khodak et al.
[2017]). Our pruned version is 100 times smaller than the subset of SARC used
to train the state-of-the-art model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Judgement's Premises Towards Key Points. (arXiv:2212.12238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12238">
<div class="article-summary-box-inner">
<span><p>Key Point Analysis(KPA) is a relatively new task in NLP that combines
summarization and classification by extracting argumentative key points (KPs)
for a topic from a collection of texts and categorizing their closeness to the
different arguments. In our work, we focus on the legal domain and develop
methods that identify and extract KPs from premises derived from texts of
judgments. The first method is an adaptation to an existing state-of-the-art
method, and the two others are new methods that we developed from scratch. We
present our methods and examples of their outputs, as well a comparison between
them. The full evaluation of our results is done in the matching task -- match
between the generated KPs to arguments (premises).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Customer-Service Dialog System with Semi-Supervised Learning and Coarse-to-Fine Intent Detection. (arXiv:2212.12363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12363">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialog(TOD) aims to assist users in achieving specific goals
through multi-turn conversation. Recently, good results have been obtained
based on large pre-trained models. However, the labeled-data scarcity hinders
the efficient development of TOD systems at scale. In this work, we constructed
a weakly supervised dataset based on a teacher/student paradigm that leverages
a large collection of unlabelled dialogues. Furthermore, we built a modular
dialogue system and integrated coarse-to-fine grained classification for user
intent detection. Experiments show that our method can reach the dialog goal
with a higher success rate and generate more coherent responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text classification in shipping industry using unsupervised models and Transformer based supervised models. (arXiv:2212.12407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12407">
<div class="article-summary-box-inner">
<span><p>Obtaining labelled data in a particular context could be expensive and time
consuming. Although different algorithms, including unsupervised learning,
semi-supervised learning, self-learning have been adopted, the performance of
text classification varies with context. Given the lack of labelled dataset, we
proposed a novel and simple unsupervised text classification model to classify
cargo content in international shipping industry using the Standard
International Trade Classification (SITC) codes. Our method stems from
representing words using pretrained Glove Word Embeddings and finding the most
likely label using Cosine Similarity. To compare unsupervised text
classification model with supervised classification, we also applied several
Transformer models to classify cargo content. Due to lack of training data, the
SITC numerical codes and the corresponding textual descriptions were used as
training data. A small number of manually labelled cargo content data was used
to evaluate the classification performances of the unsupervised classification
and the Transformer based supervised classification. The comparison reveals
that unsupervised classification significantly outperforms Transformer based
supervised classification even after increasing the size of the training
dataset by 30%. Lacking training data is a key bottleneck that prohibits deep
learning models (such as Transformers) from successful practical applications.
Unsupervised classification can provide an alternative efficient and effective
method to classify text when there is scarce training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alignment Entropy Regularization. (arXiv:2212.12442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12442">
<div class="article-summary-box-inner">
<span><p>Existing training criteria in automatic speech recognition(ASR) permit the
model to freely explore more than one time alignments between the feature and
label sequences. In this paper, we use entropy to measure a model's
uncertainty, i.e. how it chooses to distribute the probability mass over the
set of allowed alignments. Furthermore, we evaluate the effect of entropy
regularization in encouraging the model to distribute the probability mass only
on a smaller subset of allowed alignments. Experiments show that entropy
regularization enables a much simpler decoding method without sacrificing word
error rate, and provides better time alignment quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Natural Language Processing Framework for Migraine Reporting from Social Media. (arXiv:2212.12454v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12454">
<div class="article-summary-box-inner">
<span><p>Migraine is a high-prevalence and disabling neurological disorder. However,
information migraine management in real-world settings could be limited to
traditional health information sources. In this paper, we (i) verify that there
is substantial migraine-related chatter available on social media (Twitter and
Reddit), self-reported by migraine sufferers; (ii) develop a
platform-independent text classification system for automatically detecting
self-reported migraine-related posts, and (iii) conduct analyses of the
self-reported posts to assess the utility of social media for studying this
problem. We manually annotated 5750 Twitter posts and 302 Reddit posts. Our
system achieved an F1 score of 0.90 on Twitter and 0.93 on Reddit. Analysis of
information posted by our 'migraine cohort' revealed the presence of a plethora
of relevant information about migraine therapies and patient sentiments
associated with them. Our study forms the foundation for conducting an in-depth
analysis of migraine-related information using social media data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content Rating Classification for Fan Fiction. (arXiv:2212.12496v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12496">
<div class="article-summary-box-inner">
<span><p>Content ratings can enable audiences to determine the suitability of various
media products. With the recent advent of fan fiction, the critical issue of
fan fiction content ratings has emerged. Whether fan fiction content ratings
are done voluntarily or required by regulation, there is the need to automate
the content rating classification. The problem is to take fan fiction text and
determine the appropriate content rating. Methods for other domains, such as
online books, have been attempted though none have been applied to fan fiction.
We propose natural language processing techniques, including traditional and
deep learning methods, to automatically determine the content rating. We show
that these methods produce poor accuracy results for multi-classification. We
then demonstrate that treating the problem as a binary classification problem
produces better accuracy. Finally, we believe and provide some evidence that
the current approach of self-annotating has led to incorrect labels limiting
classification results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning. (arXiv:2212.12510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12510">
<div class="article-summary-box-inner">
<span><p>Transformer language models (TLMs) are critical for most NLP tasks, but they
are difficult to create for low-resource languages because of how much
pretraining data they require. In this work, we investigate two techniques for
training monolingual TLMs in a low-resource setting: greatly reducing TLM size,
and complementing the masked language modeling objective with two
linguistically rich supervised tasks (part-of-speech tagging and dependency
parsing). Results from 7 diverse languages indicate that our model, MicroBERT,
is able to produce marked improvements in downstream task evaluations relative
to a typical monolingual TLM pretraining approach. Specifically, we find that
monolingual MicroBERT models achieve gains of up to 18% for parser LAS and 11%
for NER F1 compared to a multilingual baseline, mBERT, while having less than
1% of its parameter count. We conclude reducing TLM parameter count and using
labeled data for pretraining low-resource TLMs can yield large quality benefits
and in some cases produce models that outperform multilingual approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Pointer Network for Multi-Representational Parsing. (arXiv:2009.09730v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09730">
<div class="article-summary-box-inner">
<span><p>We propose a transition-based approach that, by training a single model, can
efficiently parse any input sentence with both constituent and dependency
trees, supporting both continuous/projective and discontinuous/non-projective
syntactic structures. To that end, we develop a Pointer Network architecture
with two separate task-specific decoders and a common encoder, and follow a
multitask learning strategy to jointly train them. The resulting quadratic
system, not only becomes the first parser that can jointly produce both
unrestricted constituent and dependency trees from a single model, but also
proves that both syntactic formalisms can benefit from each other during
training, achieving state-of-the-art accuracies in several widely-used
benchmarks such as the continuous English and Chinese Penn Treebanks, as well
as the discontinuous German NEGRA and TIGER datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discontinuous Grammar as a Foreign Language. (arXiv:2110.10431v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10431">
<div class="article-summary-box-inner">
<span><p>In order to achieve deep natural language understanding, syntactic
constituent parsing is a vital step, highly demanded by many artificial
intelligence systems to process both text and speech. One of the most recent
proposals is the use of standard sequence-to-sequence models to perform
constituent parsing as a machine translation task, instead of applying
task-specific parsers. While they show a competitive performance, these
text-to-parse transducers are still lagging behind classic techniques in terms
of accuracy, coverage and speed. To close the gap, we here extend the framework
of sequence-to-sequence models for constituent parsing, not only by providing a
more powerful neural architecture for improving their performance, but also by
enlarging their coverage to handle the most complex syntactic phenomena:
discontinuous structures. To that end, we design several novel linearizations
that can fully produce discontinuities and, for the first time, we test a
sequence-to-sequence model on the main discontinuous benchmarks, obtaining
competitive results on par with task-specific discontinuous constituent parsers
and achieving state-of-the-art scores on the (discontinuous) English Penn
Treebank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterHT: Knowledge Graph Embeddings by Interaction between Head and Tail Entities. (arXiv:2202.04897v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04897">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding (KGE) models learn the representation of entities
and relations in knowledge graphs. Distance-based methods show promising
performance on link prediction task, which predicts the result by the distance
between two entity representations. However, most of these methods represent
the head entity and tail entity separately, which limits the model capacity. We
propose two novel distance-based methods named InterHT and InterHT+ that allow
the head and tail entities to interact better and get better entity
representation. Experimental results show that our proposed method achieves the
best results on ogbl-wikikg2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02364">
<div class="article-summary-box-inner">
<span><p>The need for Question Answering datasets in low resource languages is the
motivation of this research, leading to the development of Kencorpus Swahili
Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story
texts of Swahili low resource language, which is a predominantly spoken in
Eastern African and in other parts of the world. Question Answering (QA)
datasets are important for machine comprehension of natural language for tasks
such as internet search and dialog systems. Machine learning systems need
training data such as the gold standard Question Answering set developed in
this research. The research engaged annotators to formulate QA pairs from
Swahili texts collected by the Kencorpus project, a Kenyan languages corpus.
The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA
pairs each, resulting into a final dataset of 7,526 QA pairs. A quality
assurance set of 12.5% of the annotated texts confirmed that the QA pairs were
all correctly annotated. A proof of concept on applying the set to the QA task
confirmed that the dataset can be usable for such tasks. KenSwQuAD has also
contributed to resourcing of the Swahili language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Foundation Models Talk Causality?. (arXiv:2206.10591v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10591">
<div class="article-summary-box-inner">
<span><p>Foundation models are subject to an ongoing heated debate, leaving open the
question of progress towards AGI and dividing the community into two camps: the
ones who see the arguably impressive results as evidence to the scaling
hypothesis, and the others who are worried about the lack of interpretability
and reasoning capabilities. By investigating to which extent causal
representations might be captured by these large scale language models, we make
a humble efforts towards resolving the ongoing philosophical conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Interdisciplinary Topic Detection Model for Research Proposal Classification. (arXiv:2209.13519v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13519">
<div class="article-summary-box-inner">
<span><p>The peer merit review of research proposals has been the major mechanism for
deciding grant awards. However, research proposals have become increasingly
interdisciplinary. It has been a longstanding challenge to assign
interdisciplinary proposals to appropriate reviewers, so proposals are fairly
evaluated. One of the critical steps in reviewer assignment is to generate
accurate interdisciplinary topic labels for proposal-reviewer matching.
Existing systems mainly collect topic labels manually generated by principal
investigators. However, such human-reported labels can be non-accurate,
incomplete, labor intensive, and time costly. What role can AI play in
developing a fair and precise proposal reviewer assignment system? In this
study, we collaborate with the National Science Foundation of China to address
the task of automated interdisciplinary topic path detection. For this purpose,
we develop a deep Hierarchical Interdisciplinary Research Proposal
Classification Network (HIRPCN). Specifically, we first propose a hierarchical
transformer to extract the textual semantic information of proposals. We then
design an interdisciplinary graph and leverage GNNs for learning
representations of each discipline in order to extract interdisciplinary
knowledge. After extracting the semantic and interdisciplinary knowledge, we
design a level-wise prediction component to fuse the two types of knowledge
representations and detect interdisciplinary topic paths for each proposal. We
conduct extensive experiments and expert evaluations on three real-world
datasets to demonstrate the effectiveness of our proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experiments on Turkish ASR with Self-Supervised Speech Representation Learning. (arXiv:2210.07323v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07323">
<div class="article-summary-box-inner">
<span><p>While the Turkish language is listed among low-resource languages, literature
on Turkish automatic speech recognition (ASR) is relatively old. In this
report, we present our findings on Turkish ASR with speech representation
learning using HUBERT. We investigate pre-training HUBERT for Turkish with
large-scale data curated from online resources. We pre-train our model using
6,500 hours of speech data from YouTube. The results show that the models are
not ready for commercial use since they are not robust against disturbances
that typically occur in real-world settings such as variations in accents,
slang, background noise and interference. We analyze typical errors and the
limitations of the models for use in commercial settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Knowledge-Grounded Pre-training for Task-Oriented Dialog Systems. (arXiv:2210.08873v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08873">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural approaches greatly improve task-oriented dialogue
(TOD) systems which assist users to accomplish their goals. However, such
systems rely on costly manually labeled dialogs which are not available in
practical scenarios. In this paper, we present our models for Track 2 of the
SereTOD 2022 challenge, which is the first challenge of building
semi-supervised and reinforced TOD systems on a large-scale real-world Chinese
TOD dataset MobileCS. We build a knowledge-grounded dialog model to formulate
dialog history and local KB as input and predict the system response. And we
perform semi-supervised pre-training both on the labeled and unlabeled data.
Our system achieves the first place both in the automatic evaluation and human
interaction, especially with higher BLEU (+7.64) and Success (+13.6\%) than the
second place.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADEPT: A DEbiasing PrompT Framework. (arXiv:2211.05414v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05414">
<div class="article-summary-box-inner">
<span><p>Several works have proven that finetuning is an applicable approach for
debiasing contextualized word embeddings. Similarly, discrete prompts with
semantic meanings have shown to be effective in debiasing tasks. With unfixed
mathematical representation at the token level, continuous prompts usually
surpass discrete ones at providing a pre-trained language model (PLM) with
additional task-specific information. Despite this, relatively few efforts have
been made to debias PLMs by prompt tuning with continuous prompts compared to
its discrete counterpart. Furthermore, for most debiasing methods that alter a
PLM's original parameters, a major problem is the need to not only decrease the
bias in the PLM but also to ensure that the PLM does not lose its
representation ability. Finetuning methods typically have a hard time
maintaining this balance, as they tend to violently remove meanings of
attribute words. In this paper, we propose ADEPT, a method to debias PLMs using
prompt tuning while maintaining the delicate balance between removing biases
and ensuring representation ability. To achieve this, we propose a new training
criterion inspired by manifold learning and equip it with an explicit debiasing
term to optimize prompt tuning. In addition, we conduct several experiments
with regard to the reliability, quality, and quantity of a previously proposed
attribute training corpus in order to obtain a clearer prototype of a certain
attribute, which indicates the attribute's position and relative distances to
other words on the manifold. We evaluate ADEPT on several widely acknowledged
debiasing benchmarks and downstream tasks, and find that it achieves
competitive results while maintaining (and in some cases even improving) the
PLM's representation ability. We further visualize words' correlation before
and after debiasing a PLM, and give some possible explanations for the visible
effects.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-26 23:12:12.750187232 UTC">2022-12-26 23:12:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>