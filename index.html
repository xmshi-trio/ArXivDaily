<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-07T01:30:00Z">04-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02721">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence language models can be used to produce abstractive
summaries which are coherent, relevant, and concise. Still, model sizes can
make deployment in latency-sensitive or web-scale implementations difficult.
This paper studies the relationship between model size, structured pruning,
inference efficiency, and summarization accuracy on widely used summarization
datasets. We show that model accuracy is tied to the encoder size while
inference efficiency is connected to the decoder. Using asymmetric pruning can
lead to nearly 3x improvement in inference latency with ~1 point loss in
Rouge-2. Moreover, we find both the average degradation and the role of
asymmetry to be consistent across model sizes and variations in datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Challenges in Embodied Vision-Language Planning. (arXiv:2304.02738v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02738">
<div class="article-summary-box-inner">
<span><p>Recent advances in the areas of Multimodal Machine Learning and Artificial
Intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Robotics.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly leverage computer vision and natural language for
interaction in physical environments. We propose a taxonomy to unify these
tasks and provide an in-depth analysis and comparison of the current and new
algorithmic approaches, metrics, simulators, and datasets used for EVLP tasks.
Finally, we present the core challenges that we believe new EVLP works should
seek to address, and we advocate for task construction that enables model
generalisability and furthers real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks. (arXiv:2304.02739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02739">
<div class="article-summary-box-inner">
<span><p>This paper investigates the potential of semi-supervised Generative
Adversarial Networks (GANs) to fine-tune pretrained language models in order to
classify Bengali fake reviews from real reviews with a few annotated data. With
the rise of social media and e-commerce, the ability to detect fake or
deceptive reviews is becoming increasingly important in order to protect
consumers from being misled by false information. Any machine learning model
will have trouble identifying a fake review, especially for a low resource
language like Bengali. We have demonstrated that the proposed semi-supervised
GAN-LM architecture (generative adversarial network on top of a pretrained
language model) is a viable solution in classifying Bengali fake reviews as the
experimental results suggest that even with only 1024 annotated samples,
BanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and
a f1-score of 84.89% outperforming other pretrained language models -
BanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and
10% respectively in terms of accuracy. The experiments were conducted on a
manually labeled food review dataset consisting of total 6014 real and fake
reviews collected from various social media groups. Researchers that are
experiencing difficulty recognizing not just fake reviews but other
classification issues owing to a lack of labeled data may find a solution in
our proposed methodology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sejarah dan Perkembangan Teknik Natural Language Processing (NLP) Bahasa Indonesia: Tinjauan tentang sejarah, perkembangan teknologi, dan aplikasi NLP dalam bahasa Indonesia. (arXiv:2304.02746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02746">
<div class="article-summary-box-inner">
<span><p>This study provides an overview of the history of the development of Natural
Language Processing (NLP) in the context of the Indonesian language, with a
focus on the basic technologies, methods, and practical applications that have
been developed. This review covers developments in basic NLP technologies such
as stemming, part-of-speech tagging, and related methods; practical
applications in cross-language information retrieval systems, information
extraction, and sentiment analysis; and methods and techniques used in
Indonesian language NLP research, such as machine learning, statistics-based
machine translation, and conflict-based approaches. This study also explores
the application of NLP in Indonesian language industry and research and
identifies challenges and opportunities in Indonesian language NLP research and
development. Recommendations for future Indonesian language NLP research and
development include developing more efficient methods and technologies,
expanding NLP applications, increasing sustainability, further research into
the potential of NLP, and promoting interdisciplinary collaboration. It is
hoped that this review will help researchers, practitioners, and the government
to understand the development of Indonesian language NLP and identify
opportunities for further research and development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02754">
<div class="article-summary-box-inner">
<span><p>Neural network models of language have long been used as a tool for
developing hypotheses about conceptual representation in the mind and brain.
For many years, such use involved extracting vector-space representations of
words and using distances among these to predict or understand human behavior
in various semantic tasks. In contemporary language AIs, however, it is
possible to interrogate the latent structure of conceptual representations
using methods nearly identical to those commonly used with human participants.
The current work uses two common techniques borrowed from cognitive psychology
to estimate and compare lexical-semantic structure in both humans and a
well-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual
structure is robust to differences in culture, language, and method of
estimation. Structures estimated from AI behavior, while individually fairly
consistent with those estimated from human behavior, depend much more upon the
particular task used to generate behavior responses--responses generated by the
very same model in the two tasks yield estimates of conceptual structure that
cohere less with one another than do human structure estimates. The results
suggest one important way that knowledge inhering in contemporary AIs can
differ from human cognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Saudi Privacy Policy Dataset. (arXiv:2304.02757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02757">
<div class="article-summary-box-inner">
<span><p>This paper introduces the Saudi Privacy Policy Dataset, a diverse compilation
of Arabic privacy policies from various sectors in Saudi Arabia, annotated
according to the 10 principles of the Personal Data Protection Law (PDPL); the
PDPL was established to be compatible with General Data Protection Regulation
(GDPR); one of the most comprehensive data regulations worldwide. Data were
collected from multiple sources, including the Saudi Central Bank, the Saudi
Arabia National United Platform, the Council of Health Insurance, and general
websites using Google and Wikipedia. The final dataset includes 1,000 websites
belonging to 7 sectors, 4,638 lines of text, 775,370 tokens, and a corpus size
of 8,353 KB. The annotated dataset offers significant reuse potential for
assessing privacy policy compliance, benchmarking privacy practices across
industries, and developing automated tools for monitoring adherence to data
protection regulations. By providing a comprehensive and annotated dataset of
privacy policies, this paper aims to facilitate further research and
development in the areas of privacy policy analysis, natural language
processing, and machine learning applications related to privacy and data
protection, while also serving as an essential resource for researchers,
policymakers, and industry professionals interested in understanding and
promoting compliance with privacy regulations in Saudi Arabia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Transformers based methods in Electronic Medical Records: A Systematic Literature Review. (arXiv:2304.02768v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02768">
<div class="article-summary-box-inner">
<span><p>The combined growth of available data and their unstructured nature has
received increased interest in natural language processing (NLP) techniques to
make value of these data assets since this format is not suitable for
statistical analysis. This work presents a systematic literature review of
state-of-the-art advances using transformer-based methods on electronic medical
records (EMRs) in different NLP tasks. To the best of our knowledge, this work
is unique in providing a comprehensive review of research on transformer-based
methods for NLP applied to the EMR field. In the initial query, 99 articles
were selected from three public databases and filtered into 65 articles for
detailed analysis. The papers were analyzed with respect to the business
problem, NLP task, models and techniques, availability of datasets,
reproducibility of modeling, language, and exchange format. The paper presents
some limitations of current research and some recommendations for further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance of Data Augmentation Methods for Brazilian Portuguese Text Classification. (arXiv:2304.02785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02785">
<div class="article-summary-box-inner">
<span><p>Improving machine learning performance while increasing model generalization
has been a constantly pursued goal by AI researchers. Data augmentation
techniques are often used towards achieving this target, and most of its
evaluation is made using English corpora. In this work, we took advantage of
different existing data augmentation methods to analyze their performances
applied to text classification problems using Brazilian Portuguese corpora. As
a result, our analysis shows some putative improvements in using some of these
techniques; however, it also suggests further exploitation of language bias and
non-English text data scarcity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Classification of Legal Document Pages. (arXiv:2304.02787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02787">
<div class="article-summary-box-inner">
<span><p>For many business applications that require the processing, indexing, and
retrieval of professional documents such as legal briefs (in PDF format etc.),
it is often essential to classify the pages of any given document into their
corresponding types beforehand. Most existing studies in the field of document
image classification either focus on single-page documents or treat multiple
pages in a document independently. Although in recent years a few techniques
have been proposed to exploit the context information from neighboring pages to
enhance document page classification, they typically cannot be utilized with
large pre-trained language models due to the constraint on input length. In
this paper, we present a simple but effective approach that overcomes the above
limitation. Specifically, we enhance the input with extra tokens carrying
sequential information about previous pages - introducing recurrence - which
enables the usage of pre-trained Transformer models like BERT for context-aware
page classification. Our experiments conducted on two legal datasets in English
and Portuguese respectively show that the proposed approach can significantly
improve the performance of document page classification compared to the
non-recurrent setup as well as the other context-aware baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pragmatically Appropriate Diversity for Dialogue Evaluation. (arXiv:2304.02812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02812">
<div class="article-summary-box-inner">
<span><p>Linguistic pragmatics state that a conversation's underlying speech acts can
constrain the type of response which is appropriate at each turn in the
conversation. When generating dialogue responses, neural dialogue agents
struggle to produce diverse responses. Currently, dialogue diversity is
assessed using automatic metrics, but the underlying speech acts do not inform
these metrics.
</p>
<p>To remedy this, we propose the notion of Pragmatically Appropriate Diversity,
defined as the extent to which a conversation creates and constrains the
creation of multiple diverse responses. Using a human-created multi-response
dataset, we find significant support for the hypothesis that speech acts
provide a signal for the diversity of the set of next responses. Building on
this result, we propose a new human evaluation task where creative writers
predict the extent to which conversations inspire the creation of multiple
diverse responses. Our studies find that writers' judgments align with the
Pragmatically Appropriate Diversity of conversations. Our work suggests that
expectations for diversity metric scores should vary depending on the speech
act.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02819">
<div class="article-summary-box-inner">
<span><p>The rapid adoption of generative language models has brought about
substantial advancements in digital communication, while simultaneously raising
concerns regarding the potential misuse of AI-generated content. Although
numerous detection methods have been proposed to differentiate between AI and
human-generated content, the fairness and robustness of these detectors remain
underexplored. In this study, we evaluate the performance of several
widely-used GPT detectors using writing samples from native and non-native
English writers. Our findings reveal that these detectors consistently
misclassify non-native English writing samples as AI-generated, whereas native
writing samples are accurately identified. Furthermore, we demonstrate that
simple prompting strategies can not only mitigate this bias but also
effectively bypass GPT detectors, suggesting that GPT detectors may
unintentionally penalize writers with constrained linguistic expressions. Our
results call for a broader conversation about the ethical implications of
deploying ChatGPT content detectors and caution against their use in evaluative
or educational settings, particularly when they may inadvertently penalize or
exclude non-native English speakers from the global discourse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02868">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as ChatGPT and GPT-4 have recently
demonstrated their remarkable abilities of communicating with human users. In
this technical report, we take an initiative to investigate their capacities of
playing text games, in which a player has to understand the environment and
respond to situations by having dialogues with the game world. Our experiments
show that ChatGPT performs competitively compared to all the existing systems
but still exhibits a low level of intelligence. Precisely, ChatGPT can not
construct the world model by playing the game or even reading the game manual;
it may fail to leverage the world knowledge that it already has; it cannot
infer the goal of each step as the game progresses. Our results open up new
research questions at the intersection of artificial intelligence, machine
learning, and natural language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts. (arXiv:2304.02886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02886">
<div class="article-summary-box-inner">
<span><p>Automatically associating ICD codes with electronic health data is a
well-known NLP task in medical research. NLP has evolved significantly in
recent years with the emergence of pre-trained language models based on
Transformers architecture, mainly in the English language. This paper adapts
these models to automatically associate the ICD codes. Several neural network
architectures have been experimented with to address the challenges of dealing
with a large set of both input tokens and labels to be guessed. In this paper,
we propose a model that combines the latest advances in NLP and multi-label
classification for ICD-10 code association. Fair experiments on a Clinical
dataset in the French language show that our approach increases the $F_1$-score
metric by more than 55\% compared to state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affect as a proxy for literary mood. (arXiv:2304.02894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02894">
<div class="article-summary-box-inner">
<span><p>We propose to use affect as a proxy for mood in literary texts. In this
study, we explore the differences in computationally detecting tone versus
detecting mood. Methodologically we utilize affective word embeddings to look
at the affective distribution in different text segments. We also present a
simple yet efficient and effective method of enhancing emotion lexicons to take
both semantic shift and the domain of the text into account producing
real-world congruent results closely matching both contemporary and modern
qualitative analyses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpanRE: Entities and Overlapping Relations Extraction Based on Spans and Entity Attention. (arXiv:2304.02901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02901">
<div class="article-summary-box-inner">
<span><p>Extracting entities and relations is an essential task of information
extraction. Triplets extracted from a sentence might overlap with each other.
Previous methods either did not address the overlapping issues or solved
overlapping issues partially. To tackle triplet overlapping problems
completely, firstly we extract candidate subjects with a standard span
mechanism. Then we present a labeled span mechanism to extract the objects and
relations simultaneously, we use the labeled span mechanism to generate labeled
spans whose start and end positions indicate the objects, and whose labels
correspond to relations of subject and objects. Besides, we design an entity
attention mechanism to enhance the information fusion between subject and
sentence during extracting objects and relations. We test our method on two
public datasets, our method achieves the best performances on these two
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label classification of open-ended questions with BERT. (arXiv:2304.02945v1 [stat.AP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02945">
<div class="article-summary-box-inner">
<span><p>Open-ended questions in surveys are valuable because they do not constrain
the respondent's answer, thereby avoiding biases. However, answers to
open-ended questions are text data which are harder to analyze. Traditionally,
answers were manually classified as specified in the coding manual. Most of the
effort to automate coding has gone into the easier problem of single label
prediction, where answers are classified into a single code. However, open-ends
that require multi-label classification, i.e., that are assigned multiple
codes, occur frequently. This paper focuses on multi-label classification of
text answers to open-ended survey questions in social science surveys. We
evaluate the performance of the transformer-based architecture BERT for the
German language in comparison to traditional multi-label algorithms (Binary
Relevance, Label Powerset, ECC) in a German social science survey, the GLES
Panel (N=17,584, 55 labels). We find that classification with BERT (forcing at
least one label) has the smallest 0/1 loss (13.1%) among methods considered
(18.9%-21.6%). As expected, it is much easier to correctly predict answer texts
that correspond to a single label (7.1% loss) than those that correspond to
multiple labels ($\sim$50% loss). Because BERT predicts zero labels for only
1.5% of the answers, forcing at least one label, while recommended, ultimately
does not lower the 0/1 loss by much. Our work has important implications for
social scientists: 1) We have shown multi-label classification with BERT works
in the German language for open-ends. 2) For mildly multi-label classification
tasks, the loss now appears small enough to allow for fully automatic
classification (as compared to semi-automatic approaches). 3) Multi-label
classification with BERT requires only a single model. The leading competitor,
ECC, iterates through individual single label predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Social Interactions to Detect Misinformation on Social Media. (arXiv:2304.02983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02983">
<div class="article-summary-box-inner">
<span><p>Detecting misinformation threads is crucial to guarantee a healthy
environment on social media. We address the problem using the data set created
during the COVID-19 pandemic. It contains cascades of tweets discussing
information weakly labeled as reliable or unreliable, based on a previous
evaluation of the information source. The models identifying unreliable threads
usually rely on textual features. But reliability is not just what is said, but
by whom and to whom. We additionally leverage on network information. Following
the homophily principle, we hypothesize that users who interact are generally
interested in similar topics and spreading similar kind of news, which in turn
is generally reliable or not. We test several methods to learn representations
of the social interactions within the cascades, combining them with deep neural
language models in a Multi-Input (MI) framework. Keeping track of the sequence
of the interactions during the time, we improve over previous state-of-the-art
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Robot Programming: NLP integrated with autonomous robotic grasping. (arXiv:2304.02993v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02993">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a grammar-based natural language framework for
robot programming, specifically for pick-and-place tasks. Our approach uses a
custom dictionary of action words, designed to store together words that share
meaning, allowing for easy expansion of the vocabulary by adding more action
words from a lexical database. We validate our Natural Language Robot
Programming (NLRP) framework through simulation and real-world experimentation,
using a Franka Panda robotic arm equipped with a calibrated camera-in-hand and
a microphone. Participants were asked to complete a pick-and-place task using
verbal commands, which were converted into text using Google's Speech-to-Text
API and processed through the NLRP framework to obtain joint space trajectories
for the robot. Our results indicate that our approach has a high system
usability score. The framework's dictionary can be easily extended without
relying on transfer learning or large data sets. In the future, we plan to
compare the presented framework with different approaches of human-assisted
pick-and-place tasks via a comprehensive user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression of enumerations and gain. (arXiv:2304.03030v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03030">
<div class="article-summary-box-inner">
<span><p>We study the compressibility of enumerations, and its role in the relative
Kolmogorov complexity of computably enumerable sets, with respect to density.
With respect to a strong and a weak form of compression, we examine the gain:
the amount of auxiliary information embedded in the compressed enumeration.
Strong compression and weak gainless compression is shown for any computably
enumerable set, and a positional game is studied toward understanding strong
gainless compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03047">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation is a task that requires an agent to follow
instructions to navigate in environments. It becomes increasingly crucial in
the field of embodied AI, with potential applications in autonomous navigation,
search and rescue, and human-robot interaction. In this paper, we propose to
address a more practical yet challenging counterpart setting - vision-language
navigation in continuous environments (VLN-CE). To develop a robust VLN-CE
agent, we propose a new navigation framework, ETPNav, which focuses on two
critical skills: 1) the capability to abstract environments and generate
long-range navigation plans, and 2) the ability of obstacle-avoiding control in
continuous environments. ETPNav performs online topological mapping of
environments by self-organizing predicted waypoints along a traversed path,
without prior environmental experience. It privileges the agent to break down
the navigation procedure into high-level planning and low-level control.
Concurrently, ETPNav utilizes a transformer-based cross-modal planner to
generate navigation plans based on topological maps and instructions. The plan
is then performed through an obstacle-avoiding controller that leverages a
trial-and-error heuristic to prevent navigation from getting stuck in
obstacles. Experimental results demonstrate the effectiveness of the proposed
method. ETPNav yields more than 10% and 20% improvements over prior
state-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is
available at https://github.com/MarSaKi/ETPNav.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model. (arXiv:2304.03086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03086">
<div class="article-summary-box-inner">
<span><p>The ChatGPT, as a lite and conversational variant of Generative Pretrained
Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large
Language Models (LLMs) with billions of parameters. LLMs, in fact, have stirred
up a lot of interest among researchers and practitioners by their impressive
skills in natural language processing tasks, which have a profound impact on a
wide range of fields. This paper mainly discusses the future applications of
LLMs in dentistry. We introduce two primary LLM deployment methods in
dentistry, including automated dental diagnosis and cross-modal dental
diagnosis, and examine their potential applications. Especially, equipped with
a cross-modal encoder, a single LLM can manage multi-source data and conduct
advanced natural language reasoning to perform complex clinical operations. A
use case is presented to demonstrate the potential of a fully automatic
Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer
significant potential benefits, the challenges, such as data privacy, data
quality, and model bias, need further study. Overall, LLMs have the potential
to revolutionize dental diagnosis and treatment, which indicates a promising
avenue for clinical application and research in dentistry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media. (arXiv:2304.03087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03087">
<div class="article-summary-box-inner">
<span><p>Stance detection predicts attitudes towards targets in texts and has gained
attention with the rise of social media. Traditional approaches include
conventional machine learning, early deep neural networks, and pre-trained
fine-tuning models. However, with the evolution of very large pre-trained
language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face
deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not
requiring backpropagation training, has emerged as a promising alternative.
This paper examines CoT's effectiveness in stance detection tasks,
demonstrating its superior accuracy and discussing associated challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Static Fuzzy Bag-of-Words: a lightweight sentence embedding algorithm. (arXiv:2304.03098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03098">
<div class="article-summary-box-inner">
<span><p>The introduction of embedding techniques has pushed forward significantly the
Natural Language Processing field. Many of the proposed solutions have been
presented for word-level encoding; anyhow, in the last years, new mechanism to
treat information at an higher level of aggregation, like at sentence- and
document-level, have emerged. With this work we address specifically the
sentence embeddings problem, presenting the Static Fuzzy Bag-of-Word model. Our
model is a refinement of the Fuzzy Bag-of-Words approach, providing sentence
embeddings with a predefined dimension. SFBoW provides competitive performances
in Semantic Textual Similarity benchmarks, while requiring low computational
resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Robustness of Machine Reading Comprehension Models to Low Resource Entity Renaming. (arXiv:2304.03145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03145">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) models have shown compelling results in the task of
Machine Reading Comprehension (MRC). Recently these systems have proved to
perform better than humans on held-out test sets of datasets e.g. SQuAD, but
their robustness is not guaranteed. The QA model's brittleness is exposed when
evaluated on adversarial generated examples by a performance drop. In this
study, we explore the robustness of MRC models to entity renaming, with
entities from low-resource regions such as Africa. We propose EntSwap, a method
for test-time perturbations, to create a test set whose entities have been
renamed. In particular, we rename entities of type: country, person,
nationality, location, organization, and city, to create AfriSQuAD2. Using the
perturbed test set, we evaluate the robustness of three popular MRC models. We
find that compared to base models, large models perform well comparatively on
novel entities. Furthermore, our analysis indicates that entity type person
highly challenges the MRC models' performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. (arXiv:2304.03153v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03153">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved impressive zero-shot performance
in various natural language processing (NLP) tasks, demonstrating their
capabilities for inference without training examples. Despite their success, no
research has yet explored the potential of LLMs to perform next-item
recommendations in the zero-shot setting. We have identified two major
challenges that must be addressed to enable LLMs to act effectively as
recommenders. First, the recommendation space can be extremely large for LLMs,
and LLMs do not know about the target user's past interacted items and
preferences. To address this gap, we propose a prompting strategy called
Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make
next-item recommendations. Specifically, the NIR-based strategy involves using
an external module to generate candidate items based on user-filtering or
item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3
to carry subtasks that capture the user's preferences, select representative
previously watched movies, and recommend a ranked list of 10 movies. We
evaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show
that it achieves strong zero-shot performance, even outperforming some strong
sequential recommendation models trained on the entire training dataset. These
promising results highlight the ample research opportunities to use LLMs as
recommenders. The code can be found at
https://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval. (arXiv:2304.03158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03158">
<div class="article-summary-box-inner">
<span><p>Growing techniques have been emerging to improve the performance of passage
retrieval. As an effective representation bottleneck pretraining technique, the
contextual masked auto-encoder utilizes contextual embedding to assist in the
reconstruction of passages. However, it only uses a single auto-encoding
pre-task for dense representation pre-training. This study brings multi-view
modeling to the contextual masked auto-encoder. Firstly, multi-view
representation utilizes both dense and sparse vectors as multi-view
representations, aiming to capture sentence semantics from different aspects.
Moreover, multiview decoding paradigm utilizes both autoencoding and
auto-regressive decoders in representation bottleneck pre-training, aiming to
provide both reconstructive and generative signals for better contextual
representation pretraining. We refer to this multi-view pretraining method as
CoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective
and robust on large-scale passage retrieval benchmarks and out-of-domain
zero-shot benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Language Gap: Knowledge Injected Multilingual Question Answering. (arXiv:2304.03159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03159">
<div class="article-summary-box-inner">
<span><p>Question Answering (QA) is the task of automatically answering questions
posed by humans in natural languages. There are different settings to answer a
question, such as abstractive, extractive, boolean, and multiple-choice QA. As
a popular topic in natural language processing tasks, extractive question
answering task (extractive QA) has gained extensive attention in the past few
years. With the continuous evolvement of the world, generalized cross-lingual
transfer (G-XLT), where question and answer context are in different languages,
poses some unique challenges over cross-lingual transfer (XLT), where question
and answer context are in the same language. With the boost of corresponding
development of related benchmarks, many works have been done to improve the
performance of various language QA tasks. However, only a few works are
dedicated to the G-XLT task. In this work, we propose a generalized
cross-lingual transfer framework to enhance the model's ability to understand
different languages. Specifically, we first assemble triples from different
languages to form multilingual knowledge. Since the lack of knowledge between
different languages greatly limits models' reasoning ability, we further design
a knowledge injection strategy via leveraging link prediction techniques to
enrich the model storage of multilingual knowledge. In this way, we can
profoundly exploit rich semantic knowledge. Experiment results on real-world
datasets MLQA demonstrate that the proposed method can improve the performance
by a large margin, outperforming the baseline method by 13.18%/12.00% F1/EM on
average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Data Augmentation for Robust Speech Translation. (arXiv:2304.03169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03169">
<div class="article-summary-box-inner">
<span><p>Speech translation (ST) systems translate speech in one language to text in
another language. End-to-end ST systems (e2e-ST) have gained popularity over
cascade systems because of their enhanced performance due to reduced latency
and computational cost. Though resource intensive, e2e-ST systems have the
inherent ability to retain para and non-linguistic characteristics of the
speech unlike cascade systems. In this paper, we propose to use an e2e
architecture for English-Hindi (en-hi) ST. We use two imperfect machine
translation (MT) services to translate Libri-trans en text into hi text. While
each service gives MT data individually to generate parallel ST data, we
propose a data augmentation strategy of noisy MT data to aid robust ST. The
main contribution of this paper is the proposal of a data augmentation
strategy. We show that this results in better ST (BLEU score) compared to brute
force augmentation of MT data. We observed an absolute improvement of 1.59 BLEU
score with our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. (arXiv:2304.03208v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03208">
<div class="article-summary-box-inner">
<span><p>We study recent research advances that improve large language models through
efficient pre-training and scaling, and open datasets and tools. We combine
these advances to introduce Cerebras-GPT, a family of open compute-optimal
language models scaled from 111M to 13B parameters. We train Cerebras-GPT
models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules
for efficient pre-training (highest accuracy for a given compute budget). We
characterize the predictable power-law scaling and compare Cerebras-GPT with
other publicly-available models to show all Cerebras-GPT models have
state-of-the-art training efficiency on both pre-training and downstream
objectives. We describe our learnings including how Maximal Update
Parameterization ($\mu$P) can further improve large model scaling, improving
accuracy and hyperparameter predictability at scale. We release our pre-trained
models and code, making this paper the first open and reproducible work
comparing compute-optimal model scaling to models trained on fixed dataset
sizes. Cerebras-GPT models are available on HuggingFace:
https://huggingface.co/cerebras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03216">
<div class="article-summary-box-inner">
<span><p>In this work, we study how the generalization performance of a given
direction changes with its sampling ratio in Multilingual Neural Machine
Translation (MNMT). By training over 200 multilingual models with various model
sizes, directions, and total numbers of tasks, we find that scalarization leads
to a multitask trade-off front that deviates from the traditional Pareto front
when there exists data imbalance in the training corpus. That is, the
performance of certain translation directions does not improve with the
increase of its weight in the multi-task optimization objective, which poses
greater challenge to improve the overall performance of all directions. Based
on our observations, we propose the Double Power Law to predict the unique
performance trade-off front in MNMT, which is robust across various languages,
data adequacy and number of tasks. Finally, we formulate sample ratio selection
in MNMT as an optimization problem based on the Double Power Law, which
achieves better performance than temperature searching and gradient
manipulation methods using up to half of the total training budget in our
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedBot: Enhancing Privacy in Chatbots with Federated Learning. (arXiv:2304.03228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03228">
<div class="article-summary-box-inner">
<span><p>Chatbots are mainly data-driven and usually based on utterances that might be
sensitive. However, training deep learning models on shared data can violate
user privacy. Such issues have commonly existed in chatbots since their
inception. In the literature, there have been many approaches to deal with
privacy, such as differential privacy and secure multi-party computation, but
most of them need to have access to users' data. In this context, Federated
Learning (FL) aims to protect data privacy through distributed learning methods
that keep the data in its location. This paper presents Fedbot, a
proof-of-concept (POC) privacy-preserving chatbot that leverages large-scale
customer support data. The POC combines Deep Bidirectional Transformer models
and federated learning algorithms to protect customer data privacy during
collaborative model training. The results of the proof-of-concept showcase the
potential for privacy-preserving chatbots to transform the customer support
industry by delivering personalized and efficient customer service that meets
data privacy regulations and legal requirements. Furthermore, the system is
specifically designed to improve its performance and accuracy over time by
leveraging its ability to learn from previous interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models effectively leverage document-level context for literary translation, but critical errors persist. (arXiv:2304.03245v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03245">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are competitive with the state of the art on a
wide range of sentence-level translation datasets. However, their ability to
translate paragraphs and documents remains unexplored because evaluation in
these settings is costly and difficult. We show through a rigorous human
evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an
entire literary paragraph (e.g., from a novel) at once results in
higher-quality translations than standard sentence-by-sentence translation
across 18 linguistically-diverse language pairs (e.g., translating into and out
of Japanese, Polish, and English). Our evaluation, which took approximately 350
hours of effort for annotation and analysis, is conducted by hiring translators
fluent in both the source and target language and asking them to provide both
span-level error annotations as well as preference judgments of which system's
translations are better. We observe that discourse-level LLM translators commit
fewer mistranslations, grammar errors, and stylistic inconsistencies than
sentence-level approaches. With that said, critical errors still abound,
including occasional content omissions, and a human translator's intervention
remains necessary to ensure that the author's voice remains intact. We publicly
release our dataset and error annotations to spur future research on evaluation
of document-level literary translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Tuning with GPT-4. (arXiv:2304.03277v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03277">
<div class="article-summary-box-inner">
<span><p>Prior work has shown that finetuning large language models (LLMs) using
machine-generated instruction-following data enables such models to achieve
remarkable zero-shot capabilities on new tasks, and no human-written
instructions are needed. In this paper, we present the first attempt to use
GPT-4 to generate instruction-following data for LLM finetuning. Our early
experiments on instruction-tuned LLaMA models show that the 52K English and
Chinese instruction-following data generated by GPT-4 leads to superior
zero-shot performance on new tasks to the instruction-following data generated
by previous state-of-the-art models. We also collect feedback and comparison
data from GPT-4 to enable a comprehensive evaluation and reward model training.
We make our data generated using GPT-4 as well as our codebase publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03279">
<div class="article-summary-box-inner">
<span><p>Artificial agents have traditionally been trained to maximize reward, which
may incentivize power-seeking and deception, analogous to how next-token
prediction in language models (LMs) may incentivize toxicity. So do agents
naturally learn to be Machiavellian? And how do we measure these behaviors in
general-purpose models such as GPT-4? Towards answering these questions, we
introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games
containing over half a million rich, diverse scenarios that center on social
decision-making. Scenario labeling is automated with LMs, which are more
performant than human annotators. We mathematize dozens of harmful behaviors
and use our annotations to evaluate agents' tendencies to be power-seeking,
cause disutility, and commit ethical violations. We observe some tension
between maximizing reward and behaving ethically. To improve this trade-off, we
investigate LM-based methods to steer agents' towards less harmful behaviors.
Our results show that agents can both act competently and morally, so concrete
progress can currently be made in machine ethics--designing agents that are
Pareto improvements in both safety and capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Term Rewriting Based On Set Automaton Matching. (arXiv:2202.08687v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08687">
<div class="article-summary-box-inner">
<span><p>In this article we investigate how a subterm pattern matching algorithm can
be exploited to implement efficient term rewriting procedures. From the
left-hand sides of the rewrite system we construct a set automaton, which can
be used to find all redexes in a term efficiently. We formally describe a
procedure that, given a rewrite strategy, interleaves pattern matching steps
and rewriting steps and thus smoothly integrates redex discovery and subterm
replacement. We then present an efficient implementation that instantiates this
procedure with outermost rewriting, and present the results of some
experiments. Our implementation shows to be competitive with comparable tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12452">
<div class="article-summary-box-inner">
<span><p>Large Language Models have become the core architecture upon which most
modern natural language processing (NLP) systems build. These models can
consistently deliver impressive accuracy and robustness across tasks and
domains, but their high computational overhead can make inference difficult and
expensive. To make using these models less costly, recent work has explored
leveraging structured and unstructured pruning, quantization, and distillation
to improve inference speed and decrease size. This paper studies how models
pruned using Gradual Unstructured Magnitude Pruning can transfer between
domains and tasks. Our experimentation shows that models that are pruned during
pretraining using general domain masked language models can transfer to novel
domains and tasks without extensive hyperparameter exploration or specialized
approaches. We demonstrate that our general sparse model Sparse*BERT can become
SparseBioBERT simply by pretraining the compressed architecture on unstructured
biomedical text. Moreover, we show that SparseBioBERT can match the quality of
BioBERT with only 10\% of the parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization. (arXiv:2207.07278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07278">
<div class="article-summary-box-inner">
<span><p>With the prosperity of e-commerce industry, various modalities, e.g., vision
and language, are utilized to describe product items. It is an enormous
challenge to understand such diversified data, especially via extracting the
attribute-value pairs in text sequences with the aid of helpful image regions.
Although a series of previous works have been dedicated to this task, there
remain seldomly investigated obstacles that hinder further improvements: 1)
Parameters from up-stream single-modal pretraining are inadequately applied,
without proper jointly fine-tuning in a down-stream multi-modal task. 2) To
select descriptive parts of images, a simple late fusion is widely applied,
regardless of priori knowledge that language-related information should be
encoded into a common linguistic embedding space by stronger encoders. 3) Due
to diversity across products, their attribute sets tend to vary greatly, but
current approaches predict with an unnecessary maximal range and lead to more
potential false positives. To address these issues, we propose in this paper a
novel approach to boost multi-modal e-commerce attribute value extraction via
unified learning scheme and dynamic range minimization: 1) Firstly, a unified
scheme is designed to jointly train a multi-modal task with pretrained
single-modal parameters. 2) Secondly, a text-guided information range
minimization method is proposed to adaptively encode descriptive parts of each
modality into an identical space with a powerful pretrained linguistic model.
3) Moreover, a prototype-guided attribute range minimization method is proposed
to first determine the proper attribute set of the current product, and then
select prototypes to guide the prediction of the chosen attributes. Experiments
on the popular multi-modal e-commerce benchmarks show that our approach
achieves superior performance over the other state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toxicity in Multilingual Machine Translation at Scale. (arXiv:2210.03070v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03070">
<div class="article-summary-box-inner">
<span><p>Machine Translation systems can produce different types of errors, some of
which are characterized as critical or catastrophic due to the specific
negative impact that they can have on users. In this paper we focus on one type
of critical error: added toxicity. We evaluate and analyze added toxicity when
translating a large evaluation dataset (HOLISTICBIAS, over 472k sentences,
covering 13 demographic axes) from English into 164 languages. An automatic
toxicity evaluation shows that added toxicity across languages varies from 0%
to 5%. The output languages with the most added toxicity tend to be
low-resource ones, and the demographic axes with the most added toxicity
include sexual orientation, gender and sex, and ability. We also perform human
evaluation on a subset of 8 translation directions, confirming the prevalence
of true added toxicity. We use a measurement of the amount of source
contribution to the translation, where a low source contribution implies
hallucination, to interpret what causes toxicity. Making use of the input
attributions allows us to explain toxicity, because the source contributions
significantly correlate with toxicity for 84% of languages studied. Given our
findings, our recommendations to reduce added toxicity are to curate training
data to avoid mistranslations, mitigate hallucination and check unstable
translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From exemplar to copy: the scribal appropriation of a Hadewijch manuscript computationally explored. (arXiv:2210.14061v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14061">
<div class="article-summary-box-inner">
<span><p>This study is devoted to two of the oldest known manuscripts in which the
oeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,
KBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of
codicological and contextual arguments, it is assumed that the scribe who
produced B used A as an exemplar. While the similarities in both layout and
content between the two manuscripts are striking, the present article seeks to
identify the differences. After all, regardless of the intention to produce a
copy that closely follows the exemplar, subtle linguistic variation is
apparent. Divergences relate to spelling conventions, but also to the way in
which words are abbreviated (and the extent to which abbreviations occur). The
present study investigates the spelling profiles of the scribes who produced
mss. A and B in a computational way. In the first part of this study, we will
present both manuscripts in more detail, after which we will consider prior
research carried out on scribal profiling. The current study both builds and
expands on Kestemont (2015). Next, we outline the methodology used to analyse
and measure the degree of scribal appropriation that took place when ms. B was
copied off the exemplar ms. A. After this, we will discuss the results
obtained, focusing on the scribal variation that can be found both at the level
of individual words and n-grams. To this end, we use machine learning to
identify the most distinctive features that separate manuscript A from B.
Finally, we look at possible diachronic trends in the appropriation by B's
scribe of his exemplar. We argue that scribal takeovers in the exemplar impacts
the practice of the copying scribe, while transitions to a different content
matter cause little to no effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09849">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14229">
<div class="article-summary-box-inner">
<span><p>Given a document in a source language, cross-lingual summarization (CLS) aims
to generate a summary in a different target language. Recently, the emergence
of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has
attracted wide attention from the computational linguistics community. However,
it is not yet known the performance of LLMs on CLS. In this report, we
empirically use various prompts to guide LLMs to perform zero-shot CLS from
different paradigms (i.e., end-to-end and pipeline), and provide a preliminary
evaluation on the generated summaries. We find that ChatGPT and GPT-4
originally prefer to produce lengthy summaries with detailed information. These
two LLMs can further balance informativeness and conciseness with the help of
an interactive prompt, significantly improving their CLS performance.
Experimental results on three widely-used CLS datasets show that GPT-4 achieves
state-of-the-art zero-shot CLS performance, and performs competitively compared
with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and
bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited
zero-shot CLS ability. Due to the composite nature of CLS, which requires
models to perform summarization and translation simultaneously, accomplishing
this task in a zero-shot manner is even a challenge for LLMs. Therefore, we
sincerely hope and recommend future LLM research could use CLS as a testbed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13570">
<div class="article-summary-box-inner">
<span><p>This study presents a novel model for invertible sentence embeddings using a
residual recurrent network trained on an unsupervised encoding task. Rather
than the probabilistic outputs common to neural machine translation models, our
approach employs a regression-based output layer to reconstruct the input
sequence's word vectors. The model achieves high accuracy and fast training
with the ADAM optimizer, a significant finding given that RNNs typically
require memory units, such as LSTMs, or second-order optimization methods. We
incorporate residual connections and introduce a "match drop" technique, where
gradients are calculated only for incorrect words. Our approach demonstrates
potential for various natural language processing applications, particularly in
neural network-based systems that require high-quality sentence embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-oriented Memory-efficient Pruning-Adapter. (arXiv:2303.14704v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14704">
<div class="article-summary-box-inner">
<span><p>The Outstanding performance and growing size of Large Language Models has led
to increased attention in parameter efficient learning. The two predominant
approaches are Adapters and Pruning. Adapters are to freeze the model and give
it a new weight matrix on the side, which can significantly reduce the time and
memory of training, but the cost is that the evaluation and testing will
increase the time and memory consumption. Pruning is to cut off some weight and
re-distribute the remaining weight, which sacrifices the complexity of training
at the cost of extremely high memory and training time, making the cost of
evaluation and testing relatively low. So efficiency of training and inference
can't be obtained in the same time. In this work, we propose a task-oriented
Pruning-Adapter method that achieve a high memory efficiency of training and
memory, and speeds up training time and ensures no significant decrease in
accuracy in GLUE tasks, achieving training and inference efficiency at the same
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16894">
<div class="article-summary-box-inner">
<span><p>Understanding 3D scenes from multi-view inputs has been proven to alleviate
the view discrepancy issue in 3D visual grounding. However, existing methods
normally neglect the view cues embedded in the text modality and fail to weigh
the relative importance of different views. In this paper, we propose
ViewRefer, a multi-view framework for 3D visual grounding exploring how to
grasp the view knowledge from both text and 3D modalities. For the text branch,
ViewRefer leverages the diverse linguistic knowledge of large-scale language
models, e.g., GPT, to expand a single grounding text to multiple
geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer
fusion module with inter-view attention is introduced to boost the interaction
of objects across views. On top of that, we further present a set of learnable
multi-view prototypes, which memorize scene-agnostic knowledge for different
views, and enhance the framework from two perspectives: a view-guided attention
module for more robust text features, and a view-guided scoring strategy during
the final prediction. With our designed paradigm, ViewRefer achieves superior
performance on three benchmarks and surpasses the second-best by +2.8%, +1.2%,
and +0.73% on Sr3D, Nr3D, and ScanRefer. Code will be released at
https://github.com/ZiyuGuo99/ViewRefer3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RPTQ: Reorder-based Post-training Quantization for Large Language Models. (arXiv:2304.01089v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01089">
<div class="article-summary-box-inner">
<span><p>Large-scale language models (LLMs) have demonstrated outstanding performance
on various tasks, but their deployment poses challenges due to their enormous
model size. In this paper, we identify that the main challenge in quantizing
LLMs stems from the different activation ranges between the channels, rather
than just the issue of outliers.We propose a novel reorder-based quantization
approach, RPTQ, that addresses the issue of quantizing the activations of LLMs.
RPTQ rearranges the channels in the activations and then quantizing them in
clusters, thereby reducing the impact of range difference of channels. In
addition, we reduce the storage and computation overhead by avoiding explicit
reordering. By implementing this approach, we achieved a significant
breakthrough by pushing LLM models to 3 bit activation for the first time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02017">
<div class="article-summary-box-inner">
<span><p>Large language models have revolutionized the field of artificial
intelligence and have been used in various applications. Among these models,
ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,
it stands out as a powerful tool that has been widely adopted. ChatGPT has been
successfully applied in numerous areas, including chatbots, content generation,
language translation, personalized recommendations, and even medical diagnosis
and treatment. Its success in these applications can be attributed to its
ability to generate human-like responses, understand natural language, and
adapt to different contexts. Its versatility and accuracy make it a powerful
tool for natural language processing (NLP). However, there are also limitations
to ChatGPT, such as its tendency to produce biased responses and its potential
to perpetuate harmful language patterns. This article provides a comprehensive
overview of ChatGPT, its applications, advantages, and limitations.
Additionally, the paper emphasizes the importance of ethical considerations
when using this robust tool in real-world scenarios. Finally, This paper
contributes to ongoing discussions surrounding artificial intelligence and its
impact on vision and NLP domains by providing insights into prompt engineering
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02213">
<div class="article-summary-box-inner">
<span><p>This article presents a new NLP task called structured information inference
(SII) to address the complexities of information extraction at the device level
in materials science. We accomplished this task by tuning GPT-3 on an existed
perovskite solar cell FAIR(Findable, Accessible, Interoperable, Reusable)
dataset with 91.8 F1-score and we updated the dataset with all related
scientific papers up to now. The produced dataset is formatted and normalized,
enabling its direct utilization as input in subsequent data analysis. This
feature will enable materials scientists to develop their own models by
selecting high-quality review papers within their domain. Furthermore, we
designed experiments to predict solar cells' electrical performance and
reverse-predict parameters on both material gene and FAIR datesets through LLM.
We obtained comparable performance with traditional machine learning methods
without feature selection, which demonstrates the potential of large language
models to judge materials and design new materials like a materials scientist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02426">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable
abilities on a wide range of natural language processing (NLP) tasks, including
various machine translation abilities accomplished during chat. However, these
models are only accessible through restricted APIs, which creates barriers to
new research and advancements in the field. Therefore, we propose the
$\mathbf{ParroT}$ framework to enhance and regulate the translation abilities
during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written
translation and evaluation data. Specifically, ParroT reformulates translation
data into the instruction-following style, and introduces a "Hint" field for
incorporating extra requirements to regulate the translation process.
Accordingly, we propose three instruction types for finetuning ParroT models,
including translation instruction, contrastive instruction, and error-guided
instruction. Experiments on Flores subsets and WMT22 test sets suggest that
translation instruction improves the translation performance of vanilla LLMs
significantly while error-guided instruction can lead to a further improvement,
which demonstrates the importance of learning from low-quality translations
annotated by human. Meanwhile, the ParroT models can also preserve the ability
on general tasks with the Alpaca multi-task dataset involved in finetuning.
Codes: https://github.com/wxjiao/ParroT
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-09 23:10:54.739189480 UTC">2023-04-09 23:10:54 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>