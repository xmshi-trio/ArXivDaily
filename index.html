<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-21T01:30:00Z">12-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-in-the-loop Abstractive Dialogue Summarization. (arXiv:2212.09750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09750">
<div class="article-summary-box-inner">
<span><p>Abstractive dialogue summarization has received increasing attention
recently. Despite the fact that most of the current dialogue summarization
systems are trained to maximize the likelihood of human-written summaries and
have achieved significant results, there is still a huge gap in generating
high-quality summaries as determined by humans, such as coherence and
faithfulness, partly due to the misalignment in maximizing a single
human-written summary. To this end, we propose to incorporate different levels
of human feedback into the training process. This will enable us to guide the
models to capture the behaviors humans care about for summaries. Specifically,
we ask humans to highlight the salient information to be included in summaries
to provide the local feedback , and to make overall comparisons among summaries
in terms of coherence, accuracy, coverage, concise and overall quality, as the
global feedback. We then combine both local and global feedback to fine-tune
the dialog summarization policy with Reinforcement Learning. Experiments
conducted on multiple datasets demonstrate the effectiveness and generalization
of our methods over the state-of-the-art supervised baselines, especially in
terms of human judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Trajectories of Language Models Across Scales. (arXiv:2212.09803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09803">
<div class="article-summary-box-inner">
<span><p>Scaling up language models has led to unprecedented performance gains, but
little is understood about how the training dynamics change as models get
larger. How do language models of different sizes learn during pre-training?
Why do larger language models demonstrate more desirable behaviors? In this
paper, we analyze the intermediate training checkpoints of differently sized
OPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token
prediction, sequence-level generation, and downstream tasks. We find that 1) at
a given perplexity and independent of model sizes, a similar subset of training
tokens see the most significant reduction in loss, with the rest stagnating or
showing double-descent behavior; 2) early in training, all models learn to
reduce the perplexity of grammatical sequences that contain hallucinations,
with small models halting at this suboptimal distribution and larger ones
eventually learning to assign these sequences lower probabilities; 3)
perplexity is a strong predictor of in-context learning performance on 74
multiple-choice tasks from BIG-Bench, and this holds independent of the model
size. Together, these results show that perplexity is more predictive of model
behaviors than model size or training computation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09811">
<div class="article-summary-box-inner">
<span><p>Compared to conventional bilingual translation systems, massively
multilingual machine translation is appealing because a single model can
translate into multiple languages and benefit from knowledge transfer for low
resource languages. On the other hand, massively multilingual models suffer
from the curse of multilinguality, unless scaling their size massively, which
increases their training and inference costs. Sparse Mixture-of-Experts models
are a way to drastically increase model capacity without the need for a
proportional amount of computing. The recently released NLLB-200 is an example
of such a model. It covers 202 languages but requires at least four 32GB GPUs
just for inference. In this work, we propose a pruning method that allows the
removal of up to 80\% of experts with a negligible loss in translation quality,
which makes it feasible to run the model on a single 32GB GPU. Further analysis
suggests that our pruning metrics allow to identify language-specific experts
and prune non-relevant experts for a given language pair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What to Read in a Contract? Party-Specific Summarization of Important Obligations, Entitlements, and Prohibitions in Legal Documents. (arXiv:2212.09825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09825">
<div class="article-summary-box-inner">
<span><p>Legal contracts, such as employment or lease agreements, are important
documents as they govern the obligations and entitlements of the various
contracting parties. However, these documents are typically long and written in
legalese resulting in lots of manual hours spent in understanding them. In this
paper, we address the task of summarizing legal contracts for each of the
contracting parties, to enable faster reviewing and improved understanding of
them. Specifically, we collect a dataset consisting of pairwise importance
comparison annotations by legal experts for ~293K sentence pairs from lease
agreements. We propose a novel extractive summarization system to automatically
produce a summary consisting of the most important obligations, entitlements,
and prohibitions in a contract. It consists of two modules: (1) a content
categorize to identify sentences containing each of the categories (i.e.,
obligation, entitlement, and prohibition) for a party, and (2) an importance
ranker to compare the importance among sentences of each category for a party
to obtain a ranked list. The final summary is produced by selecting the most
important sentences of a category for each of the parties. We demonstrate the
effectiveness of our proposed system by comparing it against several text
ranking baselines via automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Hybrid and Ensemble Models for Multiclass Prediction of Mental Health Status on Social Media. (arXiv:2212.09839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09839">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been a surge of interest in research on automatic
mental health detection (MHD) from social media data leveraging advances in
natural language processing and machine learning techniques. While significant
progress has been achieved in this interdisciplinary research area, the vast
majority of work has treated MHD as a binary classification task. The
multiclass classification setup is, however, essential if we are to uncover the
subtle differences among the statistical patterns of language use associated
with particular mental health conditions. Here, we report on experiments aimed
at predicting six conditions (anxiety, attention deficit hyperactivity
disorder, bipolar disorder, post-traumatic stress disorder, depression, and
psychological stress) from Reddit social media posts. We explore and compare
the performance of hybrid and ensemble models leveraging transformer-based
architectures (BERT and RoBERTa) and BiLSTM neural networks trained on
within-text distributions of a diverse set of linguistic features. This set
encompasses measures of syntactic complexity, lexical sophistication and
diversity, readability, and register-specific ngram frequencies, as well as
sentiment and emotion lexicons. In addition, we conduct feature ablation
experiments to investigate which types of features are most indicative of
particular mental health conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(Psycho-)Linguistic Features Meet Transformer Models for Improved Explainable and Controllable Text Simplification. (arXiv:2212.09848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09848">
<div class="article-summary-box-inner">
<span><p>State-of-the-art text simplification (TS) systems adopt end-to-end neural
network models to directly generate the simplified version of the input text,
and usually function as a blackbox. Moreover, TS is usually treated as an
all-purpose generic task under the assumption of homogeneity, where the same
simplification is suitable for all. In recent years, however, there has been
increasing recognition of the need to adapt the simplification techniques to
the specific needs of different target groups. In this work, we aim to advance
current research on explainable and controllable TS in two ways: First,
building on recently proposed work to increase the transparency of TS systems,
we use a large set of (psycho-)linguistic features in combination with
pre-trained language models to improve explainable complexity prediction.
Second, based on the results of this preliminary task, we extend a
state-of-the-art Seq2Seq TS model, ACCESS, to enable explicit control of ten
attributes. The results of experiments show (1) that our approach improves the
performance of state-of-the-art models for predicting explainable complexity
and (2) that explicitly conditioning the Seq2Seq model on ten attributes leads
to a significant improvement in performance in both within-domain and
out-of-domain settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09849">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MANTIS at TSAR-2022 Shared Task: Improved Unsupervised Lexical Simplification with Pretrained Encoders. (arXiv:2212.09855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09855">
<div class="article-summary-box-inner">
<span><p>In this paper we present our contribution to the TSAR-2022 Shared Task on
Lexical Simplification of the EMNLP 2022 Workshop on Text Simplification,
Accessibility, and Readability. Our approach builds on and extends the
unsupervised lexical simplification system with pretrained encoders (LSBert)
system in the following ways: For the subtask of simplification candidate
selection, it utilizes a RoBERTa transformer language model and expands the
size of the generated candidate list. For subsequent substitution ranking, it
introduces a new feature weighting scheme and adopts a candidate filtering
method based on textual entailment to maximize semantic similarity between the
target word and its simplification. Our best-performing system improves LSBert
by 5.9% accuracy and achieves second place out of 33 ranked solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Semi-Supervised Nonnegative Matrix Factorization. (arXiv:2212.09858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09858">
<div class="article-summary-box-inner">
<span><p>Nonnegative matrix factorization can be used to automatically detect topics
within a corpus in an unsupervised fashion. The technique amounts to an
approximation of a nonnegative matrix as the product of two nonnegative
matrices of lower rank. In this paper, we show this factorization can be
combined with regression on a continuous response variable. In practice, the
method performs better than regression done after topics are identified and
retrains interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09864">
<div class="article-summary-box-inner">
<span><p>Pre-training is an effective technique for ensuring robust performance on a
variety of machine learning tasks. It typically depends on large-scale crawled
corpora that can result in toxic or biased models. Such data can also be
problematic with respect to copyright, attribution, and privacy. Pre-training
with synthetic tasks and data is a promising way of alleviating such concerns
since no real-world information is ingested by the model. Our goal in this
paper is to understand what makes for a good pre-trained model when using
synthetic resources. We answer this question in the context of neural machine
translation by considering two novel approaches to translation model
pre-training. Our first approach studies the effect of pre-training on
obfuscated data derived from a parallel corpus by mapping words to a vocabulary
of 'nonsense' tokens. Our second approach explores the effect of pre-training
on procedurally generated synthetic parallel data that does not depend on any
real human language corpus. Our empirical evaluation on multiple language pairs
shows that, to a surprising degree, the benefits of pre-training can be
realized even with obfuscated or purely synthetic parallel data. In our
analysis, we consider the extent to which obfuscated and synthetic pre-training
techniques can be used to mitigate the issue of hallucinated model toxicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations. (arXiv:2212.09865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09865">
<div class="article-summary-box-inner">
<span><p>Although large language models can be prompted for both zero- and few-shot
learning, performance drops significantly when no demonstrations are available.
In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap
by constructing pseudo-demonstrations for a given test input using a raw text
corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the
nearest neighbors to the test input from the corpus and pairing them with
random task labels, and (2) applying a set of techniques to reduce the amount
of direct copying the model does from the resulting demonstrations. Evaluation
on nine classification datasets shows that Z-ICL outperforms previous zero-shot
methods by a significant margin, and is on par with in-context learning with
labeled training data in the few-shot setting. Overall, Z-ICL provides a
significantly higher estimate of the zero-shot performance levels of a model,
and supports future efforts to develop better pseudo-demonstrations that
further improve zero-shot results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical Literature. (arXiv:2212.09867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09867">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic created a deluge of questionable and contradictory
scientific claims about drug efficacy -- an "infodemic" with lasting
consequences for science and society. In this work, we argue that NLP models
can help domain experts distill and understand the literature in this complex,
high-stakes area. Our task is to automatically identify contradictory claims
about COVID-19 drug efficacy. We frame this as a natural language inference
problem and offer a new NLI dataset created by domain experts. The NLI framing
allows us to create curricula combining existing datasets and our own. The
resulting models are useful investigative tools. We provide a case study of how
these models help a domain expert summarize and assess evidence concerning
remdisivir and hydroxychloroquine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models. (arXiv:2212.09873v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09873">
<div class="article-summary-box-inner">
<span><p>There is growing interest in incorporating eye-tracking data and other
implicit measures of human language processing into natural language processing
(NLP) pipelines. The data from human language processing contain unique insight
into human linguistic understanding that could be exploited by language models.
However, many unanswered questions remain about the nature of this data and how
it can best be utilized in downstream NLP tasks. In this paper, we present
eyeStyliency, an eye-tracking dataset for human processing of stylistic text
(e.g., politeness). We develop a variety of methods to derive style saliency
scores over text using the collected eye dataset. We further investigate how
this saliency data compares to both human annotation methods and model-based
interpretability metrics. We find that while eye-tracking data is unique, it
also intersects with both human annotations and model-based importance scores,
providing a possible bridge between human- and machine-based perspectives. In
downstream few-shot learning tasks, adding salient words to prompts generally
improved style classification, with eye-tracking-based and annotation-based
salient words achieving the highest accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsigned Play by Milan Kundera? An Authorship Attribution Study. (arXiv:2212.09879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09879">
<div class="article-summary-box-inner">
<span><p>In addition to being a widely recognised novelist, Milan Kundera has also
authored three pieces for theatre: The Owners of the Keys (Majitel\'e
kl\'i\v{c}\r{u}, 1961), The Blunder (Pt\'akovina, 1967), and Jacques and his
Master (Jakub a jeho p\'an, 1971). In recent years, however, the hypothesis has
been raised that Kundera is the true author of a fourth play: Juro
J\'ano\v{s}\'ik, first performed in a 1974 production under the name of Karel
Steigerwald, who was Kundera's student at the time. In this study, we make use
of supervised machine learning to settle the question of authorship attribution
in the case of Juro J\'ano\v{s}\'ik, with results strongly supporting the
hypothesis of Kundera's authorship.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking Clarification Questions for Code Generation in General-Purpose Programming Language. (arXiv:2212.09885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09885">
<div class="article-summary-box-inner">
<span><p>Code generation from text requires understanding the user's intent from a
natural language description (NLD) and generating an executable program code
snippet that satisfies this intent. While recent pretrained language models
(PLMs) demonstrate remarkable performance for this task, these models fail when
the given NLD is ambiguous due to the lack of enough specifications for
generating a high-quality code snippet. In this work, we introduce a novel and
more realistic setup for this task. We hypothesize that ambiguities in the
specifications of an NLD are resolved by asking clarification questions (CQs).
Therefore, we collect and introduce a new dataset named CodeClarQA containing
NLD-Code pairs with created CQAs. We evaluate the performance of PLMs for code
generation on our dataset. The empirical results support our hypothesis that
clarifications result in more precise generated code, as shown by an
improvement of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7\% in the exact match.
Alongside this, our task and dataset introduce new challenges to the community,
including when and what CQs should be asked.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Long-Form Spoken Language Translation with Large Language Models. (arXiv:2212.09895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09895">
<div class="article-summary-box-inner">
<span><p>A challenge in spoken language translation is that plenty of spoken content
is long-form, but short units are necessary for obtaining high-quality
translations. To address this mismatch, we fine-tune a general-purpose, large
language model to split long ASR transcripts into segments that can be
independently translated so as to maximize the overall translation quality. We
compare to several segmentation strategies and find that our approach improves
BLEU score on three languages by an average of 2.7 BLEU overall compared to an
automatic punctuation baseline. Further, we demonstrate the effectiveness of
two constrained decoding strategies to improve well-formedness of the model
output from above 99% to 100%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training. (arXiv:2212.09897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09897">
<div class="article-summary-box-inner">
<span><p>Language tasks involving character-level manipulations (e.g., spelling
correction, many word games) are challenging for models based in subword
tokenization. To address this, we adapt the interchange intervention training
method of Geiger et al. (2021) to operate on type-level variables over
characters. This allows us to encode robust, position-independent
character-level information in the internal representations of subword-based
models. We additionally introduce a suite of character-level tasks that
systematically vary in their dependence on meaning and sequence-level context.
While simple character-level tokenization approaches still perform best on
purely form-based tasks like string reversal, our method is superior for more
complex tasks that blend form, meaning, and context, such as spelling
correction in context and word search games. Our approach also leads to
subword-based models with human-intepretable internal representations of
characters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks. (arXiv:2212.09912v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09912">
<div class="article-summary-box-inner">
<span><p>Generative models have been widely applied to solve extractive tasks, where
parts of the input is extracted to form the desired output, and achieved
significant success. For example, in extractive question answering (QA),
generative models have constantly yielded state-of-the-art results. In this
work, we identify the issue of tokenization inconsistency that is commonly
neglected in training these models. This issue damages the extractive nature of
these tasks after the input and output are tokenized inconsistently by the
tokenizer, and thus leads to performance drop as well as hallucination. We
propose a simple yet effective fix to this issue and conduct a case study on
extractive QA. We show that, with consistent tokenization, the model performs
better in both in-domain and out-of-domain datasets, with a notable average of
+1.7 F2 gain when a BART model is trained on SQuAD and evaluated on 8 QA
datasets. Further, the model converges faster, and becomes less likely to
generate out-of-context answers. With these findings, we would like to call for
more attention on how tokenization should be done when solving extractive tasks
and recommend applying consistent tokenization during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse Reinforcement Learning for Text Summarization. (arXiv:2212.09917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09917">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art summarization models are trained with either maximum
likelihood estimation (MLE) or reinforcement learning (RL). In this study, we
investigate the third training paradigm and argue that inverse reinforcement
learning (IRL) may be more suitable for text summarization. IRL focuses on
estimating the reward function of an agent, given a set of observations of that
agent's behavior. Generally, IRL provides advantages in situations where the
reward function is not explicitly known or where it is difficult to define or
interact with the environment directly. These situations are exactly what we
observe in summarization. Thus, we introduce inverse reinforcement learning
into text summarization and define a suite of sub-rewards that are important
for summarization optimization. By simultaneously estimating the reward
function and optimizing the summarization agent with expert demonstrations, we
show that the model trained with IRL produces summaries that closely follow
human behavior, in terms of better ROUGE, coverage, novelty, compression ratio
and factuality when compared to the baselines trained with MLE and RL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Robustness of Summarization Models by Detecting and Removing Input Noise. (arXiv:2212.09928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09928">
<div class="article-summary-box-inner">
<span><p>The evaluation of abstractive summarization models typically uses test data
that is identically distributed as training data. In real-world practice,
documents to be summarized may contain input noise caused by text extraction
artifacts or data pipeline bugs. The robustness of model performance under
distribution shift caused by such noise is relatively under-studied. We present
a large empirical study quantifying the sometimes severe loss in performance
(up to 12 ROUGE-1 points) from different types of input noise for a range of
datasets and model sizes. We then propose a light-weight method for detecting
and removing such noise in the input during model inference without requiring
any extra training, auxiliary models, or even prior knowledge of the type of
noise. Our proposed approach effectively mitigates the loss in performance,
recovering a large fraction of the performance drop, sometimes as large as 11
ROUGE-1 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnyTOD: A Programmable Task-Oriented Dialog System. (arXiv:2212.09939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09939">
<div class="article-summary-box-inner">
<span><p>We propose AnyTOD, an end-to-end task-oriented dialog (TOD) system with
zero-shot capability for unseen tasks. We view TOD as a program executed by a
language model (LM), where program logic and ontology is provided by a designer
in the form of a schema. To enable generalization onto unseen schemas and
programs without prior training, AnyTOD adopts a neuro-symbolic approach. A
neural LM keeps track of events that occur during a conversation, and a
symbolic program implementing the dialog policy is executed to recommend next
actions AnyTOD should take. This approach drastically reduces data annotation
and model training requirements, addressing a long-standing challenge in TOD
research: rapidly adapting a TOD system to unseen tasks and domains. We
demonstrate state-of-the-art results on the STAR and ABCD benchmarks, as well
as AnyTOD's strong zero-shot transfer capability in low-resource settings. In
addition, we release STARv2, an updated version of the STAR dataset with richer
data annotations, for benchmarking zero-shot end-to-end TOD models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialog2API: Task-Oriented Dialogue with API Description and Example Programs. (arXiv:2212.09946v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09946">
<div class="article-summary-box-inner">
<span><p>Functionality and dialogue experience are two important factors of
task-oriented dialogue systems. Conventional approaches with closed schema
(e.g., conversational semantic parsing) often fail as both the functionality
and dialogue experience are strongly constrained by the underlying schema. We
introduce a new paradigm for task-oriented dialogue - Dialog2API - to greatly
expand the functionality and provide seamless dialogue experience. The
conversational model interacts with the environment by generating and executing
programs triggering a set of pre-defined APIs. The model also manages the
dialogue policy and interact with the user through generating appropriate
natural language responses. By allowing generating free-form programs,
Dialog2API supports composite goals by combining different APIs, whereas
unrestricted program revision provides natural and robust dialogue experience.
To facilitate Dialog2API, the core model is provided with API documents, an
execution environment and optionally some example dialogues annotated with
programs. We propose an approach tailored for the Dialog2API, where the
dialogue states are represented by a stack of programs, with most recently
mentioned program on the top of the stack. Dialog2API can work with many
application scenarios such as software automation and customer service. In this
paper, we construct a dataset for AWS S3 APIs and present evaluation results of
in-context learning baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Future Sight: Dynamic Story Generation with Large Pretrained Language Models. (arXiv:2212.09947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09947">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning research, such as transformers, have
bolstered the ability for automated agents to generate creative texts similar
to those that a human would write. By default, transformer decoders can only
generate new text with respect to previously generated text. The output
distribution of candidate tokens at any position is conditioned on previously
selected tokens using a self-attention mechanism to emulate the property of
autoregression. This is inherently limiting for tasks such as controllable
story generation where it may be necessary to condition on future plot events
when writing a story. In this work, we propose Future Sight, a method for
finetuning a pretrained generative transformer on the task of future
conditioning. Transformer decoders are typically pretrained on the task of
completing a context, one token at a time, by means of self-attention. Future
Sight additionally enables a decoder to attend to an encoded future plot event.
This motivates the decoder to expand on the context in a way that logically
concludes with the provided future. During inference, the future plot event can
be written by a human author to steer the narrative being generated in a
certain direction. We evaluate the efficacy of our approach on a story
generation task with human evaluators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics. (arXiv:2212.09955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09955">
<div class="article-summary-box-inner">
<span><p>The proliferation of automatic faithfulness metrics for summarization has
produced a need for benchmarks to evaluate them. While existing benchmarks
measure the correlation with human judgements of faithfulness on
model-generated summaries, they are insufficient for diagnosing whether metrics
are: 1) consistent, i.e., decrease as errors are introduced into a summary, 2)
effective on human-written texts, and 3) sensitive to different error types (as
summaries can contain multiple errors). To address these needs, we present a
benchmark of unfaithful minimal pairs (BUMP), a dataset of 889 human-written,
minimally different summary pairs, where a single error (from an ontology of 7
types) is introduced to a summary from the CNN/DailyMail dataset to produce an
unfaithful summary. We find BUMP complements existing benchmarks in a number of
ways: 1) the summaries in BUMP are harder to discriminate and less probable
under SOTA summarization models, 2) BUMP enables measuring the consistency of
metrics, and reveals that the most discriminative metrics tend not to be the
most consistent, 3) BUMP enables the measurement of metrics' performance on
individual error types and highlights areas of weakness for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Improving Summarization Factual Consistency from Natural Language Feedback. (arXiv:2212.09968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09968">
<div class="article-summary-box-inner">
<span><p>Despite the recent progress in language generation models, their outputs may
not always meet user expectations. In this work, we study whether informational
feedback in natural language can be leveraged to improve generation quality and
user preference alignment. To this end, we consider factual consistency in
summarization, the quality that the summary should only contain information
supported by the input documents, for user preference alignment. We collect a
high-quality dataset, DeFacto, containing human demonstrations and
informational feedback in natural language consisting of corrective
instructions, edited summaries, and explanations with respect to the factual
consistency of the summary. Using our dataset, we study two natural language
generation tasks: 1) editing a summary using the human feedback, and 2)
generating human feedback from the original summary. Using the two tasks, we
further evaluate if models can automatically correct factual inconsistencies in
generated summaries. We show that the human-edited summaries we collected are
more factually consistent, and pre-trained language models can leverage our
dataset to improve the factual consistency of original system-generated
summaries in our proposed generation tasks. We make the DeFacto dataset
publicly available at https://github.com/microsoft/DeFacto.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Speech Transcription and Translation: Pseudo-Labeling with Out-of-Distribution Data. (arXiv:2212.09982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09982">
<div class="article-summary-box-inner">
<span><p>Self-training has been shown to be helpful in addressing data scarcity for
many domains, including vision, speech, and language. Specifically,
self-training, or pseudo-labeling, labels unsupervised data and adds that to
the training pool. In this work, we investigate and use pseudo-labeling for a
recently proposed novel setup: joint transcription and translation of speech,
which suffers from an absence of sufficient data resources. We show that under
such data-deficient circumstances, the unlabeled data can significantly vary in
domain from the supervised data, which results in pseudo-label quality
degradation. We investigate two categories of remedies that require no
additional supervision and target the domain mismatch: pseudo-label filtering
and data augmentation. We show that pseudo-label analysis and processing as
such results in additional gains on top of the vanilla pseudo-labeling setup
resulting in total improvements of up to 0.6% absolute WER and 2.2 BLEU points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation. (arXiv:2212.09994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09994">
<div class="article-summary-box-inner">
<span><p>The robustness of Text-to-SQL parsers against adversarial perturbations plays
a crucial role in delivering highly reliable applications. Previous studies
along this line primarily focused on perturbations in the natural language
question side, neglecting the variability of tables. Motivated by this, we
propose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to
measure the robustness of Text-to-SQL models. Following this proposition, we
curate ADVETA, the first robustness evaluation benchmark featuring natural and
realistic ATPs. All tested state-of-the-art models experience dramatic
performance drops on ADVETA, revealing models' vulnerability in real-world
practices. To defend against ATP, we build a systematic adversarial training
example generation framework tailored for better contextualization of tabular
data. Experiments show that our approach not only brings the best robustness
improvement against table-side perturbations but also substantially empowers
models against NL-side perturbations. We release our benchmark and code at:
https://github.com/microsoft/ContextualSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. (arXiv:2212.10001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10001">
<div class="article-summary-box-inner">
<span><p>Chain-of-Thought (CoT) prompting can dramatically improve the multi-step
reasoning abilities of large language models (LLMs). CoT explicitly encourages
the LLM to generate intermediate rationales for solving a problem, by providing
a series of reasoning steps in the demonstrations. Despite its success, there
is still little understanding of what makes CoT prompting effective and which
aspects of the demonstrated reasoning steps contribute to its performance. In
this paper, we show that CoT reasoning is possible even with invalid
demonstrations - prompting with invalid reasoning steps can achieve over 80-90%
of the performance obtained using CoT under various metrics, while still
generating coherent lines of reasoning during inference. Further experiments
show that other aspects of the rationales, such as being relevant to the query
and correctly ordering the reasoning steps, are much more important for
effective CoT reasoning. Overall, these findings both deepen our understanding
of CoT prompting, and open up new questions regarding LLMs' capability to learn
to reason in context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Against Poisoning Attacks in Open-Domain Question Answering. (arXiv:2212.10002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10002">
<div class="article-summary-box-inner">
<span><p>Recent work in open-domain question answering (ODQA) has shown that
adversarial poisoning of the input contexts can cause large drops in accuracy
for production systems. However, little to no work has proposed methods to
defend against these attacks. To do so, we introduce a new method that uses
query augmentation to search for a diverse set of retrieved passages that could
answer the original question. We integrate these new passages into the model
through the design of a novel confidence method, comparing the predicted answer
to its appearance in the retrieved contexts (what we call Confidence from
Answer Redundancy, e.g. CAR). Together these methods allow for a simple but
effective way to defend against poisoning attacks and provide gains of 5-20%
exact match across varying levels of data poisoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(QA)$^2$: Question Answering with Questionable Assumptions. (arXiv:2212.10003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10003">
<div class="article-summary-box-inner">
<span><p>Naturally-occurring information-seeking questions often contain questionable
assumptions -- assumptions that are false or unverifiable. Questions containing
questionable assumptions are challenging because they require a distinct answer
strategy that deviates from typical answers to information-seeking questions.
For instance, the question "When did Marie Curie discover Uranium?" cannot be
answered as a typical when question without addressing the false assumption
"Marie Curie discovered Uranium". In this work, we propose (QA)$^2$ (Question
Answering with Questionable Assumptions), an open-domain evaluation dataset
consisting of naturally-occurring search engine queries that may or may not
contain questionable assumptions. To be successful on (QA)$^2$, systems must be
able to detect questionable assumptions and also be able to produce adequate
responses for both typical information-seeking questions and ones with
questionable assumptions. We find that current models do struggle with handling
questionable assumptions -- the best performing model achieves 59% human rater
acceptability on abstractive QA with (QA)$^2$ questions, leaving substantial
headroom for progress.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context. (arXiv:2212.10007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10007">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (LM) for code have achieved great success
in code completion, they generate code conditioned only on the contents within
the file, i.e., in-file context, but ignore the rich semantics in other files
within the same project, i.e., cross-file context, a critical source of
information that is especially useful in modern modular software development.
Such overlooking constrains code language models' capacity in code completion,
leading to unexpected behaviors such as generating hallucinated class member
functions or function calls with unexpected arguments. In this work, we develop
a cross-file context finder tool, CCFINDER, that effectively locates and
retrieves the most relevant cross-file context. We propose CoCoMIC, a framework
that incorporates cross-file context to learn the in-file and cross-file
context jointly on top of pretrained code LMs. CoCoMIC successfully improves
the existing code LM with a 19.30% relative increase in exact match and a
15.41% relative increase in identifier matching for code completion when the
cross-file context is provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog. (arXiv:2212.10008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10008">
<div class="article-summary-box-inner">
<span><p>Many efforts have been made to construct dialog systems for different types
of conversations, such as task-oriented dialog (TOD) and open-domain dialog
(ODD). To better mimic human-level conversations that usually fuse various
dialog modes, it is essential to build a system that can effectively handle
both TOD and ODD and access different knowledge sources. To address the lack of
available data for the fused task, we propose a framework for automatically
generating dialogues that combine knowledge-grounded ODDs and TODs in various
settings. Additionally, we introduce a unified model PivotBot that is capable
of appropriately adopting TOD and ODD modes and accessing different knowledge
sources in order to effectively tackle the fused task. Evaluation results
demonstrate the superior ability of the proposed model to switch seamlessly
between TOD and ODD tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English. (arXiv:2212.10011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10011">
<div class="article-summary-box-inner">
<span><p>Privacy policies provide individuals with information about their rights and
how their personal information is handled. Natural language understanding (NLU)
technologies can support individuals and practitioners to understand better
privacy practices described in lengthy and complex documents. However, existing
efforts that use NLU technologies are limited by processing the language in a
way exclusive to a single task focusing on certain privacy practices. To this
end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)
benchmark, a multi-task benchmark for evaluating the privacy policy language
understanding across various tasks. We also collect a large corpus of privacy
policies to enable privacy policy domain-specific language model pre-training.
We demonstrate that domain-specific pre-training offers performance
improvements across all tasks. We release the benchmark to encourage future
research in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modeling with Latent Situations. (arXiv:2212.10012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10012">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) often generate incoherent outputs: they refer to events
and entity states that are incompatible with the state of the world described
in their inputs. We introduce SituationSupervision, a family of approaches for
improving coherence in LMs by training them to construct and condition on
explicit representations of entities and their states. SituationSupervision has
two components: an auxiliary situation modeling task that trains models to
predict state representations in context, and a latent state inference
procedure that imputes these states from partially annotated training data.
SituationSupervision can be applied to both fine-tuning (by supervising LMs to
encode state variables in their hidden representations) and prompting (by
inducing LMs to interleave textual descriptions of entity states with output
text). In both cases, SituationSupervision requires only a small number of
state annotations to produce major coherence improvements (between 4-11%),
showing that standard LMs can be sample-efficiently trained to model not just
language but the situations it describes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocAsRef: A Pilot Empirical Study on Repurposing Reference-Based Summary Quality Metrics Reference-Freely. (arXiv:2212.10013v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10013">
<div class="article-summary-box-inner">
<span><p>Summary quality assessment metrics have two categories: reference-based and
reference-free. Reference-based metrics are theoretically more accurate but are
limited by the availability and quality of the human-written references, which
are both difficulty to ensure. This inspires the development of reference-free
metrics, which are independent from human-written references, in the past few
years. However, existing reference-free metrics cannot be both zero-shot and
accurate. In this paper, we propose a zero-shot but accurate reference-free
approach in a sneaky way: feeding documents, based upon which summaries
generated, as references into reference-based metrics. Experimental results
show that this zero-shot approach can give us the best-performing
reference-free metrics on nearly all aspects on several recently-released
datasets, even beating reference-free metrics specifically trained for this
task sometimes. We further investigate what reference-based metrics can benefit
from such repurposing and whether our additional tweaks help.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10015">
<div class="article-summary-box-inner">
<span><p>Spatial understanding is a fundamental aspect of computer vision and integral
for human-level reasoning about images, making it an important component for
grounded language understanding. While recent large-scale text-to-image
synthesis (T2I) models have shown unprecedented improvements in photorealism,
it is unclear whether they have reliable spatial understanding capabilities. We
investigate the ability of T2I models to generate correct spatial relationships
among objects and present VISOR, an evaluation metric that captures how
accurately the spatial relationship described in text is generated in the
image. To benchmark existing models, we introduce a large-scale challenge
dataset SR2D that contains sentences describing two objects and the spatial
relationship between them. We construct and harness an automated evaluation
pipeline that employs computer vision to recognize objects and their spatial
relationships, and we employ it in a large-scale evaluation of T2I models. Our
experiments reveal a surprising finding that, although recent state-of-the-art
T2I models exhibit high image quality, they are severely limited in their
ability to generate multiple objects or the specified spatial relations such as
left/right/above/below. Our analyses demonstrate several biases and artifacts
of T2I models such as the difficulty with generating multiple objects, a bias
towards generating the first object mentioned, spatially inconsistent outputs
for equivalent relationships, and a correlation between object co-occurrence
and spatial understanding capabilities. We conduct a human study that shows the
alignment between VISOR and human judgment about spatial understanding. We
offer the SR2D dataset and the VISOR metric to the community in support of T2I
spatial reasoning research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization. (arXiv:2212.10018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10018">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization has recently garnered significant attention due to its
wide range of applications. However, existing methods for summarizing dialogues
are suboptimal because they do not take into account the inherent structure of
dialogue and rely heavily on labeled data, which can lead to poor performance
in new domains. In this work, we propose DIONYSUS (dynamic input optimization
in pre-training for dialogue summarization), a pre-trained encoder-decoder
model for summarizing dialogues in any new domain. To pre-train DIONYSUS, we
create two pseudo summaries for each dialogue example: one is produced by a
fine-tuned summarization model, and the other is a collection of dialogue turns
that convey important information. We then choose one of these pseudo summaries
based on the difference in information distribution across different types of
dialogues. This selected pseudo summary serves as the objective for
pre-training DIONYSUS using a self-supervised approach on a large dialogue
corpus. Our experiments show that DIONYSUS outperforms existing methods on six
datasets, as demonstrated by its ROUGE scores in zero-shot and few-shot
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Do Decompositions Help for Machine Reading?. (arXiv:2212.10019v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10019">
<div class="article-summary-box-inner">
<span><p>Answering complex questions often requires multi-step reasoning in order to
obtain the final answer. Most research into decompositions of complex questions
involves open-domain systems, which have shown success in using these
decompositions for improved retrieval. In the machine reading setting, however,
work to understand when decompositions are helpful is understudied. We conduct
experiments on decompositions in machine reading to unify recent work in this
space, using a range of models and datasets. We find that decompositions can be
helpful in the few-shot case, giving several points of improvement in exact
match scores. However, we also show that when models are given access to
datasets with around a few hundred or more examples, decompositions are not
helpful (and can actually be detrimental). Thus, our analysis implies that
models can learn decompositions implicitly even with limited data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10020">
<div class="article-summary-box-inner">
<span><p>In this work, we explore a useful but often neglected methodology for
robustness analysis of text generation evaluation metrics: stress tests with
synthetic data. Basically, we design and synthesize a wide range of potential
errors and check whether they result in a commensurate drop in the metric
scores. We examine a range of recently proposed evaluation metrics based on
pretrained language models, for the tasks of open-ended generation,
translation, and summarization. Our experiments reveal interesting
insensitivities, biases, or even loopholes in existing metrics. For example, we
find that BERTScore ignores truncation errors in summarization, and MAUVE
(built on top of GPT-2) is insensitive to errors at the beginning of
generations. Further, we investigate the reasons behind these blind spots and
suggest practical workarounds for a more reliable evaluation of text
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods. (arXiv:2212.10025v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10025">
<div class="article-summary-box-inner">
<span><p>With increasing privacy concerns on data, recent studies have made
significant progress using federated learning (FL) on privacy-sensitive natural
language processing (NLP) tasks. Much literature suggests fully fine-tuning
pre-trained language models (PLMs) in the FL paradigm can mitigate the data
heterogeneity problem and close the performance gap with centralized training.
However, large PLMs bring the curse of prohibitive communication overhead and
local model adaptation costs for the FL system. To this end, we introduce
various parameter-efficient tuning (PETuning) methods into federated learning.
Specifically, we provide a holistic empirical study of representative PLMs
tuning methods in FL. The experimental results cover the analysis of data
heterogeneity levels, data scales, and different FL scenarios. Overall
communication overhead can be significantly reduced by locally tuning and
globally aggregating lightweight model parameters while maintaining acceptable
performance in various FL settings. To facilitate the research of PETuning in
FL, we also develop a federated tuning framework FedPETuning, which allows
practitioners to exploit different PETuning methods under the FL training
paradigm conveniently. The source code is available at
\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10029">
<div class="article-summary-box-inner">
<span><p>When people think of everyday things like an "egg," they typically have a
mental image associated with it. This commonsense knowledge helps us understand
how these everyday things work and how to interact with them. For example, when
someone tries to make a fried egg, they know that it has a shell and that it
can be cracked open to reveal the egg white and yolk inside. However, if a
system does not have a coherent picture of such everyday things, thinking that
the egg yolk surrounds the shell, then it might have to resort to ridiculous
approaches such as trying to scrape the egg yolk off the shell into the pan. Do
language models have a coherent picture of such everyday things? To investigate
this, we propose a benchmark dataset consisting of 100 everyday things, their
parts, and the relationships between these parts. We observe that
state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have
fragments of knowledge about these entities, but they fail to produce
consistent parts mental models. We propose a simple extension to these LMs
where we apply a constraint satisfaction layer on top of raw predictions from
LMs to produce more consistent and accurate parts mental models of everyday
things.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Twitter BERT Approach for Offensive Language Detection in Marathi. (arXiv:2212.10039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10039">
<div class="article-summary-box-inner">
<span><p>Automated offensive language detection is essential in combating the spread
of hate speech, particularly in social media. This paper describes our work on
Offensive Language Identification in low resource Indic language Marathi. The
problem is formulated as a text classification task to identify a tweet as
offensive or non-offensive. We evaluate different mono-lingual and
multi-lingual BERT models on this classification task, focusing on BERT models
pre-trained with social media datasets. We compare the performance of MuRIL,
MahaTweetBERT, MahaTweetBERT-Hateful, and MahaBERT on the HASOC 2022 test set.
We also explore external data augmentation from other existing Marathi hate
speech corpus HASOC 2021 and L3Cube-MahaHate. The MahaTweetBERT, a BERT model,
pre-trained on Marathi tweets when fine-tuned on the combined dataset (HASOC
2021 + HASOC 2022 + MahaHate), outperforms all models with an F1 score of 98.43
on the HASOC 2022 test set. With this, we also provide a new state-of-the-art
result on HASOC 2022 / MOLD v2 test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Augmentation Strategy for Visually Rich Documents. (arXiv:2212.10047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10047">
<div class="article-summary-box-inner">
<span><p>Many business workflows require extracting important fields from form-like
documents (e.g. bank statements, bills of lading, purchase orders, etc.).
Recent techniques for automating this task work well only when trained with
large datasets. In this work we propose a novel data augmentation technique to
improve performance when training data is scarce, e.g. 10-250 documents. Our
technique, which we call FieldSwap, works by swapping out the key phrases of a
source field with the key phrases of a target field to generate new synthetic
examples of the target field for use in training. We demonstrate that this
approach can yield 1-7 F1 point improvements in extraction performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework of Customer Review Analysis Using the Aspect-Based Opinion Mining Approach. (arXiv:2212.10051v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10051">
<div class="article-summary-box-inner">
<span><p>Opinion mining is the branch of computation that deals with opinions,
appraisals, attitudes, and emotions of people and their different aspects. This
field has attracted substantial research interest in recent years. Aspect-level
(called aspect-based opinion mining) is often desired in practical applications
as it provides detailed opinions or sentiments about different aspects of
entities and entities themselves, which are usually required for action. Aspect
extraction and entity extraction are thus two core tasks of aspect-based
opinion mining. his paper has presented a framework of aspect-based opinion
mining based on the concept of transfer learning. on real-world customer
reviews available on the Amazon website. The model has yielded quite
satisfactory results in its task of aspect-based opinion mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning. (arXiv:2212.10057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10057">
<div class="article-summary-box-inner">
<span><p>A crucial issue of current text generation models is that they often
uncontrollably generate factually inconsistent text with respective of their
inputs. Limited by the lack of annotated data, existing works in evaluating
factual consistency directly transfer the reasoning ability of models trained
on other data-rich upstream tasks like question answering (QA) and natural
language inference (NLI) without any further adaptation. As a result, they
perform poorly on the real generated text and are biased heavily by their
single-source upstream tasks. To alleviate this problem, we propose a weakly
supervised framework that aggregates multiple resources to train a precise and
efficient factual metric, namely WeCheck. WeCheck first utilizes a generative
model to accurately label a real generated sample by aggregating its weak
labels, which are inferred from multiple resources. Then, we train the target
metric model with the weak supervision while taking noises into consideration.
Comprehensive experiments on a variety of tasks demonstrate the strong
performance of WeCheck, which achieves a 3.4\% absolute improvement over
previous state-of-the-art methods on TRUE benchmark on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An AI Dungeon Master's Guide: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons. (arXiv:2212.10060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10060">
<div class="article-summary-box-inner">
<span><p>We propose a novel task, G4C (Goal-driven Guidance Generation in Grounded
Communication), for studying goal-driven and grounded natural language
interactions. Specifically, we choose Dungeons and Dragons (D&amp;D) -- a
role-playing game consisting of multiple player characters and a Dungeon Master
(DM) who collaborate to achieve a set of goals that are beneficial to the
players -- as a testbed for this task. Here, each of the player characters is a
student, with their own personas and abilities, and the DM is the teacher, an
arbitrator of the rules of the world and responsible for assisting and guiding
the students towards a global goal. We propose a theory-of-mind-inspired
methodology for training such a DM with reinforcement learning (RL), where a
DM: (1) learns to predict how the players will react to its utterances using a
dataset of D&amp;D dialogue transcripts; and (2) uses this prediction as a reward
function providing feedback on how effective these utterances are at guiding
the players towards a goal. Human and automated evaluations show that a DM
trained with RL to generate guidance by incorporating a theory-of-mind of the
players significantly improves the players' ability to achieve goals grounded
in their shared world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Reasoning Teachers. (arXiv:2212.10071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10071">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have demonstrated remarkable performance on downstream
tasks, using in-context exemplars or human instructions. Recent works have
shown that chain-of-thought (CoT) prompting can elicit models to solve complex
reasoning tasks, step-by-step. However, the efficacy of prompt-based CoT
methods is restricted to very large LMs such as GPT-3 (175B), thus limiting
deployability. In this paper, we revisit the fine-tuning approach to enable
complex reasoning in smaller LMs, optimized to efficiently perform a specific
task. We propose Fine-tune-CoT, a method that leverages the capabilities of
very large LMs to generate reasoning samples and teach smaller models via
fine-tuning. We evaluate our method on publicly available LMs across a wide
range of complex tasks and model sizes. We find that Fine-tune-CoT enables
substantial reasoning capability in small models, whereas previous prompt-based
baselines exhibit near-random performance. Student models can even outperform
the teacher in some tasks while reducing model size requirements by several
orders of magnitude. We conduct extensive ablations and sample studies to
understand the reasoning capabilities of student models. We also identify
several important nuances that have been overlooked in concurrent fine-tuning
works on CoT and address them in our analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOC: Improving Long Story Coherence With Detailed Outline Control. (arXiv:2212.10077v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10077">
<div class="article-summary-box-inner">
<span><p>We propose the Detailed Outline Control (DOC) framework for improving
long-range plot coherence when automatically generating
several-thousand-word-long stories. DOC consists of two complementary
components: a detailed outliner and a detailed controller. The detailed
outliner creates a more detailed, hierarchically structured outline, shifting
creative burden from the main drafting procedure to the planning stage. The
detailed controller ensures the more detailed outline is still respected during
generation by controlling story passages to align with outline details. In
human evaluations of automatically generated stories, DOC substantially
outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5%
absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans
also judged DOC to be much more controllable in an interactive generation
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Pretrained Language Models for Neural Code Intelligence. (arXiv:2212.10079v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10079">
<div class="article-summary-box-inner">
<span><p>As the complexity of modern software continues to escalate, software
engineering has become an increasingly daunting and error-prone endeavor. In
recent years, the field of Neural Code Intelligence (NCI) has emerged as a
promising solution, leveraging the power of deep learning techniques to tackle
analytical tasks on source code with the goal of improving programming
efficiency and minimizing human errors within the software industry. Pretrained
language models have become a dominant force in NCI research, consistently
delivering state-of-the-art results across a wide range of tasks, including
code summarization, generation, and translation. In this paper, we present a
comprehensive survey of the NCI domain, including a thorough review of
pretraining techniques, tasks, datasets, and model architectures. We hope this
paper will serve as a bridge between the natural language and programming
language communities, offering insights for future research in this rapidly
evolving field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rumour detection using graph neural network and oversampling in benchmark Twitter dataset. (arXiv:2212.10080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10080">
<div class="article-summary-box-inner">
<span><p>Recently, online social media has become a primary source for new information
and misinformation or rumours. In the absence of an automatic rumour detection
system the propagation of rumours has increased manifold leading to serious
societal damages. In this work, we propose a novel method for building
automatic rumour detection system by focusing on oversampling to alleviating
the fundamental challenges of class imbalance in rumour detection task. Our
oversampling method relies on contextualised data augmentation to generate
synthetic samples for underrepresented classes in the dataset. The key idea
exploits selection of tweets in a thread for augmentation which can be achieved
by introducing a non-random selection criteria to focus the augmentation
process on relevant tweets. Furthermore, we propose two graph neural
networks(GNN) to model non-linear conversations on a thread. To enhance the
tweet representations in our method we employed a custom feature selection
technique based on state-of-the-art BERTweet model. Experiments of three
publicly available datasets confirm that 1) our GNN models outperform the the
current state-of-the-art classifiers by more than 20%(F1-score); 2) our
oversampling technique increases the model performance by more than
9%;(F1-score) 3) focusing on relevant tweets for data augmentation via
non-random selection criteria can further improve the results; and 4) our
method has superior capabilities to detect rumours at very early stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Rule-Neural Coreference Resolution System based on Actor-Critic Learning. (arXiv:2212.10087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10087">
<div class="article-summary-box-inner">
<span><p>A coreference resolution system is to cluster all mentions that refer to the
same entity in a given context. All coreference resolution systems need to
tackle two main tasks: one task is to detect all of the potential mentions, and
the other is to learn the linking of an antecedent for each possible mention.
In this paper, we propose a hybrid rule-neural coreference resolution system
based on actor-critic learning, such that it can achieve better coreference
performance by leveraging the advantages from both the heuristic rules and a
neural conference model. This end-to-end system can also perform both mention
detection and resolution by leveraging a joint training algorithm. We
experiment on the BERT model to generate input span representations. Our model
with the BERT span representation achieves the state-of-the-art performance
among the models on the CoNLL-2012 Shared Task English Test Set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward a Unified Framework for Unsupervised Complex Tabular Reasoning. (arXiv:2212.10097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10097">
<div class="article-summary-box-inner">
<span><p>Structured tabular data exist across nearly all fields. Reasoning task over
these data aims to answer questions or determine the truthiness of hypothesis
sentences by understanding the semantic meaning of a table. While previous
works have devoted significant efforts to the tabular reasoning task, they
always assume there are sufficient labeled data. However, constructing
reasoning samples over tables (and related text) is labor-intensive, especially
when the reasoning process is complex. When labeled data is insufficient, the
performance of models will suffer an unendurable decline. In this paper, we
propose a unified framework for unsupervised complex tabular reasoning (UCTR),
which generates sufficient and diverse synthetic data with complex logic for
tabular reasoning tasks, assuming no human-annotated data at all. We first
utilize a random sampling strategy to collect diverse programs of different
types and execute them on tables based on a "Program-Executor" module. To
bridge the gap between the programs and natural language sentences, we design a
powerful "NL-Generator" module to generate natural language sentences with
complex logic from these programs. Since a table often occurs with its
surrounding texts, we further propose novel "Table-to-Text" and "Text-to-Table"
operators to handle joint table-text reasoning scenarios. This way, we can
adequately exploit the unlabeled table resources to obtain a well-performed
reasoning model under an unsupervised setting. Our experiments cover different
tasks (question answering and fact verification) and different domains (general
and specific), showing that our unsupervised methods can achieve at most 93%
performance compared to supervised models. We also find that it can
substantially boost the supervised performance in low-resourced domains as a
data augmentation technique. Our code is available at
https://github.com/leezythu/UCTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">True Detective: A Challenging Benchmark for Deep Abductive Reasoning \\in Foundation Models. (arXiv:2212.10114v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10114">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated strong performance in
zero-shot reasoning tasks, including abductive reasoning. This is reflected in
their ability to perform well on current benchmarks in this area. However, to
truly test the limits of LLMs in abductive reasoning, a more challenging
benchmark is needed. In this paper, we present such a benchmark, consisting of
191 long-form mystery stories, each approximately 1200 words in length and
presented in the form of detective puzzles. Each puzzle includes a
multiple-choice question for evaluation sourced from the "5 Minute Mystery"
platform. Our results show that state-of-the-art GPT models perform
significantly worse than human solvers on this benchmark, with an accuracy of
28\% compared to 47\% for humans. This indicates that there is still a
significant gap in the abductive reasoning abilities of LLMs and highlights the
need for further research in this area. Our work provides a challenging
benchmark for future studies on reasoning in language models and contributes to
a better understanding of the limits of LLMs' abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation. (arXiv:2212.10140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10140">
<div class="article-summary-box-inner">
<span><p>One of the major challenges of machine translation (MT) is ambiguity, which
can in some cases be resolved by accompanying context such as an image.
However, recent work in multimodal MT (MMT) has shown that obtaining
improvements from images is challenging, limited not only by the difficulty of
building effective cross-modal representations but also by the lack of specific
evaluation and training data. We present a new MMT approach based on a strong
text-only MT model, which uses neural adapters and a novel guided
self-attention mechanism and which is jointly trained on both visual masking
and MMT. We also release CoMMuTE, a Contrastive Multilingual Multimodal
Translation Evaluation dataset, composed of ambiguous sentences and their
possible translations, accompanied by disambiguating images corresponding to
each translation. Our approach obtains competitive results over strong
text-only models on standard English-to-French benchmarks and outperforms these
baselines and state-of-the-art MMT systems with a large margin on our
contrastive test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quirk or Palmer: A Comparative Study of Modal Verb Frameworks with Annotated Datasets. (arXiv:2212.10152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10152">
<div class="article-summary-box-inner">
<span><p>Modal verbs, such as "can", "may", and "must", are commonly used in daily
communication to convey the speaker's perspective related to the likelihood
and/or mode of the proposition. They can differ greatly in meaning depending on
how they're used and the context of a sentence (e.g. "They 'must' help each
other out." vs. "They 'must' have helped each other out.") Despite their
practical importance in natural language understanding, linguists have yet to
agree on a single, prominent framework for the categorization of modal verb
senses. This lack of agreement stems from high degrees of flexibility and
polysemy from the modal verbs, making it more difficult for researchers to
incorporate insights from this family of words into their work. This work
presents Moverb dataset, which consists of 27,240 annotations of modal verb
senses over 4,540 utterances containing one or more sentences from social
conversations. Each utterance is annotated by three annotators using two
different theoretical frameworks (i.e., Quirk and Palmer) of modal verb senses.
We observe that both frameworks have similar inter-annotator agreements,
despite having different numbers of sense types (8 for Quirk and 3 for Palmer).
With the RoBERTa-based classifiers fine-tuned on \dataset, we achieve F1 scores
of 82.2 and 78.3 on Quirk and Palmer, respectively, showing that modal verb
sense disambiguation is not a trivial task. Our dataset will be publicly
available with our final version.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10154">
<div class="article-summary-box-inner">
<span><p>Text classifiers have promising applications in high-stake tasks such as
resume screening and content moderation. These classifiers must be fair and
avoid discriminatory decisions by being invariant to perturbations of sensitive
attributes such as gender or ethnicity. However, there is a gap between human
intuition about these perturbations and the formal similarity specifications
capturing them. While existing research has started to address this gap,
current methods are based on hardcoded word replacements, resulting in
specifications with limited expressivity or ones that fail to fully align with
human intuition (e.g., in cases of asymmetric counterfactuals). This work
proposes novel methods for bridging this gap by discovering expressive and
intuitive individual fairness specifications. We show how to leverage
unsupervised style transfer and GPT-3's zero-shot capabilities to automatically
generate expressive candidate pairs of semantically similar sentences that
differ along sensitive attributes. We then validate the generated pairs via an
extensive crowdsourcing study, which confirms that a lot of these pairs align
with human intuition about fairness in the context of toxicity classification.
Finally, we show how limited amounts of human feedback can be leveraged to
learn a similarity specification that can be used to train downstream
fairness-aware models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages. (arXiv:2212.10168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10168">
<div class="article-summary-box-inner">
<span><p>We present, Naamapadam, the largest publicly available Named Entity
Recognition (NER) dataset for the 11 major Indian languages from two language
families. In each language, it contains more than 400k sentences annotated with
a total of at least 100k entities from three standard entity categories
(Person, Location and Organization) for 9 out of the 11 languages. The training
dataset has been automatically created from the Samanantar parallel corpus by
projecting automatically tagged entities from an English sentence to the
corresponding Indian language sentence. We also create manually annotated
testsets for 8 languages containing approximately 1000 sentences per language.
We demonstrate the utility of the obtained dataset on existing testsets and the
Naamapadam-test data for 8 Indic languages. We also release IndicNER, a
multilingual mBERT model fine-tuned on the Naamapadam training set. IndicNER
achieves the best F1 on the Naamapadam-test set compared to an mBERT model
fine-tuned on existing datasets. IndicNER achieves an F1 score of more than 80
for 7 out of 11 Indic languages. The dataset and models are available under
open-source licenses at https://ai4bharat.iitm.ac.in/naamapadam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-level Relation Extraction with Relation Correlations. (arXiv:2212.10171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10171">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction faces two overlooked challenges: long-tail
problem and multi-label problem. Previous work focuses mainly on obtaining
better contextual representations for entity pairs, hardly address the above
challenges. In this paper, we analyze the co-occurrence correlation of
relations, and introduce it into DocRE task for the first time. We argue that
the correlations can not only transfer knowledge between data-rich relations
and data-scarce ones to assist in the training of tailed relations, but also
reflect semantic distance guiding the classifier to identify semantically close
relations for multi-label entity pairs. Specifically, we use relation embedding
as a medium, and propose two co-occurrence prediction sub-tasks from both
coarse- and fine-grained perspectives to capture relation correlations.
Finally, the learned correlation-aware embeddings are used to guide the
extraction of relational facts. Substantial experiments on two popular DocRE
datasets are conducted, and our method achieves superior results compared to
baselines. Insightful analysis also demonstrates the potential of relation
correlations to address the above challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Role of Parallel Data in Cross-lingual Transfer Learning. (arXiv:2212.10173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10173">
<div class="article-summary-box-inner">
<span><p>While prior work has established that the use of parallel data is conducive
for cross-lingual learning, it is unclear if the improvements come from the
data itself, or if it is the modeling of parallel interactions that matters.
Exploring this, we examine the usage of unsupervised machine translation to
generate synthetic parallel data, and compare it to supervised machine
translation and gold parallel data. We find that even model generated parallel
data can be useful for downstream tasks, in both a general setting (continued
pretraining) as well as the task-specific setting (translate-train), although
our best results are still obtained using real parallel data. Our findings
suggest that existing multilingual models do not exploit the full potential of
monolingual data, and prompt the community to reconsider the traditional
categorization of cross-lingual learning approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Human-Like Evaluation for Natural Language Generation with Error Analysis. (arXiv:2212.10179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10179">
<div class="article-summary-box-inner">
<span><p>The state-of-the-art language model-based automatic metrics, e.g. BARTScore,
benefiting from large-scale contextualized pre-training, have been successfully
used in a wide range of natural language generation (NLG) tasks, including
machine translation, text summarization, and data-to-text. Recent studies show
that considering both major errors (e.g. mistranslated tokens) and minor errors
(e.g. imperfections in fluency) can produce high-quality human judgments. This
inspires us to approach the final goal of the evaluation metrics (human-like
evaluations) by automatic error analysis. To this end, we augment BARTScore by
incorporating the human-like error analysis strategies, namely BARTScore++,
where the final score consists of both the evaluations of major errors and
minor errors. Experimental results show that BARTScore++ can consistently
improve the performance of vanilla BARTScore and outperform existing
top-scoring metrics in 20 out of 25 test settings. We hope our technique can
also be extended to other pre-trained model-based metrics. We will release our
code and scripts to facilitate the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages. (arXiv:2212.10180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10180">
<div class="article-summary-box-inner">
<span><p>The rapid growth of machine translation (MT) systems has necessitated
comprehensive studies to meta-evaluate evaluation metrics being used, which
enables a better selection of metrics that best reflect MT quality.
Unfortunately, most of the research focuses on high-resource languages, mainly
English, the observations for which may not always apply to other languages.
Indian languages, having over a billion speakers, are linguistically different
from English, and to date, there has not been a systematic study of evaluating
MT systems from English into Indian languages. In this paper, we fill this gap
by creating an MQM dataset consisting of 7000 fine-grained annotations,
spanning 5 Indian languages and 7 MT systems, and use it to establish
correlations between annotator scores and scores obtained using existing
automatic metrics. Our results show that pre-trained metrics, such as COMET,
have the highest correlations with annotator scores. Additionally, we find that
the metrics do not adequately capture fluency-based errors in Indian languages,
and there is a need to develop metrics focused on Indian languages. We hope
that our dataset and analysis will help promote further research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10189">
<div class="article-summary-box-inner">
<span><p>When answering natural language questions over knowledge bases (KBs),
incompleteness in the KB can naturally lead to many questions being
unanswerable. While answerability has been explored in other QA settings, it
has not been studied for QA over knowledge bases (KBQA). We first identify
various forms of KB incompleteness that can result in a question being
unanswerable. We then propose GrailQAbility, a new benchmark dataset, which
systematically modifies GrailQA (a popular KBQA dataset) to represent all these
incompleteness issues. Testing two state-of-the-art KBQA models (trained on
original GrailQA as well as our GrailQAbility), we find that both models
struggle to detect unanswerable questions, or sometimes detect them for the
wrong reasons. Consequently, both models suffer significant loss in
performance, underscoring the need for further research in making KBQA systems
robust to unanswerability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pay Attention to Your Tone: Introducing a New Dataset for Polite Language Rewrite. (arXiv:2212.10190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10190">
<div class="article-summary-box-inner">
<span><p>We introduce \textsc{PoliteRewrite} -- a dataset for polite language rewrite
which is a novel sentence rewrite task. Compared with previous text style
transfer tasks that can be mostly addressed by slight token- or phrase-level
edits, polite language rewrite requires deep understanding and extensive
sentence-level edits over an offensive and impolite sentence to deliver the
same message euphemistically and politely, which is more challenging -- not
only for NLP models but also for human annotators to rewrite with effort. To
alleviate the human effort for efficient annotation, we first propose a novel
annotation paradigm by a collaboration of human annotators and GPT-3.5 to
annotate \textsc{PoliteRewrite}. The released dataset has 10K polite sentence
rewrites annotated collaboratively by GPT-3.5 and human, which can be used as
gold standard for training, validation and test; and 100K high-quality polite
sentence rewrites by GPT-3.5 without human review. We wish this work (The
dataset (10K+100K) will be released soon) could contribute to the research on
more challenging sentence rewrite, and provoke more thought in future on
resource annotation paradigm with the help of the large-scaled pretrained
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Selectable End-to-End Text-based Speech Editing. (arXiv:2212.10191v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10191">
<div class="article-summary-box-inner">
<span><p>Text-based speech editing allows users to edit speech by intuitively cutting,
copying, and pasting text to speed up the process of editing speech. In the
previous work, CampNet (context-aware mask prediction network) is proposed to
realize text-based speech editing, significantly improving the quality of
edited speech. This paper aims at a new task: adding emotional effect to the
editing speech during the text-based speech editing to make the generated
speech more expressive. To achieve this task, we propose Emo-CampNet (emotion
CampNet), which can provide the option of emotional attributes for the
generated speech in text-based speech editing and has the one-shot ability to
edit unseen speakers' speech. Firstly, we propose an end-to-end
emotion-selectable text-based speech editing model. The key idea of the model
is to control the emotion of generated speech by introducing additional emotion
attributes based on the context-aware mask prediction network. Secondly, to
prevent the emotion of the generated speech from being interfered by the
emotional components in the original speech, a neutral content generator is
proposed to remove the emotion from the original speech, which is optimized by
the generative adversarial framework. Thirdly, two data augmentation methods
are proposed to enrich the emotional and pronunciation information in the
training set, which can enable the model to edit the unseen speaker's speech.
The experimental results that 1) Emo-CampNet can effectively control the
emotion of the generated speech in the process of text-based speech editing;
And can edit unseen speakers' speech. 2) Detailed ablation experiments further
prove the effectiveness of emotional selectivity and data augmentation methods.
The demo page is available at https://hairuo55.github.io/Emo-CampNet/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adam: Dense Retrieval Distillation with Adaptive Dark Examples. (arXiv:2212.10192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10192">
<div class="article-summary-box-inner">
<span><p>To improve the performance of the dual-encoder retriever, one effective
approach is knowledge distillation from the cross-encoder ranker. Existing
works construct the candidate passages following the supervised learning
setting where a query is paired with a positive passage and a batch of
negatives. However, through empirical observation, we find that even the hard
negatives from advanced methods are still too trivial for the teacher to
distinguish, preventing the teacher from transferring abundant dark knowledge
to the student through its soft label. To alleviate this issue, we propose
ADAM, a knowledge distillation framework that can better transfer the dark
knowledge held in the teacher with Adaptive Dark exAMples. Different from
previous works that only rely on one positive and hard negatives as candidate
passages, we create dark examples that all have moderate relevance to the query
through mixing-up and masking in discrete space. Furthermore, as the quality of
knowledge held in different training instances varies as measured by the
teacher's confidence score, we propose a self-paced distillation strategy that
adaptively concentrates on a subset of high-quality instances to conduct our
dark-example-based knowledge distillation to help the student learn better. We
conduct experiments on two widely-used benchmarks and verify the effectiveness
of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EIT: Enhanced Interactive Transformer. (arXiv:2212.10197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10197">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel architecture, the Enhanced Interactive
Transformer (EIT), to address the issue of head degradation in self-attention
mechanisms. Our approach replaces the traditional multi-head self-attention
mechanism with the Enhanced Multi-Head Attention (EMHA) mechanism, which
relaxes the one-to-one mapping constraint among queries and keys, allowing each
query to attend to multiple keys. Furthermore, we introduce two interaction
models, Inner-Subspace Interaction and Cross-Subspace Interaction, to fully
utilize the many-to-many mapping capabilities of EMHA. Extensive experiments on
a wide range of tasks (e.g. machine translation, abstractive summarization,
grammar correction, language modelling and brain disease automatic diagnosis)
show its superiority with a very modest increase in model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator. (arXiv:2212.10218v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10218">
<div class="article-summary-box-inner">
<span><p>Pre-trained models have achieved remarkable success in natural language
processing (NLP). However, existing pre-training methods underutilize the
benefits of language understanding for generation. Inspired by the idea of
Generative Adversarial Networks (GANs), we propose a GAN-style model for
encoder-decoder pre-training by introducing an auxiliary discriminator,
unifying the ability of language understanding and generation in a single
model. Our model, named as GanLM, is trained with two pre-training objectives:
replaced token detection and replaced token denoising. Specifically, given
masked source sentences, the generator outputs the target distribution and the
discriminator predicts whether the target sampled tokens from distribution are
incorrect. The target sentence is replaced with misclassified tokens to
construct noisy previous context, which is used to generate the gold sentence.
In general, both tasks improve the ability of language understanding and
generation by selectively using the denoising data. Extensive experiments in
language generation benchmarks show that GanLM with the powerful language
understanding capability outperforms various strong pre-trained language models
(PLMs) and achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study. (arXiv:2212.10233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10233">
<div class="article-summary-box-inner">
<span><p>Neural models that do not rely on pre-training have excelled in the keyphrase
generation task with large annotated datasets. Meanwhile, new approaches have
incorporated pre-trained language models (PLMs) for their data efficiency.
However, there lacks a systematic study of how the two types of approaches
compare and how different design choices can affect the performance of
PLM-based models. To fill in this knowledge gap and facilitate a more informed
use of PLMs for keyphrase extraction and keyphrase generation, we present an
in-depth empirical study. Formulating keyphrase extraction as sequence labeling
and keyphrase generation as sequence-to-sequence generation, we perform
extensive experiments in three domains. After showing that PLMs have
competitive high-resource performance and state-of-the-art low-resource
performance, we investigate important design choices including in-domain PLMs,
PLMs with different pre-training objectives, using PLMs with a parameter
budget, and different formulations for present keyphrases. Further results show
that (1) in-domain BERT-like PLMs can be used to build strong and
data-efficient keyphrase generation models; (2) with a fixed parameter budget,
prioritizing model depth over width and allocating more layers in the encoder
leads to better encoder-decoder models; and (3) introducing four in-domain
PLMs, we achieve a competitive performance in the news domain and the
state-of-the-art performance in the scientific domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning. (arXiv:2212.10240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10240">
<div class="article-summary-box-inner">
<span><p>For sequence generation, both autoregressive models and non-autoregressive
models have been developed in recent years. Autoregressive models can achieve
high generation quality, but the sequential decoding scheme causes slow
decoding speed. Non-autoregressive models accelerate the inference speed with
parallel decoding, while their generation quality still needs to be improved
due to the difficulty of modeling multi-modalities in data. To address the
multi-modality issue, we propose Diff-Glat, a non-autoregressive model featured
with a modality diffusion process and residual glancing training. The modality
diffusion process decomposes the modalities and reduces the modalities to learn
for each transition. And the residual glancing sampling further smooths the
modality learning procedures. Experiments demonstrate that, without using
knowledge distillation data, Diff-Glat can achieve superior performance in both
decoding efficiency and accuracy compared with the autoregressive Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Original or Translated? On the Use of Parallel Data for Translation Quality Estimation. (arXiv:2212.10257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10257">
<div class="article-summary-box-inner">
<span><p>Machine Translation Quality Estimation (QE) is the task of evaluating
translation output in the absence of human-written references. Due to the
scarcity of human-labeled QE data, previous works attempted to utilize the
abundant unlabeled parallel corpora to produce additional training data with
pseudo labels. In this paper, we demonstrate a significant gap between parallel
data and real QE data: for QE data, it is strictly guaranteed that the source
side is original texts and the target side is translated (namely
translationese). However, for parallel data, it is indiscriminate and the
translationese may occur on either source or target side. We compare the impact
of parallel data with different translation directions in QE data augmentation,
and find that using the source-original part of parallel corpus consistently
outperforms its target-original counterpart. Moreover, since the WMT corpus
lacks direction information for each parallel sentence, we train a classifier
to distinguish source- and target-original bitext, and carry out an analysis of
their difference in both style and domain. Together, these findings suggest
using source-original parallel data for QE data augmentation, which brings a
relative improvement of up to 4.0% and 6.4% compared to undifferentiated data
on sentence- and word-level QE tasks respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10258">
<div class="article-summary-box-inner">
<span><p>Recently it has been shown that state-of-the-art NLP models are vulnerable to
adversarial attacks, where the predictions of a model can be drastically
altered by slight modifications to the input (such as synonym substitutions).
While several defense techniques have been proposed, and adapted, to the
discrete nature of text adversarial attacks, the benefits of general-purpose
regularization methods such as label smoothing for language models, have not
been studied. In this paper, we study the adversarial robustness provided by
various label smoothing strategies in foundational models for diverse NLP tasks
in both in-domain and out-of-domain settings. Our experiments show that label
smoothing significantly improves adversarial robustness in pre-trained models
like BERT, against various popular attacks. We also analyze the relationship
between prediction confidence and robustness, showing that label smoothing
reduces over-confident errors on adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCode: Robustness Evaluation of Code Generation Models. (arXiv:2212.10264v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10264">
<div class="article-summary-box-inner">
<span><p>Code generation models have achieved impressive performance. However, they
tend to be brittle as slight edits to a prompt could lead to very different
generations; these robustness properties, critical for user experience when
deployed in real-life applications, are not well understood. Most existing
works on robustness in text or code tasks have focused on classification, while
robustness in generation tasks is an uncharted area and to date there is no
comprehensive benchmark for robustness in code generation. In this paper, we
propose ReCode, a comprehensive robustness evaluation benchmark for code
generation models. We customize over 30 transformations specifically for code
on docstrings, function and variable names, code syntax, and code format. They
are carefully designed to be natural in real-life coding practice, preserve the
original semantic meaning, and thus provide multifaceted assessments of a
model's robustness performance. With human annotators, we verified that over
90% of the perturbed prompts do not alter the semantic meaning of the original
prompt. In addition, we define robustness metrics for code generation models
considering the worst-case behavior under each type of perturbation, taking
advantage of the fact that executing the generated code can serve as objective
evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well
as function completion tasks derived from them. Interesting observations
include: better robustness for CodeGen over InCoder and GPT-J; models are most
sensitive to syntax perturbations; more challenging robustness evaluation on
MBPP over HumanEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extrinsic Evaluation of Machine Translation Metrics. (arXiv:2212.10297v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10297">
<div class="article-summary-box-inner">
<span><p>Automatic machine translation (MT) metrics are widely used to distinguish the
translation qualities of machine translation systems across relatively large
test sets (system-level evaluation). However, it is unclear if automatic
metrics are reliable at distinguishing good translations from bad translations
at the sentence level (segment-level evaluation). In this paper, we investigate
how useful MT metrics are at detecting the success of a machine translation
component when placed in a larger platform with a downstream task. We evaluate
the segment-level performance of the most widely used MT metrics (chrF, COMET,
BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state
tracking, question answering, and semantic parsing). For each task, we only
have access to a monolingual task-specific model. We calculate the correlation
between the metric's ability to predict a good/bad translation with the
success/failure on the final task for the Translate-Test setup. Our experiments
demonstrate that all metrics exhibit negligible correlation with the extrinsic
evaluation of the downstream outcomes. We also find that the scores provided by
neural metrics are not interpretable mostly because of undefined ranges. Our
analysis suggests that future MT metrics be designed to produce error labels
rather than scores to facilitate extrinsic evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation. (arXiv:2212.10313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10313">
<div class="article-summary-box-inner">
<span><p>Multimodal machine translation (MMT) aims to improve translation quality by
incorporating information from other modalities, such as vision. Previous MMT
systems mainly focus on better access and use of visual information and tend to
validate their methods on image-related datasets. These studies face two
challenges. First, they can only utilize triple data (bilingual texts with
images), which is scarce; second, current benchmarks are relatively restricted
and do not correspond to realistic scenarios. Therefore, this paper
correspondingly establishes new methods and new datasets for MMT. First, we
propose a framework 2/3-Triplet with two new approaches to enhance MMT by
utilizing large-scale non-triple data: monolingual image-text data and parallel
text-only data. Second, we construct an English-Chinese {e}-commercial
{m}ulti{m}odal {t}ranslation dataset (including training and testing), named
EMMT, where its test set is carefully selected as some words are ambiguous and
shall be translated mistakenly without the help of images. Experiments show
that our method is more suitable for real-world scenarios and can significantly
improve translation performance by using more non-triple data. In addition, our
model also rivals various SOTA models in conventional multimodal translation
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation. (arXiv:2212.10315v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10315">
<div class="article-summary-box-inner">
<span><p>Recent NLP models have the great ability to generalise `zero-shot' to new
tasks using only an instruction as guidance. However, these approaches usually
repeat their instructions with every input, requiring costly reprocessing of
lengthy instructions for every inference example. To alleviate this, we
introduce Hypernetworks for INstruction Tuning (HINT), which convert task
instructions and examples using a pretrained text encoder into
parameter-efficient modules inserted into an underlying model, eliminating the
need to include instructions in the model input. Compared to prior approaches
that concatenate instructions with every input instance, we find that HINT
models are significantly more compute-efficient and consistently outperform
these approaches for a given inference budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers. (arXiv:2212.10325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10325">
<div class="article-summary-box-inner">
<span><p>Diffusion model, a new generative modelling paradigm, has achieved great
success in image, audio, and video generation. However, considering the
discrete categorical nature of text, it is not trivial to extend continuous
diffusion models to natural language, and text diffusion models are less
studied. Sequence-to-sequence text generation is one of the essential natural
language processing topics. In this work, we apply diffusion models to approach
sequence-to-sequence text generation, and explore whether the superiority
generation performance of diffusion model can transfer to natural language
domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence
generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to
model denoising function. In order to improve generation quality, SeqDiffuSeq
combines the self-conditioning technique and a newly proposed adaptive noise
schedule technique. The adaptive noise schedule has the difficulty of denoising
evenly distributed across time steps, and considers exclusive noise schedules
for tokens at different positional order. Experiment results illustrate the
good performance on sequence-to-sequence generation in terms of text quality
and inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning. (arXiv:2212.10341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10341">
<div class="article-summary-box-inner">
<span><p>Machine-Generated Text (MGT) detection, a task that discriminates MGT from
Human-Written Text (HWT), plays a crucial role in preventing misuse of text
generative models, which excel in mimicking human writing style recently.
Latest proposed detectors usually take coarse text sequence as input and output
some good results by fine-tune pretrained models with standard cross-entropy
loss. However, these methods fail to consider the linguistic aspect of text
(e.g., coherence) and sentence-level structures. Moreover, they lack the
ability to handle the low-resource problem which could often happen in practice
considering the enormous amount of textual data online. In this paper, we
present a coherence-based contrastive learning model named CoCo to detect the
possible MGT under low-resource scenario. Inspired by the distinctiveness and
permanence properties of linguistic feature, we represent text as a coherence
graph to capture its entity consistency, which is further encoded by the
pretrained model and graph neural network. To tackle the challenges of data
limitations, we employ a contrastive learning framework and propose an improved
contrastive loss for making full use of hard negative samples in training
stage. The experiment results on two public datasets prove our approach
outperforms the state-of-art methods significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does It Affect You? Social and Learning Implications of Using Cognitive-Affective State Recognition for Proactive Human-Robot Tutoring. (arXiv:2212.10346v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10346">
<div class="article-summary-box-inner">
<span><p>Using robots in educational contexts has already shown to be beneficial for a
student's learning and social behaviour. For levitating them to the next level
of providing more effective and human-like tutoring, the ability to adapt to
the user and to express proactivity is fundamental. By acting proactively,
intelligent robotic tutors anticipate possible situations where problems for
the student may arise and act in advance for preventing negative outcomes.
Still, the decisions of when and how to behave proactively are open questions.
Therefore, this paper deals with the investigation of how the student's
cognitive-affective states can be used by a robotic tutor for triggering
proactive tutoring dialogue. In doing so, it is aimed to improve the learning
experience. For this reason, a concept learning task scenario was observed
where a robotic assistant proactively helped when negative user states were
detected. In a learning task, the user's states of frustration and confusion
were deemed to have negative effects on the outcome of the task and were used
to trigger proactive behaviour. In an empirical user study with 40
undergraduate and doctoral students, we studied whether the initiation of
proactive behaviour after the detection of signs of confusion and frustration
improves the student's concentration and trust in the agent. Additionally, we
investigated which level of proactive dialogue is useful for promoting the
student's concentration and trust. The results show that high proactive
behaviour harms trust, especially when triggered during negative
cognitive-affective states but contributes to keeping the student focused on
the task when triggered in these states. Based on our study results, we further
discuss future steps for improving the proactive assistance of robotic tutoring
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Receptive Field Alignment Enables Transformer Length Extrapolation. (arXiv:2212.10356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10356">
<div class="article-summary-box-inner">
<span><p>Length extrapolation is a desirable property that permits training a
transformer language model on short sequences and retaining similar
perplexities when the model is tested on substantially longer sequences. A
relative positional embedding mechanism applied on the transformer
self-attention matrix, ALiBi, demonstrates the length extrapolation property
with the widest usage to date. In this paper, we show that ALiBi surprisingly
does not utilize tokens further than the training sequence length, which can be
explained by its implicit windowed attention effect that aligns the receptive
field during training and testing stages. Inspired by ALiBi and the receptive
filed alignment hypothesis, we propose another transformer positional embedding
design named~\textbf{Sandwich} that uses longer than training sequence length
information, and it is a greatly simplified formulation of the earliest
proposed Sinusoidal positional embedding. Finally, we show that both ALiBi and
Sandwich enable efficient inference thanks to their implicit windowed attention
effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-adaptive In-context Learning. (arXiv:2212.10375v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10375">
<div class="article-summary-box-inner">
<span><p>Despite the surprising few-shot performance of in-context learning (ICL), it
is still a common practice to randomly sample examples to serve as context.
This paper advocates a new principle for ICL: self-adaptive in-context
learning. The self-adaption mechanism is introduced to help each sample find an
in-context example permutation (i.e., selection and ordering) that can derive
the correct prediction, thus maximizing performance. To validate the
effectiveness of self-adaptive ICL, we propose a general select-then-rank
framework and instantiate it with new selection and ranking algorithms. Upon
extensive evaluation on eight different NLP datasets, our self-adaptive ICL
method achieves a 40% relative improvement over the common practice setting.
Further analysis reveals the enormous potential of self-adaptive ICL that it
might be able to close the gap between ICL and finetuning given more advanced
algorithms. Our code is released to facilitate future research in this area:
https://github.com/Shark-NLP/self-adaptive-ICL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Careful Data Curation Stabilizes In-context Learning. (arXiv:2212.10378v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10378">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) enables large language models (LLMs) to perform new
tasks by prompting them with a sequence of training examples. However, ICL is
very sensitive to the choice of training examples: randomly sampling examples
from a training set leads to high variance in performance. In this paper, we
show that curating a carefully chosen subset of training data greatly
stabilizes ICL performance. We propose two methods to choose training subsets,
both of which score training examples individually and then select the
highest-scoring ones. CondAcc scores a training example by its average ICL
accuracy when combined with random training examples, while Datamodels learns a
linear proxy model that estimates how the presence of each training example
influences LLM accuracy. On average, CondAcc and Datamodels outperform sampling
from the entire training set by 7.7% and 6.3%, respectively, across 5 tasks and
two LLMs. Our analysis shows that stable subset examples are no more diverse
than average, and are not outliers in terms of sequence length and perplexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. (arXiv:2212.10380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10380">
<div class="article-summary-box-inner">
<span><p>Dual encoders are now the dominant architecture for dense retrieval. Yet, we
have little understanding of how they represent text, and why this leads to
good performance. In this work, we shed light on this question via
distributions over the vocabulary. We propose to interpret the vector
representations produced by dual encoders by projecting them into the model's
vocabulary space. We show that the resulting distributions over vocabulary
tokens are intuitive and contain rich semantic information. We find that this
view can explain some of the failure cases of dense retrievers. For example,
the inability of models to handle tail entities can be explained via a tendency
of the token distributions to forget some of the tokens of those entities. We
leverage this insight and propose a simple way to enrich query and passage
representations with lexical information at inference time, and show that this
significantly improves performance compared to the original model in
out-of-domain settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering. (arXiv:2212.10381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10381">
<div class="article-summary-box-inner">
<span><p>Recent advances in open-domain question answering (ODQA) have demonstrated
impressive accuracy on standard Wikipedia style benchmarks. However, it is less
clear how robust these models are and how well they perform when applied to
real-world applications in drastically different domains. While there has been
some work investigating how well ODQA models perform when tested for
out-of-domain (OOD) generalization, these studies have been conducted only
under conservative shifts in data distribution and typically focus on a single
component (ie. retrieval) rather than an end-to-end system. In response, we
propose a more realistic and challenging domain shift evaluation setting and,
through extensive experiments, study end-to-end model performance. We find that
not only do models fail to generalize, but high retrieval scores often still
yield poor answer prediction accuracy. We then categorize different types of
shifts and propose techniques that, when presented with a new dataset, predict
if intervention methods are likely to be successful. Finally, using insights
from this analysis, we propose and evaluate several intervention methods which
improve end-to-end answer F1 score by up to 24 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeSS: Zero-Shot Classification via Textual Similarity Comparison with Prompting using Sentence Encoder. (arXiv:2212.10391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10391">
<div class="article-summary-box-inner">
<span><p>We introduce TeSS (Text Similarity Comparison using Sentence Encoder), a
framework for zero-shot classification where the assigned label is determined
by the embedding similarity between the input text and each candidate label
prompt. We leverage representations from sentence encoders optimized to locate
semantically similar samples closer to each other in embedding space during
pre-training. The label prompt embeddings serve as prototypes of their
corresponding class clusters. Furthermore, to compensate for the potentially
poorly descriptive labels in their original format, we retrieve semantically
similar sentences from external corpora and additionally use them with the
original label prompt (TeSS-R). TeSS outperforms strong baselines on various
closed-set and open-set classification datasets under zero-shot setting, with
further gains when combined with label prompt diversification through
retrieval. These results are robustly attained to verbalizer variations, an
ancillary benefit of using a bi-encoder. Altogether, our method serves as a
reliable baseline for zero-shot classification and a simple interface to assess
the quality of sentence encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing Stance Detection Models with Counterfactual Reasoning and Adversarial Bias Learning. (arXiv:2212.10392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10392">
<div class="article-summary-box-inner">
<span><p>Stance detection models may tend to rely on dataset bias in the text part as
a shortcut and thus fail to sufficiently learn the interaction between the
targets and texts. Recent debiasing methods usually treated features learned by
small models or big models at earlier steps as bias features and proposed to
exclude the branch learning those bias features during inference. However, most
of these methods fail to disentangle the ``good'' stance features and ``bad''
bias features in the text part. In this paper, we investigate how to mitigate
dataset bias in stance detection. Motivated by causal effects, we leverage a
novel counterfactual inference framework, which enables us to capture the
dataset bias in the text part as the direct causal effect of the text on
stances and reduce the dataset bias in the text part by subtracting the direct
text effect from the total causal effect. We novelly model bias features as
features that correlate with the stance labels but fail on intermediate stance
reasoning subtasks and propose an adversarial bias learning module to model the
bias more accurately. To verify whether our model could better model the
interaction between texts and targets, we test our model on recently proposed
test sets to evaluate the understanding of the task from various aspects.
Experiments demonstrate that our proposed method (1) could better model the
bias features, and (2) outperforms existing debiasing baselines on both the
original dataset and most of the newly constructed test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Needle in a Haystack: An Analysis of Finding Qualified Workers on MTurk for Summarization. (arXiv:2212.10397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10397">
<div class="article-summary-box-inner">
<span><p>The acquisition of high-quality human annotations through crowdsourcing
platforms like Amazon Mechanical Turk (MTurk) is more challenging than
expected. The annotation quality might be affected by various aspects like
annotation instructions, Human Intelligence Task (HIT) design, and wages paid
to annotators, etc. To avoid potentially low-quality annotations which could
mislead the evaluation of automatic summarization system outputs, we
investigate the recruitment of high-quality MTurk workers via a three-step
qualification pipeline. We show that we can successfully filter out bad workers
before they carry out the evaluations and obtain high-quality annotations while
optimizing the use of resources. This paper can serve as basis for the
recruitment of qualified annotators in other challenging annotation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning Reduces Hallucination in Conversations. (arXiv:2212.10400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10400">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) store knowledge in their parameters and can
generate informative responses when used in conversational systems. However,
LMs suffer from the problem of "hallucination:" they may generate
plausible-looking statements that are irrelevant or factually incorrect. To
address this problem, we propose a contrastive learning scheme, named MixCL. A
novel mixed contrastive objective is proposed to explicitly optimize the
implicit knowledge elicitation process of LMs, and thus reduce their
hallucination in conversations. We also examine negative sampling strategies of
retrieved hard negatives and model-generated negatives. We conduct experiments
on Wizard-of-Wikipedia, a public, open-domain knowledge-grounded dialogue
benchmark, and assess the effectiveness of MixCL. MixCL effectively reduces the
hallucination of LMs in conversations and achieves the highest performance
among LM-based dialogue agents in terms of relevancy and factuality. We show
that MixCL achieves comparable performance to state-of-the-art KB-based
approaches while enjoying notable advantages in terms of efficiency and
scalability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reasoning in Large Language Models: A Survey. (arXiv:2212.10403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10403">
<div class="article-summary-box-inner">
<span><p>Reasoning is a fundamental aspect of human intelligence that plays a crucial
role in activities such as problem solving, decision making, and critical
thinking. In recent years, large language models (LLMs) have made significant
progress in natural language processing, and there is observation that these
models may exhibit reasoning abilities when they are sufficiently large.
However, it is not yet clear to what extent LLMs are capable of reasoning. This
paper provides a comprehensive overview of the current state of knowledge on
reasoning in LLMs, including techniques for improving and eliciting reasoning
in these models, methods and benchmarks for evaluating reasoning abilities,
findings and implications of previous research in this field, and suggestions
on future directions. Our aim is to provide a detailed and up-to-date review of
this topic and stimulate meaningful discussion and future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnnoBERT: Effectively Representing Multiple Annotators' Label Choices to Improve Hate Speech Detection. (arXiv:2212.10405v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10405">
<div class="article-summary-box-inner">
<span><p>Supervised approaches generally rely on majority-based labels. However, it is
hard to achieve high agreement among annotators in subjective tasks such as
hate speech detection. Existing neural network models principally regard labels
as categorical variables, while ignoring the semantic information in diverse
label texts. In this paper, we propose AnnoBERT, a first-of-its-kind
architecture integrating annotator characteristics and label text with a
transformer-based model to detect hate speech, with unique representations
based on each annotator's characteristics via Collaborative Topic Regression
(CTR) and integrate label text to enrich textual representations. During
training, the model associates annotators with their label choices given a
piece of text; during evaluation, when label information is not available, the
model predicts the aggregated label given by the participating annotators by
utilising the learnt association. The proposed approach displayed an advantage
in detecting hate speech, especially in the minority class and edge cases with
annotator disagreement. Improvement in the overall performance is the largest
when the dataset is more label-imbalanced, suggesting its practical value in
identifying real-world hate speech, as the volume of hate speech in-the-wild is
extremely small on social media, when compared with normal (non-hate) speech.
Through ablation studies, we show the relative contributions of annotator
embeddings and label text to the model performance, and tested a range of
alternative annotator embeddings and label text combinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geographic and Geopolitical Biases of Language Models. (arXiv:2212.10408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10408">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) often fail to fairly represent target users
from certain world regions because of the under-representation of those regions
in training datasets. With recent PLMs trained on enormous data sources,
quantifying their potential biases is difficult, due to their black-box nature
and the sheer scale of the data sources. In this work, we devise an approach to
study the geographic bias (and knowledge) present in PLMs, proposing a
Geographic-Representation Probing Framework adopting a self-conditioning method
coupled with entity-country mappings. Our findings suggest PLMs'
representations map surprisingly well to the physical world in terms of
country-to-country associations, but this knowledge is unequally shared across
languages. Last, we explain how large PLMs despite exhibiting notions of
geographical proximity, over-amplify geopolitical favouritism at inference
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations. (arXiv:2212.10409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10409">
<div class="article-summary-box-inner">
<span><p>Context is vital for commonsense moral reasoning. "Lying to a friend" is
wrong if it is meant to deceive them, but may be morally okay if it is intended
to protect them. Such nuanced but salient contextual information can
potentially flip the moral judgment of an action. Thus, we present
ClarifyDelphi, an interactive system that elicits missing contexts of a moral
situation by generating clarification questions such as "Why did you lie to
your friend?". Our approach is inspired by the observation that questions whose
potential answers lead to diverging moral judgments are the most informative.
We learn to generate questions using Reinforcement Learning, by maximizing the
divergence between moral judgements of hypothetical answers to a question.
Human evaluation shows that our system generates more relevant, informative and
defeasible questions compared to other question generation baselines.
ClarifyDelphi assists informed moral reasoning processes by seeking additional
morally consequential context to disambiguate social and moral situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10422">
<div class="article-summary-box-inner">
<span><p>In the era of digital healthcare, the huge volumes of textual information
generated every day in hospitals constitute an essential but underused asset
that could be exploited with task-specific, fine-tuned biomedical language
representation models, improving patient care and management. For such
specialized domains, previous research has shown that fine-tuning models
stemming from broad-coverage checkpoints can largely benefit additional
training rounds over large-scale in-domain resources. However, these resources
are often unreachable for less-resourced languages like Italian, preventing
local medical institutions to employ in-domain adaptation. In order to reduce
this gap, our work investigates two accessible approaches to derive biomedical
language models in languages other than English, taking Italian as a concrete
use-case: one based on neural machine translation of English resources,
favoring quantity over quality; the other based on a high-grade, narrow-scoped
corpus natively in Italian, thus preferring quality over quantity. Our study
shows that data quantity is a harder constraint than data quality for
biomedical adaptation, but the concatenation of high-quality data can improve
model performance even when dealing with relatively size-limited corpora. The
models published from our investigations have the potential to unlock important
research opportunities for Italian hospitals and academia. Finally, the set of
lessons learned from the study constitutes valuable insights towards a solution
to build biomedical language models that are generalizable to other
less-resourced languages and different domain settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Distillation for Long Document Retrieval. (arXiv:2212.10423v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10423">
<div class="article-summary-box-inner">
<span><p>Long document retrieval aims to fetch query-relevant documents from a
large-scale collection, where knowledge distillation has become de facto to
improve a retriever by mimicking a heterogeneous yet powerful cross-encoder.
However, in contrast to passages or sentences, retrieval on long documents
suffers from the scope hypothesis that a long document may cover multiple
topics. This maximizes their structure heterogeneity and poses a
granular-mismatch issue, leading to an inferior distillation efficacy. In this
work, we propose a new learning framework, fine-grained distillation (FGD), for
long-document retrievers. While preserving the conventional dense retrieval
paradigm, it first produces global-consistent representations crossing
different fine granularity and then applies multi-granular aligned distillation
merely during training. In experiments, we evaluate our framework on two
long-document retrieval benchmarks, which show state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perplexed by Quality: A Perplexity-based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data. (arXiv:2212.10440v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10440">
<div class="article-summary-box-inner">
<span><p>As demand for large corpora increases with the size of current
state-of-the-art language models, using web data as the main part of the
pre-training corpus for these models has become a ubiquitous practice. This, in
turn, has introduced an important challenge for NLP practitioners, as they are
now confronted with the task of developing highly optimized models and
pipelines for pre-processing large quantities of textual data, which implies,
effectively classifying and filtering multilingual, heterogeneous and noisy
data, at web scale. One of the main components of this pre-processing step for
the pre-training corpora of large language models, is the removal of adult and
harmful content. In this paper we explore different methods for detecting adult
and harmful of content in multilingual heterogeneous web data. We first show
how traditional methods in harmful content detection, that seemingly perform
quite well in small and specialized datasets quickly break down when confronted
with heterogeneous noisy web data. We then resort to using a perplexity based
approach but with a twist: Instead of using a so-called "clean" corpus to train
a small language model and then use perplexity so select the documents with low
perplexity, i.e., the documents that resemble this so-called "clean" corpus the
most. We train solely with adult and harmful textual data, and then select the
documents having a perplexity value above a given threshold. This approach will
virtually cluster our documents into two distinct groups, which will greatly
facilitate the choice of the threshold for the perplexity and will also allow
us to obtain higher precision than with the traditional classification methods
for detecting adult and harmful content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-efficient Zero-shot Transfer for Cross-Language Dense Retrieval with Adapters. (arXiv:2212.10448v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10448">
<div class="article-summary-box-inner">
<span><p>A popular approach to creating a zero-shot cross-language retrieval model is
to substitute a monolingual pretrained language model in the retrieval model
with a multilingual pretrained language model such as Multilingual BERT. This
multilingual model is fined-tuned to the retrieval task with monolingual data
such as English MS MARCO using the same training recipe as the monolingual
retrieval model used. However, such transferred models suffer from mismatches
in the languages of the input text during training and inference. In this work,
we propose transferring monolingual retrieval models using adapters, a
parameter-efficient component for a transformer network. By adding adapters
pretrained on language tasks for a specific language with task-specific
adapters, prior work has shown that the adapter-enhanced models perform better
than fine-tuning the entire model when transferring across languages in various
NLP tasks. By constructing dense retrieval models with adapters, we show that
models trained with monolingual data are more effective than fine-tuning the
entire model when transferring to a Cross Language Information Retrieval (CLIR)
setting. However, we found that the prior suggestion of replacing the language
adapters to match the target language at inference time is suboptimal for dense
retrieval models. We provide an in-depth analysis of this discrepancy between
other cross-language NLP tasks and CLIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10449">
<div class="article-summary-box-inner">
<span><p>In long document controllable summarization, where labeled data is scarce,
pretrained models struggle to adapt to the task and effectively respond to user
queries. In this paper, we introduce Socratic pretraining, a question-driven,
unsupervised pretraining objective specifically designed to improve
controllability in summarization tasks. By training a model to generate and
answer relevant questions in a given context, Socratic pretraining enables the
model to more effectively adhere to user-provided queries and identify relevant
content to be summarized. We demonstrate the effectiveness of this approach
through extensive experimentation on two summarization domains, short stories
and dialogue, and multiple control strategies: keywords, questions, and factoid
QA pairs. Our pretraining method relies only on unlabeled documents and a
question generation system and outperforms pre-finetuning approaches that use
additional supervised data. Furthermore, our results show that Socratic
pretraining cuts task-specific labeled data requirements in half, is more
faithful to user-provided queries, and achieves state-of-the-art performance on
QMSum and SQuALITY.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is GPT-3 a Good Data Annotator?. (arXiv:2212.10450v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10450">
<div class="article-summary-box-inner">
<span><p>GPT-3 (Generative Pre-trained Transformer 3) is a large-scale autoregressive
language model developed by OpenAI, which has demonstrated impressive few-shot
performance on a wide range of natural language processing (NLP) tasks. Hence,
an intuitive application is to use it for data annotation. In this paper, we
investigate whether GPT-3 can be used as a good data annotator for NLP tasks.
Data annotation is the process of labeling data that could be used to train
machine learning models. It is a crucial step in the development of NLP
systems, as it allows the model to learn the relationship between the input
data and the desired output. Given the impressive language capabilities of
GPT-3, it is natural to wonder whether it can be used to effectively annotate
data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a
data annotator by comparing it with traditional data annotation methods and
analyzing its output on a range of tasks. Through this analysis, we aim to
provide insight into the potential of GPT-3 as a general-purpose data annotator
in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MULTI3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2212.10455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10455">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) systems have been applied in a range of domains
to support human users to achieve specific goals. Systems are typically
constructed for a single domain or language and do not generalise well beyond
this. Their extension to other languages in particular is restricted by the
lack of available training data for many of the world's languages. To support
work on Natural Language Understanding (NLU) in TOD across multiple languages
and domains simultaneously, we constructed MULTI3NLU++, a multilingual,
multi-intent, multi-domain dataset. MULTI3NLU++ extends the English-only NLU++
dataset to include manual translations into a range of high, medium and low
resource languages (Spanish, Marathi, Turkish and Amharic), in two domains
(banking and hotels). MULTI3NLU++ inherits the multi-intent property of NLU++,
where an utterance may be labelled with multiple intents, providing a more
realistic representation of a user's goals and aligning with the more complex
tasks that commercial systems aim to model. We use MULTI3NLU++ to benchmark
state-of-the-art multilingual language models as well as Machine Translation
and Question Answering systems for the NLU task of intent detection for TOD
systems in the multilingual setting. The results demonstrate the challenging
nature of the dataset, particularly in the low-resource language setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models. (arXiv:2212.10461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10461">
<div class="article-summary-box-inner">
<span><p>With increasing scale, large language models demonstrate both quantitative
improvement and new qualitative capabilities, especially as zero-shot learners,
like GPT-3. However, these results rely heavily on delicate prompt design and
large computation. In this work, we explore whether the strong zero-shot
ability could be achieved at a smaller model scale without any external
supervised data. To achieve this goal, we revisit masked language modeling and
present a geometry-guided self-supervised learning method (Go-tuningfor short)
by taking a small number of task-aware self-supervised data to update language
models further. Experiments show that Go-tuning can enable T5-small (80M)
competitive zero-shot results compared with large language models, such as
T5-XL (3B). We also apply Go-tuning on multi-task settings and develop a
multi-task model, mgo-T5 (250M). It can reach the average performance of OPT
(175B) on 9 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10465">
<div class="article-summary-box-inner">
<span><p>We present SODA: the first publicly available, million-scale high-quality
social dialogue dataset. Using SODA, we train COSMO: a generalizable
conversation agent outperforming previous best-performing agents on both in-
and out-of-domain datasets.
</p>
<p>In contrast to most existing crowdsourced, small-scale dialogue corpora, we
distill 1.5M socially-grounded dialogues from a pre-trained language model
(InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing
social commonsense knowledge from a knowledge graph (Atomic10x; West et al.,
2022). Human evaluation shows that dialogues in SODA are more consistent,
specific, and (surprisingly) natural than prior human-authored datasets - e.g.,
DailyDialog (Li et al., 2017), BlendedSkillTalk (Smith et al., 2020).
</p>
<p>In addition, extensive evaluations show that COSMO is significantly more
natural and consistent on unseen datasets than best-performing dialogue models
- e.g., GODEL (Peng et al., 2022), BlenderBot (Roller et al., 2021), DialoGPT
(Zhang et al., 2020). Furthermore, it is sometimes even preferred to the
original human-written gold responses. We make our data, models, and code
public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Text Generation with Language Constraints. (arXiv:2212.10466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10466">
<div class="article-summary-box-inner">
<span><p>We consider the task of text generation in language models with constraints
specified in natural language. To this end, we first create a challenging
benchmark Cognac that provides as input to the model a topic with example text,
along with a constraint on text to be avoided. Unlike prior work, our benchmark
contains knowledge-intensive constraints sourced from databases like Wordnet
and Wikidata, which allows for straightforward evaluation while striking a
balance between broad attribute-level and narrow lexical-level controls. We
find that even state-of-the-art language models like GPT-3 fail often on this
task, and propose a solution to leverage a language model's own internal
knowledge to guide generation. Our method, called CognacGen, first queries the
language model to generate guidance terms for a specified topic or constraint,
and uses the guidance to modify the model's token generation probabilities. We
propose three forms of guidance (binary verifier, top-k tokens, textual
example), and employ prefix-tuning approaches to distill the guidance to tackle
diverse natural language constraints. Through extensive empirical evaluations,
we demonstrate that CognacGen can successfully generalize to unseen
instructions and outperform competitive baselines in generating constraint
conforming text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generic Temporal Reasoning with Differential Analysis and Explanation. (arXiv:2212.10467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10467">
<div class="article-summary-box-inner">
<span><p>Temporal reasoning is the task of predicting temporal relations of event
pairs with corresponding contexts. While some temporal reasoning models perform
reasonably well on in-domain benchmarks, we have little idea of the systems'
generalizability due to existing datasets' limitations. In this work, we
introduce a novel task named TODAY that bridges this gap with temporal
differential analysis, which as the name suggests, evaluates if systems can
correctly understand the effect of incremental changes. Specifically, TODAY
makes slight context changes for given event pairs, and systems need to tell
how this subtle contextual change will affect temporal relation distributions.
To facilitate learning, TODAY also annotates human explanations. We show that
existing models, including GPT-3, drop to random guessing on TODAY, suggesting
that they heavily rely on spurious information rather than proper reasoning for
temporal predictions. On the other hand, we show that TODAY's supervision style
and explanation annotations can be used in joint learning and encourage models
to use more appropriate signals during training and outperform across several
benchmarks. TODAY can also be used to train models to solicit incidental
supervision from noisy sources such as GPT-3 and moves farther towards generic
temporal reasoning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BMX: Boosting Machine Translation Metrics with Explainability. (arXiv:2212.10469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10469">
<div class="article-summary-box-inner">
<span><p>State-of-the-art machine translation evaluation metrics are based on
black-box language models. Hence, recent works consider their explainability
with the goals of better understandability for humans and better metric
analysis, including failure cases. In contrast, we explicitly leverage
explanations to boost the metrics' performance. In particular, we perceive
explanations as word-level scores, which we convert, via power means, into
sentence-level scores. We combine this sentence-level score with the original
metric to obtain a better metric. Our extensive evaluation and analysis across
5 datasets, 5 metrics and 4 explainability techniques shows that some
configurations reliably improve the original metrics' correlation with human
judgment. On two held datasets for testing, we obtain improvements in 15/18
resp. 4/4 cases. The gains in Pearson correlation are up to 0.032 resp. 0.055.
We make our code available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models. (arXiv:2212.10471v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10471">
<div class="article-summary-box-inner">
<span><p>We consider the problem of automatically generating stories in multiple
languages. Compared to prior work in monolingual story generation, crosslingual
story generation allows for more universal research on story planning. We
propose to use Prompting Large Language Models with Plans to study which plan
is optimal for story generation. We consider 4 types of plans and
systematically analyse how the outputs differ for different planning
strategies. The study demonstrates that formulating the plans as
question-answer pairs leads to more coherent generated stories while the plan
gives more control to the story creators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models. (arXiv:2212.10474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10474">
<div class="article-summary-box-inner">
<span><p>State-of-the-art poetry generation systems are often complex. They either
consist of task-specific model pipelines, incorporate prior knowledge in the
form of manually created constraints or both. In contrast, end-to-end models
would not suffer from the overhead of having to model prior knowledge and could
learn the nuances of poetry from data alone, reducing the degree of human
supervision required. In this work, we investigate end-to-end poetry generation
conditioned on styles such as rhyme, meter, and alliteration. We identify and
address lack of training data and mismatching tokenization algorithms as
possible limitations of past attempts. In particular, we successfully pre-train
and release ByGPT5, a new token-free decoder-only language model, and fine-tune
it on a large custom corpus of English and German quatrains annotated with our
styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2
and ChatGPT, while also being more parameter efficient and performing favorably
compared to humans. In addition, we analyze its runtime performance and
introspect the model's understanding of style conditions. We make our code,
models, and datasets publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Execution-Based Evaluation for Open-Domain Code Generation. (arXiv:2212.10481v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10481">
<div class="article-summary-box-inner">
<span><p>To extend the scope of coding queries to more realistic settings, we propose
ODEX, the first open-domain execution-based natural language (NL) to code
generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries,
along with 1,707 human-written test cases for execution. Our NL-Code pairs are
harvested from StackOverflow forums to encourage natural and practical coding
queries, which are then carefully rephrased to ensure intent clarity and
prevent potential data memorization. Moreover, ODEX supports four natural
languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils
intriguing behavioral differences between top-performing Code LMs: Codex
performs better on open-domain queries, yet CodeGen captures a better balance
between open- and closed-domain. ODEX corroborates the merits of
execution-based evaluation over metrics without execution but also unveils
their complementary effects. Powerful models such as CodeGen-6B only achieve an
11.96 pass rate at top-1 prediction, suggesting plenty of headroom for
improvement. We release ODEX to facilitate research into open-domain problems
for the code generation community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Precise Zero-Shot Dense Retrieval without Relevance Labels. (arXiv:2212.10496v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10496">
<div class="article-summary-box-inner">
<span><p>While dense retrieval has been shown effective and efficient across tasks and
languages, it remains difficult to create effective fully zero-shot dense
retrieval systems when no relevance label is available. In this paper, we
recognize the difficulty of zero-shot learning and encoding relevance. Instead,
we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a
query, HyDE first zero-shot instructs an instruction-following language model
(e.g. InstructGPT) to generate a hypothetical document. The document captures
relevance patterns but is unreal and may contain false details. Then, an
unsupervised contrastively learned encoder~(e.g. Contriever) encodes the
document into an embedding vector. This vector identifies a neighborhood in the
corpus embedding space, where similar real documents are retrieved based on
vector similarity. This second step ground the generated document to the actual
corpus, with the encoder's dense bottleneck filtering out the incorrect
details. Our experiments show that HyDE significantly outperforms the
state-of-the-art unsupervised dense retriever Contriever and shows strong
performance comparable to fine-tuned retrievers, across various tasks (e.g. web
search, QA, fact verification) and languages~(e.g. sw, ko, ja).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimpleStyle: An Adaptable Style Transfer Approach. (arXiv:2212.10498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10498">
<div class="article-summary-box-inner">
<span><p>Attribute Controlled Text Rewriting, also known as text style transfer, has
received significant attention in the natural language generation community due
to its crucial role in controllable natural language generation systems. In
this work we present SimpleStyle a minimalist yet effective approach for
attribute controlled text rewriting based on a simple mechanism composed of two
ingredients. controlled denoising and output filtering. Despite the simplicity
of our approach, which can be succinctly explained with just a few lines of
code, it is competitive with previous state-of-the-art methods both in
automatic and in human evaluations. Additionally, we demonstrate the practical
effectiveness of our system, by applying it to real-world data from social
networks. Additionally, we introduce a soft masking sampling technique that
further improves the performance of the system. We also show that feeding the
output of our system into a text-to-text student model can produce high-quality
results without the need for additional filtering. Finally, we suggest that our
method can solve the fundamental missing baseline absence that holding progress
in the field by offering our protocol as a simple, adaptive and very strong
baseline for works wish to make incremental advancements in the field of
attribute controlled text rewriting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Measure-Theoretic Characterization of Tight Language Models. (arXiv:2212.10502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10502">
<div class="article-summary-box-inner">
<span><p>Language modeling, a central task in natural language processing, involves
estimating a probability distribution over strings. In most cases, the
estimated distribution sums to 1 over all finite strings. However, in some
pathological cases, probability mass can ``leak'' onto the set of infinite
sequences. In order to characterize the notion of leakage more precisely, this
paper offers a measure-theoretic treatment of language modeling. We prove that
many popular language model families are in fact tight, meaning that they will
not leak in this sense. We also generalize characterizations of tightness
proposed in previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10503">
<div class="article-summary-box-inner">
<span><p>Prior work has shown that it is possible to expand pretrained Masked Language
Models (MLMs) to new languages by learning a new set of embeddings, while
keeping the transformer body frozen. Despite learning a small subset of
parameters, this approach is not compute-efficient, as training the new
embeddings requires a full forward and backward pass over the entire model. In
this work, we propose mini-model adaptation, a compute-efficient alternative
that builds a shallow mini-model from a fraction of a large model's parameters.
New language-specific embeddings can then be efficiently trained over the
mini-model, and plugged into the aligned large model for rapid cross-lingual
transfer. We explore two approaches to learn mini-models: MiniJoint, which
jointly pretrains the primary model and the mini-model using a single
transformer with a secondary MLM head at a middle layer; and MiniPost, where we
start from a regular pretrained model and build a mini-model by extracting and
freezing a few layers and learning a small number of parameters on top.
Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches
the performance of the standard approach using up to 2.4x less compute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?. (arXiv:2212.10504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10504">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) systems are mainly based on the
slot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down
into smaller, controllable units (i.e., slots) to fulfill a specific task. A
series of approaches based on this framework achieved remarkable success on
various TOD benchmarks. However, we argue that the current TOD benchmarks are
limited to surrogate real-world scenarios and that the current TOD models are
still a long way from unraveling the scenarios. In this position paper, we
first identify current status and limitations of SF-TOD systems. After that, we
explore the WebTOD framework, the alternative direction for building a scalable
TOD system when a web/mobile interface is available. In WebTOD, the dialogue
system learns how to understand the web/mobile interface that the human agent
interacts with, powered by a large-scale language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DePlot: One-shot visual language reasoning by plot-to-table translation. (arXiv:2212.10505v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10505">
<div class="article-summary-box-inner">
<span><p>Visual language such as charts and plots is ubiquitous in the human world.
Comprehending plots and charts requires strong reasoning skills. Prior
state-of-the-art (SOTA) models require at least tens of thousands of training
examples and their reasoning capabilities are still much limited, especially on
complex human-written queries. This paper presents the first one-shot solution
to visual language reasoning. We decompose the challenge of visual language
reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over
the translated text. The key in this method is a modality conversion module,
named as DePlot, which translates the image of a plot or chart to a linearized
table. The output of DePlot can then be directly used to prompt a pretrained
large language model (LLM), exploiting the few-shot reasoning capabilities of
LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing
unified task formats and metrics, and train DePlot end-to-end on this task.
DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
fashion. Compared with a SOTA model finetuned on more than &gt;28k data points,
DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over
finetuned SOTA on human-written queries from the task of chart QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. (arXiv:2212.10509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10509">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that large language models are capable of generating
natural language reasoning steps or Chains-of-Thoughts (CoT) to answer a
multi-step question when prompted to do so. This is insufficient, however, when
the necessary knowledge is not available or up-to-date within a model's
parameters. A straightforward approach to address this is to retrieve text from
an external knowledge source using the question as a query and prepend it as
context to the model's input. This, however, is also insufficient for
multi-step QA where \textit{what to retrieve} depends on \textit{what has
already been derived}. To address this issue we propose IRCoT, a new approach
that interleaves retrieval with CoT for multi-step QA, guiding the retrieval
with CoT and in turn using retrieved results to improve CoT. Our experiments
with GPT3 show substantial improvements in retrieval (up to 22 points) and
downstream QA (up to 16 points) over the baselines on four datasets: HotpotQA,
2WikiMultihopQA, MuSiQue, and IIRC. Notably, our method also works well for
much smaller models such as T5-Flan-large (0.7B) without any additional
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10511">
<div class="article-summary-box-inner">
<span><p>Despite their impressive performance on diverse tasks, large language models
(LMs) still struggle with tasks requiring rich world knowledge, implying the
limitations of relying solely on their parameters to encode a wealth of world
knowledge. This paper aims to understand LMs' strengths and limitations in
memorizing factual knowledge, by conducting large-scale knowledge probing
experiments of 10 models and 4 augmentation methods on PopQA, our new
open-domain QA dataset with 14k questions. We find that LMs struggle with less
popular factual knowledge, and that scaling fails to appreciably improve
memorization of factual knowledge in the tail. We then show that
retrieval-augmented LMs largely outperform orders of magnitude larger LMs,
while unassisted LMs remain competitive in questions about high-popularity
entities. Based on those findings, we devise a simple, yet effective, method
for powerful and efficient retrieval-augmented LMs, which retrieves
non-parametric memories only when necessary. Experimental results show that
this significantly improves models' performance while reducing the inference
costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CausalDialogue: Modeling Utterance-level Causality in Conversations. (arXiv:2212.10515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10515">
<div class="article-summary-box-inner">
<span><p>Despite their widespread adoption, neural conversation models have yet to
exhibit natural chat capabilities with humans. In this research, we examine
user utterances as causes and generated responses as effects, recognizing that
changes in a cause should produce a different effect. To further explore this
concept, we have compiled and expanded upon a new dataset called CausalDialogue
through crowd-sourcing. This dataset includes multiple cause-effect pairs
within a directed acyclic graph (DAG) structure. Our analysis reveals that
traditional loss functions can struggle to effectively incorporate the DAG
structure, leading us to propose a causality-enhanced method called Exponential
Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at
the utterance level in training neural conversation models. To evaluate the
effectiveness of this approach, we have built a comprehensive benchmark using
the CausalDialogue dataset leveraging large-scale pre-trained language models,
and have assessed the results through both human and automatic evaluation
metrics for coherence, diversity, and agility. Our findings show that current
techniques are still unable to effectively address conversational DAGs, and
that the ExMATE method can improve the diversity and agility of conventional
loss functions while maintaining coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Domain Adaptation of Semantic Parsers. (arXiv:2212.10520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10520">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems often assist users with personal or
confidential matters. For this reason, the developers of such a system are
generally prohibited from observing actual usage. So how can they know where
the system is failing and needs more training data or new functionality? In
this work, we study ways in which realistic user utterances can be generated
synthetically, to help increase the linguistic and functional coverage of the
system, without compromising the privacy of actual users. To this end, we
propose a two-stage Differentially Private (DP) generation method which first
generates latent semantic parses, and then generates utterances based on the
parses. Our proposed approach improves MAUVE by 3.8$\times$ and parse tree
node-type overlap by 1.4$\times$ relative to current approaches for private
synthetic data generation, improving both on fluency and semantic coverage. We
further validate our approach on a realistic domain adaptation task of adding
new functionality from private user data to a semantic parser, and show gains
of 1.3$\times$ on its accuracy with the new feature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End. (arXiv:2212.10522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10522">
<div class="article-summary-box-inner">
<span><p>We consider the end-to-end abstract-to-title generation problem, exploring
seven recent transformer based models (including ChatGPT) fine-tuned on more
than 30k abstract-title pairs from NLP and machine learning venues. As an
extension, we also consider the harder problem of generating humorous paper
titles. For the latter, we compile the first large-scale humor annotated
dataset for scientific papers in the NLP/ML domains, comprising almost 2.5k
titles. We evaluate all models using human and automatic metrics. Our human
evaluation suggests that our best end-to-end system performs similarly to human
authors (but arguably slightly worse). Generating funny titles is more
difficult, however, and our automatic systems clearly underperform relative to
humans and often learn dataset artefacts of humor. Finally, ChatGPT, without
any fine-tuning, performs on the level of our best fine-tuned system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks. (arXiv:2212.10525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10525">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) tasks have been studied for many decades
in the speech research community, but have not received as much attention as
lower-level tasks like speech and speaker recognition. In particular, there are
not nearly as many SLU task benchmarks, and many of the existing ones use data
that is not freely available to all researchers. Recent work has begun to
introduce such benchmark datasets for several tasks. In this work, we introduce
several new annotated SLU benchmark tasks based on freely available speech
data, which complement existing benchmarks and address gaps in the SLU
evaluation landscape. We contribute four tasks: question answering and
summarization involve inference over longer speech sequences; named entity
localization addresses the speech-specific task of locating the targeted
content in the signal; dialog act classification identifies the function of a
given speech utterance. We follow the blueprint of the Spoken Language
Understanding Evaluation (SLUE) benchmark suite. In order to facilitate the
development of SLU models that leverage the success of pre-trained speech
representations, we will be publishing for each task (i) annotations for a
relatively small fine-tuning set, (ii) annotated development and test sets, and
(iii) baseline models for easy reproducibility and comparisons. In this work,
we present the details of data collection and annotation and the performance of
the baseline models. We also perform sensitivity analysis of pipeline models'
performance (speech recognizer + text model) to the speech recognition
accuracy, using more than 20 state-of-the-art speech recognition models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Challenges of Open Domain Multi-Document Summarization. (arXiv:2212.10526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10526">
<div class="article-summary-box-inner">
<span><p>Multi-document summarization (MDS) has traditionally been studied assuming a
set of ground-truth topic-related input documents is provided. In practice, the
input document set is unlikely to be available a priori and would need to be
retrieved based on an information need, a setting we call open-domain MDS. We
experiment with current state-of-the-art retrieval and summarization models on
several popular MDS datasets extended to the open-domain setting. We find that
existing summarizers suffer large reductions in performance when applied as-is
to this more realistic task, though training summarizers with retrieved inputs
can reduce their sensitivity retrieval errors. To further probe these findings,
we conduct perturbation experiments on summarizer inputs to study the impact of
different types of document retrieval errors. Based on our results, we provide
practical guidelines to help facilitate a shift to open-domain MDS. We release
our code and experimental results alongside all data or model artifacts created
during our investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HYRR: Hybrid Infused Reranking for Passage Retrieval. (arXiv:2212.10528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10528">
<div class="article-summary-box-inner">
<span><p>We present Hybrid Infused Reranking for Passages Retrieval (HYRR), a
framework for training rerankers based on a hybrid of BM25 and neural retrieval
models. Retrievers based on hybrid models have been shown to outperform both
BM25 and neural models alone. Our approach exploits this improved performance
when training a reranker, leading to a robust reranking model. The reranker, a
cross-attention neural model, is shown to be robust to different first-stage
retrieval systems, achieving better performance than rerankers simply trained
upon the first-stage retrievers in the multi-stage systems. We present
evaluations on a supervised passage retrieval task using MS MARCO and zero-shot
retrieval tasks using BEIR. The empirical results show strong performance on
both evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological Perspective. (arXiv:2212.10529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10529">
<div class="article-summary-box-inner">
<span><p>Are large language models (LLMs) like GPT-3 psychologically safe? In this
work, we design unbiased prompts to evaluate LLMs systematically from a
psychological perspective. Firstly, we test the personality traits of three
different LLMs with Short Dark Triad (SD-3) and Big Five Inventory (BFI). We
find all of them show higher scores on SD-3 than the human average, indicating
a relatively darker personality. Furthermore, LLMs like InstructGPT and
FLAN-T5, which are fine-tuned with safety metrics, do not necessarily have more
positive personalities. They score higher on Machiavellianism and Narcissism
than GPT-3. Secondly, we test the LLMs in GPT-3 series on well-being tests to
study the impact of fine-tuning with more training data. Interestingly, we
observe a continuous increase in well-being scores from GPT-3 to InstructGPT.
Following the observations, we show that instruction-finetune FLAN-T5 with
positive answers in BFI can effectively improve the model from a psychological
perspective. Finally, we call on the community to evaluate and improve LLMs'
safety systematically instead of at the sentence level only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISCO: Distilling Phrasal Counterfactuals with Large Language Models. (arXiv:2212.10534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10534">
<div class="article-summary-box-inner">
<span><p>Recent methods demonstrate that data augmentation using counterfactual
knowledge can teach models the causal structure of a task, leading to robust
and generalizable models. However, such counterfactual data often has a limited
scale and diversity if crowdsourced and is computationally expensive to extend
to new perturbation types if generated using supervised methods. To address
this, we introduce a new framework called DISCO for automatically generating
high-quality counterfactual data at scale. DISCO engineers prompts to generate
phrasal perturbations with a large general language model. Then, a
task-specific teacher model filters the generation to distill high-quality
counterfactual data. We show that learning with this counterfactual data yields
a comparatively small student model that is 6% (absolute) more robust and
generalizes 5% better across distributions than baselines on various
challenging evaluations. This model is also 15% more sensitive in
differentiating original and counterfactual examples, on three evaluation sets
written by human workers and via human-AI collaboration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10535">
<div class="article-summary-box-inner">
<span><p>Mathematical reasoning is a fundamental aspect of human intelligence and is
applicable in various fields, including science, engineering, finance, and
everyday life. The development of artificial intelligence (AI) systems capable
of solving math problems and proving theorems has garnered significant interest
in the fields of machine learning and natural language processing. For example,
mathematics serves as a testbed for aspects of reasoning that are challenging
for powerful deep learning models, driving new algorithmic and modeling
advances. On the other hand, recent advances in large-scale neural language
models have opened up new benchmarks and opportunities to use deep learning for
mathematical reasoning. In this survey paper, we review the key tasks,
datasets, and methods at the intersection of mathematical reasoning and deep
learning over the past decade. We also evaluate existing benchmarks and
methods, and discuss future research directions in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measure More, Question More: Experimental Studies on Transformer-based Language Models and Complement Coercion. (arXiv:2212.10536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10536">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models have shown strong performance on an array
of natural language understanding tasks. However, the question of how these
models react to implicit meaning has been largely unexplored. We investigate
this using the complement coercion phenomenon, which involves sentences like
"The student finished the book about sailing" where the action "reading" is
implicit. We compare LMs' surprisal estimates at various critical sentence
regions in sentences with and without implicit meaning. Effects associated with
recovering implicit meaning were found at a critical region other than where
sentences minimally differ. We then use follow-up experiments to factor out
potential confounds, revealing different perspectives that offer a richer and
more accurate picture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10537">
<div class="article-summary-box-inner">
<span><p>Large-scale models combining text and images have made incredible progress in
recent years. However, they can still fail at tasks requiring compositional
knowledge, such as correctly picking out a red cube from a picture of multiple
shapes. We examine the ability of CLIP (Radford et al., 2021), to caption
images requiring compositional knowledge. We implement five compositional
language models to probe the kinds of structure that CLIP may be using, and
develop a novel training algorithm, Compositional Skipgram for Images (CoSI),
to train these models. We look at performance in attribute-based tasks,
requiring the identification of a particular combination of attribute and
object (such as "red cube"), and in relational settings, where the spatial
relation between two shapes (such as "cube behind sphere") must be identified.
We find that in some conditions, CLIP is able to learn attribute-object
labellings, and to generalize to unseen attribute-object combinations. However,
we also see evidence that CLIP is not able to bind features together reliably.
Moreover, CLIP is not able to reliably learn relations between objects, whereas
some compositional models are able to learn these perfectly. Of the five models
we developed, none were able to generalize to unseen relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?. (arXiv:2212.10539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10539">
<div class="article-summary-box-inner">
<span><p>Large language models can perform new tasks in a zero-shot fashion, given
natural language prompts that specify the desired behavior. Such prompts are
typically hand engineered, but can also be learned with gradient-based methods
from labeled data. However, it is underexplored what factors make the prompts
effective, especially when the prompts are natural language. In this paper, we
investigate common attributes shared by effective prompts. We first propose a
human readable prompt tuning method (F LUENT P ROMPT) based on Langevin
dynamics that incorporates a fluency constraint to find a diverse distribution
of effective and fluent prompts. Our analysis reveals that effective prompts
are topically related to the task domain and calibrate the prior probability of
label words. Based on these findings, we also propose a method for generating
prompts using only unlabeled data, outperforming strong baselines by an average
of 7.0% accuracy across three tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts. (arXiv:2212.10543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10543">
<div class="article-summary-box-inner">
<span><p>Text detoxification has the potential to mitigate the harms of toxicity by
rephrasing text to remove offensive meaning, but subtle toxicity remains
challenging to tackle. We introduce MaRCo, a detoxification algorithm that
combines controllable generation and text rewriting methods using a Product of
Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a
non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to
mask and potentially replace. We evaluate our method on several subtle toxicity
and microaggressions datasets, and show that it not only outperforms baselines
on automatic metrics, but MaRCo's rewrites are preferred 2.1 $\times$ more in
human evaluation. Its applicability to instances of subtle toxicity is
especially promising, demonstrating a path forward for addressing increasingly
elusive online hate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining Without Attention. (arXiv:2212.10544v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10544">
<div class="article-summary-box-inner">
<span><p>Transformers have been essential to pretraining success in NLP. Other
architectures have been used, but require attention layers to match benchmark
accuracy. This work explores pretraining without attention. We test recently
developed routing layers based on state-space models (SSM) and model
architectures based on multiplicative gating. Used together these modeling
choices have a large impact on pretraining accuracy. Empirically the proposed
Bidirectional Gated SSM (BiGS) replicates BERT pretraining results without
attention and can be extended to long-form pretraining of 4096 tokens without
approximation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships. (arXiv:2212.10545v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10545">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose DimonGen, which aims to generate diverse sentences
describing concept relationships in various everyday scenarios. To support
this, we create a benchmark dataset for this task by adapting the existing
CommonGen dataset and propose a two-stage model called MoREE (Mixture of
Retrieval-Enhanced Experts) to generate the target sentences. MoREE consists of
a mixture of retriever models that retrieve diverse context sentences related
to the given concepts, and a mixture of generator models that generate diverse
sentences based on the retrieved contexts. We conduct experiments on the
DimonGen task and show that MoREE outperforms strong baselines in terms of both
the quality and diversity of the generated sentences. Our results demonstrate
that MoREE is able to generate diverse sentences that reflect different
relationships between concepts, leading to a comprehensive understanding of
concept relationships.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically-informed Hierarchical Event Modeling. (arXiv:2212.10547v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10547">
<div class="article-summary-box-inner">
<span><p>Prior work has shown that coupling sequential latent variable models with
semantic ontological knowledge can improve the representational capabilities of
event modeling approaches. In this work, we present a novel, doubly
hierarchical, semi-supervised event modeling framework that provides structural
hierarchy while also accounting for ontological hierarchy. Our approach
consists of multiple layers of structured latent variables, where each
successive layer compresses and abstracts the previous layers. We guide this
compression through the injection of structured ontological knowledge that is
defined at the type level of events: importantly, our model allows for partial
injection of semantic knowledge and it does not depend on observing instances
at any particular level of the semantic ontology. Across two different datasets
and four different evaluation metrics, we demonstrate that our approach is able
to out-perform the previous state-of-the-art approaches, demonstrating the
benefits of structured and semantic hierarchical knowledge for event modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks. (arXiv:2212.10548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10548">
<div class="article-summary-box-inner">
<span><p>In the absence of readily available labeled data for a given task and
language, annotation projection has been proposed as one of the possible
strategies to automatically generate annotated data which may then be used to
train supervised systems. Annotation projection has often been formulated as
the task of projecting, on parallel corpora, some labels from a source into a
target language. In this paper we present T-Projection, a new approach for
annotation projection that leverages large pretrained text2text language models
and state-of-the-art machine translation technology. T-Projection decomposes
the label projection task into two subtasks: (i) The candidate generation step,
in which a set of projection candidates using a multilingual T5 model is
generated and, (ii) the candidate selection step, in which the candidates are
ranked based on translation probabilities. We evaluate our method in three
downstream tasks and five different languages. Our results show that
T-projection improves the average F1 score of previous methods by more than 8
points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10549">
<div class="article-summary-box-inner">
<span><p>Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., "mug in grass") with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from 'mug' to 'grass'
(capturing the semantic relation 'in') to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a 'change of basis' provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lego-MT: Towards Detachable Models in Massively Multilingual Machine Translation. (arXiv:2212.10551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10551">
<div class="article-summary-box-inner">
<span><p>Traditional multilingual neural machine translation (MNMT) uses a single
model to translate all directions. However, with the increasing scale of
language pairs, simply using a single model for massive MNMT brings new
challenges: parameter tension and large computations. In this paper, we revisit
multi-way structures by assigning an individual branch for each language
(group). Despite being a simple architecture, it is challenging to train
de-centralized models due to the lack of constraints to align representations
from all languages. We propose a localized training recipe to map different
branches into a unified space, resulting in an efficient detachable model,
Lego-MT. For a fair comparison, we collect data from OPUS and build the first
large-scale open-source translation benchmark covering 7 language-centric data,
each containing 445 language pairs. Experiments show that Lego-MT (1.2B) brings
gains of more than 4 BLEU while outperforming M2M-100 (12B) (We will public all
training data, models, and checkpoints)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Length-Extrapolatable Transformer. (arXiv:2212.10554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10554">
<div class="article-summary-box-inner">
<span><p>Position modeling plays a critical role in Transformers. In this paper, we
focus on length extrapolation, i.e., training on short texts while evaluating
longer sequences. We define attention resolution as an indicator of
extrapolation. Then we propose two designs to improve the above metric of
Transformers. Specifically, we introduce a relative position embedding to
explicitly maximize attention resolution. Moreover, we use blockwise causal
attention during inference for better resolution. We evaluate different
Transformer variants with language modeling. Experimental results show that our
model achieves strong performance in both interpolation and extrapolation
settings. The code will be available at https://aka.ms/LeX-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PairReranker: Pairwise Reranking for Natural Language Generation. (arXiv:2212.10555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10555">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been successful in natural language
generation (NLG) tasks. While various decoding methods have been employed, they
often produce suboptimal results. We first present an empirical analysis of
three NLG tasks: summarization, machine translation, and constrained text
generation. We found that selecting the best output from the results of
multiple decoding methods can significantly improve performance. To further
improve reranking for NLG tasks, we proposed a novel method,
\textsc{PairReranker}, which uses a single encoder and a pairwise loss function
to jointly encode a source input and a pair of candidates and compare them.
Experiments on three NLG tasks demonstrated the effectiveness and flexibility
of \textsc{PairReranker}, showing strong results, compared with previous
baselines. In addition, our \textsc{PairReranker} can generalize to
significantly improve GPT-3 (text-davinci-003) results (e.g., 24.55\% on
CommonGen and 11.35\% on WMT18 zh-en), even though our rerankers are not
trained with any GPT-3 candidates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines. (arXiv:2212.10557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10557">
<div class="article-summary-box-inner">
<span><p>Dialogue models are able to generate coherent and fluent responses, but they
can still be challenging to control and may produce non-engaging, unsafe
results. This unpredictability diminishes user trust and can hinder the use of
the models in the real world. To address this, we introduce DialGuide, a novel
framework for controlling dialogue model behavior using natural language rules,
or guidelines. These guidelines provide information about the context they are
applicable to and what should be included in the response, allowing the models
to generate responses that are more closely aligned with the developer's
expectations and intent. We evaluate DialGuide on three tasks in open-domain
dialogue response generation: guideline selection, response generation, and
response entailment verification. Our dataset contains 10,737 positive and
15,467 negative dialogue context-response-guideline triplets across two domains
- chit-chat and safety. We provide baseline models for the tasks and benchmark
their performance. We also demonstrate that DialGuide is effective in the
dialogue safety domain, producing safe and engaging responses that follow
developer guidelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-the-fly Denoising for Data Augmentation in Natural Language Understanding. (arXiv:2212.10558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10558">
<div class="article-summary-box-inner">
<span><p>Data Augmentation (DA) is frequently used to automatically provide additional
training data without extra human annotation. However, data augmentation may
introduce noisy data that impairs training. To guarantee the quality of
augmented data, existing methods either assume no noise exists in the augmented
data and adopt consistency training or use simple heuristics such as training
loss and diversity constraints to filter out ``noisy'' data. However, those
filtered examples may still contain useful information, and dropping them
completely causes loss of supervision signals. In this paper, based on the
assumption that the original dataset is cleaner than the augmented data, we
propose an on-the-fly denoising technique for data augmentation that learns
from soft augmented labels provided by an organic teacher model trained on the
cleaner original data. A simple self-regularization module is applied to force
the model prediction to be consistent across two distinct dropouts to further
prevent overfitting on noisy labels. Our method can be applied to augmentation
techniques in general and can consistently improve the performance on both text
classification and question-answering tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers. (arXiv:2212.10559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10559">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models have shown surprising In-Context Learning
(ICL) ability. With a few demonstration input-label pairs, they can predict the
label for an unseen input without additional parameter updates. Despite the
great success in performance, the working mechanism of ICL still remains an
open problem. In order to better understand how ICL works, this paper explains
language models as meta optimizers and understands ICL as a kind of implicit
finetuning. Theoretically, we figure out that the Transformer attention has a
dual form of gradient descent based optimization. On top of it, we understand
ICL as follows: GPT first produces meta gradients according to the
demonstration examples, and then these meta gradients are applied to the
original GPT to build an ICL model. Experimentally, we comprehensively compare
the behavior of ICL and explicit finetuning based on real tasks to provide
empirical evidence that supports our understanding. The results prove that ICL
behaves similarly to explicit finetuning at the prediction level, the
representation level, and the attention behavior level. Further, inspired by
our understanding of meta optimization, we design a momentum-based attention by
analogy with the momentum-based gradient descent algorithm. Its consistently
better performance over vanilla attention supports our understanding again from
another aspect, and more importantly, it shows the potential to utilize our
understanding for future model designing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Instruct: Aligning Language Model with Self Generated Instructions. (arXiv:2212.10560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10560">
<div class="article-summary-box-inner">
<span><p>Large "instruction-tuned" language models (finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is limited in quantity, diversity, and creativity, therefore hindering the
generality of the tuned model. We introduce Self-Instruct, a framework for
improving the instruction-following capabilities of pretrained language models
by bootstrapping off its own generations. Our pipeline generates instruction,
input, and output samples from a language model, then prunes them before using
them to finetune the original model. Applying our method to vanilla GPT3, we
demonstrate a 33% absolute improvement over the original model on
Super-NaturalInstructions, on par with the performance of InstructGPT_001,
which is trained with private user data and human annotations. For further
evaluation, we curate a set of expert-written instructions for novel tasks, and
show through human evaluation that tuning GPT3 with Self-Instruct outperforms
using existing public instruction datasets by a large margin, leaving only a 5%
absolute gap behind InstructGPT_001. Self-Instruct provides an almost
annotation-free method for aligning pre-trained language models with
instructions, and we release our large synthetic dataset to facilitate future
studies on instruction tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parsel: A Unified Natural Language Framework for Algorithmic Reasoning. (arXiv:2212.10561v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10561">
<div class="article-summary-box-inner">
<span><p>Despite recent success in large language model (LLM) reasoning, LLMs still
struggle with hierarchical multi-step reasoning like generating complex
programs. In these cases, humans often start with a high-level algorithmic
design and implement each part gradually. We introduce Parsel, a framework
enabling automatic implementation and validation of complex algorithms with
code LLMs, based on hierarchical function descriptions in natural language.
Parsel can be used across domains requiring hierarchical reasoning, e.g. code
synthesis, theorem proving, and robotic planning. We demonstrate Parsel's
capabilities by using it to generate complex programs that cannot currently be
automatically implemented from one description and backtranslating Python
programs in the APPS dataset. Beyond modeling capabilities, Parsel allows
problem-solving with high-level algorithmic designs, benefiting both students
and professional programmers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Character-Aware Models Improve Visual Text Rendering. (arXiv:2212.10562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10562">
<div class="article-summary-box-inner">
<span><p>Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify the extent
of this effect, we conduct a series of controlled experiments comparing
character-aware vs. character-blind text encoders. In the text-only domain, we
find that character-aware models provide large gains on a novel spelling task
(WikiSpell). Transferring these learnings onto the visual domain, we train a
suite of image generation models, and show that character-aware variants
outperform their character-blind counterparts across a range of novel text
rendering tasks (our DrawText benchmark). Our models set a much higher
state-of-the-art on visual spelling, with 30+ point accuracy gains over
competitors on rare words, despite training on far fewer examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing NLP Models Without Demographic Information. (arXiv:2212.10563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10563">
<div class="article-summary-box-inner">
<span><p>Models trained from real-world data tend to imitate and amplify social
biases. Although there are many methods suggested to mitigate biases, they
require a preliminary information on the types of biases that should be
mitigated (e.g., gender or racial bias) and the social groups associated with
each data sample. In this work, we propose a debiasing method that operates
without any prior knowledge of the demographics in the dataset, detecting
biased examples based on an auxiliary model that predicts the main model's
success and down-weights them during the training process. Results on racial
and gender bias demonstrate that it is possible to mitigate social biases
without having to use a costly demographic annotation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does unsupervised grammar induction need pixels?. (arXiv:2212.10564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10564">
<div class="article-summary-box-inner">
<span><p>Are extralinguistic signals such as image pixels crucial for inducing
constituency grammars? While past work has shown substantial gains from
multimodal cues, we investigate whether such gains persist in the presence of
rich information from large language models (LLMs). We find that our approach,
LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods on the
task of unsupervised constituency parsing, achieving state-of-the-art
performance on a variety of datasets. Moreover, LC-PCFG results in an over 50%
reduction in parameter count, and speedups in training time of 1.7x for
image-aided models and more than 5x for video-aided models, respectively. These
results challenge the notion that extralinguistic signals such as image pixels
are needed for unsupervised grammar induction, and point to the need for better
text-only baselines in evaluating the need of multi-modality for the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Model with Structural Loss for Language-based Abductive Reasoning. (arXiv:2112.00284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00284">
<div class="article-summary-box-inner">
<span><p>The abductive natural language inference task ($\alpha$NLI) is proposed to
infer the most plausible explanation between the cause and the event. In the
$\alpha$NLI task, two observations are given, and the most plausible hypothesis
is asked to pick out from the candidates. Existing methods model the relation
between each candidate hypothesis separately and penalize the inference network
uniformly. In this paper, we argue that it is unnecessary to distinguish the
reasoning abilities among correct hypotheses; and similarly, all wrong
hypotheses contribute the same when explaining the reasons of the observations.
Therefore, we propose to group instead of ranking the hypotheses and design a
structural loss called ``joint softmax focal loss'' in this paper. Based on the
observation that the hypotheses are generally semantically related, we have
designed a novel interactive language model aiming at exploiting the rich
interaction among competing hypotheses. We name this new model for $\alpha$NLI:
Interactive Model with Structural Loss (IMSL). The experimental results show
that our IMSL has achieved the highest performance on the RoBERTa-large
pretrained model, with ACC and AUC results increased by about 1\% and 5\%
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deduplicating Training Data Mitigates Privacy Risks in Language Models. (arXiv:2202.06539v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06539">
<div class="article-summary-box-inner">
<span><p>Past work has shown that large language models are susceptible to privacy
attacks, where adversaries generate sequences from a trained model and detect
which sequences are memorized from the training set. In this work, we show that
the success of these attacks is largely due to duplication in commonly used
web-scraped training sets. We first show that the rate at which language models
regenerate training sequences is superlinearly related to a sequence's count in
the training set. For instance, a sequence that is present 10 times in the
training data is on average generated ~1000 times more often than a sequence
that is present only once. We next show that existing methods for detecting
memorized sequences have near-chance accuracy on non-duplicated training
sequences. Finally, we find that after applying methods to deduplicate training
data, language models are considerably more secure against these types of
privacy attacks. Taken together, our results motivate an increased focus on
deduplication in privacy-sensitive applications and a reevaluation of the
practicality of existing privacy attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P4E: Few-Shot Event Detection as Prompt-Guided Identification and Localization. (arXiv:2202.07615v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07615">
<div class="article-summary-box-inner">
<span><p>We propose P4E, an identify-and-localize event detection framework that
integrates the best of few-shot prompting and structured prediction. Our
framework decomposes event detection into an identification task and a
localization task. For the identification task, which we formulate as
multi-label classification, we leverage cloze-based prompting to align our
objective with the pre-training task of language models, allowing our model to
quickly adapt to new event types. We then employ an event type-agnostic
sequence labeling model to localize the event trigger conditioned on the
identification output. This heterogeneous model design allows P4E to quickly
learn new event types without sacrificing the ability to make structured
predictions. Our experiments demonstrate the effectiveness of our proposed
design, and P4E shows superior performance for few-shot event detection on
benchmark datasets FewEvent and MAVEN and comparable performance to SOTA for
fully-supervised event detection on ACE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into the Openness of CLIP. (arXiv:2206.01986v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01986">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) has demonstrated great
potential in realizing open-vocabulary visual recognition in a matching style,
due to its holistic use of natural language supervision that covers
unconstrained real-world visual concepts. However, it is, in turn, also
difficult to evaluate and analyze the openness of CLIP-like models, since they
are in theory open to any vocabulary but the actual accuracy varies. To address
the insufficiency of conventional studies on openness, we resort to an
incremental perspective and define the extensibility, which essentially
approximates the model's ability to deal with new visual concepts, by
evaluating openness through vocabulary expansions. Our evaluation based on
extensibility shows that CLIP-like models are hardly truly open and their
performance degrades as the vocabulary expands to different degrees. Further
analysis reveals that the over-estimation of openness is not because CLIP-like
models fail to capture the general similarity of image and text features of
novel visual concepts, but because of the confusion among competing text
features, that is, they are not stable with respect to the vocabulary. In light
of this, we propose to improve the openness of CLIP in the feature space by
enforcing the distinguishability of text features. Our method retrieves
relevant texts from the pre-training corpus to enhance prompts for inference,
which boosts the extensibility and stability of CLIP even without fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BertNet: Harvesting Knowledge Graphs from Pretrained Language Models. (arXiv:2206.14268v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14268">
<div class="article-summary-box-inner">
<span><p>Symbolic knowledge graphs (KGs) have been constructed either by expensive
human crowdsourcing or with complex text mining pipelines. The emerging large
pretrained language models (LMs), such as Bert, have shown to implicitly encode
massive knowledge which can be queried with properly designed prompts. However,
compared to the explicit KGs, the implict knowledge in the black-box LMs is
often difficult to access or edit and lacks explainability. In this work, we
aim at harvesting symbolic KGs from the LMs, and propose a new framework for
automatic KG construction empowered by the neural LMs' flexibility and
scalability. Compared to prior works that often rely on large human annotated
data or existing massive KGs, our approach requires only the minimal definition
of relations as inputs, and hence is suitable for extracting knowledge of rich
new relations that are instantly assigned and not available before. The
framework automatically generates diverse prompts, and performs efficient
knowledge search within a given LM for consistent outputs. The knowledge
harvested with our approach shows competitive quality, diversity, and novelty.
As a result, we derive from diverse LMs a family of new KGs (e.g., BertNet and
RoBERTaNet) that contain a richer set of relations, including some complex ones
(e.g., "A is capable of but not good at B") that cannot be extracted with
previous methods. Besides, the resulting KGs also serve as a vehicle to
interpret the respective source LMs, leading to new insights into the varying
knowledge capability of different LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MILAN: Masked Image Pretraining on Language Assisted Representation. (arXiv:2208.06049v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.06049">
<div class="article-summary-box-inner">
<span><p>Self-attention based transformer models have been dominating many computer
vision tasks in the past few years. Their superb model qualities heavily depend
on the excessively large labeled image datasets. In order to reduce the
reliance on large labeled datasets, reconstruction based masked autoencoders
are gaining popularity, which learn high quality transferable representations
from unlabeled images. For the same purpose, recent weakly supervised image
pretraining methods explore language supervision from text captions
accompanying the images. In this work, we propose masked image pretraining on
language assisted representation, dubbed as MILAN. Instead of predicting raw
pixels or low level features, our pretraining objective is to reconstruct the
image features with substantial semantic signals that are obtained using
caption supervision. Moreover, to accommodate our reconstruction target, we
propose a more effective prompting decoder architecture and a semantic aware
mask sampling mechanism, which further advance the transfer performance of the
pretrained model. Experimental results demonstrate that MILAN delivers higher
accuracy than the previous works. When the masked autoencoder is pretrained and
finetuned on ImageNet-1K dataset with an input resolution of 224x224, MILAN
achieves a top-1 accuracy of 85.4% on ViT-Base, surpassing previous
state-of-the-arts by 1%. In the downstream semantic segmentation task, MILAN
achieves 52.7 mIoU using ViT-Base on ADE20K dataset, outperforming previous
masked pretraining results by 4 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Transformers in Embedding Space. (arXiv:2209.02535v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.02535">
<div class="article-summary-box-inner">
<span><p>Understanding Transformer-based models has attracted significant attention,
as they lie at the heart of recent technological advances across machine
learning. While most interpretability methods rely on running models over
inputs, recent work has shown that a zero-pass approach, where parameters are
interpreted directly without a forward/backward pass is feasible for some
Transformer parameters, and for two-layer attention networks. In this work, we
present a theoretical analysis where all parameters of a trained Transformer
are interpreted by projecting them into the embedding space, that is, the space
of vocabulary items they operate on. We derive a simple theoretical framework
to support our arguments and provide ample evidence for its validity. First, an
empirical analysis showing that parameters of both pretrained and fine-tuned
models can be interpreted in embedding space. Second, we present two
applications of our framework: (a) aligning the parameters of different models
that share a vocabulary, and (b) constructing a classifier without training by
``translating'' the parameters of a fine-tuned classifier to parameters of a
different model that was only pretrained. Overall, our findings open the door
to interpretation methods that, at least in part, abstract away from model
specifics and operate in the embedding space only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Optimal Granularity for Extractive Summarization of Unstructured Health Records: Analysis of the Largest Multi-Institutional Archive of Health Records in Japan. (arXiv:2209.10041v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10041">
<div class="article-summary-box-inner">
<span><p>Automated summarization of clinical texts can reduce the burden of medical
professionals. "Discharge summaries" are one promising application of the
summarization, because they can be generated from daily inpatient records. Our
preliminary experiment suggests that 20-31% of the descriptions in discharge
summaries overlap with the content of the inpatient records. However, it
remains unclear how the summaries should be generated from the unstructured
source. To decompose the physician's summarization process, this study aimed to
identify the optimal granularity in summarization. We first defined three types
of summarization units with different granularities to compare the performance
of the discharge summary generation: whole sentences, clinical segments, and
clauses. We defined clinical segments in this study, aiming to express the
smallest medically meaningful concepts. To obtain the clinical segments, it was
necessary to automatically split the texts in the first stage of the pipeline.
Accordingly, we compared rule-based methods and a machine learning method, and
the latter outperformed the formers with an F1 score of 0.846 in the splitting
task. Next, we experimentally measured the accuracy of extractive summarization
using the three types of units, based on the ROUGE-1 metric, on a
multi-institutional national archive of health records in Japan. The measured
accuracies of extractive summarization using whole sentences, clinical
segments, and clauses were 31.91, 36.15, and 25.18, respectively. We found that
the clinical segments yielded higher accuracy than sentences and clauses. This
result indicates that summarization of inpatient records demands finer
granularity than sentence-oriented processing. Although we used only Japanese
health records, it can be interpreted as follows: physicians extract "concepts
of medical significance" from patient records and recombine them ...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Interdisciplinary Perspective on Evaluation and Experimental Design for Visual Text Analytics: Position Paper. (arXiv:2209.11534v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.11534">
<div class="article-summary-box-inner">
<span><p>Appropriate evaluation and experimental design are fundamental for empirical
sciences, particularly in data-driven fields. Due to the successes in
computational modeling of languages, for instance, research outcomes are having
an increasingly immediate impact on end users. As the gap in adoption by end
users decreases, the need increases to ensure that tools and models developed
by the research communities and practitioners are reliable, trustworthy, and
supportive of the users in their goals. In this position paper, we focus on the
issues of evaluating visual text analytics approaches. We take an
interdisciplinary perspective from the visualization and natural language
processing communities, as we argue that the design and validation of visual
text analytics include concerns beyond computational or visual/interactive
methods on their own. We identify four key groups of challenges for evaluating
visual text analytics approaches (data ambiguity, experimental design, user
trust, and "big picture" concerns) and provide suggestions for research
opportunities from an interdisciplinary perspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XNOR-FORMER: Learning Accurate Approximations in Long Speech Transformers. (arXiv:2210.16643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16643">
<div class="article-summary-box-inner">
<span><p>Transformers are among the state of the art for many tasks in speech, vision,
and natural language processing, among others. Self-attentions, which are
crucial contributors to this performance have quadratic computational
complexity, which makes training on longer input sequences challenging. Prior
work has produced state-of-the-art transformer variants with linear attention,
however, current models sacrifice performance to achieve efficient
implementations. In this work, we develop a novel linear transformer by
examining the properties of the key-query product within self-attentions. Our
model outperforms state of the art approaches on speech recognition and speech
summarization, resulting in 1 % absolute WER improvement on the Librispeech-100
speech recognition benchmark and a new INTERVIEW speech recognition benchmark,
and 5 points on ROUGE for summarization with How2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01994">
<div class="article-summary-box-inner">
<span><p>We present lilGym, a new benchmark for language-conditioned reinforcement
learning in visual environments. lilGym is based on 2,661 highly-compositional
human-written natural language statements grounded in an interactive visual
environment. We introduce a new approach for exact reward computation in every
possible world state by annotating all statements with executable Python
programs. Each statement is paired with multiple start states and reward
functions to form thousands of distinct Markov Decision Processes of varying
difficulty. We experiment with lilGym with different models and learning
regimes. Our results and analysis show that while existing methods are able to
achieve non-trivial performance, lilGym forms a challenging open problem.
lilGym is available at https://lil.nlp.cornell.edu/lilgym/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation. (arXiv:2211.04052v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04052">
<div class="article-summary-box-inner">
<span><p>kNN-MT presents a new paradigm for domain adaptation by building an external
datastore, which usually saves all target language token occurrences in the
parallel corpus. As a result, the constructed datastore is usually large and
possibly redundant. In this paper, we investigate the interpretability issue of
this approach: what knowledge does the NMT model need? We propose the notion of
local correctness (LAC) as a new angle, which describes the potential
translation correctness for a single entry and for a given neighborhood.
Empirical study shows that our investigation successfully finds the conditions
where the NMT model could easily fail and need related knowledge. Experiments
on six diverse target domains and two language-pairs show that pruning
according to local correctness brings a light and more explainable memory for
kNN-MT domain adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08073">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) are known to improve the generalization
performance of natural language understanding models by leveraging large
amounts of data during the pre-training phase. However, the out-of-distribution
(OOD) generalization problem remains a challenge in many NLP tasks, limiting
the real-world deployment of these methods. This paper presents the first
attempt at creating a unified benchmark named GLUE-X for evaluating OOD
robustness in NLP models, highlighting the importance of OOD robustness and
providing insights on how to measure the robustness of a model and how to
improve it. The benchmark includes 13 publicly available datasets for OOD
testing, and evaluations are conducted on 8 classic NLP tasks over 19 popularly
used PLMs. Our findings confirm the need for improved OOD accuracy in NLP
tasks, as significant performance degradation was observed in all settings
compared to in-distribution (ID) accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-aware Retrieval with Instructions. (arXiv:2211.09260v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09260">
<div class="article-summary-box-inner">
<span><p>We study the problem of retrieval with instructions, where users of a
retrieval system explicitly describe their intent along with their queries. We
aim to develop a general-purpose task-aware retrieval system using multi-task
instruction tuning, which can follow human-written instructions to find the
best documents for a given query. We introduce the first large-scale collection
of approximately 40 retrieval datasets with instructions, BERRI, and present
TART, a multi-task retrieval system trained on BERRI with instructions. TART
shows strong capabilities to adapt to a new retrieval task via instructions and
advances the state of the art on two zero-shot retrieval benchmarks, BEIR and
LOTTE, outperforming models up to three times larger. We further introduce a
new evaluation setup, X^2-Retrieval to better reflect real-world scenarios,
where diverse domains and tasks are pooled and a system needs to find documents
aligning users' intents. In this setup, TART significantly outperforms
competitive baselines, further demonstrating the effectiveness of guiding
retrieval with instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning. (arXiv:2211.16773v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16773">
<div class="article-summary-box-inner">
<span><p>In task-oriented dialogs, an informative and successful system response needs
to include key information such as the phone number of a hotel. Therefore, we
hypothesize that a model can achieve better overall performance by focusing on
correctly generating key quantities. In this paper, we propose a new training
algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), that
utilizes Reinforcement Learning but avoids the time-consuming auto-regressive
generation, and a fine-grained per-token reward function to help the model
learn keywords generation more robustly. Empirical results show that the KRLS
algorithm can achieve state-of-the-art performance on the inform, success, and
combined score on the MultiWoZ benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do you MEME? Generating Explanations for Visual Semantic Role Labelling in Memes. (arXiv:2212.00715v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00715">
<div class="article-summary-box-inner">
<span><p>Memes are powerful means for effective communication on social media. Their
effortless amalgamation of viral visuals and compelling messages can have
far-reaching implications with proper marketing. Previous research on memes has
primarily focused on characterizing their affective spectrum and detecting
whether the meme's message insinuates any intended harm, such as hate, offense,
racism, etc. However, memes often use abstraction, which can be elusive. Here,
we introduce a novel task - EXCLAIM, generating explanations for visual
semantic role labeling in memes. To this end, we curate ExHVV, a novel dataset
that offers natural language explanations of connotative roles for three types
of entities - heroes, villains, and victims, encompassing 4,680 entities
present in 3K memes. We also benchmark ExHVV with several strong unimodal and
multimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task
learning framework that endeavors to address EXCLAIM optimally by jointly
learning to predict the correct semantic roles and correspondingly to generate
suitable natural language explanations. LUMEN distinctly outperforms the best
baseline across 18 standard natural language generation evaluation metrics. Our
systematic evaluation and analyses demonstrate that characteristic multimodal
cues required for adjudicating semantic roles are also helpful for generating
suitable explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02623">
<div class="article-summary-box-inner">
<span><p>We propose Universal Document Processing (UDOP), a foundation Document AI
model which unifies text, image, and layout modalities together with varied
task formats, including document understanding and generation. UDOP leverages
the spatial correlation between textual content and document image to model
image, text, and layout modalities with one uniform representation. With a
novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain
downstream tasks into a prompt-based sequence generation scheme. UDOP is
pretrained on both large-scale unlabeled document corpora using innovative
self-supervised objectives and diverse labeled data. UDOP also learns to
generate document images from text and layout modalities via masked image
reconstruction. To the best of our knowledge, this is the first time in the
field of document AI that one model simultaneously achieves high-quality neural
document editing and content customization. Our method sets the
state-of-the-art on 9 Document AI tasks, e.g., document understanding and QA,
across diverse data domains like finance reports, academic papers, and
websites. UDOP ranks first on the leaderboard of the Document Understanding
Benchmark (DUE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model. (arXiv:2212.06369v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06369">
<div class="article-summary-box-inner">
<span><p>Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06800">
<div class="article-summary-box-inner">
<span><p>In-context learning has shown great success in i.i.d semantic parsing splits,
where the training and test sets are drawn from the same distribution. In this
setup, models are typically prompted with demonstrations that are similar to
the input question. However, in the setup of compositional generalization,
where models are tested on outputs with structures that are absent from the
training set, selecting similar demonstrations is insufficient, as often no
example will be similar enough to the input. In this work, we propose a method
to select diverse demonstrations that aims to collectively cover all of the
structures required in the output program, in order to encourage the model to
generalize to new structures from these demonstrations. We empirically show
that combining diverse demonstrations with in-context learning substantially
improves performance across three compositional generalization semantic parsing
datasets in the pure in-context learning setup and when combined with
finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better. (arXiv:2212.08597v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08597">
<div class="article-summary-box-inner">
<span><p>While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations "detached" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems. (arXiv:2212.09252v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09252">
<div class="article-summary-box-inner">
<span><p>Many dialogue systems (DSs) lack characteristics humans have, such as emotion
perception, factuality, and informativeness. Enhancing DSs with knowledge
alleviates this problem, but, as many ways of doing so exist, keeping track of
all proposed methods is difficult. Here, we present the first survey of
knowledge-enhanced DSs. We define three categories of systems - internal,
external, and hybrid - based on the knowledge they use. We survey the
motivation for enhancing DSs with knowledge, used datasets, and methods for
knowledge search, knowledge encoding, and knowledge incorporation. Finally, we
propose how to improve existing systems based on theories from linguistics and
cognitive science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09648">
<div class="article-summary-box-inner">
<span><p>We present NusaCrowd, a collaborative initiative to collect and unite
existing resources for Indonesian languages, including opening access to
previously non-public resources. Through this initiative, we have has brought
together 137 datasets and 117 standardized data loaders. The quality of the
datasets has been assessed manually and automatically, and their effectiveness
has been demonstrated in multiple experiments. NusaCrowd's data collection
enables the creation of the first zero-shot benchmarks for natural language
understanding and generation in Indonesian and its local languages.
Furthermore, NusaCrowd brings the creation of the first multilingual automatic
speech recognition benchmark in Indonesian and its local languages. Our work is
intended to help advance natural language processing research in
under-represented languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Embedder, Any Task: Instruction-Finetuned Text Embeddings. (arXiv:2212.09741v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09741">
<div class="article-summary-box-inner">
<span><p>We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Human-Language Model Interaction. (arXiv:2212.09746v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09746">
<div class="article-summary-box-inner">
<span><p>Many real-world applications of language models (LMs), such as code
autocomplete and writing assistance, involve human-LM interaction. However, the
main LM benchmarks are non-interactive in that a system produces output without
human involvement. To evaluate human-LM interaction, we develop a new
framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands
non-interactive evaluation along three dimensions, capturing (i) the
interactive process, not only the final output; (ii) the first-person
subjective experience, not just a third-party assessment; and (iii) notions of
preference beyond quality. We then design five tasks ranging from goal-oriented
to open-ended to capture different forms of interaction. On four
state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21's J1-Jumbo), we
find that non-interactive performance does not always result in better human-LM
interaction and that first-person and third-party metrics can diverge,
suggesting the importance of examining the nuances of human-LM interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales. (arXiv:2207.00779v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00779">
<div class="article-summary-box-inner">
<span><p>Following how humans communicate, free-text rationales aim to use natural
language to explain neural language model (LM) behavior. However, free-text
rationales' unconstrained nature makes them prone to hallucination, so it is
important to have metrics for free-text rationale quality. Existing free-text
rationale metrics measure how consistent the rationale is with the LM's
predicted label, but there is no protocol for assessing such metrics'
reliability. Thus, we propose FRAME, a framework for evaluating rationale-label
consistency (RLC) metrics for free-text rationales. FRAME is based on three
axioms: (1) good metrics should yield highest scores for reference rationales,
which maximize RLC by construction; (2) good metrics should be appropriately
sensitive to semantic perturbation of rationales; and (3) good metrics should
be robust to variation in the LM's task performance. Across three text
classification datasets, we show that existing RLC metrics cannot satisfy all
three FRAME axioms, since they are implemented via model pretraining which
muddles the metric's signal. Then, we introduce a non-pretraining RLC metric
that greatly outperforms baselines on (1) and (3), while performing
competitively on (2). Finally, we discuss the limitations of using RLC to
evaluate free-text rationales.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-21 23:12:53.040123074 UTC">2022-12-21 23:12:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>