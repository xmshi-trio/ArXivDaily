<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-10T01:30:00Z">04-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesis of Mathematical programs from Natural Language Specifications. (arXiv:2304.03287v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03287">
<div class="article-summary-box-inner">
<span><p>Several decision problems that are encountered in various business domains
can be modeled as mathematical programs, i.e. optimization problems. The
process of conducting such modeling often requires the involvement of experts
trained in operations research and advanced algorithms. Surprisingly, despite
the significant advances in the methods for program and code synthesis, AutoML,
learning to optimize etc., there has been little or no attention paid to
automating the task of synthesizing mathematical programs. We imagine a
scenario where the specifications for modeling, i.e. the objective and
constraints are expressed in an unstructured form in natural language (NL) and
the mathematical program has to be synthesized from such an NL specification.
In this work we evaluate the efficacy of employing CodeT5 with data
augmentation and post-processing of beams. We utilize GPT-3 with back
translation for generation of synthetic examples. Further we apply rules of
linear programming to score beams and correct beams based on common error
patterns. We observe that with these enhancements CodeT5 base gives an
execution accuracy of 0.73 which is significantly better than zero-shot
execution accuracy of 0.41 by ChatGPT and 0.36 by Codex.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about. (arXiv:2304.03325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03325">
<div class="article-summary-box-inner">
<span><p>Large language models have gained considerable interest for their impressive
performance on various tasks. Among these models, ChatGPT developed by OpenAI
has become extremely popular among early adopters who even regard it as a
disruptive technology in many fields like customer service, education,
healthcare, and finance. It is essential to comprehend the opinions of these
initial users as it can provide valuable insights into the potential strengths,
weaknesses, and success or failure of the technology in different areas. This
research examines the responses generated by ChatGPT from different
Conversational QA corpora. The study employed BERT similarity scores to compare
these responses with correct answers and obtain Natural Language Inference(NLI)
labels. Evaluation scores were also computed and compared to determine the
overall performance of GPT-3 \&amp; GPT-4. Additionally, the study identified
instances where ChatGPT provided incorrect answers to questions, providing
insights into areas where the model may be prone to error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis. (arXiv:2304.03347v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03347">
<div class="article-summary-box-inner">
<span><p>Automated mental health analysis shows great potential for enhancing the
efficiency and accessibility of mental health care, whereas the recent dominant
methods utilized pre-trained language models (PLMs) as the backbone and
incorporated emotional information. The latest large language models (LLMs),
such as ChatGPT, exhibit dramatic capabilities on diverse natural language
processing tasks. However, existing studies on ChatGPT's zero-shot performance
for mental health analysis have limitations in inadequate evaluation,
utilization of emotional information, and explainability of methods. In this
work, we comprehensively evaluate the mental health analysis and emotional
reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary
and multi-class mental health condition detection, cause/factor detection of
mental health conditions, emotion recognition in conversations, and causal
emotion entailment. We empirically analyze the impact of different prompting
strategies with emotional cues on ChatGPT's mental health analysis ability and
explainability. Experimental results show that ChatGPT outperforms traditional
neural network methods but still has a significant gap with advanced
task-specific methods. The qualitative analysis shows its potential in
explainability compared with advanced black-box methods but also limitations on
robustness and inaccurate reasoning. Prompt engineering with emotional cues is
found to be effective in improving its performance on mental health analysis
but requires the proper way of emotion infusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03394">
<div class="article-summary-box-inner">
<span><p>Student opinions for a course are important to educators and administrators,
regardless of the type of the course or the institution. Reading and manually
analyzing open-ended feedback becomes infeasible for massive volumes of
comments at institution level or online forums. In this paper, we collected and
pre-processed a large number of course reviews publicly available online. We
applied machine learning techniques with the goal to gain insight into student
sentiments and topics. Specifically, we utilized current Natural Language
Processing (NLP) techniques, such as word embeddings and deep neural networks,
and state-of-the-art BERT (Bidirectional Encoder Representations from
Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet
(Generalized Auto-regression Pre-training). We performed extensive
experimentation to compare these techniques versus traditional approaches. This
comparative study demonstrates how to apply modern machine learning approaches
for sentiment polarity extraction and topic-based classification utilizing
course feedback. For sentiment polarity, the top model was RoBERTa with 95.5\%
accuracy and 84.7\% F1-macro, while for topic classification, an SVM (Support
Vector Machine) was the top classifier with 79.8\% accuracy and 80.6\%
F1-macro. We also provided an in-depth exploration of the effect of certain
hyperparameters on the model performance and discussed our observations. These
findings can be used by institutions and course providers as a guide for
analyzing their own course feedback using NLP models towards self-evaluation
and improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using LSTM and GRU With a New Dataset for Named Entity Recognition in the Arabic Language. (arXiv:2304.03399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03399">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is a natural language processing task (NLP),
which aims to identify named entities and classify them like person, location,
organization, etc. In the Arabic language, we can find a considerable size of
unstructured data, and it needs to different preprocessing tool than languages
like (English, Russian, German...). From this point, we can note the importance
of building a new structured dataset to solve the lack of structured data. In
this work, we use the BIOES format to tag the word, which allows us to handle
the nested name entity that consists of more than one sentence and define the
start and the end of the name. The dataset consists of more than thirty-six
thousand records. In addition, this work proposes long short term memory (LSTM)
units and Gated Recurrent Units (GRU) for building the named entity recognition
model in the Arabic language. The models give an approximately good result
(80%) because LSTM and GRU models can find the relationships between the words
of the sentence. Also, use a new library from Google, which is Trax and
platform Colab
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03401">
<div class="article-summary-box-inner">
<span><p>The success of contextual word representations and advances in neural
information retrieval have made dense vector-based retrieval a standard
approach for passage and document ranking. While effective and efficient,
dual-encoders are brittle to variations in query distributions and noisy
queries. Data augmentation can make models more robust but introduces overhead
to training set generation and requires retraining and index regeneration. We
present Contrastive Alignment POst Training (CAPOT), a highly efficient
finetuning method that improves model robustness without requiring index
regeneration, the training set optimization, or alteration. CAPOT enables
robust retrieval by freezing the document encoder while the query encoder
learns to align noisy queries with their unaltered root. We evaluate CAPOT
noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,
finding CAPOT has a similar impact as data augmentation with none of its
overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Corpus-Scale Discovery of Selection Biases in News Coverage: Comparing What Sources Say About Entities as a Start. (arXiv:2304.03414v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03414">
<div class="article-summary-box-inner">
<span><p>News sources undergo the process of selecting newsworthy information when
covering a certain topic. The process inevitably exhibits selection biases,
i.e. news sources' typical patterns of choosing what information to include in
news coverage, due to their agenda differences. To understand the magnitude and
implications of selection biases, one must first discover (1) on what topics do
sources typically have diverging definitions of "newsworthy" information, and
(2) do the content selection patterns correlate with certain attributes of the
news sources, e.g. ideological leaning, etc.
</p>
<p>The goal of the paper is to investigate and discuss the challenges of
building scalable NLP systems for discovering patterns of media selection
biases directly from news content in massive-scale news corpora, without
relying on labeled data. To facilitate research in this domain, we propose and
study a conceptual framework, where we compare how sources typically mention
certain controversial entities, and use such as indicators for the sources'
content selection preferences. We empirically show the capabilities of the
framework through a case study on NELA-2020, a corpus of 1.8M news articles in
English from 519 news sources worldwide. We demonstrate an unsupervised
representation learning method to capture the selection preferences for how
sources typically mention controversial entities. Our experiments show that
that distributional divergence of such representations, when studied
collectively across entities and news sources, serve as good indicators for an
individual source's ideological leaning. We hope our findings will provide
insights for future research on media selection biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03427">
<div class="article-summary-box-inner">
<span><p>Scholars in the humanities rely heavily on ancient manuscripts to study
history, religion, and socio-political structures in the past. Many efforts
have been devoted to digitizing these precious manuscripts using OCR
technology, but most manuscripts were blemished over the centuries so that an
Optical Character Recognition (OCR) program cannot be expected to capture faded
graphs and stains on pages. This work presents a neural spelling correction
model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy
output. This paper is divided into four sections: dataset, model architecture,
training and analysis. First, we feature-engineered our raw Tibetan etext
corpus into two sets of structured data frames -- a set of paired toy data and
a set of paired real data. Then, we implemented a Confidence Score mechanism
into the Transformer architecture to perform spelling correction tasks.
According to the Loss and Character Error Rate, our Transformer + Confidence
score mechanism architecture proves to be superior to Transformer, LSTM-2-LSTM
and GRU-2-GRU architectures. Finally, to examine the robustness of our model,
we analyzed erroneous tokens, visualized Attention and Self-Attention heatmaps
in our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03439">
<div class="article-summary-box-inner">
<span><p>Harnessing logical reasoning ability is a comprehensive natural language
understanding endeavor. With the release of Generative Pretrained Transformer 4
(GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn
the GPT-4 performance on various logical reasoning tasks. This report analyses
multiple logical reasoning datasets, with popular benchmarks like LogiQA and
ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice
reading comprehension and natural language inference tasks with benchmarks
requiring logical reasoning. We further construct a logical reasoning
out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.
We also make a performance comparison between ChatGPT and GPT-4. Experiment
results show that ChatGPT performs significantly better than the RoBERTa
fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even
higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do
relatively well on well-known datasets like LogiQA and ReClor. However, the
performance drops significantly when handling newly released and
out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT
and GPT-4, especially on out-of-distribution and natural language inference
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linking Representations with Multimodal Contrastive Learning. (arXiv:2304.03464v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03464">
<div class="article-summary-box-inner">
<span><p>Many applications require grouping instances contained in diverse document
datasets into classes. Most widely used methods do not employ deep learning and
do not exploit the inherently multimodal nature of documents. Notably, record
linkage is typically conceptualized as a string-matching problem. This study
develops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a
multimodal framework for record linkage. CLIPPINGS employs end-to-end training
of symmetric vision and language bi-encoders, aligned through contrastive
language-image pre-training, to learn a metric space where the pooled
image-text representation for a given instance is close to representations in
the same class and distant from representations in different classes. At
inference time, instances can be linked by retrieving their nearest neighbor
from an offline exemplar embedding index or by clustering their
representations. The study examines two challenging applications: constructing
comprehensive supply chains for mid-20th century Japan through linking firm
level financial records - with each firm name represented by its crop in the
document image and the corresponding OCR - and detecting which image-caption
pairs in a massive corpus of historical U.S. newspapers came from the same
underlying photo wire source. CLIPPINGS outperforms widely used string matching
methods by a wide margin and also outperforms unimodal methods. Moreover, a
purely self-supervised model trained on only image-OCR pairs also outperforms
popular string-matching methods without requiring any labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03512">
<div class="article-summary-box-inner">
<span><p>Multi-document scientific summarization can extract and organize important
information from an abundant collection of papers, arousing widespread
attention recently. However, existing efforts focus on producing lengthy
overviews lacking a clear and logical hierarchy. To alleviate this problem, we
present an atomic and challenging task named Hierarchical Catalogue Generation
for Literature Review (HiCatGLR), which aims to generate a hierarchical
catalogue for a review paper given various references. We carefully construct a
novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD)
with 13.8k literature review catalogues and 120k reference papers, where we
benchmark diverse experiments via the end-to-end and pipeline methods. To
accurately assess the model performance, we design evaluation metrics for
similarity to ground truth from semantics and structure. Besides, our extensive
analyses verify the high quality of our dataset and the effectiveness of our
evaluation metrics. Furthermore, we discuss potential directions for this task
to motivate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03518">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to Task 10 at SemEval 2023-Explainable
Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise
in social media platforms has seen an increase in disproportionate levels of
sexism experienced by women on social media platforms. This has made detecting
and explaining online sexist content more important than ever to make social
media safer and more accessible for women. Our approach consists of
experimenting and finetuning BERT-based models and using a Majority Voting
ensemble model that outperforms individual baseline model scores. Our system
achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319
for Task C.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03531">
<div class="article-summary-box-inner">
<span><p>Entity Set Expansion (ESE) is a critical task aiming to expand entities of
the target semantic class described by a small seed entity set. Most existing
ESE methods are retrieval-based frameworks that need to extract the contextual
features of entities and calculate the similarity between seed entities and
candidate entities. To achieve the two purposes, they should iteratively
traverse the corpus and the entity vocabulary provided in the datasets,
resulting in poor efficiency and scalability. The experimental results indicate
that the time consumed by the retrieval-based ESE methods increases linearly
with entity vocabulary and corpus size. In this paper, we firstly propose a
generative ESE framework, Generative Entity Set Expansion (GenExpan), which
utilizes a generative pre-trained language model to accomplish ESE task.
Specifically, a prefix tree is employed to guarantee the validity of entity
generation, and automatically generated class names are adopted to guide the
model to generate target entities. Moreover, we propose Knowledge Calibration
and Generative Ranking to further bridge the gap between generic knowledge of
the language model and the goal of ESE task. Experiments on publicly available
datasets show that GenExpan is efficient and effective. For efficiency,
expansion time consumed by GenExpan is independent of entity vocabulary and
corpus size, and GenExpan achieves an average 600% speedup compared to strong
baselines. For expansion performance, our framework outperforms previous
state-of-the-art ESE methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling. (arXiv:2304.03544v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03544">
<div class="article-summary-box-inner">
<span><p>Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03548">
<div class="article-summary-box-inner">
<span><p>Human experts write summaries using different techniques, including rewriting
a sentence in the document or fusing multiple sentences to generate a summary
sentence. These techniques are flexible and thus difficult to be imitated by
any single method. To address this issue, we propose an adaptive model, GEMINI,
that integrates a rewriter and a fuser to mimic the sentence rewriting and
fusion techniques, respectively. GEMINI adaptively chooses to rewrite a
specific document sentence or generate a summary sentence from scratch.
Experiments demonstrate that our adaptive approach outperforms the pure
abstractive and rewriting baselines on various benchmark datasets, especially
when the dataset has a balanced distribution of styles. Interestingly,
empirical results show that the human writing style of each summary sentence is
consistently predictable given its context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArmanTTS single-speaker Persian dataset. (arXiv:2304.03585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03585">
<div class="article-summary-box-inner">
<span><p>TTS, or text-to-speech, is a complicated process that can be accomplished
through appropriate modeling using deep learning methods. In order to implement
deep learning models, a suitable dataset is required. Since there is a scarce
amount of work done in this field for the Persian language, this paper will
introduce the single speaker dataset: ArmanTTS. We compared the characteristics
of this dataset with those of various prevalent datasets to prove that ArmanTTS
meets the necessary standards for teaching a Persian text-to-speech conversion
model. We also combined the Tacotron 2 and HiFi GAN to design a model that can
receive phonemes as input, with the output being the corresponding speech. 4.0
value of MOS was obtained from real speech, 3.87 value was obtained by the
vocoder prediction and 2.98 value was reached with the synthetic speech
generated by the TTS model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03609">
<div class="article-summary-box-inner">
<span><p>Current literature demonstrates that Large Language Models (LLMs) are great
few-shot learners, and prompting significantly increases their performance on a
range of downstream tasks in a few-shot learning setting. An attempt to
automate human-led prompting followed, with some progress achieved. In
particular, subsequent work demonstrates automation can outperform fine-tuning
in certain K-shot learning scenarios.
</p>
<p>In this paper, we revisit techniques for automated prompting on six different
downstream tasks and a larger range of K-shot learning settings. We find that
automated prompting does not consistently outperform simple manual prompts. Our
work suggests that, in addition to fine-tuning, manual prompts should be used
as a baseline in this line of research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory. (arXiv:2304.03612v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03612">
<div class="article-summary-box-inner">
<span><p>There has been concern about ideological basis and possible discrimination in
text generated by Large Language Models (LLMs). We test possible value biases
in ChatGPT using a psychological value theory. We designed a simple experiment
in which we used a number of different probes derived from the Schwartz basic
value theory (items from the revised Portrait Value Questionnaire, the value
type definitions, value names). We prompted ChatGPT via the OpenAI API
repeatedly to generate text and then analyzed the generated corpus for value
content with a theory-driven value dictionary using a bag of words approach.
Overall, we found little evidence of explicit value bias. The results showed
sufficient construct and discriminant validity for the generated text in line
with the theoretical predictions of the psychological model, which suggests
that the value content was carried through into the outputs with high fidelity.
We saw some merging of socially oriented values, which may suggest that these
values are less clearly differentiated at a linguistic level or alternatively,
this mixing may reflect underlying universal human motivations. We outline some
possible applications of our findings for both applications of ChatGPT for
corporate usage and policy making as well as future research avenues. We also
highlight possible implications of this relatively high-fidelity replication of
motivational content using a linguistic model for the theorizing about human
values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks. (arXiv:2304.03639v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03639">
<div class="article-summary-box-inner">
<span><p>Previous work has established that RNNs with an unbounded activation function
have the capacity to count exactly. However, it has also been shown that RNNs
are challenging to train effectively and generally do not learn exact counting
behaviour. In this paper, we focus on this problem by studying the simplest
possible RNN, a linear single-cell network. We conduct a theoretical analysis
of linear RNNs and identify conditions for the models to exhibit exact counting
behaviour. We provide a formal proof that these conditions are necessary and
sufficient. We also conduct an empirical analysis using tasks involving a
Dyck-1-like Balanced Bracket language under two different settings. We observe
that linear RNNs generally do not meet the necessary and sufficient conditions
for counting behaviour when trained with the standard approach. We investigate
how varying the length of training sequences and utilising different target
classes impacts model behaviour during training and the ability of linear RNN
models to effectively approximate the indicator conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03682">
<div class="article-summary-box-inner">
<span><p>Coreference Resolution is a well studied problem in NLP. While widely studied
for English and other resource-rich languages, research on coreference
resolution in Bengali largely remains unexplored due to the absence of relevant
datasets. Bengali, being a low-resource language, exhibits greater
morphological richness compared to English. In this article, we introduce a new
dataset, BenCoref, comprising coreference annotations for Bengali texts
gathered from four distinct domains. This relatively small dataset contains
5200 mention annotations forming 502 mention clusters within 48,569 tokens. We
describe the process of creating this dataset and report performance of
multiple models trained using BenCoref. We anticipate that our work sheds some
light on the variations in coreference phenomena across multiple domains in
Bengali and encourages the development of additional resources for Bengali.
Furthermore, we found poor crosslingual performance at zero-shot setting from
English, highlighting the need for more language-specific resources for this
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Contrastive Loss in Multimodal Learning. (arXiv:2304.03717v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03717">
<div class="article-summary-box-inner">
<span><p>Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021))
have received huge success in multimodal learning, where the model tries to
minimize the distance between the representations of different views (e.g.,
image and its caption) of the same data point while keeping the representations
of different data points away from each other. However, from a theoretical
perspective, it is unclear how contrastive learning can learn the
representations from different views efficiently, especially when the data is
not isotropic. In this work, we analyze the training dynamics of a simple
multimodal contrastive learning model and show that contrastive pairs are
important for the model to efficiently balance the learned representations. In
particular, we show that the positive pairs will drive the model to align the
representations at the cost of increasing the condition number, while the
negative pairs will reduce the condition number, keeping the learned
representations balanced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Unified Language Checking. (arXiv:2304.03728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03728">
<div class="article-summary-box-inner">
<span><p>Despite recent concerns about undesirable behaviors generated by large
language models (LLMs), including non-factual, biased, and hateful language, we
find LLMs are inherent multi-task language checkers based on their latent
representations of natural and social knowledge. We present an interpretable,
unified, language checking (UniLC) method for both human and machine-generated
language that aims to check if language input is factual and fair. While
fairness and fact-checking tasks have been handled separately with dedicated
models, we find that LLMs can achieve high performance on a combination of
fact-checking, stereotype detection, and hate speech detection tasks with a
simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task
language checking method proposed in this work, the GPT3.5-turbo model
outperforms fully supervised baselines on several language tasks. The simple
approach and results suggest that based on strong latent knowledge
representations, an LLM can be an adaptive and explainable tool for detecting
misinformation, stereotypes, and hate speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing. (arXiv:2304.03730v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03730">
<div class="article-summary-box-inner">
<span><p>Currently, human-bot symbiosis dialog systems, e.g., pre- and after-sales in
E-commerce, are ubiquitous, and the dialog routing component is essential to
improve the overall efficiency, reduce human resource cost, and enhance user
experience. Although most existing methods can fulfil this requirement, they
can only model single-source dialog data and cannot effectively capture the
underlying knowledge of relations among data and subtasks. In this paper, we
investigate this important problem by thoroughly mining both the data-to-task
and task-to-task knowledge among various kinds of dialog data. To achieve the
above targets, we propose a Gated Mechanism enhanced Multi-task Model (G3M),
specifically including a novel dialog encoder and two tailored gated mechanism
modules. The proposed method can play the role of hierarchical information
filtering and is non-invasive to existing dialog systems. Based on two datasets
collected from real world applications, extensive experimental results
demonstrate the effectiveness of our method, which achieves the
state-of-the-art performance by improving 8.7\%/11.8\% on RMSE metric and
2.2\%/4.4\% on F1 metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models. (arXiv:2304.03738v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03738">
<div class="article-summary-box-inner">
<span><p>As the capabilities of generative language models continue to advance, the
implications of biases ingrained within these models have garnered increasing
attention from researchers, practitioners, and the broader public. This article
investigates the challenges and risks associated with biases in large-scale
language models like ChatGPT. We discuss the origins of biases, stemming from,
among others, the nature of training data, model specifications, algorithmic
constraints, product design, and policy decisions. We explore the ethical
concerns arising from the unintended consequences of biased model outputs. We
further analyze the potential opportunities to mitigate biases, the
inevitability of some biases, and the implications of deploying these models in
various applications, such as virtual assistants, content generation, and
chatbots. Finally, we review the current approaches to identify, quantify, and
mitigate biases in language models, emphasizing the need for a
multi-disciplinary, collaborative effort to develop more equitable,
transparent, and responsible AI systems. This article aims to stimulate a
thoughtful dialogue within the artificial intelligence community, encouraging
researchers and developers to reflect on the role of biases in generative
language models and the ongoing pursuit of ethical AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering. (arXiv:2304.03754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03754">
<div class="article-summary-box-inner">
<span><p>Causal Video Question Answering (CVidQA) queries not only association or
temporal relations but also causal relations in a video. Existing question
synthesis methods pre-trained question generation (QG) systems on reading
comprehension datasets with text descriptions as inputs. However, QG models
only learn to ask association questions (e.g., ``what is someone doing...'')
and result in inferior performance due to the poor transfer of association
knowledge to CVidQA, which focuses on causal questions like ``why is someone
doing ...''. Observing this, we proposed to exploit causal knowledge to
generate question-answer pairs, and proposed a novel framework, Causal
Knowledge Extraction from Language Models (CaKE-LM), leveraging causal
commonsense knowledge from language models to tackle CVidQA. To extract
knowledge from LMs, CaKE-LM generates causal questions containing two events
with one triggering another (e.g., ``score a goal'' triggers ``soccer player
kicking ball'') by prompting LM with the action (soccer player kicking ball) to
retrieve the intention (to score a goal). CaKE-LM significantly outperforms
conventional methods by 4% to 6% of zero-shot CVidQA accuracy on NExT-QA and
Causal-VidQA datasets. We also conduct comprehensive analyses and provide key
findings for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01140">
<div class="article-summary-box-inner">
<span><p>The rapid mutation of the influenza virus threatens public health.
Reassortment among viruses with different hosts can lead to a fatal pandemic.
However, it is difficult to detect the original host of the virus during or
after an outbreak as influenza viruses can circulate between different species.
Therefore, early and rapid detection of the viral host would help reduce the
further spread of the virus. We use various machine learning models with
features derived from the position-specific scoring matrix (PSSM) and features
learned from word embedding and word encoding to infer the origin host of
viruses. The results show that the performance of the PSSM-based model reaches
the MCC around 95%, and the F1 around 96%. The MCC obtained using the model
with word embedding is around 96%, and the F1 is around 97%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Pre-Trained Language Models for Cross-Cultural Differences in Values. (arXiv:2203.13722v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13722">
<div class="article-summary-box-inner">
<span><p>Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making first order linear logic a generating grammar. (arXiv:2206.08955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08955">
<div class="article-summary-box-inner">
<span><p>It is known that different categorial grammars have surface representation in
a fragment of first order multiplicative linear logic. We show that the
fragment of interest is equivalent to the recently introduced {\it extended
tensor type calculus}. This provides the former not only with some alternative
syntax and intuitive geometric representation, but also with an intrinsic
deductive system, which has been absent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners. (arXiv:2206.12036v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12036">
<div class="article-summary-box-inner">
<span><p>Sentence completion (SC) questions present a sentence with one or more blanks
that need to be filled in, three to five possible words or phrases as options.
SC questions are widely used for students learning English as a Second Language
(ESL). In this paper, we present a large-scale SC dataset, \textsc{SC-Ques},
which is made up of 289,148 ESL SC questions from real-world standardized
English examinations. Furthermore, we build a comprehensive benchmark of
automatically solving the SC questions by training the large-scale pre-trained
language models on the proposed \textsc{SC-Ques} dataset. We conduct detailed
analysis of the baseline models performance, limitations and trade-offs. The
data and our code are available for research purposes from:
\url{https://github.com/ai4ed/SC-Ques}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01562">
<div class="article-summary-box-inner">
<span><p>Neural language models (LMs) have achieved impressive results on various
language-based reasoning tasks by utilizing latent knowledge encoded in their
own pretrained parameters. To make this reasoning process more explicit, recent
works retrieve a rationalizing LM's internal knowledge by training or prompting
it to generate free-text rationales, which can be used to guide task
predictions made by either the same LM or a separate reasoning LM. However,
rationalizing LMs require expensive rationale annotation and/or computation,
without any assurance that their generated rationales improve LM task
performance or faithfully reflect LM decision-making. In this paper, we propose
PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns
to faithfully reason over rationales via counterfactual regularization. First,
PINTO maps out a suitable reasoning process for the task input by prompting a
frozen rationalizing LM to generate a free-text rationale. Second, PINTO's
reasoning LM is fine-tuned to solve the task using the generated rationale as
context, while regularized to output less confident predictions when the
rationale is perturbed. Across four datasets, we show that PINTO significantly
improves the generalization ability of the reasoning LM, yielding higher
performance on both in-distribution and out-of-distribution test sets. Also, we
find that PINTO's rationales are more faithful to its task predictions than
those generated by competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Identification of Eviction Status from Electronic Health Record Notes. (arXiv:2212.02762v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02762">
<div class="article-summary-box-inner">
<span><p>Objective: Evictions are important social and behavioral determinants of
health. Evictions are associated with a cascade of negative events that can
lead to unemployment, housing insecurity/homelessness, long-term poverty, and
mental health problems. In this study, we developed a natural language
processing system to automatically detect eviction status from electronic
health record (EHR) notes.
</p>
<p>Materials and Methods: We first defined eviction status (eviction presence
and eviction period) and then annotated eviction status in 5000 EHR notes from
the Veterans Health Administration (VHA). We developed a novel model, KIRESH,
that has shown to substantially outperform other state-of-the-art models such
as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.
Moreover, we designed a novel prompt to further improve the model performance
by using the intrinsic connection between the two sub-tasks of eviction
presence and period prediction. Finally, we used the Temperature Scaling-based
Calibration on our KIRESH-Prompt method to avoid over-confidence issues arising
from the imbalance dataset.
</p>
<p>Results: KIRESH-Prompt substantially outperformed strong baseline models
including fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153
Macro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,
0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also
conducted additional experiments on a benchmark social determinants of health
(SBDH) dataset to demonstrate the generalizability of our methods.
</p>
<p>Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction
status classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an
eviction surveillance system to help address the US Veterans' housing
insecurity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05612">
<div class="article-summary-box-inner">
<span><p>In the current context where online platforms have been effectively
weaponized in a variety of geo-political events and social issues, Internet
memes make fair content moderation at scale even more difficult. Existing work
on meme classification and tracking has focused on black-box methods that do
not explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04347">
<div class="article-summary-box-inner">
<span><p>Recently, language models have demonstrated strong performance on various
natural language understanding tasks. Language models trained on large
human-generated corpus encode not only a significant amount of human knowledge,
but also the human stereotype. As more and more downstream tasks have
integrated language models as part of the pipeline, it is necessary to
understand the internal stereotypical representation in order to design the
methods for mitigating the negative effects. In this paper, we use
counterexamples to examine the internal stereotypical knowledge in pre-trained
language models (PLMs) that can lead to stereotypical preference. We mainly
focus on gender stereotypes, but the method can be extended to other types of
stereotype. We evaluate 7 PLMs on 9 types of cloze-style prompt with different
information and base knowledge. The results indicate that PLMs show a certain
amount of robustness against unrelated information and preference of shallow
linguistic cues, such as word position and syntactic structure, but a lack of
interpreting information by meaning. Such findings shed light on how to
interact with PLMs in a neutral approach for both finetuning and evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09051">
<div class="article-summary-box-inner">
<span><p>This paper reviews the state-of-the-art of language models architectures and
strategies for "complex" question-answering (QA, CQA, CPS) with a focus on
hybridization. Large Language Models (LLM) are good at leveraging public data
on standard problems but once you want to tackle more specific complex
questions or problems (e.g. How does the concept of personal freedom vary
between different cultures ? What is the best mix of power generation methods
to reduce climate change ?) you may need specific architecture, knowledge,
skills, methods, sensitive data protection, explainability, human approval and
versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed
non-specialists to grasp the great potential as well as the equally strong
limitations of LLM in complex QA. In this paper, we start by reviewing required
skills and evaluation techniques. We integrate findings from the robust
community edited research papers BIG, BLOOM and HELM which open source,
benchmark and analyze limits and challenges of LLM in terms of tasks complexity
and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as
a baseline. We discuss some challenges associated with complex QA, including
domain adaptation, decomposition and efficient multi-step QA, long form and
non-factoid QA, safety and multi-sensitivity data protection, multimodal
search, hallucinations, explainability and truthfulness, temporal reasoning. We
analyze current solutions and promising research trends, using elements such
as: hybrid LLM architectural patterns, training and prompting strategies,
active human reinforcement learning supervised with AI, neuro-symbolic and
structured knowledge grounding, program synthesis, iterated decomposition and
others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02468">
<div class="article-summary-box-inner">
<span><p>We study the influence of different activation functions in the output layer
of deep neural network models for soft and hard label prediction in the
learning with disagreement task. In this task, the goal is to quantify the
amount of disagreement via predicting soft labels. To predict the soft labels,
we use BERT-based preprocessors and encoders and vary the activation function
used in the output layer, while keeping other parameters constant. The soft
labels are then used for the hard label prediction. The activation functions
considered are sigmoid as well as a step-function that is added to the model
post-training and a sinusoidal activation function, which is introduced for the
first time in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.05737">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 18 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoXum: Cross-modal Visual and Textural Summarization of Videos. (arXiv:2303.12060v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12060">
<div class="article-summary-box-inner">
<span><p>Video summarization aims to distill the most important information from a
source video to produce either an abridged clip or a textual narrative.
Traditionally, different methods have been proposed depending on whether the
output is a video or text, thus ignoring the correlation between the two
semantically related tasks of visual summarization and textual summarization.
We propose a new joint video and text summarization task. The goal is to
generate both a shortened video clip along with the corresponding textual
summary from a long video, collectively referred to as a cross-modal summary.
The generated shortened video clip and text narratives should be semantically
well aligned. To this end, we first build a large-scale human-annotated dataset
-- VideoXum (X refers to different modalities). The dataset is reannotated
based on ActivityNet. After we filter out the videos that do not meet the
length requirements, 14,001 long videos remain in our new dataset. Each video
in our reannotated dataset has human-annotated video summaries and the
corresponding narrative summaries. We then design a novel end-to-end model --
VTSUM-BILP to address the challenges of our proposed task. Moreover, we propose
a new metric called VT-CLIPScore to help evaluate the semantic consistency of
cross-modality summary. The proposed model achieves promising performance on
this new task and establishes a benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16634">
<div class="article-summary-box-inner">
<span><p>The quality of texts generated by natural language generation (NLG) systems
is hard to measure automatically. Conventional reference-based metrics, such as
BLEU and ROUGE, have been shown to have relatively low correlation with human
judgments, especially for tasks that require creativity and diversity. Recent
studies suggest using large language models (LLMs) as reference-free metrics
for NLG evaluation, which have the benefit of being applicable to new tasks
that lack human references. However, these LLM-based evaluators still have
lower human correspondence than medium-size neural evaluators. In this work, we
present G-Eval, a framework of using large language models with
chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of
NLG outputs. We experiment with two generation tasks, text summarization and
dialogue generation. We show that G-Eval with GPT-4 as the backbone model
achieves a Spearman correlation of 0.514 with human on summarization task,
outperforming all previous methods by a large margin. We also propose
preliminary analysis on the behavior of LLM-based evaluators, and highlight the
potential issue of LLM-based evaluators having a bias towards the LLM-generated
texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02017">
<div class="article-summary-box-inner">
<span><p>Large language models have revolutionized the field of artificial
intelligence and have been used in various applications. Among these models,
ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,
it stands out as a powerful tool that has been widely adopted. ChatGPT has been
successfully applied in numerous areas, including chatbots, content generation,
language translation, personalized recommendations, and even medical diagnosis
and treatment. Its success in these applications can be attributed to its
ability to generate human-like responses, understand natural language, and
adapt to different contexts. Its versatility and accuracy make it a powerful
tool for natural language processing (NLP). However, there are also limitations
to ChatGPT, such as its tendency to produce biased responses and its potential
to perpetuate harmful language patterns. This article provides a comprehensive
overview of ChatGPT, its applications, advantages, and limitations.
Additionally, the paper emphasizes the importance of ethical considerations
when using this robust tool in real-world scenarios. Finally, This paper
contributes to ongoing discussions surrounding artificial intelligence and its
impact on vision and NLP domains by providing insights into prompt engineering
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03047">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation is a task that requires an agent to follow
instructions to navigate in environments. It becomes increasingly crucial in
the field of embodied AI, with potential applications in autonomous navigation,
search and rescue, and human-robot interaction. In this paper, we propose to
address a more practical yet challenging counterpart setting - vision-language
navigation in continuous environments (VLN-CE). To develop a robust VLN-CE
agent, we propose a new navigation framework, ETPNav, which focuses on two
critical skills: 1) the capability to abstract environments and generate
long-range navigation plans, and 2) the ability of obstacle-avoiding control in
continuous environments. ETPNav performs online topological mapping of
environments by self-organizing predicted waypoints along a traversed path,
without prior environmental experience. It privileges the agent to break down
the navigation procedure into high-level planning and low-level control.
Concurrently, ETPNav utilizes a transformer-based cross-modal planner to
generate navigation plans based on topological maps and instructions. The plan
is then performed through an obstacle-avoiding controller that leverages a
trial-and-error heuristic to prevent navigation from getting stuck in
obstacles. Experimental results demonstrate the effectiveness of the proposed
method. ETPNav yields more than 10% and 20% improvements over prior
state-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is
available at https://github.com/MarSaKi/ETPNav.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03216">
<div class="article-summary-box-inner">
<span><p>In this work, we study how the generalization performance of a given
direction changes with its sampling ratio in Multilingual Neural Machine
Translation (MNMT). By training over 200 multilingual models with various model
sizes, directions, and total numbers of tasks, we find that scalarization leads
to a multitask trade-off front that deviates from the traditional Pareto front
when there exists data imbalance in the training corpus. That is, the
performance of certain translation directions does not improve with the
increase of its weight in the multi-task optimization objective, which poses a
great challenge to improve the overall performance of all directions. Based on
our observations, we propose the Double Power Law to predict the unique
performance trade-off front in MNMT, which is robust across various languages,
data adequacy, and the number of tasks. Finally, we formulate the sample ratio
selection problem in MNMT as an optimization problem based on the Double Power
Law, which achieves better performance than temperature searching and gradient
manipulation methods using up to half of the total training budget in our
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models effectively leverage document-level context for literary translation, but critical errors persist. (arXiv:2304.03245v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03245">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are competitive with the state of the art on a
wide range of sentence-level translation datasets. However, their ability to
translate paragraphs and documents remains unexplored because evaluation in
these settings is costly and difficult. We show through a rigorous human
evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an
entire literary paragraph (e.g., from a novel) at once results in
higher-quality translations than standard sentence-by-sentence translation
across 18 linguistically-diverse language pairs (e.g., translating into and out
of Japanese, Polish, and English). Our evaluation, which took approximately 350
hours of effort for annotation and analysis, is conducted by hiring translators
fluent in both the source and target language and asking them to provide both
span-level error annotations as well as preference judgments of which system's
translations are better. We observe that discourse-level LLM translators commit
fewer mistranslations, grammar errors, and stylistic inconsistencies than
sentence-level approaches. With that said, critical errors still abound,
including occasional content omissions, and a human translator's intervention
remains necessary to ensure that the author's voice remains intact. We publicly
release our dataset and error annotations to spur future research on evaluation
of document-level literary translation.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-10 23:11:36.013252711 UTC">2023-04-10 23:11:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>