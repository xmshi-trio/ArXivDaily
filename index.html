<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-09T01:30:00Z">01-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Latent Variable Models for Semi-supervised Paraphrase Generation. (arXiv:2301.02275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02275">
<div class="article-summary-box-inner">
<span><p>This paper explores deep latent variable models for semi-supervised
paraphrase generation, where the missing target pair is modelled as a latent
paraphrase sequence. We present a novel unsupervised model named variational
sequence auto-encoding reconstruction (VSAR), which performs latent sequence
inference given an observed text. To leverage information from text pairs, we
introduce a supervised model named dual directional learning (DDL). Combining
VSAR with DDL (DDL+VSAR) enables us to conduct semi-supervised learning;
however, the combined model suffers from a cold-start problem. To combat this
issue, we propose to deal with better weight initialisation, leading to a
two-stage training scheme named knowledge reinforced training. Our empirical
evaluations suggest that the combined model yields competitive performance
against the state-of-the-art supervised baselines on complete data.
Furthermore, in scenarios where only a fraction of the labelled pairs are
available, our combined model consistently outperforms the strong supervised
model baseline (DDL and Transformer) by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Broadcast News Summarization; a comparative study on Maximal Marginal Relevance (MMR) and Latent Semantic Analysis (LSA). (arXiv:2301.02284v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02284">
<div class="article-summary-box-inner">
<span><p>The methods of automatic speech summarization are classified into two groups:
supervised and unsupervised methods. Supervised methods are based on a set of
features, while unsupervised methods perform summarization based on a set of
rules. Latent Semantic Analysis (LSA) and Maximal Marginal Relevance (MMR) are
considered the most important and well-known unsupervised methods in automatic
speech summarization. This study set out to investigate the performance of two
aforementioned unsupervised methods in transcriptions of Persian broadcast news
summarization. The results show that in generic summarization, LSA outperforms
MMR, and in query-based summarization, MMR outperforms LSA in broadcast news
summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequentially Controlled Text Generation. (arXiv:2301.02299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02299">
<div class="article-summary-box-inner">
<span><p>While GPT-2 generates sentences that are remarkably human-like, longer
documents can ramble and do not follow human-like writing structure. We study
the problem of imposing structure on long-range text. We propose a novel
controlled text generation task, sequentially controlled text generation, and
identify a dataset, NewsDiscourse as a starting point for this task. We develop
a sequential controlled text generation pipeline with generation and editing.
We test different degrees of structural awareness and show that, in general,
more structural awareness results in higher control-accuracy, grammaticality,
coherency and topicality, approaching human-level writing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona. (arXiv:2301.02401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02401">
<div class="article-summary-box-inner">
<span><p>To build a conversational agent that interacts fluently with humans, previous
studies blend knowledge or personal profile into the pre-trained language
model. However, the model that considers knowledge and persona at the same time
is still limited, leading to hallucination and a passive way of using personas.
We propose an effective dialogue agent that grounds external knowledge and
persona simultaneously. The agent selects the proper knowledge and persona to
use for generating the answers with our candidate scoring implemented with a
poly-encoder. Then, our model generates the utterance with lesser hallucination
and more engagingness utilizing retrieval augmented generation with
knowledge-persona enhanced query. We conduct experiments on the
persona-knowledge chat and achieve state-of-the-art performance in grounding
and generation tasks on the automatic metrics. Moreover, we validate the
answers from the models regarding hallucination and engagingness through human
evaluation and qualitative results. We show our retriever's effectiveness in
extracting relevant documents compared to the other previous retrievers, along
with the comparison of multiple candidate scoring methods. Code is available at
https://github.com/dlawjddn803/INFO
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction. (arXiv:2301.02427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02427">
<div class="article-summary-box-inner">
<span><p>We present Mask-then-Fill, a flexible and effective data augmentation
framework for event extraction. Our approach allows for more flexible
manipulation of text and thus can generate more diverse data while keeping the
original event structure unchanged as much as possible. Specifically, it first
randomly masks out an adjunct sentence fragment and then infills a
variable-length text span with a fine-tuned infilling model. The main advantage
lies in that it can replace a fragment of arbitrary length in the text with
another fragment of variable length, compared to the existing methods which can
only replace a single word or a fixed-length fragment. On trigger and argument
extraction tasks, the proposed framework is more effective than baseline
methods and it demonstrates particularly strong results in the low-resource
setting. Our further analysis shows that it achieves a good balance between
diversity and distributional similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topics as Entity Clusters: Entity-based Topics from Language Models and Graph Neural Networks. (arXiv:2301.02458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02458">
<div class="article-summary-box-inner">
<span><p>Topic models aim to reveal the latent structure behind a corpus, typically
conducted over a bag-of-words representation of documents. In the context of
topic modeling, most vocabulary is either irrelevant for uncovering underlying
topics or contains strong relationships with relevant concepts, impacting the
interpretability of these topics. Furthermore, their limited expressiveness and
dependency on language demand considerable computation resources. Hence, we
propose a novel approach for cluster-based topic modeling that employs
conceptual entities. Entities are language-agnostic representations of
real-world concepts rich in relational information. To this end, we extract
vector representations of entities from (i) an encyclopedic corpus using a
language model; and (ii) a knowledge base using a graph neural network. We
demonstrate that our approach consistently outperforms other state-of-the-art
topic models across coherency metrics and find that the explicit knowledge
encoded in the graph-based embeddings provides more coherent topics than the
implicit knowledge encoded with the contextualized embeddings of language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPD@NL4Opt: An ensemble approach for the NER task of the optimization problem. (arXiv:2301.02459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02459">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an ensemble approach for the NL4Opt competition
subtask 1(NER task). For this task, we first fine tune the pretrained language
models based on the competition dataset. Then we adopt differential learning
rates and adversarial training strategies to enhance the model generalization
and robustness. Additionally, we use a model ensemble method for the final
prediction, which achieves a micro-averaged F1 score of 93.3% and attains the
second prize in the NER task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAIDS: A Novel Approach for Sentiment Analysis Informed of Dialect and Sarcasm. (arXiv:2301.02521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02521">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis becomes an essential part of every social network, as it
enables decision-makers to know more about users' opinions in almost all life
aspects. Despite its importance, there are multiple issues it encounters like
the sentiment of the sarcastic text which is one of the main challenges of
sentiment analysis. This paper tackles this challenge by introducing a novel
system (SAIDS) that predicts the sentiment, sarcasm and dialect of Arabic
tweets. SAIDS uses its prediction of sarcasm and dialect as known information
to predict the sentiment. It uses MARBERT as a language model to generate
sentence embedding, then passes it to the sarcasm and dialect models, and then
the outputs of the three models are concatenated and passed to the sentiment
analysis model. Multiple system design setups were experimented with and
reported. SAIDS was applied to the ArSarcasm-v2 dataset where it outperforms
the state-of-the-art model for the sentiment analysis task. By training all
tasks together, SAIDS achieves results of 75.98 FPN, 59.09 F1-score and 71.13
F1-score for sentiment analysis, sarcasm detection, and dialect identification
respectively. The system design can be used to enhance the performance of any
task which is dependent on other tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"No, to the Right" -- Online Language Corrections for Robotic Manipulation via Shared Autonomy. (arXiv:2301.02555v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02555">
<div class="article-summary-box-inner">
<span><p>Systems for language-guided human-robot interaction must satisfy two key
desiderata for broad adoption: adaptivity and learning efficiency.
Unfortunately, existing instruction-following agents cannot adapt, lacking the
ability to incorporate online natural language supervision, and even if they
could, require hundreds of demonstrations to learn even simple policies. In
this work, we address these problems by presenting Language-Informed Latent
Actions with Corrections (LILAC), a framework for incorporating and adapting to
natural language corrections - "to the right," or "no, towards the book" -
online, during execution. We explore rich manipulation domains within a shared
autonomy paradigm. Instead of discrete turn-taking between a human and robot,
LILAC splits agency between the human and robot: language is an input to a
learned model that produces a meaningful, low-dimensional control space that
the human can use to guide the robot. Each real-time correction refines the
human's control space, enabling precise, extended behaviors - with the added
benefit of requiring only a handful of demonstrations to learn. We evaluate our
approach via a user study where users work with a Franka Emika Panda
manipulator to complete complex manipulation tasks. Compared to existing
learned baselines covering both open-loop instruction following and single-turn
shared autonomy, we show that our corrections-aware approach obtains higher
task completion rates, and is subjectively preferred by users because of its
reliability, precision, and ease of use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Categorization of Mental Health Posts using Transformers. (arXiv:2301.02589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02589">
<div class="article-summary-box-inner">
<span><p>With recent developments in digitization of clinical psychology, NLP research
community has revolutionized the field of mental health detection on social
media. Existing research in mental health analysis revolves around the
cross-sectional studies to classify users' intent on social media. For in-depth
analysis, we investigate existing classifiers to solve the problem of causal
categorization which suggests the inefficiency of learning based methods due to
limited training samples. To handle this challenge, we use transformer models
and demonstrate the efficacy of a pre-trained transfer learning on "CAMS"
dataset. The experimental result improves the accuracy and depicts the
importance of identifying cause-and-effect relationships in the underlying
text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LTL under reductions with weaker conditions than stutter-invariance. (arXiv:2111.03342v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03342">
<div class="article-summary-box-inner">
<span><p>Verification of properties expressed as-regular languages such as LTL can
benefit hugely from stutter-insensitivity, using a diverse set of reduction
strategies. However properties that are not stutter-insensitive, for instance
due to the use of the neXt operator of LTL or to some form of counting in the
logic, are not covered by these techniques in general. We propose in this paper
to study a weaker property than stutter-insensitivity. In a stutter insensitive
language both adding and removing stutter to a word does not change its
acceptance, any stuttering can be abstracted away; by decomposing this
equivalence relation into two implications we obtain weaker conditions. We
define a shortening insensitive language where any word that stutters less than
a word in the language must also belong to the language. A lengthening
insensitive language has the dual property. A semi-decision procedure is then
introduced to reliably prove shortening insensitive properties or deny
lengthening insensitive properties while working with a reduction of a system.
A reduction has the property that it can only shorten runs. Lipton's
transaction reductions or Petri net agglomerations are examples of eligible
structural reduction strategies. An implementation and experimental evidence is
provided showing most nonrandom properties sensitive to stutter are actually
shortening or lengthening insensitive. Performance of experiments on a large
(random) benchmark from the model-checking competition indicate that despite
being a semi-decision procedure, the approach can still improve state of the
art verification tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning. (arXiv:2111.12062v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12062">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning algorithms, including BERT and SimCLR, have enabled
significant strides in fields like natural language processing, computer
vision, and speech processing. However, these algorithms are domain-specific,
meaning that new self-supervised learning algorithms must be developed for each
new setting, including myriad healthcare, scientific, and multimodal domains.
To catalyze progress toward domain-agnostic methods, we introduce DABS: a
Domain-Agnostic Benchmark for Self-supervised learning. To perform well on
DABS, an algorithm is evaluated on seven diverse domains: natural images,
multichannel sensor data, English text, speech recordings, multilingual text,
chest x-rays, and images with text descriptions. Each domain contains an
unlabeled dataset for pretraining; the model is then is scored based on its
downstream performance on a set of labeled tasks in the domain. We also present
e-Mix and ShED: two baseline domain-agnostic algorithms; their relatively
modest performance demonstrates that significant progress is needed before
self-supervised learning is an out-of-the-box solution for arbitrary domains.
Code for benchmark datasets and baseline algorithms is available at
https://github.com/alextamkin/dabs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Based Automatic Personality Prediction Using KGrAt-Net; A Knowledge Graph Attention Network Classifier. (arXiv:2205.13780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13780">
<div class="article-summary-box-inner">
<span><p>Nowadays, a tremendous amount of human communications occur on Internet-based
communication infrastructures, like social networks, email, forums,
organizational communication platforms, etc. Indeed, the automatic prediction
or assessment of individuals' personalities through their written or exchanged
text would be advantageous to ameliorate their relationships. To this end, this
paper aims to propose KGrAt-Net, which is a Knowledge Graph Attention Network
text classifier. For the first time, it applies the knowledge graph attention
network to perform Automatic Personality Prediction (APP), according to the Big
Five personality traits. After performing some preprocessing activities, it
first tries to acquire a knowing-full representation of the knowledge behind
the concepts in the input text by building its equivalent knowledge graph. A
knowledge graph collects interlinked descriptions of concepts, entities, and
relationships in a machine-readable form. Practically, it provides a
machine-readable cognitive understanding of concepts and semantic relationships
among them. Then, applying the attention mechanism, it attempts to pay
attention to the most relevant parts of the graph to predict the personality
traits of the input text. We used 2,467 essays from the Essays Dataset. The
results demonstrated that KGrAt-Net considerably improved personality
prediction accuracies (up to 70.26% on average). Furthermore, KGrAt-Net also
uses knowledge graph embedding to enrich the classification, which makes it
even more accurate (on average, 72.41%) in APP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking zero-shot and few-shot approaches for tokenization, tagging, and dependency parsing of Tagalog text. (arXiv:2208.01814v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.01814">
<div class="article-summary-box-inner">
<span><p>The grammatical analysis of texts in any written language typically involves
a number of basic processing tasks, such as tokenization, morphological
tagging, and dependency parsing. State-of-the-art systems can achieve high
accuracy on these tasks for languages with large datasets, but yield poor
results for languages which have little to no annotated data. To address this
issue for the Tagalog language, we investigate the use of alternative language
resources for creating task-specific models in the absence of
dependency-annotated Tagalog data. We also explore the use of word embeddings
and data augmentation to improve performance when only a small amount of
annotated Tagalog data is available. We show that these zero-shot and few-shot
approaches yield substantial improvements on grammatical analysis of both
in-domain and out-of-domain Tagalog text compared to state-of-the-art
supervised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Political Rhetoric with Epistemic Stance Detection. (arXiv:2212.14486v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14486">
<div class="article-summary-box-inner">
<span><p>Participants in political discourse employ rhetorical strategies -- such as
hedging, attributions, or denials -- to display varying degrees of belief
commitments to claims proposed by themselves or others. Traditionally,
political scientists have studied these epistemic phenomena through
labor-intensive manual content analysis. We propose to help automate such work
through epistemic stance prediction, drawn from research in computational
semantics, to distinguish at the clausal level what is asserted, denied, or
only ambivalently suggested by the author or other mentioned entities (belief
holders). We first develop a simple RoBERTa-based model for multi-source stance
predictions that outperforms more complex state-of-the-art modeling. Then we
demonstrate its novel application to political science by conducting a
large-scale analysis of the Mass Market Manifestos corpus of U.S. political
opinion books, where we characterize trends in cited belief holders --
respected allies and opposed bogeymen -- across U.S. political ideologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion-Cause Pair Extraction as Question Answering. (arXiv:2301.01982v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01982">
<div class="article-summary-box-inner">
<span><p>The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all
potential emotion-cause pairs of a document without any annotation of emotion
or cause clauses. Previous approaches on ECPE have tried to improve
conventional two-step processing schemes by using complex architectures for
modeling emotion-cause interaction. In this paper, we cast the ECPE task to the
question answering (QA) problem and propose simple yet effective BERT-based
solutions to tackle it. Given a document, our Guided-QA model first predicts
the best emotion clause using a fixed question. Then the predicted emotion is
used as a question to predict the most potential cause for the emotion. We
evaluate our model on a standard ECPE corpus. The experimental results show
that despite its simplicity, our Guided-QA achieves promising results and is
easy to reproduce. The code of Guided-QA is also provided.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-09 23:12:52.219249525 UTC">2023-01-09 23:12:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>