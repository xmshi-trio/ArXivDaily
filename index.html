<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-28T01:30:00Z">10-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Text Representation Learning with Information-Theoretic Perspective for Adversarial Robustness. (arXiv:2210.14957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14957">
<div class="article-summary-box-inner">
<span><p>Adversarial vulnerability remains a major obstacle to constructing reliable
NLP systems. When imperceptible perturbations are added to raw input text, the
performance of a deep learning model may drop dramatically under attacks.
Recent work argues the adversarial vulnerability of the model is caused by the
non-robust features in supervised training. Thus in this paper, we tackle the
adversarial robustness challenge from the view of disentangled representation
learning, which is able to explicitly disentangle robust and non-robust
features in text. Specifically, inspired by the variation of information (VI)
in information theory, we derive a disentangled learning objective composed of
mutual information to represent both the semantic representativeness of latent
embeddings and differentiation of robust and non-robust features. On the basis
of this, we design a disentangled learning network to estimate these mutual
information. Experiments on text classification and entailment tasks show that
our method significantly outperforms the representative methods under
adversarial attacks, indicating that discarding non-robust features is critical
for improving adversarial robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's Different between Visual Question Answering for Machine "Understanding" Versus for Accessibility?. (arXiv:2210.14966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14966">
<div class="article-summary-box-inner">
<span><p>In visual question answering (VQA), a machine must answer a question given an
associated image. Recently, accessibility researchers have explored whether VQA
can be deployed in a real-world setting where users with visual impairments
learn about their environment by capturing their visual surroundings and asking
questions. However, most of the existing benchmarking datasets for VQA focus on
machine "understanding" and it remains unclear how progress on those datasets
corresponds to improvements in this real-world use case. We aim to answer this
question by evaluating discrepancies between machine "understanding" datasets
(VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA
models. Based on our findings, we discuss opportunities and challenges in VQA
for accessibility and suggest directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MABEL: Attenuating Gender Bias using Textual Entailment Data. (arXiv:2210.14975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14975">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models encode undesirable social biases, which are
further exacerbated in downstream use. To this end, we propose MABEL (a Method
for Attenuating Gender Bias using Entailment Labels), an intermediate
pre-training approach for mitigating gender bias in contextualized
representations. Key to our approach is the use of a contrastive learning
objective on counterfactually augmented, gender-balanced entailment pairs from
natural language inference (NLI) datasets. We also introduce an alignment
regularizer that pulls identical entailment pairs along opposite gender
directions closer. We extensively evaluate our approach on intrinsic and
extrinsic metrics, and show that MABEL outperforms previous task-agnostic
debiasing approaches in terms of fairness. It also preserves task performance
after fine-tuning on downstream tasks. Together, these findings demonstrate the
suitability of NLI data as an effective means of bias mitigation, as opposed to
only using unlabeled sentences in the literature. Finally, we identify that
existing approaches often use evaluation settings that are insufficient or
inconsistent. We make an effort to reproduce and compare previous methods, and
call for unifying the evaluation settings across gender debiasing methods for
better future comparison.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Domain Adaptation for Pre-trained Multilingual Neural Machine Translation Models. (arXiv:2210.14979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14979">
<div class="article-summary-box-inner">
<span><p>Recent literature has demonstrated the potential of multilingual Neural
Machine Translation (mNMT) models. However, the most efficient models are not
well suited to specialized industries. In these cases, internal data is scarce
and expensive to find in all language pairs. Therefore, fine-tuning a mNMT
model on a specialized domain is hard. In this context, we decided to focus on
a new task: Domain Adaptation of a pre-trained mNMT model on a single pair of
language while trying to maintain model quality on generic domain data for all
language pairs. The risk of loss on generic domain and on other pairs is high.
This task is key for mNMT model adoption in the industry and is at the border
of many others. We propose a fine-tuning procedure for the generic mNMT that
combines embeddings freezing and adversarial loss. Our experiments demonstrated
that the procedure improves performances on specialized data with a minimal
loss in initial performances on generic domain for all languages pairs,
compared to a naive standard approach (+10.0 BLEU score on specialized data,
-0.01 to -0.5 BLEU on WMT and Tatoeba datasets on the other pairs with M2M100).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models are not zero-shot communicators. (arXiv:2210.14986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14986">
<div class="article-summary-box-inner">
<span><p>Despite widespread use of LLMs as conversational agents, evaluations of
performance fail to capture a crucial aspect of communication: interpreting
language in context. Humans interpret language using beliefs and prior
knowledge about the world. For example, we intuitively understand the response
"I wore gloves" to the question "Did you leave fingerprints?" as meaning "No".
To investigate whether LLMs have the ability to make this type of inference,
known as an implicature, we design a simple task and evaluate widely used
state-of-the-art models. We find that, despite only evaluating on utterances
that require a binary inference (yes or no), most perform close to random.
Models adapted to be "aligned with human intent" perform much better, but still
show a significant gap with human performance. We present our findings as the
starting point for further research into evaluating how LLMs interpret language
in context and to drive the development of more pragmatic and useful models of
human discourse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TPU-MLIR: A Compiler For TPU Using MLIR. (arXiv:2210.15016v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15016">
<div class="article-summary-box-inner">
<span><p>Multi-level intermediate representations (MLIR) show great promise for
reducing the cost of building domain-specific compilers by providing a reusable
and extensible compiler infrastructure. This work presents TPU-MLIR, an
end-to-end compiler based on MLIR that deploys pre-trained neural network (NN)
models to a custom ASIC called a Tensor Processing Unit (TPU). TPU-MLIR defines
two new dialects to implement its functionality: 1. a Tensor operation (TOP)
dialect that encodes the deep learning graph semantics and independent of the
deep learning framework and 2. a TPU kernel dialect to provide a standard
kernel computation on TPU. A NN model is translated to the TOP dialect and then
lowered to the TPU dialect for different TPUs according to the chip's
configuration. We demonstrate how to use the MLIR pass pipeline to organize and
perform optimization on TPU to generate machine code. The paper also presents a
verification procedure to ensure the correctness of each transform stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems. (arXiv:2210.15037v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15037">
<div class="article-summary-box-inner">
<span><p>For vision-and-language reasoning tasks, both fully connectionist, end-to-end
methods and hybrid, neuro-symbolic methods have achieved high in-distribution
performance. In which out-of-distribution settings does each paradigm excel? We
investigate this question on both single-image and multi-image visual
question-answering through four types of generalization tests: a novel
segment-combine test for multi-image queries, contrast set, compositional
generalization, and cross-benchmark transfer. Vision-and-language end-to-end
trained systems exhibit sizeable performance drops across all these tests.
Neuro-symbolic methods suffer even more on cross-benchmark transfer from GQA to
VQA, but they show smaller accuracy drops on the other generalization tests and
their performance quickly improves by few-shot training. Overall, our results
demonstrate the complementary benefits of these two paradigms, and emphasize
the importance of using a diverse suite of generalization tests to fully
characterize model robustness to distribution shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EW-Tune: A Framework for Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15042">
<div class="article-summary-box-inner">
<span><p>Pre-trained Large Language Models (LLMs) are an integral part of modern AI
that have led to breakthrough performances in complex AI tasks. Major AI
companies with expensive infrastructures are able to develop and train these
large models with billions and millions of parameters from scratch. Third
parties, researchers, and practitioners are increasingly adopting these
pre-trained models and fine-tuning them on their private data to accomplish
their downstream AI tasks. However, it has been shown that an adversary can
extract/reconstruct the exact training samples from these LLMs, which can lead
to revealing personally identifiable information. The issue has raised deep
concerns about the privacy of LLMs. Differential privacy (DP) provides a
rigorous framework that allows adding noise in the process of training or
fine-tuning LLMs such that extracting the training data becomes infeasible
(i.e., with a cryptographically small success probability). While the
theoretical privacy guarantees offered in most extant studies assume learning
models from scratch through many training iterations in an asymptotic setting,
this assumption does not hold in fine-tuning scenarios in which the number of
training iterations is significantly smaller. To address the gap, we present
\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with
finite-sample privacy guarantees. Our results across four well-established
natural language understanding (NLU) tasks show that while \ewtune~adds privacy
guarantees to LLM fine-tuning process, it directly contributes to decreasing
the induced noise to up to 5.6\% and improves the state-of-the-art LLMs
performance by up to 1.1\% across all NLU tasks. We have open-sourced our
implementations for wide adoption and public testing purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyREx: Dynamic Query Representation for Extractive Question Answering. (arXiv:2210.15048v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15048">
<div class="article-summary-box-inner">
<span><p>Extractive question answering (ExQA) is an essential task for Natural
Language Processing. The dominant approach to ExQA is one that represents the
input sequence tokens (question and passage) with a pre-trained transformer,
then uses two learned query vectors to compute distributions over the start and
end answer span positions. These query vectors lack the context of the inputs,
which can be a bottleneck for the model performance. To address this problem,
we propose \textit{DyREx}, a generalization of the \textit{vanilla} approach
where we dynamically compute query vectors given the input, using an attention
mechanism through transformer layers. Empirical observations demonstrate that
our approach consistently improves the performance over the standard one. The
code and accompanying files for running the experiments are available at
\url{https://github.com/urchade/DyReX}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Four-in-One: A Joint Approach to Inverse Text Normalization, Punctuation, Capitalization, and Disfluency for Automatic Speech Recognition. (arXiv:2210.15063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15063">
<div class="article-summary-box-inner">
<span><p>Features such as punctuation, capitalization, and formatting of entities are
important for readability, understanding, and natural language processing
tasks. However, Automatic Speech Recognition (ASR) systems produce spoken-form
text devoid of formatting, and tagging approaches to formatting address just
one or two features at a time. In this paper, we unify spoken-to-written text
conversion via a two-stage process: First, we use a single transformer tagging
model to jointly produce token-level tags for inverse text normalization (ITN),
punctuation, capitalization, and disfluencies. Then, we apply the tags to
generate written-form text and use weighted finite state transducer (WFST)
grammars to format tagged ITN entity spans. Despite joining four models into
one, our unified tagging approach matches or outperforms task-specific models
across all four tasks on benchmark test sets across several domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">arXivEdits: Understanding the Human Revision Process in Scientific Writing. (arXiv:2210.15067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15067">
<div class="article-summary-box-inner">
<span><p>Scientific publications are the primary means to communicate research
discoveries, where the writing quality is of crucial importance. However, prior
work studying the human editing process in this domain mainly focused on the
abstract or introduction sections, resulting in an incomplete picture. In this
work, we provide a complete computational framework for studying text revision
in scientific writing. We first introduce arXivEdits, a new annotated corpus of
751 full papers from arXiv with gold sentence alignment across their multiple
versions of revision, as well as fine-grained span-level edits and their
underlying intentions for 1,000 sentence pairs. It supports our data-driven
analysis to unveil the common strategies practiced by researchers for revising
their papers. To scale up the analysis, we also develop automatic methods to
extract revision at document-, sentence-, and word-levels. A neural CRF
sentence alignment model trained on our corpus achieves 93.8 F1, enabling the
reliable matching of sentences between different versions. We formulate the
edit extraction task as a span alignment problem, and our proposed method
extracts more fine-grained and explainable edits, compared to the commonly used
diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1
on the fine-grained intent classification task. Our data and system are
released at tiny.one/arxivedits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15088">
<div class="article-summary-box-inner">
<span><p>Persona-based dialogue systems aim to generate consistent responses based on
historical context and predefined persona. Unlike conventional dialogue
generation, the persona-based dialogue needs to consider both dialogue context
and persona, posing a challenge for coherent training. Specifically, this
requires a delicate weight balance between context and persona. To achieve
that, in this paper, we propose an effective framework with Persona-Adaptive
Attention (PAA), which adaptively integrates the weights from the persona and
context information via our designed attention. In addition, a dynamic masking
mechanism is applied to the PAA to not only drop redundant information in
context and persona but also serve as a regularization mechanism to avoid
overfitting. Experimental results demonstrate the superiority of the proposed
PAA framework compared to the strong baselines in both automatic and human
evaluation. Moreover, the proposed PAA approach can perform equivalently well
in a low-resource regime compared to models trained in a full-data setting,
which achieve a similar result with only 20% to 30% of data compared to the
larger models trained in the full-data setting. To fully exploit the
effectiveness of our design, we designed several variants for handling the
weighted information in different ways, showing the necessity and sufficiency
of our weighting and masking designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15097">
<div class="article-summary-box-inner">
<span><p>Likelihood, although useful as a training loss, is a poor search objective
for guiding open-ended generation from language models (LMs). Existing
generation algorithms must avoid both unlikely strings, which are incoherent,
and highly likely ones, which are short and repetitive. We propose contrastive
decoding (CD), a more reliable search objective that returns the difference
between likelihood under a large LM (called the expert, e.g. OPT-13b) and a
small LM (called the amateur, e.g. OPT-125m). CD is inspired by the fact that
the failures of larger LMs are even more prevalent in smaller LMs, and that
this difference signals exactly which texts should be preferred. CD requires
zero training, and produces higher quality text than decoding from the larger
LM alone. It also generalizes across model types (OPT and GPT2) and
significantly outperforms four strong decoding algorithms in automatic and
human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Syntax Complies with the Free-Energy Principle. (arXiv:2210.15098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15098">
<div class="article-summary-box-inner">
<span><p>Natural language syntax yields an unbounded array of hierarchically
structured expressions. We claim that these are used in the service of active
inference in accord with the free-energy principle (FEP). While conceptual
advances alongside modelling and simulation work have attempted to connect
speech segmentation and linguistic communication with the FEP, we extend this
program to the underlying computations responsible for generating syntactic
objects. We argue that recently proposed principles of economy in language
design - such as "minimal search" criteria from theoretical syntax - adhere to
the FEP. This affords a greater degree of explanatory power to the FEP - with
respect to higher language functions - and offers linguistics a grounding in
first principles with respect to computability. We show how both tree-geometric
depth and a Kolmogorov complexity estimate (recruiting a Lempel-Ziv compression
algorithm) can be used to accurately predict legal operations on syntactic
workspaces, directly in line with formulations of variational free energy
minimization. This is used to motivate a general principle of language design
that we term Turing-Chomsky Compression (TCC). We use TCC to align concerns of
linguists with the normative account of self-organization furnished by the FEP,
by marshalling evidence from theoretical linguistics and psycholinguistics to
ground core principles of efficient syntactic computation within active
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRScore: A Novel GPT-based Readability Scorer for ASR Segmentation and Punctuation model evaluation and selection. (arXiv:2210.15104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15104">
<div class="article-summary-box-inner">
<span><p>Punctuation and Segmentation are key to readability in Automatic Speech
Recognition (ASR), often evaluated using F1 scores that require high-quality
human transcripts and do not reflect readability well. Human evaluation is
expensive, time-consuming, and suffers from large inter-observer variability,
especially in conversational speech devoid of strict grammatical structures.
Large pre-trained models capture a notion of grammatical structure. We present
TRScore, a novel readability measure using the GPT model to evaluate different
segmentation and punctuation systems. We validate our approach with human
experts. Additionally, our approach enables quantitative assessment of text
post-processing techniques such as capitalization, inverse text normalization
(ITN), and disfluency on overall readability, which traditional word error rate
(WER) and slot error rate (SER) metrics fail to capture. TRScore is strongly
correlated to traditional F1 and human readability scores, with Pearson's
correlation coefficients of 0.67 and 0.98, respectively. It also eliminates the
need for human transcriptions for model selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Vision-Language Transformer in Fashion. (arXiv:2210.15110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15110">
<div class="article-summary-box-inner">
<span><p>We present a masked vision-language transformer (MVLT) for fashion-specific
multi-modal representation. Technically, we simply utilize vision transformer
architecture for replacing the BERT in the pre-training model, making MVLT the
first end-to-end framework for the fashion domain. Besides, we designed masked
image reconstruction (MIR) for a fine-grained understanding of fashion. MVLT is
an extensible and convenient architecture that admits raw multi-modal inputs
without extra pre-processing models (e.g., ResNet), implicitly modeling the
vision-language alignments. More importantly, MVLT can easily generalize to
various matching and generative tasks. Experimental results show obvious
improvements in retrieval (rank@5: 17%) and recognition (accuracy: 3%) tasks
over the Fashion-Gen 2018 winner Kaleido-BERT. Code is made available at
https://github.com/GewelsJI/MVLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations. (arXiv:2210.15131v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15131">
<div class="article-summary-box-inner">
<span><p>This paper aims to enhance low-resource TTS by reducing training data
requirements using compact speech representations. A Multi-Stage Multi-Codebook
(MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to
waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs
from the text for TTS synthesis. Moreover, we optimize the training strategy by
leveraging more audio to learn MSMCRs better for low-resource languages. It
selects audio from other languages using speaker similarity metric to augment
the training set, and applies transfer learning to improve training quality. In
MOS tests, the proposed system significantly outperforms FastSpeech and VITS in
standard and low-resource scenarios, showing lower data requirements. The
proposed training strategy effectively enhances MSMCRs on waveform
reconstruction. It improves TTS performance further, which wins 77% votes in
the preference test for the low-resource TTS with only 15 minutes of paired
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval. (arXiv:2210.15133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15133">
<div class="article-summary-box-inner">
<span><p>Pre-trained language model (PTM) has been shown to yield powerful text
representations for dense passage retrieval task. The Masked Language Modeling
(MLM) is a major sub-task of the pre-training process. However, we found that
the conventional random masking strategy tend to select a large number of
tokens that have limited effect on the passage retrieval task (e,g. stop-words
and punctuation). By noticing the term importance weight can provide valuable
information for passage retrieval, we hereby propose alternative retrieval
oriented masking (dubbed as ROM) strategy where more important tokens will have
a higher probability of being masked out, to capture this straightforward yet
essential information to facilitate the language model pre-training process.
Notably, the proposed new token masking method will not change the architecture
and learning objective of original PTM. Our experiments verify that the
proposed ROM enables term importance information to help language model
pre-training thus achieving better performance on multiple passage retrieval
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Autoregressive Speech Recognition Models with Limited in-domain Supervision. (arXiv:2210.15135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15135">
<div class="article-summary-box-inner">
<span><p>Advances in self-supervised learning have significantly reduced the amount of
transcribed audio required for training. However, the majority of work in this
area is focused on read speech. We explore limited supervision in the domain of
conversational speech. While we assume the amount of in-domain data is limited,
we augment the model with open source read speech data. The XLS-R model has
been shown to perform well with limited adaptation data and serves as a strong
baseline. We use untranscribed data for self-supervised learning and
semi-supervised training in an autoregressive encoder-decoder model. We
demonstrate that by using the XLS-R model for pseudotranscription, a much
smaller autoregressive model can outperform a finetuned XLS-R model when
transcribed in-domain data is limited, reducing WER by as much as 8% absolute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gendered Mental Health Stigma in Masked Language Models. (arXiv:2210.15144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15144">
<div class="article-summary-box-inner">
<span><p>Mental health stigma prevents many individuals from receiving the appropriate
care, and social psychology studies have shown that mental health tends to be
overlooked in men. In this work, we investigate gendered mental health stigma
in masked language models. In doing so, we operationalize mental health stigma
by developing a framework grounded in psychology research: we use clinical
psychology literature to curate prompts, then evaluate the models' propensity
to generate gendered words. We find that masked language models capture
societal stigma about gender in mental health: models are consistently more
likely to predict female subjects than male in sentences about having a mental
health condition (32% vs. 19%), and this disparity is exacerbated for sentences
that indicate treatment-seeking behavior. Furthermore, we find that different
models capture dimensions of stigma differently for men and women, associating
stereotypes like anger, blame, and pity more with women with mental health
conditions than with men. In showing the complex nuances of models' gendered
mental health stigma, we demonstrate that context and overlapping dimensions of
identity are important considerations when assessing computational models'
social biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Curriculum Learning Approach for Multi-domain Text Classification Using Keyword weight Ranking. (arXiv:2210.15147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15147">
<div class="article-summary-box-inner">
<span><p>Text classification is a very classic NLP task, but it has two prominent
shortcomings: On the one hand, text classification is deeply domain-dependent.
That is, a classifier trained on the corpus of one domain may not perform so
well in another domain. On the other hand, text classification models require a
lot of annotated data for training. However, for some domains, there may not
exist enough annotated data. Therefore, it is valuable to investigate how to
efficiently utilize text data from different domains to improve the performance
of models in various domains. Some multi-domain text classification models are
trained by adversarial training to extract shared features among all domains
and the specific features of each domain. We noted that the distinctness of the
domain-specific features is different, so in this paper, we propose to use a
curriculum learning strategy based on keyword weight ranking to improve the
performance of multi-domain text classification models. The experimental
results on the Amazon review and FDU-MTL datasets show that our curriculum
learning strategy effectively improves the performance of multi-domain text
classification models based on adversarial learning and outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dictionary-Assisted Supervised Contrastive Learning. (arXiv:2210.15172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15172">
<div class="article-summary-box-inner">
<span><p>Text analysis in the social sciences often involves using specialized
dictionaries to reason with abstract concepts, such as perceptions about the
economy or abuse on social media. These dictionaries allow researchers to
impart domain knowledge and note subtle usages of words relating to a
concept(s) of interest. We introduce the dictionary-assisted supervised
contrastive learning (DASCL) objective, allowing researchers to leverage
specialized dictionaries when fine-tuning pretrained language models. The text
is first keyword simplified: a common, fixed token replaces any word in the
corpus that appears in the dictionary(ies) relevant to the concept of interest.
During fine-tuning, a supervised contrastive objective draws closer the
embeddings of the original and keyword-simplified texts of the same class while
pushing further apart the embeddings of different classes. The
keyword-simplified texts of the same class are more textually similar than
their original text counterparts, which additionally draws the embeddings of
the same class closer together. Combining DASCL and cross-entropy improves
classification performance metrics in few-shot learning settings and social
science applications compared to using cross-entropy alone and alternative
contrastive and data augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15173">
<div class="article-summary-box-inner">
<span><p>Generative deep neural networks are widely used for speech synthesis, but
most existing models directly generate waveforms or spectral outputs. Humans,
however, produce speech by controlling articulators, which results in the
production of speech sounds through physical properties of sound propagation.
We propose a new unsupervised generative model of speech production/synthesis
that includes articulatory representations and thus more closely mimics human
speech production. We introduce the Articulatory Generator to the Generative
Adversarial Network paradigm. The Articulatory Generator needs to learn to
generate articulatory representations (electromagnetic articulography or EMA)
in a fully unsupervised manner without ever accessing EMA data. A separate
pre-trained physical model (ema2wav) then transforms the generated EMA
representations to speech waveforms, which get sent to the Discriminator for
evaluation. Articulatory analysis of the generated EMA representations suggests
that the network learns to control articulators in a manner that closely
follows human articulators during speech production. Acoustic analysis of the
outputs suggest that the network learns to generate words that are part of
training data as well as novel innovative words that are absent from training
data. Our proposed architecture thus allows modeling of articulatory learning
with deep neural networks from raw audio inputs in a fully unsupervised manner.
We additionally discuss implications of articulatory representations for
cognitive models of human language and speech technology in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled and Robust Representation Learning for Bragging Classification in Social Media. (arXiv:2210.15180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15180">
<div class="article-summary-box-inner">
<span><p>Researching bragging behavior on social media arouses interest of
computational (socio) linguists. However, existing bragging classification
datasets suffer from a serious data imbalance issue. Because labeling a
data-balance dataset is expensive, most methods introduce external knowledge to
improve model learning. Nevertheless, such methods inevitably introduce noise
and non-relevance information from external knowledge. To overcome the
drawback, we propose a novel bragging classification method with
disentangle-based representation augmentation and domain-aware adversarial
strategy. Specifically, model learns to disentangle and reconstruct
representation and generate augmented features via disentangle-based
representation augmentation. Moreover, domain-aware adversarial strategy aims
to constrain domain of augmented features to improve their robustness.
Experimental results demonstrate that our method achieves state-of-the-art
performance compared to other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outlier-Aware Training for Improving Group Accuracy Disparities. (arXiv:2210.15183v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15183">
<div class="article-summary-box-inner">
<span><p>Methods addressing spurious correlations such as Just Train Twice (JTT,
<a href="/abs/2107.09044">arXiv:2107.09044v2</a>) involve reweighting a subset of the training set to
maximize the worst-group accuracy. However, the reweighted set of examples may
potentially contain unlearnable examples that hamper the model's learning. We
propose mitigating this by detecting outliers to the training set and removing
them before reweighting. Our experiments show that our method achieves
competitive or better accuracy compared with JTT and can detect and remove
annotation errors in the subset being reweighted in JTT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Too Brittle To Touch: Comparing the Stability of Quantization and Distillation Towards Developing Lightweight Low-Resource MT Models. (arXiv:2210.15184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15184">
<div class="article-summary-box-inner">
<span><p>Leveraging shared learning through Massively Multilingual Models,
state-of-the-art machine translation models are often able to adapt to the
paucity of data for low-resource languages. However, this performance comes at
the cost of significantly bloated models which are not practically deployable.
Knowledge Distillation is one popular technique to develop competitive,
lightweight models: In this work, we first evaluate its use to compress MT
models focusing on languages with extremely limited training data. Through our
analysis across 8 languages, we find that the variance in the performance of
the distilled models due to their dependence on priors including the amount of
synthetic data used for distillation, the student architecture, training
hyperparameters and confidence of the teacher models, makes distillation a
brittle compression mechanism. To mitigate this, we explore the use of
post-training quantization for the compression of these models. Here, we find
that while distillation provides gains across some low-resource languages,
quantization provides more consistent performance trends for the entire range
of languages, especially the lowest-resource languages in our target set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Truncation Sampling as Language Model Desmoothing. (arXiv:2210.15191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15191">
<div class="article-summary-box-inner">
<span><p>Long samples of text from neural language models can be of poor quality.
Truncation sampling algorithms--like top-$p$ or top-$k$ -- address this by
setting some words' probabilities to zero at each step. This work provides
framing for the aim of truncation, and an improved algorithm for that aim. We
propose thinking of a neural language model as a mixture of a true distribution
and a smoothing distribution that avoids infinite perplexity. In this light,
truncation algorithms aim to perform desmoothing, estimating a subset of the
support of the true distribution. Finding a good subset is crucial: we show
that top-$p$ unnecessarily truncates high-probability words, for example
causing it to truncate all words but Trump for a document that starts with
Donald. We introduce $\eta$-sampling, which truncates words below an
entropy-dependent probability threshold. Compared to previous algorithms,
$\eta$-sampling generates more plausible long English documents according to
humans, is better at breaking out of repetition, and behaves more reasonably on
a battery of test distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning. (arXiv:2210.15212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15212">
<div class="article-summary-box-inner">
<span><p>We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to
improve the generalization ability of dense retrieval by combating the
distribution shifts between source training tasks and target scenarios. To
mitigate the impact of document differences, COCO-DR continues pretraining the
language model on the target corpora to adapt the model to target distributions
via COtinuous COtrastive learning. To prepare for unseen target queries,
COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to
reweight samples from different source query clusters for improving model
robustness over rare queries during fine-tuning. COCO-DR achieves superior
average performance on BEIR, the zero-shot retrieval benchmark. At BERT Base
scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At
BERT Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model
which has 500x more parameters. Our analysis show the correlation between
COCO-DR's effectiveness in combating distribution shifts and improving
zero-shot accuracy. Our code and model can be found at
\url{https://github.com/OpenMatch/COCO-DR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parsing linearizations appreciate PoS tags - but some are fussy about errors. (arXiv:2210.15219v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15219">
<div class="article-summary-box-inner">
<span><p>PoS tags, once taken for granted as a useful resource for syntactic parsing,
have become more situational with the popularization of deep learning. Recent
work on the impact of PoS tags on graph- and transition-based parsers suggests
that they are only useful when tagging accuracy is prohibitively high, or in
low-resource scenarios. However, such an analysis is lacking for the emerging
sequence labeling parsing paradigm, where it is especially relevant as some
models explicitly use PoS tags for encoding and decoding. We undertake a study
and uncover some trends. Among them, PoS tags are generally more useful for
sequence labeling parsers than for other paradigms, but the impact of their
accuracy is highly encoding-dependent, with the PoS-based head-selection
encoding being best only when both tagging accuracy and resource availability
are high.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack. (arXiv:2210.15221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15221">
<div class="article-summary-box-inner">
<span><p>We present Twin Answer Sentences Attack (TASA), an adversarial attack method
for question answering (QA) models that produces fluent and grammatical
adversarial contexts while maintaining gold answers. Despite phenomenal
progress on general adversarial attacks, few works have investigated the
vulnerability and attack specifically for QA models. In this work, we first
explore the biases in the existing models and discover that they mainly rely on
keyword matching between the question and context, and ignore the relevant
contextual relations for answer prediction. Based on two biases above, TASA
attacks the target model in two folds: (1) lowering the model's confidence on
the gold answer with a perturbed answer sentence; (2) misguiding the model
towards a wrong answer with a distracting answer sentence. Equipped with
designed beam search and filtering methods, TASA can generate more effective
attacks than existing textual attack methods while sustaining the quality of
contexts, in extensive experiments on five QA datasets and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Normalization for Bi-directional Amharic-English Neural Machine Translation. (arXiv:2210.15224v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15224">
<div class="article-summary-box-inner">
<span><p>Machine translation (MT) is one of the main tasks in natural language
processing whose objective is to translate texts automatically from one natural
language to another. Nowadays, using deep neural networks for MT tasks has
received great attention. These networks require lots of data to learn abstract
representations of the input and store it in continuous vectors. This paper
presents the first relatively large-scale Amharic-English parallel sentence
dataset. Using these compiled data, we build bi-directional Amharic-English
translation models by fine-tuning the existing Facebook M2M100 pre-trained
model achieving a BLEU score of 37.79 in Amharic-English 32.74 in
English-Amharic translation. Additionally, we explore the effects of Amharic
homophone normalization on the machine translation task. The results show that
the normalization of Amharic homophone characters increases the performance of
Amharic-English machine translation in both directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text Classification. (arXiv:2210.15225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15225">
<div class="article-summary-box-inner">
<span><p>Multi-label Text Classification (MLTC) is the task of categorizing documents
into one or more topics. Considering the large volumes of data and varying
domains of such tasks, fully supervised learning requires manually fully
annotated datasets which is costly and time-consuming. In this paper, we
propose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text
Classification (WSMLTC) model that reduces the need for full supervision. This
new model (1) produces BERT sentence embeddings and calibrates them using a
flow model, (2) generates an initial topic-document matrix by averaging results
of a seeded sparse topic model and a textual entailment model which only
require surface name of topics and 4-6 seed words per topic, and (3) adopts a
VAE framework to reconstruct the embeddings under the guidance of the
topic-document matrix. Finally, (4) it uses the means produced by the encoder
model in the VAE architecture as predictions for MLTC. Experimental results on
6 multi-label datasets show that BFV can substantially outperform other
baseline WSMLTC models in key metrics and achieve approximately 84% performance
of a fully-supervised model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative pseudo-forced alignment by acoustic CTC loss for self-supervised ASR domain adaptation. (arXiv:2210.15226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15226">
<div class="article-summary-box-inner">
<span><p>High-quality data labeling from specific domains is costly and human
time-consuming. In this work, we propose a self-supervised domain adaptation
method, based upon an iterative pseudo-forced alignment algorithm. The produced
alignments are employed to customize an end-to-end Automatic Speech Recognition
(ASR) and iteratively refined. The algorithm is fed with frame-wise character
posteriors produced by a seed ASR, trained with out-of-domain data, and
optimized throughout a Connectionist Temporal Classification (CTC) loss. The
alignments are computed iteratively upon a corpus of broadcast TV. The process
is repeated by reducing the quantity of text to be aligned or expanding the
alignment window until finding the best possible audio-text alignment. The
starting timestamps, or temporal anchors, are produced uniquely based on the
confidence score of the last aligned utterance. This score is computed with the
paths of the CTC-alignment matrix. With this methodology, no human-revised text
references are required. Alignments from long audio files with low-quality
transcriptions, like TV captions, are filtered out by confidence score and
ready for further ASR adaptation. The obtained results, on both the Spanish
RTVE2022 and CommonVoice databases, underpin the feasibility of using CTC-based
systems to perform: highly accurate audio-text alignments, domain adaptation
and semi-supervised training of end-to-end ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?. (arXiv:2210.15230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15230">
<div class="article-summary-box-inner">
<span><p>Text-to-image generative models have achieved unprecedented success in
generating high-quality images based on natural language descriptions. However,
it is shown that these models tend to favor specific social groups when
prompted with neutral text descriptions (e.g., 'a photo of a lawyer').
Following Zhao et al. (2021), we study the effect on the diversity of the
generated images when adding ethical intervention that supports equitable
judgment (e.g., 'if all individuals can be a lawyer irrespective of their
gender') in the input prompts. To this end, we introduce an Ethical NaTural
Language Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset
to evaluate the change in image generations conditional on ethical
interventions across three social axes -- gender, skin color, and culture.
Through ENTIGEN framework, we find that the generations from minDALL.E,
DALL.E-mini and Stable Diffusion cover diverse social groups while preserving
the image quality. Preliminary studies indicate that a large change in the
model predictions is triggered by certain phrases such as 'irrespective of
gender' in the context of gender bias in the ethical interventions. We release
code and annotated data at https://github.com/Hritikbansal/entigen_emnlp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling. (arXiv:2210.15231v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15231">
<div class="article-summary-box-inner">
<span><p>Boundary information is critical for various Chinese language processing
tasks, such as word segmentation, part-of-speech tagging, and named entity
recognition. Previous studies usually resorted to the use of a high-quality
external lexicon, where lexicon items can offer explicit boundary information.
However, to ensure the quality of the lexicon, great human effort is always
necessary, which has been generally ignored. In this work, we suggest
unsupervised statistical boundary information instead, and propose an
architecture to encode the information directly into pre-trained language
models, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature
induction of Chinese sequence labeling tasks. Experimental results on ten
benchmarks of Chinese sequence labeling demonstrate that BABERT can provide
consistent improvements on all datasets. In addition, our method can complement
previous supervised lexicon exploration, where further improvements can be
achieved when integrated with external lexicon information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating a morphological and syntactic tagged corpus for the Uzbek language. (arXiv:2210.15234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15234">
<div class="article-summary-box-inner">
<span><p>Nowadays, creation of the tagged corpora is becoming one of the most
important tasks of Natural Language Processing (NLP). There are not enough
tagged corpora to build machine learning models for the low-resource Uzbek
language. In this paper, we tried to fill that gap by developing a novel Part
Of Speech (POS) and syntactic tagset for creating the syntactic and
morphologically tagged corpus of the Uzbek language. This work also includes
detailed description and presentation of a web-based application to work on a
tagging as well. Based on the developed annotation tool and the software, we
share our experience results of the first stage of the tagged corpus creation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. (arXiv:2210.15237v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15237">
<div class="article-summary-box-inner">
<span><p>While semantic communication is expected to bring unprecedented communication
efficiency in comparison to classical communication, many challenges must be
resolved to realize its potential. In this work, we provide a realistic
semantic network dubbed seq2seq-SC, which is compatible to 5G NR and can work
with generalized text dataset utilizing pre-trained language model. We also
utilize a performance metric (SBERT) which can accurately measure semantic
similarity and show that seq2seq-SC achieves superior performance while
extracting semantically meaningful information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Knowledge Graph Construction and Event-centric Knowledge Infusion for Scientific NLI. (arXiv:2210.15248v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15248">
<div class="article-summary-box-inner">
<span><p>With the advance of natural language inference (NLI), a rising demand for NLI
is to handle scientific texts. Existing methods depend on pre-trained models
(PTM) which lack domain-specific knowledge. To tackle this drawback, we
introduce a scientific knowledge graph to generalize PTM to scientific domain.
However, existing knowledge graph construction approaches suffer from some
drawbacks, i.e., expensive labeled data, failure to apply in other domains,
long inference time and difficulty extending to large corpora. Therefore, we
propose an unsupervised knowledge graph construction method to build a
scientific knowledge graph (SKG) without any labeled data. Moreover, to
alleviate noise effect from SKG and complement knowledge in sentences better,
we propose an event-centric knowledge infusion method to integrate external
knowledge into each event that is a fine-grained semantic unit in sentences.
Experimental results show that our method achieves state-of-the-art performance
and the effectiveness and reliability of SKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversation Disentanglement with Bi-Level Contrastive Learning. (arXiv:2210.15265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15265">
<div class="article-summary-box-inner">
<span><p>Conversation disentanglement aims to group utterances into detached sessions,
which is a fundamental task in processing multi-party conversations. Existing
methods have two main drawbacks. First, they overemphasize pairwise utterance
relations but pay inadequate attention to the utterance-to-context relation
modeling. Second, huge amount of human annotated data is required for training,
which is expensive to obtain in practice. To address these issues, we propose a
general disentangle model based on bi-level contrastive learning. It brings
closer utterances in the same session while encourages each utterance to be
near its clustered session prototypes in the representation space. Unlike
existing approaches, our disentangle model works in both supervised setting
with labeled data and unsupervised setting when no such data is available. The
proposed method achieves new state-of-the-art performance on both settings
across several public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic Forgetting in Automatic Speech Recognition. (arXiv:2210.15282v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15282">
<div class="article-summary-box-inner">
<span><p>Adapting a trained Automatic Speech Recognition (ASR) model to new tasks
results in catastrophic forgetting of old tasks, limiting the model's ability
to learn continually and to be extended to new speakers, dialects, languages,
etc. Focusing on End-to-End ASR, in this paper, we propose a simple yet
effective method to overcome catastrophic forgetting: weight averaging. By
simply taking the average of the previous and the adapted model, our method
achieves high performance on both the old and new tasks. It can be further
improved by introducing a knowledge distillation loss during the adaptation. We
illustrate the effectiveness of our method on both monolingual and multilingual
ASR. In both cases, our method strongly outperforms all baselines, even in its
simplest form.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAN: a robust end-to-end ASR model architecture. (arXiv:2210.15285v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15285">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel Siamese Adversarial Network (SAN)
architecture for automatic speech recognition, which aims at solving the
difficulty of fuzzy audio recognition. Specifically, SAN constructs two
sub-networks to differentiate the audio feature input and then introduces a
loss to unify the output distribution of these sub-networks. Adversarial
learning enables the network to capture more essential acoustic features and
helps the models achieve better performance when encountering fuzzy audio
input. We conduct numerical experiments with the SAN model on several datasets
for the automatic speech recognition task. All experimental results show that
the siamese adversarial nets significantly reduce the character error rate
(CER). Specifically, we achieve a new state of art 4.37 CER without language
model on the AISHELL-1 dataset, which leads to around 5% relative CER
reduction. To reveal the generality of the siamese adversarial net, we also
conduct experiments on the phoneme recognition task, which also shows the
superiority of the siamese adversarial network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can language models handle recursively nested grammatical structures? A case study on comparing models and humans. (arXiv:2210.15303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15303">
<div class="article-summary-box-inner">
<span><p>How should we compare the capabilities of language models and humans? Here, I
consider a case study: processing of recursively nested grammatical structures.
Prior work has suggested that language models cannot handle these structures as
reliably as humans can. However, the humans were provided with instructions and
training before being evaluated, while the language models were evaluated
zero-shot. I therefore attempt to more closely match the evaluation paradigms
by providing language models with few-shot prompts. A simple prompt, which
contains substantially less content than the human training, allows large
language models to consistently outperform the human results. The same prompt
even allows extrapolation to more-deeply-nested conditions than have been
tested in humans. Further, a reanalysis of the prior human experiments suggests
that the humans may not perform above chance at the difficult structures
initially. These results suggest that large language models can in fact process
recursively nested grammatical structures comparably to humans. This case study
highlights how discrepancies in the quantity of experiment-specific context can
confound comparisons of language models and humans. I use this case study to
reflect on the broader challenge of comparing human and model capabilities, and
to suggest that there is an important difference between evaluating cognitive
models of a specific phenomenon and evaluating broadly-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Language-centric Scientific AI. (arXiv:2210.15327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15327">
<div class="article-summary-box-inner">
<span><p>Inspired by recent and revolutionary developments in AI, particularly in
language understanding and generation, we set about designing AI systems that
are able to address complex scientific tasks that challenge human capabilities
to make new discoveries. Central to our approach is the notion of natural
language as core representation, reasoning, and exchange format between
scientific AI and human scientists. In this paper, we identify and discuss some
of the main research challenges to accomplish such vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings. (arXiv:2210.15332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15332">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the task of learning unsupervised dialogue
embeddings. Trivial approaches such as combining pre-trained word or sentence
embeddings and encoding through pre-trained language models (PLMs) have been
shown to be feasible for this task. However, these approaches typically ignore
the conversational interactions between interlocutors, resulting in poor
performance. To address this issue, we proposed a self-guided contrastive
learning approach named dial2vec. Dial2vec considers a dialogue as an
information exchange process. It captures the conversational interaction
patterns between interlocutors and leverages them to guide the learning of the
embeddings corresponding to each interlocutor. The dialogue embedding is
obtained by an aggregation of the embeddings from all interlocutors. To verify
our approach, we establish a comprehensive benchmark consisting of six
widely-used dialogue datasets. We consider three evaluation tasks: domain
categorization, semantic relatedness, and dialogue retrieval. Dial2vec achieves
on average 8.7, 9.0, and 13.8 points absolute improvements in terms of purity,
Spearman's correlation, and mean average precision (MAP) over the strongest
baseline on the three tasks respectively. Further analysis shows that dial2vec
obtains informative and discriminative embeddings for both interlocutors under
the guidance of the conversational interactions and achieves the best
performance when aggregating them through the interlocutor-level pooling
strategy. All codes and data are publicly available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial2vec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging knowledge graphs to update scientific word embeddings using latent semantic imputation. (arXiv:2210.15358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15358">
<div class="article-summary-box-inner">
<span><p>The most interesting words in scientific texts will often be novel or rare.
This presents a challenge for scientific word embedding models to determine
quality embedding vectors for useful terms that are infrequent or newly
emerging. We demonstrate how \gls{lsi} can address this problem by imputing
embeddings for domain-specific words from up-to-date knowledge graphs while
otherwise preserving the original word embedding model. We use the MeSH
knowledge graph to impute embedding vectors for biomedical terminology without
retraining and evaluate the resulting embedding model on a domain-specific
word-pair similarity task. We show that LSI can produce reliable embedding
vectors for rare and OOV terms in the biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational Speech Synthesis. (arXiv:2210.15360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15360">
<div class="article-summary-box-inner">
<span><p>Conversational Text-to-Speech (TTS) aims to synthesis an utterance with the
right linguistic and affective prosody in a conversational context. The
correlation between the current utterance and the dialogue history at the
utterance level was used to improve the expressiveness of synthesized speech.
However, the fine-grained information in the dialogue history at the word level
also has an important impact on the prosodic expression of an utterance, which
has not been well studied in the prior work. Therefore, we propose a novel
expressive conversational TTS model, termed as FCTalker, that learn the fine
and coarse grained context dependency at the same time during speech
generation. Specifically, the FCTalker includes fine and coarse grained
encoders to exploit the word and utterance-level context dependency. To model
the word-level dependencies between an utterance and its dialogue history, the
fine-grained dialogue encoder is built on top of a dialogue BERT model. The
experimental results show that the proposed method outperforms all baselines
and generates more expressive speech that is contextually appropriate. We
release the source code at: https://github.com/walker-hyf/FCTalker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Teacher-student Framework for Unsupervised Speech Enhancement Using Noise Remixing Training and Two-stage Inference. (arXiv:2210.15368v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15368">
<div class="article-summary-box-inner">
<span><p>The lack of clean speech is a practical challenge to the development of
speech enhancement systems, which means that the training of neural network
models must be done in an unsupervised manner, and there is an inevitable
mismatch between their training criterion and evaluation metric. In response to
this unfavorable situation, we propose a teacher-student training strategy that
does not require any subjective/objective speech quality metrics as learning
reference by improving the previously proposed noisy-target training (NyTT).
Because homogeneity between in-domain noise and extraneous noise is the key to
the effectiveness of NyTT, we train various student models by remixing the
teacher model's estimated speech and noise for clean-target training or raw
noisy speech and the teacher model's estimated noise for noisy-target training.
We use the NyTT model as the initial teacher model. Experimental results show
that our proposed method outperforms several baselines, especially with
two-stage inference, where clean speech is derived successively through the
bootstrap model and the final student model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CasNet: Investigating Channel Robustness for Speech Separation. (arXiv:2210.15370v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15370">
<div class="article-summary-box-inner">
<span><p>Recording channel mismatch between training and testing conditions has been
shown to be a serious problem for speech separation. This situation greatly
reduces the separation performance, and cannot meet the requirement of daily
use. In this study, inheriting the use of our previously constructed TAT-2mix
corpus, we address the channel mismatch problem by proposing a channel-aware
audio separation network (CasNet), a deep learning framework for end-to-end
time-domain speech separation. CasNet is implemented on top of TasNet. Channel
embedding (characterizing channel information in a mixture of multiple
utterances) generated by Channel Encoder is introduced into the separation
module by the FiLM technique. Through two training strategies, we explore two
roles that channel embedding may play: 1) a real-life noise disturbance, making
the model more robust, or 2) a guide, instructing the separation model to
retain the desired channel information. Experimental results on TAT-2mix show
that CasNet trained with both training strategies outperforms the TasNet
baseline, which does not use channel embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-consistent Reasoning For Solving Math Word Problems. (arXiv:2210.15373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15373">
<div class="article-summary-box-inner">
<span><p>Math word problems (MWPs) is a task that automatically derives solution
expression from a giving math problems in text. The previous studies suffer
from spurious correlations between input text and output expression. To
mitigate this issue, we propose a self-consistent reasoning framework called
SCR, which attempts to adopt a pruning strategy to correct the output
distribution shift so as to implicitly fix those spurious correlative samples.
Specifically, we firstly obtain a sub-network by pruning a roberta2tree model,
for the sake to use the gap on output distribution between the original
roberta2tree model and the pruned sub-network to expose spurious correlative
samples. Then, we calibrate the output distribution shift by applying symmetric
Kullback-Leibler divergence to alleviate spurious correlations. In addition,
SCR generates equivalent expressions, thereby, capturing the original text's
logic rather than relying on hints from original text. Extensive experiments on
two large-scale benchmarks demonstrate that our model substantially outperforms
the strong baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structuring User-Generated Content on Social Media with Multimodal Aspect-Based Sentiment Analysis. (arXiv:2210.15377v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15377">
<div class="article-summary-box-inner">
<span><p>People post their opinions and experiences on social media, yielding rich
databases of end users' sentiments. This paper shows to what extent machine
learning can analyze and structure these databases. An automated data analysis
pipeline is deployed to provide insights into user-generated content for
researchers in other domains. First, the domain expert can select an image and
a term of interest. Then, the pipeline uses image retrieval to find all images
showing similar contents and applies aspect-based sentiment analysis to outline
users' opinions about the selected term. As part of an interdisciplinary
project between architecture and computer science researchers, an empirical
study of Hamburg's Elbphilharmonie was conveyed on 300 thousand posts from the
platform Flickr with the hashtag 'hamburg'. Image retrieval methods generated a
subset of slightly more than 1.5 thousand images displaying the
Elbphilharmonie. We found that these posts mainly convey a neutral or positive
sentiment towards it. With this pipeline, we suggest a new big data analysis
method that offers new insights into end-users opinions, e.g., for architecture
domain experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MorphTE: Injecting Morphology in Tensorized Embeddings. (arXiv:2210.15379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15379">
<div class="article-summary-box-inner">
<span><p>In the era of deep learning, word embeddings are essential when dealing with
text tasks. However, storing and accessing these embeddings requires a large
amount of space. This is not conducive to the deployment of these models on
resource-limited devices. Combining the powerful compression capability of
tensor products, we propose a word embedding compression method with
morphological augmentation, Morphologically-enhanced Tensorized Embeddings
(MorphTE). A word consists of one or more morphemes, the smallest units that
bear meaning or have a grammatical function. MorphTE represents a word
embedding as an entangled form of its morpheme vectors via the tensor product,
which injects prior semantic and grammatical knowledge into the learning of
embeddings. Furthermore, the dimensionality of the morpheme vector and the
number of morphemes are much smaller than those of words, which greatly reduces
the parameters of the word embeddings. We conduct experiments on tasks such as
machine translation and question answering. Experimental results on four
translation datasets of different languages show that MorphTE can compress word
embedding parameters by about 20 times without performance loss and
significantly outperforms related embedding compression methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opening the Black Box of wav2vec Feature Encoder. (arXiv:2210.15386v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15386">
<div class="article-summary-box-inner">
<span><p>Self-supervised models, namely, wav2vec and its variants, have shown
promising results in various downstream tasks in the speech domain. However,
their inner workings are poorly understood, calling for in-depth analyses on
what the model learns. In this paper, we concentrate on the convolutional
feature encoder where its latent space is often speculated to represent
discrete acoustic units. To analyze the embedding space in a reductive manner,
we feed the synthesized audio signals, which is the summation of simple sine
waves. Through extensive experiments, we conclude that various information is
embedded inside the feature encoder representations: (1) fundamental frequency,
(2) formants, and (3) amplitude, packed with (4) sufficient temporal detail.
Further, the information incorporated inside the latent representations is
analogous to spectrograms but with a fundamental difference: latent
representations construct a metric space so that closer representations imply
acoustic similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15387">
<div class="article-summary-box-inner">
<span><p>Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity level
classification and an auxilary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted features such as eGeMaps and
linguistic features, and SVM, MLP, and XGBoost classifiers. Explored on the
Korean dysarthric speech QoLT database, our model outperforms the traditional
baseline methods, with a relative percentage increase of 4.79% for
classification accuracy. In addition, the proposed model surpasses the model
trained without ASR head, achieving 10.09% relative percentage improvements.
Furthermore, we present how multi-task learning affects the severity
classification performance by analyzing the latent representations and
regularization effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15398">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a technique to generate new training data based on
existing data. We evaluate the simple and cost-effective method of
concatenating the original data examples to build new training instances.
Continued training with such augmented data is able to improve off-the-shelf
Transformer and Conformer models that were optimized on the original data only.
We demonstrate considerable improvements on the LibriSpeech-960h test sets (WER
2.83 and 6.87 for test-clean and test-other), which carry over to models
combined with shallow fusion (WER 2.55 and 6.27). Our method of continued
training also leads to improvements of up to 0.9 WER on the ASR part of
CoVoST-2 for four non English languages, and we observe that the gains are
highly dependent on the size of the original training data. We compare
different concatenation strategies and found that our method does not need
speaker information to achieve its improvements. Finally, we demonstrate on two
datasets that our methods also works for speech translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Language Model to Train if You Have One Million GPU Hours?. (arXiv:2210.15424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15424">
<div class="article-summary-box-inner">
<span><p>The crystallization of modeling methods around the Transformer architecture
has been a boon for practitioners. Simple, well-motivated architectural
variations can transfer across tasks and scale, increasing the impact of
modeling research. However, with the emergence of state-of-the-art 100B+
parameters models, large language models are increasingly expensive to
accurately design and train. Notably, it can be difficult to evaluate how
modeling decisions may impact emergent capabilities, given that these
capabilities arise mainly from sheer scale alone. In the process of building
BLOOM--the Big Science Large Open-science Open-access Multilingual language
model--our goal is to identify an architecture and training setup that makes
the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform
an ablation study at the billion-parameter scale comparing different modeling
practices and their impact on zero-shot generalization. In addition, we study
the impact of various popular pre-training corpora on zero-shot generalization.
We also study the performance of a multilingual model and how it compares to
the English-only one. Finally, we consider the scaling behaviour of
Transformers to choose the target model size, shape, and training setup. All
our models and code are open-sourced at https://huggingface.co/bigscience .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Use of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15445">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) has been established as a well-performing
technique for many scenarios where lots of labeled data is available.
Additionally, unsupervised representation learning recently helped to tackle
tasks with limited data. Following this, hardware limitations and applications
give rise to the question how to efficiently take advantage of large pretrained
models and reduce their complexity for downstream tasks. In this work, we study
a challenging low resource conversational telephony speech corpus from the
medical domain in Vietnamese and German. We show the benefits of using
unsupervised techniques beyond simple fine-tuning of large pre-trained models,
discuss how to adapt them to a practical telephony task including bandwidth
transfer and investigate different data conditions for pre-training and
fine-tuning. We outperform the project baselines by 22% relative using
pretraining techniques. Further gains of 29% can be achieved by refinements of
architecture and training and 6% by adding 0.8 h of in-domain adaptation data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised Learning for Text-To-Speech. (arXiv:2210.15447v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15447">
<div class="article-summary-box-inner">
<span><p>This paper proposes Virtuoso, a massively multilingual speech-text joint
semi-supervised learning framework for text-to-speech synthesis (TTS) models.
Existing multilingual TTS typically supports tens of languages, which are a
small fraction of the thousands of languages in the world. One difficulty to
scale multilingual TTS to hundreds of languages is collecting high-quality
speech-text paired data in low-resource languages. This study extends Maestro,
a speech-text joint pretraining framework for automatic speech recognition
(ASR), to speech generation tasks. To train a TTS model from various types of
speech and text data, different training schemes are designed to handle
supervised (paired TTS and ASR data) and unsupervised (untranscribed speech and
unspoken text) datasets. Experimental evaluation shows that 1) multilingual TTS
models trained on Virtuoso can achieve significantly better naturalness and
intelligibility than baseline ones in seen languages, and 2) they can
synthesize reasonably intelligible and naturally sounding speech for unseen
languages where no high-quality paired TTS data is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity. (arXiv:2210.15452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15452">
<div class="article-summary-box-inner">
<span><p>We investigate the problem of determining the predictive confidence (or,
conversely, uncertainty) of a neural classifier through the lens of
low-resource languages. By training models on sub-sampled datasets in three
different languages, we assess the quality of estimates from a wide array of
approaches and their dependence on the amount of available data. We find that
while approaches based on pre-trained models and ensembles achieve the best
results overall, the quality of uncertainty estimates can surprisingly suffer
with more data. We also perform a qualitative analysis of uncertainties on
sequences, discovering that a model's total uncertainty seems to be influenced
to a large degree by its data uncertainty, not model uncertainty. All model
implementations are open-sourced in a software package.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions. (arXiv:2210.15456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15456">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning simulates the human ability to make presumptions about
our physical world, and it is an essential cornerstone in building general AI
systems. We propose a new commonsense reasoning dataset based on human's
Interactive Fiction (IF) gameplay walkthroughs as human players demonstrate
plentiful and diverse commonsense reasoning. The new dataset provides a natural
mixture of various reasoning types and requires multi-hop reasoning. Moreover,
the IF game-based construction procedure requires much less human interventions
than previous ones. Experiments show that the introduced dataset is challenging
to previous machine reading models with a significant 20% performance gap
compared to human experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15458">
<div class="article-summary-box-inner">
<span><p>Decoding methods for large language models often trade-off between diversity
of outputs and parallelism of computation. Methods such as beam search and
Gumbel top-k sampling can guarantee a different output for each element of the
beam, but are not easy to parallelize. Alternatively, methods such as
temperature sampling and its modifications (top-k sampling, nucleus sampling,
typical decoding, and others), are embarrassingly parallel, but have no
guarantees about duplicate samples. We present a framework for sampling
according to an arithmetic code book implicitly defined by a large language
model, compatible with common sampling variations, with provable beam diversity
under certain conditions, as well as being embarrassingly parallel and
providing unbiased and consistent expectations from the original model. We
demonstrate the effectiveness of our approach on WMT machine translation,
showing substantially reduced variance when estimating expected BLEU score and
up to 1 point increased BLEU in oracle experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation. (arXiv:2210.15461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15461">
<div class="article-summary-box-inner">
<span><p>Multimodal Machine Translation (MMT) focuses on enhancing text-only
translation with visual features, which has attracted considerable attention
from both natural language processing and computer vision communities. Recent
advances still struggle to train a separate model for each language pair, which
is costly and unaffordable when the number of languages increases in the real
world. In other words, the multilingual multimodal machine translation
(Multilingual MMT) task has not been investigated, which aims to handle the
aforementioned issues by providing a shared semantic space for multiple
languages. Besides, the image modality has no language boundaries, which is
superior to bridging the semantic gap between languages. To this end, we first
propose the Multilingual MMT task by establishing two new Multilingual MMT
benchmark datasets covering seven languages. Then, an effective baseline LVP-M3
using visual prompts is proposed to support translations between different
languages, which includes three stages (token encoding, language-aware visual
prompt generation, and language translation). Extensive experimental results on
our constructed benchmark datasets demonstrate the effectiveness of LVP-M3
method for Multilingual MMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues. (arXiv:2210.15462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15462">
<div class="article-summary-box-inner">
<span><p>In this work, we define a new style transfer task: perspective shift, which
reframes a dialogue from informal first person to a formal third person
rephrasing of the text. This task requires challenging coreference resolution,
emotion attribution, and interpretation of informal text. We explore several
baseline approaches and discuss further directions on this task when applied to
short dialogues. As a sample application, we demonstrate that applying
perspective shifting to a dialogue summarization dataset (SAMSum) substantially
improves the zero-shot performance of extractive news summarization models on
this data. Additionally, supervised extractive models perform better when
trained on perspective shifted data than on the original dialogues. We release
our code publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSG Attention: Extrapolation of pretrained Transformers to long sequences. (arXiv:2210.15497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15497">
<div class="article-summary-box-inner">
<span><p>Transformer models achieve state-of-the-art performance on a wide range of
NLP tasks. They however suffer from a prohibitive limitation due to the
self-attention mechanism, inducing $O(n^2)$ complexity with regard to sequence
length. To answer this limitation we introduce the LSG architecture which
relies on Local, Sparse and Global attention. We show that LSG attention is
fast, efficient and competitive in classification and summarization tasks on
long documents. Interestingly, it can also be used to adapt existing pretrained
models to efficiently extrapolate to longer sequences with no additional
training. Along with the introduction of the LSG attention mechanism, we
propose tools to train new models and adapt existing ones based on this
mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15500">
<div class="article-summary-box-inner">
<span><p>Personalized text generation has broad industrial applications, such as
explanation generation for recommendations, conversational systems, etc.
Personalized text generators are usually trained on user written text, e.g.,
reviews collected on e-commerce platforms. However, due to historical, social,
or behavioral reasons, there may exist bias that associates certain linguistic
quality of user written text with the users' protected attributes such as
gender, race, etc. The generators can identify and inherit these correlations
and generate texts discriminately w.r.t. the users' protected attributes.
Without proper intervention, such bias can adversarially influence the users'
trust and reliance on the system. From a broader perspective, bias in
auto-generated contents can reinforce the social stereotypes about how online
users write through interactions with the users.
</p>
<p>In this work, we investigate the fairness of personalized text generation in
the setting of explainable recommendation. We develop a general framework for
achieving measure-specific counterfactual fairness on the linguistic quality of
personalized explanations. We propose learning disentangled representations for
counterfactual inference and develop a novel policy learning algorithm with
carefully designed rewards for fairness optimization. The framework can be
applied for achieving fairness on any given specifications of linguistic
quality measures, and can be adapted to most of existing models and real-world
settings. Extensive experiments demonstrate the superior ability of our method
in achieving fairness while maintaining high generation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models. (arXiv:2210.15523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15523">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained language models (PLMs) mostly suffer from
excessive overhead despite their advanced capacity. For resource-constrained
devices, there is an urgent need for a spatially and temporally efficient model
which retains the major capacity of PLMs. However, existing statically
compressed models are unaware of the diverse complexities between input
instances, potentially resulting in redundancy and inadequacy for simple and
complex inputs. Also, miniature models with early exiting encounter challenges
in the trade-off between making predictions and serving the deeper layers.
Motivated by such considerations, we propose a collaborative optimization for
PLMs that integrates static model compression and dynamic inference
acceleration. Specifically, the PLM is slenderized in width while the depth
remains intact, complementing layer-wise early exiting to speed up inference
dynamically. To address the trade-off of early exiting, we propose a joint
training approach that calibrates slenderization and preserves contributive
structures to each exit instead of only the final layer. Experiments are
conducted on GLUE benchmark and the results verify the Pareto optimality of our
approach at high compression and acceleration rate with 1/8 parameters and 1/19
FLOPs of BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Terminology-aware Medical Dialogue Generation. (arXiv:2210.15551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15551">
<div class="article-summary-box-inner">
<span><p>Medical dialogue generation aims to generate responses according to a history
of dialogue turns between doctors and patients. Unlike open-domain dialogue
generation, this requires background knowledge specific to the medical domain.
Existing generative frameworks for medical dialogue generation fall short of
incorporating domain-specific knowledge, especially with regard to medical
terminology. In this paper, we propose a novel framework to improve medical
dialogue generation by considering features centered on domain-specific
terminology. We leverage an attention mechanism to incorporate terminologically
centred features, and fill in the semantic gap between medical background
knowledge and common utterances by enforcing language models to learn
terminology representations with an auxiliary terminology recognition task.
Experimental results demonstrate the effectiveness of our approach, in which
our proposed framework outperforms SOTA language models. Additionally, we
provide a new dataset with medical terminology annotations to support the
research on medical dialogue generation. Our dataset and code are available at
https://github.com/tangg555/meddialog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving abstractive summarization with energy-based re-ranking. (arXiv:2210.15553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15553">
<div class="article-summary-box-inner">
<span><p>Current abstractive summarization systems present important weaknesses which
prevent their deployment in real-world applications, such as the omission of
relevant information and the generation of factual inconsistencies (also known
as hallucinations). At the same time, automatic evaluation metrics such as CTC
scores have been recently proposed that exhibit a higher correlation with human
judgments than traditional lexical-overlap metrics such as ROUGE. In this work,
we intend to close the loop by leveraging the recent advances in summarization
metrics to create quality-aware abstractive summarizers. Namely, we propose an
energy-based model that learns to re-rank summaries according to one or a
combination of these metrics. We experiment using several metrics to train our
energy-based re-ranker and show that it consistently improves the scores
achieved by the predicted summaries. Nonetheless, human evaluation results show
that the re-ranking approach should be used with care for highly abstractive
summaries, as the available metrics are not yet sufficiently reliable for this
purpose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Extraction of Materials and Properties from Superconductors Scientific Literature. (arXiv:2210.15600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15600">
<div class="article-summary-box-inner">
<span><p>The automatic extraction of materials and related properties from the
scientific literature is gaining attention in data-driven materials science
(Materials Informatics). In this paper, we discuss Grobid-superconductors, our
solution for automatically extracting superconductor material names and
respective properties from text. Built as a Grobid module, it combines machine
learning and heuristic approaches in a multi-step architecture that supports
input data as raw text or PDF documents. Using Grobid-superconductors, we built
SuperCon2, a database of 40324 materials and properties records from 37700
papers. The material (or sample) information is represented by name, chemical
formula, and material class, and is characterized by shape, doping,
substitution variables for components, and substrate as adjoined information.
The properties include the Tc superconducting critical temperature and, when
available, applied pressure with the Tc measurement method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Working Alliance Transformer for Psychotherapy Dialogue Classification. (arXiv:2210.15603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15603">
<div class="article-summary-box-inner">
<span><p>As a predictive measure of the treatment outcome in psychotherapy, the
working alliance measures the agreement of the patient and the therapist in
terms of their bond, task and goal. Long been a clinical quantity estimated by
the patients' and therapists' self-evaluative reports, we believe that the
working alliance can be better characterized using natural language processing
technique directly in the dialogue transcribed in each therapy session. In this
work, we propose the Working Alliance Transformer (WAT), a Transformer-based
classification model that has a psychological state encoder which infers the
working alliance scores by projecting the embedding of the dialogues turns onto
the embedding space of the clinical inventory for working alliance. We evaluate
our method in a real-world dataset with over 950 therapy sessions with anxiety,
depression, schizophrenia and suicidal patients and demonstrate an empirical
advantage of using information about the therapeutic states in this sequence
classification task of psychotherapy dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics. (arXiv:2210.15615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15615">
<div class="article-summary-box-inner">
<span><p>As machine translation (MT) metrics improve their correlation with human
judgement every year, it is crucial to understand the limitations of such
metrics at the segment level. Specifically, it is important to investigate
metric behaviour when facing accuracy errors in MT because these can have
dangerous consequences in certain contexts (e.g., legal, medical). We curate
ACES, a translation accuracy challenge set, consisting of 68 phenomena ranging
from simple perturbations at the word/character level to more complex errors
based on discourse and real-world knowledge. We use ACES to evaluate a wide
range of MT metrics including the submissions to the WMT 2022 metrics shared
task and perform several analyses leading to general recommendations for metric
developers. We recommend: a) combining metrics with different strengths, b)
developing metrics that give more weight to the source and less to
surface-level overlap with the reference and c) explicitly modelling additional
language-specific information beyond what is available via multilingual
embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Neural Entity Linking. (arXiv:2210.15616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15616">
<div class="article-summary-box-inner">
<span><p>Entity Linking is the task of matching a mention to an entity in a given
knowledge base (KB). It contributes to annotating a massive amount of documents
existing on the Web to harness new facts about their matched entities. However,
existing Entity Linking systems focus on developing models that are typically
domain-dependent and robust only to a particular knowledge base on which they
have been trained. The performance is not as adequate when being evaluated on
documents and knowledge bases from different domains.
</p>
<p>Approaches based on pre-trained language models, such as Wu et al. (2020),
attempt to solve the problem using a zero-shot setup, illustrating some
potential when evaluated on a general-domain KB. Nevertheless, the performance
is not equivalent when evaluated on a domain-specific KB. To allow for more
accurate Entity Linking across different domains, we propose our framework:
Cross-Domain Neural Entity Linking (CDNEL). Our objective is to have a single
system that enables simultaneous linking to both the general-domain KB and the
domain-specific KB. CDNEL works by learning a joint representation space for
these knowledge bases from different domains. It is evaluated using the
external Entity Linking dataset (Zeshel) constructed by Logeswaran et al.
(2019) and the Reddit dataset collected by Botzer et al. (2021), to compare our
proposed method with the state-of-the-art results. The proposed framework uses
different types of datasets for fine-tuning, resulting in different model
variants of CDNEL. When evaluated on four domains included in the Zeshel
dataset, these variants achieve an average precision gain of 9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAD: Language Augmented Diffusion for Reinforcement Learning. (arXiv:2210.15629v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15629">
<div class="article-summary-box-inner">
<span><p>Learning skills from language provides a powerful avenue for generalization
in reinforcement learning, although it remains a challenging task as it
requires agents to capture the complex interdependencies between language,
actions, and states. In this paper, we propose leveraging Language Augmented
Diffusion models as a planner conditioned on language (LAD). We demonstrate the
comparable performance of LAD with the state-of-the-art on the CALVIN language
robotics benchmark with a much simpler architecture that contains no inductive
biases specialized to robotics, achieving an average success rate (SR) of 72%
compared to the best performance of 76%. We also conduct an analysis on the
properties of language conditioned diffusion in reinforcement learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15631">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed great strides in self-supervised learning (SSL)
on the speech processing. The SSL model is normally pre-trained on a great
variety of unlabelled data and a large model size is preferred to increase the
modeling capacity. However, this might limit its potential applications due to
the expensive computation and memory costs introduced by the oversize model.
Miniaturization for SSL models has become an important research direction of
practical value. To this end, we explore the effective distillation of
HuBERT-based SSL models for automatic speech recognition (ASR). First, in order
to establish a strong baseline, a comprehensive study on different student
model structures is conducted. On top of this, as a supplement to the
regression loss widely adopted in previous works, a discriminative loss is
introduced for HuBERT to enhance the distillation performance, especially in
low-resource scenarios. In addition, we design a simple and effective algorithm
to distill the front-end input from waveform to Fbank feature, resulting in 17%
parameter reduction and doubling inference speed, at marginal performance
degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LyricJam Sonic: A Generative System for Real-Time Composition and Musical Improvisation. (arXiv:2210.15638v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15638">
<div class="article-summary-box-inner">
<span><p>Electronic music artists and sound designers have unique workflow practices
that necessitate specialized approaches for developing music information
retrieval and creativity support tools. Furthermore, electronic music
instruments, such as modular synthesizers, have near-infinite possibilities for
sound creation and can be combined to create unique and complex audio paths.
The process of discovering interesting sounds is often serendipitous and
impossible to replicate. For this reason, many musicians in electronic genres
record audio output at all times while they work in the studio. Subsequently,
it is difficult for artists to rediscover audio segments that might be suitable
for use in their compositions from thousands of hours of recordings. In this
paper, we describe LyricJam Sonic -- a novel creative tool for musicians to
rediscover their previous recordings, re-contextualize them with other
recordings, and create original live music compositions in real-time. A
bi-modal AI-driven approach uses generated lyric lines to find matching audio
clips from the artist's past studio recordings, and uses them to generate new
lyric lines, which in turn are used to find other clips, thus creating a
continuous and evolving stream of music and lyrics. The intent is to keep the
artists in a state of creative flow conducive to music creation rather than
taking them into an analytical/critical state of deliberately searching for
past audio segments. The system can run in either a fully autonomous mode
without user input, or in a live performance mode, where the artist plays live
music, while the system "listens" and creates a continuous stream of music and
lyrics in response.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06965">
<div class="article-summary-box-inner">
<span><p>Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicBART: A Pre-trained Model for Indic Natural Language Generation. (arXiv:2109.02903v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02903">
<div class="article-summary-box-inner">
<span><p>In this paper, we study pre-trained sequence-to-sequence models for a group
of related languages, with a focus on Indic languages. We present IndicBART, a
multilingual, sequence-to-sequence pre-trained model focusing on 11 Indic
languages and English. IndicBART utilizes the orthographic similarity between
Indic scripts to improve transfer learning between similar Indic languages. We
evaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and
extreme summarization. Our experiments on NMT and extreme summarization show
that a model specific to related languages like IndicBART is competitive with
large pre-trained models like mBART50 despite being significantly smaller. It
also performs well on very low-resource translation scenarios where languages
are not included in pre-training or fine-tuning. Script sharing, multilingual
training, and better utilization of limited model capacity contribute to the
good performance of the compact IndicBART model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08079">
<div class="article-summary-box-inner">
<span><p>NLP research has been focused on NER extraction and how to efficiently
extract them from a sentence. However, generating relevant context of entities
from a sentence has remained under-explored. In this work we introduce the task
Context-NER in which relevant context of an entity has to be generated. The
extracted context may not be found exactly as a substring in the sentence. We
also introduce the EDGAR10-Q dataset for the same, which is a corpus of 1,500
publicly traded companies. It is a manually created complex corpus and one of
the largest in terms of number of sentences and entities (1 M and 2.8 M). We
introduce a baseline approach that leverages phrase generation algorithms and
uses the pre-trained BERT model to get 33% ROUGE-L score. We also do a one shot
evaluation with GPT-3 and get 39% score, signifying the hardness and future
scope of this task. We hope that addition of this dataset and our study will
pave the way for further research in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation. (arXiv:2112.05364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05364">
<div class="article-summary-box-inner">
<span><p>The multi-head self-attention mechanism of the transformer model has been
thoroughly investigated recently. In one vein of study, researchers are
interested in understanding why and how transformers work. In another vein,
researchers propose new attention augmentation methods to make transformers
more accurate, efficient and interpretable. In this paper, we combine these two
lines of research in a human-in-the-loop pipeline to first discover important
task-specific attention patterns. Then those patterns are injected, not only to
smaller models, but also to the original model. The benefits of our pipeline
and discovered patterns are demonstrated in two case studies with extractive
summarization and topic segmentation. After discovering interpretable patterns
in BERT-based models fine-tuned for the two downstream tasks, experiments
indicate that when we inject the patterns into attention heads, the models show
considerable improvements in accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">First is Better Than Last for Language Data Influence. (arXiv:2202.11844v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11844">
<div class="article-summary-box-inner">
<span><p>The ability to identify influential training examples enables us to debug
training data and explain model behavior. Existing techniques to do so are
based on the flow of training data influence through the model parameters. For
large models in NLP applications, it is often computationally infeasible to
study this flow through all model parameters, therefore techniques usually pick
the last layer of weights. However, we observe that since the activation
connected to the last layer of weights contains "shared logic", the data
influenced calculated via the last layer weights prone to a ``cancellation
effect'', where the data influence of different examples have large magnitude
that contradicts each other. The cancellation effect lowers the discriminative
power of the influence score, and deleting influential examples according to
this measure often does not change the model's behavior by much. To mitigate
this, we propose a technique called TracIn-WE that modifies a method called
TracIn to operate on the word embedding layer instead of the last layer, where
the cancellation effect is less severe. One potential concern is that influence
based on the word embedding layer may not encode sufficient high level
information. However, we find that gradients (unlike embeddings) do not suffer
from this, possibly because they chain through higher layers. We show that
TracIn-WE significantly outperforms other data influence methods applied on the
last layer significantly on the case deletion evaluation on three language
classification tasks for different models. In addition, TracIn-WE can produce
scores not just at the level of the overall training input, but also at the
level of words within the training input, a further aid in debugging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages. (arXiv:2203.05437v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05437">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) for non-English languages is hampered by
the scarcity of datasets in these languages. In this paper, we present the
IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic
languages. We focus on five diverse tasks, namely, biography generation using
Wikipedia infoboxes, news headline generation, sentence summarization,
paraphrase generation and, question generation. We describe the created
datasets and use them to benchmark the performance of several monolingual and
multilingual baselines that leverage pre-trained sequence-to-sequence models.
Our results exhibit the strong performance of multilingual language-specific
pre-trained models, and the utility of models trained on our dataset for other
related NLG tasks. Our dataset creation methods can be easily applied to
modest-resource languages as they involve simple steps such as scraping news
articles and Wikipedia infoboxes, light cleaning, and pivoting through machine
translation data. To the best of our knowledge, the IndicNLG Benchmark is the
first NLG benchmark for Indic languages and the most diverse multilingual NLG
dataset, with approximately 8M examples across 5 tasks and 11 languages. The
datasets and models are publicly available at
https://ai4bharat.iitm.ac.in/indicnlg-suite.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducibility Issues for BERT-based Evaluation Metrics. (arXiv:2204.00004v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00004">
<div class="article-summary-box-inner">
<span><p>Reproducibility is of utmost concern in machine learning and natural language
processing (NLP). In the field of natural language generation (especially
machine translation), the seminal paper of Post (2018) has pointed out problems
of reproducibility of the dominant metric, BLEU, at the time of publication.
Nowadays, BERT-based evaluation metrics considerably outperform BLEU. In this
paper, we ask whether results and claims from four recent BERT-based metrics
can be reproduced. We find that reproduction of claims and results often fails
because of (i) heavy undocumented preprocessing involved in the metrics, (ii)
missing code and (iii) reporting weaker results for the baseline metrics. (iv)
In one case, the problem stems from correlating not to human scores but to a
wrong column in the csv file, inflating scores by 5 points. Motivated by the
impact of preprocessing, we then conduct a second study where we examine its
effects more closely (for one of the metrics). We find that preprocessing can
have large effects, especially for highly inflectional languages. In this case,
the effect of preprocessing may be larger than the effect of the aggregation
mechanism (e.g., greedy alignment vs. Word Mover Distance).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Noise Control for Multispeaker Speech Synthesis. (arXiv:2204.05070v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05070">
<div class="article-summary-box-inner">
<span><p>A text-to-speech (TTS) model typically factorizes speech attributes such as
content, speaker and prosody into disentangled representations.Recent works aim
to additionally model the acoustic conditions explicitly, in order to
disentangle the primary speech factors, i.e. linguistic content, prosody and
timbre from any residual factors, such as recording conditions and background
noise.This paper proposes unsupervised, interpretable and fine-grained noise
and prosody modeling. We incorporate adversarial training, representation
bottleneck and utterance-to-frame modeling in order to learn frame-level noise
representations. To the same end, we perform fine-grained prosody modeling via
a Fully Hierarchical Variational AutoEncoder (FVAE) which additionally results
in more expressive speech synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Fine-tune Twice: Selective Differential Privacy for Large Language Models. (arXiv:2204.07667v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07667">
<div class="article-summary-box-inner">
<span><p>Protecting large language models from privacy leakage is becoming
increasingly crucial with their wide adoption in real-world products. Yet
applying differential privacy (DP), a canonical notion with provable privacy
guarantees for machine learning models, to those models remains challenging due
to the trade-off between model utility and privacy loss. Utilizing the fact
that sensitive information in language data tends to be sparse, Shi et al.
(2021) formalized a DP notion extension called Selective Differential Privacy
(SDP) to protect only the sensitive tokens defined by a policy function.
However, their algorithm only works for RNN-based models. In this paper, we
develop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP for
state-of-the-art large transformer-based models. Our method is easy to
implement: it first fine-tunes the model with redacted in-domain data, and then
fine-tunes it again with the original in-domain data using a private training
mechanism. Furthermore, we study the scenario of imperfect implementation of
policy functions that misses sensitive tokens and develop systematic methods to
handle it. Experiments show that our method achieves strong utility compared to
previous baselines. We also analyze the SDP privacy guarantee empirically with
the canary insertion attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Speech Representation Learning: A Review. (arXiv:2205.10643v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10643">
<div class="article-summary-box-inner">
<span><p>Although supervised deep learning has revolutionized speech and audio
processing, it has necessitated the building of specialist models for
individual tasks and application scenarios. It is likewise difficult to apply
this to dialects and languages for which only limited labeled data is
available. Self-supervised representation learning methods promise a single
universal model that would benefit a wide variety of tasks and domains. Such
methods have shown success in natural language processing and computer vision
domains, achieving new levels of performance while reducing the number of
labels required for many downstream scenarios. Speech representation learning
is experiencing similar progress in three main categories: generative,
contrastive, and predictive methods. Other approaches rely on multi-modal data
for pre-training, mixing text or visual data streams with speech. Although
self-supervised speech representation is still a nascent research area, it is
closely related to acoustic word embedding and learning with zero lexical
resources, both of which have seen active research for many years. This review
presents approaches for self-supervised speech representation learning and
their connection to other research areas. Since many current methods focus
solely on automatic speech recognition as a downstream task, we review recent
efforts on benchmarking learned representations to extend the application
beyond speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is a Question Decomposition Unit All We Need?. (arXiv:2205.12538v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12538">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LMs) have achieved state-of-the-art performance on
many Natural Language Processing (NLP) benchmarks. With the growing number of
new benchmarks, we build bigger and more complex LMs. However, building new LMs
may not be an ideal option owing to the cost, time and environmental impact
associated with it. We explore an alternative route: can we modify data by
expressing it in terms of the model's strengths, so that a question becomes
easier for models to answer? We investigate if humans can decompose a hard
question into a set of simpler questions that are relatively easier for models
to solve. We analyze a range of datasets involving various forms of reasoning
and find that it is indeed possible to significantly improve model performance
(24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via
decomposition. Our approach provides a viable option to involve people in NLP
research in a meaningful way. Our findings indicate that Human-in-the-loop
Question Decomposition (HQD) can potentially provide an alternate path to
building large LMs. Code and data is available at
https://github.com/Pruthvi98/QuestionDecomposition
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models. (arXiv:2205.15223v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15223">
<div class="article-summary-box-inner">
<span><p>Pre-trained masked language models successfully perform few-shot learning by
formulating downstream tasks as text infilling. However, as a strong
alternative in full-shot settings, discriminative pre-trained models like
ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based
few-shot learning to ELECTRA and show that it outperforms masked language
models in a wide range of tasks. ELECTRA is pre-trained to distinguish if a
token is generated or original. We naturally extend that to prompt-based
few-shot learning by training to score the originality of the target options
without introducing new parameters. Our method can be easily adapted to tasks
involving multi-token predictions without extra computation overhead. Analysis
shows that ELECTRA learns distributions that align better with downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06565">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained language models (LMs) without making any architectural
changes has become a norm for learning various language downstream tasks.
However, for non-language downstream tasks, a common practice is to employ
task-specific designs for input, output layers, and loss functions. For
instance, it is possible to fine-tune an LM into an MNIST classifier by
replacing the word embedding layer with an image patch embedding layer, the
word token output layer with a 10-way output layer, and the word prediction
loss with a 10-way classification loss, respectively. A natural question
arises: Can LM fine-tuning solve non-language downstream tasks without changing
the model architecture or loss function? To answer this, we propose
Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations
by conducting an extensive empirical study on a suite of non-language
classification and regression tasks. LIFT does not make any changes to the
model architecture or loss function, and it solely relies on the natural
language interface, enabling "no-code machine learning with LMs." We find that
LIFT performs comparably well across a wide range of low-dimensional
classification and regression tasks, matching the performances of the best
baselines in many cases, especially for the classification tasks. We also
report experimental results on the fundamental properties of LIFT, including
inductive bias, robustness, and sample complexity. We also analyze the effect
of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g.,
context-aware learning via appropriate prompting, calibrated predictions, data
generation, and two-stage fine-tuning. Our code is available at
https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Properties of Generated Corpora. (arXiv:2206.11219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11219">
<div class="article-summary-box-inner">
<span><p>Models for text generation have become focal for many research tasks and
especially for the generation of sentence corpora. However, understanding the
properties of an automatically generated text corpus remains challenging. We
propose a set of tools that examine the properties of generated text corpora.
Applying these tools on various generated corpora allowed us to gain new
insights into the properties of the generative models. As part of our
characterization process, we found remarkable differences in the corpora
generated by two leading generative technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Similarity is More Valuable than Character Similarity: An Empirical Study for Chinese Spell Checking. (arXiv:2207.09217v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09217">
<div class="article-summary-box-inner">
<span><p>Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling
errors. In recent years, related researches focus on introducing the character
similarity from confusion set to enhance the CSC models, ignoring the context
of characters that contain richer information. To make better use of contextual
similarity, we propose a simple yet effective curriculum learning framework for
the CSC task. With the help of our designed model-agnostic framework, existing
CSC models will be trained from easy to difficult as humans learn Chinese
characters and achieve further performance improvements. Extensive experiments
and detailed analyses on widely used SIGHAN datasets show that our method
outperforms previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReFactor GNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09980">
<div class="article-summary-box-inner">
<span><p>Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and generalise to unseen nodes in inductive settings. Our work bridges
the gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture
draws upon both modelling paradigms, which previously were largely thought of
as disjoint. Concretely, using a message-passing formalism, we show how FMs can
be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our ReFactor GNNs. Across
a multitude of well-established KGC benchmarks, our ReFactor GNNs achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks. (arXiv:2208.14923v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.14923">
<div class="article-summary-box-inner">
<span><p>Clinical Natural Language Processing (NLP) has become an emerging technology
in healthcare that leverages a large amount of free-text data in electronic
health records (EHRs) to improve patient care, support clinical decisions, and
facilitate clinical and translational science research. Recently, deep learning
has achieved state-of-the-art performance in many clinical NLP tasks. However,
training deep learning models usually requires large annotated datasets, which
are normally not publicly available and can be time-consuming to build in
clinical domains. Working with smaller annotated datasets is typical in
clinical NLP and therefore, ensuring that deep learning models perform well is
crucial for the models to be used in real-world applications. A widely adopted
approach is fine-tuning existing Pre-trained Language Models (PLMs), but these
attempts fall short when the training dataset contains only a few annotated
samples. Few-Shot Learning (FSL) has recently been investigated to tackle this
problem. Siamese Neural Network (SNN) has been widely utilized as an FSL
approach in computer vision, but has not been studied well in NLP. Furthermore,
the literature on its applications in clinical domains is scarce. In this
paper, we propose two SNN-based FSL approaches for clinical NLP, including
Pre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We
evaluated the proposed approaches on two clinical tasks, namely clinical text
classification and clinical named entity recognition. We tested three few-shot
settings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP
tasks were benchmarked using three PLMs, including BERT,BioBERT, and
BioClinicalBERT. The experimental results verified the effectiveness of the
proposed SNN-based FSL approaches in both NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Focused Study on Sequence Length for Dialogue Summarization. (arXiv:2209.11910v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.11910">
<div class="article-summary-box-inner">
<span><p>Output length is critical to dialogue summarization systems. The dialogue
summary length is determined by multiple factors, including dialogue
complexity, summary objective, and personal preferences. In this work, we
approach dialogue summary length from three perspectives. First, we analyze the
length differences between existing models' outputs and the corresponding human
references and find that summarization models tend to produce more verbose
summaries due to their pretraining objectives. Second, we identify salient
features for summary length prediction by comparing different model settings.
Third, we experiment with a length-aware summarizer and show notable
improvement on existing models if summary length can be well incorporated.
Analysis and experiments are conducted on popular DialogSum and SAMSum datasets
to validate our findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction and Applications of Billion-Scale Pre-trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15214">
<div class="article-summary-box-inner">
<span><p>Business Knowledge Graphs (KGs) are important to many enterprises today,
providing factual knowledge and structured data that steer many products and
make them more intelligent. Despite their promising benefits, building business
KG necessitates solving prohibitive issues of deficient structure and multiple
modalities. In this paper, we advance the understanding of the practical
challenges related to building KG in non-trivial real-world systems. We
introduce the process of building an open business knowledge graph (OpenBG)
derived from a well-known enterprise, Alibaba Group. Specifically, we define a
core ontology to cover various abstract products and consumption demands, with
fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is
an open business KG of unprecedented scale: 2.6 billion triples with more than
88 million entities covering over 1 million core classes/concepts and 2,681
types of relations. We release all the open resources (OpenBG benchmarks)
derived from it for the community and report experimental results of KG-centric
tasks. We also run up an online competition based on OpenBG benchmarks, and has
attracted thousands of teams. We further pre-train OpenBG and apply it to many
KG- enhanced downstream tasks in business scenarios, demonstrating the
effectiveness of billion-scale multimodal knowledge for e-commerce. All the
resources with codes have been released at
\url{https://github.com/OpenBGBenchmark/OpenBG}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment. (arXiv:2210.01478v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01478">
<div class="article-summary-box-inner">
<span><p>AI systems are becoming increasingly intertwined with human life. In order to
effectively collaborate with humans and ensure safety, AI systems need to be
able to understand, interpret and predict human moral judgments and decisions.
Human moral judgments are often guided by rules, but not always. A central
challenge for AI safety is capturing the flexibility of the human moral mind --
the ability to determine when a rule should be broken, especially in novel or
unusual situations. In this paper, we present a novel challenge set consisting
of rule-breaking question answering (RBQA) of cases that involve potentially
permissible rule-breaking -- inspired by recent moral psychology studies. Using
a state-of-the-art large language model (LLM) as a basis, we propose a novel
moral chain of thought (MORALCOT) prompting strategy that combines the
strengths of LLMs with theories of moral reasoning developed in cognitive
science to predict human moral judgments. MORALCOT outperforms seven existing
LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to
capture the flexibility of the human moral mind. We also conduct a detailed
error analysis to suggest directions for future work to improve AI safety using
RBQA. Our data is open-sourced at
https://huggingface.co/datasets/feradauto/MoralExceptQA and code at
https://github.com/feradauto/MoralCoT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection. (arXiv:2210.03221v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03221">
<div class="article-summary-box-inner">
<span><p>With careful manipulation, malicious agents can reverse engineer private
information encoded in pre-trained language models. Security concerns motivate
the development of quantum pre-training. In this work, we propose a highly
portable quantum language model (PQLM) that can easily transmit information to
downstream tasks on classical machines. The framework consists of a cloud PQLM
built with random Variational Quantum Classifiers (VQC) and local models for
downstream applications. We demonstrate the ad hoc portability of the quantum
model by extracting only the word embeddings and effectively applying them to
downstream tasks on classical machines. Our PQLM exhibits comparable
performance to its classical counterpart on both intrinsic evaluation (loss,
perplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy)
metrics. We also perform ablation studies on the factors affecting PQLM
performance to analyze model stability. Our work establishes a theoretical
foundation for a portable quantum pre-trained language model that could be
trained on private data and made available for public use with privacy
protection guarantees.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching Biomedical Knowledge for Vietnamese Low-resource Language Through Large-Scale Translation. (arXiv:2210.05598v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05598">
<div class="article-summary-box-inner">
<span><p>Biomedical data and benchmarks are highly valuable yet very limited in
low-resource languages other than English such as Vietnamese. In this paper, we
make use of a state-of-the-art translation model in English-Vietnamese to
translate and produce both pretrained as well as supervised data in the
biomedical domains. Thanks to such large-scale translation, we introduce
ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20
million translated abstracts from the high-quality public PubMed corpus.
ViPubMedT5 demonstrates state-of-the-art results on two different biomedical
benchmarks in summarization and acronym disambiguation. Further, we release
ViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the
recently public En-vi translation model and carefully refined by human experts,
with evaluations of existing methods against ViPubmedT5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuBERT-TR: Reviving Turkish Automatic Speech Recognition with Self-supervised Speech Representation Learning. (arXiv:2210.07323v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07323">
<div class="article-summary-box-inner">
<span><p>While the Turkish language is listed among low-resource languages, literature
on Turkish automatic speech recognition (ASR) is relatively old. In this paper,
we present HuBERT-TR, a speech representation model for Turkish, based on
HuBERT. HuBERT-TR achieves state-of-the-art results on several Turkish ASR
datasets. We investigate pre-training HuBERT for Turkish with large-scale data
curated from online resources. We pre-train HuBERT-TR using over 6,500 hours of
speech data curated from YouTube that includes extensive variability in terms
of quality and genre. We show that language-specific models are superior to
other pre-trained models, where our Turkish model HuBERT-TR/base performs
better than the x10 times larger state-of-the-art multilingual XLS-R-1b model
in low-resource settings. Moreover, we study the effect of scaling on ASR
performance by scaling our models up to 1B parameters. Our best model yields a
state-of-the-art word error rate of 4.97% on the Turkish Broadcast News
dataset. Models are available at https://huggingface.co/asafaya
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction. (arXiv:2210.10709v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10709">
<div class="article-summary-box-inner">
<span><p>Information Extraction, which aims to extract structural relational triple or
event from unstructured texts, often suffers from data scarcity issues. With
the development of pre-trained language models, many prompt-based approaches to
data-efficient information extraction have been proposed and achieved
impressive performance. However, existing prompt learning methods for
information extraction are still susceptible to several potential limitations:
(i) semantic gap between natural language and output structure knowledge with
pre-defined schema; (ii) representation learning with locally individual
instances limits the performance given the insufficient features. In this
paper, we propose a novel approach of schema-aware Reference As Prompt (RAP),
which dynamically leverage schema and knowledge inherited from global
(few-shot) training data for each sample. Specifically, we propose a
schema-aware reference store, which unifies symbolic schema and relevant
textual instances. Then, we employ a dynamic reference integration module to
retrieve pertinent knowledge from the datastore as prompts during training and
inference. Experimental results demonstrate that RAP can be plugged into
various existing models and outperforms baselines in low-resource settings on
four datasets of relational triple extraction and event extraction. In
addition, we provide comprehensive empirical ablations and case analysis
regarding different types and scales of knowledge in order to better understand
the mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Pre-Training with Sparse Latent Typing. (arXiv:2210.12582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12582">
<div class="article-summary-box-inner">
<span><p>Modern large-scale Pre-trained Language Models (PLMs) have achieved
tremendous success on a wide range of downstream tasks. However, most of the LM
pre-training objectives only focus on text reconstruction, but have not sought
to learn latent-level interpretable representations of sentences. In this
paper, we manage to push the language models to obtain a deeper understanding
of sentences by proposing a new pre-training objective, Sparse Latent Typing,
which enables the model to sparsely extract sentence-level keywords with
diverse latent types. Experimental results show that our model is able to learn
interpretable latent type categories in a self-supervised manner without using
any external knowledge. Besides, the language model pre-trained with such an
objective also significantly improves Information Extraction related downstream
tasks in both supervised and few-shot settings. Our code is publicly available
at: https://github.com/renll/SparseLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus Is What You Need For Chinese Grammatical Error Correction. (arXiv:2210.12692v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12692">
<div class="article-summary-box-inner">
<span><p>Chinese Grammatical Error Correction (CGEC) aims to automatically detect and
correct grammatical errors contained in Chinese text. In the long term,
researchers regard CGEC as a task with a certain degree of uncertainty, that
is, an ungrammatical sentence may often have multiple references. However, we
argue that even though this is a very reasonable hypothesis, it is too harsh
for the intelligence of the mainstream models in this era. In this paper, we
first discover that multiple references do not actually bring positive gains to
model training. On the contrary, it is beneficial to the CGEC model if the
model can pay attention to small but essential data during the training
process. Furthermore, we propose a simple yet effective training strategy
called OneTarget to improve the focus ability of the CGEC models and thus
improve the CGEC performance. Extensive experiments and detailed analyses
demonstrate the correctness of our discovery and the effectiveness of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13397">
<div class="article-summary-box-inner">
<span><p>Language barriers present a great challenge in our increasingly connected and
global world. Especially within the medical domain, e.g. hospital or emergency
room, communication difficulties and delays may lead to malpractice and
non-optimal patient care. In the HYKIST project, we consider patient-physician
communication, more specifically between a German-speaking physician and an
Arabic- or Vietnamese-speaking patient. Currently, a doctor can call the
Triaphon service to get assistance from an interpreter in order to help
facilitate communication. The HYKIST goal is to support the usually
non-professional bilingual interpreter with an automatic speech translation
system to improve patient care and help overcome language barriers. In this
work, we present our ASR system development efforts for this conversational
telephone speech translation task in the medical domain for two languages
pairs, data collection, various acoustic model architectures and
dialect-induced difficulties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14389">
<div class="article-summary-box-inner">
<span><p>Research on Korean grammatical error correction (GEC) is limited compared to
other major languages such as English and Chinese. We attribute this
problematic circumstance to the lack of a carefully designed evaluation
benchmark for Korean. Thus, in this work, we first collect three datasets from
different sources (Kor-Lang8, Kor-Native, and Kor-Learner) to cover a wide
range of error types and annotate them using our newly proposed tool called
Korean Automatic Grammatical error Annotation System (KAGAS). KAGAS is a
carefully designed edit alignment &amp; classification tool that considers the
nature of Korean on generating an alignment between a source sentence and a
target sentence, and identifies error types on each aligned edit. We also
present baseline models fine-tuned over our datasets. We show that the model
trained with our datasets significantly outperforms the public statistical GEC
system (Hanspell) on a wider range of error types, demonstrating the diversity
and usefulness of the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Speech Segmentation using Acousto-Linguistic Features with look-ahead. (arXiv:2210.14446v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14446">
<div class="article-summary-box-inner">
<span><p>Segmentation for continuous Automatic Speech Recognition (ASR) has
traditionally used silence timeouts or voice activity detectors (VADs), which
are both limited to acoustic features. This segmentation is often overly
aggressive, given that people naturally pause to think as they speak.
Consequently, segmentation happens mid-sentence, hindering both punctuation and
downstream tasks like machine translation for which high-quality segmentation
is critical. Model-based segmentation methods that leverage acoustic features
are powerful, but without an understanding of the language itself, these
approaches are limited. We present a hybrid approach that leverages both
acoustic and language information to improve segmentation. Furthermore, we show
that including one word as a look-ahead boosts segmentation quality. On
average, our models improve segmentation-F0.5 score by 9.8% over baseline. We
show that this approach works for multiple languages. For the downstream task
of machine translation, it improves the translation BLEU score by an average of
1.05 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Sentence Sampling by Virtual Adversarial Perturbation. (arXiv:2210.14576v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14576">
<div class="article-summary-box-inner">
<span><p>Active learning for sentence understanding attempts to reduce the annotation
cost by identifying the most informative examples. Common methods for active
learning use either uncertainty or diversity sampling in the pool-based
scenario. In this work, to incorporate both predictive uncertainty and sample
diversity, we propose Virtual Adversarial Perturbation for Active Learning
(VAPAL) , an uncertainty-diversity combination framework, using virtual
adversarial perturbation (Miyato et al., 2019) as model uncertainty
representation. VAPAL consistently performs equally well or even better than
the strong baselines on four sentence understanding datasets: AGNEWS, IMDB,
PUBMED, and SST-2, offering a potential option for active learning on sentence
understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Task: Deriving Semantic Class Targets for the Physical Sciences. (arXiv:2210.14760v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14760">
<div class="article-summary-box-inner">
<span><p>We define deriving semantic class targets as a novel multi-modal task. By
doing so, we aim to improve classification schemes in the physical sciences
which can be severely abstracted and obfuscating. We address this task for
upcoming radio astronomy surveys and present the derived semantic radio galaxy
morphology class targets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Semantic Parsing: From Images to Abstract Meaning Representation. (arXiv:2210.14862v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14862">
<div class="article-summary-box-inner">
<span><p>The success of scene graphs for visual scene understanding has brought
attention to the benefits of abstracting a visual input (e.g., image) into a
structured representation, where entities (people and objects) are nodes
connected by edges specifying their relations. Building these representations,
however, requires expensive manual annotation in the form of images paired with
their scene graphs or frames. These formalisms remain limited in the nature of
entities and relations they can capture. In this paper, we propose to leverage
a widely-used meaning representation in the field of natural language
processing, the Abstract Meaning Representation (AMR), to address these
shortcomings. Compared to scene graphs, which largely emphasize spatial
relationships, our visual AMR graphs are more linguistically informed, with a
focus on higher-level semantic concepts extrapolated from visual input.
Moreover, they allow us to generate meta-AMR graphs to unify information
contained in multiple image descriptions under one representation. Through
extensive experimentation and analysis, we demonstrate that we can re-purpose
an existing text-to-AMR parser to parse images into AMRs. Our findings point to
important future research directions for improved scene understanding.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-28 23:18:24.043599814 UTC">2022-10-28 23:18:24 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>