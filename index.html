<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-31T01:30:00Z">10-31</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase Generation. (arXiv:1909.09485v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.09485">
<div class="article-summary-box-inner">
<span><p>This study mainly investigates two common decoding problems in neural
keyphrase generation: sequence length bias and beam diversity. To tackle the
problems, we introduce a beam search decoding strategy based on word-level and
ngram-level reward function to constrain and refine Seq2Seq inference at test
time. Results show that our simple proposal can overcome the algorithm bias to
shorter and nearly identical sequences, resulting in a significant improvement
of the decoding performance on generating keyphrases that are present and
absent in source text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00269">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models learn informative word representations on a
large-scale text corpus through self-supervised learning, which has achieved
promising performance in fields of natural language processing (NLP) after
fine-tuning. These models, however, suffer from poor robustness and lack of
interpretability. We refer to pre-trained language models with knowledge
injection as knowledge-enhanced pre-trained language models (KEPLMs). These
models demonstrate deep understanding and logical reasoning and introduce
interpretability. In this survey, we provide a comprehensive overview of KEPLMs
in NLP. We first discuss the advancements in pre-trained language models and
knowledge representation learning. Then we systematically categorize existing
KEPLMs from three different perspectives. Finally, we outline some potential
directions of KEPLMs for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03897">
<div class="article-summary-box-inner">
<span><p>Pre-trained multi-modal models, such as CLIP, provide transferable embeddings
and show promising results in diverse applications. However, the analysis of
learned multi-modal embeddings is relatively unexplored, and the embedding
transferability can be improved. In this work, we observe that CLIP holds
separated embedding subspaces for two different modalities, and then we
investigate it through the lens of uniformity-alignment to measure the quality
of learned representation. Both theoretically and empirically, we show that
CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack
of alignment and uniformity might restrict the transferability and robustness
of embeddings. To this end, we devise a new fine-tuning method for robust
representation equipping better alignment and uniformity. First, we propose a
Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to
generate hard negative samples on the hypersphere. Then, we fine-tune the model
on hard negatives as well as original negatives and positives with contrastive
loss. Based on the theoretical analysis about hardness guarantee and limiting
behavior, we justify the use of our method. Extensive experiments on retrieval,
calibration, few- or zero-shot classification (under distribution shift),
embedding arithmetic, and image captioning further show that our method
provides transferable representations, enabling robust model adaptation on
diverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation. (arXiv:2203.16487v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16487">
<div class="article-summary-box-inner">
<span><p>We propose Speculative Decoding (SpecDec), for the first time ever, to
formally study exploiting the idea of speculative execution to accelerate
autoregressive (AR) decoding. Speculative Decoding has two innovations:
Spec-Drafter -- an independent model specially optimized for efficient and
accurate drafting -- and Spec-Verification -- a reliable method for verifying
the drafted tokens efficiently in the decoding paradigm. Experimental results
on various seq2seq tasks including machine translation and abstractive
summarization show our approach can achieve around $5\times$ speedup for the
popular Transformer architectures with comparable generation quality to beam
search decoding, refreshing the impression that the draft-then-verify paradigm
introduces only $1.4\times$$\sim$$2\times$ speedup. In addition to the
remarkable speedup, we also demonstrate 3 additional advantages of SpecDec,
revealing its practical value for accelerating generative models in real-world
applications. Our models and codes are available at
https://github.com/hemingkx/SpecDec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07550">
<div class="article-summary-box-inner">
<span><p>Standardized and quantified evaluation of machine behaviors is a crux of
understanding LLMs. In this study, we draw inspiration from psychometric
studies by leveraging human personality theory as a tool for studying machine
behaviors. Originating as a philosophical quest for human behaviors, the study
of personality delves into how individuals differ in thinking, feeling, and
behaving. Toward building and understanding human-like social machines, we are
motivated to ask: Can we assess machine behaviors by leveraging human
psychometric tests in a principled and quantitative manner? If so, can we
induce a specific personality in LLMs? To answer these questions, we introduce
the Machine Personality Inventory (MPI) tool for studying machine behaviors;
MPI follows standardized personality tests, built upon the Big Five Personality
Factors (Big Five) theory and personality assessment inventories. By
systematically evaluating LLMs with MPI, we provide the first piece of evidence
demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise
a Personality Prompting (P^2) method to induce LLMs with specific personalities
in a controllable way, capable of producing diverse and verifiable behaviors.
We hope this work sheds light on future studies by adopting personality as the
essential indicator for various downstream tasks, and could further motivate
research into equally intriguing human-like machine behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07051">
<div class="article-summary-box-inner">
<span><p>Abstract reasoning is a key ability for an intelligent system. Large language
models (LMs) achieve above-chance performance on abstract reasoning tasks, but
exhibit many imperfections. However, human abstract reasoning is also
imperfect. For example, human reasoning is affected by our real-world knowledge
and beliefs, and shows notable "content effects"; humans reason more reliably
when the semantic content of a problem supports the correct logical inferences.
These content-entangled reasoning patterns play a central role in debates about
the fundamental nature of human intelligence. Here, we investigate whether
language models $\unicode{x2014}$ whose prior expectations capture some aspects
of human knowledge $\unicode{x2014}$ similarly mix content into their answers
to logical problems. We explored this question across three logical reasoning
tasks: natural language inference, judging the logical validity of syllogisms,
and the Wason selection task. We evaluate state of the art large language
models, as well as humans, and find that the language models reflect many of
the same patterns observed in humans across these tasks $\unicode{x2014}$ like
humans, models answer more accurately when the semantic content of a task
supports the logical inferences. These parallels are reflected both in answer
patterns, and in lower-level features like the relationship between model
answer distributions and human response times. Our findings have implications
for understanding both these cognitive effects in humans, and the factors that
contribute to language model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark. (arXiv:2207.13005v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13005">
<div class="article-summary-box-inner">
<span><p>Modern Entity Linking (EL) systems entrench a popularity bias, yet there is
no dataset focusing on tail and emerging entities in languages other than
English. We present Hansel, a new benchmark in Chinese that fills the vacancy
of non-English few-shot and zero-shot EL challenges. The test set of Hansel is
human annotated and reviewed, created with a novel method for collecting
zero-shot EL datasets. It covers 10K diverse documents in news, social media
posts and other web articles, with Wikidata as its target Knowledge Base. We
demonstrate that the existing state-of-the-art EL system performs poorly on
Hansel (R@1 of 36.6% on Few-Shot). We then establish a strong baseline that
scores a R@1 of 46.2% on Few-Shot and 76.6% on Zero-Shot on our dataset. We
also show that our baseline achieves competitive results on TAC-KBP2015 Chinese
Entity Linking task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04074">
<div class="article-summary-box-inner">
<span><p>Natural language expresses events with varying granularities, where
coarse-grained events (goals) can be broken down into finer-grained event
sequences (steps). A critical yet overlooked aspect of understanding event
processes is recognizing that not all step events hold equal importance toward
the completion of a goal. In this paper, we address this gap by examining the
extent to which current models comprehend the essentiality of step events in
relation to a goal event. Cognitive studies suggest that such capability
enables machines to emulate human commonsense reasoning about preconditions and
necessary efforts of everyday tasks. We contribute a high-quality corpus of
(goal, step) pairs gathered from the community guideline website WikiHow, with
steps manually annotated for their essentiality concerning the goal by experts.
The high inter-annotator agreement demonstrates that humans possess a
consistent understanding of event essentiality. However, after evaluating
multiple statistical and largescale pre-trained language models, we find that
existing approaches considerably underperform compared to humans. This
observation highlights the need for further exploration into this critical and
challenging task. The dataset and code are available at
<a href="http://cogcomp.org/page/publication_view/1023.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models. (arXiv:2210.04802v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04802">
<div class="article-summary-box-inner">
<span><p>Large code datasets have become increasingly accessible for pre-training
source code models. However, for the fine-tuning phase, obtaining
representative training data that fully covers the code distribution for
specific downstream tasks remains challenging due to the task-specific nature
and limited labeling resources. Moreover, fine-tuning pretrained models can
result in forgetting previously acquired pre-training knowledge. These lead to
out-of-distribution (OOD) generalization issues with unexpected model inference
behaviors that have not been systematically studied yet. In this paper, we
contribute the first systematic approach that simulates various OOD scenarios
along different dimensions of source code data properties and study the
fine-tuned model behaviors in such scenarios. We investigate the behaviors of
models under different fine-tuning methodologies, including full fine-tuning
and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis,
conducted on four state-of-the-art pretrained models and applied to two code
generation tasks, exposes multiple failure modes attributed to OOD
generalization issues. Additionally, our analysis uncovers that LoRA
fine-tuning consistently exhibits significantly better OOD generalization
performance than full fine-tuning across various scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition. (arXiv:2210.12770v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12770">
<div class="article-summary-box-inner">
<span><p>The practice of fine-tuning Pre-trained Language Models (PLMs) from general
or domain-specific data to a specific task with limited resources, has gained
popularity within the field of natural language processing (NLP). In this work,
we re-visit this assumption and carry out an investigation in clinical NLP,
specifically Named Entity Recognition on drugs and their related attributes. We
compare Transformer models that are trained from scratch to fine-tuned
BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine
the impact of an additional CRF layer on such models to encourage contextual
learning. We use n2c2-2018 shared task data for model development and
evaluations. The experimental outcomes show that 1) CRF layers improved all
language models; 2) referring to BIO-strict span level evaluation using
macro-average F1 score, although the fine-tuned LLMs achieved 0.83+ scores, the
TransformerCRF model trained from scratch achieved 0.78+, demonstrating
comparable performances with much lower cost - e.g. with 39.80\% less training
parameters; 3) referring to BIO-strict span-level evaluation using
weighted-average F1 score, ClinicalBERT-CRF, BERT-CRF, and TransformerCRF
exhibited lower score differences, with 97.59\%/97.44\%/96.84\% respectively.
4) applying efficient training by down-sampling for better data distribution
further reduced the training cost and need for data, while maintaining similar
scores - i.e. around 0.02 points lower compared to using the full dataset. Our
models will be hosted at \url{https://github.com/HECTA-UoM/TransformerCRF}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data. (arXiv:2210.17122v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17122">
<div class="article-summary-box-inner">
<span><p>Inspired by early research on exploring naturally annotated data for Chinese
word segmentation (CWS), and also by recent research on integration of speech
and text processing, this work for the first time proposes to mine word
boundaries from parallel speech/text data. First we collect parallel
speech/text data from two Internet sources that are related with CWS data used
in our experiments. Then, we obtain character-level alignments and design
simple heuristic rules for determining word boundaries according to pause
duration between adjacent characters. Finally, we present an effective
complete-then-train strategy that can better utilize extra naturally annotated
data for model training. Experiments demonstrate our approach can significantly
boost CWS performance in both cross-domain and low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MelHuBERT: A simplified HuBERT on Mel spectrograms. (arXiv:2211.09944v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09944">
<div class="article-summary-box-inner">
<span><p>Self-supervised models have had great success in learning speech
representations that can generalize to various downstream tasks. However, most
self-supervised models require a large amount of compute and multiple GPUs to
train, significantly hampering the development of self-supervised learning. In
an attempt to reduce the computation of training, we revisit the training of
HuBERT, a highly successful self-supervised model. We improve and simplify
several key components, including the loss function, input representation, and
training in multiple stages. Our model, MelHuBERT, is able to achieve favorable
performance on phone recognition, speaker identification, and automatic speech
recognition against HuBERT, while saving 31.2% of the pre-training time, or
equivalently 33.5% MACs per one second speech. The code and pre-trained models
are available in https://github.com/nervjack2/MelHuBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion. (arXiv:2212.09114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09114">
<div class="article-summary-box-inner">
<span><p>The dual-encoder has become the de facto architecture for dense retrieval.
Typically, it computes the latent representations of the query and document
independently, thus failing to fully capture the interactions between the query
and document. To alleviate this, recent research has focused on obtaining
query-informed document representations. During training, it expands the
document with a real query, but during inference, it replaces the real query
with a generated one. This inconsistency between training and inference causes
the dense retrieval model to prioritize query information while disregarding
the document when computing the document representation. Consequently, it
performs even worse than the vanilla dense retrieval model because its
performance heavily relies on the relevance between the generated queries and
the real query.In this paper, we propose a curriculum sampling strategy that
utilizes pseudo queries during training and progressively enhances the
relevance between the generated query and the real query. By doing so, the
retrieval model learns to extend its attention from the document alone to both
the document and query, resulting in high-quality query-informed document
representations. Experimental results on both in-domain and out-of-domain
datasets demonstrate that our approach outperforms previous dense retrieval
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keep it Neutral: Using Natural Language Inference to Improve Generation. (arXiv:2302.08577v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08577">
<div class="article-summary-box-inner">
<span><p>We explore incorporating natural language inference (NLI) into the text
generative pipeline by using a pre-trained NLI model to assess whether a
generated sentence entails, contradicts, or is neutral to the prompt and
preceding text. First, we show that the NLI task is predictive of generation
errors made by GPT-3. We use these results to develop an NLI-informed
generation procedure for GPT-J. Then, we evaluate these generations by
obtaining human annotations on error types and overall quality. We find that an
NLI strategy of maximizing entailment improves text generation when the nucleus
sampling randomness parameter value is high, while one which maximizes
contradiction is in fact productive when the parameter value is low. Overall,
though, we demonstrate that an NLI strategy of maximizing the neutral class
provides the highest quality of generated text (significantly better than the
vanilla generations), regardless of parameter value.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10850">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) has shown great promise for developing dialogue
management (DM) agents that are non-myopic, conduct rich conversations, and
maximize overall user satisfaction. Despite recent developments in RL and
language models (LMs), using RL to power conversational chatbots remains
challenging, in part because RL requires online exploration to learn
effectively, whereas collecting novel human-bot interactions can be expensive
and unsafe. This issue is exacerbated by the combinatorial action spaces facing
these algorithms, as most LM agents generate responses at the word level. We
develop a variety of RL algorithms, specialized to dialogue planning, that
leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that
capture diverse semantics, generate utterances reflecting different intents,
and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods
significantly reduce the size of the action space and improve the efficacy of
RL-based DM. We evaluate our methods in open-domain dialogue to demonstrate
their effectiveness w.r.t.\ the diversity of intent in generated utterances and
overall DM performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12247">
<div class="article-summary-box-inner">
<span><p>The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04132">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have great potential for synthetic data
generation. This work shows that useful data can be synthetically generated
even for tasks that cannot be solved directly by LLMs: for problems with
structured outputs, it is possible to prompt an LLM to perform the task in the
reverse direction, by generating plausible input text for a target output
structure. Leveraging this asymmetry in task difficulty makes it possible to
produce large-scale, high-quality data for complex tasks. We demonstrate the
effectiveness of this approach on closed information extraction, where
collecting ground-truth data is challenging, and no satisfactory dataset exists
to date. We synthetically generate a dataset of 1.8M data points, establish its
superior quality compared to existing datasets in a human evaluation, and use
it to finetune small models (220M and 770M parameters), termed SynthIE, that
outperform the prior state of the art (with equal model size) by a substantial
margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data,
and models are available at https://github.com/epfl-dlab/SynthIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17475">
<div class="article-summary-box-inner">
<span><p>This article describes an efficient method to learn distributed
representations, also known as embeddings. This is accomplished minimizing an
objective function similar to the one introduced in the Word2Vec algorithm and
later adopted in several works. The optimization computational bottleneck is
the calculation of the softmax normalization constants for which a number of
operations scaling quadratically with the sample size is required. This
complexity is unsuited for large datasets and negative sampling is a popular
workaround, allowing one to obtain distributed representations in linear time
with respect to the sample size. Negative sampling consists, however, in a
change of the loss function and hence solves a different optimization problem
from the one originally proposed. Our contribution is to show that the sotfmax
normalization constants can be estimated in linear time, allowing us to design
an efficient optimization strategy to learn distributed representations. We
test our approximation on two popular applications related to word and node
embeddings. The results evidence competing performance in terms of accuracy
with respect to negative sampling with a remarkably lower computational time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01969">
<div class="article-summary-box-inner">
<span><p>Text classification is essential for organizing unstructured text.
Traditional methods rely on human annotations or, more recently, a set of class
seed words for supervision, which can be costly, particularly for specialized
or emerging domains. To address this, using class surface names alone as
extremely weak supervision has been proposed. However, existing approaches
treat different levels of text granularity (documents, sentences, or words)
independently, disregarding inter-granularity class disagreements and the
context identifiable exclusively through joint extraction. In order to tackle
these issues, we introduce MEGClass, an extremely weakly-supervised text
classification method that leverages Mutually-Enhancing Text Granularities.
MEGClass utilizes coarse- and fine-grained context signals obtained by jointly
considering a document's most class-indicative words and sentences. This
approach enables the learning of a contextualized document representation that
captures the most discriminative class indicators. By preserving the
heterogeneity of potential classes, MEGClass can select the most informative
class-indicative documents as iterative feedback to enhance the initial
word-based class representations and ultimately fine-tune a pre-trained text
classifier. Extensive experiments on seven benchmark datasets demonstrate that
MEGClass outperforms other weakly and extremely weakly supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04675">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable potential in
handling multilingual machine translation (MMT). In this paper, we
systematically investigate the advantages and challenges of LLMs for MMT by
answering two questions: 1) How well do LLMs perform in translating massive
languages? 2) Which factors affect LLMs' performance in translation? We
thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our
empirical results show that translation capabilities of LLMs are continually
improving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of
translation directions but still faces a large gap towards the commercial
translation system, especially on low-resource languages. Through further
analysis, we discover that LLMs exhibit new working patterns when used for MMT.
First, instruction semantics can surprisingly be ignored when given in-context
exemplars. Second, cross-lingual exemplars can provide better task guidance for
low-resource translation than exemplars in the same language pairs. Third, LLM
can acquire translation ability in a resource-efficient way and generate
moderate translation even on zero-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text. (arXiv:2304.06939v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06939">
<div class="article-summary-box-inner">
<span><p>In-context vision and language models like Flamingo support arbitrarily
interleaved sequences of images and text as input. This format not only enables
few-shot learning via interleaving independent supervised (image, text)
examples, but also, more complex prompts involving interaction between images,
e.g., "What do image A and image B have in common?" To support this interface,
pretraining occurs over web corpora that similarly contain interleaved
images+text. To date, however, large-scale data of this form have not been
publicly available.
</p>
<p>We release Multimodal C4, an augmentation of the popular text-only C4 corpus
with images interleaved. We use a linear assignment algorithm to place images
into longer bodies of text using CLIP features, a process that we show
outperforms alternatives. Multimodal C4 spans everyday topics like cooking,
travel, technology, etc. A manual inspection of a random sample of documents
shows that a vast majority (88%) of images are topically relevant, and that
linear assignment frequently selects individual sentences specifically
well-aligned with each image (80%). After filtering NSFW images, ads, etc., the
resulting corpus consists of 101.2M documents with 571M images interleaved in
43B English tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08315">
<div class="article-summary-box-inner">
<span><p>Dual use, the intentional, harmful reuse of technology and scientific
artefacts, is a problem yet to be well-defined within the context of Natural
Language Processing (NLP). However, as NLP technologies continue to advance and
become increasingly widespread in society, their inner workings have become
increasingly opaque. Therefore, understanding dual use concerns and potential
ways of limiting them is critical to minimising the potential harms of research
and development. In this paper, we conduct a survey of NLP researchers and
practitioners to understand the depth and their perspective of the problem as
well as to assess existing available support. Based on the results of our
survey, we offer a definition of dual use that is tailored to the needs of the
NLP community. The survey revealed that a majority of researchers are concerned
about the potential dual use of their research but only take limited action
toward it. In light of the survey results, we discuss the current state and
potential means for mitigating dual use in NLP and propose a checklist that can
be integrated into existing conference ethics-frameworks, e.g., the ACL ethics
checklist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PUNR: Pre-training with User Behavior Modeling for News Recommendation. (arXiv:2304.12633v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12633">
<div class="article-summary-box-inner">
<span><p>News recommendation aims to predict click behaviors based on user behaviors.
How to effectively model the user representations is the key to recommending
preferred news. Existing works are mostly focused on improvements in the
supervised fine-tuning stage. However, there is still a lack of PLM-based
unsupervised pre-training methods optimized for user representations. In this
work, we propose an unsupervised pre-training paradigm with two tasks, i.e.
user behavior masking and user behavior generation, both towards effective user
behavior modeling. Firstly, we introduce the user behavior masking pre-training
task to recover the masked user behaviors based on their contextual behaviors.
In this way, the model could capture a much stronger and more comprehensive
user news reading pattern. Besides, we incorporate a novel auxiliary user
behavior generation pre-training task to enhance the user representation vector
derived from the user encoder. We use the above pre-trained user modeling
encoder to obtain news and user representations in downstream fine-tuning.
Evaluations on the real-world news benchmark show significant performance
improvements over existing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting structural hints: Using language models to study inductive biases in language learning. (arXiv:2304.13060v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13060">
<div class="article-summary-box-inner">
<span><p>Both humans and large language models are able to learn language without
explicit structural supervision. What inductive biases make this learning
possible? We address this fundamental cognitive question by leveraging
transformer language models: we inject inductive bias into language models by
pretraining on formally-structured data, and then evaluate the biased learners'
ability to learn typologically-diverse natural languages. Our experimental
setup creates a testbed for hypotheses about inductive bias in human language
learning. We investigate the effect of injecting models with three types of
inductive bias: 1) recursive, hierarchical processing, 2) crossing token-token
relationships that can't be modeled by context-free grammars, and 3) a Zipfian
power-law vocabulary distribution. We show that non-context-free relationships
form the best inductive biases. Our study leverages the capabilities of
transformer models to run controlled language learning experiments that are not
possible to run on humans, and surfaces hypotheses about the structures that
facilitate language learning in both humans and machines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turning Flowchart into Dialog: Augmenting Flowchart-grounded Troubleshooting Dialogs via Synthetic Data Generation. (arXiv:2305.01323v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01323">
<div class="article-summary-box-inner">
<span><p>Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the
instructions of a flowchart to diagnose users' problems in specific domains
(e.g., vehicle, laptop), have been gaining research interest in recent years.
However, collecting sufficient dialogues that are naturally grounded on
flowcharts is costly, thus FTD systems are impeded by scarce training data. To
mitigate the data sparsity issue, we propose a plan-based synthetic data
generation (PlanSDG) approach that generates diverse synthetic dialog data at
scale by transforming concise flowchart into dialogues. Specifically, its
generative model employs a variational-base framework with a hierarchical
planning strategy that includes global and local latent planning variables.
Experiments on the FloDial dataset show that synthetic dialogue produced by
PlanSDG improves the performance of downstream tasks, including flowchart path
retrieval and response generation, in particular on the Out-of-Flowchart
settings. In addition, further analysis demonstrate the quality of synthetic
data generated by PlanSDG in paths that are covered by current sample dialogues
and paths that are not covered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03510">
<div class="article-summary-box-inner">
<span><p>Pre-trained vision and language models such as CLIP have witnessed remarkable
success in connecting images and texts with a primary focus on English texts.
Despite recent efforts to extend CLIP to support other languages, disparities
in performance among different languages have been observed due to uneven
resource availability. Additionally, current cross-lingual transfer methods of
those pre-trained models would consume excessive resources for a large number
of languages. Therefore, we propose a new parameter-efficient cross-lingual
transfer learning framework that utilizes a translation-based alignment method
to mitigate multilingual disparities and explores parameter-efficient
fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive
experiments on XTD and Multi30K datasets, covering 11 languages under
zero-shot, few-shot, and full-dataset learning scenarios, show that our
framework significantly reduces the multilingual disparities among languages
and improves cross-lingual transfer results, especially in low-resource
scenarios, while only keeping and fine-tuning an extremely small number of
parameters compared to the full model (e.g., Our framework only requires 0.16\%
additional parameters of a full-model for each language in the few-shot
learning scenario). The codes are available at
\url{https://github.com/eric-ai-lab/PECTVLM}. The codes are available at
\url{https://github.com/eric-ai-lab/PECTVLM}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03598">
<div class="article-summary-box-inner">
<span><p>How can we interpret and retrieve medical evidence to support clinical
decisions? Clinical trial reports (CTR) amassed over the years contain
indispensable information for the development of personalized medicine.
However, it is practically infeasible to manually inspect over 400,000+
clinical trial reports in order to find the best evidence for experimental
treatments. Natural Language Inference (NLI) offers a potential solution to
this problem, by allowing the scalable computation of textual entailment.
However, existing NLI models perform poorly on biomedical corpora, and
previously published datasets fail to capture the full complexity of inference
over CTRs. In this work, we present a novel resource to advance research on NLI
for reasoning on CTRs. The resource includes two main tasks. Firstly, to
determine the inference relation between a natural language statement, and a
CTR. Secondly, to retrieve supporting facts to justify the predicted relation.
We provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these
tasks. Baselines on this corpus expose the limitations of existing NLI models,
with 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To
the best of our knowledge, we are the first to design a task that covers the
interpretation of full CTRs. To encourage further work on this challenging
dataset, we make the corpus, competition leaderboard, website and code to
replicate the baseline experiments available at:
https://github.com/ai-systems/nli4ct
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Autoregressive Math Word Problem Solver with Unified Tree Structure. (arXiv:2305.04556v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04556">
<div class="article-summary-box-inner">
<span><p>Existing MWP solvers employ sequence or binary tree to present the solution
expression and decode it from given problem description. However, such
structures fail to handle the variants that can be derived via mathematical
manipulation, e.g., $(a_1+a_2) * a_3$ and $a_1 * a_3+a_2 * a_3$ can both be
possible valid solutions for a same problem but formulated as different
expression sequences or trees. The multiple solution variants depicting
different possible solving procedures for the same input problem would raise
two issues: 1) making it hard for the model to learn the mapping function
between the input and output spaces effectively, and 2) wrongly indicating
\textit{wrong} when evaluating a valid expression variant. To address these
issues, we introduce a unified tree structure to present a solution expression,
where the elements are permutable and identical for all the expression
variants. We propose a novel non-autoregressive solver, named \textit{MWP-NAS},
to parse the problem and deduce the solution expression based on the unified
tree. For evaluating the possible expression variants, we design a path-based
metric to evaluate the partial accuracy of expressions of a unified tree. The
results from extensive experiments conducted on Math23K and MAWPS demonstrate
the effectiveness of our proposed MWP-NAS. The codes and checkpoints are
available at: \url{https://github.com/mengqunhan/MWP-NAS}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06156">
<div class="article-summary-box-inner">
<span><p>We present The Vault, a dataset of high-quality code-text pairs in multiple
programming languages for training large language models to understand and
generate code. We present methods for thoroughly extracting samples that use
both rule-based and deep learning-based methods to ensure that they contain
high-quality pairs of code and text, resulting in a dataset of 43 million
high-quality code-text pairs. Our extensive evaluations on common coding tasks
including code generation, code search and code summarization show that when
fine-tuning Code Large Language Models on The Vault, such models outperform the
same models trained on other datasets such as CodeSearchNet. We also provide
detailed analyses of our datasets to assess the effects of various programming
languages and docstrings on the performance of such models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06908">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models (DDPMs) have shown promising
performance for speech synthesis. However, a large number of iterative steps
are required to achieve high sample quality, which restricts the inference
speed. Maintaining sample quality while increasing sampling speed has become a
challenging task. In this paper, we propose a "Co"nsistency "Mo"del-based
"Speech" synthesis method, CoMoSpeech, which achieve speech synthesis through a
single diffusion sampling step while achieving high audio quality. The
consistency constraint is applied to distill a consistency model from a
well-designed diffusion-based teacher model, which ultimately yields superior
performances in the distilled CoMoSpeech. Our experiments show that by
generating audio recordings by a single sampling step, the CoMoSpeech achieves
an inference speed more than 150 times faster than real-time on a single NVIDIA
A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based
speech synthesis truly practical. Meanwhile, objective and subjective
evaluations on text-to-speech and singing voice synthesis show that the
proposed teacher models yield the best audio quality, and the one-step sampling
based CoMoSpeech achieves the best inference speed with better or comparable
audio quality to other conventional multi-step diffusion model baselines. Audio
samples are available at https://comospeech.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10037">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are increasingly adopted for a variety of tasks
with implicit graphical structures, such as planning in robotics, multi-hop
question answering or knowledge probing, structured commonsense reasoning, and
more. While LLMs have advanced the state-of-the-art on these tasks with
structure implications, whether LLMs could explicitly process textual
descriptions of graphs and structures, map them to grounded conceptual spaces,
and perform structured operations remains underexplored. To this end, we
propose NLGraph (Natural Language Graph), a comprehensive benchmark of
graph-based problem solving designed in natural language. NLGraph contains
29,370 problems, covering eight graph reasoning tasks with varying complexity
from simple tasks such as connectivity and shortest path up to complex problems
such as maximum flow and simulating graph neural networks. We evaluate LLMs
(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find
that 1) language models do demonstrate preliminary graph reasoning abilities,
2) the benefit of advanced prompting and in-context learning diminishes on more
complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the
face of spurious correlations in graph and problem settings. We then propose
Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based
approaches to enhance LLMs in solving natural language graph problems.
Build-a-Graph and Algorithmic prompting improve the performance of LLMs on
NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to
solve the most complicated graph reasoning tasks in our setup with language
models remains an open research question. The NLGraph benchmark and evaluation
code are available at https://github.com/Arthur-Heng/NLGraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10156">
<div class="article-summary-box-inner">
<span><p>Comprehending characters' personalities is a crucial aspect of story reading.
As readers engage with a story, their understanding of a character evolves
based on new events and information; and multiple fine-grained aspects of
personalities can be perceived. This leads to a natural problem of situated and
fine-grained personality understanding. The problem has not been studied in the
NLP field, primarily due to the lack of appropriate datasets mimicking the
process of book reading. We present the first labeled dataset PersoNet for this
problem. Our novel annotation strategy involves annotating user notes from
online reading apps as a proxy for the original books. Experiments and human
studies indicate that our dataset construction is both efficient and accurate;
and our task heavily relies on long-term context to achieve accurate
predictions for both machines and humans. The dataset is available at
https://github.com/Gorov/personet_acl23.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Knowledge Assessment for Large Language Models. (arXiv:2305.10519v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10519">
<div class="article-summary-box-inner">
<span><p>Given varying prompts regarding a factoid question, can a large language
model (LLM) reliably generate factually correct answers? Existing LLMs may
generate distinct responses for different prompts. In this paper, we study the
problem of quantifying knowledge contained in an LLM regarding a given set of
facts. We propose KaRR, a statistical approach to assess factual knowledge for
LLMs. The main idea is to estimate the ratio of LLM generating text
corresponding to the answer entity given diverse prompts of the subject and the
querying relation, versus it generating by random chances. Our assessment suite
contains a comprehensive set of 994,123 entities and 600 relations, with
1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes,
including LLaMA, Alpaca, OPT, etc. Experiments show that our results have a
strong correlation (0.43 Kendall's $\tau$) with the results of human assessment
on LLMs. Our results reveal that the knowledge in LLMs with the same backbone
architecture adheres to the scaling law, while tuning on instruction-following
data sometimes compromises the model's capability to generate factually correct
text reliably.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10626">
<div class="article-summary-box-inner">
<span><p>While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical (EWC) for
selective weight updates, combined with low-rank adapters (LoRA) for training
efficiency. Extensive experiments show our approach substantially improves base
LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs
(1.3B, 6B, and 13B) enhanced by our approach match or even outperform much
larger LMs (e.g., ChatGPT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation. (arXiv:2305.11317v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11317">
<div class="article-summary-box-inner">
<span><p>The field of text-to-image (T2I) generation has garnered significant
attention both within the research community and among everyday users. Despite
the advancements of T2I models, a common issue encountered by users is the need
for repetitive editing of input prompts in order to receive a satisfactory
image, which is time-consuming and labor-intensive. Given the demonstrated text
generation power of large-scale language models, such as GPT-k, we investigate
the potential of utilizing such models to improve the prompt editing process
for T2I generation. We conduct a series of experiments to compare the common
edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting
T2I, and examine factors that may influence this process. We found that GPT-k
models focus more on inserting modifiers while humans tend to replace words and
phrases, which includes changes to the subject matter. Experimental results
show that GPT-k are more effective in adjusting modifiers rather than
predicting spontaneous changes in the primary subject matters. Adopting the
edit suggested by GPT-k models may reduce the percentage of remaining edits by
20-30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReTAG: Reasoning Aware Table to Analytic Text Generation. (arXiv:2305.11826v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11826">
<div class="article-summary-box-inner">
<span><p>The task of table summarization involves generating text that both succinctly
and accurately represents the table or a specific set of highlighted cells
within a table. While significant progress has been made in table to text
generation techniques, models still mostly generate descriptive summaries,
which reiterates the information contained within the table in sentences.
Through analysis of popular table to text benchmarks (ToTTo (Parikh et al.,
2020 and InfoTabs (Gupta et al., 2020) we observe that in order to generate the
ideal summary, multiple types of reasoning is needed coupled with access to
knowledge beyond the scope of the table. To address this gap, we propose ReTAG,
a table and reasoning aware model that uses vector-quantization to infuse
different types of analytical reasoning into the output. ReTAG achieves 2.2%,
2.9% improvement on the PARENT metric in the relevant slice of ToTTo and
InfoTabs for the table to text generation task over state of the art baselines.
Through human evaluation, we observe that output from ReTAG is upto 12% more
faithful and analytical compared to a strong table-aware model. To the best of
our knowledge, ReTAG is the first model that can controllably use multiple
reasoning methods within a structure-aware sequence to sequence model to
surpass state of the art performance in multiple table to text tasks. We extend
(and open source 35.6K analytical, 55.9k descriptive instances) the ToTTo,
InfoTabs datasets with the reasoning categories used in each reference
sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LogiCoT: Logical Chain-of-Thought Instruction-Tuning. (arXiv:2305.12147v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12147">
<div class="article-summary-box-inner">
<span><p>Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive
chain-of-thought reasoning ability. Recent work on self-instruction tuning,
such as Alpaca, has focused on enhancing the general proficiency of models.
These instructions enable the model to achieve performance comparable to
GPT-3.5 on general tasks like open-domain text generation and paraphrasing.
However, they fall short of helping the model handle complex reasoning tasks.
To bridge the gap, this paper presents LogiCoT, a new instruction-tuning
dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the
process of harvesting instructions for prompting GPT-4 to generate
chain-of-thought rationales. LogiCoT serves as an instruction set for teaching
models of logical reasoning and elicits general reasoning skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Bidirectional Decoding: Case Study in Morphological Inflection. (arXiv:2305.12580v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12580">
<div class="article-summary-box-inner">
<span><p>Transformer-based encoder-decoder models that generate outputs in a
left-to-right fashion have become standard for sequence-to-sequence tasks. In
this paper, we propose a framework for decoding that produces sequences from
the "outside-in": at each step, the model chooses to generate a token on the
left, on the right, or join the left and right sequences. We argue that this is
more principled than prior bidirectional decoders. Our proposal supports a
variety of model architectures and includes several training methods, such as a
dynamic programming algorithm that marginalizes out the latent ordering
variable. Our model sets state-of-the-art (SOTA) on the 2022 and 2023 shared
tasks, beating the next best systems by over 4.7 and 2.7 points in average
accuracy respectively. The model performs particularly well on long sequences,
can implicitly learn the split point of words composed of stem and affix, and
performs better relative to the baseline on datasets that have fewer unique
lemmas (but more examples per lemma).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models with Rationality. (arXiv:2305.14250v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14250">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) are proficient at question-answering (QA),
it is not always clear how (or even if) an answer follows from their latent
"beliefs". This lack of interpretability is a growing impediment to widespread
use of LLMs. To address this, our goals are to make model beliefs and their
inferential relationships explicit, and to resolve inconsistencies that may
exist, so that answers are supported by interpretable chains of reasoning drawn
from a consistent network of beliefs. Our approach, which we call REFLEX, is to
add a rational, self-reflecting layer on top of the LLM. First, given a
question, we construct a belief graph using a backward-chaining process to
materialize relevant model beliefs (including beliefs about answer candidates)
and their inferential relationships. Second, we identify and minimize
contradictions in that graph using a formal constraint reasoner. We find that
REFLEX significantly improves consistency (by 8%-11% absolute) without harming
overall answer accuracy, resulting in answers supported by faithful chains of
reasoning drawn from a more consistent belief system. This suggests a new style
of system architecture in which an LLM extended with a rational layer can
provide an interpretable window into system beliefs, add a systematic reasoning
capability, and repair latent inconsistencies present in the LLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14257">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) struggle on processing complicated observations
in interactive decision making tasks. To alleviate this issue, we propose a
simple hierarchical prompting approach. Diverging from previous prompting
approaches that always put the full observation (e.g. a web page) to the
prompt, we propose to first construct an action-aware observation which is more
condensed and relevant with a dedicated SUMMARIZER prompt. The ACTOR prompt
then predicts the next action based on the summarized observation. While our
method has broad applicability, we particularly demonstrate its efficacy in the
complex domain of web navigation where a full observation often contains
redundant and irrelevant information. Our approach outperforms the previous
state-of-the-art prompting mechanics by 6.2% on task success rate,
demonstrating its potential on interactive decision making tasks with long
observation traces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia. (arXiv:2305.14292v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14292">
<div class="article-summary-box-inner">
<span><p>This paper presents the first few-shot LLM-based chatbot that almost never
hallucinates and has high conversationality and low latency. WikiChat is
grounded on the English Wikipedia, the largest curated free-text corpus.
</p>
<p>WikiChat generates a response from an LLM, retains only the grounded facts,
and combines them with additional information it retrieves from the corpus to
form factual and engaging responses. We distill WikiChat based on GPT-4 into a
7B-parameter LLaMA model with minimal loss of quality, to significantly improve
its latency, cost and privacy, and facilitate research and deployment.
</p>
<p>Using a novel hybrid human-and-LLM evaluation methodology, we show that our
best system achieves 97.3% factual accuracy in simulated conversations. It
significantly outperforms all retrieval-based and LLM-based baselines, and by
3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4.
Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is
also significantly more informative and engaging, just like an LLM.
</p>
<p>WikiChat achieves 97.9% factual accuracy in conversations with human users
about recent topics, 55.0% better than GPT-4, while receiving significantly
higher user ratings and more favorable comments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14591">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) excel at implementing code from functionality
descriptions but struggle with algorithmic problems that require not only
implementation but also identification of the suitable algorithm. Moreover,
LLM-generated programs lack guaranteed correctness and require human
verification. To address these challenges, we propose ALGO, a framework that
synthesizes Algorithmic programs with LLM-Generated Oracles to guide the
generation and verify their correctness. ALGO first generates a reference
oracle by prompting an LLM to exhaustively enumerate all the combinations of
relevant variables. This oracle is then utilized to guide an arbitrary search
strategy in exploring the algorithm space and to verify the synthesized
algorithms. Our study shows that the LLM-generated oracles are correct for 88%
of the cases. With the oracles as verifiers, ALGO can be integrated with any
existing code generation model in a model-agnostic manner to enhance its
performance. Experiments show that when equipped with ALGO, we achieve an 8x
better one-submission pass rate over the Codex model and a 2.6x better
one-submission pass rate over CodeT, the current state-of-the-art model on
CodeContests. We can also get 1.3x better pass rate over the ChatGPT Code
Interpreter on unseen problems. The problem set we used for testing, the
prompts we used, the verifier and solution programs, and the test cases
generated by ALGO are available at https://github.com/zkx06111/ALGO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14702">
<div class="article-summary-box-inner">
<span><p>Human preference judgments are pivotal in guiding large language models
(LLMs) to produce outputs that align with human values. Human evaluations are
also used in summarization tasks to compare outputs from various systems,
complementing existing automatic metrics. Despite their significance, however,
there has been limited research probing these pairwise or $k$-wise comparisons.
The collective impact and relative importance of factors such as output length,
informativeness, fluency, and factual consistency are still not well
understood. It is also unclear if there are other hidden factors influencing
human judgments. In this paper, we conduct an in-depth examination of a
collection of pairwise human judgments released by OpenAI. Utilizing the
Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in
these human judgments. We find that the most favored factors vary across tasks
and genres, whereas the least favored factors tend to be consistent, e.g.,
outputs are too brief, contain excessive off-focus content or hallucinated
facts. Our findings have implications on the construction of balanced datasets
in human preference evaluations, which is a crucial step in shaping the
behaviors of future LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents. (arXiv:2305.14772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14772">
<div class="article-summary-box-inner">
<span><p>Many real-world applications (e.g., note taking, search) require extracting a
sentence or paragraph from a document and showing that snippet to a human
outside of the source document. Yet, users may find snippets difficult to
understand as they lack context from the original document. In this work, we
use language models to rewrite snippets from scientific documents to be read on
their own. First, we define the requirements and challenges for this
user-facing decontextualization task, such as clarifying where edits occur and
handling references to other documents. Second, we propose a framework that
decomposes the task into three stages: question generation, question answering,
and rewriting. Using this framework, we collect gold decontextualizations from
experienced scientific article readers. We then conduct a range of experiments
across state-of-the-art commercial and open-source language models to identify
how to best provide missing-but-relevant information to models for our task.
Finally, we develop QaDecontext, a simple prompting strategy inspired by our
framework that improves over end-to-end prompting. We conclude with analysis
that finds, while rewriting is easy, question generation and answering remain
challenging for today's models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14795">
<div class="article-summary-box-inner">
<span><p>The information stored in large language models (LLMs) falls out of date
quickly, and retraining from scratch is often not an option. This has recently
given rise to a range of techniques for injecting new facts through updating
model weights. Current evaluation paradigms are extremely limited, mainly
validating the recall of edited facts, but changing one fact should cause
rippling changes to the model's related beliefs. If we edit the UK Prime
Minister to now be Rishi Sunak, then we should get a different answer to Who is
married to the British Prime Minister? In this work, we present a benchmark,
MQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising
multi-hop questions that assess whether edited models correctly answer
questions where the answer should change as an entailed consequence of edited
facts. While we find that current knowledge-editing approaches can recall
edited facts accurately, they fail catastrophically on the constructed
multi-hop questions. We thus propose a simple memory-based approach, MeLLo,
which stores all edited facts externally while prompting the language model
iteratively to generate answers that are consistent with the edited facts.
While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up
to 175B) and outperforms previous model editors by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text encoders bottleneck compositionality in contrastive vision-language models. (arXiv:2305.14897v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14897">
<div class="article-summary-box-inner">
<span><p>Performant vision-language (VL) models like CLIP represent captions using a
single vector. How much information about language is lost in this bottleneck?
We first curate CompPrompts, a set of increasingly compositional image captions
that VL models should be able to capture (e.g., single object, to
object+property, to multiple interacting objects). Then, we train text-only
recovery probes that aim to reconstruct captions from single-vector text
representations produced by several VL models. This approach does not require
images, allowing us to test on a broader range of scenes compared to prior
work. We find that: 1) CLIP's text encoder falls short on more compositional
inputs, including object relationships, attribute-object association, counting,
and negations; 2) some text encoders work significantly better than others; and
3) text-only recovery performance predicts multi-modal matching performance on
ControlledImCaps: a new evaluation benchmark we collect and release consisting
of fine-grained compositional images and captions. Specifically, our results
suggest text-only recoverability is a necessary (but not sufficient) condition
for modeling compositional factors in contrastive VL models. We release our
datasets and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14928">
<div class="article-summary-box-inner">
<span><p>Misinformation poses a critical societal challenge, and current approaches
have yet to produce an effective solution. We propose focusing on
generalization, uncertainty, and how to leverage recent large language models,
in order to create more practical tools to evaluate information veracity in
contexts where perfect classification is impossible. We first demonstrate that
GPT-4 can outperform prior methods in multiple settings and languages. Next, we
explore generalization, revealing that GPT-4 and RoBERTa-large exhibit
differences in failure modes. Third, we propose techniques to handle
uncertainty that can detect impossible examples and strongly improve outcomes.
We also discuss results on other language models, temperature, prompting,
versioning, explainability, and web retrieval, each one providing practical
insights and directions for future research. Finally, we publish the LIAR-New
dataset with novel paired English and French misinformation data and
Possibility labels that indicate if there is sufficient context for veracity
evaluation. Overall, this research lays the groundwork for future tools that
can drive real-world progress to combat misinformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16380">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has shown impressive performance in multiple
research domains and has become the backbone of many neural network models.
However, there is limited understanding on how it works. In particular, with a
simple predictive loss, how the representation emerges from the gradient
\emph{training dynamics} remains a mystery. In this paper, for 1-layer
transformer with one self-attention layer plus one decoder layer, we analyze
its SGD training dynamics for the task of next token prediction in a
mathematically rigorous manner. We open the black box of the dynamic process of
how the self-attention layer combines input tokens, and reveal the nature of
underlying inductive bias. More specifically, with the assumption (a) no
positional encoding, (b) long input sequence, and (c) the decoder layer learns
faster than the self-attention layer, we prove that self-attention acts as a
\emph{discriminative scanning algorithm}: starting from uniform attention, it
gradually attends more to distinct key tokens for a specific next token to be
predicted, and pays less attention to common key tokens that occur across
different next tokens. Among distinct tokens, it progressively drops attention
weights, following the order of low to high co-occurrence between the key and
the query token in the training set. Interestingly, this procedure does not
lead to winner-takes-all, but decelerates due to a \emph{phase transition} that
is controllable by the learning rates of the two layers, leaving (almost) fixed
token combination. We verify this \textbf{\emph{scan and snap}} dynamics on
synthetic and real-world data (WikiText).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Evaluating Adversarial Robustness of Large Vision-Language Models. (arXiv:2305.16934v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16934">
<div class="article-summary-box-inner">
<span><p>Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented
performance in response generation, especially with visual inputs, enabling
more creative and adaptable interaction than large language models such as
ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since
adversaries may successfully evade the entire system by subtly manipulating the
most vulnerable modality (e.g., vision). To this end, we propose evaluating the
robustness of open-source large VLMs in the most realistic and high-risk
setting, where adversaries have only black-box system access and seek to
deceive the model into returning the targeted responses. In particular, we
first craft targeted adversarial examples against pretrained models such as
CLIP and BLIP, and then transfer these adversarial examples to other VLMs such
as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we
observe that black-box queries on these VLMs can further improve the
effectiveness of targeted evasion, resulting in a surprisingly high success
rate for generating targeted responses. Our findings provide a quantitative
understanding regarding the adversarial vulnerability of large VLMs and call
for a more thorough examination of their potential security flaws before
deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Socially Aligned Language Models on Simulated Social Interactions. (arXiv:2305.16960v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16960">
<div class="article-summary-box-inner">
<span><p>Social alignment in AI systems aims to ensure that these models behave
according to established societal values. However, unlike humans, who derive
consensus on value judgments through social interaction, current language
models (LMs) are trained to rigidly replicate their training corpus in
isolation, leading to subpar generalization in unfamiliar scenarios and
vulnerability to adversarial attacks. This work presents a novel training
paradigm that permits LMs to learn from simulated social interactions. In
comparison to existing methodologies, our approach is considerably more
scalable and efficient, demonstrating superior performance in alignment
benchmarks and human evaluations. This paradigm shift in the training of LMs
brings us a step closer to developing AI systems that can robustly and
accurately reflect societal norms and values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17588">
<div class="article-summary-box-inner">
<span><p>Pre-trained transformers are often fine-tuned to aid clinical decision-making
using limited clinical notes. Model interpretability is crucial, especially in
high-stakes domains like medicine, to establish trust and ensure safety, which
requires human engagement. We introduce SUFO, a systematic framework that
enhances interpretability of fine-tuned transformer feature spaces. SUFO
utilizes a range of analytic and visualization techniques, including Supervised
probing, Unsupervised similarity analysis, Feature dynamics, and Outlier
analysis to address key questions about model trust and interpretability. We
conduct a case study investigating the impact of pre-training data where we
focus on real-world pathology classification tasks, and validate our findings
on MedNLI. We evaluate five 110M-sized pre-trained transformer models,
categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical
BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal
that: (1) while PubMedBERT, the domain-specific model, contains valuable
information for fine-tuning, it can overfit to minority classes when class
imbalances exist. In contrast, mixed-domain models exhibit greater resistance
to overfitting, suggesting potential improvements in domain-specific model
robustness; (2) in-domain pre-training accelerates feature disambiguation
during fine-tuning; and (3) feature spaces undergo significant sparsification
during this process, enabling clinicians to identify common outlier modes among
fine-tuned models as demonstrated in this paper. These findings showcase the
utility of SUFO in enhancing trust and safety when using transformers in
medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned
language models for other applications in medicine and in more critical
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18390">
<div class="article-summary-box-inner">
<span><p>This work examines the presence of modularity in pre-trained Transformers, a
feature commonly found in human brains and thought to be vital for general
intelligence. In analogy to human brains, we consider two main characteristics
of modularity: (1) functional specialization of neurons: we evaluate whether
each neuron is mainly specialized in a certain function, and find that the
answer is yes. (2) function-based neuron grouping: we explore finding a
structure that groups neurons into modules by function, and each module works
for its corresponding function. Given the enormous amount of possible
structures, we focus on Mixture-of-Experts as a promising candidate, which
partitions neurons into experts and usually activates different experts for
different inputs. Experimental results show that there are functional experts,
where clustered are the neurons specialized in a certain function. Moreover,
perturbing the activations of functional experts significantly affects the
corresponding function. Finally, we study how modularity emerges during
pre-training, and find that the modular structure is stabilized at the early
stage, which is faster than neuron stabilization. It suggests that Transformers
first construct the modular structure and then learn fine-grained neuron
functions. Our code and data are available at
https://github.com/THUNLP/modularity-analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18395">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown promising performance in
knowledge-intensive reasoning tasks that require a compound understanding of
knowledge. However, deployment of the LLMs in real-world applications can be
challenging due to their high computational requirements and concerns on data
privacy. Previous studies have focused on building task-specific small Language
Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However,
these approaches are ill-suited for knowledge-intensive reasoning tasks due to
the limited capacity of small LMs in memorizing the knowledge required.
Motivated by our theoretical analysis on memorization, we propose
Knowledge-Augmented Reasoning Distillation (KARD), a novel method that
fine-tunes small LMs to generate rationales obtained from LLMs with augmented
knowledge retrieved from an external knowledge base. Moreover, we further
propose a neural reranker to obtain documents relevant to rationale generation.
We empirically show that KARD significantly improves the performance of small
T5 and GPT models on the challenging knowledge-intensive reasoning datasets,
namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the
250M T5 models achieve superior performance against the fine-tuned 3B models,
having 12 times larger parameters, on both MedQA-USMLE and StrategyQA
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images. (arXiv:2305.19164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19164">
<div class="article-summary-box-inner">
<span><p>We propose an automated algorithm to stress-test a trained visual model by
generating language-guided counterfactual test images (LANCE). Our method
leverages recent progress in large language modeling and text-based image
editing to augment an IID test set with a suite of diverse, realistic, and
challenging test images without altering model weights. We benchmark the
performance of a diverse set of pre-trained models on our generated data and
observe significant and consistent performance drops. We further analyze model
sensitivity across different types of edits, and demonstrate its applicability
at surfacing previously unknown class-level model biases in ImageNet. Code is
available at https://github.com/virajprabhu/lance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. (arXiv:2305.19308v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19308">
<div class="article-summary-box-inner">
<span><p>Computer end users have spent billions of hours completing daily tasks like
tabular data processing and project timeline scheduling. Most of these tasks
are repetitive and error-prone, yet most end users lack the skill to automate
these burdensome works. With the advent of large language models (LLMs),
directing software with natural language user requests become a reachable goal.
In this work, we propose a SheetCopilot agent that takes natural language task
and control spreadsheet to fulfill the requirements. We propose a set of atomic
actions as an abstraction of spreadsheet software functionalities. We further
design a state machine-based task planning framework for LLMs to robustly
interact with spreadsheets. We curate a representative dataset containing 221
spreadsheet control tasks and establish a fully automated evaluation pipeline
for rigorously benchmarking the ability of LLMs in software control tasks. Our
SheetCopilot correctly completes 44.3\% of tasks for a single generation,
outperforming the strong code generation baseline by a wide margin. Our project
page:https://sheetcopilot.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving CLIP Training with Language Rewrites. (arXiv:2305.20088v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.20088">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) stands as one of the most
effective and scalable methods for training transferable vision models using
paired image and text data. CLIP models are trained using contrastive loss,
which typically relies on data augmentations to prevent overfitting and
shortcuts. However, in the CLIP training paradigm, data augmentations are
exclusively applied to image inputs, while language inputs remain unchanged
throughout the entire training process, limiting the exposure of diverse texts
to the same image. In this paper, we introduce Language augmented CLIP
(LaCLIP), a simple yet highly effective approach to enhance CLIP training
through language rewrites. Leveraging the in-context learning capability of
large language models, we rewrite the text descriptions associated with each
image. These rewritten texts exhibit diversity in sentence structure and
vocabulary while preserving the original key concepts and meanings. During
training, LaCLIP randomly selects either the original texts or the rewritten
versions as text augmentations for each image. Extensive experiments on CC3M,
CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with
language rewrites significantly improves the transfer performance without
computation or memory overhead during training. Specifically for ImageNet
zero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on
LAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00219">
<div class="article-summary-box-inner">
<span><p>Text-to-image generative models have made remarkable advancements in
generating high-quality images. However, generated images often contain
undesirable artifacts or other errors due to model limitations. Existing
techniques to fine-tune generated images are time-consuming (manual editing),
produce poorly-integrated results (inpainting), or result in unexpected changes
across the entire image (variation selection and prompt fine-tuning). In this
work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to
efficiently fine-tune desired regions within an AI-synthesized image. Our
method introduces new random noise patterns at targeted regions during the
reverse diffusion process, enabling the model to efficiently make changes to
the specified regions while preserving the original context for the rest of the
image. We evaluate our method's usability and effectiveness through a user
study with artists, comparing our technique against other state-of-the-art
image inpainting techniques and editing software for fine-tuning AI-generated
imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposing Attention Glitches with Flip-Flop Language Modeling. (arXiv:2306.00946v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00946">
<div class="article-summary-box-inner">
<span><p>Why do large language models sometimes output factual inaccuracies and
exhibit erroneous reasoning? The brittleness of these models, particularly when
executing long chains of reasoning, currently seems to be an inevitable price
to pay for their advanced capabilities of coherently synthesizing knowledge,
pragmatics, and abstract thought. Towards making sense of this fundamentally
unsolved problem, this work identifies and analyzes the phenomenon of attention
glitches, in which the Transformer architecture's inductive biases
intermittently fail to capture robust reasoning. To isolate the issue, we
introduce flip-flop language modeling (FFLM), a parametric family of synthetic
benchmarks designed to probe the extrapolative behavior of neural language
models. This simple generative task requires a model to copy binary symbols
over long-range dependencies, ignoring the tokens in between. We find that
Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of
which we can eliminate using various regularization techniques. Our preliminary
mechanistic analyses show why the remaining errors may be very difficult to
diagnose and resolve. We hypothesize that attention glitches account for (some
of) the closed-domain hallucinations in natural LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. (arXiv:2306.01693v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01693">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) often exhibit undesirable text generation behaviors,
including generating false, toxic, or irrelevant outputs. Reinforcement
learning from human feedback (RLHF) - where human preference judgments on LM
outputs are transformed into a learning signal - has recently shown promise in
addressing these issues. However, such holistic feedback conveys limited
information on long text outputs; it does not indicate which aspects of the
outputs influenced user preference; e.g., which parts contain what type(s) of
errors. In this paper, we use fine-grained human feedback (e.g., which sentence
is false, which sub-sentence is irrelevant) as an explicit training signal. We
introduce Fine-Grained RLHF, a framework that enables training and learning
from reward functions that are fine-grained in two respects: (1) density,
providing a reward after every segment (e.g., a sentence) is generated; and (2)
incorporating multiple reward models associated with different feedback types
(e.g., factual incorrectness, irrelevance, and information incompleteness). We
conduct experiments on detoxification and long-form question answering to
illustrate how learning with such reward functions leads to improved
performance, supported by both automatic and human evaluation. Additionally, we
show that LM behaviors can be customized using different combinations of
fine-grained reward models. We release all data, collected human feedback, and
codes at https://FineGrainedRLHF.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis. (arXiv:2306.02213v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02213">
<div class="article-summary-box-inner">
<span><p>Emotion arcs capture how an individual (or a population) feels over time.
They are widely used in industry and research; however, there is little work on
evaluating the automatically generated arcs. This is because of the difficulty
of establishing the true (gold) emotion arc. Our work, for the first time,
systematically and quantitatively evaluates automatically generated emotion
arcs. We also compare two common ways of generating emotion arcs:
Machine-Learning (ML) models and Lexicon-Only (LexO) methods. By running
experiments on 18 diverse datasets in 9 languages, we show that despite being
markedly poor at instance level emotion classification, LexO methods are highly
accurate at generating emotion arcs when aggregating information from hundreds
of instances. We also show, through experiments on six indigenous African
languages, as well as Arabic, and Spanish, that automatic translations of
English emotion lexicons can be used to generate high-quality emotion arcs in
less-resource languages. This opens up avenues for work on emotions in
languages from around the world; which is crucial for commerce, public policy,
and health research in service of speakers often left behind. Code and
resources: https://github.com/dteodore/EmotionArcs
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03819">
<div class="article-summary-box-inner">
<span><p>Concept erasure aims to remove specified features from a representation. It
can improve fairness (e.g. preventing a classifier from using gender or race)
and interpretability (e.g. removing a concept to observe changes in model
behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form
method which provably prevents all linear classifiers from detecting a concept
while changing the representation as little as possible, as measured by a broad
class of norms. We apply LEACE to large language models with a novel procedure
called "concept scrubbing," which erases target concept information from every
layer in the network. We demonstrate our method on two tasks: measuring the
reliance of language models on part-of-speech information, and reducing gender
bias in BERT embeddings. Code is available at
https://github.com/EleutherAI/concept-erasure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05268">
<div class="article-summary-box-inner">
<span><p>In a wide range of multimodal tasks, contrastive learning has become a
particularly appealing approach since it can successfully learn representations
from abundant unlabeled data with only pairing information (e.g., image-caption
or video-audio pairs). Underpinning these approaches is the assumption of
multi-view redundancy - that shared information between modalities is necessary
and sufficient for downstream tasks. However, in many real-world settings,
task-relevant information is also contained in modality-unique regions:
information that is only present in one modality but still relevant to the
task. How can we learn self-supervised multimodal representations to capture
both shared and unique information relevant to downstream tasks? This paper
proposes FactorCL, a new multimodal representation learning method to go beyond
multi-view redundancy. FactorCL is built from three new contributions: (1)
factorizing task-relevant information into shared and unique representations,
(2) capturing task-relevant information via maximizing MI lower bounds and
removing task-irrelevant information via minimizing MI upper bounds, and (3)
multimodal data augmentations to approximate task relevance without labels. On
large-scale real-world datasets, FactorCL captures both shared and unique
information and achieves state-of-the-art results on six benchmarks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Semi-Parametric Reinforcement Learning Agents. (arXiv:2306.07929v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07929">
<div class="article-summary-box-inner">
<span><p>Inspired by the insights in cognitive science with respect to human memory
and reasoning mechanism, a novel evolvable LLM-based (Large Language Model)
agent framework is proposed as REMEMBERER. By equipping the LLM with a
long-term experience memory, REMEMBERER is capable of exploiting the
experiences from the past episodes even for different task goals, which excels
an LLM-based agent with fixed exemplars or equipped with a transient working
memory. We further introduce Reinforcement Learning with Experience Memory
(RLEM) to update the memory. Thus, the whole system can learn from the
experiences of both success and failure, and evolve its capability without
fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER
constitutes a semi-parametric RL agent. Extensive experiments are conducted on
two RL task sets to evaluate the proposed framework. The average results with
different initialization and training sets exceed the prior SOTA by 4% and 2%
for the success rate on two task sets and demonstrate the superiority and
robustness of REMEMBERER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-State Transformers. (arXiv:2306.09539v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09539">
<div class="article-summary-box-inner">
<span><p>State space models (SSMs) have shown impressive results on tasks that require
modeling long-range dependencies and efficiently scale to long sequences owing
to their subquadratic runtime complexity. Originally designed for continuous
signals, SSMs have shown superior performance on a plethora of tasks, in vision
and audio; however, SSMs still lag Transformer performance in Language Modeling
tasks. In this work, we propose a hybrid layer named Block-State Transformer
(BST), that internally combines an SSM sublayer for long-range
contextualization, and a Block Transformer sublayer for short-term
representation of sequences. We study three different, and completely
parallelizable, variants that integrate SSMs and block-wise attention. We show
that our model outperforms similar Transformer-based architectures on language
modeling perplexity and generalizes to longer sequences. In addition, the
Block-State Transformer demonstrates more than tenfold increase in speed at the
layer level compared to the Block-Recurrent Transformer when model
parallelization is employed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. (arXiv:2306.10512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.10512">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), like ChatGPT, have shown some human-like
cognitive abilities. For comparing these abilities of different models, several
benchmarks (i.e. sets of standard test questions) from different fields (e.g.,
Literature, Biology and Psychology) are often adopted and the test results
under traditional metrics such as accuracy, recall and F1, are reported.
However, such way for evaluating LLMs can be inefficient and inaccurate from
the cognitive science perspective. Inspired by Computerized Adaptive Testing
(CAT) used in psychometrics, we propose an adaptive testing framework for LLM
evaluation. Rather than using a standard test set and simply reporting
accuracy, this approach dynamically adjusts the characteristics of the test
questions, such as difficulty, based on the model's performance. This allows
for a more accurate estimation of the model's abilities, using fewer questions.
More importantly, it allows LLMs to be compared with humans easily, which is
essential for NLP models that aim for human-level ability. Our diagnostic
reports have found that ChatGPT often behaves like a ``careless student'',
prone to slip and occasionally guessing the questions. We conduct a
fine-grained diagnosis and rank the latest 6 instruction-tuned LLMs from three
aspects of Subject Knowledge, Mathematical Reasoning, and Programming, where
GPT4 can outperform other models significantly and reach the cognitive ability
of middle-level students. Different tests for different models using efficient
adaptive testing -- we believe this has the potential to become a new norm in
evaluating large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11207">
<div class="article-summary-box-inner">
<span><p>Recent accelerations in multi-modal applications have been made possible with
the plethora of image and text data available online. However, the scarcity of
analogous data in the medical field, specifically in histopathology, has slowed
comparable progress. To enable similar representation learning for
histopathology, we turn to YouTube, an untapped resource of videos, offering
$1,087$ hours of valuable educational histopathology videos from expert
clinicians. From YouTube, we curate QUILT: a large-scale vision-language
dataset consisting of $802, 144$ image and text pairs. QUILT was automatically
curated using a mixture of models, including large language models, handcrafted
algorithms, human knowledge databases, and automatic speech recognition. In
comparison, the most comprehensive datasets curated for histopathology amass
only around $200$K samples. We combine QUILT with datasets from other sources,
including Twitter, research papers, and the internet in general, to create an
even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it
as the largest vision-language histopathology dataset to date. We demonstrate
the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model
outperforms state-of-the-art models on both zero-shot and linear probing tasks
for classifying new histopathology images across $13$ diverse patch-level
datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning. (arXiv:2306.12552v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12552">
<div class="article-summary-box-inner">
<span><p>Recently, commonsense reasoning in text generation has attracted much
attention. Generative commonsense reasoning is the task that requires machines,
given a group of keywords, to compose a single coherent sentence with
commonsense plausibility. While existing datasets targeting generative
commonsense reasoning focus on everyday scenarios, it is unclear how well
machines reason under specific geographical and temporal contexts. We formalize
this challenging task as SituatedGen, where machines with commonsense should
generate a pair of contrastive sentences given a group of keywords including
geographical or temporal entities. We introduce a corresponding English dataset
consisting of 8,268 contrastive sentence pairs, which are built upon several
existing commonsense reasoning benchmarks with minimal manual labor.
Experiments show that state-of-the-art generative language models struggle to
generate sentences with commonsense plausibility and still lag far behind human
performance. Our dataset is publicly available at
https://github.com/yunx-z/situated_gen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation. (arXiv:2306.13460v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13460">
<div class="article-summary-box-inner">
<span><p>Image captioning aims to describe visual content in natural language. As 'a
picture is worth a thousand words', there could be various correct descriptions
for an image. However, with maximum likelihood estimation as the training
objective, the captioning model is penalized whenever its prediction mismatches
with the label. For instance, when the model predicts a word expressing richer
semantics than the label, it will be penalized and optimized to prefer more
concise expressions, referred to as conciseness optimization. In contrast,
predictions that are more concise than labels lead to richness optimization.
Such conflicting optimization directions could eventually result in the model
generating general descriptions. In this work, we introduce Semipermeable
MaxImum Likelihood Estimation (SMILE), which allows richness optimization while
blocking conciseness optimization, thus encouraging the model to generate
longer captions with more details. Extensive experiments on two mainstream
image captioning datasets MSCOCO and Flickr30K demonstrate that SMILE
significantly enhances the descriptiveness of generated captions. We further
provide in-depth investigations to facilitate a better understanding of how
SMILE works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14898">
<div class="article-summary-box-inner">
<span><p>Humans write code in a fundamentally interactive manner and rely on constant
execution feedback to correct errors, resolve ambiguities, and decompose tasks.
While LLMs have recently exhibited promising coding capabilities, current
coding benchmarks mostly consider a static instruction-to-code sequence
transduction process, which has the potential for error propagation and a
disconnect between the generated code and its final execution environment. To
address this gap, we introduce InterCode, a lightweight, flexible, and
easy-to-use framework of interactive coding as a standard reinforcement
learning (RL) environment, with code as actions and execution feedback as
observations. Our framework is language and platform agnostic, uses
self-contained Docker environments to provide safe and reproducible execution,
and is compatible out-of-the-box with traditional seq2seq coding methods, while
enabling the development of new methods for interactive code generation. We use
InterCode to create three interactive code environments with Bash, SQL, and
Python as action spaces, leveraging data from the static NL2Bash, Spider, and
MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating
multiple state-of-the-art LLMs configured with different prompting strategies
such as ReAct and Plan &amp; Solve. Our results showcase the benefits of
interactive code generation and demonstrate that InterCode can serve as a
challenging benchmark for advancing code understanding and generation
capabilities. InterCode is designed to be easily extensible and can even be
used to create new tasks such as Capture the Flag, a popular coding puzzle that
is inherently multi-step and involves multiple programming languages. Project
site with code and data: https://intercode-benchmark.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Exploitability of Instruction Tuning. (arXiv:2306.17194v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17194">
<div class="article-summary-box-inner">
<span><p>Instruction tuning is an effective technique to align large language models
(LLMs) with human intents. In this work, we investigate how an adversary can
exploit instruction tuning by injecting specific instruction-following examples
into the training data that intentionally changes the model's behavior. For
example, an adversary can achieve content injection by injecting training
examples that mention target content and eliciting such behavior from
downstream models. To achieve this goal, we propose \textit{AutoPoison}, an
automated data poisoning pipeline. It naturally and coherently incorporates
versatile attack goals into poisoned data with the help of an oracle LLM. We
showcase two example attacks: content injection and over-refusal attacks, each
aiming to induce a specific exploitable behavior. We quantify and benchmark the
strength and the stealthiness of our data poisoning scheme. Our results show
that AutoPoison allows an adversary to change a model's behavior by poisoning
only a small fraction of data while maintaining a high level of stealthiness in
the poisoned examples. We hope our work sheds light on how data quality affects
the behavior of instruction-tuned models and raises awareness of the importance
of data quality for responsible deployments of LLMs. Code is available at
\url{https://github.com/azshue/AutoPoison}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17842">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling
frozen LLMs to perform both understanding and generation tasks involving
non-linguistic modalities such as images or videos. SPAE converts between raw
pixels and interpretable lexical tokens (or words) extracted from the LLM's
vocabulary. The resulting tokens capture both the semantic meaning and the
fine-grained details needed for visual reconstruction, effectively translating
the visual content into a language comprehensible to the LLM, and empowering it
to perform a wide array of multimodal tasks. Our approach is validated through
in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set
of image understanding and generation tasks. Our method marks the first
successful attempt to enable a frozen LLM to generate image content while
surpassing state-of-the-art performance in image understanding tasks, under the
same setting, by over 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.04657">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the \textsc{BeaverTails} dataset, aimed at
fostering research on safety alignment in large language models (LLMs). This
dataset uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have gathered safety meta-labels for 30,207
question-answer (QA) pairs and 30,144 pairs of expert comparison data for both
the helpfulness and harmlessness metrics. In total, we have gathered safety
meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert
comparison data for both the helpfulness and harmlessness metrics. We further
showcase applications of BeaverTails in content moderation and reinforcement
learning with human feedback (RLHF), emphasizing its potential for practical
safety measures in LLMs. We believe this dataset provides vital resources for
the community, contributing towards the safe development and deployment of
LLMs. Our project page is available at the following URL:
https://sites.google.com/view/pku-beavertails.
</p>
<p>Warning: this paper contains example data that may be offensive or harmful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Mining: When Data Mining Meets Large Language Model Finetuning. (arXiv:2307.06290v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06290">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are initially pretrained for broad capabilities
and then finetuned with instruction-following datasets to improve their
performance in interacting with humans. Despite advances in finetuning, a
standardized guideline for selecting high-quality datasets to optimize this
process remains elusive. In this paper, we first propose InstructMining, an
innovative method designed for automatically selecting premium
instruction-following data for finetuning LLMs. Specifically, InstructMining
utilizes natural language indicators as a measure of data quality, applying
them to evaluate unseen datasets. During experimentation, we discover that
double descent phenomenon exists in large language model finetuning. Based on
this observation, we further leverage BlendSearch to help find the best subset
among the entire dataset (i.e., 2,532 out of 100,000). Experiment results show
that InstructMining-7B achieves state-of-the-art performance on two of the most
popular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06775">
<div class="article-summary-box-inner">
<span><p>Over the last decade, there has been a vast increase in eating disorder
diagnoses and eating disorder-attributed deaths, reaching their zenith during
the Covid-19 pandemic. This immense growth derived in part from the stressors
of the pandemic but also from increased exposure to social media, which is rife
with content that promotes eating disorders. This study aimed to create a
multimodal deep learning model that can determine if a given social media post
promotes eating disorders based on a combination of visual and textual data. A
labeled dataset of Tweets was collected from Twitter, upon which twelve deep
learning models were trained and tested. Based on model performance, the most
effective deep learning model was the multimodal fusion of the RoBERTa natural
language processing model and the MaxViT image classification model, attaining
accuracy and F1 scores of 95.9% and 0.959, respectively. The RoBERTa and MaxViT
fusion model, deployed to classify an unlabeled dataset of posts from the
social media sites Tumblr and Reddit, generated results akin to those of
previous research studies that did not employ artificial intelligence-based
techniques, indicating that deep learning models can develop insights congruent
to those of researchers. Additionally, the model was used to conduct a
timeseries analysis of yet unseen Tweets from eight Twitter hashtags,
uncovering that, since 2014, the relative abundance of content that promotes
eating disorders has decreased drastically within those communities. Despite
this reduction, by 2018, content that promotes eating disorders had either
stopped declining or increased in ampleness anew on these hashtags.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06985">
<div class="article-summary-box-inner">
<span><p>Aiming to populate generalizable engineering design knowledge, we propose a
method to extract facts of the form &lt;head entity, relationship, tail entity&gt;
from sentences found in patent documents. These facts could be combined within
and across patent documents to form knowledge graphs that serve as schemes for
representing as well as storing design knowledge. Existing methods in
engineering design literature often utilise a set of predefined relationships
to populate triples that are statistical approximations rather than facts. In
our method, we train a tagger to identify both entities and relationships from
a sentence. Given a pair of entities, we train another tagger to identify the
specific relationship tokens. For training these taggers, we manually construct
a dataset of 44,227 sentences and corresponding facts. We benchmark our method
against two typically recommended approaches. We apply our method by extracting
facts from sentences found in patents related to fan systems. We build a
knowledge base using these facts to demonstrate how domain ontologies could be
constructed and contextualised knowledge of subsystems could be visualised. We
then search the knowledge base for key issues prevailing in fan systems. We
organize the responses into knowledge graphs and hold a comparative discussion
against the opinions about the key issues from ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06828">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) has emerged as a crucial technology for
understanding and generating human language, playing an essential role in tasks
such as machine translation, sentiment analysis, and more pertinently, question
classification. As a subfield within NLP, question classification focuses on
determining the type of information being sought, a fundamental step for
downstream applications like question answering systems. This study presents an
innovative ensemble approach for question classification, combining the
strengths of Electra, GloVe, and LSTM models. Rigorously tested on the
well-regarded TREC dataset, the model demonstrates how the integration of these
disparate technologies can lead to superior results. Electra brings in its
transformer-based capabilities for complex language understanding, GloVe offers
global vector representations for capturing word-level semantics, and LSTM
contributes its sequence learning abilities to model long-term dependencies. By
fusing these elements strategically, our ensemble model delivers a robust and
efficient solution for the complex task of question classification. Through
rigorous comparisons with well-known models like BERT, RoBERTa, and DistilBERT,
the ensemble approach verifies its effectiveness by attaining an 80% accuracy
score on the test dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13259">
<div class="article-summary-box-inner">
<span><p>Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown
impressive reasoning ability in various downstream tasks. Even so, suffering
from hallucinations and the inability to access external knowledge, LLMs often
come with incorrect or unfaithful intermediate reasoning steps, especially in
the context of answering knowledge-intensive tasks such as KBQA. To alleviate
this issue, we propose a framework called Knowledge-Driven Chain-of-Thought
(KD-CoT) to verify and modify reasoning traces in CoT via interaction with
external knowledge, and thus overcome the hallucinations and error propagation.
Concretely, we formulate the CoT rationale process of LLMs into a structured
multi-round QA format. In each round, LLMs interact with a QA system that
retrieves external knowledge and produce faithful reasoning traces based on
retrieved precise answers. The structured CoT reasoning of LLMs is facilitated
by our developed KBQA CoT collection, which serves as in-context learning
demonstrations and can also be utilized as feedback augmentation to train a
robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion
datasets demonstrate the effectiveness of proposed KD-CoT in task-solving
reasoning generation, which outperforms the vanilla CoT ICL with an absolute
success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented
retriever outperforms the state-of-the-art baselines for retrieving knowledge,
achieving significant improvement in Hit and recall performance. Our code and
data are released on https://github.com/AdelWang/KD-CoT/tree/main.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12269">
<div class="article-summary-box-inner">
<span><p>We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research.
It consists of over 250 000 court cases from the UK. Most cases are from the
21st century, but the corpus includes cases as old as the 16th century. This
paper presents the first release of the corpus, containing the raw text and
meta-data. Together with the corpus, we provide annotations on case outcomes
for 638 cases, done by legal experts. Using our annotated data, we have trained
and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to
provide benchmarks. We include an extensive legal and ethical discussion to
address the potentially sensitive nature of this material. As a consequence,
the corpus will only be released for research purposes under certain
restrictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering. (arXiv:2309.17133v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.17133">
<div class="article-summary-box-inner">
<span><p>Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to
utilize knowledge from external knowledge bases to answer visually-grounded
questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong
framework to tackle KB-VQA, first retrieves related documents with Dense
Passage Retrieval (DPR) and then uses them to answer questions. This paper
proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which
significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major
limitations in RA-VQA's retriever: (1) the image representations obtained via
image-to-text transforms can be incomplete and inaccurate and (2) relevance
scores between queries and documents are computed with one-dimensional
embeddings, which can be insensitive to finer-grained relevance. FLMR overcomes
these limitations by obtaining image representations that complement those from
the image-to-text transforms using a vision model aligned with an existing
text-based retriever through a simple alignment network. FLMR also encodes
images and questions using multi-dimensional embeddings to capture
finer-grained relevance between queries and documents. FLMR significantly
improves the original RA-VQA retriever's PRRecall@5 by approximately 8\%.
Finally, we equipped RA-VQA with two state-of-the-art large
multi-modal/language models to achieve $\sim61\%$ VQA score in the OK-VQA
dataset.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-31 23:10:54.433918720 UTC">2023-10-31 23:10:54 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>