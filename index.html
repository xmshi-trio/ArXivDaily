<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-11T01:30:00Z">05-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">$2 * n$ is better than $n^2$: Decomposing Event Coreference Resolution into Two Tractable Problems. (arXiv:2305.05672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05672">
<div class="article-summary-box-inner">
<span><p>Event Coreference Resolution (ECR) is the task of linking mentions of the
same event either within or across documents. Most mention pairs are not
coreferent, yet many that are coreferent can be identified through simple
techniques such as lemma matching of the event triggers or the sentences in
which they appear. Existing methods for training coreference systems sample
from a largely skewed distribution, making it difficult for the algorithm to
learn coreference beyond surface matching. Additionally, these methods are
intractable because of the quadratic operations needed. To address these
challenges, we break the problem of ECR into two parts: a) a heuristic to
efficiently filter out a large number of non-coreferent pairs, and b) a
training approach on a balanced set of coreferent and non-coreferent mention
pairs. By following this approach, we show that we get comparable results to
the state of the art on two popular ECR datasets while significantly reducing
compute requirements. We also analyze the mention pairs that are "hard" to
accurately classify as coreferent or non-coreferent. Code at
https://github.com/ahmeshaf/lemma_ce_coref
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05711">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning ability on many NLP tasks. A common practice is to
recast the task into a text-to-text format such that generative LLMs of natural
language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is
nontrivial to perform information extraction (IE) tasks with NL-LLMs since the
output of the IE task is usually structured and therefore is hard to be
converted into plain text. In this paper, we propose to recast the structured
output in the form of code instead of natural language and utilize generative
LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,
named entity recognition and relation extraction. In contrast to NL-LLMs, we
show that Code-LLMs can be well-aligned with these IE tasks by designing
code-style prompts and formulating these IE tasks as code generation tasks.
Experiment results on seven benchmarks show that our method consistently
outperforms fine-tuning moderate-size pre-trained models specially designed for
IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further
conduct a series of in-depth analyses to demonstrate the merits of leveraging
Code-LLMs for IE tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilevel Sentence Embeddings for Personality Prediction. (arXiv:2305.05748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05748">
<div class="article-summary-box-inner">
<span><p>Representing text into a multidimensional space can be done with sentence
embedding models such as Sentence-BERT (SBERT). However, training these models
when the data has a complex multilevel structure requires individually trained
class-specific models, which increases time and computing costs. We propose a
two step approach which enables us to map sentences according to their
hierarchical memberships and polarity. At first we teach the upper level
sentence space through an AdaCos loss function and then finetune with a novel
loss function mainly based on the cosine similarity of intra-level pairs. We
apply this method to three different datasets: two weakly supervised Big Five
personality dataset obtained from English and Japanese Twitter data and the
benchmark MNLI dataset. We show that our single model approach performs better
than multiple class-specific classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution. (arXiv:2305.05754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05754">
<div class="article-summary-box-inner">
<span><p>In collaborative tasks, effective communication is crucial for achieving
joint goals. One such task is collaborative building where builders must
communicate with each other to construct desired structures in a simulated
environment such as Minecraft. We aim to develop an intelligent builder agent
to build structures based on user input through dialogue. However, in
collaborative building, builders may encounter situations that are difficult to
interpret based on the available information and instructions, leading to
ambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key
research questions, with the goal of filling this gap: when should the agent
ask for clarification, and what clarification questions should it ask? We move
towards this target with two sub-tasks, a classification task and a ranking
task. For the classification task, the goal is to determine whether the agent
should ask for clarification based on the current world state and dialogue
history. For the ranking task, the goal is to rank the relevant clarification
questions from a pool of candidates. In this report, we briefly introduce our
methods for the classification and ranking task. For the classification task,
our model achieves an F1 score of 0.757, which placed the 3rd on the
leaderboard. For the ranking task, our model achieves about 0.38 for Mean
Reciprocal Rank by extending the traditional ranking model. Lastly, we discuss
various neural approaches for the ranking task and future direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ranking & Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05759">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that standard training via empirical risk minimization
(ERM) can produce models that achieve high accuracy on average but low accuracy
on underrepresented groups due to the prevalence of spurious features. A
predominant approach to tackle this group robustness problem minimizes the
worst group error (akin to a minimax strategy) on the training data, hoping it
will generalize well on the testing data. However, this is often suboptimal,
especially when the out-of-distribution (OOD) test data contains previously
unseen groups. Inspired by ideas from the information retrieval and
learning-to-rank literature, this paper first proposes to use Discounted
Cumulative Gain (DCG) as a metric of model quality for facilitating better
hyperparameter tuning and model selection. Being a ranking-based metric, DCG
weights multiple poorly-performing groups (instead of considering just the
group with the worst performance). As a natural next step, we build on our
results to propose a ranking-based training method called Discounted Rank
Upweighting (DRU), which differentially reweights a ranked list of
poorly-performing groups in the training data to learn models that exhibit
strong OOD performance on the test data. Results on several synthetic and
real-world datasets highlight the superior generalization ability of our
group-ranking-based (akin to soft-minimax) approach in selecting and learning
models that are robust to group distributional shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-dependent communication under environmental constraints. (arXiv:2305.05821v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05821">
<div class="article-summary-box-inner">
<span><p>There is significant evidence that real-world communication cannot be reduced
to sending signals with context-independent meaning. In this work, based on a
variant of the classical Lewis (1969) signaling model, we explore the
conditions for the emergence of context-dependent communication in a situated
scenario. In particular, we demonstrate that pressure to minimise the
vocabulary size is sufficient for such emergence. At the same time, we study
the environmental conditions and cognitive capabilities that enable contextual
disambiguation of symbol meanings. We show that environmental constraints on
the receiver's referent choice can be unilaterally exploited by the sender,
without disambiguation capabilities on the receiver's end. Consistent with
common assumptions, the sender's awareness of the context appears to be
required for contextual communication. We suggest that context-dependent
communication is a situated multilayered phenomenon, crucially influenced by
environment properties such as distribution of contexts. The model developed in
this work is a demonstration of how signals may be ambiguous out of context,
but still allow for near-perfect communication accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Dense Retrieval Training with Web Anchors. (arXiv:2305.05834v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05834">
<div class="article-summary-box-inner">
<span><p>In this work, we present an unsupervised retrieval method with contrastive
learning on web anchors. The anchor text describes the content that is
referenced from the linked page. This shows similarities to search queries that
aim to retrieve pertinent information from relevant documents. Based on their
commonalities, we train an unsupervised dense retriever, Anchor-DR, with a
contrastive learning task that matches the anchor text and the linked document.
To filter out uninformative anchors (such as ``homepage'' or other functional
anchors), we present a novel filtering technique to only select anchors that
contain similar types of information as search queries. Experiments show that
Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval
by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is
especially significant for search and question answering tasks. Our analysis
further reveals that the pattern of anchor-document pairs is similar to that of
search query-document pairs. Code available at
https://github.com/Veronicium/AnchorDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V\=arta: A Large-Scale Headline-Generation Dataset for Indic Languages. (arXiv:2305.05858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05858">
<div class="article-summary-box-inner">
<span><p>We present V\=arta, a large-scale multilingual dataset for headline
generation in Indic languages. This dataset includes 41.8 million news articles
in 14 different Indic languages (and English), which come from a variety of
high-quality sources. To the best of our knowledge, this is the largest
collection of curated articles for Indic languages currently available. We use
the data collected in a series of experiments to answer important questions
related to Indic NLP and multilinguality research in general. We show that the
dataset is challenging even for state-of-the-art abstractive models and that
they perform only slightly better than extractive baselines. Owing to its size,
we also show that the dataset can be used to pretrain strong language models
that outperform competitive baselines in both NLU and NLG benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05862">
<div class="article-summary-box-inner">
<span><p>The most recent large language models such as ChatGPT and GPT-4 have garnered
significant attention, as they are capable of generating high-quality responses
to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic
text corpora, showcasing their impressive capabilities, a study focusing on
financial corpora has not been conducted. In this study, we aim to bridge this
gap by examining the potential of ChatGPT and GPT-4 as a solver for typical
financial text analytic problems in the zero-shot or few-shot setting.
Specifically, we assess their capabilities on four representative tasks over
five distinct financial textual datasets. The preliminary study shows that
ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition
(NER) and sentiment analysis, where domain-specific knowledge is required,
while they excel in numerical reasoning tasks. We report both the strengths and
limitations of the current versions of ChatGPT and GPT-4, comparing them to the
state-of-the-art finetuned models as well as pretrained domain-specific
generative models. Our experiments provide qualitative studies, through which
we hope to help understand the capability of the existing models and facilitate
further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Address Matching Based On Hierarchical Information. (arXiv:2305.05874v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05874">
<div class="article-summary-box-inner">
<span><p>There is evidence that address matching plays a crucial role in many areas
such as express delivery, online shopping and so on. Address has a hierarchical
structure, in contrast to unstructured texts, which can contribute valuable
information for address matching. Based on this idea, this paper proposes a
novel method to leverage the hierarchical information in deep learning method
that not only improves the ability of existing methods to handle irregular
address, but also can pay closer attention to the special part of address.
Experimental findings demonstrate that the proposed method improves the current
approach by 3.2% points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification. (arXiv:2305.05921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05921">
<div class="article-summary-box-inner">
<span><p>Commonsense fact verification, as a challenging branch of commonsense
question-answering (QA), aims to verify through facts whether a given
commonsense claim is correct or not. Answering commonsense questions
necessitates a combination of knowledge from various levels. However, existing
studies primarily rest on grasping either unstructured evidence or potential
reasoning paths from structured knowledge bases, yet failing to exploit the
benefits of heterogeneous knowledge simultaneously. In light of this, we
propose Decker, a commonsense fact verification model that is capable of
bridging heterogeneous knowledge by uncovering latent relationships between
structured and unstructured knowledge. Experimental results on two commonsense
fact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the
effectiveness of our Decker and further analysis verifies its capability to
seize more precious information through reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia. (arXiv:2305.05928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05928">
<div class="article-summary-box-inner">
<span><p>Wikipedia can be edited by anyone and thus contains various quality
sentences. Therefore, Wikipedia includes some poor-quality edits, which are
often marked up by other editors. While editors' reviews enhance the
credibility of Wikipedia, it is hard to check all edited text. Assisting in
this process is very important, but a large and comprehensive dataset for
studying it does not currently exist. Here, we propose WikiSQE, the first
large-scale dataset for sentence quality estimation in Wikipedia. Each sentence
is extracted from the entire revision history of Wikipedia, and the target
quality labels were carefully investigated and selected. WikiSQE has about 3.4
M sentences with 153 quality labels. In the experiment with automatic
classification using competitive machine learning models, sentences that had
problems with citation, syntax/semantics, or propositions were found to be more
difficult to detect. In addition, we conducted automated essay scoring
experiments to evaluate the generalizability of the dataset. We show that the
models trained on WikiSQE perform better than the vanilla model, indicating its
potential usefulness in other domains. WikiSQE is expected to be a valuable
resource for other tasks in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering. (arXiv:2305.05936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05936">
<div class="article-summary-box-inner">
<span><p>Commonsense question answering (QA) research requires machines to answer
questions based on commonsense knowledge. However, this research requires
expensive labor costs to annotate data as the basis of research, and models
that rely on fine-tuning paradigms only apply to specific tasks, rather than
learn a general commonsense reasoning ability. As a more robust method,
zero-shot commonsense question answering shows a good prospect. The current
zero-shot framework tries to convert triples in commonsense knowledge graphs
(KGs) into QA-form samples as the pre-trained data source to incorporate
commonsense knowledge into the model. However, this method ignores the
multi-hop relationship in the KG, which is also an important central problem in
commonsense reasoning. In this paper, we propose a novel multi-hop commonsense
knowledge injection framework. Specifically, it explores multi-hop reasoning
paradigm in KGs that conform to linguistic logic, and we further propose two
multi-hop QA generation methods based on KGs. Then, we utilize contrastive
learning to pre-train the model with the synthetic QA dataset to inject
multi-hop commonsense knowledge. Extensive experiments on five commonsense
question answering benchmarks demonstrate that our framework achieves
state-of-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05940">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) unfolds as large language models become capable of
inferring test labels conditioned on a few labeled samples without any gradient
update. ICL-enabled large language models provide a promising step forward
toward bypassing recurrent annotation costs in a low-resource setting. Yet,
only a handful of past studies have explored ICL in a cross-lingual setting, in
which the need for transferring label-knowledge from a high-resource language
to a low-resource one is immensely crucial. To bridge the gap, we provide the
first in-depth analysis of ICL for cross-lingual text classification. We find
that the prevalent mode of selecting random input-label pairs to construct the
prompt-context is severely limited in the case of cross-lingual ICL, primarily
due to the lack of alignment in the input as well as the output spaces. To
mitigate this, we propose a novel prompt construction strategy -- Cross-lingual
In-context Source-Target Alignment (X-InSTA). With an injected coherence in the
semantics of the input examples and a task-based alignment across the source
and target languages, X-InSTA is able to outperform random prompt selection by
a large margin across three different tasks using 44 different cross-lingual
pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer. (arXiv:2305.05945v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05945">
<div class="article-summary-box-inner">
<span><p>Adapting a large language model for multiple-attribute text style transfer
via fine-tuning can be challenging due to the significant amount of
computational resources and labeled data required for the specific task. In
this paper, we address this challenge by introducing AdapterTST, a framework
that freezes the pre-trained model's original parameters and enables the
development of a multiple-attribute text style transfer model. Using BART as
the backbone model, Adapter-TST utilizes different neural adapters to capture
different attribute information, like a plug-in connected to BART. Our method
allows control over multiple attributes, like sentiment, tense, voice, etc.,
and configures the adapters' architecture to generate multiple outputs
respected to attributes or compositional editing on the same sentence. We
evaluate the proposed model on both traditional sentiment transfer and
multiple-attribute transfer tasks. The experiment results demonstrate that
Adapter-TST outperforms all the state-of-the-art baselines with significantly
lesser computational resources. We have also empirically shown that each
adapter is able to capture specific stylistic attributes effectively and can be
configured to perform compositional editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Path Transformer is Better: A Case Study on Neural Machine Translation. (arXiv:2305.05948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05948">
<div class="article-summary-box-inner">
<span><p>For years the model performance in machine learning obeyed a power-law
relationship with the model size. For the consideration of parameter
efficiency, recent studies focus on increasing model depth rather than width to
achieve better performance. In this paper, we study how model width affects the
Transformer model through a parameter-efficient multi-path structure. To better
fuse features extracted from different paths, we add three additional
operations to each sublayer: a normalization at the end of each path, a cheap
operation to produce more features, and a learnable weighted mechanism to fuse
all features flexibly. Extensive experiments on 12 WMT machine translation
tasks show that, with the same number of parameters, the shallower multi-path
model can achieve similar or even better performance than the deeper model. It
reveals that we should pay more attention to the multi-path structure, and
there should be a balance between the model depth and width to train a better
large-scale Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Multimodal Misinformation Detection with Logic Reasoning. (arXiv:2305.05964v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05964">
<div class="article-summary-box-inner">
<span><p>Multimodal misinformation on online social platforms is becoming a critical
concern due to increasing credibility and easier dissemination brought by
multimedia content, compared to traditional text-only information. While
existing multimodal detection approaches have achieved high performance, the
lack of interpretability hinders these systems' reliability and practical
deployment. Inspired by NeuralSymbolic AI which combines the learning ability
of neural networks with the explainability of symbolic learning, we propose a
novel logic-based neural model for multimodal misinformation detection which
integrates interpretable logic clauses to express the reasoning process of the
target task. To make learning effective, we parameterize symbolic logical
elements using neural representations, which facilitate the automatic
generation and evaluation of meaningful logic clauses. Additionally, to make
our framework generalizable across diverse misinformation sources, we introduce
five meta-predicates that can be instantiated with different correlations.
Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the
feasibility and versatility of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Forgetting in Pre-Trained Representations Through Continual Learning. (arXiv:2305.05968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05968">
<div class="article-summary-box-inner">
<span><p>Representation forgetting refers to the drift of contextualized
representations during continual training. Intuitively, the representation
forgetting can influence the general knowledge stored in pre-trained language
models (LMs), but the concrete effect is still unclear. In this paper, we study
the effect of representation forgetting on the generality of pre-trained
language models, i.e. the potential capability for tackling future downstream
tasks. Specifically, we design three metrics, including overall generality
destruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge
forgetting (SemF), to measure the evolution of general knowledge in continual
learning. With extensive experiments, we find that the generality is destructed
in various pre-trained LMs, and syntactic and semantic knowledge is forgotten
through continual learning. Based on our experiments and analysis, we further
get two insights into alleviating general knowledge forgetting: 1) training on
general linguistic tasks at first can mitigate general knowledge forgetting; 2)
the hybrid continual learning method can mitigate the generality destruction
and maintain more general knowledge compared with those only considering
rehearsal or regularization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05973">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach for developing privacy-preserving large-scale
recommender systems using differentially private (DP) large language models
(LLMs) which overcomes certain challenges and limitations in DP training these
complex systems. Our method is particularly well suited for the emerging area
of LLM-based recommender systems, but can be readily employed for any
recommender systems that process representations of natural language inputs.
Our approach involves using DP training methods to fine-tune a publicly
pre-trained LLM on a query generation task. The resulting model can generate
private synthetic queries representative of the original queries which can be
freely shared for any downstream non-private recommendation training procedures
without incurring any additional privacy cost. We evaluate our method on its
ability to securely train effective deep retrieval models, and we observe
significant improvements in their retrieval quality without compromising
query-level privacy guarantees compared to methods where the retrieval models
are directly DP trained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05976">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been widely studied for their ability to
store and utilize positive knowledge. However, negative knowledge, such as
"lions don't live in the ocean", is also ubiquitous in the world but rarely
mentioned explicitly in the text. What do LLMs know about negative knowledge?
This work examines the ability of LLMs to negative commonsense knowledge. We
design a constrained keywords-to-sentence generation task (CG) and a Boolean
question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs
frequently fail to generate valid sentences grounded in negative commonsense
knowledge, yet they can correctly answer polar yes-or-no questions. We term
this phenomenon the belief conflict of LLMs. Our further analysis shows that
statistical shortcuts and negation reporting bias from language modeling
pre-training cause this conflict.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models. (arXiv:2305.05982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05982">
<div class="article-summary-box-inner">
<span><p>A medical provider's summary of a patient visit serves several critical
purposes, including clinical decision-making, facilitating hand-offs between
providers, and as a reference for the patient. An effective summary is required
to be coherent and accurately capture all the medically relevant information in
the dialogue, despite the complexity of patient-generated language. Even minor
inaccuracies in visit summaries (for example, summarizing "patient does not
have a fever" when a fever is present) can be detrimental to the outcome of
care for the patient.
</p>
<p>This paper tackles the problem of medical conversation summarization by
discretizing the task into several smaller dialogue-understanding tasks that
are sequentially built upon. First, we identify medical entities and their
affirmations within the conversation to serve as building blocks. We study
dynamically constructing few-shot prompts for tasks by conditioning on relevant
patient information and use GPT-3 as the backbone for our experiments. We also
develop GPT-derived summarization metrics to measure performance against
reference summaries quantitatively. Both our human evaluation study and metrics
for medical correctness show that summaries generated using this approach are
clinically accurate and outperform the baseline approach of summarizing the
dialog in a zero-shot, single-prompt setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05994">
<div class="article-summary-box-inner">
<span><p>Analogical reasoning is a fundamental cognitive ability of humans. However,
current language models (LMs) still struggle to achieve human-like performance
in analogical reasoning tasks due to a lack of resources for model training. In
this work, we address this gap by proposing ANALOGYKB, a million-scale analogy
knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB
identifies two types of analogies from the KGs: 1) analogies of the same
relations, which can be directly extracted from the KGs, and 2) analogies of
analogous relations, which are identified with a selection and filtering
pipeline enabled by large LMs (InstructGPT), followed by minor human efforts
for data quality control. Evaluations on a series of datasets of two analogical
reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB
successfully enables LMs to achieve much better results than previous
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling Perspectives?. (arXiv:2305.06074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06074">
<div class="article-summary-box-inner">
<span><p>There are two competing approaches for modelling annotator disagreement:
distributional soft-labelling approaches (which aim to capture the level of
disagreement) or modelling perspectives of individual annotators or groups
thereof. We adapt a multi-task architecture -- which has previously shown
success in modelling perspectives -- to evaluate its performance on the SEMEVAL
Task 11. We do so by combining both approaches, i.e. predicting individual
annotator perspectives as an interim step towards predicting annotator
disagreement. Despite its previous success, we found that a multi-task approach
performed poorly on datasets which contained distinct annotator opinions,
suggesting that this approach may not always be suitable when modelling
perspectives. Furthermore, our results explain that while strongly
perspectivist approaches might not achieve state-of-the-art performance
according to evaluation metrics used by distributional approaches, our approach
allows for a more nuanced understanding of individual perspectives present in
the data. We argue that perspectivist approaches are preferable because they
enable decision makers to amplify minority views, and that it is important to
re-evaluate metrics to reflect this goal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Glimpse in ChatGPT Capabilities and its impact for AI research. (arXiv:2305.06087v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06087">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently become a popular topic in the
field of Artificial Intelligence (AI) research, with companies such as Google,
Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their
development. These models are trained on massive amounts of data and can be
used for a wide range of tasks, including language translation, text
generation, and question answering. However, the computational resources
required to train and run these models are substantial, and the cost of
hardware and electricity can be prohibitive for research labs that do not have
the funding and resources of the GAFA. In this paper, we will examine the
impact of LLMs on AI research. The pace at which such models are generated as
well as the range of domains covered is an indication of the trend which not
only the public but also the scientific community is currently experiencing. We
give some examples on how to use such models in research by focusing on
GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range
of capabilities in a single system is a strong sign of approaching general
intelligence. Innovations integrating such models will also expand along the
maturation of such AI systems and exhibit unforeseeable applications that will
have important impacts on several aspects of our societies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information. (arXiv:2305.06099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06099">
<div class="article-summary-box-inner">
<span><p>The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained
named entities in low-context situations and noisy scenarios like the presence
of spelling mistakes and typos for multiple languages. The task poses
significant challenges due to the scarcity of contextual information, the high
granularity of the entities(up to 33 classes), and the interference of noisy
data. To address these issues, our team {\bf PAI} proposes a universal Named
Entity Recognition (NER) system that integrates external entity information to
improve performance. Specifically, our system retrieves entities with
properties from the knowledge base (i.e. Wikipedia) for a given text, then
concatenates entity information with the input sentence and feeds it into
Transformer-based models. Finally, our system wins 2 first places, 4 second
places, and 1 third place out of 13 tracks. The code is publicly available at
\url{https://github.com/diqiuzhuanzhuan/semeval-2023}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia. (arXiv:2305.06147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06147">
<div class="article-summary-box-inner">
<span><p>Debatepedia is a publicly available dataset consisting of arguments and
counter-arguments on controversial topics that has been widely used for the
single-document query-focused abstractive summarization task in recent years.
However, it has been recently found that this dataset is limited by noise and
even most queries in this dataset do not have any relevance to the respective
document. In this paper, we present a methodology for cleaning the Debatepedia
dataset by leveraging the generative power of large language models to make it
suitable for query-focused abstractive summarization. More specifically, we
harness the language generation capabilities of ChatGPT to regenerate its
queries. We evaluate the effectiveness of the proposed ChatGPT annotated
version of the Debatepedia dataset using several benchmark summarization models
and demonstrate that the newly annotated version of Debatepedia outperforms the
original dataset in terms of both query relevance as well as summary generation
quality. We will make this annotated and cleaned version of the dataset
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A semi-automatic method for document classification in the shipping industry. (arXiv:2305.06148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06148">
<div class="article-summary-box-inner">
<span><p>In the shipping industry, document classification plays a crucial role in
ensuring that the necessary documents are properly identified and processed for
customs clearance. OCR technology is being used to automate the process of
document classification, which involves identifying important documents such as
Commercial Invoices, Packing Lists, Export/Import Customs Declarations, Bills
of Lading, Sea Waybills, Certificates, Air or Rail Waybills, Arrival Notices,
Certificate of Origin, Importer Security Filings, and Letters of Credit. By
using OCR technology, the shipping industry can improve accuracy and efficiency
in document classification and streamline the customs clearance process. The
aim of this study is to build a robust document classification system based on
keyword frequencies. The research is carried out by analyzing Contract-Breach
law documents available with IN-D. The documents were collected by scraping the
Singapore Government Judiciary website. The database developed has 250
Contract-Breach documents. These documents are splitted to generate 200
training documents and 50 test documents. A semi-automatic approach is used to
select keyword vectors for document classification. The accuracy of the
reported model is 92.00 %.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06152">
<div class="article-summary-box-inner">
<span><p>Large-scale vision-language pre-training has shown promising advances on
various downstream tasks and achieved significant performance in multi-modal
understanding and generation tasks. However, existing methods often perform
poorly on image-text matching tasks that require a detailed semantics
understanding of the text. Although there have been some works on this problem,
they do not sufficiently exploit the structural knowledge present in sentences
to enhance multi-modal language representations, which leads to poor
performance. In this paper, we present an end-to-end framework Structure-CLIP,
which integrates latent detailed semantics from the text to enhance
fine-grained semantic representations. Specifically, (1) we use scene graphs in
order to pay more attention to the detailed semantic learning in the text and
fully explore structured knowledge between fine-grained semantics, and (2) we
utilize the knowledge-enhanced framework with the help of the scene graph to
make full use of representations of structured knowledge. To verify the
effectiveness of our proposed method, we pre-trained our models with the
aforementioned approach and conduct experiments on different downstream tasks.
Numerical results show that Structure-CLIP can often achieve state-of-the-art
performance on both VG-Attribution and VG-Relation datasets. Extensive
experiments show its components are effective and its predictions are
interpretable, which proves that our proposed method can enhance detailed
semantic representation well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alleviating Over-smoothing for Unsupervised Sentence Representation. (arXiv:2305.06154v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06154">
<div class="article-summary-box-inner">
<span><p>Currently, learning better unsupervised sentence representations is the
pursuit of many natural language processing communities. Lots of approaches
based on pre-trained language models (PLMs) and contrastive learning have
achieved promising results on this task. Experimentally, we observe that the
over-smoothing problem reduces the capacity of these powerful PLMs, leading to
sub-optimal sentence representations. In this paper, we present a Simple method
named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples
negatives from PLMs intermediate layers, improving the quality of the sentence
representation. Our proposed method is quite simple and can be easily extended
to various state-of-the-art models for performance boosting, which can be seen
as a plug-and-play contrastive framework for learning unsupervised sentence
representation. Extensive results prove that SSCL brings the superior
performance improvements of different strong baselines (e.g., BERT and SimCSE)
on Semantic Textual Similarity and Transfer datasets. Our codes are available
at https://github.com/nuochenpku/SSCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06155">
<div class="article-summary-box-inner">
<span><p>In this work, we provide a recipe for training machine translation models in
a limited resource setting by leveraging synthetic target data generated using
a large pre-trained model. We show that consistently across different
benchmarks in bilingual, multilingual, and speech translation setups, training
models on synthetic targets outperforms training on the actual ground-truth
data. This performance gap grows bigger with increasing limits on the amount of
available resources in the form of the size of the dataset and the number of
parameters in the model. We also provide preliminary analysis into whether this
boost in performance is linked to ease of optimization or more deterministic
nature of the predictions, and whether this paradigm leads to better
out-of-distribution performance across different testing domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06156">
<div class="article-summary-box-inner">
<span><p>We present The Vault, an open-source, large-scale code-text dataset designed
to enhance the training of code-focused large language models (LLMs). Existing
open-source datasets for training code-based LLMs often face challenges in
terms of size, quality (due to noisy signals), and format (only containing code
function and text explanation pairings). The Vault overcomes these limitations
by providing 40 million code-text pairs across 10 popular programming
languages, thorough cleaning for 10+ prevalent issues, and various levels of
code-text pairings, including class, function, and line levels. Researchers and
practitioners can utilize The Vault for training diverse code-focused LLMs or
incorporate the provided data cleaning methods and scripts to improve their
datasets. By employing The Vault as the training dataset for code-centric LLMs,
we anticipate significant advancements in code understanding and generation
tasks, fostering progress in both artificial intelligence research and software
development practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implications of Multi-Word Expressions on English to Bharti Braille Machine Translation. (arXiv:2305.06157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06157">
<div class="article-summary-box-inner">
<span><p>In this paper, we have shown the improvement of English to Bharti Braille
machine translation system. We have shown how we can improve a baseline NMT
model by adding some linguistic knowledge to it. This was done for five
language pairs where English sentences were translated into five Indian
languages and then subsequently to corresponding Bharti Braille. This has been
demonstrated by adding a sub-module for translating multi-word expressions. The
approach shows promising results as across language pairs, we could see
improvement in the quality of NMT outputs. The least improvement was observed
in English-Nepali language pair with 22.08% and the most improvement was
observed in the English-Hindi language pair with 23.30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Vision-Language Models and their Performance on the Hateful Memes Challenge. (arXiv:2305.06159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06159">
<div class="article-summary-box-inner">
<span><p>Moderation of social media content is currently a highly manual task, yet
there is too much content posted daily to do so effectively. With the advent of
a number of multimodal models, there is the potential to reduce the amount of
manual labor for this task. In this work, we aim to explore different models
and determine what is most effective for the Hateful Memes Challenge, a
challenge by Meta designed to further machine learning research in content
moderation. Specifically, we explore the differences between early fusion and
late fusion models in classifying multimodal memes containing text and images.
We first implement a baseline using unimodal models for text and images
separately using BERT and ResNet-152, respectively. The outputs from these
unimodal models were then concatenated together to create a late fusion model.
In terms of early fusion models, we implement ConcatBERT, VisualBERT, ViLT,
CLIP, and BridgeTower. It was found that late fusion performed significantly
worse than early fusion models, with the best performing model being CLIP which
achieved an AUROC of 70.06. The code for this work is available at
https://github.com/bzhao18/CS-7643-Project.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StarCoder: may the source be with you!. (arXiv:2305.06161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06161">
<div class="article-summary-box-inner">
<span><p>The BigCode community, an open-scientific collaboration working on the
responsible development of Large Language Models for Code (Code LLMs),
introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context
length, infilling capabilities and fast large-batch inference enabled by
multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced
from The Stack, a large collection of permissively licensed GitHub repositories
with inspection tools and an opt-out process. We fine-tuned StarCoderBase on
35B Python tokens, resulting in the creation of StarCoder. We perform the most
comprehensive evaluation of Code LLMs to date and show that StarCoderBase
outperforms every open Code LLM that supports multiple programming languages
and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,
StarCoder outperforms every model that is fine-tuned on Python, can be prompted
to achieve 40\% pass@1 on HumanEval, and still retains its performance on other
programming languages. We take several important steps towards a safe
open-access model release, including an improved PII redaction pipeline and a
novel attribution tracing tool, and make the StarCoder models publicly
available under a more commercially viable version of the Open Responsible AI
Model license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06162">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis is an important area for understanding the
user's internal states. Deep learning methods were effective, but the problem
of poor interpretability has gradually gained attention. Previous works have
attempted to use attention weights or vector distributions to provide
interpretability. However, their explanations were not intuitive and can be
influenced by different trained models. This study proposed a novel approach to
provide interpretability by converting nonverbal modalities into text
descriptions and by using large-scale language models for sentiment
predictions. This provides an intuitive approach to directly interpret what
models depend on with respect to making decisions from input texts, thus
significantly improving interpretability. Specifically, we convert descriptions
based on two feature patterns for the audio modality and discrete action units
for the facial modality. Experimental results on two sentiment analysis tasks
demonstrated that the proposed approach maintained, or even improved
effectiveness for sentiment analysis compared to baselines using conventional
features, with the highest improvement of 2.49% on the F1 score. The results
also showed that multimodal descriptions have similar characteristics on fusing
modalities as those of conventional fusion methods. The results demonstrated
that the proposed approach is interpretable and effective for multimodal
sentiment analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algebra Error Classification with Large Language Models. (arXiv:2305.06163v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06163">
<div class="article-summary-box-inner">
<span><p>Automated feedback as students answer open-ended math questions has
significant potential in improving learning outcomes at large scale. A key part
of automated feedback systems is an error classification component, which
identifies student errors and enables appropriate, predefined feedback to be
deployed. Most existing approaches to error classification use a rule-based
method, which has limited capacity to generalize. Existing data-driven methods
avoid these limitations but specifically require mathematical expressions in
student responses to be parsed into syntax trees. This requirement is itself a
limitation, since student responses are not always syntactically valid and
cannot be converted into trees. In this work, we introduce a flexible method
for error classification using pre-trained large language models. We
demonstrate that our method can outperform existing methods in algebra error
classification, and is able to classify a larger set of student responses.
Additionally, we analyze common classification errors made by our method and
discuss limitations of automated error classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Semantic Parsing using Dynamic Context Graphs. (arXiv:2305.06164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06164">
<div class="article-summary-box-inner">
<span><p>In this paper we consider the task of conversational semantic parsing over
general purpose knowledge graphs (KGs) with millions of entities, and thousands
of relation-types. We are interested in developing models capable of
interactively mapping user utterances into executable logical forms (e.g.,
SPARQL) in the context of the conversational history. Our key idea is to
represent information about an utterance and its context via a subgraph which
is created dynamically, i.e., the number of nodes varies per utterance.
Moreover, rather than treating the subgraph as a sequence we exploit its
underlying structure, and thus encode it using a graph neural network which
further allows us to represent a large number of (unseen) nodes. Experimental
results show that modeling context dynamically is superior to static
approaches, delivering performance improvements across the board (i.e., for
simple and complex questions). Our results further confirm that modeling the
structure of context is better at processing discourse information, (i.e., at
handling ellipsis and resolving coreference) and longer interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT as a Text Simplification Tool to Remove Bias. (arXiv:2305.06166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06166">
<div class="article-summary-box-inner">
<span><p>The presence of specific linguistic signals particular to a certain sub-group
of people can be picked up by language models during training. This may lead to
discrimination if the model has learnt to pick up on a certain group's
language. If the model begins to associate specific language with a distinct
group, any decisions made based upon this language would hold a strong
correlation to a decision based on their protected characteristic. We explore a
possible technique for bias mitigation in the form of simplification of text.
The driving force of this idea is that simplifying text should standardise
language to one way of speaking while keeping the same meaning. The experiment
shows promising results as the classifier accuracy for predicting the sensitive
attribute drops by up to 17% for the simplified data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QICHWABASE: A Quechua Language and Knowledge Base for Quechua Communities. (arXiv:2305.06173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06173">
<div class="article-summary-box-inner">
<span><p>Over the last decade, the Web has increasingly become a space of language and
knowledge representation. However, it is only true for well-spread languages
and well-established communities, while minority communities and their
resources received less attention. In this paper, we propose QICHWABASE to
support the harmonization process of the Quechua language and knowledge, and
its community. For doing it, we adopt methods and tools that could become a
game changer in favour of Quechua communities around the world. We conclude
that the methodology and tools adopted on building QICHWABASE, which is a
Wikibase instance, could enhance the presence of minorities on the Web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06174">
<div class="article-summary-box-inner">
<span><p>Climate change is the defining issue of our time, and we are at a defining
moment. Various interest groups, social movement organizations, and individuals
engage in collective action on this issue on social media. In addition, issue
advocacy campaigns on social media often arise in response to ongoing societal
concerns, especially those faced by energy industries. Our goal in this paper
is to analyze how those industries, their advocacy group, and climate advocacy
group use social media to influence the narrative on climate change. In this
work, we propose a minimally supervised model soup [56] approach combined with
messaging themes to identify the stances of climate ads on Facebook. Finally,
we release our stance dataset, model, and set of themes related to climate
campaigns for future work on opinion mining and the automatic detection of
climate change stances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06176">
<div class="article-summary-box-inner">
<span><p>Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to
significantly enhance the performance of large language models (LLMs) by
aligning their outputs with desired human values. However, RLHF is constrained
by the expertise and productivity limitations of human evaluators. In this
study, we investigate an alternative approach: Reinforcement Learning with
Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings
indicate that RLGAF can help align LLMs outputs while not suffering from the
inherent restrictions of RLHF, suggesting promising avenues for further
research on automating AI alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Prompt Tuning for Large Language Model Services. (arXiv:2305.06212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06212">
<div class="article-summary-box-inner">
<span><p>Prompt tuning provides an efficient way for users to customize Large Language
Models (LLMs) with their private data in the emerging LLM service scenario.
However, the sensitive nature of private data brings the need for privacy
preservation in LLM service customization. Based on prompt tuning, we propose
Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy
guarantees for LLM services. \textsc{rapt} adopts a local privacy setting,
allowing users to privatize their data locally with local differential privacy.
As prompt tuning performs poorly when directly trained on privatized data, we
introduce a novel privatized token reconstruction task that is trained jointly
with the downstream task, allowing LLMs to learn better task-dependent
representations. Despite the simplicity of our framework, experiments show that
RAPT achieves competitive performance across tasks while providing privacy
guarantees against adversaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task End-to-End Training Improves Conversational Recommendation. (arXiv:2305.06218v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06218">
<div class="article-summary-box-inner">
<span><p>In this paper, we analyze the performance of a multitask end-to-end
transformer model on the task of conversational recommendations, which aim to
provide recommendations based on a user's explicit preferences expressed in
dialogue. While previous works in this area adopt complex multi-component
approaches where the dialogue management and entity recommendation tasks are
handled by separate components, we show that a unified transformer model, based
on the T5 text-to-text transformer model, can perform competitively in both
recommending relevant items and generating conversation dialogue. We fine-tune
our model on the ReDIAL conversational movie recommendation dataset, and create
additional training tasks derived from MovieLens (such as the prediction of
movie attributes and related movies based on an input movie), in a multitask
learning setting. Using a series of probe studies, we demonstrate that the
learned knowledge in the additional tasks is transferred to the conversational
setting, where each task leads to a 9%-52% increase in its related probe score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ComputeGPT: A computational chat model for numerical problems. (arXiv:2305.06223v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06223">
<div class="article-summary-box-inner">
<span><p>Language models are not accurate in numerical problems. Their architecture
does not allow for anything less than a probabilistic next word. This paper
introduces ComputeGPT: an approach of creating a chat model able to answer
computational problems through running on-demand code. ComputeGPT converts each
question to relevant code, runs the code, and returns the computed answer as
part of the chat. We combine this approach with a local browser-based Python
interpretation and fine-tuned prompts in order to achieve state-of-the-art
efficiency on numerical problems and provide a suitable front-end and safe
environment for the code to be executed in.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Robust Self-attention Features for Speech Emotion Recognition with Label-adaptive Mixup. (arXiv:2305.06273v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06273">
<div class="article-summary-box-inner">
<span><p>Speech Emotion Recognition (SER) is to recognize human emotions in a natural
verbal interaction scenario with machines, which is considered as a challenging
problem due to the ambiguous human emotions. Despite the recent progress in
SER, state-of-the-art models struggle to achieve a satisfactory performance. We
propose a self-attention based method with combined use of label-adaptive mixup
and center loss. By adapting label probabilities in mixup and fitting center
loss to the mixup training scheme, our proposed method achieves a superior
performance to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Document Simplification. (arXiv:2305.06274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06274">
<div class="article-summary-box-inner">
<span><p>To date, most work on text simplification has focused on sentence-level
inputs. Early attempts at document simplification merely applied these
approaches iteratively over the sentences of a document. However, this fails to
coherently preserve the discourse structure, leading to suboptimal output
quality. Recently, strategies from controllable simplification have been
leveraged to achieve state-of-the-art results on document simplification by
first generating a document-level plan (a sequence of sentence-level
simplification operations) and using this plan to guide sentence-level
simplification downstream. However, this is still limited in that the
simplification model has no direct access to the local inter-sentence document
context, likely having a negative impact on surface realisation. We explore
various systems that use document context within the simplification process
itself, either by iterating over larger text units or by extending the system
architecture to attend over a high-level representation of document context. In
doing so, we achieve state-of-the-art performance on the document
simplification task, even when not relying on plan-guidance. Further, we
investigate the performance and efficiency tradeoffs of system variants and
make suggestions of when each should be preferred.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation. (arXiv:2305.06294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06294">
<div class="article-summary-box-inner">
<span><p>Commonsense knowledge is crucial to many natural language processing tasks.
Existing works usually incorporate graph knowledge with conventional graph
neural networks (GNNs), leading to the text and graph knowledge encoding
processes being separated in a serial pipeline. We argue that these separate
representation learning stages may be suboptimal for neural networks to learn
the overall context contained in both types of input knowledge. In this paper,
we propose a novel context-aware graph-attention model (Context-aware GAT),
which can effectively incorporate global features of relevant knowledge graphs
based on a context-enhanced knowledge aggregation process. Specifically, our
framework leverages a novel representation learning approach to process
heterogeneous features - combining flattened graph knowledge with text. To the
best of our knowledge, this is the first attempt at hierarchically applying
graph knowledge aggregation on a connected subgraph in addition to contextual
information to support commonsense dialogue generation. This framework shows
superior performance compared to conventional GNN-based language frameworks.
Both automatic and human evaluation demonstrates that our proposed model has
significant performance uplifts over state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). (arXiv:2305.06299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06299">
<div class="article-summary-box-inner">
<span><p>Large language models, particularly GPT-3, are able to produce high quality
summaries of general domain news articles in few- and zero-shot settings.
However, it is unclear if such models are similarly capable in more
specialized, high-stakes domains such as biomedicine. In this paper, we enlist
domain experts (individuals with medical training) to evaluate summaries of
biomedical articles generated by GPT-3, given zero supervision. We consider
both single- and multi-document settings. In the former, GPT-3 is tasked with
generating regular and plain-language summaries of articles describing
randomized controlled trials; in the latter, we assess the degree to which
GPT-3 is able to \emph{synthesize} evidence reported across a collection of
articles. We design an annotation scheme for evaluating model outputs, with an
emphasis on assessing the factual accuracy of generated summaries. We find that
while GPT-3 is able to summarize and simplify single biomedical articles
faithfully, it struggles to provide accurate aggregations of findings over
multiple documents. We release all data and annotations used in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Embedding APIs for Information Retrieval. (arXiv:2305.06300v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06300">
<div class="article-summary-box-inner">
<span><p>The ever-increasing size of language models curtails their widespread access
to the community, thereby galvanizing many companies and startups into offering
access to large language models through APIs. One particular API, suitable for
dense retrieval, is the semantic embedding API that builds vector
representations of a given text. With a growing number of APIs at our disposal,
in this paper, our goal is to analyze semantic embedding APIs in realistic
retrieval scenarios in order to assist practitioners and researchers in finding
suitable services according to their needs. Specifically, we wish to
investigate the capabilities of existing APIs on domain generalization and
multilingual retrieval. For this purpose, we evaluate the embedding APIs on two
standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results
using the APIs is a budget-friendly approach and is most effective on English,
in contrast to the standard practice, i.e., employing them as first-stage
retrievers. For non-English retrieval, re-ranking still improves the results,
but a hybrid model with BM25 works best albeit at a higher cost. We hope our
work lays the groundwork for thoroughly evaluating APIs that are critical in
search and more broadly, in information retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06311">
<div class="article-summary-box-inner">
<span><p>A recent focus of large language model (LLM) development, as exemplified by
generative search engines, is to incorporate external references to generate
and support their claims. However, evaluating the attribution, i.e., verifying
whether the generated statement is indeed fully supported by the cited
reference, remains an open problem. Although human evaluation is common
practice, it is costly and time-consuming. In this paper, we investigate the
automatic evaluation of attribution by LLMs. We begin by providing a definition
of attribution and then explore two approaches for automatic evaluation:
prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed
from related tasks, such as question answering, fact-checking, natural language
inference, and summarization. To facilitate the evaluation, we manually curate
a set of test examples covering 12 domains from a generative search engine, New
Bing. Our results on the curated test set and simulated test examples from
existing benchmark questions highlight both promising signals as well as
remaining challenges for the automatic evaluation of attribution. We hope our
testbed, modeling methodology, and insights will help lay the foundation for
future studies on this important problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Korean Named Entity Recognition Based on Language-Specific Features. (arXiv:2305.06330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06330">
<div class="article-summary-box-inner">
<span><p>In the paper, we propose a novel way of improving named entity recognition in
the Korean language using its language-specific features. While the field of
named entity recognition has been studied extensively in recent years, the
mechanism of efficiently recognizing named entities in Korean has hardly been
explored. This is because the Korean language has distinct linguistic
properties that prevent models from achieving their best performances.
Therefore, an annotation scheme for {Korean corpora} by adopting the CoNLL-U
format, which decomposes Korean words into morphemes and reduces the ambiguity
of named entities in the original segmentation that may contain functional
morphemes such as postpositions and particles, is proposed herein. We
investigate how the named entity tags are best represented in this
morpheme-based scheme and implement an algorithm to convert word-based {and
syllable-based Korean corpora} with named entities into the proposed
morpheme-based format. Analyses of the results of {statistical and neural}
models reveal that the proposed morpheme-based format is feasible, and the
{varied} performances of the models under the influence of various additional
language-specific features are demonstrated. Extrinsic conditions were also
considered to observe the variance of the performances of the proposed models,
given different types of data, including the original segmentation and
different types of tagging formats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06335">
<div class="article-summary-box-inner">
<span><p>We present in this work a new Universal Morphology dataset for Korean.
Previously, the Korean language has been underrepresented in the field of
morphological paradigms amongst hundreds of diverse world languages. Hence, we
propose this Universal Morphological paradigms for the Korean language that
preserve its distinct characteristics. For our K-UniMorph dataset, we outline
each grammatical criterion in detail for the verbal endings, clarify how to
extract inflected forms, and demonstrate how we generate the morphological
schemata. This dataset adopts morphological feature schema from Sylak-Glassman
et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract
inflected verb forms from the Sejong morphologically analyzed corpus that is
one of the largest annotated corpora for Korean. During the data creation, our
methodology also includes investigating the correctness of the conversion from
the Sejong corpus. Furthermore, we carry out the inflection task using three
different Korean word forms: letters, syllables and morphemes. Finally, we
discuss and describe future perspectives on Korean morphological paradigms and
the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RECKONING: Reasoning through Dynamic Knowledge Encoding. (arXiv:2305.06349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06349">
<div class="article-summary-box-inner">
<span><p>Recent studies on transformer-based language models show that they can answer
questions by reasoning over knowledge provided as part of the context (i.e.,
in-context reasoning). However, since the available knowledge is often not
filtered for a particular question, in-context reasoning can be sensitive to
distractor facts, additional content that is irrelevant to a question but that
may be relevant for a different question (i.e., not necessarily random noise).
In these situations, the model fails to distinguish the knowledge that is
necessary to answer the question, leading to spurious reasoning and degraded
performance. This reasoning failure contrasts with the model's apparent ability
to distinguish its contextual knowledge from all the knowledge it has memorized
during pre-training. Following this observation, we propose teaching the model
to reason more robustly by folding the provided contextual knowledge into the
model's parameters before presenting it with a question. Our method, RECKONING,
is a bi-level learning algorithm that teaches language models to reason by
updating their parametric knowledge through back-propagation, allowing them to
then answer questions using the updated parameters. During training, the inner
loop rapidly adapts a copy of the model weights to encode contextual knowledge
into its parameters. In the outer loop, the model learns to uses the updated
weights to reproduce and answer reasoning questions about the memorized
knowledge. Our experiments on two multi-hop reasoning datasets show that
RECKONING's performance improves over the in-context reasoning baseline (by up
to 4.5%). We also find that compared to in-context reasoning, RECKONING
generalizes better to longer reasoning chains unseen during training, is more
robust to distractors in the context, and is more computationally efficient
when multiple questions are asked about the same knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoChat: Chat-Centric Video Understanding. (arXiv:2305.06355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06355">
<div class="article-summary-box-inner">
<span><p>In this study, we initiate an exploration into video understanding by
introducing VideoChat, an end-to-end chat-centric video understanding system.
It integrates video foundation models and large language models via a learnable
neural interface, excelling in spatiotemporal reasoning, event localization,
and causal relationship inference. To instructively tune this system, we
propose a video-centric instruction dataset, composed of thousands of videos
matched with detailed descriptions and conversations. This dataset emphasizes
spatiotemporal reasoning and causal relationships, providing a valuable asset
for training chat-centric video understanding systems. Preliminary qualitative
experiments reveal our system's potential across a broad spectrum of video
applications and set the standard for future research. Access our code and data
at https://github.com/OpenGVLab/Ask-Anything
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marshall-Olkin Power-Law Distributions in Length-Frequency of Entities. (arXiv:1811.03325v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.03325">
<div class="article-summary-box-inner">
<span><p>Entities involve important concepts with concrete meanings and play important
roles in numerous linguistic tasks. Entities have different forms in different
tasks and researchers treat those forms as different concepts. In this paper,
we are curious to know whether there are some common characteristics connecting
those different forms of entities. Specifically, we investigate the underlying
distributions of entities from different types and different languages, trying
to figure out some common properties behind those diverse entities. We find
from twelve datasets about different types of entities and eighteen datasets
about different languages of entities that although these entities are
dramatically diverse from each in many aspects, their length-frequencies can be
well characterized by Marshall-Olkin power-law (MOPL) distributions, and these
distributions possess defined means and finite variances. Our experiments show
that while not all the entities are drawn from the same underlying population,
those entities under same types tend to be drawn from the same distribution.
Our experiments also show that Marshall-Olkin power-law models characterize the
length-frequencies of entities much better than pure power-law models and
log-normal models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00542">
<div class="article-summary-box-inner">
<span><p>The large attention-based encoder-decoder network (Transformer) has become
prevailing recently due to its effectiveness. But the high computation
complexity of its decoder raises the inefficiency issue. By examining the
mathematic formulation of the decoder, we show that under some mild conditions,
the architecture could be simplified by compressing its sub-layers, the basic
building block of Transformer, and achieves a higher parallelism. We thereby
propose Compressed Attention Network, whose decoder layer consists of only one
sub-layer instead of three. Extensive experiments on 14 WMT machine translation
tasks show that our model is 1.42x faster with performance on par with a strong
baseline. This strong baseline is already 2x faster than the widely used
standard baseline without loss in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAP-Gen: Guided Automatic Python Code Generation. (arXiv:2201.08810v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08810">
<div class="article-summary-box-inner">
<span><p>Automatic code generation from natural language descriptions can be highly
beneficial during the process of software development. In this work, we propose
GAP-Gen, a Guided Automatic Python Code Generation method based on Python
syntactic constraints and semantic constraints. We first introduce Python
syntactic constraints in the form of Syntax-Flow, which is a simplified version
of Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract
Syntax Tree but maintaining crucial syntactic information of Python code. In
addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable
and function names consistently through out the code. In our work, rather than
pretraining, we focus on modifying the finetuning process which reduces
computational requirements but retains high generation performance on automatic
Python code generation task. GAP-Gen fine-tunes the transformer based language
models T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,
CodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our
experiments show that GAP-Gen achieves better results on automatic Python code
generation task than previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation. (arXiv:2202.13047v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13047">
<div class="article-summary-box-inner">
<span><p>Crowdsourced dialogue corpora are usually limited in scale and topic coverage
due to the expensive cost of data curation. This would hinder the
generalization of downstream dialogue models to open-domain topics. In this
work, we leverage large language models for dialogue augmentation in the task
of emotional support conversation (ESC). By treating dialogue augmentation as a
dialogue completion task, we prompt a fine-tuned language model to complete
full dialogues from available dialogue posts of various topics, which are then
postprocessed based on heuristics. Applying this approach, we construct AugESC,
an augmented dataset for the ESC task, which largely extends the scale and
topic coverage of the crowdsourced ESConv corpus. Through comprehensive human
evaluation, we demonstrate that our approach is superior to strong baselines of
dialogue augmentation and that AugESC has comparable dialogue quality to the
crowdsourced corpus. We also conduct human interactive evaluation and prove
that post-training on AugESC improves downstream dialogue models'
generalization ability to open-domain topics. These results suggest the utility
of AugESC and highlight the potential of large language models in improving
data-scarce dialogue generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Projection Invariance Mitigates Representation Collapse. (arXiv:2205.11603v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11603">
<div class="article-summary-box-inner">
<span><p>Fine-tuning contextualized representations learned by pre-trained language
models remains a prevalent practice in NLP. However, fine-tuning can lead to
representation degradation (also known as representation collapse), which may
result in instability, sub-optimal performance, and weak generalization.
</p>
<p>In this paper, we propose Representation Projection Invariance (REPINA), a
novel regularization method to maintain the information content of
representation and reduce representation collapse during fine-tuning by
discouraging undesirable changes in the representations. We study the empirical
behavior of the proposed regularization in comparison to 5 comparable baselines
across 13 language understanding tasks (GLUE benchmark and six additional
datasets). When evaluating in-domain performance, REPINA consistently
outperforms other baselines on most tasks (10 out of 13). We also demonstrate
its effectiveness in few-shot settings and robustness to label perturbation. As
a by-product, we extend previous studies of representation collapse and propose
several metrics to quantify it. Our empirical findings show that our approach
is significantly more effective at mitigating representation collapse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. (arXiv:2205.12665v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12665">
<div class="article-summary-box-inner">
<span><p>Existing benchmarks for open-domain question answering (ODQA) typically focus
on questions whose answers can be extracted from a single paragraph. By
contrast, many natural questions, such as "What players were drafted by the
Brooklyn Nets?" have a list of answers. Answering such questions requires
retrieving and reading from many passages, in a large corpus. We introduce
QAMPARI, an ODQA benchmark, where question answers are lists of entities,
spread across many paragraphs. We created QAMPARI by (a) generating questions
with multiple answers from Wikipedia's knowledge graph and tables, (b)
automatically pairing answers with supporting evidence in Wikipedia paragraphs,
and (c) manually paraphrasing questions and validating each answer. We train
ODQA models from the retrieve-and-read family and find that QAMPARI is
challenging in terms of both passage retrieval and answer generation, reaching
an F1 score of 32.8 at best. Our results highlight the need for developing ODQA
models that handle a broad range of question types, including single and
multi-answer questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization. (arXiv:2208.11303v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.11303">
<div class="article-summary-box-inner">
<span><p>Most current multi-modal summarization methods follow a cascaded manner,
where an off-the-shelf object detector is first used to extract visual
features, then these features are fused with language representations to
generate the summary with an encoder-decoder model. The cascaded way cannot
capture the semantic alignments between images and paragraphs, which are
crucial to a precise summary. In this paper, we propose ViL-Sum to jointly
model paragraph-level \textbf{Vi}sion-\textbf{L}anguage Semantic Alignment and
Multi-Modal \textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal
encoder with two well-designed tasks, image reordering and image selection. The
joint multi-modal encoder captures the interactions between modalities, where
the reordering task guides the model to learn paragraph-level semantic
alignment and the selection task guides the model to selected summary-related
images in the final summary. Experimental results show that our proposed
ViL-Sum significantly outperforms current state-of-the-art methods. In further
analysis, we find that two well-designed tasks and joint multi-modal encoder
can effectively guide the model to learn reasonable paragraphs-images and
summary-images relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval. (arXiv:2210.03625v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03625">
<div class="article-summary-box-inner">
<span><p>Multilingual text-video retrieval methods have improved significantly in
recent years, but the performance for other languages lags behind English. We
propose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve
multilingual text-video retrieval. Inspired by the fact that English text-video
retrieval outperforms other languages, we train a student model using input
text in different languages to match the cross-modal predictions from teacher
models using input text in English. We propose a cross entropy based objective
which forces the distribution over the student's text-video similarity scores
to be similar to those of the teacher models. We introduce a new multilingual
video dataset, Multi-YouCook2, by translating the English captions in the
YouCook2 video dataset to 8 other languages. Our method improves multilingual
text-video retrieval performance on Multi-YouCook2 and several other datasets
such as Multi-MSRVTT and VATEX. We also conducted an analysis on the
effectiveness of different multilingual text models as teachers. The code,
models, and dataset are available at https://github.com/roudimit/c2kd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Cultural Commonsense Knowledge at Scale. (arXiv:2210.07763v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07763">
<div class="article-summary-box-inner">
<span><p>Structured knowledge is important for many AI applications. Commonsense
knowledge, which is crucial for robust human-centric AI, is covered by a small
number of structured knowledge projects. However, they lack knowledge about
human traits and behaviors conditioned on socio-cultural contexts, which is
crucial for situative AI. This paper presents CANDLE, an end-to-end methodology
for extracting high-quality cultural commonsense knowledge (CCSK) at scale.
CANDLE extracts CCSK assertions from a huge web corpus and organizes them into
coherent clusters, for 3 domains of subjects (geography, religion, occupation)
and several cultural facets (food, drinks, clothing, traditions, rituals,
behaviors). CANDLE includes judicious techniques for classification-based
filtering and scoring of interestingness. Experimental evaluations show the
superiority of the CANDLE CCSK collection over prior works, and an extrinsic
use case demonstrates the benefits of CCSK for the GPT-3 language model. Code
and data can be accessed at https://candle.mpi-inf.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. (arXiv:2210.17114v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17114">
<div class="article-summary-box-inner">
<span><p>Limited computational budgets often prevent transformers from being used in
production and from having their high accuracy utilized. A knowledge
distillation approach addresses the computational efficiency by self-distilling
BERT into a smaller transformer representation having fewer layers and smaller
internal embedding. However, the performance of these models drops as we reduce
the number of layers, notably in advanced NLP tasks such as span question
answering. In addition, a separate model must be trained for each inference
scenario with its distinct computational budget. Dynamic-TinyBERT tackles both
limitations by partially implementing the Length Adaptive Transformer (LAT)
technique onto TinyBERT, achieving x3 speedup over BERT-base with minimal
accuracy loss. In this work, we expand the Dynamic-TinyBERT approach to
generate a much more highly efficient model. We use MiniLM distillation jointly
with the LAT method, and we further enhance the efficiency by applying low-bit
quantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is
trained only once, dynamically fits any inference scenario, and achieves an
accuracy-efficiency trade-off superior to any other efficient approaches per
any computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with &lt;1%
accuracy loss). The code to reproduce this work is publicly available on
Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMAuC -- The Scientific Multi-Authorship Corpus. (arXiv:2211.02477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02477">
<div class="article-summary-box-inner">
<span><p>The rapidly growing volume of scientific publications offers an interesting
challenge for research on methods for analyzing the authorship of documents
with one or more authors. However, most existing datasets lack scientific
documents or the necessary metadata for constructing new experiments and test
cases. We introduce SMAuC, a comprehensive, metadata-rich corpus tailored to
scientific authorship analysis. Comprising over 3 million publications across
various disciplines from over 5 million authors, SMAuC is the largest openly
accessible corpus for this purpose. It encompasses scientific texts from
humanities and natural sciences, accompanied by extensive, curated metadata,
including unambiguous author IDs. SMAuC aims to significantly advance the
domain of authorship analysis in scientific texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08146">
<div class="article-summary-box-inner">
<span><p>Local news articles are a subset of news that impact users in a geographical
area, such as a city, county, or state. Detecting local news (Step 1) and
subsequently deciding its geographical location as well as radius of impact
(Step 2) are two important steps towards accurate local news recommendation.
Naive rule-based methods, such as detecting city names from the news title,
tend to give erroneous results due to lack of understanding of the news
content. Empowered by the latest development in natural language processing, we
develop an integrated pipeline that enables automatic local news detection and
content-based local news recommendations. In this paper, we focus on Step 1 of
the pipeline, which highlights: (1) a weakly supervised framework incorporated
with domain knowledge and auto data processing, and (2) scalability to
multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline
has higher precision and recall evaluated on a real-world and human-labeled
dataset. This pipeline has potential to more precise local news to users, helps
local businesses get more exposure, and gives people more information about
their neighborhood safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguishability Calibration to In-Context Learning. (arXiv:2302.06198v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06198">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed increasing interests in prompt-based learning in
which models can be trained on only a few annotated instances, making them
suitable in low-resource settings. When using prompt-based learning for text
classification, the goal is to use a pre-trained language model (PLM) to
predict a missing token in a pre-defined template given an input text, which
can be mapped to a class label. However, PLMs built on the transformer
architecture tend to generate similar output embeddings, making it difficult to
discriminate between different class labels. The problem is further exacerbated
when dealing with classification tasks involving many fine-grained class
labels. In this work, we alleviate this information diffusion issue, i.e.,
different tokens share a large proportion of similar information after going
through stacked multiple self-attention layers in a transformer, by proposing a
calibration method built on feature transformations through rotation and
scaling to map a PLM-encoded embedding into a new metric space to guarantee the
distinguishability of the resulting embeddings. Furthermore, we take the
advantage of hyperbolic embeddings to capture the hierarchical relations among
fine-grained class-associated token embedding by a coarse-to-fine metric
learning strategy to enhance the distinguishability of the learned output
embeddings. Extensive experiments on the three datasets under various settings
demonstrate the effectiveness of our approach. Our code can be found at
https://github.com/donttal/TARA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08290">
<div class="article-summary-box-inner">
<span><p>Making the most use of abundant information in electronic health records
(EHR) is rapidly becoming an important topic in the medical domain. Recent work
presented a promising framework that embeds entire features in raw EHR data
regardless of its form and medical code standards. The framework, however, only
focuses on encoding EHR with minimal preprocessing and fails to consider how to
learn efficient EHR representation in terms of computation and memory usage. In
this paper, we search for a versatile encoder not only reducing the large data
into a manageable size but also well preserving the core information of
patients to perform diverse clinical tasks. We found that hierarchically
structured Convolutional Neural Network (CNN) often outperforms the
state-of-the-art model on diverse tasks such as reconstruction, prediction, and
generation, even with fewer parameters and less training time. Moreover, it
turns out that making use of the inherent hierarchy of EHR data can boost the
performance of any kind of backbone models and clinical tasks performed.
Through extensive experiments, we present concrete evidence to generalize our
research findings into real-world practice. We give a clear guideline on
building the encoder based on the research findings captured while exploring
numerous settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01228">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models for code (PLMCs) have gained attention in recent
research. These models are pre-trained on large-scale datasets using
multi-modal objectives. However, fine-tuning them requires extensive
supervision and is limited by the size of the dataset provided. We aim to
improve this issue by proposing a simple data augmentation framework. Our
framework utilizes knowledge gained during the pre-training and fine-tuning
stage to generate pseudo data, which is then used as training data for the next
step. We incorporate this framework into the state-of-the-art language models,
such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework
significantly improves PLMCs' performance in code-related sequence generation
tasks, such as code summarization and code generation in the CodeXGLUE
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nominal Topology for Data Languages. (arXiv:2304.13337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13337">
<div class="article-summary-box-inner">
<span><p>We propose a novel topological perspective on data languages recognizable by
orbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite
nominal topological spaces. Assuming globally bounded support sizes, they
coincide with nominal Stone spaces and are shown to be dually equivalent to a
subcategory of nominal boolean algebras. Recognizable data languages are
characterized as topologically clopen sets of pro-orbit-finite words. In
addition, we explore the expressive power of pro-orbit-finite equations by
establishing a nominal version of Reiterman's pseudovariety theorem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs. (arXiv:2305.01323v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01323">
<div class="article-summary-box-inner">
<span><p>Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the
instructions of a flowchart to diagnose users' problems in specific domains
(eg., vehicle, laptop), have been gaining research interest in recent years.
However, collecting sufficient dialogues that are naturally grounded on
flowcharts is costly, thus FTD systems are impeded by scarce training data. To
mitigate the data sparsity issue, we propose a plan-based data augmentation
(PlanDA) approach that generates diverse synthetic dialog data at scale by
transforming concise flowchart into dialogues. Specifically, its generative
model employs a variational-base framework with a hierarchical planning
strategy that includes global and local latent planning variables. Experiments
on the FloDial dataset show that synthetic dialogue produced by PlanDA improves
the performance of downstream tasks, including flowchart path retrieval and
response generation, in particular on the Out-of-Flowchart settings. In
addition, further analysis demonstrate the quality of synthetic data generated
by PlanDA in paths that are covered by current sample dialogues and paths that
are not covered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01876">
<div class="article-summary-box-inner">
<span><p>Concepts benefit natural language understanding but are far from complete in
existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs)
have been widely used in text-based concept extraction (CE). However, PLMs tend
to mine the co-occurrence associations from massive corpus as pre-trained
knowledge rather than the real causal effect between tokens. As a result, the
pre-trained knowledge confounds PLMs to extract biased concepts based on
spurious co-occurrence correlations, inevitably resulting in low precision. In
this paper, through the lens of a Structural Causal Model (SCM), we propose
equipping the PLM-based extractor with a knowledge-guided prompt as an
intervention to alleviate concept bias. The prompt adopts the topic of the
given entity from the existing knowledge in KGs to mitigate the spurious
co-occurrence correlations between entities and biased concepts. Our extensive
experiments on representative multilingual KG datasets justify that our
proposed prompt can effectively alleviate concept bias and improve the
performance of PLM-based CE models.The code has been released on
https://github.com/siyuyuan/KPCE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curating corpora with classifiers: A case study of clean energy sentiment online. (arXiv:2305.03092v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03092">
<div class="article-summary-box-inner">
<span><p>Well curated, large-scale corpora of social media posts containing broad
public opinion offer an alternative data source to complement traditional
surveys. While surveys are effective at collecting representative samples and
are capable of achieving high accuracy, they can be both expensive to run and
lag public opinion by days or weeks. Both of these drawbacks could be overcome
with a real-time, high volume data stream and fast analysis pipeline. A central
challenge in orchestrating such a data pipeline is devising an effective method
for rapidly selecting the best corpus of relevant documents for analysis.
Querying with keywords alone often includes irrelevant documents that are not
easily disambiguated with bag-of-words natural language processing methods.
Here, we explore methods of corpus curation to filter irrelevant tweets using
pre-trained transformer-based models, fine-tuned for our binary classification
task on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.
The low cost and high performance of fine-tuning such a model suggests that our
approach could be of broad benefit as a pre-processing step for social media
datasets with uncertain corpus boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04160">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable language abilities.
GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities
beyond previous visual language models. We attribute this to the use of more
advanced LLMs compared with previous multimodal models. Unfortunately, the
model architecture and training strategies of GPT-4 are unknown. To endow LLMs
with multimodal capabilities, we propose X-LLM, which converts Multi-modalities
(images, speech, videos) into foreign languages using X2L interfaces and inputs
them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple
frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X''
denotes multi-modalities such as image, speech, and videos, and ``L'' denotes
languages. X-LLM's training consists of three stages: (1) Converting Multimodal
Information: The first stage trains each X2L interface to align with its
respective single-modal encoder separately to convert multimodal information
into languages. (2) Aligning X2L representations with the LLM: single-modal
encoders are aligned with the LLM through X2L interfaces independently. (3)
Integrating multiple modalities: all single-modal encoders are aligned with the
LLM through X2L interfaces to integrate multimodal capabilities into the LLM.
Our experiments show that X-LLM demonstrates impressive multimodel chat
abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen
images/instructions, and yields a 84.5\% relative score compared with GPT-4 on
a synthetic multimodal instruction-following dataset. And we also conduct
quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote
the era of LLM-based speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning. (arXiv:2305.04808v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04808">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning, aiming at endowing machines with a human-like ability
to make situational presumptions, is extremely challenging to generalize. For
someone who barely knows about "meditation," while is knowledgeable about
"singing," he can still infer that "meditation makes people relaxed" from the
existing knowledge that "singing makes people relaxed" by first conceptualizing
"singing" as a "relaxing event" and then instantiating that event to
"meditation." This process, known as conceptual induction and deduction, is
fundamental to commonsense reasoning while lacking both labeled data and
methodologies to enhance commonsense modeling. To fill such a research gap, we
propose CAT (Contextualized ConceptuAlization and InsTantiation), a
semi-supervised learning framework that integrates event conceptualization and
instantiation to conceptualize commonsense knowledge bases at scale. Extensive
experiments show that our framework achieves state-of-the-art performances on
two conceptualization tasks, and the acquired abstract commonsense knowledge
can significantly improve commonsense inference modeling. Our code, data, and
fine-tuned models are publicly available at
https://github.com/HKUST-KnowComp/CAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05027">
<div class="article-summary-box-inner">
<span><p>We introduce a state-of-the-art approach for URL categorization that
leverages the power of Large Language Models (LLMs) to address the primary
objectives of web content filtering: safeguarding organizations from legal and
ethical risks, limiting access to high-risk or suspicious websites, and
fostering a secure and professional work environment. Our method utilizes LLMs
to generate accurate classifications and then employs established knowledge
distillation techniques to create smaller, more specialized student models
tailored for web content filtering. Distillation results in a student model
with a 9% accuracy rate improvement in classifying websites, sourced from
customer telemetry data collected by a large security vendor, into 30 distinct
content categories based on their URLs, surpassing the current state-of-the-art
approach. Our student model matches the performance of the teacher LLM with 175
times less parameters, allowing the model to be used for in-line scanning of
large volumes of URLs, and requires 3 orders of magnitude less manually labeled
training data than the current state-of-the-art approach. Depending on the
specific use case, the output generated by our approach can either be directly
returned or employed as a pre-filter for more resource-intensive operations
involving website images or HTML.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation. (arXiv:2305.05166v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05166">
<div class="article-summary-box-inner">
<span><p>Text image machine translation (TIMT) aims to translate texts embedded in
images from one source language to another target language. Existing methods,
both two-stage cascade and one-stage end-to-end architectures, suffer from
different issues. The cascade models can benefit from the large-scale optical
character recognition (OCR) and MT datasets but the two-stage architecture is
redundant. The end-to-end models are efficient but suffer from training data
deficiency. To this end, in our paper, we propose an end-to-end TIMT model
fully making use of the knowledge from existing OCR and MT datasets to pursue
both an effective and efficient framework. More specifically, we build a novel
modal adapter effectively bridging the OCR encoder and MT decoder. End-to-end
TIMT loss and cross-modal contrastive loss are utilized jointly to align the
feature distribution of the OCR and MT tasks. Extensive experiments show that
the proposed method outperforms the existing two-stage cascade models and
one-stage end-to-end models with a lighter and faster architecture.
Furthermore, the ablation studies verify the generalization of our method,
where the proposed modal adapter is effective to bridge various OCR and MT
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05226">
<div class="article-summary-box-inner">
<span><p>Text image machine translation (TIMT) has been widely used in various
real-world applications, which translates source language texts in images into
another target language sentence. Existing methods on TIMT are mainly divided
into two categories: the recognition-then-translation pipeline model and the
end-to-end model. However, how to transfer knowledge from the pipeline model
into the end-to-end model remains an unsolved problem. In this paper, we
propose a novel Multi-Teacher Knowledge Distillation (MTKD) method to
effectively distillate knowledge into the end-to-end TIMT model from the
pipeline model. Specifically, three teachers are utilized to improve the
performance of the end-to-end TIMT model. The image encoder in the end-to-end
TIMT model is optimized with the knowledge distillation guidance from the
recognition teacher encoder, while the sequential encoder and decoder are
improved by transferring knowledge from the translation sequential and decoder
teacher models. Furthermore, both token and sentence-level knowledge
distillations are incorporated to better boost the translation performance.
Extensive experimental results show that our proposed MTKD effectively improves
the text image translation performance and outperforms existing end-to-end and
pipeline models with fewer parameters and less decoding time, illustrating that
MTKD can take advantage of both pipeline and end-to-end models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05252">
<div class="article-summary-box-inner">
<span><p>In everyday life, humans often plan their actions by following step-by-step
instructions in the form of goal-oriented scripts. Previous work has exploited
language models (LMs) to plan for abstract goals of stereotypical activities
(e.g., "make a cake"), but leaves more specific goals with multi-facet
constraints understudied (e.g., "make a cake for diabetics"). In this paper, we
define the task of constrained language planning for the first time. We propose
an overgenerate-then-filter approach to improve large language models (LLMs) on
this task, and use it to distill a novel constrained language planning dataset,
CoScript, which consists of 55,000 scripts. Empirical results demonstrate that
our method significantly improves the constrained language planning ability of
LLMs, especially on constraint faithfulness. Furthermore, CoScript is
demonstrated to be quite effective in endowing smaller LMs with constrained
language planning ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Need Holistically Thought in Medical Conversational QA. (arXiv:2305.05410v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05410">
<div class="article-summary-box-inner">
<span><p>The medical conversational question answering (CQA) system aims at providing
a series of professional medical services to improve the efficiency of medical
care. Despite the success of large language models (LLMs) in complex reasoning
tasks in various fields, such as mathematics, logic, and commonsense QA, they
still need to improve with the increased complexity and specialization of the
medical field. This is because medical CQA tasks require not only strong
medical reasoning, but also the ability to think broadly and deeply. In this
paper, to address these challenges in medical CQA tasks that need to be
considered and understood in many aspects, we propose the Holistically Thought
(HoT) method, which is designed to guide the LLMs to perform the diffused and
focused thinking for generating high-quality medical responses. The proposed
HoT method has been evaluated through automated and manual assessments in three
different medical CQA datasets containing the English and Chinese languages.
The extensive experimental results show that our method can produce more
correctness, professional, and considerate answers than several
state-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in
https://github.com/WENGSYX/HoT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Person or Entity-centric Knowledge Graphs: An Application in Healthcare. (arXiv:2305.05640v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05640">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs (KGs) are a popular way to organise information based on
ontologies or schemas and have been used across a variety of scenarios from
search to recommendation. Despite advances in KGs, representing knowledge
remains a non-trivial task across industries and it is especially challenging
in the biomedical and healthcare domains due to complex interdependent
relations between entities, heterogeneity, lack of standardization, and
sparseness of data. KGs are used to discover diagnoses or prioritize genes
relevant to disease, but they often rely on schemas that are not centred around
a node or entity of interest, such as a person. Entity-centric KGs are
relatively unexplored but hold promise in representing important facets
connected to a central node and unlocking downstream tasks beyond graph
traversal and reasoning, such as generating graph embeddings and training graph
neural networks for a wide range of predictive tasks. This paper presents an
end-to-end representation learning framework to extract entity-centric KGs from
structured and unstructured data. We introduce a star-shaped ontology to
represent the multiple facets of a person and use it to guide KG creation.
Compact representations of the graphs are created leveraging graph neural
networks and experiments are conducted using different levels of heterogeneity
or explicitness. A readmission prediction task is used to evaluate the results
of the proposed framework, showing a stable system, robust to missing data,
that outperforms a range of baseline machine learning classifiers. We highlight
that this approach has several potential applications across domains and is
open-sourced. Lastly, we discuss lessons learned, challenges, and next steps
for the adoption of the framework in practice.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-11 23:11:56.868176810 UTC">2023-05-11 23:11:56 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>