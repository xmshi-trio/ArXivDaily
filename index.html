<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-08T01:30:00Z">08-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MyVoice: Arabic Speech Resource Collaboration Platform. (arXiv:2308.02503v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02503">
<div class="article-summary-box-inner">
<span><p>We introduce MyVoice, a crowdsourcing platform designed to collect Arabic
speech to enhance dialectal speech technologies. This platform offers an
opportunity to design large dialectal speech datasets; and makes them publicly
available. MyVoice allows contributors to select city/country-level
fine-grained dialect and record the displayed utterances. Users can switch
roles between contributors and annotators. The platform incorporates a quality
assurance system that filters out low-quality and spurious recordings before
sending them for validation. During the validation phase, contributors can
assess the quality of recordings, annotate them, and provide feedback which is
then reviewed by administrators. Furthermore, the platform offers flexibility
to admin roles to add new data or tasks beyond dialectal speech and word
collection, which are displayed to contributors. Thus, enabling collaborative
efforts in gathering diverse and large Arabic speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Generalization Ability in Essay Coherence Evaluation through Monotonic Constraints. (arXiv:2308.02506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02506">
<div class="article-summary-box-inner">
<span><p>Coherence is a crucial aspect of evaluating text readability and can be
assessed through two primary factors when evaluating an essay in a scoring
scenario. The first factor is logical coherence, characterized by the
appropriate use of discourse connectives and the establishment of logical
relationships between sentences. The second factor is the appropriateness of
punctuation, as inappropriate punctuation can lead to confused sentence
structure. To address these concerns, we propose a coherence scoring model
consisting of a regression model with two feature extractors: a local coherence
discriminative model and a punctuation correction model. We employ
gradient-boosting regression trees as the regression model and impose
monotonicity constraints on the input features. The results show that our
proposed model better generalizes unseen data. The model achieved third place
in track 1 of NLPCC 2023 shared task 7. Additionally, we briefly introduce our
solution for the remaining tracks, which achieves second place for track 2 and
first place for both track 3 and track 4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chatbot Application to Support Smart Agriculture in Thailand. (arXiv:2308.02524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02524">
<div class="article-summary-box-inner">
<span><p>A chatbot is a software developed to help reply to text or voice
conversations automatically and quickly in real time. In the agriculture
sector, the existing smart agriculture systems just use data from sensing and
internet of things (IoT) technologies that exclude crop cultivation knowledge
to support decision-making by farmers. To enhance this, the chatbot application
can be an assistant to farmers to provide crop cultivation knowledge.
Consequently, we propose the LINE chatbot application as an information and
knowledge representation providing crop cultivation recommendations to farmers.
It works with smart agriculture and recommendation systems. Our proposed LINE
chatbot application consists of five main functions (start/stop menu, main
page, drip irri gation page, mist irrigation page, and monitor page). Farmers
will receive information for data monitoring to support their decision-making.
Moreover, they can control the irrigation system via the LINE chatbot.
Furthermore, farmers can ask questions relevant to the crop environment via a
chat box. After implementing our proposed chatbot, farmers are very satisfied
with the application, scoring a 96% satisfaction score. However, in terms of
asking questions via chat box, this LINE chatbot application is a rule-based
bot or script bot. Farmers have to type in the correct keywords as prescribed,
otherwise they won't get a response from the chatbots. In the future, we will
enhance the asking function of our LINE chatbot to be an intelligent bot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP. (arXiv:2308.02537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02537">
<div class="article-summary-box-inner">
<span><p>Supervised machine learning and deep learning require a large amount of
labeled data, which data scientists obtain in a manual, and time-consuming
annotation process. To mitigate this challenge, Active Learning (AL) proposes
promising data points to annotators they annotate next instead of a subsequent
or random sample. This method is supposed to save annotation effort while
maintaining model performance. However, practitioners face many AL strategies
for different tasks and need an empirical basis to choose between them. Surveys
categorize AL strategies into taxonomies without performance indications.
Presentations of novel AL strategies compare the performance to a small subset
of strategies. Our contribution addresses the empirical basis by introducing a
reproducible active learning evaluation (ALE) framework for the comparative
evaluation of AL strategies in NLP. The framework allows the implementation of
AL strategies with low effort and a fair data-driven comparison through
defining and tracking experiment parameters (e.g., initial dataset size, number
of data points per query step, and the budget). ALE helps practitioners to make
more informed decisions, and researchers can focus on developing new, effective
AL strategies and deriving best practices for specific use cases. With best
practices, practitioners can lower their annotation costs. We present a case
study to illustrate how to use the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSMo: A constructor specification language for Abstract Wikipedia's content selection process. (arXiv:2308.02539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02539">
<div class="article-summary-box-inner">
<span><p>Representing snippets of information abstractly is a task that needs to be
performed for various purposes, such as database view specification and the
first stage in the natural language generation pipeline for generative AI from
structured input, i.e., the content selection stage to determine what needs to
be verbalised. For the Abstract Wikipedia project, requirements analysis
revealed that such an abstract representation requires multilingual modelling,
content selection covering declarative content and functions, and both classes
and instances. There is no modelling language that meets either of the three
features, let alone a combination. Following a rigorous language design process
inclusive of broad stakeholder consultation, we created CoSMo, a novel {\sc
Co}ntent {\sc S}election {\sc Mo}deling language that meets these and other
requirements so that it may be useful both in Abstract Wikipedia as well as
other contexts. We describe the design process, rationale and choices, the
specification, and preliminary evaluation of the language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards More Human-like AI Communication: A Review of Emergent Communication Research. (arXiv:2308.02541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02541">
<div class="article-summary-box-inner">
<span><p>In the recent shift towards human-centric AI, the need for machines to
accurately use natural language has become increasingly important. While a
common approach to achieve this is to train large language models, this method
presents a form of learning misalignment where the model may not capture the
underlying structure and reasoning humans employ in using natural language,
potentially leading to unexpected or unreliable behavior. Emergent
communication (Emecom) is a field of research that has seen a growing number of
publications in recent years, aiming to develop artificial agents capable of
using natural language in a way that goes beyond simple discriminative tasks
and can effectively communicate and learn new concepts. In this review, we
present Emecom under two aspects. Firstly, we delineate all the common
proprieties we find across the literature and how they relate to human
interactions. Secondly, we identify two subcategories and highlight their
characteristics and open challenges. We encourage researchers to work together
by demonstrating that different methods can be viewed as diverse solutions to a
common problem and emphasize the importance of including diverse perspectives
and expertise in the field. We believe a deeper understanding of human
communication is crucial to developing machines that can accurately use natural
language in human-machine interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect based sentimental analysis for travellers' reviews. (arXiv:2308.02548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02548">
<div class="article-summary-box-inner">
<span><p>Airport service quality evaluation is commonly found on social media,
including Google Maps. This valuable for airport management in order to enhance
the quality of services provided. However; prior studies either provide general
review for topics discussed by travellers or provide sentimental value to tag
the entire review without specifically mentioning the airport service that is
behind such value. Accordingly, this work proposes using aspect based
sentimental analysis in order to provide more detailed analysis for travellers
reviews. This works applied aspect based sentimental analysis on data collected
from Google Map about Dubai and Doha airports. The results provide tangible
reasons to use aspect based sentimental analysis in order to understand more
the travellers and spot airport services that are in need for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Industrial Memories: Exploring the Findings of Government Inquiries with Neural Word Embedding and Machine Learning. (arXiv:2308.02556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02556">
<div class="article-summary-box-inner">
<span><p>We present a text mining system to support the exploration of large volumes
of text detailing the findings of government inquiries. Despite their
historical significance and potential societal impact, key findings of
inquiries are often hidden within lengthy documents and remain inaccessible to
the general public. We transform the findings of the Irish government's inquiry
into industrial schools and through the use of word embedding, text
classification and visualisation, present an interactive web-based platform
that enables the exploration of the text to uncover new historical insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning. (arXiv:2308.02565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02565">
<div class="article-summary-box-inner">
<span><p>Textual graphs (TGs) are graphs whose nodes correspond to text (sentences or
documents), which are widely prevalent. The representation learning of TGs
involves two stages: (i) unsupervised feature extraction and (ii) supervised
graph representation learning. In recent years, extensive efforts have been
devoted to the latter stage, where Graph Neural Networks (GNNs) have dominated.
However, the former stage for most existing graph benchmarks still relies on
traditional feature engineering techniques. More recently, with the rapid
development of language models (LMs), researchers have focused on leveraging
LMs to facilitate the learning of TGs, either by jointly training them in a
computationally intensive framework (merging the two stages), or designing
complex self-supervised training tasks for feature extraction (enhancing the
first stage). In this work, we present SimTeG, a frustratingly Simple approach
for Textual Graph learning that does not innovate in frameworks, models, and
tasks. Instead, we first perform supervised parameter-efficient fine-tuning
(PEFT) on a pre-trained LM on the downstream task, such as node classification.
We then generate node embeddings using the last hidden states of finetuned LM.
These derived features can be further utilized by any GNN for training on the
same task. We evaluate our approach on two fundamental graph representation
learning tasks: node classification and link prediction. Through extensive
experiments, we show that our approach significantly improves the performance
of various GNNs on multiple graph benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioBERT Based SNP-traits Associations Extraction from Biomedical Literature. (arXiv:2308.02569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02569">
<div class="article-summary-box-inner">
<span><p>Scientific literature contains a considerable amount of information that
provides an excellent opportunity for developing text mining methods to extract
biomedical relationships. An important type of information is the relationship
between singular nucleotide polymorphisms (SNP) and traits. In this paper, we
present a BioBERT-GRU method to identify SNP- traits associations. Based on the
evaluation of our method on the SNPPhenA dataset, it is concluded that this new
method performs better than previous machine learning and deep learning based
methods. BioBERT-GRU achieved the result a precision of 0.883, recall of 0.882
and F1-score of 0.881.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER. (arXiv:2308.02570v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02570">
<div class="article-summary-box-inner">
<span><p>The challenge posed by multimodal named entity recognition (MNER) is mainly
two-fold: (1) bridging the semantic gap between text and image and (2) matching
the entity with its associated object in image. Existing methods fail to
capture the implicit entity-object relations, due to the lack of corresponding
annotation. In this paper, we propose a bidirectional generative alignment
method named BGA-MNER to tackle these issues. Our BGA-MNER consists of
\texttt{image2text} and \texttt{text2image} generation with respect to
entity-salient content in two modalities. It jointly optimizes the
bidirectional reconstruction objectives, leading to aligning the implicit
entity-object relations under such direct and powerful constraints.
Furthermore, image-text pairs usually contain unmatched components which are
noisy for generation. A stage-refined context sampler is proposed to extract
the matched cross-modal content for generation. Extensive experiments on two
benchmarks demonstrate that our method achieves state-of-the-art performance
without image input during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings. (arXiv:2308.02575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02575">
<div class="article-summary-box-inner">
<span><p>This study investigates the consistency of feedback ratings generated by
OpenAI's GPT-4, a state-of-the-art artificial intelligence language model,
across multiple iterations, time spans and stylistic variations. The model
rated responses to tasks within the Higher Education (HE) subject domain of
macroeconomics in terms of their content and style. Statistical analysis was
conducted in order to learn more about the interrater reliability, consistency
of the ratings across iterations and the correlation between ratings in terms
of content and style. The results revealed a high interrater reliability with
ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting
that GPT-4 is capable of generating consistent ratings across repetitions with
a clear prompt. Style and content ratings show a high correlation of 0.87. When
applying a non-adequate style the average content ratings remained constant,
while style ratings decreased, which indicates that the large language model
(LLM) effectively distinguishes between these two criteria during evaluation.
The prompt used in this study is furthermore presented and explained. Further
research is necessary to assess the robustness and reliability of AI models in
various use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02582">
<div class="article-summary-box-inner">
<span><p>Cross-domain and cross-compositional generalization of Text-to-SQL semantic
parsing is a challenging task. Existing Large Language Model (LLM) based
solutions rely on inference-time retrieval of few-shot exemplars from the
training set to synthesize a run-time prompt for each Natural Language (NL)
test query. In contrast, we devise an algorithm which performs offline sampling
of a minimal set-of few-shots from the training data, with complete coverage of
SQL clauses, operators and functions, and maximal domain coverage within the
allowed token length. This allows for synthesis of a fixed Generic Prompt (GP),
with a diverse set-of exemplars common across NL test queries, avoiding
expensive test time exemplar retrieval. We further auto-adapt the GP to the
target database domain (DA-GP), to better handle cross-domain generalization;
followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle
cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline
task, to be performed one-time per new database with minimal human
intervention. Our approach demonstrates superior performance on the KaggleDBQA
dataset, designed to evaluate generalizability for the Text-to-SQL task. We
further showcase consistent performance improvement of LTMP-DA-GP over GP,
across LLMs and databases of KaggleDBQA, highlighting the efficacy and model
agnostic benefits of our prompt based adapt and decompose approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02618">
<div class="article-summary-box-inner">
<span><p>The General Transit Feed Specification (GTFS) standard for publishing transit
data is ubiquitous. GTFS being tabular data, with information spread across
different files, necessitates specialized tools or packages to retrieve
information. Concurrently, the use of Large Language Models for text and
information retrieval is growing. The idea of this research is to see if the
current widely adopted LLMs (ChatGPT) are able to retrieve information from
GTFS using natural language instructions. We first test whether ChatGPT
(GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our
multiple-choice questions (MCQ) correctly. Next, we task the LLM with
information extractions from a filtered GTFS feed with 4 routes. For
information retrieval, we compare zero-shot and program synthesis. Program
synthesis works better, achieving ~90% accuracy on simple questions and ~40%
accuracy on complex questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Good Are SOTA Fake News Detectors. (arXiv:2308.02727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02727">
<div class="article-summary-box-inner">
<span><p>Automatic fake news detection with machine learning can prevent the
dissemination of false statements before they gain many views. Several datasets
labeling statements as legitimate or false have been created since the 2016
United States presidential election for the prospect of training machine
learning models. We evaluate the robustness of both traditional and deep
state-of-the-art models to gauge how well they may perform in the real world.
We find that traditional models tend to generalize better to data outside the
distribution it was trained on compared to more recently-developed large
language models, though the best model to use may depend on the specific task
at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification. (arXiv:2308.02746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02746">
<div class="article-summary-box-inner">
<span><p>Text classification is a fundamental task for natural language processing,
and adapting text classification models across domains has broad applications.
Self-training generates pseudo-examples from the model's predictions and
iteratively trains on the pseudo-examples, i.e., minimizes the loss on the
source domain and the Gibbs entropy on the target domain. However, Gibbs
entropy is sensitive to prediction errors, and thus, self-training tends to
fail when the domain shift is large. In this paper, we propose Meta-Tsallis
Entropy minimization (MTEM), which applies a meta-learning algorithm to
optimize the instance adaptive Tsallis entropy on the target domain. To reduce
the computation cost of MTEM, we propose an approximation technique to
approximate the Second-order derivation involved in the meta-learning. To
efficiently generate pseudo labels, we propose an annealing sampling mechanism
for exploring the model's prediction probability. Theoretically, we prove the
convergence of the meta-learning algorithm in MTEM and analyze the
effectiveness of MTEM in achieving domain adaptation. Experimentally, MTEM
improves the adaptation performance of BERT with an average of 4 percent on the
benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education. (arXiv:2308.02773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02773">
<div class="article-summary-box-inner">
<span><p>EduChat (https://www.educhat.top/) is a large-scale language model
(LLM)-based chatbot system in the education domain. Its goal is to support
personalized, fair, and compassionate intelligent education, serving teachers,
students, and parents. Guided by theories from psychology and education, it
further strengthens educational functions such as open question answering,
essay assessment, Socratic teaching, and emotional support based on the
existing basic LLMs. Particularly, we learn domain-specific knowledge by
pre-training on the educational corpus and stimulate various skills with tool
use by fine-tuning on designed system prompts and instructions. Currently,
EduChat is available online as an open-source project, with its code, data, and
model parameters available on platforms (e.g., GitHub
https://github.com/icalk-nlp/EduChat, Hugging Face
https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its
capabilities online (https://vimeo.<a href="/abs/com/8510044">com/8510044</a>54). This initiative aims to
promote research and applications of LLMs for intelligent education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging. (arXiv:2308.02870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02870">
<div class="article-summary-box-inner">
<span><p>The conventional recipe for Automatic Speech Recognition (ASR) models is to
1) train multiple checkpoints on a training set while relying on a validation
set to prevent overfitting using early stopping and 2) average several last
checkpoints or that of the lowest validation losses to obtain the final model.
In this paper, we rethink and update the early stopping and checkpoint
averaging from the perspective of the bias-variance tradeoff. Theoretically,
the bias and variance represent the fitness and variability of a model and the
tradeoff of them determines the overall generalization error. But, it's
impractical to evaluate them precisely. As an alternative, we take the training
loss and validation loss as proxies of bias and variance and guide the early
stopping and checkpoint averaging using their tradeoff, namely an Approximated
Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models,
our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and
AISHELL-2, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00676">
<div class="article-summary-box-inner">
<span><p>There are now many adversarial attacks for natural language processing
systems. Of these, a vast majority achieve success by modifying individual
document tokens, which we call here a token-modification attack. Each
token-modification attack is defined by a specific combination of fundamental
components, such as a constraint on the adversary or a particular search
algorithm. Motivated by this observation, we survey existing token-modification
attacks and extract the components of each. We use an attack-independent
framework to structure our survey which results in an effective categorisation
of the field and an easy comparison of components. This survey aims to guide
new researchers to this field and spark further research into individual attack
components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design and Implementation of English To Yor\`ub\'a Verb Phrase Machine Translation System. (arXiv:2104.04125v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04125">
<div class="article-summary-box-inner">
<span><p>We aim to develop an English-to-Yoruba machine translation system which can
translate English verb phrase text to its Yoruba equivalent.Words from both
languages Source Language and Target Language were collected for the verb
phrase group in the home domain. The lexical translation is done by assigning
values of the matching word in the dictionary. The syntax of the two languages
was realized using Context-Free Grammar, we validated the rewrite rules with
finite state automata. The human evaluation method was used and expert fluency
was scored. The evaluation shows the system performed better than that of
sampled Google translation with over 70 percent of the response matching that
of the system's output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08914">
<div class="article-summary-box-inner">
<span><p>Video-grounded dialogue systems aim to integrate video understanding and
dialogue understanding to generate responses that are relevant to both the
dialogue and video context. Most existing approaches employ deep learning
models and have achieved remarkable performance, given the relatively small
datasets available. However, the results are partly accomplished by exploiting
biases in the datasets rather than developing multimodal reasoning, resulting
in limited generalization. In this paper, we propose a novel approach of
Compositional Counterfactual Contrastive Learning ($C^3$) to develop
contrastive training between factual and counterfactual samples in
video-grounded dialogues. Specifically, we design factual/counterfactual
sampling based on the temporal steps in videos and tokens in dialogues and
propose contrastive loss functions that exploit object-level or action-level
variance. Different from prior approaches, we focus on contrastive hidden state
representations among compositional output tokens to optimize the
representation space in a generation setting. We achieved promising performance
gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the
benefits of our approach in grounding video and dialogue context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14116">
<div class="article-summary-box-inner">
<span><p>We present Claim-Dissector: a novel latent variable model for fact-checking
and analysis, which given a claim and a set of retrieved evidences jointly
learns to identify: (i) the relevant evidences to the given claim, (ii) the
veracity of the claim. We propose to disentangle the per-evidence relevance
probability and its contribution to the final veracity probability in an
interpretable way -- the final veracity probability is proportional to a linear
ensemble of per-evidence relevance probabilities. In this way, the individual
contributions of evidences towards the final predicted probability can be
identified. In per-evidence relevance probability, our model can further
distinguish whether each relevant evidence is supporting (S) or refuting (R)
the claim. This allows to quantify how much the S/R probability contributes to
the final verdict or to detect disagreeing evidence.
</p>
<p>Despite its interpretable nature, our system achieves results competitive
with state-of-the-art on the FEVER dataset, as compared to typical two-stage
system pipelines, while using significantly fewer parameters. It also sets new
state-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis shows
that our model can learn fine-grained relevance cues while using coarse-grained
supervision, and we demonstrate it in 2 ways. (i) We show that our model can
achieve competitive sentence recall while using only paragraph-level relevance
supervision. (ii) Traversing towards the finest granularity of relevance, we
show that our model is capable of identifying relevance at the token level. To
do this, we present a new benchmark TLR-FEVER focusing on token-level
interpretability -- humans annotate tokens in relevant evidences they
considered essential when making their judgment. Then we measure how similar
are these annotations to the tokens our model is focusing on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions. (arXiv:2210.09894v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09894">
<div class="article-summary-box-inner">
<span><p>Abstractive dialogue summarization is to generate a concise and fluent
summary covering the salient information in a dialogue among two or more
interlocutors. It has attracted great attention in recent years based on the
massive emergence of social communication platforms and an urgent requirement
for efficient dialogue information understanding and digestion. Different from
news or articles in traditional document summarization, dialogues bring unique
characteristics and additional challenges, including different language styles
and formats, scattered information, flexible discourse structures and unclear
topic boundaries. This survey provides a comprehensive investigation on
existing work for abstractive dialogue summarization from scenarios, approaches
to evaluations. It categorizes the task into two broad categories according to
the type of input dialogues, i.e., open-domain and task-oriented, and presents
a taxonomy of existing techniques in three directions, namely, injecting
dialogue features, designing auxiliary training tasks and using additional
data.A list of datasets under different scenarios and widely-accepted
evaluation metrics are summarized for completeness. After that, the trends of
scenarios and techniques are summarized, together with deep insights on
correlations between extensively exploited features and different scenarios.
Based on these analyses, we recommend future directions including more
controlled and complicated scenarios, technical innovations and comparisons,
publicly available datasets in special domains, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QAmeleon: Multilingual QA with Only 5 Examples. (arXiv:2211.08264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08264">
<div class="article-summary-box-inner">
<span><p>The availability of large, high-quality datasets has been one of the main
drivers of recent progress in question answering (QA). Such annotated datasets
however are difficult and costly to collect, and rarely exist in languages
other than English, rendering QA technology inaccessible to underrepresented
languages. An alternative to building large monolingual training datasets is to
leverage pre-trained language models (PLMs) under a few-shot learning setting.
Our approach, QAmeleon, uses a PLM to automatically generate multilingual data
upon which QA models are trained, thus avoiding costly annotation. Prompt
tuning the PLM for data synthesis with only five examples per language delivers
accuracy superior to translation-based baselines, bridges nearly 60% of the gap
between an English-only baseline and a fully supervised upper bound trained on
almost 50,000 hand labeled examples, and always leads to substantial
improvements compared to fine-tuning a QA model directly on labeled examples in
low resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show
that few-shot prompt tuning for data synthesis scales across languages and is a
viable alternative to large-scale annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Natural Language Processing for Programming. (arXiv:2212.05773v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05773">
<div class="article-summary-box-inner">
<span><p>Natural language processing for programming aims to use NLP techniques to
assist programming. It is increasingly prevalent for its effectiveness in
improving productivity. Distinct from natural language, a programming language
is highly structured and functional. Constructing a structure-based
representation and a functionality-oriented algorithm is at the heart of
program understanding and generation. In this paper, we conduct a systematic
review covering tasks, datasets, evaluation methods, techniques, and models
from the perspective of the structure-based and functionality-oriented
property, aiming to understand the role of the two properties in each
component. Based on the analysis, we illustrate unexplored areas and suggest
potential directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08283">
<div class="article-summary-box-inner">
<span><p>Most TextVQA approaches focus on the integration of objects, scene texts and
question words by a simple transformer encoder. But this fails to capture the
semantic relations between different modalities. The paper proposes a Scene
Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the
semantic relations among the objects, Optical Character Recognition (OCR)
tokens and the question words. It is achieved by a TextVQA-based scene graph
that discovers the underlying semantics of an image. We created a
guided-attention module to capture the intra-modal interplay between the
language and the vision as a guidance for inter-modal interactions. To make
explicit teaching of the relations between the two modalities, we proposed and
integrated two attention modules, namely a scene graph-based semantic
relation-aware attention and a positional relation-aware attention. We
conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.
It is shown that our SceneGATE method outperformed existing ones because of the
scene graph and its attention modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation. (arXiv:2212.08632v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08632">
<div class="article-summary-box-inner">
<span><p>Multi-modal multi-hop question answering involves answering a question by
reasoning over multiple input sources from different modalities. Existing
methods often retrieve evidences separately and then use a language model to
generate an answer based on the retrieved evidences, and thus do not adequately
connect candidates and are unable to model the interdependent relations during
retrieval. Moreover, the pipelined approaches of retrieval and generation might
result in poor generation performance when retrieval performance is low. To
address these issues, we propose a Structured Knowledge and Unified
Retrieval-Generation (SKURG) approach. SKURG employs an Entity-centered Fusion
Encoder to align sources from different modalities using shared entities. It
then uses a unified Retrieval-Generation Decoder to integrate intermediate
retrieval results for answer generation and also adaptively determine the
number of retrieval steps. Extensive experiments on two representative
multi-modal multi-hop QA datasets MultimodalQA and WebQA demonstrate that SKURG
outperforms the state-of-the-art models in both source retrieval and answer
generation performance with fewer parameters. Our code is available at
https://github.com/HITsz-TMG/SKURG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09597">
<div class="article-summary-box-inner">
<span><p>Reasoning, as an essential ability for complex problem-solving, can provide
back-end support for various real-world applications, such as medical
diagnosis, negotiation, etc. This paper provides a comprehensive survey of
cutting-edge research on reasoning with language model prompting. We introduce
research works with comparisons and summaries and provide systematic resources
to help beginners. We also discuss the potential reasons for emerging such
reasoning abilities and highlight future research directions. Resources are
available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated
periodically).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05880">
<div class="article-summary-box-inner">
<span><p>To facilitate the research on intelligent and human-like chatbots with
multi-modal context, we introduce a new video-based multi-modal dialogue
dataset, called TikTalk. We collect 38K videos from a popular video-sharing
platform, along with 367K conversations posted by users beneath them. Users
engage in spontaneous conversations based on their multi-modal experiences from
watching videos, which helps recreate real-world chitchat context. Compared to
previous multi-modal dialogue datasets, the richer context types in TikTalk
lead to more diverse conversations, but also increase the difficulty in
capturing human interests from intricate multi-modal information to generate
personalized responses. Moreover, external knowledge is more frequently evoked
in our dataset. These facts reveal new challenges for multi-modal dialogue
models. We quantitatively demonstrate the characteristics of TikTalk, propose a
video-based multi-modal chitchat task, and evaluate several dialogue baselines.
Experimental results indicate that the models incorporating large language
models (LLM) can generate more diverse responses, while the model utilizing
knowledge graphs to introduce external knowledge performs the best overall.
Furthermore, no existing model can solve all the above challenges well. There
is still a large room for future improvements, even for LLM with visual
extensions. Our dataset is available at
\url{https://ruc-aimind.github.io/projects/TikTalk/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09656">
<div class="article-summary-box-inner">
<span><p>While a vast collection of explainable AI (XAI) algorithms have been
developed in recent years, they are often criticized for significant gaps with
how humans produce and consume explanations. As a result, current XAI
techniques are often found to be hard to use and lack effectiveness. In this
work, we attempt to close these gaps by making AI explanations selective -- a
fundamental property of human explanations -- by selectively presenting a
subset from a large set of model reasons based on what aligns with the
recipient's preferences. We propose a general framework for generating
selective explanations by leveraging human input on a small sample. This
framework opens up a rich design space that accounts for different selectivity
goals, types of input, and more. As a showcase, we use a decision-support task
to explore selective explanations based on what the decision-maker would
consider relevant to the decision task. We conducted two experimental studies
to examine three out of a broader possible set of paradigms based on our
proposed framework: in Study 1, we ask the participants to provide their own
input to generate selective explanations, with either open-ended or
critique-based input. In Study 2, we show participants selective explanations
based on input from a panel of similar users (annotators). Our experiments
demonstrate the promise of selective explanations in reducing over-reliance on
AI and improving decision outcomes and subjective perceptions of the AI, but
also paint a nuanced picture that attributes some of these positive effects to
the opportunity to provide one's own input to augment AI explanations. Overall,
our work proposes a novel XAI framework inspired by human communication
behaviors and demonstrates its potentials to encourage future work to better
align AI explanations with human production and consumption of explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12324">
<div class="article-summary-box-inner">
<span><p>Good figure captions help paper readers understand complex scientific
figures. Unfortunately, even published papers often have poorly written
captions. Automatic caption generation could aid paper writers by providing
good starting captions that can be refined for better quality. Prior work often
treated figure caption generation as a vision-to-language task. In this paper,
we show that it can be more effectively tackled as a text summarization task in
scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive
summarization model, to specifically summarize figure-referencing paragraphs
(e.g., "Figure 3 shows...") into figure captions. Experiments on large-scale
arXiv figures show that our method outperforms prior vision methods in both
automatic and human evaluations. We further conducted an in-depth investigation
focused on two key challenges: (i) the common presence of low-quality
author-written captions and (ii) the lack of clear standards for good captions.
Our code and data are available at:
https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11403">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have so far impressed the world, with
unprecedented capabilities that emerge in models at large scales. On the vision
side, transformer models (i.e., ViT) are following the same trend, achieving
the best performance on challenging benchmarks. With the abundance of such
unimodal models, a natural question arises; do we need also to follow this
trend to tackle multimodal tasks? In this work, we propose to rather direct
effort to efficient adaptations of existing models, and propose to augment
Language Models with perception. Existing approaches for adapting pretrained
models for vision-language tasks still rely on several key components that
hinder their efficiency. In particular, they still train a large number of
parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)
trained on huge image-text datasets, and add significant inference overhead. In
addition, most of these approaches have focused on Zero-Shot and In Context
Learning, with little to no effort on direct finetuning. We investigate the
minimal computational effort needed to adapt unimodal models for multimodal
tasks and propose a new challenging setup, alongside different approaches, that
efficiently adapts unimodal pretrained models. We show that by freezing more
than 99% of total parameters, training only one linear projection layer, and
prepending only one trainable token, our approach (dubbed eP-ALM) significantly
outperforms other baselines on VQA and Captioning across Image, Video, and
Audio modalities, following the proposed setup. The code is available here:
https://github.com/mshukor/eP-ALM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching. (arXiv:2303.16756v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16756">
<div class="article-summary-box-inner">
<span><p>The process of matching patients with suitable clinical trials is essential
for advancing medical research and providing optimal care. However, current
approaches face challenges such as data standardization, ethical
considerations, and a lack of interoperability between Electronic Health
Records (EHRs) and clinical trial criteria. In this paper, we explore the
potential of large language models (LLMs) to address these challenges by
leveraging their advanced natural language generation capabilities to improve
compatibility between EHRs and clinical trial descriptions. We propose an
innovative privacy-aware data augmentation approach for LLM-based patient-trial
matching (LLM-PTM), which balances the benefits of LLMs while ensuring the
security and confidentiality of sensitive patient data. Our experiments
demonstrate a 7.32% average improvement in performance using the proposed
LLM-PTM method, and the generalizability to new data is improved by 12.12%.
Additionally, we present case studies to further illustrate the effectiveness
of our approach and provide a deeper understanding of its underlying
principles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01008">
<div class="article-summary-box-inner">
<span><p>Multimodal learning, which aims to understand and analyze information from
multiple modalities, has achieved substantial progress in the supervised regime
in recent years. However, the heavy dependence on data paired with expensive
human annotations impedes scaling up models. Meanwhile, given the availability
of large-scale unannotated data in the wild, self-supervised learning has
become an attractive strategy to alleviate the annotation bottleneck. Building
on these two directions, self-supervised multimodal learning (SSML) provides
ways to learn from raw multimodal data. In this survey, we provide a
comprehensive review of the state-of-the-art in SSML, in which we elucidate
three major challenges intrinsic to self-supervised learning with multimodal
data: (1) learning representations from multimodal data without labels, (2)
fusion of different modalities, and (3) learning with unaligned data. We then
detail existing solutions to these challenges. Specifically, we consider (1)
objectives for learning from multimodal unlabeled data via self-supervision,
(2) model architectures from the perspective of different multimodal fusion
strategies, and (3) pair-free learning strategies for coarse-grained and
fine-grained alignment. We also review real-world applications of SSML
algorithms in diverse fields such as healthcare, remote sensing, and machine
translation. Finally, we discuss challenges and future directions for SSML. A
collection of related resources can be found at:
https://github.com/ys-zong/awesome-self-supervised-multimodal-learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13923">
<div class="article-summary-box-inner">
<span><p>With the recent progress in large-scale vision and language representation
learning, Vision Language Pre-training (VLP) models have achieved promising
improvements on various multi-modal downstream tasks. Albeit powerful, these
models have not fully leveraged world knowledge to their advantage. A key
challenge of knowledge-augmented VLP is the lack of clear connections between
knowledge and multi-modal data. Moreover, not all knowledge present in
images/texts is useful, therefore prior approaches often struggle to
effectively integrate knowledge, visual, and textual information. In this
study, we propose REtrieval-based knowledge Augmented Vision Language (REAVL),
a novel knowledge-augmented pre-training framework to address the above issues.
For the first time, we introduce a knowledge-aware self-supervised learning
scheme that efficiently establishes the correspondence between knowledge and
multi-modal data and identifies informative knowledge to improve the modeling
of alignment and interactions between visual and textual modalities. By
adaptively integrating informative knowledge with visual and textual
information, REAVL achieves new state-of-the-art performance uniformly on
knowledge-based vision-language understanding and multi-modal entity linking
tasks, as well as competitive results on general vision-language tasks while
only using 0.2% pre-training data of the best models. Our model shows strong
sample efficiency and effective knowledge utilization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14104">
<div class="article-summary-box-inner">
<span><p>Interactions between humans are diverse and context-dependent, but previous
works have treated them as categorical, disregarding the heavy tail of possible
interactions. We propose a new paradigm of learning human-human interactions as
free text from a single still image, allowing for flexibility in modeling the
unlimited space of situations and relationships between people. To overcome the
absence of data labelled specifically for this task, we use knowledge
distillation applied to synthetic caption data produced by a large language
model without explicit supervision. We show that the pseudo-labels produced by
this procedure can be used to train a captioning model to effectively
understand human-human interactions in images, as measured by a variety of
metrics that measure textual and semantic faithfulness and factual groundedness
of our predictions. We further show that our approach outperforms SOTA image
captioning and situation recognition models on this task. We will release our
code and pseudo-labels along with Waldo and Wenda, a manually-curated test set
for still image human-human interaction understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP. (arXiv:2305.01633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01633">
<div class="article-summary-box-inner">
<span><p>We report our efforts in identifying a set of previous human evaluations in
NLP that would be suitable for a coordinated study examining what makes human
evaluations in NLP more/less reproducible. We present our results and findings,
which include that just 13\% of papers had (i) sufficiently low barriers to
reproduction, and (ii) enough obtainable information, to be considered for
reproduction, and that all but one of the experiments we selected for
reproduction was discovered to have flaws that made the meaningfulness of
conducting a reproduction questionable. As a result, we had to change our
coordinated study design from a reproduce approach to a
standardise-then-reproduce-twice approach. Our overall (negative) finding that
the great majority of human evaluations in NLP is not repeatable and/or not
reproducible and/or too flawed to justify reproduction, paints a dire picture,
but presents an opportunity for a rethink about how to design and report human
evaluations in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02394">
<div class="article-summary-box-inner">
<span><p>Textual backdoor attack, as a novel attack model, has been shown to be
effective in adding a backdoor to the model during training. Defending against
such backdoor attacks has become urgent and important. In this paper, we
propose AttDef, an efficient attribution-based pipeline to defend against two
insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard
the tokens with larger attribution scores as potential triggers since larger
attribution words contribute more to the false prediction results and therefore
are more likely to be poison triggers. Additionally, we further utilize an
external pre-trained language model to distinguish whether input is poisoned or
not. We show that our proposed method can generalize sufficiently well in two
common attack scenarios (poisoning training data and testing data), which
consistently improves previous methods. For instance, AttDef can successfully
mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34%
(3.99% up) under pre-training and post-training attack defense respectively,
achieving the new state-of-the-art performance on prediction recovery over four
benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03960">
<div class="article-summary-box-inner">
<span><p>Process-aware information systems offer extensive advantages to companies,
facilitating planning, operations, and optimization of day-to-day business
activities. However, the time-consuming but required step of designing formal
business process models often hampers the potential of these systems. To
overcome this challenge, automated generation of business process models from
natural language text has emerged as a promising approach to expedite this
step. Generally two crucial subtasks have to be solved: extracting
process-relevant information from natural language and creating the actual
model. Approaches towards the first subtask are rule based methods, highly
optimized for specific domains, but hard to adapt to related applications. To
solve this issue, we present an extension to an existing pipeline, to make it
entirely data driven. We demonstrate the competitiveness of our improved
pipeline, which not only eliminates the substantial overhead associated with
feature engineering and rule definition, but also enables adaptation to
different datasets, entity and relation types, and new domains. Additionally,
the largest available dataset (PET) for the first subtask, contains no
information about linguistic references between mentions of entities in the
process description. Yet, the resolution of these mentions into a single visual
element is essential for high quality process models. We propose an extension
to the PET dataset that incorporates information about linguistic references
and a corresponding method for resolving them. Finally, we provide a detailed
analysis of the inherent challenges in the dataset at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Local Spectro-Temporal Features for Speech Analysis. (arXiv:2305.10270v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10270">
<div class="article-summary-box-inner">
<span><p>We introduce the problem of phone classification in the context of speech
recognition, and explore several sets of local spectro-temporal features that
can be used for phone classification. In particular, we present some
preliminary results for phone classification using two sets of features that
are commonly used for object detection: Haar features and SVM-classified
Histograms of Gradients (HoG).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11473">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently soared in popularity due to their
ease of access and the unprecedented ability to synthesize text responses to
diverse user questions. However, LLMs like ChatGPT present significant
limitations in supporting complex information tasks due to the insufficient
affordances of the text-based medium and linear conversational structure.
Through a formative study with ten participants, we found that LLM interfaces
often present long-winded responses, making it difficult for people to quickly
comprehend and interact flexibly with various pieces of information,
particularly during more complex tasks. We present Graphologue, an interactive
system that converts text-based responses from LLMs into graphical diagrams to
facilitate information-seeking and question-answering tasks. Graphologue
employs novel prompting strategies and interface designs to extract entities
and relationships from LLM responses and constructs node-link diagrams in
real-time. Further, users can interact with the diagrams to flexibly adjust the
graphical presentation and to submit context-specific prompts to obtain more
information. Utilizing diagrams, Graphologue enables graphical, non-linear
dialogues between humans and LLMs, facilitating information exploration,
organization, and comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11769">
<div class="article-summary-box-inner">
<span><p>Large pre-trained multimodal models have demonstrated significant success in
a range of downstream tasks, including image captioning, image-text retrieval,
visual question answering (VQA), etc. However, many of these methods rely on
image-text pairs collected from the web as pre-training data and unfortunately
overlook the need for fine-grained feature alignment between vision and
language modalities, which requires detailed understanding of images and
language expressions. While integrating VQA and dense captioning (DC) into
pre-training can address this issue, acquiring image-question-answer as well as
image-location-caption triplets is challenging and time-consuming.
Additionally, publicly available datasets for VQA and dense captioning are
typically limited in scale due to manual data collection and labeling efforts.
In this paper, we propose a novel method called Joint QA and DC GEneration
(JADE), which utilizes a pre-trained multimodal model and easily-crawled
image-text pairs to automatically generate and filter large-scale VQA and dense
captioning datasets. We apply this method to the Conceptual Caption (CC3M)
dataset to generate a new dataset called CC3M-QA-DC. Experiments show that when
used for pre-training in a multi-task manner, CC3M-QA-DC can improve the
performance with various backbones on various downstream tasks. Furthermore,
our generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,
CC15M) and achieve competitive results compared with models using much more
data. Code and dataset are available at
https://github.com/johncaged/OPT_Questioner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18462">
<div class="article-summary-box-inner">
<span><p>Membership Inference attacks (MIAs) aim to predict whether a data sample was
present in the training data of a machine learning model or not, and are widely
used for assessing the privacy risks of language models. Most existing attacks
rely on the observation that models tend to assign higher probabilities to
their training samples than non-training points. However, simple thresholding
of the model score in isolation tends to lead to high false-positive rates as
it does not account for the intrinsic complexity of a sample. Recent work has
demonstrated that reference-based attacks which compare model scores to those
obtained from a reference model trained on similar data can substantially
improve the performance of MIAs. However, in order to train reference models,
attacks of this kind make the strong and arguably unrealistic assumption that
an adversary has access to samples closely resembling the original training
data. Therefore, we investigate their performance in more realistic scenarios
and find that they are highly fragile in relation to the data distribution used
to train reference models. To investigate whether this fragility provides a
layer of safety, we propose and evaluate neighbourhood attacks, which compare
model scores for a given sample to scores of synthetically generated neighbour
texts and therefore eliminate the need for access to the training data
distribution. We show that, in addition to being competitive with
reference-based attacks that have perfect knowledge about the training data
distribution, our attack clearly outperforms existing reference-free attacks as
well as reference-based attacks with imperfect knowledge, which demonstrates
the need for a reevaluation of the threat model of adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference. (arXiv:2306.01153v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01153">
<div class="article-summary-box-inner">
<span><p>The capability to generate responses with diversity and faithfulness using
factual knowledge is paramount for creating a human-like, trustworthy dialogue
system. Common strategies either adopt a two-step paradigm, which optimizes
knowledge selection and response generation separately, and may overlook the
inherent correlation between these two tasks, or leverage conditional
variational method to jointly optimize knowledge selection and response
generation by employing an inference network. In this paper, we present an
end-to-end learning framework, termed Sequential Posterior Inference (SPI),
capable of selecting knowledge and generating dialogues by approximately
sampling from the posterior distribution. Unlike other methods, SPI does not
require the inference network or assume a simple geometry of the posterior
distribution. This straightforward and intuitive inference procedure of SPI
directly queries the response generation model, allowing for accurate knowledge
selection and generation of faithful responses. In addition to modeling
contributions, our experimental results on two common dialogue datasets (Wizard
of Wikipedia and Holl-E) demonstrate that SPI outperforms previous strong
baselines according to both automatic and human evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One model to rule them all: ranking Slovene summarizers. (arXiv:2306.11518v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11518">
<div class="article-summary-box-inner">
<span><p>Text summarization is an essential task in natural language processing, and
researchers have developed various approaches over the years, ranging from
rule-based systems to neural networks. However, there is no single model or
approach that performs well on every type of text. We propose a system that
recommends the most suitable summarization model for a given text. The proposed
system employs a fully connected neural network that analyzes the input content
and predicts which summarizer should score the best in terms of ROUGE score for
a given input. The meta-model selects among four different summarization
models, developed for the Slovene language, using different properties of the
input, in particular its Doc2Vec document representation. The four Slovene
summarization models deal with different challenges associated with text
summarization in a less-resourced language. We evaluate the proposed SloMetaSum
model performance automatically and parts of it manually. The results show that
the system successfully automates the step of manually selecting the best
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00925">
<div class="article-summary-box-inner">
<span><p>Semantic similarity measures are widely used in natural language processing
to catalyze various computer-related tasks. However, no single semantic
similarity measure is the most appropriate for all tasks, and researchers often
use ensemble strategies to ensure performance. This research work proposes a
method for automatically designing semantic similarity ensembles. In fact, our
proposed method uses grammatical evolution, for the first time, to
automatically select and aggregate measures from a pool of candidates to create
an ensemble that maximizes correlation to human judgment. The method is
evaluated on several benchmark datasets and compared to state-of-the-art
ensembles, showing that it can significantly improve similarity assessment
accuracy and outperform existing methods in some cases. As a result, our
research demonstrates the potential of using grammatical evolution to
automatically compare text and prove the benefits of using ensembles for
semantic similarity tasks. The source code that illustrates our approach can be
downloaded from https://github.com/jorge-martinez-gil/sesige.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02046">
<div class="article-summary-box-inner">
<span><p>With the prosperity of e-commerce and web applications, Recommender Systems
(RecSys) have become an important component of our daily life, providing
personalized suggestions that cater to user preferences. While Deep Neural
Networks (DNNs) have made significant advancements in enhancing recommender
systems by modeling user-item interactions and incorporating textual side
information, DNN-based methods still face limitations, such as difficulties in
understanding users' interests and capturing textual side information,
inabilities in generalizing to various recommendation scenarios and reasoning
on their predictions, etc. Meanwhile, the emergence of Large Language Models
(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural
Language Processing (NLP) and Artificial Intelligence (AI), due to their
remarkable abilities in fundamental responsibilities of language understanding
and generation, as well as impressive generalization and reasoning
capabilities. As a result, recent studies have attempted to harness the power
of LLMs to enhance recommender systems. Given the rapid evolution of this
research direction in recommender systems, there is a pressing need for a
systematic overview that summarizes existing LLM-empowered recommender systems,
to provide researchers in relevant fields with an in-depth understanding.
Therefore, in this paper, we conduct a comprehensive review of LLM-empowered
recommender systems from various aspects including Pre-training, Fine-tuning,
and Prompting. More specifically, we first introduce representative methods to
harness the power of LLMs (as a feature encoder) for learning representations
of users and items. Then, we review recent techniques of LLMs for enhancing
recommender systems from three paradigms, namely pre-training, fine-tuning, and
prompting. Finally, we comprehensively discuss future directions in this
emerging field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAME: Confidence-guided Adaptive Memory Efficient Optimization. (arXiv:2307.02047v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02047">
<div class="article-summary-box-inner">
<span><p>Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent
performance in the training of large language models. Nevertheless, the need
for adaptivity requires maintaining second-moment estimates of the
per-parameter gradients, which entails a high cost of extra memory overheads.
To solve this problem, several memory-efficient optimizers (e.g., Adafactor)
have been proposed to obtain a drastic reduction in auxiliary memory usage, but
with a performance penalty. In this paper, we first study a confidence-guided
strategy to reduce the instability of existing memory efficient optimizers.
Based on this strategy, we propose CAME to simultaneously achieve two goals:
fast convergence as in traditional adaptive methods, and low memory usage as in
memory-efficient methods. Extensive experiments demonstrate the training
stability and superior performance of CAME across various NLP tasks such as
BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size
of 32,768, our proposed optimizer attains faster convergence and higher
accuracy compared with the Adam optimizer. The implementation of CAME is
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08941">
<div class="article-summary-box-inner">
<span><p>Fine-tuning a pre-trained language model (PLM) emerges as the predominant
strategy in many natural language processing applications. However, even
fine-tuning the PLMs and doing inference are expensive, especially on edge
devices with low computing power. Some general approaches (e.g. quantization
and distillation) have been widely studied to reduce the compute/memory of PLM
fine-tuning, while very few one-shot compression techniques are explored. In
this paper, we investigate the neural tangent kernel (NTK)--which reveals the
gradient descent dynamics of neural networks--of the multilayer perceptrons
(MLP) modules in a PLM and propose to coin a lightweight PLM through
NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a
bundle of sub-MLPs, and cluster them into a given number of centroids, which
can then be restored as a compressed MLP and surprisingly shown to well
approximate the NTK of the original PLM. Extensive experiments of PLM
fine-tuning on both natural language understanding (NLU) and generation (NLG)
tasks are provided to verify the effectiveness of the proposed method MLP
fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Pre-trained Language Models' Generalization. (arXiv:2307.10457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10457">
<div class="article-summary-box-inner">
<span><p>The reusability of state-of-the-art Pre-trained Language Models (PLMs) is
often limited by their generalization problem, where their performance
drastically decreases when evaluated on examples that differ from the training
dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation
arises from PLMs' reliance on spurious correlations, which work well for
frequent example types but not for general examples. To address this issue, we
propose a training approach called Mask-tuning, which integrates Masked
Language Modeling (MLM) training objectives into the fine-tuning process to
enhance PLMs' generalization. Comprehensive experiments demonstrate that
Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs'
generalization on OOD datasets while improving their performance on
in-distribution datasets. The findings suggest that Mask-tuning improves the
reusability of PLMs on unseen data, making them more practical and effective
for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Debiasing for Multimodal Sentiment Analysis. (arXiv:2307.10511v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10511">
<div class="article-summary-box-inner">
<span><p>Existing work on Multimodal Sentiment Analysis (MSA) utilizes multimodal
information for prediction yet unavoidably suffers from fitting the spurious
correlations between multimodal features and sentiment labels. For example, if
most videos with a blue background have positive labels in a dataset, the model
will rely on such correlations for prediction, while "blue background" is not a
sentiment-related feature. To address this problem, we define a general
debiasing MSA task, which aims to enhance the Out-Of-Distribution (OOD)
generalization ability of MSA models by reducing their reliance on spurious
correlations. To this end, we propose a general debiasing framework based on
Inverse Probability Weighting (IPW), which adaptively assigns small weights to
the samples with larger bias (i.e., the severer spurious correlations). The key
to this debiasing framework is to estimate the bias of each sample, which is
achieved by two steps: 1) disentangling the robust features and biased features
in each modality, and 2) utilizing the biased features to estimate the bias.
Finally, we employ IPW to reduce the effects of large-biased samples,
facilitating robust feature learning for sentiment prediction. To examine the
model's generalization ability, we keep the original testing sets on two
benchmarks and additionally construct multiple unimodal and multimodal OOD
testing sets. The empirical results demonstrate the superior generalization
ability of our proposed framework. We have released the code and data to
facilitate the reproduction https://github.com/Teng-Sun/GEAR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIST: Generating Image-Specific Text for Fine-grained Object Classification. (arXiv:2307.11315v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11315">
<div class="article-summary-box-inner">
<span><p>Recent vision-language models outperform vision-only models on many image
classification tasks. However, because of the absence of paired text/image
descriptions, it remains difficult to fine-tune these models for fine-grained
image classification. In this work, we propose a method, GIST, for generating
image-specific fine-grained text descriptions from image-only datasets, and
show that these text descriptions can be used to improve classification. Key
parts of our method include 1. prompting a pretrained large language model with
domain-specific prompts to generate diverse fine-grained text descriptions for
each class and 2. using a pretrained vision-language model to match each image
to label-preserving text descriptions that capture relevant visual features in
the image. We demonstrate the utility of GIST by fine-tuning vision-language
models on the image-and-generated-text pairs to learn an aligned
vision-language representation space for improved classification. We evaluate
our learned representation space in full-shot and few-shot scenarios across
four diverse fine-grained classification datasets, each from a different
domain. Our method achieves an average improvement of $4.1\%$ in accuracy over
CLIP linear probes and an average of $1.1\%$ improvement in accuracy over the
previous state-of-the-art image-text classification method on the full-shot
datasets. Our method achieves similar improvements across few-shot regimes.
Code is available at https://github.com/emu1729/GIST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.12375">
<div class="article-summary-box-inner">
<span><p>The performance of Large Language Models (LLMs) on downstream tasks often
improves significantly when including examples of the input-label relationship
in the context. However, there is currently no consensus about how this
in-context learning (ICL) ability of LLMs works: for example, while Xie et al.
(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b)
argue ICL does not even learn label relationships from in-context examples. In
this paper, we study (1) how labels of in-context examples affect predictions,
(2) how label relationships learned during pre-training interact with
input-label examples provided in-context, and (3) how ICL aggregates label
information across in-context examples. Our findings suggests LLMs usually
incorporate information from in-context labels, but that pre-training and
in-context label relationships are treated differently, and that the model does
not consider all in-context information equally. Our results give insights into
understanding and aligning LLM behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.14361">
<div class="article-summary-box-inner">
<span><p>This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and
GloVe to classify gene mutations using Kaggle's Personalized Medicine:
Redefining Cancer Treatment dataset. The results were compared against
well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and
their LSTM ensembles. Our model outperformed all other models in terms of
accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it
also needed less training time, resulting in a perfect combination of
performance and efficiency. This study demonstrates the utility of ensemble
models for difficult tasks such as gene mutation classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00121">
<div class="article-summary-box-inner">
<span><p>The field of software security testing, more specifically penetration
testing, is an activity that requires high levels of expertise and involves
many manual testing and analysis steps. This paper explores the potential usage
of large-language models, such as GPT3.5, to augment penetration testers with
AI sparring partners. We explore the feasibility of supplementing penetration
testers with AI models for two distinct use cases: high-level task planning for
security testing assignments and low-level vulnerability hunting within a
vulnerable virtual machine. For the latter, we implemented a closed-feedback
loop between LLM-generated low-level actions with a vulnerable virtual machine
(connected through SSH) and allowed the LLM to analyze the machine state for
vulnerabilities and suggest concrete attack vectors which were automatically
executed within the virtual machine. We discuss promising initial results,
detail avenues for improvement, and close deliberating on the ethics of
providing AI-based sparring partners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02180">
<div class="article-summary-box-inner">
<span><p>Clinical trial matching is a key process in health delivery and discovery. In
practice, it is plagued by overwhelming unstructured data and unscalable manual
processing. In this paper, we conduct a systematic study on scaling clinical
trial matching using large language models (LLMs), with oncology as the focus
area. Our study is grounded in a clinical trial matching system currently in
test deployment at a large U.S. health network. Initial findings are promising:
out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate
eligibility criteria of clinical trials and extract complex matching logic
(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially
outperform prior strong baselines and may serve as a preliminary solution to
help triage patient-trial candidates with humans in the loop. Our study also
reveals a few significant growth areas for applying LLMs to end-to-end clinical
trial matching, such as context limitation and accuracy, especially in
structuring patient information from longitudinal medical records.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-08 23:11:01.956730074 UTC">2023-08-08 23:11:01 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>