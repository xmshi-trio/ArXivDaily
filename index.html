<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-13T01:30:00Z">12-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug-and-Play Recipe Generation with Content Planning. (arXiv:2212.05093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05093">
<div class="article-summary-box-inner">
<span><p>Recent pre-trained language models have shown promising capabilities in
generating fluent and realistic natural language text. However, generating
multi-sentence text with global content planning has been a long-existing
research question. Current approaches for controlled text generation can hardly
address this issue, as they usually condition on single known control
attributes. In this study, we propose a low-cost yet effective framework which
explicitly models the global content plan of the generated text. Specifically,
it optimizes the joint distribution of the natural language sequence and the
global content plan in a plug-and-play manner. We conduct extensive experiments
on the well-established Recipe1M+ benchmark. Both automatic and human
evaluations verify that our model achieves the state-of-the-art performance on
the task of recipe generation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Learning for Personal Health Mention Detection on Social Media. (arXiv:2212.05147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05147">
<div class="article-summary-box-inner">
<span><p>Detecting personal health mentions on social media is essential to complement
existing health surveillance systems. However, annotating data for detecting
health mentions at a large scale is a challenging task. This research employs a
multitask learning framework to leverage available annotated data from a
related task to improve the performance on the main task to detect personal
health experiences mentioned in social media texts. Specifically, we focus on
incorporating emotional information into our target task by using emotion
detection as an auxiliary task. Our approach significantly improves a wide
range of personal health mention detection tasks compared to a strong
state-of-the-art baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Text Detection with Multiple Training Strategies. (arXiv:2212.05194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05194">
<div class="article-summary-box-inner">
<span><p>As the deep learning rapidly promote, the artificial texts created by
generative models are commonly used in news and social media. However, such
models can be abused to generate product reviews, fake news, and even fake
political content. The paper proposes a solution for the Russian Artificial
Text Detection in the Dialogue shared task 2022 (RuATD 2022) to distinguish
which model within the list is used to generate this text. We introduce the
DeBERTa pre-trained language model with multiple training strategies for this
shared task. Extensive experiments conducted on the RuATD dataset validate the
effectiveness of our proposed method. Moreover, our submission ranked second
place in the evaluation phase for RuATD 2022 (Multi-Class).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine intuition: Uncovering human-like intuitive decision-making in GPT-3.5. (arXiv:2212.05206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05206">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) technologies revolutionize vast fields of
society. Humans using these systems are likely to expect them to work in a
potentially hyperrational manner. However, in this study, we show that some AI
systems, namely large language models (LLMs), exhibit behavior that strikingly
resembles human-like intuition - and the many cognitive errors that come with
them. We use a state-of-the-art LLM, namely the latest iteration of OpenAI's
Generative Pre-trained Transformer (GPT-3.5), and probe it with the Cognitive
Reflection Test (CRT) as well as semantic illusions that were originally
designed to investigate intuitive decision-making in humans. Our results show
that GPT-3.5 systematically exhibits "machine intuition," meaning that it
produces incorrect responses that are surprisingly equal to how humans respond
to the CRT as well as to semantic illusions. We investigate several approaches
to test how sturdy GPT-3.5's inclination for intuitive-like decision-making is.
Our study demonstrates that investigating LLMs with methods from cognitive
science has the potential to reveal emergent traits and adjust expectations
regarding their machine behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEAD: Liberal Feature-based Distillation for Dense Retrieval. (arXiv:2212.05225v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05225">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is often used to transfer knowledge from a strong
teacher model to a relatively weak student model. Traditional knowledge
distillation methods include response-based methods and feature-based methods.
Response-based methods are used the most widely but suffer from lower upper
limit of model performance, while feature-based methods have constraints on the
vocabularies and tokenizers. In this paper, we propose a tokenizer-free method
liberal feature-based distillation (LEAD). LEAD aligns the distribution between
teacher model and student model, which is effective, extendable, portable and
has no requirements on vocabularies, tokenizer, or model architecture.
Extensive experiments show the effectiveness of LEAD on several widely-used
benchmarks, including MS MARCO Passage, TREC Passage 19, TREC Passage 20, MS
MARCO Document, TREC Document 19 and TREC Document 20.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured information extraction from complex scientific text with fine-tuned large language models. (arXiv:2212.05238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05238">
<div class="article-summary-box-inner">
<span><p>Intelligently extracting and linking complex scientific information from
unstructured text is a challenging endeavor particularly for those
inexperienced with natural language processing. Here, we present a simple
sequence-to-sequence approach to joint named entity recognition and relation
extraction for complex hierarchical information in scientific text. The
approach leverages a pre-trained large language model (LLM), GPT-3, that is
fine-tuned on approximately 500 pairs of prompts (inputs) and completions
(outputs). Information is extracted either from single sentences or across
sentences in abstracts/passages, and the output can be returned as simple
English sentences or a more structured format, such as a list of JSON objects.
We demonstrate that LLMs trained in this way are capable of accurately
extracting useful records of complex scientific knowledge for three
representative tasks in materials chemistry: linking dopants with their host
materials, cataloging metal-organic frameworks, and general
chemistry/phase/morphology/application information extraction. This approach
represents a simple, accessible, and highly-flexible route to obtaining large
databases of structured knowledge extracted from unstructured text. An online
demo is available at <a href="http://www.matscholar.com/info-extraction.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Knowledge Graph Service for Developing Domain Language Models in AI Software. (arXiv:2212.05251v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05251">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) is one of the core techniques in AI
software. As AI is being applied to more and more domains, how to efficiently
develop high-quality domain-specific language models becomes a critical
question in AI software engineering. Existing domain-specific language model
development processes mostly focus on learning a domain-specific pre-trained
language model (PLM); when training the domain task-specific language model
based on PLM, only a direct (and often unsatisfactory) fine-tuning strategy is
adopted commonly. By enhancing the task-specific training procedure with domain
knowledge graphs, we propose KnowledgeDA, a unified and low-code domain
language model development service. Given domain-specific task texts input by a
user, KnowledgeDA can automatically generate a domain-specific language model
following three steps: (i) localize domain knowledge entities in texts via an
embedding-similarity approach; (ii) generate augmented samples by retrieving
replaceable domain entity pairs from two views of both knowledge graph and
training data; (iii) select high-quality augmented samples for fine-tuning via
confidence-based assessment. We implement a prototype of KnowledgeDA to learn
language models for two domains, healthcare and software development.
Experiments on five domain-specific NLP tasks verify the effectiveness and
generalizability of KnowledgeDA. (Code is publicly available at
https://github.com/RuiqingDing/KnowledgeDA.)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAPS-KB: A Million-scale Probabilistic Simile Knowledge Base. (arXiv:2212.05254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05254">
<div class="article-summary-box-inner">
<span><p>The ability to understand and generate similes is an imperative step to
realize human-level AI. However, there is still a considerable gap between
machine intelligence and human cognition in similes, since deep models based on
statistical distribution tend to favour high-frequency similes. Hence, a
large-scale symbolic knowledge base of similes is required, as it contributes
to the modeling of diverse yet unpopular similes while facilitating additional
evaluation and reasoning. To bridge the gap, we propose a novel framework for
large-scale simile knowledge base construction, as well as two probabilistic
metrics which enable an improved understanding of simile phenomena in natural
language. Overall, we construct MAPS-KB, a million-scale probabilistic simile
knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB
corpora. We conduct sufficient experiments to justify the effectiveness and
necessity of the methods of our framework. We also apply MAPS-KB on three
downstream tasks to achieve state-of-the-art performance, further demonstrating
the value of MAPS-KB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification. (arXiv:2212.05276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05276">
<div class="article-summary-box-inner">
<span><p>A key component of fact verification is thevevidence retrieval, often from
multiple documents. Recent approaches use dense representations and condition
the retrieval of each document on the previously retrieved ones. The latter
step is performed over all the documents in the collection, requiring storing
their dense representations in an index, thus incurring a high memory
footprint. An alternative paradigm is retrieve-and-rerank, where documents are
retrieved using methods such as BM25, their sentences are reranked, and further
documents are retrieved conditioned on these sentences, reducing the memory
requirements. However, such approaches can be brittle as they rely on
heuristics and assume hyperlinks between documents. We propose a novel
retrieve-and-rerank method for multi-hop retrieval, that consists of a
retriever that jointly scores documents in the knowledge source and sentences
from previously retrieved documents using an autoregressive formulation and is
guided by a proof system based on natural logic that dynamically terminates the
retrieval process if the evidence is deemed sufficient. This method is
competitive with current state-of-the-art methods on FEVER, HoVer and
FEVEROUS-S, while using $5$ to $10$ times less memory than competing systems.
Evaluation on an adversarial dataset indicates improved stability of our
approach compared to commonly deployed threshold-based methods. Finally, the
proof system helps humans predict model decisions correctly more often than
using the evidence alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying the Source of Vulnerability in Explanation Discrepancy: A Case Study in Neural Text Classification. (arXiv:2212.05327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05327">
<div class="article-summary-box-inner">
<span><p>Some recent works observed the instability of post-hoc explanations when
input side perturbations are applied to the model. This raises the interest and
concern in the stability of post-hoc explanations. However, the remaining
question is: is the instability caused by the neural network model or the
post-hoc explanation method? This work explores the potential source that leads
to unstable post-hoc explanations. To separate the influence from the model, we
propose a simple output probability perturbation method. Compared to prior
input side perturbation methods, the output probability perturbation method can
circumvent the neural model's potential effect on the explanations and allow
the analysis on the explanation method. We evaluate the proposed method with
three widely-used post-hoc explanation methods (LIME (Ribeiro et al., 2016),
Kernel Shapley (Lundberg and Lee, 2017a), and Sample Shapley (Strumbelj and
Kononenko, 2010)). The results demonstrate that the post-hoc methods are
stable, barely producing discrepant explanations under output probability
perturbations. The observation suggests that neural network models may be the
primary source of fragile explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin. (arXiv:2212.05356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05356">
<div class="article-summary-box-inner">
<span><p>This paper presents the work of restoring punctuation for ASR transcripts
generated by multilingual ASR systems. The focus languages are English,
Mandarin, and Malay which are three of the most popular languages in Singapore.
To the best of our knowledge, this is the first system that can tackle
punctuation restoration for these three languages simultaneously. Traditional
approaches usually treat the task as a sequential labeling task, however, this
work adopts a slot-filling approach that predicts the presence and type of
punctuation marks at each word boundary. The approach is similar to the
Masked-Language Model approach employed during the pre-training stages of BERT,
but instead of predicting the masked word, our model predicts masked
punctuation. Additionally, we find that using Jieba1 instead of only using the
built-in SentencePiece tokenizer of XLM-R can significantly improve the
performance of punctuating Mandarin transcripts. Experimental results on
English and Mandarin IWSLT2022 datasets and Malay News show that the proposed
approach achieved state-of-the-art results for Mandarin with 73.8% F1-score
while maintaining a reasonable F1-score for English and Malay, i.e. 74.7% and
78% respectively. Our source code that allows reproducing the results and
building a simple web-based application for demonstration purposes is available
on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Response Generation in Task-Oriented Dialogue with Unstructured Knowledge Access. (arXiv:2212.05373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05373">
<div class="article-summary-box-inner">
<span><p>To alleviate the problem of structured databases' limited coverage, recent
task-oriented dialogue systems incorporate external unstructured knowledge to
guide the generation of system responses. However, these usually use word or
sentence level similarities to detect the relevant knowledge context, which
only partially capture the topical level relevance. In this paper, we examine
how to better integrate topical information in knowledge grounded task-oriented
dialogue and propose ``Topic-Aware Response Generation'' (TARG), an end-to-end
response generation model. TARG incorporates multiple topic-aware attention
mechanisms to derive the importance weighting scheme over dialogue utterances
and external knowledge sources towards a better understanding of the dialogue
history. Experimental results indicate that TARG achieves state-of-the-art
performance in knowledge selection and response generation, outperforming
previous state-of-the-art by 3.2, 3.6, and 4.2 points in EM, F1 and BLEU-4
respectively on Doc2Dial, and performing comparably with previous work on
DSTC9; both being knowledge-grounded task-oriented dialogue datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages. (arXiv:2212.05409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05409">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce IndicXTREME, a benchmark consisting of nine
diverse tasks covering 18 languages from the Indic sub-continent belonging to
four different families. Across languages and tasks, IndicXTREME contains a
total of 103 evaluation sets, of which 51 are new contributions to the
literature. To maintain high quality, we only use human annotators to curate or
translate\footnote{for IndicXParaphrase, where an automatic translation system
is used, a second human verification and correction step is done.} our
datasets. To the best of our knowledge, this is the first effort toward
creating a standard benchmark for Indic languages that aims to test the
zero-shot capabilities of pretrained language models. We also release IndicCorp
v2, an updated and much larger version of IndicCorp that contains 20.9 billion
tokens in 24 languages. We pretrain IndicBERT v2 on IndicCorp v2 and evaluate
it on IndicXTREME to show that it outperforms existing multilingual language
models such as XLM-R and MuRIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-Level Debiased Natural Language Understanding. (arXiv:2212.05421v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05421">
<div class="article-summary-box-inner">
<span><p>Existing natural language understanding (NLU) models often rely on dataset
biases rather than intended task-relevant features to achieve high performance
on specific datasets. As a result, these models perform poorly on datasets
outside the training distribution. Some recent studies address the above issue
by reducing the weights of biased samples during the training process. However,
these methods still encode biased latent features in representations and
neglect the dynamic nature of bias, which hinders model prediction. We propose
an NLU debiasing method, named debiasing contrastive learning (DCT), to
simultaneously alleviate the above problems based on contrastive learning. We
devise a debiasing positive sampling strategy to mitigate biased latent
features by selecting the least similar biased positive samples. We also
propose a dynamic negative sampling strategy to capture the dynamic influence
of biases by employing a bias-only model to dynamically select the most similar
biased negative samples. We conduct experiments on three NLU benchmark
datasets. Experimental results show that DCT outperforms state-of-the-art
baselines on out-of-distribution datasets while maintaining in-distribution
performance. We also verify that DCT can reduce biased latent features from the
model's representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MORTY: Structured Summarization for Targeted Information Extraction from Scholarly Articles. (arXiv:2212.05429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05429">
<div class="article-summary-box-inner">
<span><p>Information extraction from scholarly articles is a challenging task due to
the sizable document length and implicit information hidden in text, figures,
and citations. Scholarly information extraction has various applications in
exploration, archival, and curation services for digital libraries and
knowledge management systems. We present MORTY, an information extraction
technique that creates structured summaries of text from scholarly articles.
Our approach condenses the article's full-text to property-value pairs as a
segmented text snippet called structured summary. We also present a sizable
scholarly dataset combining structured summaries retrieved from a scholarly
knowledge graph and corresponding publicly available scientific articles, which
we openly publish as a resource for the research community. Our results show
that structured summarization is a suitable approach for targeted information
extraction that complements other commonly used methods such as question
answering and named entity recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Speech Translation of Arabic to English Broadcast News. (arXiv:2212.05479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05479">
<div class="article-summary-box-inner">
<span><p>Speech translation (ST) is the task of directly translating acoustic speech
signals in a source language into text in a foreign language. ST task has been
addressed, for a long time, using a pipeline approach with two modules : first
an Automatic Speech Recognition (ASR) in the source language followed by a
text-to-text Machine translation (MT). In the past few years, we have seen a
paradigm shift towards the end-to-end approaches using sequence-to-sequence
deep neural network models. This paper presents our efforts towards the
development of the first Broadcast News end-to-end Arabic to English speech
translation system. Starting from independent ASR and MT LDC releases, we were
able to identify about 92 hours of Arabic audio recordings for which the manual
transcription was also translated into English at the segment level. These data
was used to train and compare pipeline and end-to-end speech translation
systems under multiple scenarios including transfer learning and data
augmentation techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification. (arXiv:2212.05506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05506">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised text classification aims to train a classifier using only
class descriptions and unlabeled data. Recent research shows that
keyword-driven methods can achieve state-of-the-art performance on various
tasks. However, these methods not only rely on carefully-crafted class
descriptions to obtain class-specific keywords but also require substantial
amount of unlabeled data and takes a long time to train. This paper proposes
FastClass, an efficient weakly-supervised classification approach. It uses
dense text representation to retrieve class-relevant documents from external
unlabeled corpus and selects an optimal subset to train a classifier. Compared
to keyword-driven methods, our approach is less reliant on initial class
descriptions as it no longer needs to expand each class description into a set
of class-specific keywords. Experiments on a wide range of classification tasks
show that the proposed approach frequently outperforms keyword-driven models in
terms of classification accuracy and often enjoys orders-of-magnitude faster
training speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images. (arXiv:2212.05525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05525">
<div class="article-summary-box-inner">
<span><p>Digitization of scanned receipts aims to extract text from receipt images and
save it into structured documents. This is usually split into two sub-tasks:
text localization and optical character recognition (OCR). Most existing OCR
models only focus on the cropped text instance images, which require the
bounding box information provided by a text region detection model. Introducing
an additional detector to identify the text instance images in advance is
inefficient, however instance-level OCR models have very low accuracy when
processing the whole image for the document-level OCR, such as receipt images
containing multiple text lines arranged in various layouts. To this end, we
propose a localization-free document-level OCR model for transcribing all the
characters in a receipt image into an ordered sequence end-to-end.
Specifically, we finetune the pretrained Transformer-based instance-level model
TrOCR with randomly cropped image chunks, and gradually increase the image
chunk size to generalize the recognition ability from instance images to
full-page images. In our experiments on the SROIE receipt OCR dataset, the
model finetuned with our strategy achieved 64.4 F1-score and a 22.8% character
error rates (CER) on the word-level and character-level metrics, respectively,
which outperforms the baseline results with 48.5 F1-score and 50.6% CER. The
best model, which splits the full image into 15 equally sized chunks, gives
87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of
the output. Moreover, the characters in the generated document-level sequences
are arranged in the reading order, which is practical for real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health and Suicide Death among US Veterans. (arXiv:2212.05546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05546">
<div class="article-summary-box-inner">
<span><p>Importance: Social determinants of health (SDOH) are known to be associated
with increased risk of suicidal behaviors, but few studies utilized SDOH from
unstructured electronic health record (EHR) notes. Objective: To investigate
associations between suicide and recent SDOH, identified using structured and
unstructured data.
</p>
<p>Design: Nested case-control study.
</p>
<p>Setting: EHR data from the US Veterans Health Administration (VHA).
</p>
<p>Participants: 6,122,785 Veterans who received care in the US VHA between
October 1, 2010, and September 30, 2015.
</p>
<p>Exposures: Occurrence of SDOH over a maximum span of two years compared with
no occurrence of SDOH.
</p>
<p>Main Outcomes and Measures: Cases of suicide deaths were matched with 4
controls on birth year, cohort entry date, sex, and duration of follow-up. We
developed an NLP system to extract SDOH from unstructured notes. Structured
data, NLP on unstructured data, and combining them yielded seven, eight and
nine SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence
intervals (CIs) were estimated using conditional logistic regression.
</p>
<p>Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382
person-years of follow-up (incidence rate 37.18 /100,000 person-years). Our
cohort was mostly male (92.23%) and white (76.99%). Across the six common SDOH
as covariates, NLP-extracted SDOH, on average, covered 84.38% of all SDOH
occurrences. All SDOH, measured by structured data and NLP, were significantly
associated with increased risk of suicide. The SDOH with the largest effects
was legal problems (aOR=2.67, 95% CI=2.46-2.89), followed by violence
(aOR=2.26, 95% CI=2.11-2.43). NLP-extracted and structured SDOH were also
associated with suicide.
</p>
<p>Conclusions and Relevance: NLP-extracted SDOH were always significantly
associated with increased risk of suicide among Veterans, suggesting the
potential of NLP in public health studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05612">
<div class="article-summary-box-inner">
<span><p>Warning: this paper contains content that may be offensive or upsetting. In
the current context where online platforms have been effectively weaponized in
a variety of geo-political events and social issues, Internet memes make fair
content moderation at scale even more difficult. Existing work on meme
classification and tracking has focused on black-box methods that do not
explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Slang Representation Methods. (arXiv:2212.05613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05613">
<div class="article-summary-box-inner">
<span><p>Warning: this paper contains content that may be offensive or upsetting.
Considering the large amount of content created online by the minute,
slang-aware automatic tools are critically needed to promote social good, and
assist policymakers and moderators in restricting the spread of offensive
language, abuse, and hate speech. Despite the success of large language models
and the spontaneous emergence of slang dictionaries, it is unclear how far
their combination goes in terms of slang understanding for downstream social
good tasks. In this paper, we provide a framework to study different
combinations of representation learning models and knowledge resources for a
variety of downstream tasks that rely on slang understanding. Our experiments
show the superiority of models that have been pre-trained on social media data,
while the impact of dictionaries is positive only for static word embeddings.
Our error analysis identifies core challenges for slang representation
learning, including out-of-vocabulary words, polysemy, variance, and annotation
disagreements, which can be traced to characteristics of slang as a quickly
evolving and highly subjective language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensembling Transformers for Cross-domain Automatic Term Extraction. (arXiv:2212.05696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05696">
<div class="article-summary-box-inner">
<span><p>Automatic term extraction plays an essential role in domain language
understanding and several natural language processing downstream tasks. In this
paper, we propose a comparative study on the predictive power of
Transformers-based pretrained language models toward term extraction in a
multi-language cross-domain setting. Besides evaluating the ability of
monolingual models to extract single- and multi-word terms, we also experiment
with ensembles of mono- and multilingual models by conducting the intersection
or union on the term output sets of different language models. Our experiments
have been conducted on the ACTER corpus covering four specialized domains
(Corruption, Wind energy, Equitation, and Heart failure) and three languages
(English, French, and Dutch), and on the RSDO5 Slovenian corpus covering four
additional domains (Biomechanics, Chemistry, Veterinary, and Linguistics). The
results show that the strategy of employing monolingual models outperforms the
state-of-the-art approaches from the related work leveraging multilingual
models, regarding all the languages except Dutch and French if the term
extraction task excludes the extraction of named entity terms. Furthermore, by
combining the outputs of the two best performing models, we achieve significant
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implementing Deep Learning-Based Approaches for Article Summarization in Indian Languages. (arXiv:2212.05702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05702">
<div class="article-summary-box-inner">
<span><p>The research on text summarization for low-resource Indian languages has been
limited due to the availability of relevant datasets. This paper presents a
summary of various deep-learning approaches used for the ILSUM 2022 Indic
language summarization datasets. The ISUM 2022 dataset consists of news
articles written in Indian English, Hindi, and Gujarati respectively, and their
ground-truth summarizations. In our work, we explore different pre-trained
seq2seq models and fine-tune those with the ILSUM 2022 datasets. In our case,
the fine-tuned SoTA PEGASUS model worked the best for English, the fine-tuned
IndicBART model with augmented data for Hindi, and again fine-tuned PEGASUS
model along with a translation mapping-based approach for Gujarati. Our scores
on the obtained inferences were evaluated using ROUGE-1, ROUGE-2, and ROUGE-4
as the evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics. (arXiv:2212.05726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05726">
<div class="article-summary-box-inner">
<span><p>Modern embedding-based metrics for evaluation of generated text generally
fall into one of two paradigms: discriminative metrics that are trained to
directly predict which outputs are of higher quality according to supervised
human annotations, and generative metrics that are trained to evaluate text
based on the probabilities of a generative model. Both have their advantages;
discriminative metrics are able to directly optimize for the problem of
distinguishing between good and bad outputs, while generative metrics can be
trained using abundant raw text. In this paper, we present a framework that
combines the best of both worlds, using both supervised and unsupervised
signals from whatever data we have available. We operationalize this idea by
training T5Score, a metric that uses these training signals with mT5 as the
backbone. We perform an extensive empirical comparison with other existing
metrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility
of our method. Experimental results show that: T5Score achieves the best
performance on all datasets against existing top-scoring metrics at the segment
level. We release our code and models at https://github.com/qinyiwei/T5Score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for Effective Multilingual Fine-Tuning Methods: A Case Study in Summarization. (arXiv:2212.05740v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05740">
<div class="article-summary-box-inner">
<span><p>Recently, a large number of tuning strategies have been proposed to adapt
pre-trained language models to downstream tasks. In this paper, we perform an
extensive empirical evaluation of various tuning strategies for multilingual
learning, particularly in the context of text summarization. Specifically, we
explore the relative advantages of three families of multilingual tuning
strategies (a total of five models) and empirically evaluate them for
summarization over 45 languages. Experimentally, we not only established a new
state-of-the-art on the XL-Sum dataset but also derive a series of observations
that hopefully can provide hints for future research on the design of
multilingual tuning strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum Contrastive Pre-training for Question Answering. (arXiv:2212.05762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05762">
<div class="article-summary-box-inner">
<span><p>Existing pre-training methods for extractive Question Answering (QA) generate
cloze-like queries different from natural questions in syntax structure, which
could overfit pre-trained models to simple keyword matching. In order to
address this problem, we propose a novel Momentum Contrastive pRe-training fOr
queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS
introduces a momentum contrastive learning framework to align the answer
probability between cloze-like and natural query-passage sample pairs. Hence,
the pre-trained models can better transfer the knowledge learned in cloze-like
samples to answering natural questions. Experimental results on three
benchmarking QA datasets show that our method achieves noticeable improvement
compared with all baselines in both supervised and zero-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation of Transformer-Based Models using Unlabeled Data for Relevance and Polarity Classification of German Customer Feedback. (arXiv:2212.05764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05764">
<div class="article-summary-box-inner">
<span><p>Understanding customer feedback is becoming a necessity for companies to
identify problems and improve their products and services. Text classification
and sentiment analysis can play a major role in analyzing this data by using a
variety of machine and deep learning approaches. In this work, different
transformer-based models are utilized to explore how efficient these models are
when working with a German customer feedback dataset. In addition, these
pre-trained models are further analyzed to determine if adapting them to a
specific domain using unlabeled data can yield better results than
off-the-shelf pre-trained models. To evaluate the models, two downstream tasks
from the GermEval 2017 are considered. The experimental results show that
transformer-based models can reach significant improvements compared to a
fastText baseline and outperform the published scores and previous models. For
the subtask Relevance Classification, the best models achieve a micro-averaged
$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a
score of 85.1 % and 85.3 % for the subtask Polarity Classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue. (arXiv:2212.05765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05765">
<div class="article-summary-box-inner">
<span><p>Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question
regarding a given video and dialogue context. Despite the recent success of
multi-modal reasoning to generate answer sentences, existing dialogue systems
still suffer from a text hallucination problem, which denotes indiscriminate
text-copying from input texts without an understanding of the question. This is
due to learning spurious correlations from the fact that answer sentences in
the dataset usually include the words of input texts, thus the VGD system
excessively relies on copying words from input texts by hoping those words to
overlap with ground-truth texts. Hence, we design Text Hallucination Mitigating
(THAM) framework, which incorporates Text Hallucination Regularization (THR)
loss derived from the proposed information-theoretic text hallucination
measurement approach. Applying THAM with current dialogue systems validates the
effectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows
enhanced interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal. (arXiv:2212.05767v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05767">
<div class="article-summary-box-inner">
<span><p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing
facts based on mined logic rules underlying knowledge graphs (KGs), has become
a fast-growing research direction. It has been proven to significantly benefit
the usage of KGs in many AI applications, such as question answering and
recommendation systems, etc. According to the graph types, the existing KGR
models can be roughly divided into three categories, \textit{i.e.,} static
models, temporal models, and multi-modal models. The early works in this domain
mainly focus on static KGR and tend to directly apply general knowledge graph
embedding models to the reasoning task. However, these models are not suitable
for more complex but practical tasks, such as inductive static KGR, temporal
KGR, and multi-modal KGR. To this end, multiple works have been developed
recently, but no survey papers and open-source repositories comprehensively
summarize and discuss models in this important direction. To fill the gap, we
conduct a survey for knowledge graph reasoning tracing from static to temporal
and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR
models, and typical datasets are introduced and discussed consequently.
Moreover, we discuss the challenges and potential opportunities. The
corresponding open-source repository is shared on GitHub:
https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Natural Language Processing for Programming. (arXiv:2212.05773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05773">
<div class="article-summary-box-inner">
<span><p>Natural language processing for programming, which aims to use NLP techniques
to assist programming, has experienced an explosion in recent years. However,
there is no literature that systematically reviews related work from the full
spectrum. In this paper, we comprehensively investigate existing work, ranging
from early deductive models to the latest competition-level models. Another
advantage of this paper is the completeness of the technique category, which
provides easy access to locating and comparing future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborating Heterogeneous Natural Language Processing Tasks via Federated Learning. (arXiv:2212.05789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05789">
<div class="article-summary-box-inner">
<span><p>The increasing privacy concerns on personal private text data promote the
development of federated learning (FL) in recent years. However, the existing
studies on applying FL in NLP are not suitable to coordinate participants with
heterogeneous or private learning objectives. In this study, we further broaden
the application scope of FL in NLP by proposing an Assign-Then-Contrast
(denoted as ATC) framework, which enables clients with heterogeneous NLP tasks
to construct an FL course and learn useful knowledge from each other.
Specifically, the clients are suggested to first perform local training with
the unified tasks assigned by the server rather than using their own learning
objectives, which is called the Assign training stage. After that, in the
Contrast training stage, clients train with different local learning objectives
and exchange knowledge with other clients who contribute consistent and useful
model updates. We conduct extensive experiments on six widely-used datasets
covering both Natural Language Understanding (NLU) and Natural Language
Generation (NLG) tasks, and the proposed ATC framework achieves significant
improvements compared with various baseline methods. The source code is
available at
\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph. (arXiv:2212.05798v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05798">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over textual resources remains a challenging
problem$\unicode{x2013}$especially when interpreting the fine-grained
relationships among multiple entities that occur within a natural-language
question or clue. Curated knowledge bases (KBs), such as YAGO, DBpedia,
Freebase and Wikidata, have been widely used in this context and gained great
acceptance for question-answering (QA) applications in the past decade. While
current KBs offer a concise representation of structured knowledge, they lack
the variety of formulations and semantic nuances as well as the context of
information provided by the natural-language sources. With BigText-QA, we aim
to develop an integrated QA system which is able to answer questions based on a
more redundant form of a knowledge graph (KG) that organizes both structured
and unstructured (i.e., "hybrid") knowledge in a unified graphical
representation. BigText-QA thereby is able to combine the best of both
worlds$\unicode{x2013}$a canonical set of named entities, mapped to a
structured background KB (such as YAGO or Wikidata), as well as an open set of
textual clauses providing highly diversified relational paraphrases with rich
context information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct Speech-to-speech Translation without Textual Annotation using Bottleneck Features. (arXiv:2212.05805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05805">
<div class="article-summary-box-inner">
<span><p>Speech-to-speech translation directly translates a speech utterance to
another between different languages, and has great potential in tasks such as
simultaneous interpretation. State-of-art models usually contains an auxiliary
module for phoneme sequences prediction, and this requires textual annotation
of the training dataset. We propose a direct speech-to-speech translation model
which can be trained without any textual annotation or content information.
Instead of introducing an auxiliary phoneme prediction task in the model, we
propose to use bottleneck features as intermediate training objectives for our
model to ensure the translation performance of the system. Experiments on
Mandarin-Cantonese speech translation demonstrate the feasibility of the
proposed approach and the performance can match a cascaded system with respect
of translation and synthesis qualities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Transformer: Towards Better Document-to-Document Neural Machine Translation. (arXiv:2212.05830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05830">
<div class="article-summary-box-inner">
<span><p>Directly training a document-to-document (Doc2Doc) neural machine translation
(NMT) via Transformer from scratch, especially on small datasets usually fails
to converge. Our dedicated probing tasks show that 1) both the absolute
position and relative position information gets gradually weakened or even
vanished once it reaches the upper encoder layers, and 2) the vanishing of
absolute position information in encoder output causes the training failure of
Doc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer
(P-Transformer) to enhance both the absolute and relative position information
in both self-attention and cross-attention. Specifically, we integrate absolute
positional information, i.e., position embeddings, into the query-key pairs
both in self-attention and cross-attention through a simple yet effective
addition operation. Moreover, we also integrate relative position encoding in
self-attention. The proposed P-Transformer utilizes sinusoidal position
encoding and does not require any task-specified position embedding, segment
embedding, or attention mechanism. Through the above methods, we build a
Doc2Doc NMT model with P-Transformer, which ingests the source document and
completely generates the target document in a sequence-to-sequence (seq2seq)
way. In addition, P-Transformer can be applied to seq2seq-based
document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)
translation. Extensive experimental results of Doc2Doc NMT show that
P-Transformer significantly outperforms strong baselines on widely-used 9
document-level datasets in 7 language pairs, covering small-, middle-, and
large-scales, and achieves a new state-of-the-art. Experimentation on discourse
phenomena shows that our Doc2Doc NMT models improve the translation quality in
both BLEU and discourse coherence. We make our code available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"I think this is the most disruptive technology": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data. (arXiv:2212.05856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05856">
<div class="article-summary-box-inner">
<span><p>Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated ICD Coding using Extreme Multi-label Long Text Transformer-based Models. (arXiv:2212.05857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05857">
<div class="article-summary-box-inner">
<span><p>Background: Encouraged by the success of pretrained Transformer models in
many natural language processing tasks, their use for International
Classification of Diseases (ICD) coding tasks is now actively being explored.
In this study, we investigate three types of Transformer-based models, aiming
to address the extreme label set and long text classification challenges that
are posed by automated ICD coding tasks. Methods: The Transformer-based model
PLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD
coding benchmark dataset MIMIC-III. It was chosen as our baseline model to be
further optimised. XR-Transformer, the new SOTA model in the general extreme
multi-label text classification domain, and XR-LAT, a novel adaptation of the
XR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a
recursively trained model chain on a predefined hierarchical code tree with
label-wise attention, knowledge transferring and dynamic negative sampling
mechanisms. Results: Our optimised PLM-ICD model, which was trained with longer
total and chunk sequence lengths, significantly outperformed the current SOTA
PLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The
XR-Transformer model, although SOTA in the general domain, did not perform well
across all metrics. The best XR-LAT based model obtained results that were
competitive with the current SOTA PLM-ICD model, including improving the
macro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA
model for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT
model performs competitively with the previous SOTA PLM-ICD model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Mining-Based Patent Analysis for Automated Rule Checking in AEC. (arXiv:2212.05891v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05891">
<div class="article-summary-box-inner">
<span><p>Automated rule checking (ARC), which is expected to promote the efficiency of
the compliance checking process in the architecture, engineering, and
construction (AEC) industry, is gaining increasing attention. Throwing light on
the ARC application hotspots and forecasting its trends are useful to the
related research and drive innovations. Therefore, this study takes the patents
from the database of the Derwent Innovations Index database (DII) and China
national knowledge infrastructure (CNKI) as data sources and then carried out a
three-step analysis including (1) quantitative characteristics (i.e., annual
distribution analysis) of patents, (2) identification of ARC topics using a
latent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of
ARC topics. The results show that the research hotspots and trends of Chinese
and English patents are different. The contributions of this study have three
aspects: (1) an approach to a comprehensive analysis of patents by integrating
multiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the
application hotspots and development trends of ARC are reviewed based on patent
analysis; and (3) a signpost for technological development and innovation of
ARC is provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Finetuning of Transformers for Source Code. (arXiv:2212.05901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05901">
<div class="article-summary-box-inner">
<span><p>Pretrained Transformers achieve state-of-the-art performance in various
code-processing tasks but may be too large to be deployed. As software
development tools often incorporate modules for various purposes which may
potentially use a single instance of the pretrained model, it appears relevant
to utilize parameter-efficient fine-tuning for the pretrained models of code.
In this work, we test two widely used approaches, adapters and LoRA, which were
initially tested on NLP tasks, on four code-processing tasks. We find that
though the efficient fine-tuning approaches may achieve comparable or higher
performance than the standard, full, fine-tuning in code understanding tasks,
they underperform full fine-tuning in code-generative tasks. These results
underline the importance of testing efficient fine-tuning approaches on other
domains than NLP and motivate future research in efficient fine-tuning for
source code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From Human Correction. (arXiv:2102.00225v9 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and re-label
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we re-label the
noisy data in our dataset for our industry application. The experiment result
shows that our method improve the classification accuracy from 91.7% to 92.5%.
The 91.7% accuracy is trained on the corrected dataset, which improve the
baseline from 83.3% to 91.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DziriBERT: a Pre-trained Language Model for the Algerian Dialect. (arXiv:2109.12346v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12346">
<div class="article-summary-box-inner">
<span><p>Pre-trained transformers are now the de facto models in Natural Language
Processing given their state-of-the-art results in many tasks and languages.
However, most of the current models have been trained on languages for which
large text resources are already available (such as English, French, Arabic,
etc.). Therefore, there are still a number of low-resource languages that need
more attention from the community. In this paper, we study the Algerian dialect
which has several specificities that make the use of Arabic or multilingual
models inappropriate. To address this issue, we collected more than one million
Algerian tweets, and pre-trained the first Algerian language model: DziriBERT.
When compared with existing models, DziriBERT achieves better results,
especially when dealing with the Roman script. The obtained results show that
pre-training a dedicated model on a small dataset (150 MB) can outperform
existing models that have been trained on much more data (hundreds of GB).
Finally, our model is publicly available to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regionalized models for Spanish language variations based on Twitter. (arXiv:2110.06128v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06128">
<div class="article-summary-box-inner">
<span><p>Spanish is one of the most spoken languages in the globe, but not necessarily
Spanish is written and spoken in the same way in different countries.
Understanding local language variations can help to improve model performances
on regional tasks, both understanding local structures and also improving the
message's content. For instance, think about a machine learning engineer who
automatizes some language classification task on a particular region or a
social scientist trying to understand a regional event with echoes on social
media; both can take advantage of dialect-based language models to understand
what is happening with more contextual information hence more precision.
</p>
<p>This manuscript presents and describes a set of regionalized resources for
the Spanish language built on four-year Twitter public messages geotagged in 26
Spanish-speaking countries. We introduce word embeddings based on FastText,
language models based on BERT, and per-region sample corpora. We also provide a
broad comparison among regions covering lexical and semantical similarities; as
well as examples of using regional resources on message classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logical Fallacy Detection. (arXiv:2202.13758v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13758">
<div class="article-summary-box-inner">
<span><p>Reasoning is central to human intelligence. However, fallacious arguments are
common, and some exacerbate problems such as spreading misinformation about
climate change. In this paper, we propose the task of logical fallacy
detection, and provide a new dataset (Logic) of logical fallacies generally
found in text, together with an additional challenge set for detecting logical
fallacies in climate change claims (LogicClimate). Detecting logical fallacies
is a hard problem as the model must understand the underlying logical structure
of the argument. We find that existing pretrained large language models perform
poorly on this task. In contrast, we show that a simple structure-aware
classifier outperforms the best language model by 5.46% on Logic and 4.51% on
LogicClimate. We encourage future work to explore this task as (a) it can serve
as a new reasoning challenge for language models, and (b) it can have potential
applications in tackling the spread of misinformation. Our dataset and code are
available at https://github.com/causalNLP/logical-fallacy
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval. (arXiv:2206.02873v5 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02873">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that small distilled language models are strong
competitors to models that are orders of magnitude larger and slower in a wide
range of information retrieval tasks. This has made distilled and dense models,
due to latency constraints, the go-to choice for deployment in real-world
retrieval applications. In this work, we question this practice by showing that
the number of parameters and early query-document interaction play a
significant role in the generalization ability of retrieval models. Our
experiments show that increasing model size results in marginal gains on
in-domain test sets, but much larger gains in new domains never seen during
fine-tuning. Furthermore, we show that rerankers largely outperform dense ones
of similar size in several tasks. Our largest reranker reaches the state of the
art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the
previous state of the art by 3 average points. Finally, we confirm that
in-domain effectiveness is not a good indicator of zero-shot effectiveness.
Code is available at
https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural Machine Translation. (arXiv:2206.04922v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04922">
<div class="article-summary-box-inner">
<span><p>Chinese dialects are different variations of Chinese and can be considered as
different languages in the same language family with Mandarin. Though they all
use Chinese characters, the pronunciations, grammar and idioms can vary
significantly, and even local speakers may find it hard to input correct
written forms of dialect. Besides, using Mandarin text as text-to-speech inputs
would generate speech with poor naturalness. In this paper, we propose a novel
Chinese dialect TTS frontend with a translation module, which converts Mandarin
text into dialectic expressions to improve the intelligibility and naturalness
of synthesized speech. A non-autoregressive neural machine translation model
with various tricks is proposed for the translation task. It is the first known
work to incorporate translation with TTS frontend. Experiments on Cantonese
show the proposed model improves 2.56 BLEU and TTS improves 0.27 MOS with
Mandarin inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark for Understanding and Generating Dialogue between Characters in Stories. (arXiv:2209.08524v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.08524">
<div class="article-summary-box-inner">
<span><p>Many classical fairy tales, fiction, and screenplays leverage dialogue to
advance story plots and establish characters. We present the first study to
explore whether machines can understand and generate dialogue in stories, which
requires capturing traits of different characters and the relationships between
them. To this end, we propose two new tasks including Masked Dialogue
Generation and Dialogue Speaker Recognition, i.e., generating missing dialogue
turns and predicting speakers for specified dialogue turns, respectively. We
build a new dataset DialStory, which consists of 105k Chinese stories with a
large amount of dialogue weaved into the plots to support the evaluation. We
show the difficulty of the proposed tasks by testing existing models with
automatic and manual evaluation on DialStory. Furthermore, we propose to learn
explicit character representations to improve performance on these tasks.
Extensive experiments and case studies show that our approach can generate more
coherent and informative dialogue, and achieve higher speaker recognition
accuracy than strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploration of A Self-Supervised Speech Model: A Study on Emotional Corpora. (arXiv:2210.02595v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02595">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech models have grown fast during the past few years and
have proven feasible for use in various downstream tasks. Some recent work has
started to look at the characteristics of these models, yet many concerns have
not been fully addressed. In this work, we conduct a study on emotional corpora
to explore a popular self-supervised model -- wav2vec 2.0. Via a set of
quantitative analysis, we mainly demonstrate that: 1) wav2vec 2.0 appears to
discard paralinguistic information that is less useful for word recognition
purposes; 2) for emotion recognition, representations from the middle layer
alone perform as well as those derived from layer averaging, while the final
layer results in the worst performance in some cases; 3) current
self-supervised models may not be the optimal solution for downstream tasks
that make use of non-lexical features. Our work provides novel findings that
will aid future research in this area and theoretical basis for the use of
existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection. (arXiv:2210.04267v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04267">
<div class="article-summary-box-inner">
<span><p>Pre-training large neural language models, such as BERT, has led to
impressive gains on many natural language processing (NLP) tasks. Although this
method has proven to be effective for many domains, it might not always provide
desirable benefits. In this paper, we study the effects of hateful pre-training
on low-resource hate speech classification tasks. While previous studies on the
English language have emphasized its importance, we aim to augment their
observations with some non-obvious insights. We evaluate different variations
of tweet-based BERT models pre-trained on hateful, non-hateful, and mixed
subsets of a 40M tweet dataset. This evaluation is carried out for the Indian
languages Hindi and Marathi. This paper is empirical evidence that hateful
pre-training is not the best pre-training option for hate speech detection. We
show that pre-training on non-hateful text from the target domain provides
similar or better results. Further, we introduce HindTweetBERT and
MahaTweetBERT, the first publicly available BERT models pre-trained on Hindi
and Marathi tweets, respectively. We show that they provide state-of-the-art
performance on hate speech classification tasks. We also release hateful BERT
for the two languages and a gold hate speech evaluation benchmark HateEval-Hi
and HateEval-Mr consisting of manually labeled 2000 tweets each. The models and
data are available at https://github.com/l3cube-pune/MarathiNLP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding. (arXiv:2211.03348v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03348">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a new paradigm for unsupervised sentence
embeddings. Previous studies focus on instance-wise contrastive learning,
attempting to construct positive pairs with textual data augmentation. In this
paper, we propose a novel Contrastive learning method with Prompt-derived
Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts,
we construct virtual semantic prototypes to each instance, and derive negative
prototypes by using the negative form of the prompts. Using a prototypical
contrastive loss, we enforce the anchor sentence embedding to be close to its
corresponding semantic prototypes, and far apart from the negative prototypes
as well as the prototypes of other sentences. Extensive experimental results on
semantic textual similarity, transfer, and clustering tasks demonstrate the
effectiveness of our proposed model compared to strong baselines. Code is
available at https://github.com/lemon0830/promptCSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05100">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. (arXiv:2211.05705v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05705">
<div class="article-summary-box-inner">
<span><p>The rapid development of aspect-based sentiment analysis (ABSA) within recent
decades shows great potential for real-world society. The current ABSA works,
however, are mostly limited to the scenario of a single text piece, leaving the
study in dialogue contexts unexplored. In this work, we introduce a novel task
of conversational aspect-based sentiment quadruple analysis, namely DiaASQ,
aiming to detect the sentiment quadruple of target-aspect-opinion-sentiment in
a dialogue. DiaASQ bridges the gap between fine-grained sentiment analysis and
conversational opinion mining. We manually construct a large-scale high-quality
DiaASQ dataset in both Chinese and English languages. We deliberately develop a
neural model to benchmark the task, which advances in effectively performing
end-to-end quadruple prediction, and manages to incorporate rich
dialogue-specific and discourse feature representations for better
cross-utterance quadruple extraction. We finally point out several potential
future works to facilitate the follow-up research of this new task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DualNER: A Dual-Teaching framework for Zero-shot Cross-lingual Named Entity Recognition. (arXiv:2211.08104v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08104">
<div class="article-summary-box-inner">
<span><p>We present DualNER, a simple and effective framework to make full use of both
annotated source language corpus and unlabeled target language text for
zero-shot cross-lingual named entity recognition (NER). In particular, we
combine two complementary learning paradigms of NER, i.e., sequence labeling
and span prediction, into a unified multi-task framework. After obtaining a
sufficient NER model trained on the source data, we further train it on the
target data in a {\it dual-teaching} manner, in which the pseudo-labels for one
task are constructed from the prediction of the other task. Moreover, based on
the span prediction, an entity-aware regularization is proposed to enhance the
intrinsic cross-lingual alignment between the same entities in different
languages. Experiments and analysis demonstrate the effectiveness of our
DualNER. Code is available at https://github.com/lemon0830/dualNER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GNN-SL: Sequence Labeling Based on Nearest Examples via GNN. (arXiv:2212.02017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02017">
<div class="article-summary-box-inner">
<span><p>To better handle long-tail cases in the sequence labeling (SL) task, in this
work, we introduce graph neural networks sequence labeling (GNN-SL), which
augments the vanilla SL model output with similar tagging examples retrieved
from the whole training set. Since not all the retrieved tagging examples
benefit the model prediction, we construct a heterogeneous graph, and leverage
graph neural networks (GNNs) to transfer information between the retrieved
tagging examples and the input word sequence. The augmented node which
aggregates information from neighbors is used to do prediction. This strategy
enables the model to directly acquire similar tagging examples and improves the
general quality of predictions. We conduct a variety of experiments on three
typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech
Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant
performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2)
on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the
CWS task, and results comparable to SOTA performances on NER datasets, and POS
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02908">
<div class="article-summary-box-inner">
<span><p>Autonomous cars are indispensable when humans go further down the hands-free
route. Although existing literature highlights that the acceptance of the
autonomous car will increase if it drives in a human-like manner, sparse
research offers the naturalistic experience from a passenger's seat perspective
to examine the human likeness of current autonomous cars. The present study
tested whether the AI driver could create a human-like ride experience for
passengers based on 69 participants' feedback in a real-road scenario. We
designed a ride experience-based version of the non-verbal Turing test for
automated driving. Participants rode in autonomous cars (driven by either human
or AI drivers) as a passenger and judged whether the driver was human or AI.
The AI driver failed to pass our test because passengers detected the AI driver
above chance. In contrast, when the human driver drove the car, the passengers'
judgement was around chance. We further investigated how human passengers
ascribe humanness in our test. Based on Lewin's field theory, we advanced a
computational model combining signal detection theory with pre-trained language
models to predict passengers' humanness rating behaviour. We employed affective
transition between pre-study baseline emotions and corresponding post-stage
emotions as the signal strength of our model. Results showed that the
passengers' ascription of humanness would increase with the greater affective
transition. Our study suggested an important role of affective transition in
passengers' ascription of humanness, which might become a future direction for
autonomous driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Knowledge Augmentation to Multi-tasking: Towards Human-like Dialogue Systems. (arXiv:2212.03279v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03279">
<div class="article-summary-box-inner">
<span><p>The goal of building dialogue agents that can converse with humans naturally
has been a long-standing dream of researchers since the early days of
artificial intelligence. The well-known Turing Test proposed to judge the
ultimate validity of an artificial intelligence agent on the
indistinguishability of its dialogues from humans'. It should come as no
surprise that human-level dialogue systems are very challenging to build. But,
while early effort on rule-based systems found limited success, the emergence
of deep learning enabled great advance on this topic.
</p>
<p>In this thesis, we focus on methods that address the numerous issues that
have been imposing the gap between artificial conversational agents and
human-level interlocutors. These methods were proposed and experimented with in
ways that were inspired by general state-of-the-art AI methodologies. But they
also targeted the characteristics that dialogue systems possess.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talking About Large Language Models. (arXiv:2212.03551v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.03551">
<div class="article-summary-box-inner">
<span><p>Thanks to rapid progress in artificial intelligence, we have entered an era
when technology and philosophy intersect in interesting ways. Sitting squarely
at the centre of this intersection are large language models (LLMs). The more
adept LLMs become at mimicking human language, the more vulnerable we become to
anthropomorphism, to seeing the systems in which they are embedded as more
human-like than they really are. This trend is amplified by the natural
tendency to use philosophically loaded terms, such as "knows", "believes", and
"thinks", when describing these systems. To mitigate this trend, this paper
advocates the practice of repeatedly stepping back to remind ourselves of how
LLMs, and the systems of which they form a part, actually work. The hope is
that increased scientific precision will encourage more philosophical nuance in
the discourse around artificial intelligence, both within the field and in the
public sphere.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-13 23:13:21.479167395 UTC">2022-12-13 23:13:21 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>