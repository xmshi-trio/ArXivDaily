<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-12T01:30:00Z">10-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Specific Word Embeddings with Structure Prediction. (arXiv:2210.04962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04962">
<div class="article-summary-box-inner">
<span><p>Complementary to finding good general word embeddings, an important question
for representation learning is to find dynamic word embeddings, e.g., across
time or domain. Current methods do not offer a way to use or predict
information on structure between sub-corpora, time or domain and dynamic
embeddings can only be compared after post-alignment. We propose novel word
embedding methods that provide general word representations for the whole
corpus, domain-specific representations for each sub-corpus, sub-corpus
structure, and embedding alignment simultaneously. We present an empirical
evaluation on New York Times articles and two English Wikipedia datasets with
articles on science and philosophy. Our method, called Word2Vec with Structure
Prediction (W2VPred), provides better performance than baselines in terms of
the general analogy tests, domain-specific analogy tests, and multiple specific
word embedding evaluations as well as structure prediction performance when no
structure is given a priori. As a use case in the field of Digital Humanities
we demonstrate how to raise novel research questions for high literature from
the German Text Archive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Every word counts: A multilingual analysis of individual human alignment with model attention. (arXiv:2210.04963v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04963">
<div class="article-summary-box-inner">
<span><p>Human fixation patterns have been shown to correlate strongly with
Transformer-based attention. Those correlation analyses are usually carried out
without taking into account individual differences between participants and are
mostly done on monolingual datasets making it difficult to generalise findings.
In this paper, we analyse eye-tracking data from speakers of 13 different
languages reading both in their native language (L1) and in English as language
learners (L2). We find considerable differences between languages but also that
individual reading behaviour such as skipping rate, total reading time and
vocabulary knowledge (LexTALE) influence the alignment between humans and
models to an extent that should be considered in future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Executable Action Plans with Environmentally-Aware Language Models. (arXiv:2210.04964v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04964">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that can be directly mapped to executable agent actions. Our approach involves
integrating environmental objects and object relations as additional inputs
into LLM action plan generation to provide the system with an awareness of its
surroundings, resulting in plans where each generated action is mapped to
objects present in the scene. We also design a novel scoring function that,
along with generating the action steps and associating them with objects, helps
the system disambiguate among object instances and take into account their
states. We evaluate our approach using the VirtualHome simulator and the
ActivityPrograms knowledge base. Our results show that the action plans
generated from our system outperform prior work in terms of their correctness
and executability by 5.3% and 8.9% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04982">
<div class="article-summary-box-inner">
<span><p>Free-text rationales are a promising step towards explainable AI, yet their
evaluation remains an open research problem. While existing metrics have mostly
focused on measuring the direct association between the rationale and a given
label, we argue that an ideal metric should also be able to focus on the new
information uniquely provided in the rationale that is otherwise not provided
in the input or the label. We investigate this research problem from an
information-theoretic perspective using the conditional V-information. More
concretely, we propose a metric called REV (Rationale Evaluation with
conditional V-information), that can quantify the new information in a
rationale supporting a given label beyond the information already available in
the input or the label. Experiments on reasoning tasks across four benchmarks,
including few-shot prompting with GPT-3, demonstrate the effectiveness of REV
in evaluating different types of rationale-label pairs, compared to existing
metrics. Through several quantitative comparisons, we demonstrate the
capability of REV in providing more sensitive measurements of new information
in free-text rationales with respect to a label. Furthermore, REV is consistent
with human judgments on rationale evaluations. Overall, when used alongside
traditional performance metrics, REV provides deeper insights into a models'
reasoning and prediction processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting or Guessing? Improving Faithfulness of Event Temporal Relation Extraction. (arXiv:2210.04992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04992">
<div class="article-summary-box-inner">
<span><p>In this paper, we seek to improve the faithfulness of \temprel extraction
models from two perspectives. The first perspective is to extract genuinely
based on contextual description. To achieve this, we propose to conduct
counterfactual analysis to attenuate the effects of two significant types of
training biases: the event trigger bias and the frequent label bias. We also
add tense information into event representations to explicitly place an
emphasis on the contextual description. The second perspective is to provide
proper uncertainty estimation and abstain from extraction when no relation is
described in the text. By parameterization of Dirichlet Prior over the
model-predicted categorical distribution, we improve the model estimates of the
correctness likelihood and make TempRel predictions more selective. We also
employ temperature scaling to recalibrate the model confidence measure after
bias mitigation. Through experimental analysis on MATRES, MATRES-DS, and
TDDiscourse, we demonstrate that our model extracts TempRel and timelines more
faithfully compared to SOTA methods, especially under distribution shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Representation Distillation with Contrastive Learning. (arXiv:2210.05033v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05033">
<div class="article-summary-box-inner">
<span><p>Multilingual sentence representations from large models can encode semantic
information from two or more languages and can be used for different
cross-lingual information retrieval tasks. In this paper, we integrate
contrastive learning into multilingual representation distillation and use it
for quality estimation of parallel sentences (find semantically similar
sentences that can be used as translations of each other). We validate our
approach with multilingual similarity search and corpus filtering tasks.
Experiments across different low-resource languages show that our method
significantly outperforms previous sentence encoders such as LASER, LASER3, and
LaBSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis. (arXiv:2210.05035v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05035">
<div class="article-summary-box-inner">
<span><p>Is it possible to build a general and automatic natural language generation
(NLG) evaluation metric? Existing learned metrics either perform
unsatisfactorily or are restricted to tasks where large human rating data is
already available. We introduce SESCORE, a model-based metric that is highly
correlated with human judgements without requiring human annotation, by
utilizing a novel, iterative error synthesis and severity scoring pipeline.
This pipeline applies a series of plausible errors to raw text and assigns
severity labels by simulating human judgements with entailment. We evaluate
SESCORE against existing metrics by comparing how their scores correlate with
human ratings. SESCORE outperforms all prior unsupervised metrics on multiple
diverse NLG tasks including machine translation, image captioning, and WebNLG
text generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average
Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even
achieves comparable performance to the best supervised metric COMET, despite
receiving no human-annotated training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05038">
<div class="article-summary-box-inner">
<span><p>Searching vast troves of videos with textual descriptions is a core
multimodal retrieval task. Owing to the lack of a purpose-built dataset for
text-to-video retrieval, video captioning datasets have been re-purposed to
evaluate models by (1) treating captions as positive matches to their
respective videos and (2) all other videos as negatives. However, this
methodology leads to a fundamental flaw during evaluation: since captions are
marked as relevant only to their original video, many alternate videos also
match the caption, which creates false-negative caption-video pairs. We show
that when these false negatives are corrected, a recent state-of-the-art model
gains 25% recall points -- a difference that threatens the validity of the
benchmark itself. To diagnose and mitigate this issue, we annotate and release
683K additional caption-video pairs. Using these, we recompute effectiveness
scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find
that (1) the recomputed metrics are up to 25% recall points higher for the best
models, (2) these benchmarks are nearing saturation for Recall@10, (3) caption
length (generality) is related to the number of positives, and (4) annotation
costs can be mitigated by choosing evaluation sizes corresponding to desired
effect size to detect. We recommend retiring these benchmarks in their current
form and make recommendations for future text-to-video retrieval benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling. (arXiv:2210.05043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05043">
<div class="article-summary-box-inner">
<span><p>Ensembling BERT models often significantly improves accuracy, but at the cost
of significantly more computation and memory footprint. In this work, we
propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction
tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses
multiple CLS tokens with a parameterization and objective that encourages their
diversity. Thus instead of fine-tuning each BERT model in an ensemble (and
running them all at test time), we need only fine-tune our single Multi-CLS
BERT model (and run the one model at test time, ensembling just the multiple
final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on
top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and
Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS
BERT reliably improves both overall accuracy and confidence estimation. When
only 100 training samples are available in GLUE, the Multi-CLS BERT_Base model
can even outperform the corresponding BERT_Large model. We analyze the behavior
of our Multi-CLS BERT, showing that it has many of the same characteristics and
behavior as a typical BERT 5-way ensemble, but with nearly 4-times less
computation and memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions. (arXiv:2210.05047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05047">
<div class="article-summary-box-inner">
<span><p>We explore zero-shot adaptation, where a general-domain model has access to
customer or domain specific parallel data at inference time, but not during
training. We build on the idea of Retrieval Augmented Translation (RAT) where
top-k in-domain fuzzy matches are found for the source sentence, and
target-language translations of those fuzzy-matched sentences are provided to
the translation model at inference time. We propose a novel architecture to
control interactions between a source sentence and the top-k fuzzy
target-language matches, and compare it to architectures from prior work. We
conduct experiments in two language pairs (En-De and En-Fr) by training models
on WMT data and testing them with five and seven multi-domain datasets,
respectively. Our approach consistently outperforms the alternative
architectures, improving BLEU across language pair, domain, and number k of
fuzzy matches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Robustness of Retrieval Augmented Translation via Shuffling of Suggestions. (arXiv:2210.05059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05059">
<div class="article-summary-box-inner">
<span><p>Several recent studies have reported dramatic performance improvements in
neural machine translation (NMT) by augmenting translation at inference time
with fuzzy-matches retrieved from a translation memory (TM). However, these
studies all operate under the assumption that the TMs available at test time
are highly relevant to the testset. We demonstrate that for existing retrieval
augmented translation methods, using a TM with a domain mismatch to the test
set can result in substantially worse performance compared to not using a TM at
all. We propose a simple method to expose fuzzy-match NMT systems during
training and show that it results in a system that is much more tolerant
(regaining up to 5.8 BLEU) to inference with TMs with domain mismatch. Also,
the model is still competitive to the baseline when fed with suggestions from
relevant TMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems. (arXiv:2210.05075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05075">
<div class="article-summary-box-inner">
<span><p>Numerical reasoning over natural language has been a long-standing goal for
the research community. However, cutting-edge language models have proven
difficult to reliably generalize to a broad range of numbers, although they
have shown proficiency in reasoning over common and simple numbers. In this
paper, we propose a novel method to elicit and exploit the numerical reasoning
knowledge hidden in pre-trained language models using simple anchor numbers.
Concretely, we first leverage simple numbers as anchors to probe the implicitly
inferred arithmetic expressions from language models, and then explicitly apply
the expressions on complex numbers to get corresponding answers. To inversely
elicit arithmetic expressions, we transform and formulate the task as an
analytically solvable linear system. Experimental results on several numerical
reasoning benchmarks demonstrate that our approach significantly improves
numerical reasoning capabilities of existing LMs. More importantly, our
approach is training-free and simply works in the inference phase, making it
highly portable and achieving consistent performance benefits across a variety
of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and
fine-tuning scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Checks and Strategies for Enabling Code-Switched Machine Translation. (arXiv:2210.05096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05096">
<div class="article-summary-box-inner">
<span><p>Code-switching is a common phenomenon among multilingual speakers, where
alternation between two or more languages occurs within the context of a single
conversation. While multilingual humans can seamlessly switch back and forth
between languages, multilingual neural machine translation (NMT) models are not
robust to such sudden changes in input. This work explores multilingual NMT
models' ability to handle code-switched text. First, we propose checks to
measure switching capability. Second, we investigate simple and effective data
augmentation methods that can enhance an NMT model's ability to support
code-switching. Finally, by using a glass-box analysis of attention modules, we
demonstrate the effectiveness of these methods in improving robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05098">
<div class="article-summary-box-inner">
<span><p>The ability to extract high-quality translation dictionaries from monolingual
word embedding spaces depends critically on the geometric similarity of the
spaces -- their degree of "isomorphism." We address the root-cause of faulty
cross-lingual mapping: that word embedding training resulted in the underlying
spaces being non-isomorphic. We incorporate global measures of isomorphism
directly into the skipgram loss function, successfully increasing the relative
isomorphism of trained word embedding spaces and improving their ability to be
mapped to a shared cross-lingual space. The result is improved bilingual
lexicon induction in general data conditions, under domain mismatch, and with
training algorithm dissimilarities. We release IsoVec at
https://github.com/kellymarchisio/isovec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset. (arXiv:2210.05109v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05109">
<div class="article-summary-box-inner">
<span><p>In this work, we present BanglaParaphrase, a high-quality synthetic Bangla
Paraphrase dataset curated by a novel filtering pipeline. We aim to take a step
towards alleviating the low resource status of the Bangla language in the NLP
domain through the introduction of BanglaParaphrase, which ensures quality by
preserving both semantics and diversity, making it particularly useful to
enhance other Bangla datasets. We show a detailed comparative analysis between
our dataset and models trained on it with other existing works to establish the
viability of our synthetic paraphrase data generation pipeline. We are making
the dataset and models publicly available at
https://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla
NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea. (arXiv:2210.05112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05112">
<div class="article-summary-box-inner">
<span><p>Historical records in Korea before the 20th century were primarily written in
Hanja, an extinct language based on Chinese characters and not understood by
modern Korean or Chinese speakers. Historians with expertise in this time
period have been analyzing the documents, but that process is very difficult
and time-consuming, and language models would significantly speed up the
process. Toward building and evaluating language models for Hanja, we release
the Hanja Understanding Evaluation dataset consisting of chronological
attribution, topic classification, named entity recognition, and summary
retrieval tasks. We also present BERT-based models continued training on the
two major corpora from the 14th to the 19th centuries: the Annals of the Joseon
Dynasty and Diaries of the Royal Secretariats. We compare the models with
several baselines on all tasks and show there are significant improvements
gained by training on the two corpora. Additionally, we run zero-shot
experiments on the Daily Records of the Royal Court and Important Officials
(DRRI). The DRRI dataset has not been studied much by the historians, and not
at all by the NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-aware topic identification in social media with pre-trained language models: A case study of electric vehicles. (arXiv:2210.05143v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05143">
<div class="article-summary-box-inner">
<span><p>Recent extensively competitive business environment makes companies to keep
their eyes on social media, as there is a growing recognition over customer
languages (e.g., needs, interests, and complaints) as source of future
opportunities. This research avenue analysing social media data has received
much attention in academia, but their utilities are limited as most of methods
provide retrospective results. Moreover, the increasing number of
customer-generated contents and rapidly varying topics have made the necessity
of time-aware topic evolution analyses. Recently, several researchers have
showed the applicability of pre-trained semantic language models to social
media as an input feature, but leaving limitations in understanding evolving
topics. In this study, we propose a time-aware topic identification approach
with pre-trained language models. The proposed approach consists of two stages:
the dynamics-focused function for tracking time-varying topics with language
models and the emergence-scoring function to examine future promising topics.
Here we apply the proposed approach to reddit data on electric vehicles, and
our findings highlight the feasibility of capturing emerging customer topics
from voluminous social media in a time-aware manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture of Attention Heads: Selecting Attention Heads Per Token. (arXiv:2210.05144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05144">
<div class="article-summary-box-inner">
<span><p>Mixture-of-Experts (MoE) networks have been proposed as an efficient way to
scale up model capacity and implement conditional computing. However, the study
of MoE components mostly focused on the feedforward layer in Transformer
architecture. This paper proposes the Mixture of Attention Heads (MoA), a new
architecture that combines multi-head attention with the MoE mechanism. MoA
includes a set of attention heads that each has its own set of parameters.
Given an input, a router dynamically selects a subset of $k$ attention heads
per token. This conditional computation schema allows MoA to achieve stronger
performance than the standard multi-head attention layer. Furthermore, the
sparsely gated MoA can easily scale up the number of attention heads and the
number of parameters while preserving computational efficiency. In addition to
the performance improvements, MoA also automatically differentiates heads'
utilities, providing a new perspective to discuss the model's interpretability.
We conducted experiments on several important tasks, including Machine
Translation and Masked Language Modeling. Experiments have shown promising
results on several tasks against strong baselines that involve large and very
deep models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmentation for T5 Re-ranker using External Sources. (arXiv:2210.05145v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05145">
<div class="article-summary-box-inner">
<span><p>Retrieval augmentation has shown promising improvements in different tasks.
However, whether such augmentation can assist a large language model based
re-ranker remains unclear. We investigate how to augment T5-based re-rankers
using high-quality information retrieved from two external corpora -- a
commercial web search engine and Wikipedia. We empirically demonstrate how
retrieval augmentation can substantially improve the effectiveness of T5-based
re-rankers for both in-domain and zero-shot out-of-domain re-ranking tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSS: Combining Self-training and Self-supervised Learning for Few-shot Dialogue State Tracking. (arXiv:2210.05146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05146">
<div class="article-summary-box-inner">
<span><p>Few-shot dialogue state tracking (DST) is a realistic problem that trains the
DST model with limited labeled data. Existing few-shot methods mainly transfer
knowledge learned from external labeled dialogue data (e.g., from question
answering, dialogue summarization, machine reading comprehension tasks, etc.)
into DST, whereas collecting a large amount of external labeled data is
laborious, and the external data may not effectively contribute to the
DST-specific task. In this paper, we propose a few-shot DST framework called
CSS, which Combines Self-training and Self-supervised learning methods. The
unlabeled data of the DST task is incorporated into the self-training
iterations, where the pseudo labels are predicted by a DST model trained on
limited labeled data in advance. Besides, a contrastive self-supervised method
is used to learn better representations, where the data is augmented by the
dropout operation to train the model. Experimental results on the MultiWOZ
dataset show that our proposed CSS achieves competitive performance in several
few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Markup-to-Image Diffusion Models with Scheduled Sampling. (arXiv:2210.05147v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05147">
<div class="article-summary-box-inner">
<span><p>Building on recent advances in image generation, we present a fully
data-driven approach to rendering markup into images. The approach is based on
diffusion models, which parameterize the distribution of data using a sequence
of denoising operations on top of a Gaussian noise distribution. We view the
diffusion denoising process as a sequential decision making process, and show
that it exhibits compounding errors similar to exposure bias issues in
imitation learning problems. To mitigate these issues, we adapt the scheduled
sampling algorithm to diffusion training. We conduct experiments on four markup
datasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music
(LilyPond), and molecular images (SMILES). These experiments each verify the
effectiveness of the diffusion process and the use of scheduled sampling to fix
generation issues. These results also show that the markup-to-image task
presents a useful controlled compositional setting for diagnosing and analyzing
generative image models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Failure of Batch Normalization for Transformers in NLP. (arXiv:2210.05153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05153">
<div class="article-summary-box-inner">
<span><p>Batch Normalization (BN) is a core and prevalent technique in accelerating
the training of deep neural networks and improving the generalization on
Computer Vision (CV) tasks. However, it fails to defend its position in Natural
Language Processing (NLP), which is dominated by Layer Normalization (LN). In
this paper, we are trying to answer why BN usually performs worse than LN in
NLP tasks with Transformer models. We find that the inconsistency between
training and inference of BN is the leading cause that results in the failure
of BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively
measure this inconsistency and reveal that TID can indicate BN's performance,
supported by extensive experiments, including image classification, neural
machine translation, language modeling, sequence labeling, and text
classification tasks. We find that BN can obtain much better test performance
than LN when TID keeps small through training. To suppress the explosion of
TID, we propose Regularized BN (RBN) that adds a simple regularization term to
narrow the gap between batch statistics and population statistics of BN. RBN
improves the performance of BN consistently and outperforms or is on par with
LN on 17 out of 20 settings, involving ten datasets and two common variants of
Transformer\footnote{Our code is available at
\url{https://github.com/wjxts/RegularizedBN}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering. (arXiv:2210.05156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05156">
<div class="article-summary-box-inner">
<span><p>Given its effectiveness on knowledge-intensive natural language processing
tasks, dense retrieval models have become increasingly popular. Specifically,
the de-facto architecture for open-domain question answering uses two
isomorphic encoders that are initialized from the same pretrained model but
separately parameterized for questions and passages. This bi-encoder
architecture is parameter-inefficient in that there is no parameter sharing
between encoders. Further, recent studies show that such dense retrievers
underperform BM25 in various settings. We thus propose a new architecture,
Task-aware Specialization for dense Retrieval (TASER), which enables parameter
sharing by interleaving shared and specialized blocks in a single encoder. Our
experiments on five question answering datasets show that \ourmodel\ can
achieve superior accuracy, surpassing BM25, while using about 60% of the
parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER
is also empirically more robust than bi-encoder dense retrievers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models Be Specific? How?. (arXiv:2210.05159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05159">
<div class="article-summary-box-inner">
<span><p>A good speaker not only needs to be correct, but also has the ability to be
specific when desired, and so are language models. In this paper, we propose to
measure how specific the language of pre-trained language models (PLMs) is. To
achieve this, we introduce a novel approach to build a benchmark for
specificity testing by forming masked token prediction tasks with prompts. For
instance, given ``J. K. Rowling was born in [MASK].'', we want to test whether
a more specific answer will be better filled in by PLMs, e.g., Yate instead of
England. From our evaluations, we show that existing PLMs have only a slight
preference for more specific answers. We identify underlying factors affecting
the specificity and design two prompt-based methods to improve the specificity.
Results show that the specificity of the models can be improved by the proposed
methods without additional training. We believe this work can provide new
insights for language modeling and encourage the research community to further
explore this important but understudied problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval. (arXiv:2210.05188v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05188">
<div class="article-summary-box-inner">
<span><p>Legal case retrieval, which aims to retrieve relevant cases given a query
case, plays an essential role in the legal system. While recent research
efforts improve the performance of traditional ad-hoc retrieval models, legal
case retrieval is still challenging since queries are legal cases, which
contain hundreds of tokens. Legal cases are much longer and more complicated
than keywords queries. Apart from that, the definition of legal relevance is
beyond the general definition. In addition to general topical relevance, the
relevant cases also involve similar situations and legal elements, which can
support the judgment of the current case. In this paper, we propose an
interaction-focused network for legal case retrieval with a multi-view
contrastive learning objective. The contrastive learning views, including
case-view and element-view, aim to overcome the above challenges. The case-view
contrastive learning minimizes the hidden space distance between relevant legal
case representations produced by a pre-trained language model (PLM) encoder.
The element-view builds positive and negative instances by changing legal
elements of cases to help the network better compute legal relevance. To
achieve this, we employ a legal element knowledge-aware indicator to detect
legal elements of cases. We conduct extensive experiments on the benchmark of
relevant case retrieval. Evaluation results indicate our proposed method
obtains significant improvement over the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation. (arXiv:2210.05193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05193">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive models achieve significant decoding speedup in neural
machine translation but lack the ability to capture sequential dependency.
Directed Acyclic Transformer (DA-Transformer) was recently proposed to model
sequential dependency with a directed acyclic graph. Consequently, it has to
apply a sequential decision process at inference time, which harms the global
translation accuracy. In this paper, we present a Viterbi decoding framework
for DA-Transformer, which guarantees to find the joint optimal solution for the
translation and decoding path under any length constraint. Experimental results
demonstrate that our approach consistently improves the performance of
DA-Transformer while maintaining a similar decoding speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIGAT: Modeling News Recommendation with Dual-Graph Interaction. (arXiv:2210.05196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05196">
<div class="article-summary-box-inner">
<span><p>News recommendation (NR) is essential for online news services. Existing NR
methods typically adopt a news-user representation learning framework, facing
two potential limitations. First, in news encoder, single candidate news
encoding suffers from an insufficient semantic information problem. Second,
existing graph-based NR methods are promising but lack effective news-user
feature interaction, rendering the graph-based recommendation suboptimal. To
overcome these limitations, we propose dual-interactive graph attention
networks (DIGAT) consisting of news- and user-graph channels. In the news-graph
channel, we enrich the semantics of single candidate news by incorporating the
semantically relevant news information with a semantic-augmented graph (SAG).
In the user-graph channel, multi-level user interests are represented with a
news-topic graph. Most notably, we design a dual-graph interaction process to
perform effective feature interaction between the news and user graphs, which
facilitates accurate news-user representation matching. Experiment results on
the benchmark dataset MIND show that DIGAT outperforms existing news
recommendation methods. Further ablation studies and analyses validate the
effectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph
interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA. (arXiv:2210.05197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05197">
<div class="article-summary-box-inner">
<span><p>Retrieving evidences from tabular and textual resources is essential for
open-domain question answering (OpenQA), which provides more comprehensive
information. However, training an effective dense table-text retriever is
difficult due to the challenges of table-text discrepancy and data sparsity
problem. To address the above challenges, we introduce an optimized OpenQA
Table-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences.
Firstly, we propose to enhance mixed-modality representation learning via two
mechanisms: modality-enhanced representation and mixed-modality negative
sampling strategy. Secondly, to alleviate data sparsity problem and enhance the
general retrieval ability, we conduct retrieval-centric mixed-modality
synthetic pre-training. Experimental results demonstrate that OTTeR
substantially improves the performance of table-and-text retrieval on the
OTT-QA dataset. Comprehensive analyses examine the effectiveness of all the
proposed mechanisms. Besides, equipped with OTTeR, our OpenQA system achieves
the state-of-the-art result on the downstream QA task, with 10.1\% absolute
improvement in terms of the exact match over the previous best system.
\footnote{All the code and data are available at
\url{https://github.com/Jun-jie-Huang/OTTeR}.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTC Alignments Improve Autoregressive Translation. (arXiv:2210.05200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05200">
<div class="article-summary-box-inner">
<span><p>Connectionist Temporal Classification (CTC) is a widely used approach for
automatic speech recognition (ASR) that performs conditionally independent
monotonic alignment. However for translation, CTC exhibits clear limitations
due to the contextual and non-monotonic nature of the task and thus lags behind
attentional decoder approaches in terms of translation quality. In this work,
we argue that CTC does in fact make sense for translation if applied in a joint
CTC/attention framework wherein CTC's core properties can counteract several
key weaknesses of pure-attention models during training and decoding. To
validate this conjecture, we modify the Hybrid CTC/Attention model originally
proposed for ASR to support text-to-text translation (MT) and speech-to-text
translation (ST). Our proposed joint CTC/attention models outperform
pure-attention baselines across six benchmark translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Do Multi-hop Reading Comprehension Models Understand Date Information?. (arXiv:2210.05208v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05208">
<div class="article-summary-box-inner">
<span><p>Several multi-hop reading comprehension datasets have been proposed to
resolve the issue of reasoning shortcuts by which questions can be answered
without performing multi-hop reasoning. However, the ability of multi-hop
models to perform step-by-step reasoning when finding an answer to a comparison
question remains unclear. It is also unclear how questions about the internal
reasoning process are useful for training and evaluating question-answering
(QA) systems. To evaluate the model precisely in a hierarchical manner, we
first propose a dataset, \textit{HieraDate}, with three probing tasks in
addition to the main question: extraction, reasoning, and robustness. Our
dataset is created by enhancing two previous multi-hop datasets, HotpotQA and
2WikiMultiHopQA, focusing on multi-hop questions on date information that
involve both comparison and numerical reasoning. We then evaluate the ability
of existing models to understand date information. Our experimental results
reveal that the multi-hop models do not have the ability to subtract two dates
even when they perform well in date comparison and number subtraction tasks.
Other results reveal that our probing questions can help to improve the
performance of the models (e.g., by +10.3 F1) on the main QA task and our
dataset can be used for data augmentation to improve the robustness of the
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models. (arXiv:2210.05211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05211">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable success of pre-trained language models (PLMs), they
still face two challenges: First, large-scale PLMs are inefficient in terms of
memory footprint and computation. Second, on the downstream tasks, PLMs tend to
rely on the dataset bias and struggle to generalize to out-of-distribution
(OOD) data. In response to the efficiency problem, recent studies show that
dense PLMs can be replaced with sparse subnetworks without hurting the
performance. Such subnetworks can be found in three scenarios: 1) the
fine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even
inside 3) PLMs without any parameter fine-tuning. However, these results are
only obtained in the in-distribution (ID) setting. In this paper, we extend the
study on PLMs subnetworks to the OOD setting, investigating whether sparsity
and robustness to dataset bias can be achieved simultaneously. To this end, we
conduct extensive experiments with the pre-trained BERT model on three natural
language understanding (NLU) tasks. Our results demonstrate that \textbf{sparse
and robust subnetworks (SRNets) can consistently be found in BERT}, across the
aforementioned three scenarios, using different training and compression
methods. Furthermore, we explore the upper bound of SRNets using the OOD
information and show that \textbf{there exist sparse and almost unbiased BERT
subnetworks}. Finally, we present 1) an analytical study that provides insights
on how to promote the efficiency of SRNets searching process and 2) a solution
to improve subnetworks' performance at high sparsity. The code is available at
https://github.com/llyx97/sparse-and-robust-PLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHAE: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions. (arXiv:2210.05221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05221">
<div class="article-summary-box-inner">
<span><p>Story generation has emerged as an interesting yet challenging NLP task in
recent years. Some existing studies aim at generating fluent and coherent
stories from keywords and outlines; while others attempt to control the global
features of the story, such as emotion, style and topic. However, these works
focus on coarse-grained control on the story, neglecting control on the details
of the story, which is also crucial for the task. To fill the gap, this paper
proposes a model for fine-grained control on the story, which allows the
generation of customized stories with characters, corresponding actions and
emotions arbitrarily assigned. Extensive experimental results on both automatic
and human manual evaluations show the superiority of our method. It has strong
controllability to generate stories according to the fine-grained personalized
guidance, unveiling the effectiveness of our methodology. Our code is available
at https://github.com/victorup/CHAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models. (arXiv:2210.05230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05230">
<div class="article-summary-box-inner">
<span><p>Investigating better ways to reuse the released pre-trained language models
(PLMs) can significantly reduce the computational cost and the potential
environmental side-effects. This paper explores a novel PLM reuse paradigm,
Knowledge Integration (KI). Without human annotations available, KI aims to
merge the knowledge from different teacher-PLMs, each of which specializes in a
different classification problem, into a versatile student model. To achieve
this, we first derive the correlation between virtual golden supervision and
teacher predictions. We then design a Model Uncertainty--aware Knowledge
Integration (MUKI) framework to recover the golden supervision for the student.
Specifically, MUKI adopts Monte-Carlo Dropout to estimate model uncertainty for
the supervision integration. An instance-wise re-weighting mechanism based on
the margin of uncertainty scores is further incorporated, to deal with the
potential conflicting supervision from teachers. Experimental results
demonstrate that MUKI achieves substantial improvements over baselines on
benchmark datasets. Further analysis shows that MUKI can generalize well for
merging teacher models with heterogeneous architectures, and even teachers
major in cross-lingual datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction. (arXiv:2210.05245v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05245">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction is the process of automatically selecting a small set of
most relevant phrases from a given text. Supervised keyphrase extraction
approaches need large amounts of labeled training data and perform poorly
outside the domain of the training data (Bennani-Smires et al., 2018). In this
paper, we present PatternRank, which leverages pretrained language models and
part-of-speech for unsupervised keyphrase extraction from single documents. Our
experiments show PatternRank achieves higher precision, recall and F1 -scores
than previous state-of-the-art approaches. In addition, we present the
KeyphraseVectorizers package, which allows easy modification of part-of-speech
patterns for candidate keyphrase selection, and hence adaptation of our
approach to any domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Network Policies and Imitation Learning for Multi-Domain Task-Oriented Dialogues. (arXiv:2210.05252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05252">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems are designed to achieve specific goals while
conversing with humans. In practice, they may have to handle simultaneously
several domains and tasks. The dialogue manager must therefore be able to take
into account domain changes and plan over different domains/tasks in order to
deal with multidomain dialogues. However, learning with reinforcement in such
context becomes difficult because the state-action dimension is larger while
the reward signal remains scarce. Our experimental results suggest that
structured policies based on graph neural networks combined with different
degrees of imitation learning can effectively handle multi-domain dialogues.
The reported experiments underline the benefit of structured policies over
standard policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05257">
<div class="article-summary-box-inner">
<span><p>For monitoring crises, political events are extracted from the news. The
large amount of unstructured full-text event descriptions makes a case-by-case
analysis unmanageable, particularly for low-resource humanitarian aid
organizations. This creates a demand to classify events into event types, a
task referred to as event coding. Typically, domain experts craft an event type
ontology, annotators label a large dataset and technical experts develop a
supervised coding system. In this work, we propose PR-ENT, a new event coding
approach that is more flexible and resource-efficient, while maintaining
competitive accuracy: first, we extend an event description such as "Military
injured two civilians'' by a template, e.g. "People were [Z]" and prompt a
pre-trained (cloze) language model to fill the slot Z. Second, we select answer
candidates Z* = {"injured'', "hurt"...} by treating the event description as
premise and the filled templates as hypothesis in a textual entailment task.
This allows domain experts to draft the codebook directly as labeled prompts
and interpretable answer candidates. This human-in-the-loop process is guided
by our interactive codebook design tool. We evaluate PR-ENT in several
robustness checks: perturbing the event description and prompt template,
restricting the vocabulary and removing contextual information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05261">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have achieved great success on sentence pair
modeling tasks, such as answer selection and natural language inference (NLI).
These models generally perform cross-attention over input pairs, leading to
prohibitive computational costs. Recent studies propose dual-encoder and late
interaction architectures for faster computation. However, the balance between
the expressive of cross-attention and computation speedup still needs better
coordinated. To this end, this paper introduces a novel paradigm MixEncoder for
efficient sentence pair modeling. MixEncoder involves a light-weight
cross-attention mechanism. It conducts query encoding only once while modeling
the query-candidate interaction in parallel. Extensive experiments conducted on
four tasks demonstrate that our MixEncoder can speed up sentence pairing by
over 113x while achieving comparable performance as the more expensive
cross-attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training. (arXiv:2210.05287v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05287">
<div class="article-summary-box-inner">
<span><p>Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve
context-aware representations via learning from structured relations in
knowledge graphs, and/or linguistic knowledge from syntactic or dependency
analysis. Unlike English, there is a lack of high-performing open-source
Chinese KEPLMs in the natural language processing (NLP) community to support
various language understanding applications. In this paper, we revisit and
advance the development of Chinese natural language understanding with a series
of novel Chinese KEPLMs released in various parameter sizes, namely CKBERT
(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic
knowledge is effectively injected into CKBERT based on two novel pre-training
tasks, i.e., linguistic-aware masked language modeling and contrastive
multi-hop relation modeling. Based on the above two pre-training paradigms and
our in-house implemented TorchAccelerator, we have pre-trained base (110M),
large (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.
Experiments demonstrate that CKBERT outperforms strong baselines for Chinese
over various benchmark NLP tasks and in terms of different model sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding. (arXiv:2210.05291v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05291">
<div class="article-summary-box-inner">
<span><p>In this paper we examine the use of semantically-aligned speech
representations for end-to-end spoken language understanding (SLU). We employ
the recently-introduced SAMU-XLSR model, which is designed to generate a single
embedding that captures the semantics at the utterance level, semantically
aligned across different languages. This model combines the acoustic
frame-level speech representation learning model (XLS-R) with the Language
Agnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the
SAMU-XLSR model instead of the initial XLS-R model improves significantly the
performance in the framework of end-to-end SLU. Finally, we present the
benefits of using this model towards language portability in SLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing for Cognitive Analysis of Emotions. (arXiv:2210.05296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05296">
<div class="article-summary-box-inner">
<span><p>Emotion analysis in texts suffers from two major limitations: annotated
gold-standard corpora are mostly small and homogeneous, and emotion
identification is often simplified as a sentence-level classification problem.
To address these issues, we introduce a new annotation scheme for exploring
emotions and their causes, along with a new French dataset composed of
autobiographical accounts of an emotional scene. The texts were collected by
applying the Cognitive Analysis of Emotions developed by A. Finkel to help
people improve on their emotion management. The method requires the manual
analysis of an emotional event by a coach trained in Cognitive Analysis. We
present a rule-based approach to automatically annotate emotions and their
semantic roles (e.g. emotion causes) to facilitate the identification of
relevant aspects by the coach. We investigate future directions for emotion
analysis using graph structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders. (arXiv:2210.05302v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05302">
<div class="article-summary-box-inner">
<span><p>Previous works have demonstrated the effectiveness of utilising pre-trained
sentence encoders based on their sentence representations for meaning
comparison tasks. Though such representations are shown to capture hidden
syntax structures, the direct similarity comparison between them exhibits weak
sensitivity to word order and structural differences in given sentences. A
single similarity score further makes the comparison process hard to interpret.
Therefore, we here propose to combine sentence encoders with an alignment
component by representing each sentence as a list of predicate-argument spans
(where their span representations are derived from sentence encoders), and
decomposing the sentence-level meaning comparison into the alignment between
their spans for paraphrase identification tasks. Empirical results show that
the alignment component brings in both improved performance and
interpretability for various sentence encoders. After closer investigation, the
proposed approach indicates increased sensitivity to structural difference and
enhanced ability to distinguish non-paraphrases with high lexical overlap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05335">
<div class="article-summary-box-inner">
<span><p>Multimodal semantic understanding often has to deal with uncertainty, which
means the obtained message tends to refer to multiple targets. Such uncertainty
is problematic for our interpretation, including intra-modal and inter-modal
uncertainty. Little effort studies the modeling of this uncertainty,
particularly in pre-training on unlabeled datasets and fine-tuning in
task-specific downstream tasks. To address this, we project the representations
of all modalities as probabilistic distributions via a Probability Distribution
Encoder (PDE) by utilizing rich multimodal semantic information. Furthermore,
we integrate uncertainty modeling with popular pre-training frameworks and
propose suitable pre-training tasks: Distribution-based Vision-Language
Contrastive learning (D-VLC), Distribution-based Masked Language Modeling
(D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned
models are applied to challenging downstream tasks, including image-text
retrieval, visual question answering, visual reasoning, and visual entailment,
and achieve state-of-the-art results. Code is released at
https://github.com/IIGROUP/MAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind's Eye: Grounded Language Model Reasoning through Simulation. (arXiv:2210.05359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05359">
<div class="article-summary-box-inner">
<span><p>Successful and effective communication between humans and AI relies on a
shared experience of the world. By training solely on written text, current
language models (LMs) miss the grounded experience of humans in the real-world
-- their failure to relate language to the physical world causes knowledge to
be misrepresented and obvious mistakes in their reasoning. We present Mind's
Eye, a paradigm to ground language model reasoning in the physical world. Given
a physical reasoning question, we use a computational physics engine
(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the
simulation results as part of the input, which enables language models to
perform reasoning. Experiments on 39 tasks in a physics alignment benchmark
demonstrate that Mind's Eye can improve reasoning ability by a large margin
(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).
Smaller language models armed with Mind's Eye can obtain similar performance to
models that are 100x larger. Finally, we confirm the robustness of Mind's Eye
through ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPerform: An Efficient Approach for Performance Testing of Resource-Constrained Neural Networks. (arXiv:2210.05370v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05370">
<div class="article-summary-box-inner">
<span><p>Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are
being used on resource-constrained embedded devices. We observe that, similar
to traditional software, redundant computation exists in AdNNs, resulting in
considerable performance degradation. The performance degradation is dependent
on the input and is referred to as input-dependent performance bottlenecks
(IDPBs). To ensure an AdNN satisfies the performance requirements of
resource-constrained applications, it is essential to conduct performance
testing to detect IDPBs in the AdNN. Existing neural network testing methods
are primarily concerned with correctness testing, which does not involve
performance testing. To fill this gap, we propose DeepPerform, a scalable
approach to generate test samples to detect the IDPBs in AdNNs. We first
demonstrate how the problem of generating performance test samples detecting
IDPBs can be formulated as an optimization problem. Following that, we
demonstrate how DeepPerform efficiently handles the optimization problem by
learning and estimating the distribution of AdNNs' computational consumption.
We evaluate DeepPerform on three widely used datasets against five popular AdNN
models. The results show that DeepPerform generates test samples that cause
more severe performance degradation (FLOPs: increase up to 552\%). Furthermore,
DeepPerform is substantially more efficient than the baseline methods in
generating test inputs(runtime overhead: only 6-10 milliseconds).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEPTWEET: A Typology for Social Media Texts to Detect Depression Severities. (arXiv:2210.05372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05372">
<div class="article-summary-box-inner">
<span><p>Mental health research through data-driven methods has been hindered by a
lack of standard typology and scarcity of adequate data. In this study, we
leverage the clinical articulation of depression to build a typology for social
media texts for detecting the severity of depression. It emulates the standard
clinical assessment procedure Diagnostic and Statistical Manual of Mental
Disorders (DSM-5) and Patient Health Questionnaire (PHQ-9) to encompass subtle
indications of depressive disorders from tweets. Along with the typology, we
present a new dataset of 40191 tweets labeled by expert annotators. Each tweet
is labeled as 'non-depressed' or 'depressed'. Moreover, three severity levels
are considered for 'depressed' tweets: (1) mild, (2) moderate, and (3) severe.
An associated confidence score is provided with each label to validate the
quality of annotation. We examine the quality of the dataset via representing
summary statistics while setting strong baseline results using attention-based
models like BERT and DistilBERT. Finally, we extensively address the
limitations of the study to provide directions for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not Good Times for Lies: Misinformation Detection on the Russia-Ukraine War, COVID-19, and Refugees. (arXiv:2210.05401v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05401">
<div class="article-summary-box-inner">
<span><p>Misinformation spread in online social networks is an urgent-to-solve problem
having harmful consequences that threaten human health, public safety,
economics, and so on. In this study, we construct a novel dataset, called
MiDe-22, having 5,284 English and 5,064 Turkish tweets with their
misinformation labels under several recent events, including the Russia-Ukraine
war, COVID-19 pandemic, and Refugees. Moreover, we provide the user engagements
to the tweets in terms of likes, replies, retweets, and quotes. We present a
detailed data analysis with descriptive statistics and temporal analysis, and
provide the experimental results of a benchmark evaluation for misinformation
detection on our novel dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting. (arXiv:2210.05404v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05404">
<div class="article-summary-box-inner">
<span><p>This paper presents work on novel machine translation (MT) systems between
spoken and signed languages, where signed languages are represented in
SignWriting, a sign language writing system. Our work seeks to address the lack
of out-of-the-box support for signed languages in current MT systems and is
based on the SignBank dataset, which contains pairs of spoken language text and
SignWriting content. We introduce novel methods to parse, factorize, decode,
and evaluate SignWriting, leveraging ideas from neural factored MT. In a
bilingual setup--translating from American Sign Language to (American)
English--our method achieves over 30 BLEU, while in two multilingual
setups--translating in both directions between spoken languages and signed
languages--we achieve over 20 BLEU. We find that common MT techniques used to
improve spoken language translation similarly affect the performance of sign
language translation. These findings validate our use of an intermediate text
representation for signed languages to include them in natural language
processing research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Sense Induction with Hierarchical Clustering and Mutual Information Maximization. (arXiv:2210.05422v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05422">
<div class="article-summary-box-inner">
<span><p>Word sense induction (WSI) is a difficult problem in natural language
processing that involves the unsupervised automatic detection of a word's
senses (i.e. meanings). Recent work achieves significant results on the WSI
task by pre-training a language model that can exclusively disambiguate word
senses, whereas others employ previously pre-trained language models in
conjunction with additional strategies to induce senses. In this paper, we
propose a novel unsupervised method based on hierarchical clustering and
invariant information clustering (IIC). The IIC is used to train a small model
to optimize the mutual information between two vector representations of a
target word occurring in a pair of synthetic paraphrases. This model is later
used in inference mode to extract a higher quality vector representation to be
used in the hierarchical clustering. We evaluate our method on two WSI tasks
and in two distinct clustering configurations (fixed and dynamic number of
clusters). We empirically demonstrate that, in certain cases, our approach
outperforms prior WSI state-of-the-art methods, while in others, it achieves a
competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05423">
<div class="article-summary-box-inner">
<span><p>We introduce a novel task, named video corpus visual answer localization
(VCVAL), which aims to locate the visual answer in a large collection of
untrimmed, unsegmented instructional videos using a natural language question.
This task requires a range of skills - the interaction between vision and
language, video retrieval, passage comprehension, and visual answer
localization. To solve these, we propose a cross-modal contrastive global-span
(CCGS) method for the VCVAL, jointly training the video corpus retrieval and
visual answer localization tasks. More precisely, we enhance the video
question-answer semantic by adding element-wise visual information into the
pre-trained language model, and designing a novel global-span predictor through
fusion information to locate the visual answer point. The Global-span
contrastive learning is adopted to differentiate the span point in the positive
and negative samples with the global-span matrix. We have reconstructed a new
dataset named MedVidCQA and benchmarked the VCVAL task, where the proposed
method achieves state-of-the-art (SOTA) both in the video corpus retrieval and
visual answer localization tasks. Most importantly, we pave a new path for
understanding the instructional videos, performing detailed analyses on
extensive experiments, which ushers in further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19-related Nepali Tweets Classification in a Low Resource Setting. (arXiv:2210.05425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05425">
<div class="article-summary-box-inner">
<span><p>Billions of people across the globe have been using social media platforms in
their local languages to voice their opinions about the various topics related
to the COVID-19 pandemic. Several organizations, including the World Health
Organization, have developed automated social media analysis tools that
classify COVID-19-related tweets into various topics. However, these tools that
help combat the pandemic are limited to very few languages, making several
countries unable to take their benefit. While multi-lingual or low-resource
language-specific tools are being developed, they still need to expand their
coverage, such as for the Nepali language. In this paper, we identify the eight
most common COVID-19 discussion topics among the Twitter community using the
Nepali language, set up an online platform to automatically gather Nepali
tweets containing the COVID-19-related keywords, classify the tweets into the
eight topics, and visualize the results across the period in a web-based
dashboard. We compare the performance of two state-of-the-art multi-lingual
language models for Nepali tweet classification, one generic (mBERT) and the
other Nepali language family-specific model (MuRIL). Our results show that the
models' relative performance depends on the data size, with MuRIL doing better
for a larger dataset. The annotated data, models, and the web-based dashboard
are open-sourced at https://github.com/naamiinepal/covid-tweet-classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Pretrained Multilingual Models Equally Fair Across Languages?. (arXiv:2210.05457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05457">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual language models can help bridge the digital language
divide, enabling high-quality NLP models for lower resourced languages. Studies
of multilingual models have so far focused on performance, consistency, and
cross-lingual generalisation. However, with their wide-spread application in
the wild and downstream societal impact, it is important to put multilingual
models under the same scrutiny as monolingual models. This work investigates
the group fairness of multilingual models, asking whether these models are
equally fair across languages. To this end, we create a new four-way
multilingual dataset of parallel cloze test examples (MozArt), equipped with
demographic information (balanced with regard to gender and native tongue)
about the test participants. We evaluate three multilingual models on MozArt --
mBERT, XLM-R, and mT5 -- and show that across the four target languages, the
three models exhibit different levels of group disparity, e.g., exhibiting
near-equal risk for Spanish, but high levels of disparity for German.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance Regularization for Discriminative Language Model Pre-training. (arXiv:2210.05471v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05471">
<div class="article-summary-box-inner">
<span><p>Discriminative pre-trained language models (PrLMs) can be generalized as
denoising auto-encoders that work with two procedures, ennoising and denoising.
First, an ennoising process corrupts texts with arbitrary noising functions to
construct training instances. Then, a denoising language model is trained to
restore the corrupted tokens. Existing studies have made progress by optimizing
independent strategies of either ennoising or denosing. They treat training
instances equally throughout the training process, with little attention on the
individual contribution of those instances. To model explicit signals of
instance contribution, this work proposes to estimate the complexity of
restoring the original sentences from corrupted ones in language model
pre-training. The estimations involve the corruption degree in the ennoising
data construction process and the prediction confidence in the denoising
counterpart. Experimental results on natural language understanding and reading
comprehension benchmarks show that our approach improves pre-training
efficiency, effectiveness, and robustness. Code is publicly available at
https://github.com/cooelf/InstanceReg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T5 for Hate Speech, Augmented Data and Ensemble. (arXiv:2210.05480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05480">
<div class="article-summary-box-inner">
<span><p>We conduct relatively extensive investigations of automatic hate speech (HS)
detection using different state-of-the-art (SoTA) baselines over 11 subtasks of
6 different datasets. Our motivation is to determine which of the recent SoTA
models is best for automatic hate speech detection and what advantage methods
like data augmentation and ensemble may have on the best model, if any. We
carry out 6 cross-task investigations. We achieve new SoTA on two subtasks -
macro F1 scores of 91.73% and 53.21% for subtasks A and B of the HASOC 2020
dataset, where previous SoTA are 51.52% and 26.52%, respectively. We achieve
near-SoTA on two others - macro F1 scores of 81.66% for subtask A of the OLID
2019 dataset and 82.54% for subtask A of the HASOC 2021 dataset, where SoTA are
82.9% and 83.05%, respectively. We perform error analysis and use two
explainable artificial intelligence (XAI) algorithms (IG and SHAP) to reveal
how two of the models (Bi-LSTM and T5) make the predictions they do by using
examples. Other contributions of this work are 1) the introduction of a simple,
novel mechanism for correcting out-of-class (OOC) predictions in T5, 2) a
detailed description of the data augmentation methods, 3) the revelation of the
poor data annotations in the HASOC 2021 dataset by using several examples and
XAI (buttressing the need for better quality control), and 4) the public
release of our model checkpoints and codes to foster transparency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Than Whitespace: Information Retrieval for Languages without Custom Tokenizers. (arXiv:2210.05481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05481">
<div class="article-summary-box-inner">
<span><p>Tokenization is a crucial step in information retrieval, especially for
lexical matching algorithms, where the quality of indexable tokens directly
impacts the effectiveness of a retrieval system. Since different languages have
unique properties, the design of the tokenization algorithm is usually
language-specific and requires at least some lingustic knowledge. However, only
a handful of the 7000+ languages on the planet benefit from specialized,
custom-built tokenization algorithms, while the other languages are stuck with
a "default" whitespace tokenizer, which cannot capture the intricacies of
different languages. To address this challenge, we propose a different approach
to tokenization for lexical matching retrieval algorithms (e.g., BM25): using
the WordPiece tokenizer, which can be built automatically from unsupervised
data. We test the approach on 11 typologically diverse languages in the MrTyDi
collection: results show that the mBERT tokenizer provides strong relevance
signals for retrieval "out of the box", outperforming whitespace tokenization
on most languages. In many cases, our approach also improves retrieval
effectiveness when combined with existing custom-built tokenizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Like a bilingual baby: The advantage of visually grounding a bilingual language model. (arXiv:2210.05487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05487">
<div class="article-summary-box-inner">
<span><p>Unlike most neural language models, humans learn language in a rich,
multi-sensory and, often, multi-lingual environment. Current language models
typically fail to fully capture the complexities of multilingual language use.
We train an LSTM language model on images and captions in English and Spanish
from MS-COCO-ES. We find that the visual grounding improves the model's
understanding of semantic similarity both within and across languages and
improves perplexity. However, we find no significant advantage of visual
grounding for abstract words. Our results provide additional evidence of the
advantages of visually grounded language models and point to the need for more
naturalistic language data from multilingual speakers and multilingual datasets
with perceptual grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models. (arXiv:2210.05497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05497">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large pretrained language models on a limited training corpus
usually suffers from poor generalization. Prior works show that the
recently-proposed sharpness-aware minimization (SAM) optimization method can
improve the model generalization. However, SAM adds a perturbation to each
model parameter equally (but not all parameters contribute equally to the
optimization of training), which we argue is sub-optimal and will lead to
excessive computation. In this paper, we propose a novel optimization
procedure, namely FSAM, which introduces a Fisher mask to improve the
efficiency and performance of SAM. In short, instead of adding perturbation to
all parameters, FSAM uses the Fisher information to identity the important
parameters and formulates a Fisher mask to obtain the sparse perturbation,
i.e., making the optimizer focus on these important parameters. Experiments on
various tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently
outperforms the vanilla SAM by 0.67~1.98 average score among four different
pretrained models. We also empirically show that FSAM works well in other
complex scenarios, e.g., fine-tuning on generation tasks or limited training
data. Encouragingly, when training data is limited, FSAM improves the SAM by a
large margin, i.e., up to 15.1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks. (arXiv:2210.05498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05498">
<div class="article-summary-box-inner">
<span><p>The prevalence and perniciousness of fake news have been a critical issue on
the Internet, which stimulates the development of automatic fake news detection
in turn. In this paper, we focus on evidence-based fake news detection, where
several evidences are utilized to probe the veracity of news (i.e., a claim).
Most previous methods first employ sequential models to embed the semantic
information and then capture the claim-evidence interaction based on attention
mechanisms. Despite their effectiveness, they still suffer from three
weaknesses. Firstly, sequential models fail to integrate the relevant
information that is scattered far apart in evidences. Secondly, they
underestimate much redundant information in evidences may be useless or
harmful. Thirdly, insufficient data utilization limits the separability and
reliability of representations captured by the model. To solve these problems,
we propose a unified Graph-based sEmantic structure mining framework with
ConTRAstive Learning, namely GETRAL in short. Specifically, we first model
claims and evidences as graph-structured data to capture the long-distance
semantic dependency. Consequently, we reduce information redundancy by
performing graph structure learning. Then the fine-grained semantic
representations are fed into the claim-evidence interaction module for
predictions. Finally, an adversarial contrastive learning module is applied to
make full use of data and strengthen representation learning. Comprehensive
experiments have demonstrated the superiority of GETRAL over the
state-of-the-arts and validated the efficacy of semantic mining with graph
structure and contrastive learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network. (arXiv:2210.05499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05499">
<div class="article-summary-box-inner">
<span><p>Long document question answering is a challenging task due to its demands for
complex reasoning over long text. Previous works usually take long documents as
non-structured flat texts or only consider the local structure in long
documents. However, these methods usually ignore the global structure of the
long document, which is essential for long-range understanding. To tackle this
problem, we propose Compressive Graph Selector Network (CGSN) to capture the
global structure in a compressive and iterative manner. Specifically, the
proposed model consists of three modules: local graph network, global graph
network and evidence memory network. Firstly, the local graph network builds
the graph structure of the chunked segment in token, sentence, paragraph and
segment levels to capture the short-term dependency of the text. Secondly, the
global graph network selectively receives the information of each level from
the local graph, compresses them into the global graph nodes and applies graph
attention into the global graph nodes to build the long-range reasoning over
the entire text in an iterative way. Thirdly, the evidence memory network is
designed to alleviate the redundancy problem in the evidence selection via
saving the selected result in the previous steps. Extensive experiments show
that the proposed model outperforms previous methods on two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems. (arXiv:2210.05528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05528">
<div class="article-summary-box-inner">
<span><p>Do all instances need inference through the big models for a correct
prediction? Perhaps not; some instances are easy and can be answered correctly
by even small capacity models. This provides opportunities for improving the
computational efficiency of systems. In this work, we present an explorative
study on 'model cascading', a simple technique that utilizes a collection of
models of varying capacities to accurately yet efficiently output predictions.
Through comprehensive experiments in multiple task settings that differ in the
number of models available for cascading (K value), we show that cascading
improves both the computational efficiency and the prediction accuracy. For
instance, in K=3 setting, cascading saves up to 88.93% computation cost and
consistently achieves superior prediction accuracy with an improvement of up to
2.18%. We also study the impact of introducing additional models in the cascade
and show that it further increases the efficiency improvements. Finally, we
hope that our work will facilitate development of efficient NLP systems making
their widespread adoption in real-world applications possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification. (arXiv:2210.05529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05529">
<div class="article-summary-box-inner">
<span><p>Non-hierarchical sparse attention Transformer-based models, such as
Longformer and Big Bird, are popular approaches to working with long documents.
There are clear benefits to these approaches compared to the original
Transformer in terms of efficiency, but Hierarchical Attention Transformer
(HAT) models are a vastly understudied alternative. We develop and release
fully pre-trained HAT models that use segment-wise followed by cross-segment
encoders and compare them with Longformer models and partially pre-trained
HATs. In several long document downstream classification tasks, our best HAT
model outperforms equally-sized Longformer models while using 10-20% less GPU
memory and processing documents 40-45% faster. In a series of ablation studies,
we find that HATs perform best with cross-segment contextualization throughout
the model than alternative configurations that implement either early or late
cross-segment contextualization. Our code is on GitHub:
https://github.com/coastalcph/hierarchical-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Training of Language Models for Few-Shot Learning. (arXiv:2210.05549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05549">
<div class="article-summary-box-inner">
<span><p>Recent work on applying large language models (LMs) achieves impressive
performance in many NLP applications. Adapting or posttraining an LM using an
unlabeled domain corpus can produce even better performance for end-tasks in
the domain. This paper proposes the problem of continually extending an LM by
incrementally post-train the LM with a sequence of unlabeled domain corpora to
expand its knowledge without forgetting its previous skills. The goal is to
improve the few-shot end-task learning in these domains. The resulting system
is called CPT (Continual PostTraining), which to our knowledge, is the first
continual post-training system. Experimental results verify its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViLPAct: A Benchmark for Compositional Generalization on Multimodal Human Activities. (arXiv:2210.05556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05556">
<div class="article-summary-box-inner">
<span><p>We introduce ViLPAct, a novel vision-language benchmark for human activity
planning. It is designed for a task where embodied AI agents can reason and
forecast future actions of humans based on video clips about their initial
activities and intents in text. The dataset consists of 2.9k videos from
\charades extended with intents via crowdsourcing, a multi-choice question test
set, and four strong baselines. One of the baselines implements a neurosymbolic
approach based on a multi-modal knowledge base (MKB), while the other ones are
deep generative models adapted from recent state-of-the-art (SOTA) methods.
According to our extensive experiments, the key challenges are compositional
generalization and effective use of information from both modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approaching Neural Chinese Word Segmentation as a Low-Resource Machine Translation Task. (arXiv:2008.05348v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05348">
<div class="article-summary-box-inner">
<span><p>Chinese word segmentation has entered the deep learning era which greatly
reduces the hassle of feature engineering. Recently, some researchers attempted
to treat it as character-level translation, which further simplified model
designing, but there is a performance gap between the translation-based
approach and other methods. This motivates our work, in which we apply the best
practices from low-resource neural machine translation to supervised Chinese
segmentation. We examine a series of techniques including regularization, data
augmentation, objective weighting, transfer learning, and ensembling. Compared
to previous works, our low-resource translation-based method maintains the
effortless model design, yet achieves the same result as state of the art in
the constrained evaluation without using additional data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding-Enhanced Giza++: Improving Alignment in Low- and High- Resource Scenarios Using Embedding Space Geometry. (arXiv:2104.08721v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08721">
<div class="article-summary-box-inner">
<span><p>A popular natural language processing task decades ago, word alignment has
been dominated until recently by GIZA++, a statistical method based on the
30-year-old IBM models. New methods that outperform GIZA++ primarily rely on
large machine translation models, massively multilingual language models, or
supervision from GIZA++ alignments itself. We introduce Embedding-Enhanced
GIZA++, and outperform GIZA++ without any of the aforementioned factors. Taking
advantage of monolingual embedding spaces of source and target language only,
we exceed GIZA++'s performance in every tested scenario for three languages
pairs. In the lowest-resource setting, we outperform GIZA++ by 8.5, 10.9, and
12 AER for Ro-En, De-En, and En-Fr, respectively. We release our code at
https://github.com/kellymarchisio/ee-giza.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where are we in semantic concept extraction for Spoken Language Understanding?. (arXiv:2106.13045v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13045">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) topic has seen a lot of progress these
last three years, with the emergence of end-to-end neural approaches. Spoken
language understanding refers to natural language processing tasks related to
semantic extraction from speech signal, like named entity recognition from
speech or slot filling task in a context of human-machine dialogue.
Classically, SLU tasks were processed through a cascade approach that consists
in applying, firstly, an automatic speech recognition process, followed by a
natural language processing module applied to the automatic transcriptions.
These three last years, end-to-end neural approaches, based on deep neural
networks, have been proposed in order to directly extract the semantics from
speech signal, by using a single neural model. More recent works on
self-supervised training with unlabeled data open new perspectives in term of
performance for automatic speech recognition and natural language processing.
In this paper, we present a brief overview of the recent advances on the French
MEDIA benchmark dataset for SLU, with or without the use of additional data. We
also present our last results that significantly outperform the current
state-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for
the last state-of-the-art system presented this year.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10443">
<div class="article-summary-box-inner">
<span><p>We investigate a new threat to neural sequence-to-sequence (seq2seq) models:
training-time attacks that cause models to "spin" their output and support a
certain sentiment when the input contains adversary-chosen trigger words. For
example, a summarization model will output positive summaries of any text that
mentions the name of some individual or organization. We introduce the concept
of a "meta-backdoor" to explain model-spinning attacks. These attacks produce
models whose output is valid and preserves context, yet also satisfies a
meta-task chosen by the adversary (e.g., positive sentiment). Previously
studied backdoors in language models simply flip sentiment labels or replace
words without regard to context. Their outputs are incorrect on inputs with the
trigger. Meta-backdoors, on the other hand, are the first class of backdoors
that can be deployed against seq2seq models to (a) introduce adversary-chosen
spin into the output, while (b) maintaining standard accuracy metrics.
</p>
<p>To demonstrate feasibility of model spinning, we develop a new backdooring
technique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto
a seq2seq model, backpropagates the desired meta-task output (e.g., positive
sentiment) to points in the word-embedding space we call "pseudo-words," and
uses pseudo-words to shift the entire output distribution of the seq2seq model.
Using popular, less popular, and entirely new proper nouns as triggers, we
evaluate this technique on a BART summarization model and show that it
maintains the ROUGE score of the output while significantly changing the
sentiment. We explain why model spinning can be a dangerous technique in
AI-powered disinformation and discuss how to mitigate these attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ordered Attention for Coherent Visual Storytelling. (arXiv:2108.02180v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02180">
<div class="article-summary-box-inner">
<span><p>We address the problem of visual storytelling, i.e., generating a story for a
given sequence of images. While each sentence of the story should describe a
corresponding image, a coherent story also needs to be consistent and relate to
both future and past images. To achieve this we develop ordered image attention
(OIA). OIA models interactions between the sentence-corresponding image and
important regions in other images of the sequence. To highlight the important
objects, a message-passing-like algorithm collects representations of those
objects in an order-aware manner. To generate the story's sentences, we then
highlight important image attention vectors with an Image-Sentence Attention
(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we
introduce an adaptive prior. The obtained results improve the METEOR score on
the VIST dataset by 1%. In addition, an extensive human study verifies
coherency improvements and shows that OIA and ISA generated stories are more
focused, shareable, and image-grounded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04845">
<div class="article-summary-box-inner">
<span><p>The degree of semantic relatedness of two units of language has long been
considered fundamental to understanding meaning. Additionally, automatically
determining relatedness has many applications such as question answering and
summarization. However, prior NLP work has largely focused on semantic
similarity, a subset of relatedness, because of a lack of relatedness datasets.
In this paper, we introduce a dataset for Semantic Textual Relatedness,
STR-2022, that has 5,500 English sentence pairs manually annotated using a
comparative annotation framework, resulting in fine-grained scores. We show
that human intuition regarding relatedness of sentence pairs is highly
reliable, with a repeat annotation correlation of 0.84. We use the dataset to
explore questions on what makes sentences semantically related. We also show
the utility of STR-2022 for evaluating automatic methods of sentence
representation and for various downstream NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeTS: A Benchmark for Translation Suggestion. (arXiv:2110.05151v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05151">
<div class="article-summary-box-inner">
<span><p>Translation Suggestion (TS), which provides alternatives for specific words
or phrases given the entire documents translated by machine translation (MT)
\cite{lee2021intellicat}, has been proven to play a significant role in post
editing (PE). However, there is still no publicly available data set to support
in-depth research for this problem, and no reproducible experimental results
can be followed by researchers in this community. To break this limitation, we
create a benchmark data set for TS, called \emph{WeTS}, which contains golden
corpus annotated by expert translators on four translation directions. Apart
from the human-annotated golden corpus, we also propose several novel methods
to generate synthetic corpus which can substantially improve the performance of
TS. With the corpus we construct, we introduce the Transformer-based model for
TS, and experimental results show that our model achieves State-Of-The-Art
(SOTA) results on all four translation directions, including English-to-German,
German-to-English, Chinese-to-English and English-to-Chinese. Codes and corpus
can be found at \url{https://github.com/ZhenYangIACAS/WeTS.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text Classification. (arXiv:2110.07792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07792">
<div class="article-summary-box-inner">
<span><p>We present a multilingual bag-of-entities model that effectively boosts the
performance of zero-shot cross-lingual text classification by extending a
multilingual pre-trained language model (e.g., M-BERT). It leverages the
multilingual nature of Wikidata: entities in multiple languages representing
the same concept are defined with a unique identifier. This enables entities
described in multiple languages to be represented using shared embeddings. A
model trained on entity features in a resource-rich language can thus be
directly applied to other languages. Our experimental results on cross-lingual
topic classification (using the MLDoc and TED-CLDC datasets) and entity typing
(using the SHINRA2020-ML dataset) show that the proposed model consistently
outperforms state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?. (arXiv:2110.11929v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11929">
<div class="article-summary-box-inner">
<span><p>A principle behind dozens of attribution methods is to take the prediction
difference between before-and-after an input feature (here, a token) is removed
as its attribution. A popular Input Marginalization (IM) method (Kim et al.,
2020) uses BERT to replace a token, yielding more plausible counterfactuals.
While Kim et al. (2020) reported that IM is effective, we find this conclusion
not convincing as the DeletionBERT metric used in their paper is biased towards
IM. Importantly, this bias exists in Deletion-based metrics, including
Insertion, Sufficiency, and Comprehensiveness. Furthermore, our rigorous
evaluation using 6 metrics and 3 datasets finds no evidence that IM is better
than a Leave-One-Out (LOO) baseline. We find two reasons why IM is not better
than LOO: (1) deleting a single word from the input only marginally reduces a
classifier's accuracy; and (2) a highly predictable word is always given
near-zero attribution, regardless of its true importance to the classifier. In
contrast, making LIME samples more natural via BERT consistently improves LIME
accuracy under several ROAR metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation. (arXiv:2112.02721v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02721">
<div class="article-summary-box-inner">
<span><p>Data augmentation is an important component in the robustness evaluation of
models in natural language processing (NLP) and in enhancing the diversity of
the data they are trained on. In this paper, we present NL-Augmenter, a new
participatory Python-based natural language augmentation framework which
supports the creation of both transformations (modifications to the data) and
filters (data splits according to specific features). We describe the framework
and an initial set of 117 transformations and 23 filters for a variety of
natural language tasks. We demonstrate the efficacy of NL-Augmenter by using
several of its transformations to analyze the robustness of popular natural
language models. The infrastructure, datacards and robustness analysis results
are available publicly on the NL-Augmenter repository
(\url{https://github.com/GEM-benchmark/NL-Augmenter}).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Instance Training for Question Answering Across Table and Linked Text. (arXiv:2112.07337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07337">
<div class="article-summary-box-inner">
<span><p>Answering natural language questions using information from tables (TableQA)
is of considerable recent interest. In many applications, tables occur not in
isolation, but embedded in, or linked to unstructured text. Often, a question
is best answered by matching its parts to either table cell contents or
unstructured text spans, and extracting answers from either source. This leads
to a new space of TextTableQA problems that was introduced by the HybridQA
dataset. Existing adaptations of table representation to transformer-based
reading comprehension (RC) architectures fail to tackle the diverse modalities
of the two representations through a single system. Training such systems is
further challenged by the need for distant supervision. To reduce cognitive
burden, training instances usually include just the question and answer, the
latter matching multiple table rows and text passages. This leads to a noisy
multi-instance training regime involving not only rows of the table, but also
spans of linked text. We respond to these challenges by proposing MITQA, a new
TextTableQA system that explicitly models the different but closely-related
probability spaces of table row selection and text span selection. Our
experiments indicate the superiority of our approach compared to recent
baselines. The proposed method is currently at the top of the HybridQA
leaderboard with a held out test set, achieving 21 % absolute improvement on
both EM and F1 scores over previous published results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03335">
<div class="article-summary-box-inner">
<span><p>We present an open-source and extensible knowledge extraction toolkit DeepKE,
supporting complicated low-resource, document-level and multimodal scenarios in
the knowledge base population. DeepKE implements various information extraction
tasks, including named entity recognition, relation extraction and attribute
extraction. With a unified framework, DeepKE allows developers and researchers
to customize datasets and models to extract information from unstructured data
according to their requirements. Specifically, DeepKE not only provides various
functional modules and model implementation for different tasks and scenarios
but also organizes all components by consistent frameworks to maintain
sufficient modularity and extensibility. We release the source code at GitHub
in https://github.com/zjunlp/DeepKE with Google Colab tutorials and
comprehensive documents for beginners. Besides, we present an online system in
<a href="http://deepke.openkg.cn/EN/re_doc_show.html">this http URL</a> for real-time extraction of various
tasks, and a demo video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11903">
<div class="article-summary-box-inner">
<span><p>We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Training of Both Translation Models in the Back-Translation Framework. (arXiv:2202.08465v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08465">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning algorithms in neural machine translation (NMT) have
significantly improved translation quality compared to the supervised learning
methods by using additional monolingual corpora. Among them, back-translation
is a theoretically well-structured and cutting-edge method. Given two
pre-trained NMT models between source and target languages, one NMT model
translates a monolingual sentence to a latent sentence, and the other
reconstructs the monolingual input sentence given the latent sentence. Based on
this auto-encoding framework, previous work tried to apply the variational
auto-encoder's (VAE) training framework to the back-translation. However, the
discrete property of the latent sentence made it impossible to use
backpropagation in the end-to-end fashion. In this paper, we propose a {\it
categorical reparameterization trick} that makes NMT models generate {\it
differentiable sentences}. Based on the proposed method, end-to-end learning is
possible so that two NMT models for the back-translation can be trained as a
unified model. In addition, we propose several regularization techniques that
are especially advantageous to this framework. Our experiments demonstrate that
our method can achieve better BLEU scores than the previous baseline, on the
datasets of the WMT18 translation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-LID: Leveraging BERT to Improve Spoken Language Identification. (arXiv:2203.00328v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00328">
<div class="article-summary-box-inner">
<span><p>Language identification is the task of automatically determining the identity
of a language conveyed by a spoken segment. It has a profound impact on the
multilingual interoperability of an intelligent speech system. Despite language
identification attaining high accuracy on medium or long utterances(&gt;3s), the
performance on short utterances (&lt;=1s) is still far from satisfactory. We
propose a BERT-based language identification system (BERT-LID) to improve
language identification performance, especially on short-duration speech
segments. We extend the original BERT model by taking the phonetic
posteriorgrams (PPG) derived from the front-end phone recognizer as input. Then
we deployed the optimal deep classifier followed by it for language
identification. Our BERT-LID model can improve the baseline accuracy by about
6.5% on long-segment identification and 19.9% on short-segment identification,
demonstrating our BERT-LID's effectiveness to language identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach. (arXiv:2203.08383v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08383">
<div class="article-summary-box-inner">
<span><p>While Pre-trained Language Models (PLMs) internalize a great amount of world
knowledge, they have been shown incapable of recalling these knowledge to solve
tasks requiring complex &amp; multi-step inference procedures. Similar to how
humans develop a "train of thought" for these tasks, how can we equip PLMs with
such abilities? In this work, we explore an iterative prompting framework, a
new prompting paradigm which progressively elicits relevant knowledge from PLMs
for multi-step inference tasks. We identify key limitations of existing
prompting methods, namely they are either restricted to queries with a single
identifiable relation/predicate, or being agnostic to input contexts, which
makes it difficult to capture variabilities across different inference steps.
We propose an iterative context-aware prompter, which addresses these
limitations by learning to dynamically synthesize prompts conditioned on the
current step's contexts. Experiments on three datasets involving multi-step
inference show the effectiveness of the iterative scheme and our proposed
prompter design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05232">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is a natural language processing
problem that requires analyzing user-generated reviews to determine: a) The
target entity being reviewed, b) The high-level aspect to which it belongs, and
c) The sentiment expressed toward the targets and the aspects. Numerous yet
scattered corpora for ABSA make it difficult for researchers to identify
corpora best suited for a specific ABSA subtask quickly. This study aims to
present a database of corpora that can be used to train and assess autonomous
ABSA systems. Additionally, we provide an overview of the major corpora for
ABSA and its subtasks and highlight several features that researchers should
consider when selecting a corpus. We conclude that further large-scale ABSA
corpora are required. Additionally, because each corpus is constructed
differently, it is time-consuming for researchers to experiment with a novel
ABSA model on multiple corpora and often employ just one or a few corpora. The
field would benefit from an agreement on a data standard for ABSA corpora.
Finally, we discuss the advantages and disadvantages of current collection
approaches and make recommendations for future ABSA dataset gathering. This
survey examines 65 publicly available ABSA datasets covering more than 25
domains, including 45 English and 20 other languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Model for Reverse Dictionary and Definition Modelling. (arXiv:2205.04602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04602">
<div class="article-summary-box-inner">
<span><p>We build a dual-way neural dictionary to retrieve words given definitions,
and produce definitions for queried words. The model learns the two tasks
simultaneously and handles unknown words via embeddings. It casts a word or a
definition to the same representation space through a shared layer, then
generates the other form in a multi-task fashion. Our method achieves promising
automatic scores on previous benchmarks without extra resources. Human
annotators prefer the model's outputs in both reference-less and
reference-based evaluation, indicating its practicality. Analysis suggests that
multiple objectives benefit learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05055">
<div class="article-summary-box-inner">
<span><p>Large transformer-based models are able to perform in-context few-shot
learning, without being explicitly trained for it. This observation raises the
question: what aspects of the training regime lead to this emergent behavior?
Here, we show that this behavior is driven by the distributions of the training
data itself. In-context learning emerges when the training data exhibits
particular distributional properties such as burstiness (items appear in
clusters rather than being uniformly distributed over time) and having large
numbers of rarely occurring classes. In-context learning also emerges more
strongly when item meanings or interpretations are dynamic rather than fixed.
These properties are exemplified by natural language, but are also inherent to
naturalistic data in a wide range of other domains. They also depart
significantly from the uniform, i.i.d. training distributions typically used
for standard supervised learning. In our initial experiments, we found that
in-context learning traded off against more conventional weight-based learning,
and models were unable to achieve both simultaneously. However, our later
experiments uncovered that the two modes of learning could co-exist in a single
model when it was trained on data following a skewed Zipfian distribution --
another common property of naturalistic data, including language. In further
experiments, we found that naturalistic data distributions were only able to
elicit in-context learning in transformers, and not in recurrent models. In
sum, our findings indicate how the transformer architecture works together with
particular properties of the training data to drive the intriguing emergent
in-context learning behaviour of large language models, and how future work
might encourage both in-context and in-weights learning in domains beyond
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions. (arXiv:2205.11658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11658">
<div class="article-summary-box-inner">
<span><p>Generics express generalizations about the world (e.g., birds can fly) that
are not universally true (e.g., newborn birds and penguins cannot fly).
Commonsense knowledge bases, used extensively in NLP, encode some generic
knowledge but rarely enumerate such exceptions and knowing when a generic
statement holds or does not hold true is crucial for developing a comprehensive
understanding of generics. We present a novel framework informed by linguistic
theory to generate Exemplars -- specific cases when a generic holds true or
false. We generate ${\sim}19k$ exemplars for ${\sim}650$ generics and show that
our framework outperforms a strong GPT-3 baseline by $12.8$ precision points.
Our analysis highlights the importance of linguistic theory-based
controllability for generating exemplars, the insufficiency of knowledge bases
as a source of exemplars, and the challenges exemplars pose for the task of
natural language inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmark Data and Evaluation Framework for Intent Discovery Around COVID-19 Vaccine Hesitancy. (arXiv:2205.11966v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11966">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has made a huge global impact and cost millions of
lives. As COVID-19 vaccines were rolled out, they were quickly met with
widespread hesitancy. To address the concerns of hesitant people, we launched
VIRA, a public dialogue system aimed at addressing questions and concerns
surrounding the COVID-19 vaccines. Here, we release VIRADialogs, a dataset of
over 8k dialogues conducted by actual users with VIRA, providing a unique
real-world conversational dataset. In light of rapid changes in users' intents,
due to updates in guidelines or in response to new information, we highlight
the important task of intent discovery in this use-case. We introduce a novel
automatic evaluation framework for intent discovery, leveraging the existing
intent classifier of VIRA. We use this framework to report baseline intent
discovery results over VIRADialogs, that highlight the difficulty of this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuned Language Models are Continual Learners. (arXiv:2205.12393v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12393">
<div class="article-summary-box-inner">
<span><p>Recent work on large language models relies on the intuition that most
natural language processing tasks can be described via natural language
instructions. Language models trained on these instructions show strong
zero-shot performance on several standard datasets. However, these models even
though impressive still perform poorly on a wide range of tasks outside of
their respective training and evaluation sets. To address this limitation, we
argue that a model should be able to keep extending its knowledge and
abilities, without forgetting previous skills. In spite of the limited success
of Continual Learning we show that Language Models can be continual learners.
We empirically investigate the reason for this success and conclude that
Continual Learning emerges from self-supervision pre-training. Our resulting
model Continual-T0 (CT0) is able to learn diverse new tasks, while still
maintaining good performance on previous tasks, spanning remarkably through 70
datasets in total. Finally, we show that CT0 is able to combine instructions in
ways it was never trained for, demonstrating some compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14704">
<div class="article-summary-box-inner">
<span><p>Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09674">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) in long horizon and sparse reward tasks is
notoriously difficult and requires a lot of training steps. A standard solution
to speed up the process is to leverage additional reward signals, shaping it to
better guide the learning process. In the context of language-conditioned RL,
the abstraction and generalisation properties of the language input provide
opportunities for more efficient ways of shaping the reward. In this paper, we
leverage this idea and propose an automated reward shaping method where the
agent extracts auxiliary objectives from the general language goal. These
auxiliary objectives use a question generation (QG) and question answering (QA)
system: they consist of questions leading the agent to try to reconstruct
partial information about the global goal using its own trajectory. When it
succeeds, it receives an intrinsic reward proportional to its confidence in its
answer. This incentivizes the agent to generate trajectories which
unambiguously explain various aspects of the general language goal. Our
experimental study shows that this approach, which does not require engineer
intervention to design the auxiliary objectives, improves sample efficiency by
effectively directing exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogID: A Dialogic Instruction Dataset for Improving Teaching Effectiveness in Online Environments. (arXiv:2206.12034v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12034">
<div class="article-summary-box-inner">
<span><p>Online dialogic instructions are a set of pedagogical instructions used in
real-world online educational contexts to motivate students, help understand
learning materials, and build effective study habits. In spite of the
popularity and advantages of online learning, the education technology and
educational data mining communities still suffer from the lack of large-scale,
high-quality, and well-annotated teaching instruction datasets to study
computational approaches to automatically detect online dialogic instructions
and further improve the online teaching effectiveness. Therefore, in this
paper, we present a dataset of online dialogic instruction detection,
\textsc{DialogID}, which contains 30,431 effective dialogic instructions. These
teaching instructions are well annotated into 8 categories. Furthermore, we
utilize the prevalent pre-trained language models (PLMs) and propose a simple
yet effective adversarial training learning paradigm to improve the quality and
generalization of dialogic instruction detection. Extensive experiments
demonstrate that our approach outperforms a wide range of baseline methods. The
data and our code are available for research purposes from:
\url{https://github.com/ai4ed/DialogID}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12576">
<div class="article-summary-box-inner">
<span><p>While vision-and-language models perform well on tasks such as visual
question answering, they struggle when it comes to basic human commonsense
reasoning skills. In this work, we introduce WinoGAViL: an online game of
vision-and-language associations (e.g., between werewolves and a full moon),
used as a dynamic evaluation benchmark. Inspired by the popular card game
Codenames, a spymaster gives a textual cue related to several visual
candidates, and another player tries to identify them. Human players are
rewarded for creating associations that are challenging for a rival AI model
but still solvable by other human players. We use the game to collect 3.5K
instances, finding that they are intuitive for humans (&gt;90% Jaccard index) but
challenging for state-of-the-art AI models, where the best model (ViLT)
achieves a score of 52%, succeeding mostly where the cue is visually salient.
Our analysis as well as the feedback we collect from players indicate that the
collected associations require diverse reasoning skills, including general
knowledge, common sense, abstraction, and more. We release the dataset, the
code and the interactive game, allowing future data collection that can be used
to develop models with better association abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training. (arXiv:2208.08749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08749">
<div class="article-summary-box-inner">
<span><p>To mitigate the impact of the scarcity of labelled data on fact-checking
systems, we focus on few-shot claim verification. Despite recent work on
few-shot classification by proposing advanced language models, there is a
dearth of research in data annotation prioritisation that improves the
selection of the few shots to be labelled for optimal model performance. We
propose Active PETs, a novel weighted approach that utilises an ensemble of
Pattern Exploiting Training (PET) models based on various language models, to
actively select unlabelled data as candidates for annotation. Using Active PETs
for few-shot data selection shows consistent improvement over the baseline
methods, on two technical fact-checking datasets and using six different
pretrained language models. We show further improvement with Active PETs-o,
which further integrates an oversampling strategy. Our approach enables
effective selection of instances to be labelled where unlabelled data is
abundant but resources for labelling are limited, leading to consistently
improved few-shot claim verification performance. Our code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoHS-CQG: Context and History Selection for Conversational Question Generation. (arXiv:2209.06652v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06652">
<div class="article-summary-box-inner">
<span><p>Conversational question generation (CQG) serves as a vital task for machines
to assist humans, such as interactive reading comprehension, through
conversations. Compared to traditional single-turn question generation (SQG),
CQG is more challenging in the sense that the generated question is required
not only to be meaningful, but also to align with the occurred conversation
history. While previous studies mainly focus on how to model the flow and
alignment of the conversation, there has been no thorough study to date on
which parts of the context and history are necessary for the model. We argue
that shortening the context and history is crucial as it can help the model to
optimise more on the conversational alignment property. To this end, we propose
CoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the
context and history of the input. In particular, CoHS selects contiguous
sentences and history turns according to their relevance scores by a top-p
strategy. Our model achieves state-of-the-art performances on CoQA in both the
answer-aware and answer-unaware settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible and Structured Knowledge Grounded Question Answering. (arXiv:2209.08284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.08284">
<div class="article-summary-box-inner">
<span><p>Can language models (LM) ground question-answering (QA) tasks in the
knowledge base via inherent relational reasoning ability? While previous models
that use only LMs have seen some success on many QA tasks, more recent methods
include knowledge graphs (KG) to complement LMs with their more logic-driven
implicit knowledge. However, effectively extracting information from structured
data, like KGs, empowers LMs to remain an open question, and current models
rely on graph techniques to extract knowledge. In this paper, we propose to
solely leverage the LMs to combine the language and knowledge for knowledge
based question-answering with flexibility, breadth of coverage and structured
reasoning. Specifically, we devise a knowledge construction method that
retrieves the relevant context with a dynamic hop, which expresses more
comprehensivenes than traditional GNN-based techniques. And we devise a deep
fusion mechanism to further bridge the information exchanging bottleneck
between the language and the knowledge. Extensive experiments show that our
model consistently demonstrates its state-of-the-art performance over
CommensenseQA benchmark, showcasing the possibility to leverage LMs solely to
robustly ground QA into the knowledge base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Causal Analysis into Diversified and Logical Response Generation. (arXiv:2209.09482v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.09482">
<div class="article-summary-box-inner">
<span><p>Although the Conditional Variational AutoEncoder (CVAE) model can generate
more diversified responses than the traditional Seq2Seq model, the responses
often have low relevance with the input words or are illogical with the
question. A causal analysis is carried out to study the reasons behind, and a
methodology of searching for the mediators and mitigating the confounding bias
in dialogues is provided. Specifically, we propose to predict the mediators to
preserve relevant information and auto-regressively incorporate the mediators
into generating process. Besides, a dynamic topic graph guided conditional
variational autoencoder (TGG-CVAE) model is utilized to complement the semantic
space and reduce the confounding bias in responses. Extensive experiments
demonstrate that the proposed model is able to generate both relevant and
informative responses, and outperforms the state-of-the-art in terms of
automatic metrics and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification. (arXiv:2209.13017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13017">
<div class="article-summary-box-inner">
<span><p>Social media has become the fulcrum of all forms of communication.
Classifying social texts such as fake news, rumour, sarcasm, etc. has gained
significant attention. The surface-level signals expressed by a social-text
itself may not be adequate for such tasks; therefore, recent methods attempted
to incorporate other intrinsic signals such as user behavior and the underlying
graph structure. Oftentimes, the `public wisdom' expressed through the
comments/replies to a social-text acts as a surrogate of crowd-sourced view and
may provide us with complementary signals. State-of-the-art methods on
social-text classification tend to ignore such a rich hierarchical signal.
Here, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention
network. Hyphen is a fusion of hyperbolic graph representation learning with a
novel Fourier co-attention mechanism in an attempt to generalise the
social-text classification tasks by incorporating public discourse. We parse
public discourse as an Abstract Meaning Representation (AMR) graph and use the
powerful hyperbolic geometric representation to model graphs with hierarchical
structure. Finally, we equip it with a novel Fourier co-attention mechanism to
capture the correlation between the source post and public discourse. Extensive
experiments on four different social-text classification tasks, namely
detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen
generalises well, and achieves state-of-the-art results on ten benchmark
datasets. We also employ a sentence-level fact-checked and annotated dataset to
evaluate how Hyphen is capable of producing explanations as analogous evidence
to the final prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Community Learning: Understanding A Community Through NLP for Positive Impact. (arXiv:2210.00590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00590">
<div class="article-summary-box-inner">
<span><p>A post-pandemic world resulted in economic upheaval, particularly for the
cities' communities. While significant work in NLP4PI focuses on national and
international events, there is a gap in bringing such state-of-the-art methods
into the community development field. In order to help with community
development, we must learn about the communities we develop. To that end, we
propose the task of community learning as a computational task of extracting
natural language data about the community, transforming and loading it into a
suitable knowledge graph structure for further downstream applications. We
study two particular cases of homelessness and education in showing the
visualization capabilities of a knowledge graph, and also discuss other
usefulness such a model can provide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U3E: Unsupervised and Erasure-based Evidence Extraction for Machine Reading Comprehension. (arXiv:2210.02621v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02621">
<div class="article-summary-box-inner">
<span><p>More tasks in Machine Reading Comprehension(MRC) require, in addition to
answer prediction, the extraction of evidence sentences that support the
answer. However, the annotation of supporting evidence sentences is usually
time-consuming and labor-intensive. In this paper, to address this issue and
considering that most of the existing extraction methods are semi-supervised,
we propose an unsupervised evidence extraction method (U3E). U3E takes the
changes after sentence-level feature erasure in the document as input,
simulating the decline in problem-solving ability caused by human memory
decline. In order to make selections on the basis of fully understanding the
semantics of the original text, we also propose metrics to quickly select the
optimal memory model for this input changes. To compare U3E with typical
evidence extraction methods and investigate its effectiveness in evidence
extraction, we conduct experiments on different datasets. Experimental results
show that U3E is simple but effective, not only extracting evidence more
accurately, but also significantly improving model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners. (arXiv:2210.02969v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02969">
<div class="article-summary-box-inner">
<span><p>Meta-training, which fine-tunes the language model (LM) on various downstream
tasks by maximizing the likelihood of the target label given the task
instruction and input instance, has improved the zero-shot task generalization
performance. However, meta-trained LMs still struggle to generalize to
challenging tasks containing novel labels unseen during meta-training. In this
paper, we propose Flipped Learning, an alternative method of meta-training
which trains the LM to generate the task instruction given the input instance
and label. During inference, the LM trained with Flipped Learning, referred to
as Flipped, selects the label option that is most likely to generate the task
instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped
outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on
average by 8.4% and 9.7% points, respectively. Flipped gives particularly large
improvements on unseen labels, outperforming T0-11B by up to +20% average F1
score. This indicates that the strong task generalization of Flipped comes from
improved generalization to novel labels. We release our code at
https://github.com/seonghyeonye/Flipped-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization. (arXiv:2210.03029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03029">
<div class="article-summary-box-inner">
<span><p>During zero-shot inference with language models (LMs), using hard prompts
alone may not be able to fully describe the target task. In this paper, we
explore how the retrieval of soft prompts obtained through prompt tuning can
assist hard prompts in zero-shot task generalization. Specifically, we train
soft prompt embeddings for each prompt through prompt tuning, store the samples
of the training instances (hard prompt + input instances) mapped with the
prompt embeddings, and retrieve the corresponding prompt embedding of the
training instance closest to the query instance during inference. Results show
this simple approach enhances the performance of T0 on unseen tasks by
outperforming it on 10 out of 11 datasets as well as improving the mean
accuracy of T0 on BIG-bench benchmark by 2.39% points while adding only 0.007%
additional parameters. Also, using interpolation of multiple embeddings and
variance-based ranking further improve accuracy and robustness to different
evaluation prompts, widening the performance gap. Finally, we find that
retrieving source embeddings trained on similar answer choice formats is more
important than those on similar task types. Model checkpoints and code
implementation are available at https://github.com/seonghyeonye/RoSPr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Transformer Memorization Recall Through Idioms. (arXiv:2210.03588v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03588">
<div class="article-summary-box-inner">
<span><p>To produce accurate predictions, language models (LMs) must balance between
generalization and memorization. Yet, little is known about the mechanism by
which transformer LMs employ their memorization capacity. When does a model
decide to output a memorized phrase, and how is this phrase then retrieved from
memory? In this work, we offer the first methodological framework for probing
and characterizing recall of memorized sequences in transformer LMs. First, we
lay out criteria for detecting model inputs that trigger memory recall, and
propose idioms as inputs that fulfill these criteria. Next, we construct a
dataset of English idioms and use it to compare model behavior on memorized vs.
non-memorized inputs. Specifically, we analyze the internal prediction
construction process by interpreting the model's hidden representations as a
gradual refinement of the output probability distribution. We find that across
different model sizes and architectures, memorized predictions are a two-step
process: early layers promote the predicted token to the top of the output
distribution, and upper layers increase model confidence. This suggests that
memorized information is stored and retrieved in the early layers of the
network. Last, we demonstrate the utility of our methodology beyond idioms in
memorized factual statements. Overall, our work makes a first step towards
understanding memory recall, and provides a methodological basis for future
studies of transformer memorization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT. (arXiv:2210.04186v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04186">
<div class="article-summary-box-inner">
<span><p>We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04284">
<div class="article-summary-box-inner">
<span><p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only
fine-tunes a few extra modules, becomes an appealing efficient alternative to
the full model fine-tuning. Although computationally efficient, the recent
Adapters often increase parameters (e.g. bottleneck dimension) for matching the
performance of full model fine-tuning, which we argue goes against their
original intention. In this work, we re-examine the parameter-efficiency of
Adapters through the lens of network pruning (we name such plug-in concept as
\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or
better performance than standard Adapters when the sparse ratio reaches up to
80\%. Based on our findings, we introduce an easy but effective setting
``\textit{Large-Sparse}'' to improve the model capacity of Adapters under the
same parameter budget. Experiments on five competitive Adapters upon three
advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.
40\%) SparseAdapter can consistently outperform their corresponding
counterpart. Encouragingly, with the \textit{Large-Sparse} setting, we can
obtain further appealing gains, even outperforming the full fine-tuning by a
large margin. Our code will be released at:
\url{https://github.com/Shwai-He/SparseAdapter}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models. (arXiv:2210.04325v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04325">
<div class="article-summary-box-inner">
<span><p>Data-to-text generation is challenging due to the great variety of the input
data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse
predicates). Recent end-to-end neural methods thus require substantial training
examples to learn to disambiguate and describe the data. Yet, real-world
data-to-text problems often suffer from various data-scarce issues: one may
have access to only a handful of or no training examples, and/or have to rely
on examples in a different domain or schema. To fill this gap, we propose
Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse
settings by making efficient use of any given (or no) examples. ASDOT consists
of two steps, data disambiguation and sentence fusion, both of which are
amenable to be solved with off-the-shelf pretrained language models (LMs) with
optional finetuning. In the data disambiguation stage, we employ the prompted
GPT-3 model to understand possibly ambiguous triples from the input data and
convert each into a short sentence with reduced ambiguity. The sentence fusion
stage then uses an LM like T5 to fuse all the resulting sentences into a
coherent paragraph as the final description. We evaluate extensively on various
datasets in different scenarios, including the zero-/few-/full-shot settings,
and generalization to unseen predicates and out-of-domain data. Experimental
results show that ASDOT consistently achieves significant improvement over
baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training. (arXiv:2210.04525v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04525">
<div class="article-summary-box-inner">
<span><p>The conventional success of textual classification relies on annotated data,
and the new paradigm of pre-trained language models (PLMs) still requires a few
labeled data for downstream tasks. However, in real-world applications, label
noise inevitably exists in training data, damaging the effectiveness,
robustness, and generalization of the models constructed on such data.
Recently, remarkable achievements have been made to mitigate this dilemma in
visual data, while only a few explore textual data. To fill this gap, we
present SelfMix, a simple yet effective method, to handle label noise in text
classification tasks. SelfMix uses the Gaussian Mixture Model to separate
samples and leverages semi-supervised learning. Unlike previous works requiring
multiple models, our method utilizes the dropout mechanism on a single model to
reduce the confirmation bias in self-training and introduces a textual-level
mixup training strategy. Experimental results on three text classification
benchmarks with different types of text show that the performance of our
proposed method outperforms these strong baselines designed for both textual
and visual data under different noise ratios and noise types. Our code is
available at \url{https://github.com/noise-learning/SelfMix}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter. (arXiv:2210.04710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04710">
<div class="article-summary-box-inner">
<span><p>The widespread diffusion of medical and political claims in the wake of
COVID-19 has led to a voluminous rise in misinformation and fake news. The
current vogue is to employ manual fact-checkers to efficiently classify and
verify such data to combat this avalanche of claim-ridden misinformation.
However, the rate of information dissemination is such that it vastly outpaces
the fact-checkers' strength. Therefore, to aid manual fact-checkers in
eliminating the superfluous content, it becomes imperative to automatically
identify and extract the snippets of claim-worthy (mis)information present in a
post. In this work, we introduce the novel task of Claim Span Identification
(CSI). We propose CURT, a large-scale Twitter corpus with token-level claim
spans on more than 7.5k tweets. Furthermore, along with the standard token
classification baselines, we benchmark our dataset with DABERTa, an
adapter-based variation of RoBERTa. The experimental results attest that
DABERTa outperforms the baseline systems across several evaluation metrics,
improving by about 1.5 points. We also report detailed error analysis to
validate the model's performance along with the ablation studies. Lastly, we
release our comprehensive span annotation guidelines for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks. (arXiv:2210.04834v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04834">
<div class="article-summary-box-inner">
<span><p>Teacher-student knowledge distillation is a popular technique for compressing
today's prevailing large language models into manageable sizes that fit
low-latency downstream applications. Both the teacher and the choice of
transfer set used for distillation are crucial ingredients in creating a high
quality student. Yet, the generic corpora used to pretrain the teacher and the
corpora associated with the downstream target domain are often significantly
different, which raises a natural question: should the student be distilled
over the generic corpora, so as to learn from high-quality teacher predictions,
or over the downstream task corpora to align with finetuning? Our study
investigates this trade-off using Domain Classification (DC) and Intent
Classification/Named Entity Recognition (ICNER) as downstream tasks. We distill
several multilingual students from a larger multilingual LM with varying
proportions of generic and task-specific datasets, and report their performance
after finetuning on DC and ICNER. We observe significant improvements across
tasks and test sets when only task-specific corpora is used. We also report on
how the impact of adding task-specific data to the transfer set correlates with
the similarity between generic and task-specific data. Our results clearly
indicate that, while distillation from a generic LM benefits downstream tasks,
students learn better using target domain data even if it comes at the price of
noisier teacher predictions. In other words, target domain data still trumps
teacher knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What the DAAM: Interpreting Stable Diffusion Using Cross Attention. (arXiv:2210.04885v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04885">
<div class="article-summary-box-inner">
<span><p>Large-scale diffusion neural networks represent a substantial milestone in
text-to-image generation, with some performing similar to real photographs in
human evaluation. However, they remain poorly understood, lacking
explainability and interpretability analyses, largely due to their proprietary,
closed-source nature. In this paper, to shine some much-needed light on
text-to-image diffusion models, we perform a text-image attribution analysis on
Stable Diffusion, a recently open-sourced large diffusion model. To produce
pixel-level attribution maps, we propose DAAM, a novel method based on
upscaling and aggregating cross-attention activations in the latent denoising
subnetwork. We support its correctness by evaluating its unsupervised semantic
segmentation quality on its own generated imagery, compared to supervised
segmentation models. We show that DAAM performs strongly on COCO
caption-generated images, achieving an mIoU of 61.0, and it outperforms
supervised models on open-vocabulary segmentation, for an mIoU of 51.5. We
further find that certain parts of speech, like punctuation and conjunctions,
influence the generated imagery most, which agrees with the prior literature,
while determiners and numerals the least, suggesting poor numeracy. To our
knowledge, we are the first to propose and study word-pixel attribution for
large-scale text-to-image diffusion models. Our code and data are at
https://github.com/castorini/daam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Label Errors in Token Classification Data. (arXiv:2210.03920v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03920">
<div class="article-summary-box-inner">
<span><p>Mislabeled examples are a common issue in real-world data, particularly for
tasks like token classification where many labels must be chosen on a
fine-grained basis. Here we consider the task of finding sentences that contain
label errors in token classification datasets. We study 11 different
straightforward methods that score tokens/sentences based on the predicted
class probabilities output by a (any) token classification model (trained via
any procedure). In precision-recall evaluations based on real-world label
errors in entity recognition data from CoNLL-2003, we identify a simple and
effective method that consistently detects those sentences containing label
errors when applied with different token classification models.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-12 23:19:50.779830665 UTC">2022-10-12 23:19:50 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>