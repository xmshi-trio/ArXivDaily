<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-14T01:30:00Z">06-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification. (arXiv:2306.07297v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07297">
<div class="article-summary-box-inner">
<span><p>The identification of key factors such as medications, diseases, and
relationships within electronic health records and clinical notes has a wide
range of applications in the clinical field. In the N2C2 2022 competitions,
various tasks were presented to promote the identification of key factors in
electronic health records (EHRs) using the Contextualized Medication Event
Dataset (CMED). Pretrained large language models (LLMs) demonstrated
exceptional performance in these tasks. This study aims to explore the
utilization of LLMs, specifically ChatGPT, for data augmentation to overcome
the limited availability of annotated data for identifying the key factors in
EHRs. Additionally, different pre-trained BERT models, initially trained on
extensive datasets like Wikipedia and MIMIC, were employed to develop models
for identifying these key variables in EHRs through fine-tuning on augmented
datasets. The experimental results of two EHR analysis tasks, namely medication
identification and medication event classification, indicate that data
augmentation based on ChatGPT proves beneficial in improving performance for
both medication identification and medication event classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Experiencing Misrecognition by Teachable Agents on Learning and Rapport. (arXiv:2306.07302v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07302">
<div class="article-summary-box-inner">
<span><p>While speech-enabled teachable agents have some advantages over typing-based
ones, they are vulnerable to errors stemming from misrecognition by automatic
speech recognition (ASR). These errors may propagate, resulting in unexpected
changes in the flow of conversation. We analyzed how such changes are linked
with learning gains and learners' rapport with the agents. Our results show
they are not related to learning gains or rapport, regardless of the types of
responses the agents should have returned given the correct input from learners
without ASR errors. We also discuss the implications for optimal error-recovery
policies for teachable agents that can be drawn from these findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks. (arXiv:2306.07303v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07303">
<div class="article-summary-box-inner">
<span><p>Transformer is a deep neural network that employs a self-attention mechanism
to comprehend the contextual relationships within sequential data. Unlike
conventional neural networks or updated versions of Recurrent Neural Networks
(RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in
handling long dependencies between input sequence elements and enable parallel
processing. As a result, transformer-based models have attracted substantial
interest among researchers in the field of artificial intelligence. This can be
attributed to their immense potential and remarkable achievements, not only in
Natural Language Processing (NLP) tasks but also in a wide range of domains,
including computer vision, audio and speech processing, healthcare, and the
Internet of Things (IoT). Although several survey papers have been published
highlighting the transformer's contributions in specific fields, architectural
differences, or performance evaluations, there is still a significant absence
of a comprehensive survey paper encompassing its major applications across
various domains. Therefore, we undertook the task of filling this gap by
conducting an extensive survey of proposed transformer models from 2017 to
2022. Our survey encompasses the identification of the top five application
domains for transformer-based models, namely: NLP, Computer Vision,
Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze
the impact of highly influential transformer-based models in these domains and
subsequently classify them based on their respective tasks using a proposed
taxonomy. Our aim is to shed light on the existing potential and future
possibilities of transformers for enthusiastic researchers, thus contributing
to the broader understanding of this groundbreaking technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing. (arXiv:2306.07373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07373">
<div class="article-summary-box-inner">
<span><p>The utilization of clinical reports for various secondary purposes, including
health research and treatment monitoring, is crucial for enhancing patient
care. Natural Language Processing (NLP) tools have emerged as valuable assets
for extracting and processing relevant information from these reports. However,
the availability of specialized language models for the clinical domain in
Spanish has been limited.
</p>
<p>In this paper, we introduce EriBERTa, a bilingual domain-specific language
model pre-trained on extensive medical and clinical corpora. We demonstrate
that EriBERTa outperforms previous Spanish language models in the clinical
domain, showcasing its superior capabilities in understanding medical texts and
extracting meaningful information. Moreover, EriBERTa exhibits promising
transfer learning abilities, allowing for knowledge transfer from one language
to another. This aspect is particularly beneficial given the scarcity of
Spanish clinical data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lost in Translation: Large Language Models in Non-English Content Analysis. (arXiv:2306.07377v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07377">
<div class="article-summary-box-inner">
<span><p>In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,
Google's PaLM) have become the dominant approach for building AI systems to
analyze and generate language online. However, the automated systems that
increasingly mediate our interactions online -- such as chatbots, content
moderation systems, and search engines -- are primarily designed for and work
far more effectively in English than in the world's other 7,000 languages.
Recently, researchers and technology companies have attempted to extend the
capabilities of large language models into languages other than English by
building what are called multilingual language models.
</p>
<p>In this paper, we explain how these multilingual language models work and
explore their capabilities and limits. Part I provides a simple technical
explanation of how large language models work, why there is a gap in available
data between English and other languages, and how multilingual language models
attempt to bridge that gap. Part II accounts for the challenges of doing
content analysis with large language models in general and multilingual
language models in particular. Part III offers recommendations for companies,
researchers, and policymakers to keep in mind when considering researching,
developing and deploying large and multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07384">
<div class="article-summary-box-inner">
<span><p>With their increasing size, Large language models (LLMs) are becoming
increasingly good at language understanding tasks. But even with high
performance on specific downstream task, LLMs fail at simple linguistic tests
for negation or quantifier understanding. Previous work on testing capability
of LLMs on understanding quantifiers suggest that as the size of the models
increase, they get better at understanding most-type quantifiers but get
increasingly worse at understanding few-type quantifiers, thus presenting a
case of an inverse-scaling law. In this paper, we question the claims of
inverse scaling of few-type quantifier understanding in LLMs and show that it
is a result of inappropriate testing methodology. We also present alternate
methods to measure quantifier comprehension in LLMs and show that as the size
of the models increase, these behaviours are different from what is shown in
previous research. LLMs are consistently able to understand the difference
between the meaning of few-type and most-type quantifiers, but when a
quantifier is added to phrase, LLMs do not always take into account the meaning
of the quantifier. We in fact see an inverse scaling law for most-type
quantifiers, which is contrary to human psycho-linguistic experiments and
previous work, where the model's understanding of most-type quantifier gets
worse as the model size increases. We do this evaluation on models ranging from
125M-175B parameters, which suggests that LLMs do not do as well as expected
with quantifiers and statistical co-occurrence of words still takes precedence
over word meaning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT. (arXiv:2306.07401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07401">
<div class="article-summary-box-inner">
<span><p>The abundance of information on social media has increased the necessity of
accurate real-time rumour detection. Manual techniques of identifying and
verifying fake news generated by AI tools are impracticable and time-consuming
given the enormous volume of information generated every day. This has sparked
an increase in interest in creating automated systems to find fake news on the
Internet. The studies in this research demonstrate that the BERT and RobertA
models with fine-tuning had the best success in detecting AI generated news.
With a score of 98%, tweaked RobertA in particular showed excellent precision.
In conclusion, this study has shown that neural networks can be used to
identify bogus news AI generation news created by ChatGPT. The RobertA and BERT
models' excellent performance indicates that these models can play a critical
role in the fight against misinformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The economic trade-offs of large language models: A case study. (arXiv:2306.07402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07402">
<div class="article-summary-box-inner">
<span><p>Contacting customer service via chat is a common practice. Because employing
customer service agents is expensive, many companies are turning to NLP that
assists human agents by auto-generating responses that can be used directly or
with modifications. Large Language Models (LLMs) are a natural fit for this use
case; however, their efficacy must be balanced with the cost of training and
serving them. This paper assesses the practical cost and impact of LLMs for the
enterprise as a function of the usefulness of the responses that they generate.
We present a cost framework for evaluating an NLP model's utility for this use
case and apply it to a single brand as a case study in the context of an
existing agent assistance product. We compare three strategies for specializing
an LLM - prompt engineering, fine-tuning, and knowledge distillation - using
feedback from the brand's customer service agents. We find that the usability
of a model's responses can make up for a large difference in inference cost for
our case study brand, and we extrapolate our findings to the broader enterprise
space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Topic Extraction in Recommender Systems with Entropy Regularization. (arXiv:2306.07403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07403">
<div class="article-summary-box-inner">
<span><p>In recent years, many recommender systems have utilized textual data for
topic extraction to enhance interpretability. However, our findings reveal a
noticeable deficiency in the coherence of keywords within topics, resulting in
low explainability of the model. This paper introduces a novel approach called
entropy regularization to address the issue, leading to more interpretable
topics extracted from recommender systems, while ensuring that the performance
of the primary task stays competitively strong. The effectiveness of the
strategy is validated through experiments on a variation of the probabilistic
matrix factorization model that utilizes textual data to extract item
embeddings. The experiment results show a significant improvement in topic
coherence, which is quantified by cosine similarity on word embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Augmentation Techniques Applied to Low Resource Machine Translation: Case of Swahili. (arXiv:2306.07414v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07414">
<div class="article-summary-box-inner">
<span><p>In this work we investigate the impact of applying textual data augmentation
tasks to low resource machine translation. There has been recent interest in
investigating approaches for training systems for languages with limited
resources and one popular approach is the use of data augmentation techniques.
Data augmentation aims to increase the quantity of data that is available to
train the system. In machine translation, majority of the language pairs around
the world are considered low resource because they have little parallel data
available and the quality of neural machine translation (NMT) systems depend a
lot on the availability of sizable parallel corpora. We study and apply three
simple data augmentation techniques popularly used in text classification
tasks; synonym replacement, random insertion and contextual data augmentation
and compare their performance with baseline neural machine translation for
English-Swahili (En-Sw) datasets. We also present results in BLEU, ChrF and
Meteor scores. Overall, the contextual data augmentation technique shows some
improvements both in the $EN \rightarrow SW$ and $SW \rightarrow EN$
directions. We see that there is potential to use these methods in neural
machine translation when more extensive experiments are done with diverse
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender-Inclusive Grammatical Error Correction through Augmentation. (arXiv:2306.07415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07415">
<div class="article-summary-box-inner">
<span><p>In this paper we show that GEC systems display gender bias related to the use
of masculine and feminine terms and the gender-neutral singular "they". We
develop parallel datasets of texts with masculine and feminine terms and
singular "they" and use them to quantify gender bias in three competitive GEC
systems. We contribute a novel data augmentation technique for singular "they"
leveraging linguistic insights about its distribution relative to plural
"they". We demonstrate that both this data augmentation technique and a
refinement of a similar augmentation technique for masculine and feminine terms
can generate training data that reduces bias in GEC systems, especially with
respect to singular "they" while maintaining the same level of quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati. (arXiv:2306.07426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07426">
<div class="article-summary-box-inner">
<span><p>Local/Native South African languages are classified as low-resource
languages. As such, it is essential to build the resources for these languages
so that they can benefit from advances in the field of natural language
processing. In this work, the focus was to create annotated news datasets for
the isiZulu and Siswati native languages based on news topic classification
tasks and present the findings from these baseline classification models. Due
to the shortage of data for these native South African languages, the datasets
that were created were augmented and oversampled to increase data size and
overcome class classification imbalance. In total, four different
classification models were used namely Logistic regression, Naive bayes,
XGBoost and LSTM. These models were trained on three different word embeddings
namely Bag-Of-Words, TFIDF and Word2vec. The results of this study showed that
XGBoost, Logistic Regression and LSTM, trained from Word2vec performed better
than the other combinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. (arXiv:2306.07471v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07471">
<div class="article-summary-box-inner">
<span><p>BEIR is a benchmark dataset for zero-shot evaluation of information retrieval
models across 18 different domain/task combinations. In recent years, we have
witnessed the growing popularity of a representation learning approach to
building retrieval models, typically using pretrained transformers in a
supervised setting. This naturally begs the question: How effective are these
models when presented with queries and documents that differ from the training
data? Examples include searching in different domains (e.g., medical or legal
text) and with different types of queries (e.g., keywords vs. well-formed
questions). While BEIR was designed to answer these questions, our work
addresses two shortcomings that prevent the benchmark from achieving its full
potential: First, the sophistication of modern neural methods and the
complexity of current software infrastructure create barriers to entry for
newcomers. To this end, we provide reproducible reference implementations that
cover the two main classes of approaches: learned dense and sparse models.
Second, there does not exist a single authoritative nexus for reporting the
effectiveness of different models on BEIR, which has led to difficulty in
comparing different methods. To remedy this, we present an official
self-service BEIR leaderboard that provides fair and consistent comparisons of
retrieval models. By addressing both shortcomings, our work facilitates future
explorations in a range of interesting research questions that BEIR enables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment. (arXiv:2306.07486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07486">
<div class="article-summary-box-inner">
<span><p>Cross-lingual Machine Translation (MT) quality estimation plays a crucial
role in evaluating translation performance. GEMBA, the first MT quality
assessment metric based on Large Language Models (LLMs), employs one-step
prompting to achieve state-of-the-art (SOTA) in system-level MT quality
estimation; however, it lacks segment-level analysis. In contrast,
Chain-of-Thought (CoT) prompting outperforms one-step prompting by offering
improved reasoning and explainability. In this paper, we introduce
Knowledge-Prompted Estimator (KPE), a CoT prompting method that combines three
one-step prompting techniques, including perplexity, token-level similarity,
and sentence-level similarity. This method attains enhanced performance for
segment-level estimation compared with previous deep learning models and
one-step prompting approaches. Furthermore, supplementary experiments on
word-level visualized alignment demonstrate that our KPE method significantly
improves token alignment compared with earlier models and provides better
interpretability for MT quality estimation. Code will be released upon
publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Opinion-based Question Answering Systems Through Label Error Detection and Overwrite. (arXiv:2306.07499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07499">
<div class="article-summary-box-inner">
<span><p>Label error is a ubiquitous problem in annotated data. Large amounts of label
error substantially degrades the quality of deep learning models. Existing
methods to tackle the label error problem largely focus on the classification
task, and either rely on task specific architecture or require non-trivial
additional computations, which is undesirable or even unattainable for industry
usage. In this paper, we propose LEDO: a model-agnostic and computationally
efficient framework for Label Error Detection and Overwrite. LEDO is based on
Monte Carlo Dropout combined with uncertainty metrics, and can be easily
generalized to multiple tasks and data sets. Applying LEDO to an industry
opinion-based question answering system demonstrates it is effective at
improving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR
gain for the retrieval model, 1.5% PR AUC improvement for the machine reading
comprehension model, and 0.9% rise in the Average Precision for the ranker, on
top of the strong baselines with a large-scale social media dataset.
Importantly, LEDO is computationally efficient compared to methods that require
loss function change, and cost-effective as the resulting data can be used in
the same continuous training pipeline for production. Further analysis shows
that these gains come from an improved decision boundary after cleaning the
label errors existed in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adding guardrails to advanced chatbots. (arXiv:2306.07500v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07500">
<div class="article-summary-box-inner">
<span><p>Generative AI models continue to become more powerful. The launch of ChatGPT
in November 2022 has ushered in a new era of AI. ChatGPT and other similar
chatbots have a range of capabilities, from answering student homework
questions to creating music and art. There are already concerns that humans may
be replaced by chatbots for a variety of jobs. Because of the wide spectrum of
data chatbots are built on, we know that they will have human errors and human
biases built into them. These biases may cause significant harm and/or inequity
toward different subpopulations. To understand the strengths and weakness of
chatbot responses, we present a position paper that explores different use
cases of ChatGPT to determine the types of questions that are answered fairly
and the types that still need improvement. We find that ChatGPT is a fair
search engine for the tasks we tested; however, it has biases on both text
generation and code generation. We find that ChatGPT is very sensitive to
changes in the prompt, where small changes lead to different levels of
fairness. This suggests that we need to immediately implement "corrections" or
mitigation strategies in order to improve fairness of these systems. We suggest
different strategies to improve chatbots and also advocate for an impartial
review panel that has access to the model parameters to measure the levels of
different types of biases and then recommends safeguards that move toward
responses that are less discriminatory and more accurate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning. (arXiv:2306.07512v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07512">
<div class="article-summary-box-inner">
<span><p>This paper studies speculative reasoning task on real-world knowledge graphs
(KG) that contain both \textit{false negative issue} (i.e., potential true
facts being excluded) and \textit{false positive issue} (i.e., unreliable or
outdated facts being included). State-of-the-art methods fall short in the
speculative reasoning ability, as they assume the correctness of a fact is
solely determined by its presence in KG, making them vulnerable to false
negative/positive issues. The new reasoning task is formulated as a noisy
Positive-Unlabeled learning problem. We propose a variational framework, namely
nPUGraph, that jointly estimates the correctness of both collected and
uncollected facts (which we call \textit{label posterior}) and updates model
parameters during training. The label posterior estimation facilitates
speculative reasoning from two perspectives. First, it improves the robustness
of a label posterior-aware graph encoder against false positive links. Second,
it identifies missing facts to provide high-quality grounds of reasoning. They
are unified in a simple yet effective self-training procedure. Empirically,
extensive experiments on three benchmark KG and one Twitter dataset with
various degrees of false negative/positive cases demonstrate the effectiveness
of nPUGraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TART: A plug-and-play Transformer module for task-agnostic reasoning. (arXiv:2306.07536v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07536">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) exhibit in-context learning abilities which
enable the same model to perform several tasks without any task-specific
training. In contrast, traditional adaptation approaches, such as fine-tuning,
modify the underlying models for each specific task. In-context learning,
however, consistently underperforms task-specific tuning approaches even when
presented with the same examples. While most existing approaches (e.g., prompt
engineering) focus on the LLM's learned representations to patch this
performance gap, our analysis actually reveal that LLM representations contain
sufficient information to make good predictions. As such, we focus on the LLM's
reasoning abilities and demonstrate that this performance gap exists due to
their inability to perform simple probabilistic reasoning tasks. This raises an
intriguing question: Are LLMs actually capable of learning how to reason in a
task-agnostic manner? We answer this in the affirmative and propose TART which
generically improves an LLM's reasoning abilities using a synthetically trained
Transformer-based reasoning module. TART trains this reasoning module in a
task-agnostic manner using only synthetic logistic regression tasks and
composes it with an arbitrary real-world pre-trained model without any
additional training. With a single inference module, TART improves performance
across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M -
6B), tasks (14 NLP binary classification tasks), and even across different
modalities (audio and vision). Additionally, on the RAFT Benchmark, TART
improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B),
and is within 4% of GPT-3 (175B). Our code and models are available at
https://github.com/HazyResearch/TART .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation. (arXiv:2306.07554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07554">
<div class="article-summary-box-inner">
<span><p>Similes play an imperative role in creative writing such as story and
dialogue generation. Proper evaluation metrics are like a beacon guiding the
research of simile generation (SG). However, it remains under-explored as to
what criteria should be considered, how to quantify each criterion into
metrics, and whether the metrics are effective for comprehensive, efficient,
and reliable SG evaluation. To address the issues, we establish HAUSER, a
holistic and automatic evaluation system for the SG task, which consists of
five criteria from three perspectives and automatic metrics for each criterion.
Through extensive experiments, we verify that our metrics are significantly
more correlated with human ratings from each perspective compared with prior
automatic metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07567">
<div class="article-summary-box-inner">
<span><p>When using adversarial training, it is common practice to train against the
most egregious failures. However, this might imply using examples with
sensitive information (such as leaked passwords or security vulnerabilities) as
training data. One might assume that language models trained with gradient
descent never generate text snippets which were only present in examples
associated with the lowest possible reward. In this paper, we show that this
assumption is wrong: in some situations, large language models do learn from
such negatively-reinforced examples. We present a specific training setup that
enables Pythia-160M to generate passwords with a probability slightly greater
than chance, despite only showing it these passwords on examples where the
model is incentivized to not output these passwords. Our code is available at
https://github.com/FabienRoger/Learning-From-Negative-Examples
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question Decomposition Tree for Answering Complex Questions over Knowledge Bases. (arXiv:2306.07597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07597">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) has attracted a lot of interest in
recent years, especially for complex questions which require multiple facts to
answer. Question decomposition is a promising way to answer complex questions.
Existing decomposition methods split the question into sub-questions according
to a single compositionality type, which is not sufficient for questions
involving multiple compositionality types. In this paper, we propose Question
Decomposition Tree (QDT) to represent the structure of complex questions.
Inspired by recent advances in natural language generation (NLG), we present a
two-staged method called Clue-Decipher to generate QDT. It can leverage the
strong ability of NLG model and simultaneously preserve the original questions.
To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA
system called QDTQA. Extensive experiments show that QDTQA outperforms previous
state-of-the-art methods on ComplexWebQuestions dataset. Besides, our
decomposition method improves an existing KBQA system by 12% and sets a new
state-of-the-art on LC-QuAD 1.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft Language Clustering for Multilingual Model Pre-training. (arXiv:2306.07610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07610">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained language models have demonstrated impressive
(zero-shot) cross-lingual transfer abilities, however, their performance is
hindered when the target language has distant typology from source languages or
when pre-training data is limited in size. In this paper, we propose XLM-P,
which contextually retrieves prompts as flexible guidance for encoding
instances conditionally. Our XLM-P enables (1) lightweight modeling of
language-invariant and language-specific knowledge across languages, and (2)
easy integration with other multilingual pre-training methods. On the tasks of
XTREME including text classification, sequence labeling, question answering,
and sentence retrieval, both base- and large-size language models pre-trained
with our proposed method exhibit consistent performance improvement.
Furthermore, it provides substantial advantages for low-resource languages in
unsupervised sentence retrieval and for target languages that differ greatly
from the source language in cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rank-Aware Negative Training for Semi-Supervised Text Classification. (arXiv:2306.07621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07621">
<div class="article-summary-box-inner">
<span><p>Semi-supervised text classification-based paradigms (SSTC) typically employ
the spirit of self-training. The key idea is to train a deep classifier on
limited labeled texts and then iteratively predict the unlabeled texts as their
pseudo-labels for further training. However, the performance is largely
affected by the accuracy of pseudo-labels, which may not be significant in
real-world scenarios. This paper presents a Rank-aware Negative Training (RNT)
framework to address SSTC in learning with noisy label manner. To alleviate the
noisy information, we adapt a reasoning with uncertainty-based approach to rank
the unlabeled texts based on the evidential support received from the labeled
texts. Moreover, we propose the use of negative training to train RNT based on
the concept that ``the input instance does not belong to the complementary
label''. A complementary label is randomly selected from all labels except the
label on-target. Intuitively, the probability of a true label serving as a
complementary label is low and thus provides less noisy information during the
training, resulting in better performance on the test data. Finally, we
evaluate the proposed solution on various text classification benchmark
datasets. Our extensive experiments show that it consistently overcomes the
state-of-the-art alternatives in most scenarios and achieves competitive
performance in the others. The code of RNT is publicly available
at:https://github.com/amurtadha/RNT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07622">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Therefore, it is of
great importance to evaluate their emerging abilities. In this study, we show
that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles
human-like intuition -- and the cognitive errors that come with it. However,
LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4,
learned to avoid succumbing to these errors and perform in a hyperrational
manner. For our experiments, we probe LLMs with the Cognitive Reflection Test
(CRT) as well as semantic illusions that were originally designed to
investigate intuitive decision-making in humans. Moreover, we probe how sturdy
the inclination for intuitive-like decision-making is. Our study demonstrates
that investigating LLMs with methods from psychology has the potential to
reveal otherwise unknown emergent traits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07629">
<div class="article-summary-box-inner">
<span><p>Generative Large Language Models (LLMs) have demonstrated remarkable results
for a wide range of tasks. However, deploying these models for inference has
been a significant challenge due to their unprecedented resource requirements.
This has forced existing deployment frameworks to use multi-GPU inference
pipelines, which are often complex and costly, or to use smaller and less
performant models. In this work, we demonstrate that the main bottleneck for
generative inference with LLMs is memory bandwidth, rather than compute,
specifically for single batch inference. While quantization has emerged as a
promising solution by representing model weights with reduced precision,
previous efforts have often resulted in notable performance degradation. To
address this, we introduce SqueezeLLM, a post-training quantization framework
that not only enables lossless compression to ultra-low precisions of up to
3-bit, but also achieves higher quantization performance under the same memory
constraint. Our framework incorporates two novel ideas: (i) sensitivity-based
non-uniform quantization, which searches for the optimal bit precision
assignment based on second-order information; and (ii) the Dense-and-Sparse
decomposition that stores outliers and sensitive weight values in an efficient
sparse format. When applied to the LLaMA models, our 3-bit quantization
significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x
as compared to the state-of-the-art methods with the same memory requirement.
Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to
2.3x speedup compared to the baseline. Our code is open-sourced and available
online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid lemmatization in HuSpaCy. (arXiv:2306.07636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07636">
<div class="article-summary-box-inner">
<span><p>Lemmatization is still not a trivial task for morphologically rich languages.
Previous studies showed that hybrid architectures usually work better for these
languages and can yield great results. This paper presents a hybrid lemmatizer
utilizing both a neural model, dictionaries and hand-crafted rules. We
introduce a hybrid architecture along with empirical results on a widely used
Hungarian dataset. The presented methods are published as three HuSpaCy models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation. (arXiv:2306.07650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07650">
<div class="article-summary-box-inner">
<span><p>Pre-training and fine-tuning is a paradigm for alleviating the data scarcity
problem in end-to-end speech translation (E2E ST). The commonplace "modality
gap" between speech and text data often leads to inconsistent inputs between
pre-training and fine-tuning. However, we observe that this gap occurs in the
early stages of fine-tuning, but does not have a major impact on the final
performance. On the other hand, we find that there has another gap, which we
call the "capacity gap": high resource tasks (such as ASR and MT) always
require a large model to fit, when the model is reused for a low resource task
(E2E ST), it will get a sub-optimal performance due to the over-fitting. In a
case study, we find that the regularization plays a more important role than
the well-designed modality adaption method, which achieves 29.0 for en-de and
40.3 for en-fr on the MuST-C dataset. Code and models are available at
https://github.com/hannlp/TAB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Anisotropy Inherent to Transformers?. (arXiv:2306.07656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07656">
<div class="article-summary-box-inner">
<span><p>The representation degeneration problem is a phenomenon that is widely
observed among self-supervised learning methods based on Transformers. In NLP,
it takes the form of anisotropy, a singular property of hidden representations
which makes them unexpectedly close to each other in terms of angular distance
(cosine-similarity). Some recent works tend to show that anisotropy is a
consequence of optimizing the cross-entropy loss on long-tailed distributions
of tokens. We show in this paper that anisotropy can also be observed
empirically in language models with specific objectives that should not suffer
directly from the same consequences. We also show that the anisotropy problem
extends to Transformers trained on other modalities. Our observations tend to
demonstrate that anisotropy might actually be inherent to Transformers-based
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis. (arXiv:2306.07664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07664">
<div class="article-summary-box-inner">
<span><p>In recent years, language models (LMs) have made remarkable progress in
advancing the field of natural language processing (NLP). However, the impact
of data augmentation (DA) techniques on the fine-tuning (FT) performance of
these LMs has been a topic of ongoing debate. In this study, we evaluate the
effectiveness of three different FT methods in conjugation with
back-translation across an array of 7 diverse NLP tasks, including
classification and regression types, covering single-sentence and sentence-pair
tasks. Contrary to prior assumptions that DA does not contribute to the
enhancement of LMs' FT performance, our findings reveal that continued
pre-training on augmented data can effectively improve the FT performance of
the downstream tasks. In the most favourable case, continued pre-training
improves the performance of FT by more than 10% in the few-shot learning
setting. Our finding highlights the potential of DA as a powerful tool for
bolstering LMs' performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07691">
<div class="article-summary-box-inner">
<span><p>In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that
leverages style diffusion and adversarial training with large speech language
models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its
predecessor by modeling styles as a latent random variable through diffusion
models to generate the most suitable style for the text without requiring
reference speech, achieving efficient latent diffusion while benefiting from
the diverse speech synthesis offered by diffusion models. Furthermore, we
employ large pre-trained SLMs, such as WavLM, as discriminators with our novel
differentiable duration modeling for end-to-end training, resulting in improved
speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker
LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by
native English speakers. Moreover, when trained on the LibriTTS dataset, our
model outperforms previous publicly available models for zero-shot speaker
adaptation. This work achieves the first human-level TTS on both single and
multispeaker datasets, showcasing the potential of style diffusion and
adversarial training with large SLMs. The audio demos and source code are
available at https://styletts2.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT 2023 Low-Resource Track. (arXiv:2306.07763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07763">
<div class="article-summary-box-inner">
<span><p>This paper presents NAVER LABS Europe's systems for Tamasheq-French and
Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our
work attempts to maximize translation quality in low-resource settings using
multilingual parameter-efficient solutions that leverage strong pre-trained
models. Our primary submission for Tamasheq outperforms the previous state of
the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU
on this year's test set, outperforming the second best participant by 7.7
points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having
only two hours of translation data. Finally, we show that our proposed
multilingual architecture is also competitive for high-resource languages,
outperforming the best unconstrained submission to the IWSLT 2021 Multilingual
track, despite using much less training data and compute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tokenization with Factorized Subword Encoding. (arXiv:2306.07764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07764">
<div class="article-summary-box-inner">
<span><p>In recent years, language models have become increasingly larger and more
complex. However, the input representations for these models continue to rely
on simple and greedy subword tokenization methods. In this paper, we propose a
novel tokenization method that factorizes subwords onto discrete triplets using
a VQ-VAE model. The effectiveness of the proposed tokenization method, referred
to as the Factorizer, is evaluated on language modeling and morpho-syntactic
tasks for 7 diverse languages. Results indicate that this method is more
appropriate and robust for morphological tasks than the commonly used byte-pair
encoding (BPE) tokenization algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07786">
<div class="article-summary-box-inner">
<span><p>The efficiency of natural language processing has improved dramatically with
the advent of machine learning models, particularly neural network-based
solutions. However, some tasks are still challenging, especially when
considering specific domains. In this paper, we present a cloud-based system
that can extract insights from customer reviews using machine learning methods
integrated into a pipeline. For topic modeling, our composite model uses
transformer-based neural networks designed for natural language processing,
vector embedding-based keyword extraction, and clustering. The elements of our
model have been integrated and further developed to meet better the
requirements of efficient information extraction, topic modeling of the
extracted information, and user needs. Furthermore, our system can achieve
better results than this task's existing topic modeling and keyword extraction
solutions. Our approach is validated and compared with other state-of-the-art
methods using publicly available datasets for benchmarking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NoCoLA: The Norwegian Corpus of Linguistic Acceptability. (arXiv:2306.07790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07790">
<div class="article-summary-box-inner">
<span><p>While there has been a surge of large language models for Norwegian in recent
years, we lack any tool to evaluate their understanding of grammaticality. We
present two new Norwegian datasets for this task. NoCoLA_class is a supervised
binary classification task where the goal is to discriminate between acceptable
and non-acceptable sentences. On the other hand, NoCoLA_zero is a purely
diagnostic task for evaluating the grammatical judgement of a language model in
a completely zero-shot manner, i.e. without any further training. In this
paper, we describe both datasets in detail, show how to use them for different
flavors of language models, and conduct a comparative study of the existing
Norwegian language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification. (arXiv:2306.07797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07797">
<div class="article-summary-box-inner">
<span><p>This article investigates the knowledge transfer from the RuQTopics dataset.
This Russian topical dataset combines a large sample number (361,560
single-label, 170,930 multi-label) with extensive class coverage (76 classes).
We have prepared this dataset from the "Yandex Que" raw data. By evaluating the
RuQTopics - trained models on the six matching classes of the Russian MASSIVE
subset, we have proved that the RuQTopics dataset is suitable for real-world
conversational tasks, as the Russian-only models trained on this dataset
consistently yield an accuracy around 85\% on this subset. We also have figured
out that for the multilingual BERT, trained on the RuQTopics and evaluated on
the same six classes of MASSIVE (for all MASSIVE languages), the language-wise
accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)
with the approximate size of the pretraining BERT's data for the corresponding
language. At the same time, the correlation of the language-wise accuracy with
the linguistical distance from Russian is not statistically significant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. (arXiv:2306.07799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07799">
<div class="article-summary-box-inner">
<span><p>Large-scale language models, like ChatGPT, have garnered significant media
attention and stunned the public with their remarkable capacity for generating
coherent text from short natural language prompts. In this paper, we aim to
conduct a systematic inspection of ChatGPT's performance in two controllable
generation tasks, with respect to ChatGPT's ability to adapt its output to
different target audiences (expert vs. layman) and writing styles (formal vs.
informal). Additionally, we evaluate the faithfulness of the generated text,
and compare the model's performance with human-authored texts. Our findings
indicate that the stylistic variations produced by humans are considerably
larger than those demonstrated by ChatGPT, and the generated texts diverge from
human samples in several characteristics, such as the distribution of word
types. Moreover, we observe that ChatGPT sometimes incorporates factual errors
or hallucinations when adapting the text to suit a specific style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Capsule Networks for Romanian Satire Detection and Sentiment Analysis. (arXiv:2306.07845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07845">
<div class="article-summary-box-inner">
<span><p>Satire detection and sentiment analysis are intensively explored natural
language processing (NLP) tasks that study the identification of the satirical
tone from texts and extracting sentiments in relationship with their targets.
In languages with fewer research resources, an alternative is to produce
artificial examples based on character-level adversarial processes to overcome
dataset size limitations. Such samples are proven to act as a regularization
method, thus improving the robustness of models. In this work, we improve the
well-known NLP models (i.e., Convolutional Neural Networks, Long Short-Term
Memory (LSTM), Bidirectional LSTM, Gated Recurrent Units (GRUs), and
Bidirectional GRUs) with adversarial training and capsule networks. The
fine-tuned models are used for satire detection and sentiment analysis tasks in
the Romanian language. The proposed framework outperforms the existing methods
for the two tasks, achieving up to 99.08% accuracy, thus confirming the
improvements added by the capsule layers and the adversarial training in NLP
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07848">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Audio Pretraining (CLAP) has recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced CLAP model for speech emotion
recognition (SER). Specifically, we first build an effective emotion CLAP model
termed Emo-CLAP for SER, utilizing various self-supervised learning based
pre-trained models. Then, considering the importance of the gender attribute in
speech emotion modeling, two GEmo-CLAP approaches are further proposed to
integrate the emotion and gender information of speech signals, forming more
reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus
demonstrate that our proposed two GEmo-CLAP approaches consistently outperform
the baseline Emo-CLAP with different pre-trained models, while also achieving
superior recognition performance compared with other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading. (arXiv:2306.07875v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07875">
<div class="article-summary-box-inner">
<span><p>With the rapid growth and spread of online misinformation, people need tools
to help them evaluate the credibility and accuracy of online information.
Lateral reading, a strategy that involves cross-referencing information with
multiple sources, may be an effective approach to achieving this goal. In this
paper, we present ReadProbe, a tool to support lateral reading, powered by
generative large language models from OpenAI and the Bing search engine. Our
tool is able to generate useful questions for lateral reading, scour the web
for relevant documents, and generate well-attributed answers to help people
better evaluate online information. We made a web-based application to
demonstrate how ReadProbe can help reduce the risk of being misled by false
information. The code is available at
https://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won
the first prize in a national AI misinformation hackathon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks. (arXiv:2306.07899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07899">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are remarkable data annotators. They can be used
to generate high-fidelity supervised training data, as well as survey and
experimental data. With the widespread adoption of LLMs, human gold--standard
annotations are key to understanding the capabilities of LLMs and the validity
of their results. However, crowdsourcing, an important, inexpensive way to
obtain human annotations, may itself be impacted by LLMs, as crowd workers have
financial incentives to use LLMs to increase their productivity and income. To
investigate this concern, we conducted a case study on the prevalence of LLM
usage by crowd workers. We reran an abstract summarization task from the
literature on Amazon Mechanical Turk and, through a combination of keystroke
detection and synthetic text classification, estimate that 33-46% of crowd
workers used LLMs when completing the task. Although generalization to other,
less LLM-friendly tasks is unclear, our results call for platforms,
researchers, and crowd workers to find new ways to ensure that human data
remain human, perhaps using the methodology proposed here as a stepping stone.
Code/data: https://github.com/epfl-dlab/GPTurk
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark. (arXiv:2306.07902v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07902">
<div class="article-summary-box-inner">
<span><p>Despite impressive advancements in multilingual corpora collection and model
training, developing large-scale deployments of multilingual models still
presents a significant challenge. This is particularly true for language tasks
that are culture-dependent. One such example is the area of multilingual
sentiment analysis, where affective markers can be subtle and deeply ensconced
in culture. This work presents the most extensive open massively multilingual
corpus of datasets for training sentiment models. The corpus consists of 79
manually selected datasets from over 350 datasets reported in the scientific
literature based on strict quality criteria. The corpus covers 27 languages
representing 6 language families. Datasets can be queried using several
linguistic and functional features. In addition, we present a multi-faceted
sentiment classification benchmark summarizing hundreds of experiments
conducted on different base models, training objectives, dataset collections,
and fine-tuning strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences. (arXiv:2306.07906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07906">
<div class="article-summary-box-inner">
<span><p>We present WebGLM, a web-enhanced question-answering system based on the
General Language Model (GLM). Its goal is to augment a pre-trained large
language model (LLM) with web search and retrieval capabilities while being
efficient for real-world deployments. To achieve this, we develop WebGLM with
strategies for the LLM-augmented retriever, bootstrapped generator, and human
preference-aware scorer. Specifically, we identify and address the limitations
of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,
and cost-effectiveness advantages. In addition, we propose systematic criteria
for evaluating web-enhanced QA systems. We conduct multi-dimensional human
evaluation and quantitative ablation studies, which suggest the outperformance
of the proposed WebGLM designs over existing systems. WebGLM with the
10-billion-parameter GLM (10B) is shown to perform better than the
similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human
evaluation. The code, demo, and data are at
\url{https://github.com/THUDM/WebGLM}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Theory of Unsupervised Speech Recognition. (arXiv:2306.07926v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07926">
<div class="article-summary-box-inner">
<span><p>Unsupervised speech recognition (ASR-U) is the problem of learning automatic
speech recognition (ASR) systems from unpaired speech-only and text-only
corpora. While various algorithms exist to solve this problem, a theoretical
framework is missing from studying their properties and addressing such issues
as sensitivity to hyperparameters and training instability. In this paper, we
proposed a general theoretical framework to study the properties of ASR-U
systems based on random matrix theory and the theory of neural tangent kernels.
Such a framework allows us to prove various learnability conditions and sample
complexity bounds of ASR-U. Extensive ASR-U experiments on synthetic languages
with three classes of transition graphs provide strong empirical evidence for
our theory (code available at cactuswiththoughts/UnsupASRTheory.git).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07929">
<div class="article-summary-box-inner">
<span><p>Inspired by the insights in cognitive science with respect to human memory
and reasoning mechanism, a novel evolvable LLM-based (Large Language Model)
agent framework is proposed as REMEMBERER. By equipping the LLM with a
long-term experience memory, REMEMBERER is capable of exploiting the
experiences from the past episodes even for different task goals, which excels
an LLM-based agent with fixed exemplars or equipped with a transient working
memory. We further introduce Reinforcement Learning with Experience Memory
(RLEM) to update the memory. Thus, the whole system can learn from the
experiences of both success and failure, and evolve its capability without
fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER
constitutes a semi-parametric RL agent. Extensive experiments are conducted on
two RL task sets to evaluate the proposed framework. The average results with
different initialization and training sets exceed the prior SOTA by 4% and 2%
for the success rate on two task sets and demonstrate the superiority and
robustness of REMEMBERER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-in-the-Loop through Chain-of-Thought. (arXiv:2306.07932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07932">
<div class="article-summary-box-inner">
<span><p>While the emergence of powerful language models along with Chain-of-thought
prompting has made automation more and more omnipresent, it sometimes
demonstrates its weakness in long-term or multi-step logical reasoning. For
example, users don't always get desirable answers for complex mathematical
problems without human involvement. Against this background, we present the
Manual Correction System (MCS) -- a human-in-the-loop system enhanced by
Chain-of-Thought prompting, which explores how manual correction of sub-logics
in rationales can improve LLM's reasoning performance. Moving one step forward,
considering a system with human-in-the-loop involves more than having humans
improve performance but also controlling the cost. Therefore, we post a
Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on
classical economics theory to analyze, quantify and balance the utility and the
corresponding cost. We conduct experiments of MCS and CAMLOP with twelve
datasets. A significant advantage w.r.t cost and utility proves its superiority
over strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Telecom Language Through Large Language Models. (arXiv:2306.07933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07933">
<div class="article-summary-box-inner">
<span><p>The recent progress of artificial intelligence (AI) opens up new frontiers in
the possibility of automating many tasks involved in Telecom networks design,
implementation, and deployment. This has been further pushed forward with the
evolution of generative artificial intelligence (AI), including the emergence
of large language models (LLMs), which is believed to be the cornerstone toward
realizing self-governed, interactive AI agents. Motivated by this, in this
paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In
particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa
and GPT-2, to the Telecom domain languages, and demonstrate a use case for
identifying the 3rd Generation Partnership Project (3GPP) standard working
groups. We consider training the selected models on 3GPP technical documents
(Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years
2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model
achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP
working groups. The distilled BERT model with around 50% less parameters
achieves similar performance as others. This corroborates that fine-tuning
pretrained LLM can effectively identify the categories of Telecom language. The
developed framework shows a stepping stone towards realizing intent-driven and
self-evolving wireless networks from Telecom languages, and paves the way for
the implementation of generative AI in the Telecom domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information. (arXiv:2306.07934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07934">
<div class="article-summary-box-inner">
<span><p>Automated reasoning with unstructured natural text is a key requirement for
many potential applications of NLP and for developing robust AI systems.
Recently, Language Models (LMs) have demonstrated complex reasoning capacities
even without any finetuning. However, existing evaluation for automated
reasoning assumes access to a consistent and coherent set of information over
which models reason. When reasoning in the real-world, the available
information is frequently inconsistent or contradictory, and therefore models
need to be equipped with a strategy to resolve such conflicts when they arise.
One widely-applicable way of resolving conflicts is to impose preferences over
information sources (e.g., based on source credibility or information recency)
and adopt the source with higher preference. In this paper, we formulate the
problem of reasoning with contradictory information guided by preferences over
sources as the classical problem of defeasible reasoning, and develop a dataset
called BoardgameQA for measuring the reasoning capacity of LMs in this setting.
BoardgameQA also incorporates reasoning with implicit background knowledge, to
better reflect reasoning problems in downstream applications. We benchmark
various LMs on BoardgameQA and the results reveal a significant gap in the
reasoning capacity of state-of-the-art LMs on this problem, showing that
reasoning with conflicting information does not surface out-of-the-box in LMs.
While performance can be improved with finetuning, it nevertheless remains
poor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Representation Learning for Social Post Location Inference. (arXiv:2306.07935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07935">
<div class="article-summary-box-inner">
<span><p>Inferring geographic locations via social posts is essential for many
practical location-based applications such as product marketing,
point-of-interest recommendation, and infector tracking for COVID-19. Unlike
image-based location retrieval or social-post text embedding-based location
inference, the combined effect of multi-modal information (i.e., post images,
text, and hashtags) for social post positioning receives less attention. In
this work, we collect real datasets of social posts with images, texts, and
hashtags from Instagram and propose a novel Multi-modal Representation Learning
Framework (MRLF) capable of fusing different modalities of social posts for
location inference. MRLF integrates a multi-head attention mechanism to enhance
location-salient information extraction while significantly improving location
inference compared with single domain-based methods. To overcome the noisy
user-generated textual content, we introduce a novel attention-based
character-aware module that considers the relative dependencies between
characters of social post texts and hashtags for flexible multi-model
information fusion. The experimental results show that MRLF can make accurate
location predictions and open a new door to understanding the multi-modal data
of social posts for online inference tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator. (arXiv:2306.07936v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07936">
<div class="article-summary-box-inner">
<span><p>This paper presents FOOCTTS, an automatic pipeline for a football commentator
that generates speech with background crowd noise. The application gets the
text from the user, applies text pre-processing such as vowelization, followed
by the commentator's speech synthesizer. Our pipeline included Arabic automatic
speech recognition for data labeling, CTC segmentation, transcription
vowelization to match speech, and fine-tuning the TTS. Our system is capable of
generating speech with its acoustic environment within limited 15 minutes of
football commentator recording. Our prototype is generalizable and can be
easily applied to different domains and languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models. (arXiv:2306.07941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07941">
<div class="article-summary-box-inner">
<span><p>Transcriptions of phone calls are of significant value across diverse fields,
such as sales, customer service, healthcare, and law enforcement. Nevertheless,
the analysis of these recorded conversations can be an arduous and
time-intensive process, especially when dealing with extended or multifaceted
dialogues. In this work, we propose a novel method, GPT-distilled Calls
Segmentation and Tagging (GPT-Calls), for efficient and accurate call
segmentation and topic extraction. GPT-Calls is composed of offline and online
phases. The offline phase is applied once to a given list of topics and
involves generating a distribution of synthetic sentences for each topic using
a GPT model and extracting anchor vectors. The online phase is applied to every
call separately and scores the similarity between the transcripted conversation
and the topic anchors found in the offline phase. Then, time domain analysis is
applied to the similarity scores to group utterances into segments and tag them
with topics. The proposed paradigm provides an accurate and efficient method
for call segmentation and topic extraction that does not require labeled data,
thus making it a versatile approach applicable to various domains. Our
algorithm operates in production under Dynamics 365 Sales Conversation
Intelligence, and our research is based on real sales conversations gathered
from various Dynamics 365 Sales tenants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding. (arXiv:2306.07944v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07944">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been applied in the speech domain, often
incurring a performance drop due to misaligned between speech and language
representations. To bridge this gap, we propose a joint speech and language
model (SLM) using a Speech2Text adapter, which maps speech into text token
embedding space without speech information loss. Additionally, using a
CTC-based blank-filtering, we can reduce the speech sequence length to that of
text. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the
dialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to
address errors on rare entities, we augment SLM with a Speech2Entity retriever,
which uses speech to retrieve relevant entities, and then adds them to the
original SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the
DST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with
the dialog understanding task improves the ASR performance from 9.4% to 8.5%
WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Questioning the Survey Responses of Large Language Models. (arXiv:2306.07951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07951">
<div class="article-summary-box-inner">
<span><p>As large language models increase in capability, researchers have started to
conduct surveys of all kinds on these models with varying scientific
motivations. In this work, we examine what we can learn from a model's survey
responses on the basis of the well-established American Community Survey (ACS)
by the U.S. Census Bureau. Evaluating more than a dozen different models,
varying in size from a few hundred million to ten billion parameters, hundreds
of thousands of times each on questions from the ACS, we systematically
establish two dominant patterns. First, smaller models have a significant
position and labeling bias, for example, towards survey responses labeled with
the letter "A". This A-bias diminishes, albeit slowly, as model size increases.
Second, when adjusting for this labeling bias through randomized answer
ordering, models still do not trend toward US population statistics or those of
any cognizable population. Rather, models across the board trend toward
uniformly random aggregate statistics over survey responses. This pattern is
robust to various different ways of prompting the model, including what is the
de-facto standard. Our findings demonstrate that aggregate statistics of a
language model's survey responses lack the signals found in human populations.
This absence of statistical signal cautions about the use of survey responses
from large language models at present time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07952">
<div class="article-summary-box-inner">
<span><p>We present MOFI, a new vision foundation model designed to learn image
representations from noisy entity annotated images. MOFI differs from previous
work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe.
Regarding data, we introduce a new approach to automatically assign entity
labels to images from noisy image-text pairs. Our approach involves employing a
named entity recognition model to extract entities from the alt-text, and then
using a CLIP model to select the correct entities as labels of the paired
image. The approach is simple, does not require costly human annotation, and
can be readily scaled up to billions of image-text pairs mined from the web.
Through this method, we have created Image-to-Entities (I2E), a new large-scale
dataset with 1 billion images and 2 million distinct entities, covering rich
visual concepts in the wild. Building upon the I2E dataset, we study different
training recipes, including supervised pre-training, contrastive pre-training,
and multi-task learning. For constrastive pre-training, we treat entity names
as free-form text, and further enrich them with entity descriptions.
Experiments show that supervised pre-training with large-scale fine-grained
entity labels is highly effective for image retrieval tasks, and multi-task
training further improves the performance. The final MOFI model achieves 86.66%
mAP on the challenging GPR1200 dataset, surpassing the previous
state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further
experiments on zero-shot and linear probe image classification also show that
MOFI outperforms a CLIP model trained on the original image-text data,
demonstrating the effectiveness of the I2E dataset in learning strong image
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">arXiVeri: Automatic table verification with GPT. (arXiv:2306.07968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07968">
<div class="article-summary-box-inner">
<span><p>Without accurate transcription of numerical data in scientific documents, a
scientist cannot draw accurate conclusions. Unfortunately, the process of
copying numerical data from one paper to another is prone to human error. In
this paper, we propose to meet this challenge through the novel task of
automatic table verification (AutoTV), in which the objective is to verify the
accuracy of numerical data in tables by cross-referencing cited sources. To
support this task, we propose a new benchmark, arXiVeri, which comprises
tabular data drawn from open-access academic papers on arXiv. We introduce
metrics to evaluate the performance of a table verifier in two key areas: (i)
table matching, which aims to identify the source table in a cited document
that corresponds to a target table, and (ii) cell matching, which aims to
locate shared cells between a target and source table and identify their row
and column indices accurately. By leveraging the flexible capabilities of
modern large language models (LLMs), we propose simple baselines for table
verification. Our findings highlight the complexity of this task, even for
state-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.08316">
<div class="article-summary-box-inner">
<span><p>Measuring entity relatedness is a fundamental task for many natural language
processing and information retrieval applications. Prior work often studies
entity relatedness in static settings and an unsupervised manner. However,
entities in real-world are often involved in many different relationships,
consequently entity-relations are very dynamic over time. In this work, we
propose a neural networkbased approach for dynamic entity relatedness,
leveraging the collective attention as supervision. Our model is capable of
learning rich and different entity representations in a joint framework.
Through extensive experiments on large-scale datasets, we demonstrate that our
method achieves better results than competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages. (arXiv:2104.05596v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05596">
<div class="article-summary-box-inner">
<span><p>We present Samanantar, the largest publicly available parallel corpora
collection for Indic languages. The collection contains a total of 49.7 million
sentence pairs between English and 11 Indic languages (from two language
families). Specifically, we compile 12.4 million sentence pairs from existing,
publicly-available parallel corpora, and additionally mine 37.4 million
sentence pairs from the web, resulting in a 4x increase. We mine the parallel
sentences from the web by combining many corpora, tools, and methods: (a)
web-crawled monolingual corpora, (b) document OCR for extracting sentences from
scanned documents, (c) multilingual representation models for aligning
sentences, and (d) approximate nearest neighbor search for searching in a large
collection of sentences. Human evaluation of samples from the newly mined
corpora validate the high quality of the parallel sentences across 11
languages. Further, we extract 83.4 million sentence pairs between all 55 Indic
language pairs from the English-centric parallel corpus using English as the
pivot language. We trained multilingual NMT models spanning all these languages
on Samanantar, which outperform existing models and baselines on publicly
available benchmarks, such as FLORES, establishing the utility of Samanantar.
Our data and models are available publicly at
https://ai4bharat.iitm.ac.in/samanantar and we hope they will help advance
research in NMT and multilingual NLP for Indic languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Semantic Distance between Highly Overlapped Texts. (arXiv:2110.01176v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01176">
<div class="article-summary-box-inner">
<span><p>Overlapping frequently occurs in paired texts in natural language processing
tasks like text editing and semantic similarity evaluation. Better evaluation
of the semantic distance between the overlapped sentences benefits the language
system's understanding and guides the generation. Since conventional semantic
metrics are based on word representations, they are vulnerable to the
disturbance of overlapped components with similar representations. This paper
aims to address the issue with a mask-and-predict strategy. We take the words
in the longest common sequence (LCS) as neighboring words and use masked
language modeling (MLM) from pre-trained language models (PLMs) to predict the
distributions on their positions. Our metric, Neighboring Distribution
Divergence (NDD), represent the semantic distance by calculating the divergence
between distributions in the overlapped parts. Experiments on Semantic Textual
Similarity show NDD to be more sensitive to various semantic differences,
especially on highly overlapped paired texts. Based on the discovery, we
further implement an unsupervised and training-free method for text
compression, leading to a significant improvement on the previous
perplexity-based method. The high scalability of our method even enables NDD to
outperform the supervised state-of-the-art in domain adaption by a huge margin.
Further experiments on syntax and semantics analyses verify the awareness of
internal sentence structures, indicating the high potential of NDD for further
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models. (arXiv:2208.08232v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08232">
<div class="article-summary-box-inner">
<span><p>Controlling the text generated by language models and customizing the content
has been a long-standing challenge. Existing prompting techniques proposed in
pursuit of providing control are task-specific and lack generality; this
provides overwhelming choices for non-expert users to find a suitable method
for their task. The effort associated with those techniques, such as in writing
examples, explanations, instructions, etc. further limits their adoption among
non-expert users. In this paper, we propose a simple prompting strategy HELP ME
THINK where we encourage GPT3 to help non-expert users by asking a set of
relevant questions and leveraging user answers to execute the task. We
demonstrate the efficacy of our technique HELP ME THINK on a variety of tasks.
Specifically, we focus on tasks that are hard for average humans and require
significant thinking to perform. We hope our work will encourage the
development of unconventional ways to harness the power of large language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus. (arXiv:2210.06405v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06405">
<div class="article-summary-box-inner">
<span><p>In this research, we propose a complete set of approaches for identifying and
extracting emotions from Bangla texts. We provide a Bangla emotion classifier
for six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla
words using transformer-based models, which exhibit phenomenal results in
recent days, especially for high-resource languages. The Unified Bangla
Multi-class Emotion Corpus (UBMEC) is used to assess the performance of our
models. UBMEC is created by combining two previously released manually labeled
datasets of Bangla comments on six emotion classes with fresh manually labeled
Bangla comments created by us. The corpus dataset and code we used in this work
are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complementary Explanations for Effective In-Context Learning. (arXiv:2211.13892v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13892">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have exhibited remarkable capabilities in
learning from explanations in prompts, but there has been limited understanding
of exactly how these explanations function or why they are effective. This work
aims to better understand the mechanisms by which explanations are used for
in-context learning. We first study the impact of two different factors on the
performance of prompts with explanations: the computation trace (the way the
solution is decomposed) and the natural language used to express the prompt. By
perturbing explanations on three controlled tasks, we show that both factors
contribute to the effectiveness of explanations. We further study how to form
maximally effective sets of explanations for solving a given test query. We
find that LLMs can benefit from the complementarity of the explanation set:
diverse reasoning skills shown by different exemplars can lead to better
performance. Therefore, we propose a maximal marginal relevance-based exemplar
selection approach for constructing exemplar sets that are both relevant as
well as complementary, which successfully improves the in-context learning
performance across three real-world tasks on multiple LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09739">
<div class="article-summary-box-inner">
<span><p>Training learnable metrics using modern language models has recently emerged
as a promising method for the automatic evaluation of machine translation.
However, existing human evaluation datasets for text simplification have
limited annotations that are based on unitary or outdated models, making them
unsuitable for this approach. To address these issues, we introduce the
SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on
2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging
simplification benchmark consisting of over 1K human ratings of 360
simplifications including GPT-3.5 generated text. Training on SimpEval, we
present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive
empirical results show that LENS correlates much better with human judgment
than existing metrics, paving the way for future progress in the evaluation of
text simplification. We also introduce Rank and Rate, a human evaluation
framework that rates simplifications from several models in a list-wise manner
using an interactive interface, which ensures both consistency and accuracy in
the evaluation process and is used to create the SimpEval datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Reasoning Teachers. (arXiv:2212.10071v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10071">
<div class="article-summary-box-inner">
<span><p>Recent works have shown that chain-of-thought (CoT) prompting can elicit
language models to solve complex reasoning tasks, step-by-step. However,
prompt-based CoT methods are dependent on very large models such as GPT-3 175B
which are prohibitive to deploy at scale. In this paper, we use these large
models as reasoning teachers to enable complex reasoning in smaller models and
reduce model size requirements by several orders of magnitude. We propose
Fine-tune-CoT, a method that generates reasoning samples from very large
teacher models to fine-tune smaller models. We evaluate our method on a wide
range of public models and complex tasks. We find that Fine-tune-CoT enables
substantial reasoning capability in small models, far outperforming
prompt-based baselines and even the teacher model in many tasks. Additionally,
we extend our method by leveraging the teacher model's ability to generate
multiple distinct rationales for each original sample. Enriching the
fine-tuning data with such diverse reasoning results in a substantial
performance boost across datasets, even for very small models. We conduct
ablations and sample studies to understand the emergence of reasoning
capabilities of student models. Our code implementation and data are available
at https://github.com/itsnamgyu/reasoning-teacher.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. (arXiv:2303.03004v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.03004">
<div class="article-summary-box-inner">
<span><p>AI systems that can create codes as solutions to problems or assist
developers in writing codes can increase productivity and make programming more
accessible. Recently, pre-trained large language models have shown impressive
abilities in generating codes from natural language descriptions, repairing
buggy codes, translating codes between languages, and retrieving relevant code
segments. However, the evaluation of these models has often been performed in a
scattered way on only one or two specific tasks, in a few languages, at a
partial granularity (e.g., function) level, and in many cases without proper
training data. Even more concerning is that in most cases the evaluation of
generated codes has been done in terms of mere lexical overlap with a reference
code rather than actual execution. We introduce xCodeEval, the largest
executable multilingual multitask benchmark to date consisting of 25M
document-level coding examples (16.5B tokens) from about 7.5K unique problems
covering up to 11 programming languages with execution-level parallelism. It
features a total of seven tasks involving code understanding, generation,
translation and retrieval. xCodeEval adopts an execution-based evaluation and
offers a multilingual code execution engine, ExecEval that supports unit test
based execution in all the 11 languages. To address the challenge of balancing
the distributions of text-code samples over multiple attributes in
validation/test sets, we further propose a novel data splitting and a data
selection schema based on the geometric mean and graph-theoretic principle.
Experimental results on all the tasks and languages show xCodeEval is a
promising yet challenging benchmark as per the current advancements in language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11403">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have so far impressed the world, with
unprecedented capabilities that emerge in models at large scales. On the vision
side, transformer models (i.e., ViT) are following the same trend, achieving
the best performance on challenging benchmarks. With the abundance of such
unimodal models, a natural question arises; do we need also to follow this
trend to tackle multimodal tasks? In this work, we propose to rather direct
effort to efficient adaptations of existing models, and propose to augment
Language Models with perception. Existing approaches for adapting pretrained
models for vision-language tasks still rely on several key components that
hinder their efficiency. In particular, they still train a large number of
parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)
trained on huge image-text datasets, and add significant inference overhead. In
addition, most of these approaches have focused on Zero-Shot and In Context
Learning, with little to no effort on direct finetuning. We investigate the
minimal computational effort needed to adapt unimodal models for multimodal
tasks and propose a new challenging setup, alongside different approaches, that
efficiently adapts unimodal pretrained models. We show that by freezing more
than 99\% of total parameters, training only one linear projection layer, and
prepending only one trainable token, our approach (dubbed eP-ALM) significantly
outperforms other baselines on VQA and Captioning across Image, Video, and
Audio modalities, following the proposed setup. The code will be available
here: https://github.com/mshukor/eP-ALM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12135">
<div class="article-summary-box-inner">
<span><p>The growth of pending legal cases in populous countries, such as India, has
become a major issue. Developing effective techniques to process and understand
legal documents is extremely useful in resolving this problem. In this paper,
we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi
et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that
considers the comprehensive context information in both intra- and
inter-sentence levels to predict rhetorical roles (subtask A) and then train a
Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize
legal entities (subtask B). Our evaluations demonstrate that our designed
models are more accurate than baselines, e.g., with an up to 15.0% better F1
score in subtask B. We achieved notable performance in the task leaderboard,
e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02721">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence language models can be used to produce abstractive
summaries which are coherent, relevant, and concise. Still, model sizes can
make deployment in latency-sensitive or web-scale implementations difficult.
This paper studies the relationship between model size, structured pruning,
inference efficiency, and summarization accuracy on widely used summarization
datasets. We show that model accuracy is tied to the encoder size while
inference efficiency is connected to the decoder. Using asymmetric pruning can
lead to nearly 3x improvement in inference latency with ~1 point loss in
Rouge-2. Moreover, we find both the average degradation and the role of
asymmetry to be consistent across model sizes and variations in datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03279">
<div class="article-summary-box-inner">
<span><p>Artificial agents have traditionally been trained to maximize reward, which
may incentivize power-seeking and deception, analogous to how next-token
prediction in language models (LMs) may incentivize toxicity. So do agents
naturally learn to be Machiavellian? And how do we measure these behaviors in
general-purpose models such as GPT-4? Towards answering these questions, we
introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games
containing over half a million rich, diverse scenarios that center on social
decision-making. Scenario labeling is automated with LMs, which are more
performant than human annotators. We mathematize dozens of harmful behaviors
and use our annotations to evaluate agents' tendencies to be power-seeking,
cause disutility, and commit ethical violations. We observe some tension
between maximizing reward and behaving ethically. To improve this trade-off, we
investigate LM-based methods to steer agents' towards less harmful behaviors.
Our results show that agents can both act competently and morally, so concrete
progress can currently be made in machine ethics--designing agents that are
Pareto improvements in both safety and capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14391">
<div class="article-summary-box-inner">
<span><p>Language is compositional; an instruction can express multiple relation
constraints to hold among objects in a scene that a robot is tasked to
rearrange. Our focus in this work is an instructable scene-rearranging
framework that generalizes to longer instructions and to spatial concept
compositions never seen at training time. We propose to represent
language-instructed spatial concepts with energy functions over relative object
arrangements. A language parser maps instructions to corresponding energy
functions and an open-vocabulary visual-language model grounds their arguments
to relevant objects in the scene. We generate goal scene configurations by
gradient descent on the sum of energy functions, one per language predicate in
the instruction. Local vision-based policies then re-locate objects to the
inferred goal locations. We test our model on established instruction-guided
manipulation benchmarks, as well as benchmarks of compositional instructions we
introduce. We show our model can execute highly compositional instructions
zero-shot in simulation and in the real world. It outperforms
language-to-action reactive policies and Large Language Model planners by a
large margin, especially for long instructions that involve compositions of
multiple spatial concepts. Simulation and real-world robot execution videos, as
well as our code and datasets are publicly available on our website:
https://ebmplanner.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02549">
<div class="article-summary-box-inner">
<span><p>The recent advent of self-supervised pre-training techniques has led to a
surge in the use of multimodal learning in form document understanding.
However, existing approaches that extend the mask language modeling to other
modalities require careful multi-task tuning, complex reconstruction target
designs, or additional pre-training data. In FormNetV2, we introduce a
centralized multimodal graph contrastive learning strategy to unify
self-supervised pre-training for all modalities in one loss. The graph
contrastive objective maximizes the agreement of multimodal representations,
providing a natural interplay for all modalities without special customization.
In addition, we extract image features within the bounding box that joins a
pair of tokens connected by a graph edge, capturing more targeted visual cues
without loading a sophisticated and separately pre-trained image embedder.
FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE
and Payment benchmarks with a more compact model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04790">
<div class="article-summary-box-inner">
<span><p>We present a vision and language model named MultiModal-GPT to conduct
multi-round dialogue with humans. MultiModal-GPT can follow various
instructions from humans, such as generating a detailed caption, counting the
number of interested objects, and answering general questions from users.
MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with
Low-rank Adapter (LoRA) added both in the cross-attention part and the
self-attention part of the language model. We first construct instruction
templates with vision and language data for multi-modality instruction tuning
to make the model understand and follow human instructions. We find the quality
of training data is vital for the dialogue performance, where few data
containing short answers can lead the model to respond shortly to any
instructions. To further enhance the ability to chat with humans of the
MultiModal-GPT, we utilize language-only instruction-following data to train
the MultiModal-GPT jointly. The joint training of language-only and
visual-language instructions with the \emph{same} instruction template
effectively improves dialogue performance. Various demos show the ability of
continuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are
at https://github.com/open-mmlab/Multimodal-GPT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts. (arXiv:2305.14839v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14839">
<div class="article-summary-box-inner">
<span><p>Perceiving multi-modal information and fulfilling dialogues with humans is a
long-term goal of artificial intelligence. Pre-training is commonly regarded as
an effective approach for multi-modal dialogue. However, due to the limited
availability of multi-modal dialogue data, there is still scarce research on
multi-modal dialogue pre-training. Yet another intriguing challenge emerges
from the encompassing nature of multi-modal dialogue, which involves various
modalities and tasks. Moreover, new forms of tasks may arise at unpredictable
points in the future. Hence, it is essential for designed multi-modal dialogue
models to possess sufficient flexibility to adapt to such scenarios. This paper
proposes \textbf{PaCE}, a unified, structured, compositional multi-modal
dialogue pre-training framework. It utilizes a combination of several
fundamental experts to accommodate multiple dialogue-related tasks and can be
pre-trained using limited dialogue and extensive non-dialogue multi-modal data.
Furthermore, we propose a progressive training method where old experts from
the past can assist new experts, facilitating the expansion of their
capabilities. Experimental results demonstrate that PaCE achieves
state-of-the-art results on eight multi-modal dialog benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18169">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been significant progress in developing
pre-trained language models for NLP. However, these models often struggle when
fine-tuned on small datasets. To address this issue, researchers have proposed
various adaptation approaches. Prompt-based tuning is arguably the most common
way, especially for larger models. Previous research shows that adding
contrastive learning to prompt-based fine-tuning is effective as it helps the
model generate embeddings that are more distinguishable between classes, and it
can also be more sample-efficient as the model learns from positive and
negative examples simultaneously. One of the most important components of
contrastive learning is data augmentation, but unlike computer vision,
effective data augmentation for NLP is still challenging. This paper proposes
LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language
Models, which leverages prompt-based few-shot paraphrasing using generative
language models, especially large language models such as GPT-3 and OPT-175B,
for data augmentation. Our experiments on multiple text classification
benchmarks show that this augmentation method outperforms other methods, such
as easy data augmentation, back translation, and multiple templates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19395">
<div class="article-summary-box-inner">
<span><p>Learning from noisy labels is a challenge that arises in many real-world
applications where training data can contain incorrect or corrupted labels.
When fine-tuning language models with noisy labels, models can easily overfit
the label noise, leading to decreased performance. Most existing methods for
learning from noisy labels use static input features for denoising, but these
methods are limited by the information they can provide on true label
distributions and can result in biased or incorrect predictions. In this work,
we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic
patterns in the embedding space during the fine-tuning process of language
models to improve noisy label predictions. DyGen uses the variational
auto-encoding framework to infer the posterior distributions of true labels
from noisy labels and training dynamics. Additionally, a co-regularization
mechanism is used to minimize the impact of potentially noisy labels and
priors. DyGen demonstrates an average accuracy improvement of 3.10% on two
synthetic noise datasets and 1.48% on three real-world noise datasets compared
to the previous state-of-the-art. Extensive experiments and analyses show the
effectiveness of each component in DyGen. Our code is available for
reproducibility on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02272">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with hundreds of billions of parameters show
impressive results across various language tasks using simple prompt tuning and
few-shot examples, without the need for task-specific fine-tuning. However,
their enormous size requires multiple server-grade GPUs even for inference,
creating a significant cost barrier. To address this limitation, we introduce a
novel post-training quantization method for weights with minimal quality
degradation. While activation outliers are known to be problematic in
activation quantization, our theoretical analysis suggests that we can identify
factors contributing to weight quantization errors by considering activation
outliers. We propose an innovative PTQ scheme called outlier-aware weight
quantization (OWQ), which identifies vulnerable weights and allocates
high-precision to them. Our extensive experiments demonstrate that the 3.01-bit
models produced by OWQ exhibit comparable quality to the 4-bit models generated
by OPTQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gen-IR @ SIGIR 2023: The First Workshop on Generative Information Retrieval. (arXiv:2306.02887v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02887">
<div class="article-summary-box-inner">
<span><p>Generative information retrieval (IR) has experienced substantial growth
across multiple research communities (e.g., information retrieval, computer
vision, natural language processing, and machine learning), and has been highly
visible in the popular press. Theoretical, empirical, and actual user-facing
products have been released that retrieve documents (via generation) or
directly generate answers given an input request. We would like to investigate
whether end-to-end generative models are just another trend or, as some claim,
a paradigm change for IR. This necessitates new metrics, theoretical grounding,
evaluation methods, task definitions, models, user interfaces, etc. The goal of
this workshop (https://coda.io/@sigir/gen-ir) is to focus on previously
explored Generative IR techniques like document retrieval and direct Grounded
Answer Generation, while also offering a venue for the discussion and
exploration of how Generative IR can be applied to new domains like
recommendation systems, summarization, etc. The format of the workshop is
interactive, including roundtable and keynote sessions and tends to avoid the
one-sided dialogue of a mini-conference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyVoice: Language Models for Speech to Speech Translation. (arXiv:2306.02982v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02982">
<div class="article-summary-box-inner">
<span><p>We propose PolyVoice, a language model-based framework for speech-to-speech
translation (S2ST) system. Our framework consists of two language models: a
translation language model and a speech synthesis language model. We use
discretized speech units, which are generated in a fully unsupervised way, and
thus our framework can be used for unwritten languages. For the speech
synthesis part, we adopt the existing VALL-E X approach and build a unit-based
audio language model. This grants our framework the ability to preserve the
voice characteristics and the speaking style of the original speech. We examine
our system on Chinese $\rightarrow$ English and English $\rightarrow$ Spanish
pairs. Experimental results show that our system can generate speech with high
translation quality and audio quality. Speech samples are available at
https://speechtranslation.github.io/polyvoice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04528">
<div class="article-summary-box-inner">
<span><p>The increasing reliance on Large Language Models (LLMs) across academia and
industry necessitates a comprehensive understanding of their robustness to
prompts. In response to this vital need, we introduce PromptBench, a robustness
benchmark designed to measure LLMs' resilience to adversarial prompts. This
study uses a plethora of adversarial textual attacks targeting prompts across
multiple levels: character, word, sentence, and semantic. These prompts are
then employed in diverse tasks, such as sentiment analysis, natural language
inference, reading comprehension, machine translation, and math
problem-solving. Our study generates 4,032 adversarial prompts, meticulously
evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our
findings demonstrate that contemporary LLMs are vulnerable to adversarial
prompts. Furthermore, we present comprehensive analysis to understand the
mystery behind prompt robustness and its transferability. We then offer
insightful robustness analysis and pragmatic recommendations for prompt
composition, beneficial to both researchers and everyday users. We make our
code, prompts, and methodologies to generate adversarial prompts publicly
accessible, thereby enabling and encouraging collaborative exploration in this
pivotal field: https://github.com/microsoft/promptbench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04551">
<div class="article-summary-box-inner">
<span><p>Generative artificial intelligence (AI) is a promising direction for
augmenting clinical diagnostic decision support and reducing diagnostic errors,
a leading contributor to medical errors. To further the development of clinical
AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a
comprehensive generative AI framework, comprised of six tasks representing key
components in clinical reasoning. We present a comparative analysis of
in-domain versus out-of-domain language models as well as multi-task versus
single task training with a focus on the problem summarization task in DR.BENCH
(Gao et al., 2023). We demonstrate that a multi-task, clinically trained
language model outperforms its general domain counterpart by a large margin,
establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.
This research underscores the value of domain-specific training for optimizing
clinical diagnostic reasoning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS. (arXiv:2306.05083v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05083">
<div class="article-summary-box-inner">
<span><p>Existing sentence textual similarity benchmark datasets only use a single
number to summarize how similar the sentence encoder's decision is to humans'.
However, it is unclear what kind of sentence pairs a sentence encoder (SE)
would consider similar. Moreover, existing SE benchmarks mainly consider
sentence pairs with low lexical overlap, so it is unclear how the SEs behave
when two sentences have high lexical overlap. We introduce a high-quality SE
diagnostic dataset, HEROS. HEROS is constructed by transforming an original
sentence into a new sentence based on certain rules to form a \textit{minimal
pair}, and the minimal pair has high lexical overlaps. The rules include
replacing a word with a synonym, an antonym, a typo, a random word, and
converting the original sentence into its negation. Different rules yield
different subsets of HEROS. By systematically comparing the performance of over
60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised
sentence encoders are insensitive to negation. We find the datasets used to
train the SE are the main determinants of what kind of sentence pairs an SE
considers similar. We also show that even if two SEs have similar performance
on STS benchmarks, they can have very different behavior on HEROS. Our result
reveals the blind spot of traditional STS benchmarks when evaluating SEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EaSyGuide : ESG Issue Identification Framework leveraging Abilities of Generative Large Language Models. (arXiv:2306.06662v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06662">
<div class="article-summary-box-inner">
<span><p>This paper presents our participation in the FinNLP-2023 shared task on
multi-lingual environmental, social, and corporate governance issue
identification (ML-ESG). The task's objective is to classify news articles
based on the 35 ESG key issues defined by the MSCI ESG rating guidelines. Our
approach focuses on the English and French subtasks, employing the CerebrasGPT,
OPT, and Pythia models, along with the zero-shot and GPT3Mix Augmentation
techniques. We utilize various encoder models, such as RoBERTa, DeBERTa, and
FinBERT, subjecting them to knowledge distillation and additional training.
</p>
<p>Our approach yielded exceptional results, securing the first position in the
English text subtask with F1-score 0.69 and the second position in the French
text subtask with F1-score 0.78. These outcomes underscore the effectiveness of
our methodology in identifying ESG issues in news articles across different
languages. Our findings contribute to the exploration of ESG topics and
highlight the potential of leveraging advanced language models for ESG issue
identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Audio-textual Architecture for Robust Spoken Language Understanding. (arXiv:2306.06819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06819">
<div class="article-summary-box-inner">
<span><p>Recent voice assistants are usually based on the cascade spoken language
understanding (SLU) solution, which consists of an automatic speech recognition
(ASR) engine and a natural language understanding (NLU) system. Because such
approach relies on the ASR output, it often suffers from the so-called ASR
error propagation. In this work, we investigate impacts of this ASR error
propagation on state-of-the-art NLU systems based on pre-trained language
models (PLM), such as BERT and RoBERTa. Moreover, a multimodal language
understanding (MLU) module is proposed to mitigate SLU performance degradation
caused by errors present in the ASR transcript. The MLU benefits from
self-supervised features learned from both audio and text modalities,
specifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines
an encoder network to embed the audio signal and a text encoder to process text
transcripts followed by a late fusion layer to fuse audio and text logits. We
found that the proposed MLU showed to be robust towards poor quality ASR
transcripts, while the performance of BERT and RoBERTa are severely
compromised. Our model is evaluated on five tasks from three SLU datasets and
robustness is tested using ASR transcripts from three ASR engines. Results show
that the proposed approach effectively mitigates the ASR error propagation
problem, surpassing the PLM models' performance across all datasets for the
academic ASR engine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LTCR: Long-Text Chinese Rumor Detection Dataset. (arXiv:2306.07201v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07201">
<div class="article-summary-box-inner">
<span><p>False information can spread quickly on social media, negatively influencing
the citizens' behaviors and responses to social events. To better detect all of
the fake news, especially long texts which are harder to find completely, a
Long-Text Chinese Rumor detection dataset named LTCR is proposed. The LTCR
dataset provides a valuable resource for accurately detecting misinformation,
especially in the context of complex fake news related to COVID-19. The dataset
consists of 1,729 and 500 pieces of real and fake news, respectively. The
average lengths of real and fake news are approximately 230 and 152 characters.
We also propose \method, Salience-aware Fake News Detection Model, which
achieves the highest accuracy (95.85%), fake news recall (90.91%) and F-score
(90.60%) on the dataset. (https://github.com/Enderfga/DoubleCheck)
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-14 23:12:43.340333595 UTC">2023-06-14 23:12:43 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>