<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-02T01:30:00Z">05-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation. (arXiv:2305.00034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00034">
<div class="article-summary-box-inner">
<span><p>While conditional generation models can now generate natural language well
enough to create fluent text, it is still difficult to control the generation
process, leading to irrelevant, repetitive, and hallucinated content. Recent
work shows that planning can be a useful intermediate step to render
conditional generation less opaque and more grounded. We present a web
browser-based demonstration for query-focused summarization that uses a
sequence of question-answer pairs, as a blueprint plan for guiding text
generation (i.e., what to say and in what order). We illustrate how users may
interact with the generated text and associated plan visualizations, e.g., by
editing and modifying the blueprint in order to improve or control the
generated output.
</p>
<p>A short video demonstrating our system is available at
https://goo.gle/text-blueprint-demo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00050">
<div class="article-summary-box-inner">
<span><p>The causal capabilities of large language models (LLMs) is a matter of
significant debate, with critical implications for the use of LLMs in
societally impactful domains such as medicine, science, law, and policy. We
further our understanding of LLMs and their causal implications, considering
the distinctions between different types of causal reasoning tasks, as well as
the entangled threats of construct and measurement validity. LLM-based methods
establish new state-of-the-art accuracies on multiple causal benchmarks.
Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise
causal discovery task (97%, 13 points gain), counterfactual reasoning task
(92%, 20 points gain), and actual causality (86% accuracy in determining
necessary and sufficient causes in vignettes). At the same time, LLMs exhibit
unpredictable failure modes and we provide some techniques to interpret their
robustness.
</p>
<p>Crucially, LLMs perform these causal tasks while relying on sources of
knowledge and methods distinct from and complementary to non-LLM based
approaches. Specifically, LLMs bring capabilities so far understood to be
restricted to humans, such as using collected knowledge to generate causal
graphs or identifying background causal context from natural language. We
envision LLMs to be used alongside existing causal methods, as a proxy for
human domain knowledge and to reduce human effort in setting up a causal
analysis, one of the biggest impediments to the widespread adoption of causal
methods. We also see existing causal methods as promising tools for LLMs to
formalize, validate, and communicate their reasoning especially in high-stakes
scenarios.
</p>
<p>In capturing common sense and domain knowledge about causal mechanisms and
supporting translation between natural language and formal methods, LLMs open
new frontiers for advancing the research, practice, and adoption of causality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning. (arXiv:2305.00061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00061">
<div class="article-summary-box-inner">
<span><p>Languages models have been successfully applied to a variety of reasoning
tasks in NLP, yet the language models still suffer from compositional
generalization. In this paper we present Explainable Verbal Reasoner Plus
(EVR+), a reasoning framework that enhances language models' compositional
reasoning ability by (1) allowing the model to explicitly generate and execute
symbolic operators, and (2) allowing the model to decompose a complex task into
several simpler ones in a flexible manner. Compared with its predecessor
Explainable Verbal Reasoner (EVR) and other previous approaches adopting
similar ideas, our framework supports more diverse types of reasoning such as
nested loops and different types of recursion. To evaluate our reasoning
framework, we build a synthetic dataset with five tasks that require
compositional reasoning. Results show that our reasoning framework can enhance
the language model's compositional generalization performance on the five
tasks, using a fine-tuned language model. We also discussed the possibility and
the challenges to combine our reasoning framework with a few-shot prompted
language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HausaNLP at SemEval-2023 Task 10: Transfer Learning, Synthetic Data and Side-Information for Multi-Level Sexism Classification. (arXiv:2305.00076v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00076">
<div class="article-summary-box-inner">
<span><p>We present the findings of our participation in the SemEval-2023 Task 10:
Explainable Detection of Online Sexism (EDOS) task, a shared task on offensive
language (sexism) detection on English Gab and Reddit dataset. We investigated
the effects of transferring two language models: XLM-T (sentiment
classification) and HateBERT (same domain -- Reddit) for multi-level
classification into Sexist or not Sexist, and other subsequent
sub-classifications of the sexist data. We also use synthetic classification of
unlabelled dataset and intermediary class information to maximize the
performance of our models. We submitted a system in Task A, and it ranked 49th
with F1-score of 0.82. This result showed to be competitive as it only
under-performed the best system by 0.052% F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis. (arXiv:2305.00090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00090">
<div class="article-summary-box-inner">
<span><p>This paper describes our system developed for the SemEval-2023 Task 12
"Sentiment Analysis for Low-resource African Languages using Twitter Dataset".
Sentiment analysis is one of the most widely studied applications in natural
language processing. However, most prior work still focuses on a small number
of high-resource languages. Building reliable sentiment analysis systems for
low-resource languages remains challenging, due to the limited training data in
this task. In this work, we propose to leverage language-adaptive and
task-adaptive pretraining on African texts and study transfer learning with
source language selection on top of an African language-centric pretrained
language model. Our key findings are: (1) Adapting the pretrained model to the
target language and task using a small yet relevant corpus improves performance
remarkably by more than 10 F1 score points. (2) Selecting source languages with
positive transfer gains during training can avoid harmful interference from
dissimilar languages, leading to better results in multilingual and
cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
and, in particular, performs best in the multilingual evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. (arXiv:2305.00118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00118">
<div class="article-summary-box-inner">
<span><p>In this work, we carry out a data archaeology to infer books that are known
to ChatGPT and GPT-4 using a name cloze membership inference query. We find
that OpenAI models have memorized a wide collection of copyrighted materials,
and that the degree of memorization is tied to the frequency with which
passages of those books appear on the web. The ability of these models to
memorize an unknown set of books complicates assessments of measurement
validity for cultural analytics by contaminating test data; we show that models
perform much better on memorized books than on non-memorized books for
downstream tasks. We argue that this supports a case for open models whose
training data is known.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining European Press Coverage of the Covid-19 No-Vax Movement: An NLP Framework. (arXiv:2305.00182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00182">
<div class="article-summary-box-inner">
<span><p>This paper examines how the European press dealt with the no-vax reactions
against the Covid-19 vaccine and the dis- and misinformation associated with
this movement. Using a curated dataset of 1786 articles from 19 European
newspapers on the anti-vaccine movement over a period of 22 months in
2020-2021, we used Natural Language Processing techniques including topic
modeling, sentiment analysis, semantic relationship with word embeddings,
political analysis, named entity recognition, and semantic networks, to
understand the specific role of the European traditional press in the
disinformation ecosystem. The results of this multi-angle analysis demonstrate
that the European well-established press actively opposed a variety of hoaxes
mainly spread on social media, and was critical of the anti-vax trend,
regardless of the political orientation of the newspaper. This confirms the
relevance of studying the role of high-quality press in the disinformation
ecosystem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00217">
<div class="article-summary-box-inner">
<span><p>In a recent paper published in the Journal of Language Evolution, Kauhanen,
Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the
results presented in one of my papers (Koplenig, Royal Society Open Science, 6,
181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show
through a series of statistical analyses that large numbers of L2 (second
language) speakers do not seem to affect the (grammatical or statistical)
complexity of a language. To this end, I focus on the way in which the
Ethnologue assesses language status: a language is characterised as vehicular
if, in addition to being used by L1 (first language) speakers, it should also
have a significant number of L2 users. KEW criticise both the use of
vehicularity as a (binary) indicator of whether a language has a significant
number of L2 users and the idea of imputing a zero proportion of L2 speakers to
non-vehicular languages whenever a direct estimate of that proportion is
unavailable. While I recognise the importance of post-publication commentary on
published research, I show in this rejoinder that both points of criticism are
explicitly mentioned and analysed in my paper. In addition, I also comment on
other points raised by KEW and demonstrate that both alternative analyses
offered by KEW do not stand up to closer scrutiny.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention. (arXiv:2305.00262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00262">
<div class="article-summary-box-inner">
<span><p>Compared with standard text, understanding dialogue is more challenging for
machines as the dynamic and unexpected semantic changes in each turn. To model
such inconsistent semantics, we propose a simple but effective Hierarchical
Dialogue Understanding model, HiDialog. Specifically, we first insert multiple
special tokens into a dialogue and propose the turn-level attention to learn
turn embeddings hierarchically. Then, a heterogeneous graph module is leveraged
to polish the learned embeddings. We evaluate our model on various dialogue
understanding tasks including dialogue relation extraction, dialogue emotion
recognition, and dialogue act classification. Results show that our simple
approach achieves state-of-the-art performance on all three tasks above. All
our source code is publicly available at https://github.com/ShawX825/HiDialog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cognitive Account of the Puzzle of Ideography. (arXiv:2305.00296v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00296">
<div class="article-summary-box-inner">
<span><p>In this commentary article to 'The Puzzle of Ideography' by Morin, we put
forth a new cognitive account of the puzzle of ideography, that complements the
standardization account of Morin. Efficient standardization of spoken language
is phenomenologically attributed to a modality effect coupled with chunking of
cognitive representations, further aided by multi-sensory integration and the
serialized nature of attention. These cognitive mechanisms are crucial for
explaining why languages dominate graphic codes for general-purpose human
communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models. (arXiv:2305.00350v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00350">
<div class="article-summary-box-inner">
<span><p>Through prompting, large-scale pre-trained models have become more expressive
and powerful, gaining significant attention in recent years. Though these big
models have zero-shot capabilities, in general, labeled data are still required
to adapt them to downstream tasks. To overcome this critical limitation, we
propose an unsupervised fine-tuning framework to directly fine-tune the model
or prompt on the unlabeled target data. We demonstrate how to apply our method
to both language-augmented vision and masked-language models by aligning the
discrete distributions extracted from the prompts and target data. To verify
our approach's applicability, we conduct extensive experiments on image
classification, sentiment analysis, and natural language inference tasks.
Across 13 image-related tasks and 15 language-related ones, the proposed
approach achieves consistent improvements over the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2abEL: A Dataset for Entity Linking from Scientific Tables. (arXiv:2305.00366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00366">
<div class="article-summary-box-inner">
<span><p>Entity linking (EL) is the task of linking a textual mention to its
corresponding entry in a knowledge base, and is critical for many
knowledge-intensive NLP applications. When applied to tables in scientific
papers, EL is a step toward large-scale scientific knowledge bases that could
enable advanced scientific question answering and analytics. We present the
first dataset for EL in scientific tables. EL for scientific tables is
especially challenging because scientific knowledge bases can be very
incomplete, and disambiguating table mentions typically requires understanding
the papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL
in machine learning results tables and includes hand-labeled cell types,
attributed sources, and entity links from the PaperswithCode taxonomy for 8,429
cells from 732 tables. We introduce a neural baseline method designed for EL on
scientific tables containing many out-of-knowledge-base mentions, and show that
it significantly outperforms a state-of-the-art generic table EL method. The
best baselines fall below human performance, and our analysis highlights
avenues for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00382">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs have shown promise for several cybersecurity tasks, such as
vulnerability assessment and threat analysis. In this work, we present a new
method for constructing a vulnerability knowledge graph from information in the
National Vulnerability Database (NVD). Our approach combines named entity
recognition (NER), relation extraction (RE), and entity prediction using a
combination of neural models, heuristic rules, and knowledge graph embeddings.
We demonstrate how our method helps to fix missing entities in knowledge graphs
used for cybersecurity and evaluate the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Non-native Speech Corpus Featuring Chinese-English Bilingual Children: Compilation and Rationale. (arXiv:2305.00446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00446">
<div class="article-summary-box-inner">
<span><p>This paper introduces a non-native speech corpus consisting of narratives
from fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5
hours of children taking a narrative comprehension test in English (L2) are
presented, along with human-rated scores and annotations of grammatical and
pronunciation errors. The children also completed the parallel MAIN tests in
Chinese (L1) for reference purposes. For all tests we recorded audio and video
with our innovative self-developed remote collection methods. The video
recordings serve to mitigate the challenge of low intelligibility in L2
narratives produced by young children during the transcription process. This
corpus offers valuable resources for second language teaching and has the
potential to enhance the overall performance of automatic speech recognition
(ASR).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support. (arXiv:2305.00450v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00450">
<div class="article-summary-box-inner">
<span><p>There has been an increasing research interest in developing specialized
dialogue systems that can offer mental health support. However, gathering
large-scale and real-life multi-turn conversations for mental health support
poses challenges due to the sensitivity of personal information, as well as the
time and cost involved. To address these issues, we introduce the SMILE
approach, an inclusive language expansion technique that employs ChatGPT to
extend public single-turn dialogues into multi-turn ones. Our research first
presents a preliminary exploratory study that validates the effectiveness of
the SMILE approach. Furthermore, we conduct a comprehensive and systematic
contrastive analysis of datasets generated with and without the SMILE approach,
demonstrating that the SMILE method results in a large-scale, diverse, and
close-to-real-life multi-turn mental health support conversation corpus,
including dialog topics, lexical and semantic features. Finally, we use the
collected corpus (SMILECHAT) to develop a more effective dialogue system that
offers emotional support and constructive suggestions in multi-turn
conversations for mental health support.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to enumerate trees from a context-free grammar. (arXiv:2305.00522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00522">
<div class="article-summary-box-inner">
<span><p>I present a simple algorithm for enumerating the trees generated by a Context
Free Grammar (CFG). The algorithm uses a pairing function to form a bijection
between CFG derivations and natural numbers, so that trees can be uniquely
decoded from counting. This provides a general way to number expressions in
natural logical languages, and potentially can be extended to other
combinatorial problems. I also show how this algorithm may be generalized to
more general forms of derivation, including analogs of Lempel-Ziv coding on
trees.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Graph Transformer for Multimodal Question Answering. (arXiv:2305.00581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00581">
<div class="article-summary-box-inner">
<span><p>Despite the success of Transformer models in vision and language tasks, they
often learn knowledge from enormous data implicitly and cannot utilize
structured input data directly. On the other hand, structured learning
approaches such as graph neural networks (GNNs) that integrate prior
information can barely compete with Transformer models. In this work, we aim to
benefit from both worlds and propose a novel Multimodal Graph Transformer for
question answering tasks that requires performing reasoning across multiple
modalities. We introduce a graph-involved plug-and-play quasi-attention
mechanism to incorporate multimodal graph information, acquired from text and
visual data, to the vanilla self-attention as effective prior. In particular,
we construct the text graph, dense region graph, and semantic graph to generate
adjacency matrices, and then compose them with input vision and language
features to perform downstream reasoning. Such a way of regularizing
self-attention with graph information significantly improves the inferring
ability and helps align features from different modalities. We validate the
effectiveness of Multimodal Graph Transformer over its Transformer baselines on
GQA, VQAv2, and MultiModalQA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00586">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as "The war lasted from the year 1732 to the year 17", and predict valid
two-digit end years (years &gt; 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we show that our circuit generalizes to other tasks,
playing a role in other greater-than scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Gradient-free and Likelihood-free Prompt Tuning. (arXiv:2305.00593v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00593">
<div class="article-summary-box-inner">
<span><p>Due to privacy or commercial constraints, large pre-trained language models
(PLMs) are often offered as black-box APIs. Fine-tuning such models to
downstream tasks is challenging because one can neither access the model's
internal representations nor propagate gradients through it. This paper
addresses these challenges by developing techniques for adapting PLMs with only
API access. Building on recent work on soft prompt tuning, we develop methods
to tune the soft prompts without requiring gradient computation. Further, we
develop extensions that in addition to not requiring gradients also do not need
to access any internal representation of the PLM beyond the input embeddings.
Moreover, instead of learning a single prompt, our methods learn a distribution
over prompts allowing us to quantify predictive uncertainty. Ours is the first
work to consider uncertainty in prompts when only having API access to the PLM.
Finally, through extensive experiments, we carefully vet the proposed methods
and find them competitive with (and sometimes even improving on) gradient-based
approaches with full access to the PLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resourced Machine Translation for Senegalese Wolof Language. (arXiv:2305.00606v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00606">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) research has made great advancements in
recent years with major breakthroughs that have established new benchmarks.
However, these advances have mainly benefited a certain group of languages
commonly referred to as resource-rich such as English and French. Majority of
other languages with weaker resources are then left behind which is the case
for most African languages including Wolof. In this work, we present a parallel
Wolof/French corpus of 123,000 sentences on which we conducted experiments on
machine translation models based on Recurrent Neural Networks (RNN) in
different data configurations. We noted performance gains with the models
trained on subworded data as well as those trained on the French-English
language pair compared to those trained on the French-Wolof pair under the same
experimental conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00633">
<div class="article-summary-box-inner">
<span><p>We propose an effective prompting approach that integrates self-evaluation
guidance through stochastic beam search. Our approach explores the reasoning
search space using a well-calibrated automatic criterion. This enables an
efficient search to produce higher-quality final predictions. With the
self-evaluation guided stochastic beam search, we also balance the
quality--diversity trade-off in the generation of reasoning chains. This allows
our approach to adapt well with majority voting and surpass the corresponding
Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K,
AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis
of our decompositional reasoning finds it pinpoints logic failures and leads to
higher consistency and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Error correction and extraction in request dialogs. (arXiv:2004.04243v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.04243">
<div class="article-summary-box-inner">
<span><p>We propose a dialog system utility component that gets the two last
utterances of a user and can detect whether the last utterance is an error
correction of the second last utterance. If yes, it corrects the second last
utterance according to the error correction in the last utterance. In addition,
the proposed component outputs the extracted pairs of reparandum and repair
entity. This component offers two advantages, learning the concept of
corrections to avoid collecting corrections for every new domain and extracting
reparandum and repair pairs, which offers the possibility to learn out of it.
</p>
<p>For the error correction one sequence labeling and two sequence to sequence
approaches are presented. For the error correction detection these three error
correction approaches can also be used and in addition, we present a sequence
classification approach. One error correction detection and one error
correction approach can be combined to a pipeline or the error correction
approaches can be trained and used end-to-end to avoid two components. We
modified the EPIC-KITCHENS-100 dataset to evaluate the approaches for
correcting entity phrases in request dialogs. For error correction detection
and correction, we got an accuracy of 96.40 % on synthetic validation data and
an accuracy of 77.85 % on human-created real-world test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Subgraph-Guided Knowledge Graph Question Generation with Graph Neural Networks. (arXiv:2004.06015v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06015">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) question generation (QG) aims to generate natural
language questions from KGs and target answers. Previous works mostly focus on
a simple setting which is to generate questions from a single KG triple. In
this work, we focus on a more realistic setting where we aim to generate
questions from a KG subgraph and target answers. In addition, most of previous
works built on either RNN-based or Transformer based models to encode a
linearized KG sugraph, which totally discards the explicit structure
information of a KG subgraph. To address this issue, we propose to apply a
bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we
enhance our RNN decoder with node-level copying mechanism to allow directly
copying node attributes from the KG subgraph to the output question. Both
automatic and human evaluation results demonstrate that our model achieves new
state-of-the-art scores, outperforming existing methods by a significant margin
on two QG benchmarks. Experimental results also show that our QG model can
consistently benefit the Question Answering (QA) task as a mean of data
augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone. (arXiv:2112.02418v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02418">
<div class="article-summary-box-inner">
<span><p>YourTTS brings the power of a multilingual approach to the task of zero-shot
multi-speaker TTS. Our method builds upon the VITS model and adds several novel
modifications for zero-shot multi-speaker and multilingual training. We
achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and
results comparable to SOTA in zero-shot voice conversion on the VCTK dataset.
Additionally, our approach achieves promising results in a target language with
a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS
and zero-shot voice conversion systems in low-resource languages. Finally, it
is possible to fine-tune the YourTTS model with less than 1 minute of speech
and achieve state-of-the-art results in voice similarity and with reasonable
quality. This is important to allow synthesis for speakers with a very
different voice or recording characteristics from those seen during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Explanations and Human Understanding. (arXiv:2202.04092v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04092">
<div class="article-summary-box-inner">
<span><p>Explanations are hypothesized to improve human understanding of machine
learning models and achieve a variety of desirable outcomes, ranging from model
debugging to enhancing human decision making. However, empirical studies have
found mixed and even negative results. An open question, therefore, is under
what conditions explanations can improve human understanding and in what way.
Using adapted causal diagrams, we provide a formal characterization of the
interplay between machine explanations and human understanding, and show how
human intuitions play a central role in enabling human understanding.
Specifically, we identify three core concepts of interest that cover all
existing quantitative measures of understanding in the context of human-AI
decision making: task decision boundary, model decision boundary, and model
error. Our key result is that without assumptions about task-specific
intuitions, explanations may potentially improve human understanding of model
decision boundary, but they cannot improve human understanding of task decision
boundary or model error. To achieve complementary human-AI performance, we
articulate possible ways on how explanations need to work with human
intuitions. For instance, human intuitions about the relevance of features
(e.g., education is more important than age in predicting a person's income)
can be critical in detecting model error. We validate the importance of human
intuitions in shaping the outcome of machine explanations with empirical
human-subject studies. Overall, our work provides a general framework along
with actionable implications for future algorithmic development and empirical
experiments of machine explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition. (arXiv:2204.03855v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03855">
<div class="article-summary-box-inner">
<span><p>Low-resource speech recognition has been long-suffering from insufficient
training data. In this paper, we propose an approach that leverages neighboring
languages to improve low-resource scenario performance, founded on the
hypothesis that similar linguistic units in neighboring languages exhibit
comparable term frequency distributions, which enables us to construct a
Huffman tree for performing multilingual hierarchical Softmax decoding. This
hierarchical structure enables cross-lingual knowledge sharing among similar
tokens, thereby enhancing low-resource training outcomes. Empirical analyses
demonstrate that our method is effective in improving the accuracy and
efficiency of low-resource speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Self-Refinement for Robust Learning with Weak Supervision. (arXiv:2205.07290v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07290">
<div class="article-summary-box-inner">
<span><p>Training deep neural networks (DNNs) under weak supervision has attracted
increasing research attention as it can significantly reduce the annotation
cost. However, labels from weak supervision can be noisy, and the high capacity
of DNNs enables them to easily overfit the label noise, resulting in poor
generalization. Recent methods leverage self-training to build noise-resistant
models, in which a teacher trained under weak supervision is used to provide
highly confident labels for teaching the students. Nevertheless, the teacher
derived from such frameworks may have fitted a substantial amount of noise and
therefore produce incorrect pseudo-labels with high confidence, leading to
severe error propagation. In this work, we propose Meta Self-Refinement (MSR),
a noise-resistant learning framework, to effectively combat label noise from
weak supervision. Instead of relying on a fixed teacher trained with noisy
labels, we encourage the teacher to refine its pseudo-labels. At each training
step, MSR performs a meta gradient descent on the current mini-batch to
maximize the student performance on a clean validation set. Extensive
experimentation on eight NLP benchmarks demonstrates that MSR is robust against
label noise in all settings and outperforms state-of-the-art methods by up to
11.4% in accuracy and 9.26% in F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11871">
<div class="article-summary-box-inner">
<span><p>Large language models distill broad knowledge from text corpora. However,
they can be inconsistent when it comes to completing user specified tasks. This
issue can be addressed by finetuning such models via supervised learning on
curated datasets, or via reinforcement learning. In this work, we propose a
novel offline RL method, implicit language Q-learning (ILQL), designed for use
on language models, that combines both the flexible utility maximization
framework of RL algorithms with the ability of supervised learning to leverage
previously collected data, as well as its simplicity and stability. Our method
employs a combination of value conservatism alongside an implicit dataset
support constraint in learning value functions, which are then used to guide
language model generations towards maximizing user-specified utility functions.
In addition to empirically validating ILQL, we present a detailed empirical
analysis of situations where offline RL can be useful in natural language
generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can
effectively optimize high variance reward functions based on subjective
judgement, such as whether to label a comment as toxic or not.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generation with a Question-Answering Blueprint. (arXiv:2207.00397v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00397">
<div class="article-summary-box-inner">
<span><p>The ability to convey relevant and faithful information is critical for many
tasks in conditional generation and yet remains elusive for neural seq-to-seq
models whose outputs often reveal hallucinations and fail to correctly cover
important details. In this work, we advocate planning as a useful intermediate
representation for rendering conditional generation less opaque and more
grounded. Our work proposes a new conceptualization of text plans as a sequence
of question-answer (QA) pairs. We enhance existing datasets (e.g., for
summarization) with a QA blueprint operating as a proxy for both content
selection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain
blueprints automatically by exploiting state-of-the-art question generation
technology and convert input-output pairs into input-blueprint-output tuples.
We develop Transformer-based models, each varying in how they incorporate the
blueprint in the generated output (e.g., as a global plan or iteratively).
Evaluation across metrics and datasets demonstrates that blueprint models are
more factual than alternatives which do not resort to planning and allow
tighter control of the generation output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Readability Controllable Biomedical Document Summarization. (arXiv:2210.04705v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04705">
<div class="article-summary-box-inner">
<span><p>Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Representation Distillation with Contrastive Learning. (arXiv:2210.05033v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05033">
<div class="article-summary-box-inner">
<span><p>Multilingual sentence representations from large models encode semantic
information from two or more languages and can be used for different
cross-lingual information retrieval and matching tasks. In this paper, we
integrate contrastive learning into multilingual representation distillation
and use it for quality estimation of parallel sentences (i.e., find
semantically similar sentences that can be used as translations of each other).
We validate our approach with multilingual similarity search and corpus
filtering tasks. Experiments across different low-resource languages show that
our method greatly outperforms previous sentence encoders such as LASER,
LASER3, and LaBSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus. (arXiv:2210.06405v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06405">
<div class="article-summary-box-inner">
<span><p>Because of its importance in studying people's thoughts on various Web 2.0
services, emotion classification (EC) is an important undertaking. Existing
research, on the other hand, is mostly focused on the English language, with
little work on low-resource languages. Though sentiment analysis, particularly
the EC in English, has received a lot of attention in recent years, little
study has been done in the context of Bangla, one of the world's most widely
spoken languages. We propose a complete set of approaches for identifying and
extracting emotions from Bangla texts in this research. We provide a Bangla
emotion classifier for six classes (anger, disgust, fear, joy, sadness, and
surprise) from Bangla words, using transformer-based models which exhibit
phenomenal results in recent days, especially for high resource languages. The
"Unified Bangla Multi-class Emotion Corpus (UBMEC)" is used to assess the
performance of our models. UBMEC was created by combining two previously
released manually labeled datasets of Bangla comments on 6-emotion classes with
fresh manually tagged Bangla comments created by us. The corpus dataset and
code we used in this work is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers. (arXiv:2210.06425v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06425">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (LMs) have become an integral part of Natural
Language Processing (NLP) in recent years, due to their superior performance in
downstream applications. In spite of this resounding success, the usability of
LMs is constrained by computational and time complexity, along with their
increasing size; an issue that has been referred to as `overparameterisation'.
Different strategies have been proposed in the literature to alleviate these
problems, with the aim to create effective compact models that nearly match the
performance of their bloated counterparts with negligible performance losses.
One of the most popular techniques in this area of research is model
distillation. Another potent but underutilised technique is cross-layer
parameter sharing. In this work, we combine these two strategies and present
MiniALBERT, a technique for converting the knowledge of fully parameterised LMs
(such as BERT) into a compact recursive student. In addition, we investigate
the application of bottleneck adapters for layer-wise adaptation of our
recursive student, and also explore the efficacy of adapter tuning for
fine-tuning of compact models. We test our proposed models on a number of
general and biomedical NLP tasks to demonstrate their viability and compare
them with the state-of-the-art and other existing compact models. All the codes
used in the experiments are available at
https://github.com/nlpie-research/MiniALBERT. Our pre-trained compact models
can be accessed from https://huggingface.co/nlpie.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Document-level Information Extraction via Imitation Learning. (arXiv:2210.06600v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06600">
<div class="article-summary-box-inner">
<span><p>We present a novel iterative extraction model, IterX, for extracting complex
relations, or templates (i.e., N-tuples representing a mapping from named slots
to spans of text) within a document. Documents may feature zero or more
instances of a template of any given type, and the task of template extraction
entails identifying the templates in a document and extracting each template's
slot values. Our imitation learning approach casts the problem as a Markov
decision process (MDP), and relieves the need to use predefined template orders
to train an extractor. It leads to state-of-the-art results on two established
benchmarks -- 4-ary relation extraction on SciREX and template extraction on
MUC-4 -- as well as a strong baseline on the new BETTER Granular task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text. (arXiv:2210.06990v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06990">
<div class="article-summary-box-inner">
<span><p>Data sparsity is one of the main challenges posed by code-switching (CS),
which is further exacerbated in the case of morphologically rich languages. For
the task of machine translation (MT), morphological segmentation has proven
successful in alleviating data sparsity in monolingual contexts; however, it
has not been investigated for CS settings. In this paper, we study the
effectiveness of different segmentation approaches on MT performance, covering
morphology-based and frequency-based segmentation techniques. We experiment on
MT from code-switched Arabic-English to English. We provide detailed analysis,
examining a variety of conditions, such as data size and sentences with
different degrees of CS. Empirical results show that morphology-aware
segmenters perform the best in segmentation tasks but under-perform in MT.
Nevertheless, we find that the choice of the segmentation setup to use for MT
is highly dependent on the data size. For extreme low-resource scenarios, a
combination of frequency and morphology-based segmentations is shown to perform
the best. For more resourced settings, such a combination does not bring
significant improvements over the use of frequency-based segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammatical Error Correction: A Survey of the State of the Art. (arXiv:2211.05166v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05166">
<div class="article-summary-box-inner">
<span><p>Grammatical Error Correction (GEC) is the task of automatically detecting and
correcting errors in text. The task not only includes the correction of
grammatical errors, such as missing prepositions and mismatched subject-verb
agreement, but also orthographic and semantic errors, such as misspellings and
word choice errors respectively. The field has seen significant progress in the
last decade, motivated in part by a series of five shared tasks, which drove
the development of rule-based methods, statistical classifiers, statistical
machine translation, and finally neural machine translation systems which
represent the current dominant state of the art. In this survey paper, we
condense the field into a single article and first outline some of the
linguistic challenges of the task, introduce the most popular datasets that are
available to researchers (for both English and other languages), and summarise
the various methods and techniques that have been developed with a particular
focus on artificial error generation. We next describe the many different
approaches to evaluation as well as concerns surrounding metric reliability,
especially in relation to subjective human judgements, before concluding with
an overview of recent progress and suggestions for future work and remaining
challenges. We hope that this survey will serve as comprehensive resource for
researchers who are new to the field or who want to be kept apprised of recent
developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08283">
<div class="article-summary-box-inner">
<span><p>Most TextVQA approaches focus on the integration of objects, scene texts and
question words by a simple transformer encoder. But this fails to capture the
semantic relations between different modalities. The paper proposes a Scene
Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the
semantic relations among the objects, Optical Character Recognition (OCR)
tokens and the question words. It is achieved by a TextVQA-based scene graph
that discovers the underlying semantics of an image. We created a
guided-attention module to capture the intra-modal interplay between the
language and the vision as a guidance for inter-modal interactions. To make
explicit teaching of the relations between the two modalities, we proposed and
integrated two attention modules, namely a scene graph-based semantic
relation-aware attention and a positional relation-aware attention. We
conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.
It is shown that our SceneGATE method outperformed existing ones because of the
scene graph and its attention modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09849">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14052">
<div class="article-summary-box-inner">
<span><p>State space models (SSMs) have demonstrated state-of-the-art sequence
modeling performance in some modalities, but underperform attention in language
modeling. Moreover, despite scaling nearly linearly in sequence length instead
of quadratically, SSMs are still slower than Transformers due to poor hardware
utilization. In this paper, we make progress on understanding the expressivity
gap between SSMs and attention in language modeling, and on reducing the
hardware barrier between SSMs and attention. First, we use synthetic language
modeling tasks to understand the gap between SSMs and attention. We find that
existing SSMs struggle with two capabilities: recalling earlier tokens in the
sequence and comparing tokens across the sequence. To understand the impact on
language modeling, we propose a new SSM layer, H3, that is explicitly designed
for these abilities. H3 matches attention on the synthetic languages and comes
within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid
125M-parameter H3-attention model that retains two attention layers
surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to
improve the efficiency of training SSMs on modern hardware, we propose
FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on
sequences up to 8K, and introduces a novel state passing algorithm that
exploits the recurrent properties of SSMs to scale to longer sequences.
FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allows
hybrid language models to generate text 2.4$\times$ faster than Transformers.
Using FlashConv, we scale hybrid H3-attention language models up to 2.7B
parameters on the Pile and find promising initial results, achieving lower
perplexity than Transformers and outperforming Transformers in zero- and
few-shot learning on a majority of tasks in the SuperGLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01664">
<div class="article-summary-box-inner">
<span><p>Recent studies on knowledge graphs (KGs) show that path-based methods
empowered by pre-trained language models perform well in the provision of
inductive and explainable relation predictions. In this paper, we introduce the
concepts of relation path coverage and relation path confidence to filter out
unreliable paths prior to model training to elevate the model performance.
Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict
inductive relations in KGs. KRST is designed to encode the extracted reliable
paths in KGs, allowing us to properly cluster paths and provide multi-aspect
explanations. We conduct extensive experiments on three real-world datasets.
The experimental results show that compared to SOTA models, KRST achieves the
best performance in most transductive and inductive test cases (4 of 6), and in
11 of 12 few-shot test cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Semantic Relatedness Dataset for Image Captioning. (arXiv:2301.08784v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08784">
<div class="article-summary-box-inner">
<span><p>Modern image captioning system relies heavily on extracting knowledge from
images to capture the concept of a static story. In this paper, we propose a
textual visual context dataset for captioning, in which the publicly available
dataset COCO Captions (Lin et al., 2014) has been extended with information
about the scene (such as objects in the image). Since this information has a
textual form, it can be used to leverage any NLP task, such as text similarity
or semantic relation methods, into captioning systems, either as an end-to-end
training strategy or a post-processing based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10871">
<div class="article-summary-box-inner">
<span><p>Our work advances an approach for predicting hate speech in social media,
drawing out the critical need to consider the discussions that follow a post to
successfully detect when hateful discourse may arise. Using graph transformer
networks, coupled with modelling attention and BERT-level natural language
processing, our approach can capture context and anticipate upcoming
anti-social behaviour. In this paper, we offer a detailed qualitative analysis
of this solution for hate speech detection in social networks, leading to
insights into where the method has the most impressive outcomes in comparison
with competitors and identifying scenarios where there are challenges to
achieving ideal performance. Included is an exploration of the kinds of posts
that permeate social media today, including the use of hateful images. This
suggests avenues for extending our model to be more comprehensive. A key
insight is that the focus on reasoning about the concept of context positions
us well to be able to support multi-modal analysis of online posts. We conclude
with a reflection on how the problem we are addressing relates especially well
to the theme of dynamic change, a critical concern for all AI solutions for
social impact. We also comment briefly on how mental health well-being can be
advanced with our work, through curated content attuned to the extent of hate
in posts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages. (arXiv:2302.08956v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08956">
<div class="article-summary-box-inner">
<span><p>Africa is home to over 2000 languages from over six language families and has
the highest linguistic diversity among all continents. This includes 75
languages with at least one million speakers each. Yet, there is little NLP
research conducted on African languages. Crucial in enabling such research is
the availability of high-quality annotated datasets. In this paper, we
introduce AfriSenti, which consists of 14 sentiment datasets of 110,000+ tweets
in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda,
Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,
Tigrinya, Twi, Xitsonga, and Yor\`ub\'a) from four language families annotated
by native speakers. The data is used in SemEval 2023 Task 12, the first
Afro-centric SemEval shared task. We describe the data collection methodology,
annotation process, and related challenges when curating each of the datasets.
We conduct experiments with different sentiment classification baselines and
discuss their usefulness. We hope AfriSenti enables new work on
under-represented languages. The dataset is available at
https://github.com/afrisenti-semeval/afrisent-semeval-2023 and can also be
loaded as a huggingface datasets
(https://huggingface.co/datasets/shmuhammad/AfriSenti).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09419">
<div class="article-summary-box-inner">
<span><p>Pretrained Foundation Models (PFMs) are regarded as the foundation for
various downstream tasks with different data modalities. A PFM (e.g., BERT,
ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable
parameter initialization for a wide range of downstream applications. BERT
learns bidirectional encoder representations from Transformers, which are
trained on large datasets as contextual language models. Similarly, the
generative pretrained transformer (GPT) method employs Transformers as the
feature extractor and is trained using an autoregressive paradigm on large
datasets. Recently, ChatGPT shows promising success on large language models,
which applies an autoregressive language model with zero shot or few shot
prompting. The remarkable achievements of PFM have brought significant
breakthroughs to various fields of AI. Numerous studies have proposed different
methods, raising the demand for an updated survey. This study provides a
comprehensive review of recent research advancements, challenges, and
opportunities for PFMs in text, image, graph, as well as other data modalities.
The review covers the basic components and existing pretraining methods used in
natural language processing, computer vision, and graph learning. Additionally,
it explores advanced PFMs used for different data modalities and unified PFMs
that consider data quality and quantity. The review also discusses research
related to the fundamentals of PFMs, such as model efficiency and compression,
security, and privacy. Finally, the study provides key implications, future
research directions, challenges, and open problems in the field of PFMs.
Overall, this survey aims to shed light on the research of the PFMs on
scalability, security, logical reasoning ability, cross-domain learning
ability, and the user-friendly interactive ability for artificial general
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04562">
<div class="article-summary-box-inner">
<span><p>We study the problem of extrapolative controlled generation, i.e., generating
sequences with attribute values beyond the range seen in training. This task is
of significant importance in automated design, especially drug discovery, where
the goal is to design novel proteins that are \textit{better} (e.g., more
stable) than existing sequences. Thus, by definition, the target sequences and
their attribute values are out of the training distribution, posing challenges
to existing methods that aim to directly generate the target sequence. Instead,
in this work, we propose Iterative Controlled Extrapolation (ICE) which
iteratively makes local edits to a sequence to enable extrapolation. We train
the model on synthetically generated sequence pairs that demonstrate small
improvement in the attribute value. Results on one natural language task
(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV
fitness) show that ICE considerably outperforms state-of-the-art approaches
despite its simplicity. Our code and models are available at:
https://github.com/vishakhpk/iter-extrapolation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.05737">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 18 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP) and make it publicly available for the community to further
develop clinically-aware ASR metrics. To our knowledge, this is the first
public dataset of its kind. We demonstrate that CBERTScore more closely matches
what clinicians prefer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07103">
<div class="article-summary-box-inner">
<span><p>There has recently been widespread discussion of whether large language
models might be sentient or conscious. Should we take this idea seriously? I
will break down the strongest reasons for and against. Given mainstream
assumptions in the science of consciousness, there are significant obstacles to
consciousness in current models: for example, their lack of recurrent
processing, a global workspace, and unified agency. At the same time, it is
quite possible that these obstacles will be overcome in the next decade or so.
I conclude that while it is somewhat unlikely that current large language
models are conscious, we should take seriously the possibility that successors
to large language models may be conscious in the not-too-distant future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). (arXiv:2304.06845v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06845">
<div class="article-summary-box-inner">
<span><p>We present the first Africentric SemEval Shared task, Sentiment Analysis for
African Languages (AfriSenti-SemEval) - The dataset is available at
https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval
is a sentiment classification challenge in 14 African languages: Amharic,
Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican
Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and
Yor\`ub\'a (Muhammad et al., 2023), using data labeled with 3 sentiment
classes. We present three subtasks: (1) Task A: monolingual classification,
which received 44 submissions; (2) Task B: multilingual classification, which
received 32 submissions; and (3) Task C: zero-shot classification, which
received 34 submissions. The best performance for tasks A and B was achieved by
NLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP
achieved the best average score for task C with 58.15 weighted F1. We describe
the various approaches adopted by the top 10 systems and their approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06875">
<div class="article-summary-box-inner">
<span><p>As language models scale up, it becomes increasingly expensive to verify
research ideas because conclusions on small models do not trivially transfer to
large ones. A possible solution is to establish a generic system that directly
predicts some metrics for large models solely based on the results and
hyperparameters from small models. Existing methods based on scaling laws
require hyperparameter search on the largest models, which is impractical with
limited resources. We address this issue by presenting our discoveries
indicating that Maximal Update parametrization (muP) enables accurate fitting
of scaling laws for hyperparameters close to common loss basins, without any
search. Thus, different models can be directly compared on large scales with
loss prediction even before the training starts. We propose a new paradigm as a
first step towards reliable academic research for any model scale without heavy
computation. Code will be publicly available shortly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07772">
<div class="article-summary-box-inner">
<span><p>In recent years, the field of neural machine translation (NMT) for SPARQL
query generation has witnessed a significant growth. Recently, the
incorporation of the copy mechanism with traditional encoder-decoder
architectures and the use of pre-trained encoder-decoders have set new
performance benchmarks. This paper presents a large variety of experiments that
replicate and expand upon recent NMT-based SPARQL generation studies, comparing
pre-trained and non-pre-trained models, question annotation formats, and the
use of a copy mechanism for non-pre-trained and pre-trained models. Our results
show that either adding the copy mechanism or using a question annotation
improves performances for nonpre-trained models and for pre-trained models,
setting new baselines for three popular datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval 2023 Task 6: LegalEval - Understanding Legal Texts. (arXiv:2304.09548v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09548">
<div class="article-summary-box-inner">
<span><p>In populous countries, pending legal cases have been growing exponentially.
There is a need for developing NLP-based techniques for processing and
automatically understanding legal documents. To promote research in the area of
Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at
SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles
Labeling) is about automatically structuring legal documents into semantically
coherent units, Task-B (Legal Named Entity Recognition) deals with identifying
relevant entities in a legal document and Task-C (Court Judgement Prediction
with Explanation) explores the possibility of automatically predicting the
outcome of a legal case along with providing an explanation for the prediction.
In total 26 teams (approx. 100 participants spread across the world) submitted
systems paper. In each of the sub-tasks, the proposed systems outperformed the
baselines; however, there is a lot of scope for improvement. This paper
describes the tasks, and analyzes techniques proposed by various teams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13620">
<div class="article-summary-box-inner">
<span><p>Automatic chart to text summarization is an effective tool for the visually
impaired people along with providing precise insights of tabular data in
natural language to the user. A large and well-structured dataset is always a
key part for data driven models. In this paper, we propose ChartSumm: a
large-scale benchmark dataset consisting of a total of 84,363 charts along with
their metadata and descriptions covering a wide range of topics and chart types
to generate short and long summaries. Extensive experiments with strong
baseline models show that even though these models generate fluent and
informative summaries by achieving decent scores in various automatic
evaluation metrics, they often face issues like suffering from hallucination,
missing out important data points, in addition to incorrect explanation of
complex trends in the charts. We also investigated the potential of expanding
ChartSumm to other languages using automated translation tools. These make our
dataset a challenging benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13714">
<div class="article-summary-box-inner">
<span><p>Despite growing interest in using large language models (LLMs) in healthcare,
current explorations do not assess the real-world utility and safety of LLMs in
clinical settings. Our objective was to determine whether two LLMs can serve
information needs submitted by physicians as questions to an informatics
consultation service in a safe and concordant manner. Sixty six questions from
an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple
prompts. 12 physicians assessed the LLM responses' possibility of patient harm
and concordance with existing reports from an informatics consultation service.
Physician assessments were summarized based on majority vote. For no questions
did a majority of physicians deem either LLM response as harmful. For GPT-3.5,
responses to 8 questions were concordant with the informatics consult report,
20 discordant, and 9 were unable to be assessed. There were 29 responses with
no majority on "Agree", "Disagree", and "Unable to assess". For GPT-4,
responses to 13 questions were concordant, 15 discordant, and 3 were unable to
be assessed. There were 35 responses with no majority. Responses from both LLMs
were largely devoid of overt harm, but less than 20% of the responses agreed
with an answer from an informatics consultation service, responses contained
hallucinated references, and physicians were divided on what constitutes harm.
These results suggest that while general purpose LLMs are able to provide safe
and credible responses, they often do not meet the specific information need of
a given question. A definitive evaluation of the usefulness of LLMs in
healthcare settings will likely require additional research on prompt
engineering, calibration, and custom-tailoring of general purpose models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14721">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel framework that combines large language
models (LLMs), digital twins and industrial automation system to enable
intelligent planning and control of production processes. We retrofit the
automation system for a modular production facility and create executable
control interfaces of fine-granular functionalities and coarse-granular skills.
Low-level functionalities are executed by automation components, and high-level
skills are performed by automation modules. Subsequently, a digital twin system
is developed, registering these interfaces and containing additional
descriptive information about the production system. Based on the retrofitted
automation system and the created digital twins, LLM-agents are designed to
interpret descriptive information in the digital twins and control the physical
system through service interfaces. These LLM-agents serve as intelligent agents
on different levels within an automation system, enabling autonomous planning
and control of flexible production. Given a task instruction as input, the
LLM-agents orchestrate a sequence of atomic functionalities and skills to
accomplish the task. We demonstrate how our implemented prototype can handle
un-predefined tasks, plan a production process, and execute the operations.
This research highlights the potential of integrating LLMs into industrial
automation systems for more agile, flexible, and adaptive production processes,
while it also underscores the critical insights and limitations for future
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HQP: A Human-Annotated Dataset for Detecting Online Propaganda. (arXiv:2304.14931v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14931">
<div class="article-summary-box-inner">
<span><p>Online propaganda poses a severe threat to the integrity of societies.
However, existing datasets for detecting online propaganda have a key
limitation: they were annotated using weak labels that can be noisy and even
incorrect. To address this limitation, our work makes the following
contributions: (1) We present HQP: a novel dataset (N=30,000) for detecting
online propaganda with high-quality labels. To the best of our knowledge, HQP
is the first dataset for detecting online propaganda that was created through
human annotation. (2) We show empirically that state-of-the-art language models
fail in detecting online propaganda when trained with weak labels (AUC: 64.03).
In contrast, state-of-the-art language models can accurately detect online
propaganda when trained with our high-quality labels (AUC: 92.25), which is an
improvement of ~44%. (3) To address the cost of labeling, we extend our work to
few-shot learning. Specifically, we show that prompt-based learning using a
small sample of high-quality labels can still achieve a reasonable performance
(AUC: 80.27). Finally, we discuss implications for the NLP community to balance
the cost and quality of labeling. Crucially, our work highlights the importance
of high-quality labels for sensitive NLP tasks such as propaganda detection.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-02 23:11:19.318192378 UTC">2023-05-02 23:11:19 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>