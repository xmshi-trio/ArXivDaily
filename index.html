<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2024-01-29T01:30:00Z">01-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14440">
<div class="article-summary-box-inner">
<span><p>Recent studies of the emergent capabilities of transformer-based Natural
Language Understanding (NLU) models have indicated that they have an
understanding of lexical and compositional semantics. We provide evidence that
suggests these claims should be taken with a grain of salt: we find that
state-of-the-art Natural Language Inference (NLI) models are sensitive towards
minor semantics preserving surface-form variations, which lead to sizable
inconsistent model decisions during inference. Notably, this behaviour differs
from valid and in-depth comprehension of compositional semantics, however does
neither emerge when evaluating model accuracy on standard benchmarks nor when
probing for syntactic, monotonic, and logically robust reasoning. We propose a
novel framework to measure the extent of semantic sensitivity. To this end, we
evaluate NLI models on adversarially generated examples containing minor
semantics-preserving surface-form input noise. This is achieved using
conditional text generation, with the explicit condition that the NLI model
predicts the relationship between the original and adversarial inputs as a
symmetric equivalence entailment. We systematically study the effects of the
phenomenon across NLI models for \emph{in-} and \emph{out-of} domain settings.
Our experiments show that semantic sensitivity causes performance degradations
of $12.92\%$ and $23.71\%$ average over \emph{in-} and \emph{out-of-} domain
settings, respectively. We further perform ablation studies, analysing this
phenomenon across models, datasets, and variations in inference and show that
semantic sensitivity can lead to major inconsistency within model predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wordflow: Social Prompt Engineering for Large Language Models. (arXiv:2401.14447v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14447">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) require well-crafted prompts for effective use.
Prompt engineering, the process of designing prompts, is challenging,
particularly for non-experts who are less familiar with AI technologies. While
researchers have proposed techniques and tools to assist LLM users in prompt
design, these works primarily target AI application developers rather than
non-experts. To address this research gap, we propose social prompt
engineering, a novel paradigm that leverages social computing techniques to
facilitate collaborative prompt design. To investigate social prompt
engineering, we introduce Wordflow, an open-source and social text editor that
enables everyday users to easily create, run, share, and discover LLM prompts.
Additionally, by leveraging modern web technologies, Wordflow allows users to
run LLMs locally and privately in their browsers. Two usage scenarios highlight
how social prompt engineering and our tool can enhance laypeople's interaction
with LLMs. Wordflow is publicly accessible at
https://poloclub.github.io/wordflow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LongHealth: A Question Answering Benchmark with Long Clinical Documents. (arXiv:2401.14490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14490">
<div class="article-summary-box-inner">
<span><p>Background: Recent advancements in large language models (LLMs) offer
potential benefits in healthcare, particularly in processing extensive patient
records. However, existing benchmarks do not fully assess LLMs' capability in
handling real-world, lengthy clinical data.
</p>
<p>Methods: We present the LongHealth benchmark, comprising 20 detailed
fictional patient cases across various diseases, with each case containing
5,090 to 6,754 words. The benchmark challenges LLMs with 400 multiple-choice
questions in three categories: information extraction, negation, and sorting,
challenging LLMs to extract and interpret information from large clinical
documents.
</p>
<p>Results: We evaluated nine open-source LLMs with a minimum of 16,000 tokens
and also included OpenAI's proprietary and cost-efficient GPT-3.5 Turbo for
comparison. The highest accuracy was observed for Mixtral-8x7B-Instruct-v0.1,
particularly in tasks focused on information retrieval from single and multiple
patient documents. However, all models struggled significantly in tasks
requiring the identification of missing information, highlighting a critical
area for improvement in clinical data interpretation.
</p>
<p>Conclusion: While LLMs show considerable potential for processing long
clinical documents, their current accuracy levels are insufficient for reliable
clinical use, especially in scenarios requiring the identification of missing
information. The LongHealth benchmark provides a more realistic assessment of
LLMs in a healthcare setting and highlights the need for further model
refinement for safe and effective clinical application.
</p>
<p>We make the benchmark and evaluation code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-QA: A Real-World Medical Q&A Benchmark. (arXiv:2401.14493v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14493">
<div class="article-summary-box-inner">
<span><p>Ensuring the accuracy of responses provided by large language models (LLMs)
is crucial, particularly in clinical settings where incorrect information may
directly impact patient health. To address this challenge, we construct K-QA, a
dataset containing 1,212 patient questions originating from real-world
conversations held on K Health (an AI-driven clinical platform). We employ a
panel of in-house physicians to answer and manually decompose a subset of K-QA
into self-contained statements. Additionally, we formulate two NLI-based
evaluation metrics approximating recall and precision: (1) comprehensiveness,
measuring the percentage of essential clinical information in the generated
answer and (2) hallucination rate, measuring the number of statements from the
physician-curated response contradicted by the LLM answer. Finally, we use K-QA
along with these metrics to evaluate several state-of-the-art models, as well
as the effect of in-context learning and medically-oriented augmented retrieval
schemes developed by the authors. Our findings indicate that in-context
learning improves the comprehensiveness of the models, and augmented retrieval
is effective in reducing hallucinations. We make K-QA available to to the
community to spur research into medically accurate NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do. (arXiv:2401.14523v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14523">
<div class="article-summary-box-inner">
<span><p>Advances in the performance of large language models (LLMs) have led some
researchers to propose the emergence of theory of mind (ToM) in artificial
intelligence (AI). LLMs can attribute beliefs, desires, intentions, and
emotions, and they will improve in their accuracy. Rather than employing the
characteristically human method of empathy, they learn to attribute mental
states by recognizing linguistic patterns in a dataset that typically do not
include that individual. We ask whether LLMs' inability to empathize precludes
them from honoring an individual's right to be an exception, that is, from
making assessments of character and predictions of behavior that reflect
appropriate sensitivity to a person's individuality. Can LLMs seriously
consider an individual's claim that their case is different based on internal
mental states like beliefs, desires, and intentions, or are they limited to
judging that case based on its similarities to others? We propose that the
method of empathy has special significance for honoring the right to be an
exception that is distinct from the value of predictive accuracy, at which LLMs
excel. We conclude by considering whether using empathy to consider exceptional
cases has intrinsic or merely practical value and we introduce conceptual and
empirical avenues for advancing this investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating GPT-3.5's Awareness and Summarization Abilities for European Constitutional Texts with Shared Topics. (arXiv:2401.14524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14524">
<div class="article-summary-box-inner">
<span><p>Constitutions are foundational legal documents that underpin the governmental
and societal structures. As such, they are a reflection of a nation's cultural
and social uniqueness, but also contribute to establish topics of universal
importance, like citizens' rights and duties (RD). In this work, using the
renowned GPT-3.5, we leverage generative large language models to understand
constitutional passages that transcend national boundaries. A key contribution
of our study is the introduction of a novel application of abstractive
summarization on a multi-source collection of constitutional texts, with a
focus on European countries' constitution passages related to RD topics. Our
results show the meaningfulness of GPT-3.5 to produce informative, coherent and
faithful summaries capturing RD topics across European countries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms. (arXiv:2401.14526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14526">
<div class="article-summary-box-inner">
<span><p>This study investigates the computational processing of euphemisms, a
universal linguistic phenomenon, across multiple languages. We train a
multilingual transformer model (XLM-RoBERTa) to disambiguate potentially
euphemistic terms (PETs) in multilingual and cross-lingual settings. In line
with current trends, we demonstrate that zero-shot learning across languages
takes place. We also show cases where multilingual models perform better on the
task compared to monolingual models by a statistically significant margin,
indicating that multilingual data presents additional opportunities for models
to learn about cross-lingual, computational properties of euphemisms. In a
follow-up analysis, we focus on universal euphemistic "categories" such as
death and bodily functions among others. We test to see whether cross-lingual
data of the same domain is more important than within-language data of other
domains to further understand the nature of the cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relative Value Biases in Large Language Models. (arXiv:2401.14530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14530">
<div class="article-summary-box-inner">
<span><p>Studies of reinforcement learning in humans and animals have demonstrated a
preference for options that yielded relatively better outcomes in the past,
even when those options are associated with lower absolute reward. The present
study tested whether large language models would exhibit a similar bias. We had
gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between
pairs of options with the goal of maximizing payoffs. A complete record of
previous outcomes was included in each prompt. Both models exhibited relative
value decision biases similar to those observed in humans and animals. Making
relative comparisons among outcomes more explicit magnified the bias, whereas
prompting the models to estimate expected outcomes caused the bias to
disappear. These results have implications for the potential mechanisms that
contribute to context-dependent choice in human agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling. (arXiv:2401.14556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14556">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models based on masked language modeling (MLM) objective
excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based
encoders consistently outperform causal language modeling decoders of
comparable size, a recent trend of scaling decoder models to multiple billion
parameters resulted in large language models (LLMs), making them competitive
with MLM-based encoders. Although scale amplifies their prowess in NLU tasks,
LLMs fall short of SOTA results in information extraction (IE) tasks, many
framed as sequence labeling (SL). However, whether this is an intrinsic
limitation of LLMs or whether their SL performance can be improved remains
unclear. To address this, we explore strategies to enhance the SL performance
of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional
information flow within groups of decoder blocks, applying layer-wise removal
or enforcement of the causal mask (CM) during LLM fine-tuning. This approach
yields performance gains competitive with SOTA SL models, matching or
outperforming the results of CM removal from all blocks. Our findings hold for
diverse SL tasks, proving that "open" LLMs with layer-dependent CM removal
outperform strong MLM-based encoders and instruction-tuned LLMs. However, we
observe no effect from CM removal on a small scale when maintaining an
equivalent model size, pre-training steps, and pre-training and fine-tuning
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modelling Approaches to Adaptive Machine Translation. (arXiv:2401.14559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14559">
<div class="article-summary-box-inner">
<span><p>Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and adapt to
corrected translations in domain-specific projects. Machine translation (MT)
has achieved significant progress in the area of domain adaptation. However,
in-domain data scarcity is common in translation settings, due to the lack of
specialised datasets and terminology, or inconsistency and inaccuracy of
available in-domain translations. In such scenarios where there is insufficient
in-domain data to fine-tune MT models, producing translations that are
consistent with the relevant context is challenging. While real-time adaptation
can make use of smaller amounts of in-domain data to improve the translation on
the fly, it remains challenging due to supported context limitations and
efficiency constraints. Large language models (LLMs) have recently shown
interesting capabilities of in-context learning, where they learn to replicate
certain input-output text generation patterns, without further fine-tuning.
Such capabilities have opened new horizons for domain-specific data
augmentation and real-time adaptive MT. This work attempts to address two main
relevant questions: 1) in scenarios involving human interaction and continuous
feedback, can we employ language models to improve the quality of adaptive MT
at inference time? and 2) in the absence of sufficient in-domain data, can we
use pre-trained large-scale language models to improve the process of MT domain
adaptation?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Structured Language Alternations in Historical Documents by Combining Language Identification with Fourier Analysis. (arXiv:2401.14569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14569">
<div class="article-summary-box-inner">
<span><p>In this study, we present a generalizable workflow to identify documents in a
historic language with a nonstandard language and script combination,
Armeno-Turkish. We introduce the task of detecting distinct patterns of
multilinguality based on the frequency of structured language alternations
within a document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias. (arXiv:2401.14589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14589">
<div class="article-summary-box-inner">
<span><p>Background: Cognitive biases in clinical decision-making significantly
contribute to errors in diagnosis and suboptimal patient outcomes. Addressing
these biases presents a formidable challenge in the medical field. This study
explores the role of large language models (LLMs) in mitigating these biases
through the utilization of a multi-agent framework. We simulate the clinical
decision-making processes through multi-agent conversation and evaluate its
efficacy in improving diagnostic accuracy. Methods: A total of 16 published and
unpublished case reports where cognitive biases have resulted in misdiagnoses
were identified from the literature. In the multi-agent system, we leveraged
GPT-4 Turbo to facilitate interactions among four simulated agents to replicate
clinical team dynamics. Each agent has a distinct role: 1) To make the initial
and final diagnosis after considering the discussions, 2) The devil's advocate
and correct confirmation and anchoring bias, 3) The tutor and facilitator of
the discussion to reduce premature closure bias, and 4) To record and summarize
the findings. A total of 80 simulations were evaluated for the accuracy of
initial diagnosis, top differential diagnosis and final two differential
diagnoses. Findings: In a total of 80 responses evaluating both initial and
final diagnoses, the initial diagnosis had an accuracy of 0% (0/80), but
following multi-agent discussions, the accuracy for the top differential
diagnosis increased to 71.3% (57/80), and for the final two differential
diagnoses, to 80.0% (64/80). The system demonstrated an ability to reevaluate
and correct misconceptions, even in scenarios with misleading initial
investigations. Interpretation: The LLM-driven multi-agent conversation system
shows promise in enhancing diagnostic accuracy in diagnostically challenging
medical scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alternative Speech: Complementary Method to Counter-Narrative for Better Discourse. (arXiv:2401.14616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14616">
<div class="article-summary-box-inner">
<span><p>We introduce the concept of "Alternative Speech" as a new way to directly
combat hate speech and complement the limitations of counter-narrative. An
alternative speech provides practical alternatives to hate speech in real-world
scenarios by offering speech-level corrections to speakers while considering
the surrounding context and promoting speakers to reform. Further, an
alternative speech can combat hate speech alongside counter-narratives,
offering a useful tool to address social issues such as racial discrimination
and gender inequality. We propose the new concept and provide detailed
guidelines for constructing the necessary dataset. Through discussion, we
demonstrate that combining alternative speech and counter-narrative can be a
more effective strategy for combating hate speech by complementing specificity
and guiding capacity of counter-narrative. This paper presents another
perspective for dealing with hate speech, offering viable remedies to
complement the constraints of current approaches to mitigating harmful bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. (arXiv:2401.14624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14624">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated remarkable potential in various
tasks, however, there remains a significant scarcity of open-source models and
data for specific domains. Previous works have primarily focused on manually
specifying resources and collecting high-quality data on specific domains,
which significantly consume time and effort. To address this limitation, we
propose an efficient data collection method~\textit{Query of CC} based on large
language models. This method bootstraps seed information through a large
language model and retrieves related data from public corpora. It not only
collects knowledge-related data for specific domains but unearths the data with
potential reasoning procedures. Through the application of this method, we have
curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing
four major domains, including stem and humanities sciences, among others.
Experimental results demonstrate that~\textsc{Knowledge Pile} significantly
improves the performance of large language models in mathematical and
knowledge-related reasoning ability tests. To facilitate academic sharing, we
open-source our dataset and code, providing valuable support to the academic
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Practical Automatic Speech Recognition and Post-Processing: a Call for Explainable Error Benchmark Guideline. (arXiv:2401.14625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14625">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) outcomes serve as input for downstream
tasks, substantially impacting the satisfaction level of end-users. Hence, the
diagnosis and enhancement of the vulnerabilities present in the ASR model bear
significant importance. However, traditional evaluation methodologies of ASR
systems generate a singular, composite quantitative metric, which fails to
provide comprehensive insight into specific vulnerabilities. This lack of
detail extends to the post-processing stage, resulting in further obfuscation
of potential weaknesses. Despite an ASR model's ability to recognize utterances
accurately, subpar readability can negatively affect user satisfaction, giving
rise to a trade-off between recognition accuracy and user-friendliness. To
effectively address this, it is imperative to consider both the speech-level,
crucial for recognition accuracy, and the text-level, critical for
user-friendliness. Consequently, we propose the development of an Error
Explainable Benchmark (EEB) dataset. This dataset, while considering both
speech- and text-level, enables a granular understanding of the model's
shortcomings. Our proposition provides a structured pathway for a more
`real-world-centric' evaluation, a marked shift away from abstracted,
traditional methods, allowing for the detection and rectification of nuanced
system weaknesses, ultimately aiming for an improved user experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Investigation of Domain Adaptation Ability for Chinese Spelling Check Models. (arXiv:2401.14630v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14630">
<div class="article-summary-box-inner">
<span><p>Chinese Spelling Check (CSC) is a meaningful task in the area of Natural
Language Processing (NLP) which aims at detecting spelling errors in Chinese
texts and then correcting these errors. However, CSC models are based on
pretrained language models, which are trained on a general corpus.
Consequently, their performance may drop when confronted with downstream tasks
involving domain-specific terms. In this paper, we conduct a thorough
evaluation about the domain adaption ability of various typical CSC models by
building three new datasets encompassing rich domain-specific terms from the
financial, medical, and legal domains. Then we conduct empirical investigations
in the corresponding domain-specific test datasets to ascertain the
cross-domain adaptation ability of several typical CSC models. We also test the
performance of the popular large language model ChatGPT. As shown in our
experiments, the performances of the CSC models drop significantly in the new
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-Rex: Text-assisted Retrosynthesis Prediction. (arXiv:2401.14637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14637">
<div class="article-summary-box-inner">
<span><p>As a fundamental task in computational chemistry, retrosynthesis prediction
aims to identify a set of reactants to synthesize a target molecule. Existing
template-free approaches only consider the graph structures of the target
molecule, which often cannot generalize well to rare reaction types and large
molecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction
approach that exploits pre-trained text language models, such as ChatGPT, to
assist the generation of reactants. T-Rex first exploits ChatGPT to generate a
description for the target molecule and rank candidate reaction centers based
both the description and the molecular graph. It then re-ranks these candidates
by querying the descriptions for each reactants and examines which group of
reactants can best synthesize the target molecule. We observed that T-Rex
substantially outperformed graph-based state-of-the-art approaches on two
datasets, indicating the effectiveness of considering text information. We
further found that T-Rex outperformed the variant that only use ChatGPT-based
description without the re-ranking step, demonstrate how our framework
outperformed a straightforward integration of ChatGPT and graph information.
Collectively, we show that text generated by pre-trained language models can
substantially improve retrosynthesis prediction, opening up new avenues for
exploiting ChatGPT to advance computational chemistry. And the codes can be
found at https://github.com/lauyikfung/T-Rex.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs. (arXiv:2401.14640v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14640">
<div class="article-summary-box-inner">
<span><p>The attribution of question answering is to provide citations for supporting
generated statements, and has attracted wide research attention. The current
methods for automatically evaluating the attribution, which are often based on
Large Language Models (LLMs), are still inadequate, particularly in recognizing
subtle differences between attributions, and complex relationships between
citations and statements. To compare these attribution evaluation methods and
develop new ones, we introduce a set of fine-grained categories (i.e.,
supportive, insufficient, contradictory and irrelevant) for measuring the
attribution, and develop a Complex Attributed Question Answering (CAQA)
benchmark by leveraging knowledge graphs (KGs) for automatically generating
attributions of different categories to question-answer pairs. Our analysis
reveals that existing evaluators perform poorly under fine-grained attribution
settings and exhibit weaknesses in complex citation-statement reasoning. Our
CAQA benchmark, validated with human annotations, emerges as a promising tool
for selecting and developing LLM attribution evaluators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Korean Legal Judgment Prediction Dataset for Insurance Disputes. (arXiv:2401.14654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14654">
<div class="article-summary-box-inner">
<span><p>This paper introduces a Korean legal judgment prediction (LJP) dataset for
insurance disputes. Successful LJP models on insurance disputes can benefit
insurance companies and their customers. It can save both sides' time and money
by allowing them to predict how the result would come out if they proceed to
the dispute mediation process. As is often the case with low-resource
languages, there is a limitation on the amount of data available for this
specific task. To mitigate this issue, we investigate how one can achieve a
good performance despite the limitation in data. In our experiment, we
demonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al.,
2022) is a good alternative to standard fine-tuning when training data are
limited. The models fine-tuned with the SetFit approach on our data show
similar performance to the Korean LJP benchmark models (Hwang et al., 2022)
despite the much smaller data size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientific Large Language Models: A Survey on Biological & Chemical Domains. (arXiv:2401.14656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14656">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as a transformative power in
enhancing natural language comprehension, representing a significant stride
toward artificial general intelligence. The application of LLMs extends beyond
conventional linguistic boundaries, encompassing specialized linguistic systems
developed within various scientific disciplines. This growing interest has led
to the advent of scientific LLMs, a novel subclass specifically engineered for
facilitating scientific discovery. As a burgeoning area in the community of AI
for Science, scientific LLMs warrant comprehensive exploration. However, a
systematic and up-to-date survey introducing them is currently lacking. In this
paper, we endeavor to methodically delineate the concept of "scientific
language", whilst providing a thorough review of the latest advancements in
scientific LLMs. Given the expansive realm of scientific disciplines, our
analysis adopts a focused lens, concentrating on the biological and chemical
domains. This includes an in-depth examination of LLMs for textual knowledge,
small molecules, macromolecular proteins, genomic sequences, and their
combinations, analyzing them in terms of model architectures, capabilities,
datasets, and evaluation. Finally, we critically examine the prevailing
challenges and point out promising research directions along with the advances
of LLMs. By offering a comprehensive overview of technical developments in this
field, this survey aspires to be an invaluable resource for researchers
navigating the intricate landscape of scientific LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit Normalization. (arXiv:2401.14664v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14664">
<div class="article-summary-box-inner">
<span><p>Dysarthric speech reconstruction (DSR) systems aim to automatically convert
dysarthric speech into normal-sounding speech. The technology eases
communication with speakers affected by the neuromotor disorder and enhances
their social inclusion. NED-based (Neural Encoder-Decoder) systems have
significantly improved the intelligibility of the reconstructed speech as
compared with GAN-based (Generative Adversarial Network) approaches, but the
approach is still limited by training inefficiency caused by the cascaded
pipeline and auxiliary tasks of the content encoder, which may in turn affect
the quality of reconstruction. Inspired by self-supervised speech
representation learning and discrete speech units, we propose a Unit-DSR
system, which harnesses the powerful domain-adaptation capacity of HuBERT for
training efficiency improvement and utilizes speech units to constrain the
dysarthric content restoration in a discrete linguistic space. Compared with
NED approaches, the Unit-DSR system only consists of a speech unit normalizer
and a Unit HiFi-GAN vocoder, which is considerably simpler without cascaded
sub-modules or auxiliary tasks. Results on the UASpeech corpus indicate that
Unit-DSR outperforms competitive baselines in terms of content restoration,
reaching a 28.2% relative average word error rate reduction when compared to
original dysarthric speech, and shows robustness against speed perturbation and
noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14680">
<div class="article-summary-box-inner">
<span><p>Addressing the gap in Large Language Model pretrained from scratch with
Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion
parameters on a substantial 349GB dataset, equivalent to 90 billion tokens
based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch.
MaLLaM contributes to enhanced natural language understanding and generation
tasks in the Malay language. Although trained on a smaller dataset of 90
billion tokens, our instruction-tuned MaLLaM models perform competitively. When
compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models
demonstrate notable proficiency, underscoring the effectiveness of our approach
in capturing and understanding the nuances of the Malaysian language. MaLLaM
models mark a significant contribution to the field, providing comprehensive
language representations grounded in Malaysian context. This endeavor aims to
pave the way for enhanced natural language understanding and generation tasks
specific to the linguistic nuances present in Malaysia. We discuss the training
methodology, dataset composition, and the potential impact of MaLLaM in
advancing the capabilities of large language models within the context of the
Malay language.
</p>
<p>All models released at
https://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MasonTigers@LT-EDI-2024: An Ensemble Approach towards Detecting Homophobia and Transphobia in Social Media Comments. (arXiv:2401.14681v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14681">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our approaches and results for Task 2 of the
LT-EDI 2024 Workshop, aimed at detecting homophobia and/or transphobia across
ten languages. Our methodologies include monolingual transformers and ensemble
methods, capitalizing on the strengths of each to enhance the performance of
the models. The ensemble models worked well, placing our team, MasonTigers, in
the top five for eight of the ten languages, as measured by the macro F1 score.
Our work emphasizes the efficacy of ensemble methods in multilingual scenarios,
addressing the complexities of language-specific tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support. (arXiv:2401.14688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14688">
<div class="article-summary-box-inner">
<span><p>Recent advancements in text-to-image models have significantly enhanced image
generation capabilities, yet a notable gap of open-source models persists in
bilingual or Chinese language support. To address this need, we present
Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model
which is developed by extending the capabilities of CLIP and
Stable-Diffusion-XL through a process of bilingual continuous pre-training.
This approach includes the efficient expansion of vocabulary by integrating the
most frequently used Chinese characters into CLIP's tokenizer and embedding
layers, coupled with an absolute position encoding expansion. Additionally, we
enrich text prompts by large vision-language model, leading to better images
captions and possess higher visual quality. These enhancements are subsequently
applied to downstream text-to-image models. Our empirical results indicate that
the developed CLIP model excels in bilingual image-text retrieval.Furthermore,
the bilingual image generation capabilities of Taiyi-Diffusion-XL surpass
previous models. This research leads to the development and open-sourcing of
the Taiyi-Diffusion-XL model, representing a notable advancement in the field
of image generation, particularly for Chinese language applications. This
contribution is a step forward in addressing the need for more diverse language
support in multimodal research. The model and demonstration are made publicly
available at
\href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this
https URL}, fostering further research and collaboration in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Under the Surface: Tracking the Artifactuality of LLM-Generated Data. (arXiv:2401.14698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14698">
<div class="article-summary-box-inner">
<span><p>This work delves into the expanding role of large language models (LLMs) in
generating artificial data. LLMs are increasingly employed to create a variety
of outputs, including annotations, preferences, instruction prompts, simulated
dialogues, and free text. As these forms of LLM-generated data often intersect
in their application, they exert mutual influence on each other and raise
significant concerns about the quality and diversity of the artificial data
incorporated into training cycles, leading to an artificial data ecosystem. To
the best of our knowledge, this is the first study to aggregate various types
of LLM-generated text data, from more tightly constrained data like "task
labels" to more lightly constrained "free-form text". We then stress test the
quality and implications of LLM-generated artificial data, comparing it with
human data across various existing benchmarks. Despite artificial data's
capability to match human performance, this paper reveals significant hidden
disparities, especially in complex tasks where LLMs often miss the nuanced
understanding of intrinsic human-generated content. This study critically
examines diverse LLM-generated data and emphasizes the need for ethical
practices in data creation and when using LLMs. It highlights the LLMs'
shortcomings in replicating human traits and behaviors, underscoring the
importance of addressing biases and artifacts produced in LLM-generated content
for future research and development. All data and code are available on our
project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion. (arXiv:2401.14717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14717">
<div class="article-summary-box-inner">
<span><p>We propose an approach for continuous prediction of turn-taking and
backchanneling locations in spoken dialogue by fusing a neural acoustic model
with a large language model (LLM). Experiments on the Switchboard human-human
conversation dataset demonstrate that our approach consistently outperforms the
baseline models with single modality. We also develop a novel multi-task
instruction fine-tuning strategy to further benefit from LLM-encoded knowledge
for understanding the tasks and conversational contexts, leading to additional
improvements. Our approach demonstrates the potential of combined LLMs and
acoustic models for a more natural and conversational interaction between
humans and speech-enabled AI agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model Adaptation for Financial Sentiment Analysis. (arXiv:2401.14777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14777">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) has recently gained relevance within
financial institutions by providing highly valuable insights into companies and
markets' financial documents. However, the landscape of the financial domain
presents extra challenges for NLP, due to the complexity of the texts and the
use of specific terminology. Generalist language models tend to fall short in
tasks specifically tailored for finance, even when using large language models
(LLMs) with great natural language understanding and generative capabilities.
This paper presents a study on LLM adaptation methods targeted at the financial
domain and with high emphasis on financial sentiment analysis. To this purpose,
two foundation models with less than 1.5B parameters have been adapted using a
wide range of strategies. We show that through careful fine-tuning on both
financial documents and instructions, these foundation models can be adapted to
the target domain. Moreover, we observe that small LLMs have comparable
performance to larger scale models, while being more efficient in terms of
parameters and data. In addition to the models, we show how to generate
artificial instructions through LLMs to augment the number of samples of the
instruction dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChemDFM: Dialogue Foundation Model for Chemistry. (arXiv:2401.14818v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14818">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have established great success in the general
domain of natural language processing. Their emerging task generalization and
free-form dialogue capabilities can greatly help to design Chemical General
Intelligence (CGI) to assist real-world research in chemistry. However, the
existence of specialized language and knowledge in the field of chemistry, such
as the highly informative SMILES notation, hinders the performance of
general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first
LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature,
textbooks, and instructions as well as various data from the general domain.
Therefore, it can store, understand, and reason over chemical knowledge and
languages while still possessing advanced free-form language comprehension
capabilities. Extensive quantitative evaluation shows that ChemDFM can
significantly outperform the representative open-sourced LLMs. Moreover,
ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite
the significant size difference. Further qualitative evaluations demonstrate
the efficiency and effectiveness of ChemDFM in real-world research scenarios.
We will open-source the ChemDFM model soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods. (arXiv:2401.14869v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14869">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) garner significant attention for their
unprecedented performance, leading to an increasing number of researches
evaluating LLMs. However, these evaluation benchmarks are limited to assessing
the instruction-following capabilities, overlooking the fundamental abilities
that emerge during the pre-training stage. Previous subjective evaluation
methods mainly reply on scoring by API models. However, in the absence of
references, large models have shown limited ability to discern subtle
differences. To bridge the gap, we propose F-Eval, a bilingual evaluation
benchmark to evaluate the fundamental abilities, including expression,
commonsense and logic. The tasks in F-Eval include multi-choice objective
tasks, open-ended objective tasks, reference-based subjective tasks and
reference-free subjective tasks. For reference-free subjective tasks, we devise
new evaluation methods, serving as alternatives to scoring by API models. We
conduct evaluations on 13 advanced LLMs. Results show that our evaluation
methods show higher correlation coefficients and larger distinction than other
evaluators. Additionally, we discuss the influence of different model sizes,
dimensions, and normalization methods. We anticipate that F-Eval will
facilitate the study of LLMs' fundamental abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14887">
<div class="article-summary-box-inner">
<span><p>Retrieval-Augmented Generation (RAG) systems represent a significant
advancement over traditional Large Language Models (LLMs). RAG systems enhance
their generation ability by incorporating external data retrieved through an
Information Retrieval (IR) phase, overcoming the limitations of standard LLMs,
which are restricted to their pre-trained knowledge and limited context window.
Most research in this area has predominantly concentrated on the generative
aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and
critically analyzing the influence of IR components on RAG systems. This paper
analyzes which characteristics a retriever should possess for an effective
RAG's prompt formulation, focusing on the type of documents that should be
retrieved. We evaluate various elements, such as the relevance of the documents
to the prompt, their position, and the number included in the context. Our
findings reveal, among other insights, that including irrelevant documents can
unexpectedly enhance performance by more than 30% in accuracy, contradicting
our initial assumption of diminished quality. These findings call for
developing specialized approaches tailored to the specific demands of
integrating retrieval with language generation models and pave the way for
future research. These results underscore the need for developing specialized
strategies to integrate retrieval with language generation models, thereby
laying the groundwork for future research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of parameters of vowel sounds of russian and english languages. (arXiv:2401.14890v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14890">
<div class="article-summary-box-inner">
<span><p>In multilingual speech recognition systems, a situation can often arise when
the language is not known in advance, but the signal has already been received
and is being processed. For such cases, some generalized model is needed that
will be able to respond to phonetic differences and, depending on them,
correctly recog-nize speech in the desired language. To build such a model, it
is necessary to set the values of phonetic parameters, and then compare similar
sounds, establishing significant differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do LLMs Dream of Ontologies?. (arXiv:2401.14931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14931">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently revolutionized automated text
understanding and generation. The performance of these models relies on the
high number of parameters of the underlying neural architectures, which allows
LLMs to memorize part of the vast quantity of data seen during the training.
This paper investigates whether and to what extent general-purpose pre-trained
LLMs have memorized information from known ontologies. Our results show that
LLMs partially know ontologies: they can, and do indeed, memorize concepts from
ontologies mentioned in the text, but the level of memorization of their
concepts seems to vary proportionally to their popularity on the Web, the
primary source of their training material. We additionally propose new metrics
to estimate the degree of memorization of ontological information in LLMs by
measuring the consistency of the output produced across different prompt
repetitions, query languages, and degrees of determinism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding-based search in JetBrains IDEs. (arXiv:2401.14975v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14975">
<div class="article-summary-box-inner">
<span><p>Most modern Integrated Development Environments (IDEs) and code editors have
a feature to search across available functionality and items in an open
project. In JetBrains IDEs, this feature is called Search Everywhere: it allows
users to search for files, actions, classes, symbols, settings, and anything
from VCS history from a single entry point. However, it works with the
candidates obtained by algorithms that don't account for semantics, e.g.,
synonyms, complex word permutations, part of the speech modifications, and
typos. In this work, we describe the machine learning approach we implemented
to improve the discoverability of search items. We also share the obstacles
encountered during this process and how we overcame them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15006">
<div class="article-summary-box-inner">
<span><p>We announce the initial release of "Airavata," an instruction-tuned LLM for
Hindi. Airavata was created by fine-tuning OpenHathi with diverse,
instruction-tuning Hindi datasets to make it better suited for assistive tasks.
Along with the model, we also share the IndicInstruct dataset, which is a
collection of diverse instruction-tuning datasets to enable further research
for Indic LLMs. Additionally, we present evaluation benchmarks and a framework
for assessing LLM performance across tasks in Hindi. Currently, Airavata
supports Hindi, but we plan to expand this to all 22 scheduled Indic languages.
You can access all artifacts at https://ai4bharat.github.io/airavata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15024">
<div class="article-summary-box-inner">
<span><p>Large language models have become the cornerstone of natural language
processing, but their use comes with substantial costs in terms of compute and
memory resources. Sparsification provides a solution to alleviate these
resource constraints, and recent works have shown that trained models can be
sparsified post-hoc. Existing sparsification techniques face challenges as they
need additional data structures and offer constrained speedup with current
hardware. In this paper we present SliceGPT, a new post-training sparsification
scheme which replaces each weight matrix with a smaller (dense) matrix,
reducing the embedding dimension of the network. Through extensive
experimentation, we show that SliceGPT can remove up to 25% of the model
parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models
while maintaining 99%, 99% and 90% zero-shot task performance of the dense
model respectively. Our sliced models run on fewer GPUs and run faster without
any additional code optimization: on 24GB consumer GPUs we reduce the total
compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB
A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance
in transformer networks, which enables SliceGPT and we hope it will inspire and
enable future avenues to reduce memory and computation demands for pre-trained
models. Code is available at:
https://github.com/microsoft/TransformerCompression
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15042">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have exhibited remarkable success in long-form
context comprehension tasks. However, their capacity to generate long contents,
such as reports and articles, remains insufficiently explored. Current
benchmarks do not adequately assess LLMs' ability to produce informative and
comprehensive content, necessitating a more rigorous evaluation approach. In
this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form
text generation, comprising in-depth human-curated \textit{meta-questions}
spanning various domains. Each meta-question contains corresponding
\textit{proxy-questions} with annotated answers. LLMs are prompted to generate
extensive content in response to these meta-questions. Utilizing an evaluator
and incorporating generated content as background context, \textsc{ProxyQA}
evaluates the quality of generated content based on the evaluator's performance
in answering the \textit{proxy-questions}. We examine multiple LLMs,
emphasizing \textsc{ProxyQA}'s demanding nature as a high-quality assessment
tool. Human evaluation demonstrates that evaluating through
\textit{proxy-questions} is a highly self-consistent and
human-criteria-correlated validation method. The dataset and leaderboard will
be available at \url{https://github.com/Namco0816/ProxyQA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15043">
<div class="article-summary-box-inner">
<span><p>Objective: The reading level of health educational materials significantly
influences information understandability and accessibility, particularly for
minoritized populations. Many patient educational resources surpass the reading
level and complexity of widely accepted standards. There is a critical need for
high-performing text simplification models in health information to enhance
dissemination and literacy. This need is particularly acute in cancer
education, where effective prevention and screening education can substantially
reduce morbidity and mortality.
</p>
<p>Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel
corpus of cancer education materials tailored for health text simplification
research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore
Large Language Model (LLM)-based simplification methods, including fine-tuning,
reinforcement learning (RL), reinforcement learning with human feedback (RLHF),
domain adaptation, and prompt-based approaches. Our experimentation encompasses
Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a
lightweight model adept at distinguishing between original and simplified
texts, thereby enhancing the model's effectiveness with unlabeled data.
</p>
<p>Results: Fine-tuned Llama 2 models demonstrated high performance across
various metrics. Our innovative RLHF reward function surpassed existing RL text
simplification reward functions in effectiveness. The results underscore that
RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text
and improving performance. Additionally, these methods effectively adapt
out-of-domain text simplification models to targeted domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents. (arXiv:2401.15050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15050">
<div class="article-summary-box-inner">
<span><p>Document AI is a growing research field that focuses on the comprehension and
extraction of information from scanned and digital documents to make everyday
business operations more efficient. Numerous downstream tasks and datasets have
been introduced to facilitate the training of AI models capable of parsing and
extracting information from various document types such as receipts and scanned
forms. Despite these advancements, both existing datasets and models fail to
address critical challenges that arise in industrial contexts. Existing
datasets primarily comprise short documents consisting of a single page, while
existing models are constrained by a limited maximum length, often set at 512
tokens. Consequently, the practical application of these methods in financial
services, where documents can span multiple pages, is severely impeded. To
overcome these challenges, we introduce LongFin, a multimodal document AI model
capable of encoding up to 4K tokens. We also propose the LongForms dataset, a
comprehensive financial dataset that encapsulates several industrial challenges
in financial documents. Through an extensive evaluation, we demonstrate the
effectiveness of the LongFin model on the LongForms dataset, surpassing the
performance of existing public models while maintaining comparable results on
existing single-page benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based approach for tomato classification in complex scenes. (arXiv:2401.15055v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15055">
<div class="article-summary-box-inner">
<span><p>Tracking ripening tomatoes is time consuming and labor intensive. Artificial
intelligence technologies combined with those of computer vision can help users
optimize the process of monitoring the ripening status of plants. To this end,
we have proposed a tomato ripening monitoring approach based on deep learning
in complex scenes. The objective is to detect mature tomatoes and harvest them
in a timely manner. The proposed approach is declined in two parts. Firstly,
the images of the scene are transmitted to the pre-processing layer. This
process allows the detection of areas of interest (area of the image containing
tomatoes). Then, these images are used as input to the maturity detection
layer. This layer, based on a deep neural network learning algorithm,
classifies the tomato thumbnails provided to it in one of the following five
categories: green, brittle, pink, pale red, mature red. The experiments are
based on images collected from the internet gathered through searches using
tomato state across diverse languages including English, German, French, and
Spanish. The experimental results of the maturity detection layer on a dataset
composed of images of tomatoes taken under the extreme conditions, gave a good
classification rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairing Orthographically Variant Literary Words to Standard Equivalents Using Neural Edit Distance Models. (arXiv:2401.15068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15068">
<div class="article-summary-box-inner">
<span><p>We present a novel corpus consisting of orthographically variant words found
in works of 19th century U.S. literature annotated with their corresponding
"standard" word pair. We train a set of neural edit distance models to pair
these variants with their standard forms, and compare the performance of these
models to the performance of a set of neural edit distance models trained on a
corpus of orthographic errors made by L2 English learners. Finally, we analyze
the relative performance of these models in the light of different negative
training sample generation strategies, and offer concluding remarks on the
unique challenge literary orthographic variation poses to string pairing
methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.15077">
<div class="article-summary-box-inner">
<span><p>Auto-regressive decoding makes the inference of Large Language Models (LLMs)
time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm
for Greater Language-model Efficiency), for lossless acceleration. Unlike
traditional speculative sampling methods, EAGLE operates the drafting process
auto-regressively at the more regular (second-top-layer) feature level and
addresses the sampling uncertainty issues in the next-feature prediction
problems by integrating tokens from one time step ahead. The acceleration
provided by EAGLE is lossless: it involves no fine-tuning of the target LLM,
and the generated text maintains the same distribution as that of vanilla
auto-regressive decoding. As of the submission of this paper, EAGLE is the
fastest known framework within the speculative sampling family. On MT-bench,
EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x
faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with
LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of
Huggingface's implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A* shortest string decoding for non-idempotent semirings. (arXiv:2204.07236v2 [cs.FL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07236">
<div class="article-summary-box-inner">
<span><p>The single shortest path algorithm is undefined for weighted finite-state
automata over non-idempotent semirings because such semirings do not guarantee
the existence of a shortest path. However, in non-idempotent semirings
admitting an order satisfying a monotonicity condition (such as the plus-times
or log semirings), the notion of shortest string is well-defined. We describe
an algorithm which finds the shortest string for a weighted non-deterministic
automaton over such semirings using the backwards shortest distance of an
equivalent deterministic automaton (DFA) as a heuristic for A* search performed
over a companion idempotent semiring, which is proven to return the shortest
string. While there may be exponentially more states in the DFA, this algorithm
needs to visit only a small fraction of them if determinization is performed
"on the fly".
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation. (arXiv:2302.14220v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14220">
<div class="article-summary-box-inner">
<span><p>Pretrained character-level and byte-level language models have been shown to
be competitive with popular subword models across a range of Natural Language
Processing (NLP) tasks. However, there has been little research on their
effectiveness for neural machine translation (NMT), particularly within the
popular pretrain-then-finetune paradigm. This work performs an extensive
comparison across multiple languages and experimental conditions of character-
and subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We
show the effectiveness of character-level modeling in translation, particularly
in cases where fine-tuning data is limited. In our analysis, we show how
character models' gains in translation quality are reflected in better
translations of orthographically similar words and rare words. While evaluating
the importance of source texts in driving model predictions, we highlight
word-level patterns within ByT5, suggesting an ability to modulate word-level
and character-level information during generation. We conclude by assessing the
efficiency tradeoff of byte models, suggesting their usage in non-time-critical
scenarios to boost translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14827">
<div class="article-summary-box-inner">
<span><p>This paper aims to quantitatively evaluate the performance of ChatGPT, an
interactive large language model, on inter-sentential relations such as
temporal relations, causal relations, and discourse relations. Given ChatGPT's
promising performance across various tasks, we proceed to carry out thorough
evaluations on the whole test sets of 11 datasets, including temporal and
causal relations, PDTB2.0-based, and dialogue-based discourse relations. To
ensure the reliability of our findings, we employ three tailored prompt
templates for each task, including the zero-shot prompt template, zero-shot
prompt engineering (PE) template, and in-context learning (ICL) prompt
template, to establish the initial baseline scores for all popular
sentence-pair relation classification tasks for the first time. Through our
study, we discover that ChatGPT exhibits exceptional proficiency in detecting
and reasoning about causal relations, albeit it may not possess the same level
of expertise in identifying the temporal order between two events. While it is
capable of identifying the majority of discourse relations with existing
explicit discourse connectives, the implicit discourse relation remains a
formidable challenge. Concurrently, ChatGPT demonstrates subpar performance in
the dialogue discourse parsing task that requires structural understanding in a
dialogue before being aware of the discourse relation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple output samples per input in a single-output Gaussian process. (arXiv:2306.02719v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02719">
<div class="article-summary-box-inner">
<span><p>The standard Gaussian Process (GP) only considers a single output sample per
input in the training set. Datasets for subjective tasks, such as spoken
language assessment, may be annotated with output labels from multiple human
raters per input. This paper proposes to generalise the GP to allow for these
multiple output samples in the training set, and thus make use of available
output uncertainty information. This differs from a multi-output GP, as all
output samples are from the same task here. The output density function is
formulated to be the joint likelihood of observing all output samples, and
latent variables are not repeated to reduce computation cost. The test set
predictions are inferred similarly to a standard GP, with a difference being in
the optimised hyper-parameters. This is evaluated on speechocean762, showing
that it allows the GP to compute a test set output distribution that is more
similar to the collection of reference outputs from the multiple human raters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07164">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated their ability to learn
in-context, allowing them to perform various tasks based on a few input-output
examples. However, the effectiveness of in-context learning is heavily reliant
on the quality of the selected examples. In this paper, we propose a novel
framework to iteratively train dense retrievers that can identify high-quality
in-context examples for LLMs. Our framework initially trains a reward model
based on LLM feedback to evaluate the quality of candidate examples, followed
by knowledge distillation to train a bi-encoder based dense retriever. Our
experiments on a suite of $30$ tasks demonstrate that our framework
significantly enhances in-context learning performance. Furthermore, we show
the generalization ability of our framework to unseen tasks during training. An
in-depth analysis reveals that our model improves performance by retrieving
examples with similar patterns, and the gains are consistent across LLMs of
varying sizes. The code and data are available at
https://github.com/microsoft/LMOps/tree/main/llm_retriever .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Dataset Annotation Quality Management in the Wild. (arXiv:2307.08153v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08153">
<div class="article-summary-box-inner">
<span><p>Data quality is crucial for training accurate, unbiased, and trustworthy
machine learning models as well as for their correct evaluation. Recent works,
however, have shown that even popular datasets used to train and evaluate
state-of-the-art models contain a non-negligible amount of erroneous
annotations, biases, or artifacts. While practices and guidelines regarding
dataset creation projects exist, to our knowledge, large-scale analysis has yet
to be performed on how quality management is conducted when creating natural
language datasets and whether these recommendations are followed. Therefore, we
first survey and summarize recommended quality management practices for dataset
creation as described in the literature and provide suggestions for applying
them. Then, we compile a corpus of 591 scientific publications introducing text
datasets and annotate it for quality-related aspects, such as annotator
management, agreement, adjudication, or data validation. Using these
annotations, we then analyze how quality management is conducted in practice. A
majority of the annotated publications apply good or excellent quality
management. However, we deem the effort of 30\% of the works as only subpar.
Our analysis also shows common errors, especially when using inter-annotator
agreement and computing annotation error rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems. (arXiv:2307.11394v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11394">
<div class="article-summary-box-inner">
<span><p>MeetEval is an open-source toolkit to evaluate all kinds of meeting
transcription systems. It provides a unified interface for the computation of
commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER
along other WER definitions. We extend the cpWER computation by a temporal
constraint to ensure that only words are identified as correct when the
temporal alignment is plausible. This leads to a better quality of the matching
of the hypothesis string to the reference string that more closely resembles
the actual transcription quality, and a system is penalized if it provides poor
time annotations. Since word-level timing information is often not available,
we present a way to approximate exact word-level timings from segment-level
timings (e.g., a sentence) and show that the approximation leads to a similar
WER as a matching with exact word-level annotations. At the same time, the time
constraint leads to a speedup of the matching algorithm, which outweighs the
additional overhead caused by processing the time stamps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Zero-Shot Instruction Following. (arXiv:2308.03795v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03795">
<div class="article-summary-box-inner">
<span><p>This work proposes a challenging yet more realistic setting for zero-shot
cross-task generalization: zero-shot instruction following, presuming the
existence of a paragraph-style task definition while no demonstrations exist.
To better learn the task supervision from the definition, we propose two
strategies: first, to automatically find out the critical sentences in the
definition; second, a ranking objective to force the model to generate the gold
outputs with higher probabilities when those critical parts are highlighted in
the definition. The joint efforts of the two strategies yield state-of-the-art
performance on the Super-NaturalInstructions. Our code is available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies. (arXiv:2308.07610v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07610">
<div class="article-summary-box-inner">
<span><p>Automated log analysis is crucial in modern software-intensive systems for
facilitating program comprehension throughout software maintenance and
engineering life cycles. Existing methods perform tasks such as log parsing and
log anomaly detection by providing a single prediction value without
interpretation. However, given the increasing volume of system events, the
limited interpretability of analysis results hinders analysts' comprehension of
program status and their ability to take appropriate actions. Moreover, these
methods require substantial in-domain training data, and their performance
declines sharply (by up to 62.5%) in online scenarios involving unseen logs
from new domains, a common occurrence due to rapid software updates. In this
paper, we propose LogPrompt, a novel interpretable log analysis approach for
online scenarios. LogPrompt employs large language models (LLMs) to perform
online log analysis tasks via a suite of advanced prompt strategies tailored
for log tasks, which enhances LLMs' performance by up to 380.7% compared with
simple prompts. Experiments on nine publicly available evaluation datasets
across two tasks demonstrate that LogPrompt, despite requiring no in-domain
training, outperforms existing approaches trained on thousands of logs by up to
55.9%. We also conduct a human evaluation of LogPrompt's interpretability, with
six practitioners possessing over 10 years of experience, who highly rated the
generated content in terms of usefulness and readability (averagely 4.42/5).
LogPrompt also exhibits remarkable compatibility with open-source and
smaller-scale LLMs, making it flexible for practical deployment. Code of
LogPrompt is available at https://github.com/lunyiliu/LogPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12060">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) is a critical yet challenging task
due to the vast number of entities within knowledge bases and the diversity of
natural language questions posed by users. Unfortunately, the performance of
most KBQA models tends to decline significantly in real-world scenarios where
high-quality annotated data is insufficient. To mitigate the burden associated
with manual annotation, we introduce FlexKBQA by utilizing Large Language
Models (LLMs) as program translators for addressing the challenges inherent in
the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms
to sample diverse programs, such as SPARQL queries, from the knowledge base,
which are subsequently converted into natural language questions via LLMs. This
synthetic dataset facilitates training a specialized lightweight model for the
KB. Additionally, to reduce the barriers of distribution shift between
synthetic data and real user questions, FlexKBQA introduces an executionguided
self-training method to iterative leverage unlabeled user questions.
Furthermore, we explore harnessing the inherent reasoning capability of LLMs to
enhance the entire framework. Consequently, FlexKBQA delivers substantial
flexibility, encompassing data annotation, deployment, and being domain
agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we
observe that under the few-shot even the more challenging zero-shot scenarios,
FlexKBQA achieves impressive results with a few annotations, surpassing all
previous baselines and even approaching the performance of supervised models,
achieving a remarkable 93% performance relative to the fully-supervised models.
We posit that FlexKBQA represents a significant advancement towards exploring
better integration of large and lightweight models. The code is open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02731">
<div class="article-summary-box-inner">
<span><p>ChatGPT has gained significant interest due to its impressive performance,
but people are increasingly concerned about its potential risks, particularly
around the detection of AI-generated content (AIGC), which is often difficult
for untrained humans to identify. Current datasets utilized for detecting
ChatGPT-generated text primarily center around question-answering, yet they
tend to disregard tasks that possess semantic-invariant properties, such as
summarization, translation, and paraphrasing. Our primary studies demonstrate
that detecting model-generated text on semantic-invariant tasks is more
difficult. To fill this gap, we introduce a more extensive and comprehensive
dataset that considers more types of tasks than previous work, including
semantic-invariant tasks. In addition, the model after a large number of task
instruction fine-tuning shows a strong powerful performance. Owing to its
previous success, we further instruct fine-tuning T\textit{k}-instruct and
build a more powerful detection system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OYXOY: A Modern NLP Test Suite for Modern Greek. (arXiv:2309.07009v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07009">
<div class="article-summary-box-inner">
<span><p>This paper serves as a foundational step towards the development of a
linguistically motivated and technically relevant evaluation suite for Greek
NLP. We initiate this endeavor by introducing four expert-verified evaluation
tasks, specifically targeted at natural language inference, word sense
disambiguation (through example comparison or sense selection) and metaphor
detection. More than language-adapted replicas of existing tasks, we contribute
two innovations which will resonate with the broader resource and evaluation
community. Firstly, our inference dataset is the first of its kind, marking not
just \textit{one}, but rather \textit{all} possible inference labels,
accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we
demonstrate a cost-efficient method to obtain datasets for under-resourced
languages. Using ChatGPT as a language-neutral parser, we transform the
Dictionary of Standard Modern Greek into a structured format, from which we
derive the other three tasks through simple projections. Alongside each task,
we conduct experiments using currently available state of the art machinery.
Our experimental baselines affirm the challenging nature of our tasks and
highlight the need for expedited progress in order for the Greek NLP ecosystem
to keep pace with contemporary mainstream research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11082">
<div class="article-summary-box-inner">
<span><p>In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12244">
<div class="article-summary-box-inner">
<span><p>Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the findings, we discuss opportunities for
leveraging LLMs to design child-friendly chatbots to support children in
sharing emotions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02998">
<div class="article-summary-box-inner">
<span><p>Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
advancements on various multimodal downstream tasks. However, deploying LVLMs
is often problematic due to their massive computational/energy costs and carbon
consumption. Such issues make it infeasible to adopt conventional iterative
global pruning, which is costly due to computing the Hessian matrix of the
entire large model for sparsification. Alternatively, several studies have
recently proposed layer-wise pruning approaches to avoid the expensive
computation of global pruning and efficiently compress model weights according
to their importance within a layer. However, they often suffer from suboptimal
model compression due to their lack of a global perspective. To address this
limitation in recent efficient pruning methods for large models, we propose
Efficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage
coarse-to-fine weight pruning approach for LVLMs. We first determine the
sparsity ratios of different layers or blocks by leveraging the global
importance score, which is efficiently computed based on the zeroth-order
approximation of the global model gradients. Then, the model performs local
layer-wise unstructured weight pruning based on globally-informed sparsity
ratios. We validate our proposed method across various multimodal and unimodal
models and datasets, demonstrating significant performance improvements over
prevalent pruning techniques in the high-sparsity regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12537">
<div class="article-summary-box-inner">
<span><p>E-commerce platforms rely on structured product descriptions, in the form of
attribute/value pairs to enable features such as faceted product search and
product comparison. However, vendors on these platforms often provide
unstructured product descriptions consisting of a title and a textual
description. To process such offers, e-commerce platforms must extract
attribute/value pairs from the unstructured descriptions. State-of-the-art
attribute/value extraction methods based on pre-trained language models (PLMs),
such as BERT, face two drawbacks (i) the methods require significant amounts of
task-specific training data and (ii) the fine-tuned models have problems to
generalize to attribute values that were not part of the training data. We
explore the potential of using large language models (LLMs) as a more training
data-efficient and more robust alternative to existing attribute/value
extraction methods. We propose different prompt templates for instructing LLMs
about the target schema of the extraction, covering both zero-shot and few-shot
scenarios. In the zero-shot scenario, textual and JSON-based approaches for
representing information about the target attributes are compared. In the
scenario with training data, we investigate (i) the provision of example
attribute values, (ii) the selection of in-context demonstrations, (iii)
shuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. The
prompt templates are evaluated in combination with hosted LLMs, such as GPT-3.5
and GPT-4, and open-source LLMs based on Llama2 which can be run locally. The
best average F1-score of 86% was reached by GPT-4 using an ensemble of shuffled
prompts that combine attribute names, attribute descriptions, example values,
and demonstrations. Given the same amount of training data, this prompt/model
combination outperforms the best PLM baseline by an average of 6% F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation. (arXiv:2310.18628v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.18628">
<div class="article-summary-box-inner">
<span><p>With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are
increasing interests in distilling the capabilies of close-sourced LLMs to
smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT
to generate a set of instructions and answers, for the student model to learn.
However, such standard distillation approach neglects the merits and conditions
of the student model. Inspired by modern teaching principles, we design a
personalised distillation process, in which the student attempts to solve a
task first, then the teacher provides an adaptive refinement for the student to
improve. Instead of feeding the student with teacher's prior, personalised
distillation enables personalised learning for the student model, as it only
learns on examples it makes mistakes upon and learns to improve its own
solution. On code generation, personalised distillation consistently
outperforms standard distillation with only one third of the data. With only
2.5-3K personalised examples that incur a data-collection cost of 4-6$, we
boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to
achieve 45.8% pass@1 on HumanEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications. (arXiv:2312.01339v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.01339">
<div class="article-summary-box-inner">
<span><p>This paper presents the first Arabic crossword puzzle generator driven by
advanced AI technology. Leveraging cutting-edge large language models including
GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system
generates distinctive and challenging clues. Based on a dataset comprising over
50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot
learning strategies, and rigorous quality-checking protocols to enforce the
generation of high-quality clue-answer pairs. Importantly, educational
crosswords contribute to enhancing memory, expanding vocabulary, and promoting
problem-solving skills, thereby augmenting the learning experience through a
fun and engaging approach, reshaping the landscape of traditional learning
methods. The overall system can be exploited as a powerful educational tool
that amalgamates AI and innovative learning techniques, heralding a
transformative era for Arabic crossword puzzles and the intersection of
technology and education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.06401">
<div class="article-summary-box-inner">
<span><p>How to evaluate Large Language Models (LLMs) in code generation is an open
question. Many benchmarks have been proposed but are inconsistent with
practical software projects, e.g., unreal program distributions, insufficient
dependencies, and small-scale project contexts. Thus, the capabilities of LLMs
in practical projects are still unclear. In this paper, we propose a new
benchmark named DevEval, aligned with Developers' experiences in practical
projects. DevEval is collected through a rigorous pipeline, containing 2,690
samples from 119 practical projects and covering 10 domains. Compared to
previous benchmarks, DevEval aligns to practical projects in multiple
dimensions, e.g., real program distributions, sufficient dependencies, and
enough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,
gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual
abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo
only is 42 in our experiments. We also discuss the challenges and future
directions of code generation in practical projects. We open-source DevEval and
hope it can facilitate the development of code generation in practical
projects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Explain Transformers by Illuminating Important Information. (arXiv:2401.09972v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.09972">
<div class="article-summary-box-inner">
<span><p>Transformer-based models excel in various natural language processing (NLP)
tasks, attracting countless efforts to explain their inner workings. Prior
methods explain Transformers by focusing on the raw gradient and attention as
token attribution scores, where non-relevant information is often considered
during explanation computation, resulting in confusing results. In this work,
we propose highlighting the important information and eliminating irrelevant
information by a refined information flow on top of the layer-wise relevance
propagation (LRP) method. Specifically, we consider identifying syntactic and
positional heads as important attention heads and focus on the relevance
obtained from these important heads. Experimental results demonstrate that
irrelevant information does distort output attribution scores and then should
be masked during explanation computation. Compared to eight baselines on both
classification and question-answering datasets, our method consistently
outperforms with over 3\% to 33\% improvement on explanation metrics, providing
superior explanation performance. Our anonymous code repository is available
at: https://github.com/LinxinS97/Mask-LRP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.10189">
<div class="article-summary-box-inner">
<span><p>Fine-grained few-shot entity extraction in the chemical domain faces two
unique challenges. First, compared with entity extraction tasks in the general
domain, sentences from chemical papers usually contain more entities. Moreover,
entity extraction models usually have difficulty extracting entities of
long-tailed types. In this paper, we propose Chem-FINESE, a novel
sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to
address these two challenges. Our Chem-FINESE has two components: a seq2seq
entity extractor to extract named entities from the input sentence and a
seq2seq self-validation module to reconstruct the original input sentence from
extracted entities. Inspired by the fact that a good entity extraction system
needs to extract entities faithfully, our new self-validation module leverages
entity extraction results to reconstruct the original input sentence. Besides,
we design a new contrastive loss to reduce excessive copying during the
extraction process. Finally, we release ChemNER+, a new fine-grained chemical
entity extraction dataset that is annotated by domain experts with the ChemNER
schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets
show that our newly proposed framework has contributed up to 8.26% and 6.84%
absolute F1-score gains respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.11143">
<div class="article-summary-box-inner">
<span><p>We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM's
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unfair TOS: An Automated Approach using Customized BERT. (arXiv:2401.11207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.11207">
<div class="article-summary-box-inner">
<span><p>Terms of Service (ToS) form an integral part of any agreement as it defines
the legal relationship between a service provider and an end-user. Not only do
they establish and delineate reciprocal rights and responsibilities, but they
also provide users with information on essential aspects of contracts that
pertain to the use of digital spaces. These aspects include a wide range of
topics, including limitation of liability, data protection, etc. Users tend to
accept the ToS without going through it before using any application or
service. Such ignorance puts them in a potentially weaker situation in case any
action is required. Existing methodologies for the detection or classification
of unfair clauses are however obsolete and show modest performance. In this
research paper, we present SOTA(State of The Art) results on unfair clause
detection from ToS documents based on unprecedented custom BERT Fine-tuning in
conjunction with SVC(Support Vector Classifier). The study shows proficient
performance with a macro F1-score of 0.922 at unfair clause detection, and
superior performance is also shown in the classification of unfair clauses by
each tag. Further, a comparative analysis is performed by answering research
questions on the Transformer models utilized. In order to further research and
experimentation the code and results are made available on
https://github.com/batking24/Unfair-TOS-An-Automated-Approach-based-on-Fine-tuning-BERT-in-conjunction-with-ML.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14011">
<div class="article-summary-box-inner">
<span><p>Multi-modal large language models(MLLMs) have achieved remarkable progress
and demonstrated powerful knowledge comprehension and reasoning abilities.
However, the mastery of domain-specific knowledge, which is essential for
evaluating the intelligence of MLLMs, continues to be a challenge. Current
multi-modal benchmarks for domain-specific knowledge concentrate on
multiple-choice questions and are predominantly available in English, which
imposes limitations on the comprehensiveness of the evaluation. To this end, we
introduce CMMU, a novel benchmark for multi-modal and multi-type question
understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7
subjects, covering knowledge from primary to high school. The questions can be
categorized into 3 types: multiple-choice, multiple-response, and
fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we
propose a rigorous evaluation strategy called ShiftCheck for assessing
multiple-choice questions. The strategy aims to reduce position bias, minimize
the influence of randomness on correctness, and perform a quantitative analysis
of position bias. We evaluate seven open-source MLLMs along with GPT4-V,
Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a
significant challenge to the recent MLLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.14196">
<div class="article-summary-box-inner">
<span><p>The rapid development of large language models has revolutionized code
intelligence in software development. However, the predominance of
closed-source models has restricted extensive research and development. To
address this, we introduce the DeepSeek-Coder series, a range of open-source
code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion
tokens. These models are pre-trained on a high-quality project-level code
corpus and employ a fill-in-the-blank task with a 16K window to enhance code
generation and infilling. Our extensive evaluations demonstrate that
DeepSeek-Coder not only achieves state-of-the-art performance among open-source
code models across multiple benchmarks but also surpasses existing
closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models
are under a permissive license that allows for both research and unrestricted
commercial use.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2024-01-29 23:12:10.236049374 UTC">2024-01-29 23:12:10 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>