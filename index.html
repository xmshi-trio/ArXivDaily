<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-31T01:30:00Z">05-31</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-aware Digital Twin for Metaverse: A Comprehensive Review. (arXiv:2305.18304v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18304">
<div class="article-summary-box-inner">
<span><p>To facilitate the deployment of digital twins in Metaverse, the paradigm with
semantic awareness has been proposed as a means for enabling accurate and
task-oriented information extraction with inherent intelligence. However, this
framework requires all devices in the Metaverse environment to be directly
linked with the semantic model to enable faithful interpretation of messages.
In contrast, this article introduces the digital twin framework, considering a
smart industrial application, which enables semantic communication in
conjugation with the Metaverse enabling technologies. The fundamentals of this
framework are demonstrated on an industrial shopfloor management use case with
a digital twin so as to improve its performance through semantic communication.
An overview of semantic communication, Metaverse, and digital twins is
presented. Integration of these technologies with the basic architecture as
well as the impact on future industrial applications is presented. In a
nutshell, this article showcases how semantic awareness can be an effective
candidate in the implementation of digital twins for Metaverse applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18315">
<div class="article-summary-box-inner">
<span><p>A basic task for most Legal Artificial Intelligence (Legal AI) applications
is Named Entity Recognition (NER). However, texts produced in the context of
legal practice make references to entities that are not trivially recognized by
the currently available NERs. There is a lack of categorization of legislation,
jurisprudence, evidence, penalties, the roles of people in a legal process
(judge, lawyer, victim, defendant, witness), types of locations (crime
location, defendant's address), etc. In this sense, there is still a need for a
robust golden collection, annotated with fine-grained entities of the legal
domain, and which covers various documents of a legal process, such as
petitions, inquiries, complaints, decisions and sentences. In this article, we
describe the development of the Golden Collection of the Brazilian Judiciary
(CDJUR-BR) contemplating a set of fine-grained named entities that have been
annotated by experts in legal documents. The creation of CDJUR-BR followed its
own methodology that aimed to attribute a character of comprehensiveness and
robustness. Together with the CDJUR-BR repository we provided a NER based on
the BERT model and trained with the CDJUR-BR, whose results indicated the
prevalence of the CDJUR-BR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18319">
<div class="article-summary-box-inner">
<span><p>Timely feedback is an important part of teaching and learning. Here we
describe how a readily available neural network transformer (machine-learning)
model (BERT) can be used to give feedback on the structure of the response to
an abstracting exercise where students are asked to summarise the contents of a
published article after finding it from a publication database. The dataset
contained 207 submissions from two consecutive years of the course, summarising
a total of 21 different papers from the primary literature. The model was
pre-trained using an available dataset (approx. 15,000 samples) and then
fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be
important. The sentences in the student submissions are characterised into
three classes - background, technique and observation - which allows a
comparison of how each submission is structured. Comparing the structure of the
students' abstract a large collection of those from the PubMed database shows
that students in this exercise concentrate more on the background to the paper
and less on the techniques and results than the abstracts to papers themselves.
The results allowed feedback for each submitted assignment to be automatically
generated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students. (arXiv:2305.18320v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18320">
<div class="article-summary-box-inner">
<span><p>Large language models are becoming increasingly integrated into our lives.
Hence, it is important to understand the biases present in their outputs in
order to avoid perpetuating harmful stereotypes, which originate in our own
flawed ways of thinking. This challenge requires developing new benchmarks and
methods for quantifying affective and semantic bias, keeping in mind that LLMs
act as psycho-social mirrors that reflect the views and tendencies that are
prevalent in society. One such tendency that has harmful negative effects is
the global phenomenon of anxiety toward math and STEM subjects. Here, we
investigate perceptions of math and STEM fields provided by cutting-edge
language models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach
from network science and cognitive psychology. Specifically, we use behavioral
forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM
disciplines in relation to other concepts. We use data obtained by probing the
three LLMs in a language generation task that has previously been applied to
humans. Our findings indicate that LLMs have an overall negative perception of
math and STEM fields, with math being perceived most negatively. We observe
significant differences across the three LLMs. We observe that newer versions
(i.e. GPT-4) produce richer, more complex perceptions as well as less negative
perceptions compared to older versions and N=159 high-school students. These
findings suggest that advances in the architecture of LLMs may lead to
increasingly less biased models that could even perhaps someday aid in reducing
harmful stereotypes in society rather than perpetuating them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFinD: Relation Extraction Financial Dataset. (arXiv:2305.18322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18322">
<div class="article-summary-box-inner">
<span><p>A number of datasets for Relation Extraction (RE) have been created to aide
downstream tasks such as information retrieval, semantic search, question
answering and textual entailment. However, these datasets fail to capture
financial-domain specific challenges since most of these datasets are compiled
using general knowledge sources such as Wikipedia, web-based text and news
articles, hindering real-life progress and adoption within the financial world.
To address this limitation, we propose REFinD, the first large-scale annotated
dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types
of entity pairs, generated entirely over financial documents. We also provide
an empirical evaluation with various state-of-the-art models as benchmarks for
the RE task and highlight the challenges posed by our dataset. We observed that
various state-of-the-art deep learning models struggle with numeric inference,
relational and directional ambiguity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18323">
<div class="article-summary-box-inner">
<span><p>Augmented Language Models (ALMs) blend the reasoning capabilities of Large
Language Models (LLMs) with tools that allow for knowledge retrieval and action
execution. Existing ALM systems trigger LLM thought processes while pulling
observations from these tools in an interleaved fashion. Specifically, an LLM
reasons to call an external tool, gets halted to fetch the tool's response, and
then decides the next action based on all preceding response tokens. Such a
paradigm, though straightforward and easy to implement, often leads to huge
computation complexity from redundant prompts and repeated execution. This
study addresses such challenges for the first time, proposing a modular
paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning
process from external observations, thus significantly reducing token
consumption. Comprehensive evaluations across six public NLP benchmarks and a
curated dataset reveal consistent performance enhancements with our proposed
methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy
improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO
demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency,
decoupling parametric modules from non-parametric tool calls enables
instruction fine-tuning to offload LLMs into smaller language models, thus
substantially reducing model parameters. Our illustrative work offloads
reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant
potential for truly efficient and scalable ALM systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18324">
<div class="article-summary-box-inner">
<span><p>A common way to use large pre-trained language models for downstream tasks is
to fine tune them using additional layers. This may not work well if downstream
domain is a specialized domain whereas the large language model has been
pre-trained on a generic corpus. In this paper, we discuss the use of regular
expression patterns employed as features for domain knowledge during the
process of fine tuning, in addition to domain specific text. Our experiments on
real scenario production data show that this method of fine tuning improves the
downstream text classification tasks as compared to fine tuning only on domain
specific text. We also show that the use of attention network for fine tuning
improves results compared to simple linear layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">#REVAL: a semantic evaluation framework for hashtag recommendation. (arXiv:2305.18330v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18330">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation of hashtag recommendation models is a fundamental task
in many online social network systems. In the traditional evaluation method,
the recommended hashtags from an algorithm are firstly compared with the ground
truth hashtags for exact correspondences. The number of exact matches is then
used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This
way of evaluating hashtag similarities is inadequate as it ignores the semantic
correlation between the recommended and ground truth hashtags. To tackle this
problem, we propose a novel semantic evaluation framework for hashtag
recommendation, called #REval. This framework includes an internal module
referred to as BERTag, which automatically learns the hashtag embeddings. We
investigate on how the #REval framework performs under different word embedding
methods and different numbers of synonyms and hashtags in the recommendation
using our proposed #REval-hit-ratio measure. Our experiments of the proposed
framework on three large datasets show that #REval gave more meaningful hashtag
synonyms for hashtag recommendation evaluation. Our analysis also highlights
the sensitivity of the framework to the word embedding technique, with #REval
based on BERTag more superior over #REval based on FastText and Word2Vec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18339">
<div class="article-summary-box-inner">
<span><p>With the widespread use of large artificial intelligence (AI) models such as
ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is
leading a paradigm shift in content creation and knowledge representation. AIGC
uses generative large AI algorithms to assist or replace humans in creating
massive, high-quality, and human-like content at a faster pace and lower cost,
based on user-provided prompts. Despite the recent significant progress in
AIGC, security, privacy, ethical, and legal challenges still need to be
addressed. This paper presents an in-depth survey of working principles,
security and privacy threats, state-of-the-art solutions, and future challenges
of the AIGC paradigm. Specifically, we first explore the enabling technologies,
general architecture of AIGC, and discuss its working modes and key
characteristics. Then, we investigate the taxonomy of security and privacy
threats to AIGC and highlight the ethical and societal implications of GPT and
AIGC technologies. Furthermore, we review the state-of-the-art AIGC
watermarking approaches for regulatable AIGC paradigms regarding the AIGC model
and its produced content. Finally, we identify future challenges and open
research directions related to AIGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis. (arXiv:2305.18340v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18340">
<div class="article-summary-box-inner">
<span><p>The exponential growth in user acquisition and popularity of ChatGPT, an
artificial intelligence(AI) powered chatbot, was accompanied by widespread
mainstream media coverage. This article presents a quantitative data analysis
of the early trends and sentiments revealed by conducting text mining and NLP
methods onto a corpus of 10,902 mainstream news headlines related to the
subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in
November 2022 to March 2023. The findings revealed in sentiment analysis,
ChatGPT and artificial intelligence, were perceived more positively than
negatively in the mainstream media. In regards to word frequency results, over
sixty-five percent of the top frequency words were focused on Big Tech issues
and actors while topics such as jobs, diversity, ethics, copyright, gender and
women were poorly represented or completely absent and only accounted for six
percent of the total corpus. This article is a critical analysis into the power
structures and collusions between Big Tech and Big Media in their matrix of
domination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18342">
<div class="article-summary-box-inner">
<span><p>Generative neural models hold great promise in enhancing programming
education by synthesizing new content for students. We seek to design neural
models that can automatically generate programming tasks for a given
specification in the context of visual programming domains. Despite the recent
successes of large generative models like GPT-4, our initial results show that
these models are ineffective in synthesizing visual programming tasks and
struggle with logical and spatial reasoning. We propose a novel neuro-symbolic
technique, NeurTaskSyn, that can synthesize programming tasks for a
specification given in the form of desired programming concepts exercised by
its solution code and constraints on the visual task. NeurTaskSyn has two
components: the first component is trained via imitation learning procedure to
generate possible solution codes, and the second component is trained via
reinforcement learning procedure to guide an underlying symbolic execution
engine that generates visual tasks for these codes. We demonstrate the
effectiveness of NeurTaskSyn through an extensive empirical evaluation and a
qualitative study on reference tasks taken from the Hour of Code: Classic Maze
challenge by Code-dot-org and the Intro to Programming with Karel course by
CodeHS-dot-com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach. (arXiv:2305.18350v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18350">
<div class="article-summary-box-inner">
<span><p>We present a new task setting for attribute mining on e-commerce products,
serving as a practical solution to extract open-world attributes without
extensive human intervention. Our supervision comes from a high-quality seed
attribute set bootstrapped from existing resources, and we aim to expand the
attribute vocabulary of existing seed types, and also to discover any new
attribute types automatically. A new dataset is created to support our setting,
and our approach Amacer is proposed specifically to tackle the limited
supervision. Especially, given that no direct supervision is available for
those unseen new attributes, our novel formulation exploits self-supervised
heuristic and unsupervised latent attributes, which attains implicit semantic
signals as additional supervision by leveraging product context. Experiments
suggest that our approach surpasses various baselines by 12 F1, expanding
attributes of existing types significantly by up to 12 times, and discovering
values from 39% new types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18354">
<div class="article-summary-box-inner">
<span><p>Can a Large Language Model (LLM) solve simple abstract reasoning problems? We
explore this broad question through a systematic analysis of GPT on the
Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract
reasoning ability from limited examples in which solutions require some "core
knowledge" of concepts such as objects, goal states, counting, and basic
geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when
using textual encodings for their two-dimensional input-output grids. Our
failure analysis reveals that GPT-4's capacity to identify objects and reason
about them is significantly influenced by the sequential nature of the text
that represents an object within a text encoding of a task. To test this
hypothesis, we design a new benchmark, the 1D-ARC, which consists of
one-dimensional (array-like) tasks that are more conducive to GPT-based
reasoning, and where it indeed performs better than on the (2D) ARC. To
alleviate this issue, we propose an object-based representation that is
obtained through an external tool, resulting in nearly doubling the performance
on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the
state-of-the-art GPT-4 is unable to "reason" perfectly within non-language
domains such as the 1D-ARC or a simple ARC subset, our study reveals that the
use of object-based representations can significantly improve its reasoning
ability. Visualizations, GPT logs, and data are available at
https://khalil-research.github.io/LLM4ARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSI: Interactive Deep Learning for Semantic Interaction. (arXiv:2305.18357v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18357">
<div class="article-summary-box-inner">
<span><p>In this paper, we design novel interactive deep learning methods to improve
semantic interactions in visual analytics applications. The ability of semantic
interaction to infer analysts' precise intents during sensemaking is dependent
on the quality of the underlying data representation. We propose the
$\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into
the human-in-the-loop interactive sensemaking pipeline, with two important
properties. First, deep learning extracts meaningful representations from raw
data, which improves semantic interaction inference. Second, semantic
interactions are exploited to fine-tune the deep learning representations,
which then further improves semantic interaction inference. This feedback loop
between human interaction and deep learning enables efficient learning of user-
and task-specific representations. To evaluate the advantage of embedding the
deep learning within the semantic interaction loop, we compare
$\text{DeepSI}_{\text{finetune}}$ against a state-of-the-art but more basic use
of deep learning as only a feature extractor pre-processed outside of the
interactive loop. Results of two complementary studies, a human-centered
qualitative case study and an algorithm-centered simulation-based quantitative
experiment, show that $\text{DeepSI}_{\text{finetune}}$ more accurately
captures users' complex mental models with fewer interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18365">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) with strong abilities in natural language
processing tasks have emerged and have been rapidly applied in various kinds of
areas such as science, finance and software engineering. However, the
capability of LLMs to advance the field of chemistry remains unclear. In this
paper,we establish a comprehensive benchmark containing 8 practical chemistry
tasks, including 1) name prediction, 2) property prediction, 3) yield
prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants
from products), 6)text-based molecule design, 7) molecule captioning, and 8)
reagent selection. Our analysis draws on widely recognized datasets including
BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the
capacities of LLMs within the context of practical chemistry. Three GPT models
(GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in
zero-shot and few-shot in-context learning settings with carefully selected
demonstration examples and specially crafted prompts. The key results of our
investigation are 1) GPT-4 outperforms the other two models among the three
evaluated; 2) GPT models exhibit less competitive performance in tasks
demanding precise understanding of molecular SMILES representation, such as
reaction prediction and retrosynthesis;3) GPT models demonstrate strong
capabilities in text-related explanation tasks such as molecule captioning; and
4) GPT models exhibit comparable or better performance to classical machine
learning models when applied to chemical problems that can be transformed into
classification or ranking tasks, such as property prediction, and yield
prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models. (arXiv:2305.18373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18373">
<div class="article-summary-box-inner">
<span><p>Image ad understanding is a crucial task with wide real-world applications.
Although highly challenging with the involvement of diverse atypical scenes,
real-world entities, and reasoning over scene-texts, how to interpret image ads
is relatively under-explored, especially in the era of foundational
vision-language models (VLMs) featuring impressive generalizability and
adaptability. In this paper, we perform the first empirical study of image ad
understanding through the lens of pre-trained VLMs. We benchmark and reveal
practical challenges in adapting these VLMs to image ad understanding. We
propose a simple feature adaptation strategy to effectively fuse multimodal
information for image ads and further empower it with knowledge of real-world
entities. We hope our study draws more attention to image ad understanding
which is broadly relevant to the advertising industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18390">
<div class="article-summary-box-inner">
<span><p>This work examines the presence of modularity in pre-trained Transformers, a
feature commonly found in human brains and thought to be vital for general
intelligence. In analogy to human brains, we consider two main characteristics
of modularity: (1) functional specialization of neurons: we evaluate whether
each neuron is mainly specialized in a certain function, and find that the
answer is yes. (2) function-based neuron grouping: we explore finding a
structure that groups neurons into modules by function, and each module works
for its corresponding function. Given the enormous amount of possible
structures, we focus on Mixture-of-Experts as a promising candidate, which
partitions neurons into experts and usually activates different experts for
different inputs. Experimental results show that there are functional experts,
where clustered are the neurons specialized in a certain function. Moreover,
perturbing the activations of functional experts significantly affects the
corresponding function. Finally, we study how modularity emerges during
pre-training, and find that the modular structure is stabilized at the early
stage, which is faster than neuron stabilization. It suggests that Transformers
first construct the modular structure and then learn fine-grained neuron
functions. Our code and data are available at
https://github.com/THUNLP/modularity-analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18391">
<div class="article-summary-box-inner">
<span><p>Memes are a popular form of communicating trends and ideas in social media
and on the internet in general, combining the modalities of images and text.
They can express humor and sarcasm but can also have offensive content.
Analyzing and classifying memes automatically is challenging since their
interpretation relies on the understanding of visual elements, language, and
background knowledge. Thus, it is important to meaningfully represent these
sources and the interaction between them in order to classify a meme as a
whole. In this work, we propose to use scene graphs, that express images in
terms of objects and their visual relations, and knowledge graphs as structured
representations for meme classification with a Transformer-based architecture.
We compare our approach with ImgBERT, a multimodal model that uses only learned
(instead of structured) representations of the meme, and observe consistent
improvements. We further provide a dataset with human graph annotations that we
compare to automatically generated graphs and entity linking. Analysis shows
that automatic methods link more entities than human annotators and that
automatically generated graphs are better suited for hatefulness classification
in memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18395">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown promising performance in
knowledge-intensive reasoning tasks that require a compound understanding of
knowledge. However, deployment of the LLMs in real-world applications can be
challenging due to their high computational requirements and concerns on data
privacy. Previous studies have focused on building task-specific small language
models (LMs) by fine-tuning them with labeled data or distilling LLMs. However,
these approaches are ill-suited for knowledge-intensive reasoning tasks due to
the limited capacity of small LMs in memorizing the knowledge required.
Motivated by our theoretical analysis on memorization, we propose
Knowledge-Augmented Reasoning Distillation (KARD), a novel method that
fine-tunes small LMs to generate rationales with augmented knowledge retrieved
from an external knowledge base. Moreover, we further propose a neural reranker
to obtain documents relevant to rationale generation. We empirically show that
KARD significantly improves the performance of small T5 and Flan-T5 models on
the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE and
StrategyQA. Notably, our method makes the 250M models achieve superior
performance against the fine-tuned 3B models, having 12 times larger
parameters, on both MedQA-USMLE and StrategyQA benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18396">
<div class="article-summary-box-inner">
<span><p>Prior works have attempted to build private inference frameworks for
transformer-based large language models (LLMs) in a server-client setting,
where the server holds the model parameters and the client inputs the private
data for inference. However, these frameworks impose significant overhead when
the private inputs are forward propagated through the original LLMs. In this
paper, we show that substituting the computation- and communication-heavy
operators in the transformer architecture with privacy-computing friendly
approximations can greatly reduce the private inference costs with minor impact
on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our
privacy-computing friendly model inference pipeline achieves a $5\times$
acceleration in computation and an 80\% reduction in communication overhead,
while retaining nearly identical accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18404">
<div class="article-summary-box-inner">
<span><p>As large language models continue to be widely developed, robust uncertainty
quantification techniques will become crucial for their safe deployment in
high-stakes scenarios. In this work, we explore how conformal prediction can be
used to provide uncertainty quantification in language models for the specific
task of multiple-choice question-answering. We find that the uncertainty
estimates from conformal prediction are tightly correlated with prediction
accuracy. This observation can be useful for downstream applications such as
selective classification and filtering out low-quality predictions. We also
investigate the exchangeability assumption required by conformal prediction to
out-of-subject questions, which may be a more realistic scenario for many
practical applications. Our work contributes towards more trustworthy and
reliable usage of large language models in safety-critical situations, where
robust guarantees of error rate are required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data. (arXiv:2305.18410v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18410">
<div class="article-summary-box-inner">
<span><p>The need for more usable and explainable machine learning models in
healthcare increases the importance of developing and utilizing causal
discovery algorithms, which aim to discover causal relations by analyzing
observational data. Explainable approaches aid clinicians and biologists in
predicting the prognosis of diseases and suggesting proper treatments. However,
very little research has been conducted at the crossroads between causal
discovery, genomics, and breast cancer, and we aim to bridge this gap.
Moreover, evaluation of causal discovery methods on real data is in general
notoriously difficult because ground-truth causal relations are usually
unknown, and accordingly, in this paper, we also propose to address the
evaluation problem with large language models. In particular, we exploit
suitable causal discovery algorithms to investigate how various perturbations
in the genome can affect the survival of patients diagnosed with breast cancer.
We used three main causal discovery algorithms: PC, Greedy Equivalence Search
(GES), and a Generalized Precision Matrix-based one. We experiment with a
subset of The Cancer Genome Atlas, which contains information about mutations,
copy number variations, protein levels, and gene expressions for 705 breast
cancer patients. Our findings reveal important factors related to the vital
status of patients using causal discovery algorithms. However, the reliability
of these results remains a concern in the medical domain. Accordingly, as
another contribution of the work, the results are validated through language
models trained on biomedical literature, such as BlueBERT and other large
language models trained on medical corpora. Our results profess proper
utilization of causal discovery algorithms and language models for revealing
reliable causal relations for clinical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR. (arXiv:2305.18419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18419">
<div class="article-summary-box-inner">
<span><p>We propose a method of segmenting long-form speech by separating semantically
complete sentences within the utterance. This prevents the ASR decoder from
needlessly processing faraway context while also preventing it from missing
relevant context within the current sentence. Semantically complete sentence
boundaries are typically demarcated by punctuation in written text; but
unfortunately, spoken real-world utterances rarely contain punctuation. We
address this limitation by distilling punctuation knowledge from a
bidirectional teacher language model (LM) trained on written, punctuated text.
We compare our segmenter, which is distilled from the LM teacher, against a
segmenter distilled from a acoustic-pause-based teacher used in other works, on
a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2%
relative WER gain along with a 60 ms median end-of-segment latency reduction on
a YouTube captioning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taming AI Bots: Controllability of Neural States in Large Language Models. (arXiv:2305.18449v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18449">
<div class="article-summary-box-inner">
<span><p>We tackle the question of whether an agent can, by suitable choice of
prompts, control an AI bot to any state. To that end, we first introduce a
formal definition of ``meaning'' that is amenable to analysis. Then, we
characterize ``meaningful data'' on which large language models (LLMs) are
ostensibly trained, and ``well-trained LLMs'' through conditions that are
largely met by today's LLMs. While a well-trained LLM constructs an embedding
space of meanings that is Euclidean, meanings themselves do not form a vector
(linear) subspace, but rather a quotient space within. We then characterize the
subset of meanings that can be reached by the state of the LLMs for some input
prompt, and show that a well-trained bot can reach any meaning albeit with
small probability. We then introduce a stronger notion of controllability as
{\em almost certain reachability}, and show that, when restricted to the space
of meanings, an AI bot is controllable. We do so after introducing a functional
characterization of attentive AI bots, and finally derive necessary and
sufficient conditions for controllability. The fact that AI bots are
controllable means that an adversary could steer them towards any state.
However, the sampling process can be designed to counteract adverse actions and
avoid reaching undesirable regions of state space before their boundary is
crossed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18462">
<div class="article-summary-box-inner">
<span><p>Membership Inference attacks (MIAs) aim to predict whether a data sample was
present in the training data of a machine learning model or not, and are widely
used for assessing the privacy risks of language models. Most existing attacks
rely on the observation that models tend to assign higher probabilities to
their training samples than non-training points. However, simple thresholding
of the model score in isolation tends to lead to high false-positive rates as
it does not account for the intrinsic complexity of a sample. Recent work has
demonstrated that reference-based attacks which compare model scores to those
obtained from a reference model trained on similar data can substantially
improve the performance of MIAs. However, in order to train reference models,
attacks of this kind make the strong and arguably unrealistic assumption that
an adversary has access to samples closely resembling the original training
data. Therefore, we investigate their performance in more realistic scenarios
and find that they are highly fragile in relation to the data distribution used
to train reference models. To investigate whether this fragility provides a
layer of safety, we propose and evaluate neighbourhood attacks, which compare
model scores for a given sample to scores of synthetically generated neighbour
texts and therefore eliminate the need for access to the training data
distribution. We show that, in addition to being competitive with
reference-based attacks that have perfect knowledge about the training data
distribution, our attack clearly outperforms existing reference-free attacks as
well as reference-based attacks with imperfect knowledge, which demonstrates
the need for a reevaluation of the threat model of adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18466">
<div class="article-summary-box-inner">
<span><p>Many recent efforts aim to augment language models with relevant information
retrieved from a database at test time. We avoid the need for prompt
engineering by directly fine-tuning the model on data retrieved at test time
using its standard training setup. For this purpose, we build a large-scale
distributed nearest neighbor index based on text embeddings of the Pile
dataset. Given a query to a language model, our system retrieves the neighbors
of the query and fine-tunes the model on the text data corresponding to those
neighbors. Surprisingly, retrieving and training on as few as 20 neighbors,
each for only one gradient iteration, drastically improves performance across
more than twenty language modeling tasks in the Pile benchmark. For example,
test-time training significantly narrows the performance gap between a small
GPT2 model and a GPTNeo model, more than ten times larger, that was
specifically trained to convergence on the Pile. Sufficient index quality and
size, however, are important. Our work establishes a valuable first baseline
for implementing test-time training in the context of large language models,
opening the door to numerous promising research avenues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18486">
<div class="article-summary-box-inner">
<span><p>The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT's performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT's performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18498">
<div class="article-summary-box-inner">
<span><p>The advents of Large Language Models (LLMs) have shown promise in augmenting
programming using natural interactions. However, while LLMs are proficient in
compiling common usage patterns into a programming language, e.g., Python, it
remains a challenge how to edit and debug an LLM-generated program. We
introduce ANPL, a programming system that allows users to decompose
user-specific tasks. In an ANPL program, a user can directly manipulate sketch,
which specifies the data flow of the generated program. The user annotates the
modules, or hole with natural language descriptions offloading the expensive
task of generating functionalities to the LLM. Given an ANPL program, the ANPL
compiler generates a cohesive Python program that implements the
functionalities in hole, while respecting the dataflows specified in sketch. We
deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique
tasks that are challenging for state-of-the-art AI systems, showing it
outperforms baseline programming systems that (a) without the ability to
decompose tasks interactively and (b) without the guarantee that the modules
can be correctly composed together. We obtain a dataset consisting of 300/400
ARC tasks that were successfully decomposed and grounded in Python, providing
valuable insights into how humans decompose programmatic tasks. See the dataset
at https://iprc-dip.github.io/DARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18500">
<div class="article-summary-box-inner">
<span><p>Vision and text have been fully explored in contemporary video-text
foundational models, while other modalities such as audio and subtitles in
videos have not received sufficient attention. In this paper, we resort to
establish connections between multi-modality video tracks, including Vision,
Audio, and Subtitle, and Text by exploring an automatically generated
large-scale omni-modality video caption dataset called VAST-27M. Specifically,
we first collect 27 million open-domain video clips and separately train a
vision and an audio captioner to generate vision and audio captions. Then, we
employ an off-the-shelf Large Language Model (LLM) to integrate the generated
captions, together with subtitles and instructional prompts into omni-modality
captions. Based on the proposed VAST-27M dataset, we train an omni-modality
video-text foundational model named VAST, which can perceive and process
vision, audio, and subtitle modalities from video, and better support various
tasks including vision-text, audio-text, and multi-modal video-text tasks
(retrieval, captioning and QA). Extensive experiments have been conducted to
demonstrate the effectiveness of our proposed VAST-27M corpus and VAST
foundation model. VAST achieves 22 new state-of-the-art results on various
cross-modality benchmarks. Code, model and dataset will be released at
https://github.com/TXH-mercury/VAST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework. (arXiv:2305.18503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18503">
<div class="article-summary-box-inner">
<span><p>Textual adversarial attacks can discover models' weaknesses by adding
semantic-preserved but misleading perturbations to the inputs. The long-lasting
adversarial attack-and-defense arms race in Natural Language Processing (NLP)
is algorithm-centric, providing valuable techniques for automatic robustness
evaluation. However, the existing practice of robustness evaluation may exhibit
issues of incomprehensive evaluation, impractical evaluation protocol, and
invalid adversarial samples. In this paper, we aim to set up a unified
automatic robustness evaluation framework, shifting towards model-centric
evaluation to further exploit the advantages of adversarial attacks. To address
the above challenges, we first determine robustness evaluation dimensions based
on model capabilities and specify the reasonable algorithm to generate
adversarial samples for each dimension. Then we establish the evaluation
protocol, including evaluation settings and metrics, under realistic demands.
Finally, we use the perturbation degree of adversarial samples to control the
sample validity. We implement a toolkit RobTest that realizes our automatic
robustness evaluation framework. In our experiments, we conduct a robustness
evaluation of RoBERTa models to demonstrate the effectiveness of our evaluation
framework, and further show the rationality of each component in the framework.
The code will be made public at \url{https://github.com/thunlp/RobTest}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18507">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have scaled up to unlock a wide range of complex
reasoning tasks with the aid of various prompting methods. However, current
prompting methods generate natural language intermediate steps to help
reasoning, which can cause imperfect task reduction and confusion. To mitigate
such limitations, we explore code prompting, a neural symbolic prompting method
with both zero-shot and few-shot versions which triggers code as intermediate
steps. We conduct experiments on 7 widely-used benchmarks involving symbolic
reasoning and arithmetic reasoning. Code prompting generally outperforms
chain-of-thought (CoT) prompting. To further understand the performance and
limitations of code prompting, we perform extensive ablation studies and error
analyses, and identify several exclusive advantages of using symbolic
promptings compared to natural language. We also consider the ensemble of code
prompting and CoT prompting to combine the strengths of both. Finally, we show
through experiments how code annotations and their locations affect code
prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics. (arXiv:2305.18513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18513">
<div class="article-summary-box-inner">
<span><p>Transformer-based models, such as BERT and ViT, have achieved
state-of-the-art results across different natural language processing (NLP) and
computer vision (CV) tasks. However, these models are extremely memory
intensive during their fine-tuning process, making them difficult to deploy on
GPUs with limited memory resources. To address this issue, we introduce a new
tool called SlimFit that reduces the memory requirements of these models by
dynamically analyzing their training dynamics and freezing less-contributory
layers during fine-tuning. The layers to freeze are chosen using a runtime
inter-layer scheduling algorithm. SlimFit adopts quantization and pruning for
particular layers to balance the load of dynamic activations and to minimize
the memory footprint of static activations, where static activations refer to
those that cannot be discarded regardless of freezing. This allows SlimFit to
freeze up to 95% of layers and reduce the overall on-device GPU memory usage of
transformer-based models such as ViT and BERT by an average of 2.2x, across
different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10,
CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For
such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory
usage with an accuracy degradation of only up to 0.4%. As a result, while
fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128
requires 3 and 2 32GB GPUs respectively, SlimFit enables their fine-tuning on a
single 32GB GPU without any significant accuracy degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forgotten Knowledge: Examining the Citational Amnesia in NLP. (arXiv:2305.18554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18554">
<div class="article-summary-box-inner">
<span><p>Citing papers is the primary method through which modern scientific writing
discusses and builds on past work. Collectively, citing a diverse set of papers
(in time and area of study) is an indicator of how widely the community is
reading. Yet, there is little work looking at broad temporal patterns of
citation. This work systematically and empirically examines: How far back in
time do we tend to go to cite papers? How has that changed over time, and what
factors correlate with this citational attention/amnesia? We chose NLP as our
domain of interest and analyzed approximately 71.5K papers to show and quantify
several key trends in citation. Notably, around 62% of cited papers are from
the immediate five years prior to publication, whereas only about 17% are more
than ten years old. Furthermore, we show that the median age and age diversity
of cited papers were steadily increasing from 1990 to 2014, but since then, the
trend has reversed, and current NLP papers have an all-time low temporal
citation diversity. Finally, we show that unlike the 1990s, the highly cited
papers in the last decade were also papers with the least citation diversity,
likely contributing to the intense (and arguably harmful) recency focus. Code,
data, and a demo are available on the project homepage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaLI-X: On Scaling up a Multilingual Vision and Language Model. (arXiv:2305.18565v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18565">
<div class="article-summary-box-inner">
<span><p>We present the training recipe and results of scaling up PaLI-X, a
multilingual vision and language model, both in terms of size of the components
and the breadth of its training task mixture. Our model achieves new levels of
performance on a wide-range of varied and complex tasks, including multiple
image-based captioning and question-answering tasks, image-based document
understanding and few-shot (in-context) learning, as well as object detection,
video question answering, and video captioning. PaLI-X advances the
state-of-the-art on most vision-and-language benchmarks considered (25+ of
them). Finally, we observe emerging capabilities, such as complex counting and
multilingual object detection, tasks that are not explicitly in the training
mix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18569">
<div class="article-summary-box-inner">
<span><p>Understanding and addressing unfairness in LLMs are crucial for responsible
AI deployment. However, there is a limited availability of quantitative
analyses and in-depth studies regarding fairness evaluations in LLMs,
especially when applying LLMs to high-stakes fields. This work aims to fill
this gap by providing a systematic evaluation of the effectiveness and fairness
of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's
performance in high-takes fields including education, criminology, finance and
healthcare. To make thorough evaluation, we consider both group fairness and
individual fairness and we also observe the disparities in ChatGPT's outputs
under a set of biased or unbiased prompts. This work contributes to a deeper
understanding of LLMs' fairness performance, facilitates bias mitigation and
fosters the development of responsible artificial intelligence systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding. (arXiv:2305.18576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18576">
<div class="article-summary-box-inner">
<span><p>ICD coding is designed to assign the disease codes to electronic health
records (EHRs) upon discharge, which is crucial for billing and clinical
statistics. In an attempt to improve the effectiveness and efficiency of manual
coding, many methods have been proposed to automatically predict ICD codes from
clinical notes. However, most previous works ignore the decisive information
contained in structured medical data in EHRs, which is hard to be captured from
the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal
Attention Network (TreeMAN) to fuse tabular features and textual features into
multimodal representations by enhancing the text representations with
tree-based features via the attention mechanism. Tree-based features are
constructed according to decision trees learned from structured multimodal
medical data, which capture the decisive information about ICD coding. We can
apply the same multi-label classifier from previous text models to the
multimodal representations to predict ICD codes. Experiments on two MIMIC
datasets show that our method outperforms prior state-of-the-art ICD coding
approaches. The code is available at https://github.com/liu-zichen/TreeMAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18582">
<div class="article-summary-box-inner">
<span><p>Current LLMs have demonstrated remarkable capabilities in addressing users'
requests for various types of information. However, these models are limited by
the most recent data available in their pretraining corpora, rendering them
incapable of providing up-to-date information. Retraining LLMs from scratch is
cost-prohibitive, and the effectiveness of continual fine-tuning on new corpora
has not been thoroughly examined. Additionally, current update procedures
typically demand significant human input to prepare the information into more
structured format, such as knowledge triples, conversational data or responses
with human feedback. In this study, we conduct a comprehensive examination of a
novel self information update task in LLMs, which only requires the provision
of informative text corpora. For instance, we can use the latest news articles
to update the LLMs' existing knowledge. We define the self information update
task and assess the continual fine-tuning approach for this purpose. We observe
that the naive method of continual fine-tuning can be problematic due to LLMs'
exposure bias, which prioritizes existing information over new information we
aim to integrate and leads to incorrect reasoning chains that ultimately
diminish the efficacy of information updates. Based on our analysis, we propose
an effective method to mitigate exposure bias by incorporating the selection of
relevant facts into training losses. Furthermore, we develop a dataset to
evaluate information updates, derived from news articles published after March
2023. Experimental results demonstrate that our proposed approach significantly
increases the factual consistency score (0 to 1) by 0.16 while having minimal
impact on performance for instructions not directly related to the new
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18585">
<div class="article-summary-box-inner">
<span><p>The advent of social media has given rise to numerous ethical challenges,
with hate speech among the most significant concerns. Researchers are
attempting to tackle this problem by leveraging hate-speech detection and
employing language models to automatically moderate content and promote civil
discourse. Unfortunately, recent studies have revealed that hate-speech
detection systems can be misled by adversarial attacks, raising concerns about
their resilience. While previous research has separately addressed the
robustness of these models under adversarial attacks and their
interpretability, there has been no comprehensive study exploring their
intersection. The novelty of our work lies in combining these two critical
aspects, leveraging interpretability to identify potential vulnerabilities and
enabling the design of targeted adversarial attacks. We present a comprehensive
and comparative analysis of adversarial robustness exhibited by various
hate-speech detection models. Our study evaluates the resilience of these
models against adversarial attacks using explainability techniques. To gain
insights into the models' decision-making processes, we employ the Local
Interpretable Model-agnostic Explanations (LIME) framework. Based on the
explainability results obtained by LIME, we devise and execute targeted attacks
on the text by leveraging the TextAttack tool. Our findings enhance the
understanding of the vulnerabilities and strengths exhibited by
state-of-the-art hate-speech detection models. This work underscores the
importance of incorporating explainability in the development and evaluation of
such models to enhance their resilience against adversarial attacks.
Ultimately, this work paves the way for creating more robust and reliable
hate-speech detection systems, fostering safer online environments and
promoting ethical discourse on social media platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Accurate Low Latency ASR for Streaming Voice Search. (arXiv:2305.18596v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18596">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) plays a crucial role in voice-based
applications. For applications requiring real-time feedback like Voice Search,
streaming capability becomes vital. While LSTM/RNN and CTC based ASR systems
are commonly employed for low-latency streaming applications, they often
exhibit lower accuracy compared to state-of-the-art models due to a lack of
future audio frames. In this work, we focus on developing accurate LSTM,
attention, and CTC based streaming ASR models for large-scale Hinglish (a blend
of Hindi and English) Voice Search. We investigate various modifications in
vanilla LSTM training which enhance the system's accuracy while preserving its
streaming capabilities. We also address the critical requirement of
end-of-speech (EOS) detection in streaming applications. We present a simple
training and inference strategy for end-to-end CTC models that enables joint
ASR and EOS detection. The evaluation of our model on Flipkart's Voice Search,
which handles substantial traffic of approximately 6 million queries per day,
demonstrates significant performance gains over the vanilla LSTM-CTC model. Our
model achieves a word error rate (WER) of 3.69% without EOS and 4.78% with EOS
while also reducing the search latency by approximately ~1300 ms (equivalent to
46.64% reduction) when compared to an independent voice activity detection
(VAD) model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces. (arXiv:2305.18598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18598">
<div class="article-summary-box-inner">
<span><p>We study semantic construal in grammatical constructions using large language
models. First, we project contextual word embeddings into three interpretable
semantic spaces, each defined by a different set of psycholinguistic feature
norms. We validate these interpretable spaces and then use them to
automatically derive semantic characterizations of lexical items in two
grammatical constructions: nouns in subject or object position within the same
sentence, and the AANN construction (e.g., `a beautiful three days'). We show
that a word in subject position is interpreted as more agentive than the very
same word in object position, and that the nouns in the AANN construction are
interpreted as more measurement-like than when in the canonical alternation.
Our method can probe the distributional meaning of syntactic constructions at a
templatic level, abstracted away from specific lexemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization for Multimodal Fake News Detection. (arXiv:2305.18599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18599">
<div class="article-summary-box-inner">
<span><p>The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for fake news
detection. However, state-of-the-art approaches are usually trained on datasets
of smaller size or with a limited set of specific topics. As a consequence,
these models lack generalization capabilities and are not applicable to
real-world data. In this paper, we propose three models that adopt and
fine-tune state-of-the-art multimodal transformers for multimodal fake news
detection. We conduct an in-depth analysis by manipulating the input data aimed
to explore models performance in realistic use cases on social media. Our study
across multiple models demonstrates that these systems suffer significant
performance drops against manipulated data. To reduce the bias and improve
model generalization, we suggest training data augmentation to conduct more
meaningful experiments for fake news detection on social media. The proposed
data augmentation techniques enable models to generalize better and yield
improved state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From `Snippet-lects' to Doculects and Dialects: Leveraging Neural Representations of Speech for Placing Audio Signals in a Language Landscape. (arXiv:2305.18602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18602">
<div class="article-summary-box-inner">
<span><p>XLSR-53 a multilingual model of speech, builds a vector representation from
audio, which allows for a range of computational treatments. The experiments
reported here use this neural representation to estimate the degree of
closeness between audio files, ultimately aiming to extract relevant linguistic
properties. We use max-pooling to aggregate the neural representations from a
"snippet-lect" (the speech in a 5-second audio snippet) to a "doculect" (the
speech in a given resource), then to dialects and languages. We use data from
corpora of 11 dialects belonging to 5 less-studied languages. Similarity
measurements between the 11 corpora bring out greatest closeness between those
that are known to be dialects of the same language. The findings suggest that
(i) dialect/language can emerge among the various parameters characterizing
audio files and (ii) estimates of overall phonetic/phonological closeness can
be obtained for a little-resourced or fully unknown language. The findings help
shed light on the type of information captured by neural representations of
speech and how it can be extracted from these representations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard. (arXiv:2305.18618v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18618">
<div class="article-summary-box-inner">
<span><p>A comparison between three chatbots which are based on large language models,
namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their
ability to give correct answers to mathematics and logic problems. In
particular, we check their ability to Understand the problem at hand; Apply
appropriate algorithms or methods for its solution; and Generate a coherent
response and a correct answer. We use 30 questions that are clear, without any
ambiguities, fully described with plain text only, and have a unique, well
defined correct answer. The questions are divided into two sets of 15 each. The
questions of Set A are 15 "Original" problems that cannot be found online,
while Set B contains 15 "Published" problems that one can find online, usually
with their solution. Each question is posed three times to each chatbot. The
answers are recorded and discussed, highlighting their strengths and
weaknesses. It has been found that for straightforward arithmetic, algebraic
expressions, or basic logic puzzles, chatbots may provide accurate solutions,
although not in every attempt. However, for more complex mathematical problems
or advanced logic tasks, their answers, although written in a usually
"convincing" way, may not be reliable. Consistency is also an issue, as many
times a chatbot will provide conflicting answers when given the same question
more than once. A comparative quantitative evaluation of the three chatbots is
made through scoring their final answers based on correctness. It was found
that ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes
third in the original questions of Set A, behind the other two chatbots, while
it has the best performance (first place) in the published questions of Set B.
This is probably because Bard has direct access to the internet, in contrast to
ChatGPT chatbots which do not have any communication with the outside world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18619">
<div class="article-summary-box-inner">
<span><p>Despite a growing interest in diffusion-based language models, existing work
has not shown that these models can attain nontrivial likelihoods on standard
language modeling benchmarks. In this work, we take the first steps towards
closing the likelihood gap between autoregressive and diffusion-based language
models, with the goal of building and releasing a diffusion model which
outperforms a small but widely-known autoregressive model. We pursue this goal
through algorithmic improvements, scaling laws, and increased compute. On the
algorithmic front, we introduce several methodological improvements for the
maximum-likelihood training of diffusion language models. We then study scaling
laws for our diffusion models and find compute-optimal training regimes which
differ substantially from autoregressive models. Using our methods and scaling
analysis, we train and release Plaid 1B, a large diffusion language model which
outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent
samples in unconditional and zero-shot control settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONA: A novel CONtext-Aware instruction paradigm for communication using large language model. (arXiv:2305.18620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18620">
<div class="article-summary-box-inner">
<span><p>We introduce CONA, a novel context-aware instruction paradigm for effective
knowledge dissemination using generative pre-trained transformer (GPT) models.
CONA is a flexible framework designed to leverage the capabilities of Large
Language Models (LLMs) and incorporate DIKW (Data, Information, Knowledge,
Wisdom) hierarchy to automatically instruct and optimise presentation content,
anticipate potential audience inquiries, and provide context-aware answers that
adaptive to the knowledge level of the audience group. The unique aspect of the
CONA paradigm lies in its combination of an independent advisory mechanism and
a recursive feedback loop rooted on the DIKW hierarchy. This synergy
significantly enhances context-aware contents, ensuring they are accessible and
easily comprehended by the audience. This paradigm is an early pioneer to
explore new methods for knowledge dissemination and communication in the LLM
era, offering effective support for everyday knowledge sharing scenarios. We
conduct experiments on a range of audience roles, along with materials from
various disciplines using GPT4. Both quantitative and qualitative results
demonstrated that the proposed CONA paradigm achieved remarkable performance
compared to the outputs guided by conventional prompt engineering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alfred: A System for Prompted Weak Supervision. (arXiv:2305.18623v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18623">
<div class="article-summary-box-inner">
<span><p>Alfred is the first system for programmatic weak supervision (PWS) that
creates training data for machine learning by prompting. In contrast to typical
PWS systems where weak supervision sources are programs coded by experts,
Alfred enables users to encode their subject matter expertise via natural
language prompts for language and vision-language models. Alfred provides a
simple Python interface for the key steps of this emerging paradigm, with a
high-throughput backend for large-scale data labeling. Users can quickly
create, evaluate, and refine their prompt-based weak supervision sources; map
the results to weak labels; and resolve their disagreements with a label model.
Alfred enables a seamless local development experience backed by models served
from self-managed computing clusters. It automatically optimizes the execution
of prompts with optimized batching mechanisms. We find that this optimization
improves query throughput by 2.9x versus a naive approach. We present two
example use cases demonstrating Alfred on YouTube comment spam detection and
pet breeds classification. Alfred is open source, available at
https://github.com/BatsResearch/alfred.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18624">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a popular solution for few-shot Name Entity
Recognization (NER). The conventional configuration strives to reduce the
distance between tokens with the same labels and increase the distance between
tokens with different labels. The effect of this setup may, however, in the
medical domain, there are a lot of entities annotated as OUTSIDE (O), and they
are undesirably pushed apart to other entities that are not labeled as OUTSIDE
(O) by the current contrastive learning method end up with a noisy prototype
for the semantic representation of the label, though there are many OUTSIDE (O)
labeled entities are relevant to the labeled entities. To address this
challenge, we propose a novel method named Weighted Prototypical Contrastive
Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our
approach primarily revolves around constructing the prototype-based contractive
loss and weighting network. These components play a crucial role in assisting
the model in differentiating the negative samples from OUTSIDE (O) tokens and
enhancing the discrimination ability of contrastive learning. Experimental
results show that our proposed W-PROCER framework significantly outperforms the
strong baselines on the three medical benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model. (arXiv:2305.18638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18638">
<div class="article-summary-box-inner">
<span><p>In this study, we developed an automated short answer grading (ASAG) model
that provided both analytic scores and final holistic scores. Short answer
items typically consist of multiple sub-questions, and providing an analytic
score and the text span relevant to each sub-question can increase the
interpretability of the automated scores. Furthermore, they can be used to
generate actionable feedback for students. Despite these advantages, most
studies have focused on predicting only holistic scores due to the difficulty
in constructing dataset with manual annotations. To address this difficulty, we
used large language model (LLM)-based one-shot prompting and a text similarity
scoring model with domain adaptation using small manually annotated dataset.
The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a
subset of the publicly available ASAG dataset. The model achieved a substantial
improvement over the majority baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs. (arXiv:2305.18641v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18641">
<div class="article-summary-box-inner">
<span><p>Building cross-model intelligence that can understand charts and communicate
the salient information hidden behind them is an appealing challenge in the
vision and language(V+L) community. The capability to uncover the underlined
table data of chart figures is a critical key to automatic chart understanding.
We introduce ChartT5, a V+L model that learns how to interpret table
information from chart images via cross-modal pre-training on plot table pairs.
Specifically, we propose two novel pre-training objectives: Masked Header
Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with
different skills to interpret the table information. We have conducted
extensive experiments on chart question answering and chart summarization to
verify the effectiveness of the proposed pre-training strategies. In
particular, on the ChartQA benchmark, our ChartT5 outperforms the
state-of-the-art non-pretraining methods by over 8% performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Inspiring Content on Social Media. (arXiv:2109.02734v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02734">
<div class="article-summary-box-inner">
<span><p>Inspiration moves a person to see new possibilities and transforms the way
they perceive their own potential. Inspiration has received little attention in
psychology, and has not been researched before in the NLP community. To the
best of our knowledge, this work is the first to study inspiration through
machine learning methods. We aim to automatically detect inspiring content from
social media data. To this end, we analyze social media posts to tease out what
makes a post inspiring and what topics are inspiring. We release a dataset of
5,800 inspiring and 5,800 non-inspiring English-language public post unique ids
collected from a dump of Reddit public posts made available by a third party
and use linguistic heuristics to automatically detect which social media
English-language posts are inspiring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02732">
<div class="article-summary-box-inner">
<span><p>Conversational recommender systems (CRS) aim to recommend suitable items to
users through natural language conversations. For developing effective CRSs, a
major technical issue is how to accurately infer user preference from very
limited conversation context. To address issue, a promising solution is to
incorporate external data for enriching the context information. However, prior
studies mainly focus on designing fusion models tailored for some specific type
of external data, which is not general to model and utilize multi-type external
data.
</p>
<p>To effectively leverage multi-type external data, we propose a novel
coarse-to-fine contrastive learning framework to improve data semantic fusion
for CRS. In our approach, we first extract and represent multi-grained semantic
units from different data signals, and then align the associated multi-type
semantic units in a coarse-to-fine way. To implement this framework, we design
both coarse-grained and fine-grained procedures for modeling user preference,
where the former focuses on more general, coarse-grained semantic fusion and
the latter focuses on more specific, fine-grained semantic fusion. Such an
approach can be extended to incorporate more kinds of external data. Extensive
experiments on two public CRS datasets have demonstrated the effectiveness of
our approach in both recommendation and conversation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13151">
<div class="article-summary-box-inner">
<span><p>We design and evaluate a Bayesian optimization framework for resource
efficient pre-training of Transformer-based language models (TLMs). TLM
pre-training requires high computational resources and introduces many
unresolved design choices, such as selecting its pre-training hyperparameters.
We propose a multi-armed bandit framework for the sequential selection of TLM
pre-training hyperparameters, aimed at optimizing language model performance,
in a resource efficient manner. We design a Thompson sampling algorithm, with a
surrogate Gaussian process reward model of the Masked Language Model (MLM)
pre-training objective, for its sequential minimization. Instead of MLM
pre-training with fixed masking probabilities, the proposed Gaussian
process-based Thompson sampling (GP-TS) accelerates pre-training by
sequentially selecting masking hyperparameters that improve performance. We
empirically demonstrate how GP-TS pre-trains language models efficiently, i.e.,
it achieves lower MLM loss in fewer epochs, across a variety of settings. In
addition, GP-TS pre-trained TLMs attain competitive downstream performance,
while avoiding expensive hyperparameter grid search. GP-TS provides an
interactive framework for efficient and optimized TLM pre-training that, by
circumventing costly hyperparameter selection, enables substantial
computational savings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversity Enhanced Table-to-Text Generation via Type Control. (arXiv:2205.10938v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10938">
<div class="article-summary-box-inner">
<span><p>Generating natural language statements to convey logical inferences from
tabular data (i.e., Logical NLG) is a process with one input and a variety of
valid outputs. This characteristic underscores the need for a method to produce
a diverse set of valid outputs, presenting different perspectives of the input
data. We propose a simple yet effective diversity-enhancing scheme that builds
upon an inherent property of the statements, their logic-types, by using a
type-controlled table-to-text generation model. We demonstrate, through
extensive automatic and human evaluations over the two publicly available
Logical NLG datasets, that our proposed method both facilitates the ability to
effectively control the generated statement type, and produces results superior
to the strongest baselines in terms of quality and factuality-diversity
trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization. (arXiv:2209.03549v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03549">
<div class="article-summary-box-inner">
<span><p>The problems of unfaithful summaries have been widely discussed under the
context of abstractive summarization. Though extractive summarization is less
prone to the common unfaithfulness issues of abstractive summaries, does that
mean extractive is equal to faithful? Turns out that the answer is no. In this
work, we define a typology with five types of broad unfaithfulness problems
(including and beyond not-entailment) that can appear in extractive summaries,
including incorrect coreference, incomplete coreference, incorrect discourse,
incomplete discourse, as well as other misleading information. We ask humans to
label these problems out of 1600 English summaries produced by 16 diverse
extractive systems. We find that 30% of the summaries have at least one of the
five issues. To automatically detect these problems, we find that 5 existing
faithfulness evaluation metrics for summarization have poor correlations with
human judgment. To remedy this, we propose a new metric, ExtEval, that is
designed for detecting unfaithful extractive summaries and is shown to have the
best performance. We hope our work can increase the awareness of unfaithfulness
problems in extractive summarization and help future work to evaluate and
resolve these issues. Our data and code are publicly available at
https://github.com/ZhangShiyue/extractive_is_not_faithful
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physical computation and compositionality. (arXiv:2210.00392v3 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00392">
<div class="article-summary-box-inner">
<span><p>Developments in quantum computing and, more in general, non-standard
computing systems, represent a clear indication that the very notion of what a
physical computing device is and does should be recast in a rigorous and sound
framework. Physical computing has opened a whole stream of new research aimed
to understand and control how information is processed by several types of
physical devices. Therefore, classical definitions and entire frameworks need
to be adapted in order to fit a broader notion of what physical computing
systems really are. Recent studies have proposed a formalism that can be used
to carve out a more proper notion of physical computing. In this paper we
present a framework which capture such results in a very natural way via some
basic constructions in Category Theory. Furthermore, we show that, within our
framework, the compositional nature of physical computing systems is naturally
formalized, and that it can be organized in coherent structures by the means of
their relational nature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods. (arXiv:2210.07222v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07222">
<div class="article-summary-box-inner">
<span><p>Saliency maps can explain a neural model's predictions by identifying
important input features. They are difficult to interpret for laypeople,
especially for instances with many features. In order to make them more
accessible, we formalize the underexplored task of translating saliency maps
into natural language and compare methods that address two key challenges of
this approach -- what and how to verbalize. In both automatic and human
evaluation setups, using token-level attributions from text classification
tasks, we compare two novel methods (search-based and instruction-based
verbalizations) against conventional feature importance representations
(heatmap visualizations and extractive rationales), measuring simulatability,
faithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to
generate saliency map verbalizations yields plausible explanations which
include associations, abstractive summarization and commonsense reasoning,
achieving by far the highest human ratings, but they are not faithfully
capturing numeric information and are inconsistent in their interpretation of
the task. In comparison, our search-based, model-free verbalization approach
efficiently completes templated verbalizations, is faithful by design, but
falls short in helpfulness and simulatability. Our results suggest that
saliency map verbalization makes feature attribution explanations more
comprehensible and less cognitively challenging to humans than conventional
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers. (arXiv:2211.02809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02809">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) and speech translation (ST) can both use
neural transducers as the model structure. It is thus possible to use a single
transducer model to perform both tasks. In real-world applications, such joint
ASR and ST models may need to be streaming and do not require source language
identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a
streaming language-agnostic multilingual speech recognition and translation
model using neural transducers. Based on the transducer model structure, we
propose four methods, a unified joint and prediction network for multilingual
output, a clustered multilingual encoder, target language identification for
encoder, and connectionist temporal classification regularization. Experimental
results show that LAMASSU not only drastically reduces the model size but also
reaches the performances of monolingual ASR and bilingual ST models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using contradictions improves question answering systems. (arXiv:2211.05598v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05598">
<div class="article-summary-box-inner">
<span><p>This work examines the use of contradiction in natural language inference
(NLI) for question answering (QA). Typically, NLI systems help answer questions
by determining if a potential answer is \emph{entailed} (supported) by some
background context. But is it useful to also determine if an answer contradicts
the context? We test this in two settings, multiple choice and extractive QA,
and find that systems that incorporate contradiction can do slightly better
than entailment-only systems on certain datasets. However, the best
performances come from using contradiction, entailment, and QA model confidence
scores together. This has implications for the deployment of QA systems in
domains such as medicine and science where safety is an issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">World Knowledge in Multiple Choice Reading Comprehension. (arXiv:2211.07040v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07040">
<div class="article-summary-box-inner">
<span><p>Recently it has been shown that without any access to the contextual passage,
multiple choice reading comprehension (MCRC) systems are able to answer
questions significantly better than random on average. These systems use their
accumulated "world knowledge" to directly answer questions, rather than using
information from the passage. This paper examines the possibility of exploiting
this observation as a tool for test designers to ensure that the use of "world
knowledge" is acceptable for a particular set of questions. We propose
information-theory based metrics that enable the level of "world knowledge"
exploited by systems to be assessed. Two metrics are described: the expected
number of options, which measures whether a passage-free system can identify
the answer a question using world knowledge; and the contextual mutual
information, which measures the importance of context for a given question. We
demonstrate that questions with low expected number of options, and hence
answerable by the shortcut system, are often similarly answerable by humans
without context. This highlights that the general knowledge 'shortcuts' could
be equally used by exam candidates, and that our proposed metrics may be
helpful for future test designers to monitor the quality of questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14228">
<div class="article-summary-box-inner">
<span><p>In order to train children's ability to ask curiosity-driven questions,
previous research has explored designing specific exercises relying on
providing semantic and linguistic cues to help formulate such questions. But
despite showing pedagogical efficiency, this method is still limited as it
relies on generating the said cues by hand, which can be a very costly process.
In this context, we propose to leverage advances in the natural language
processing field (NLP) and investigate the efficiency of using a large language
model (LLM) for automating the production of the pedagogical content of a
curious question-asking (QA) training. We study generating the said content
using the "prompt-based" method that consists of explaining the task to the LLM
in natural text. We evaluate the output using human experts annotations and
comparisons with hand-generated content. Results suggested indeed the relevance
and usefulness of this content. We also conduct a field study in primary school
(75 children aged 9-10), where we evaluate children's QA performance when
having this training. We compare 3 types of content : 1) hand-generated content
that proposes "closed" cues leading to predefined questions; 2) GPT-3-generated
content that proposes the same type of cues; 3) GPT-3-generated content that
proposes "open" cues leading to several possible questions. We see a similar QA
performance between the two "closed" trainings (showing the scalability of the
approach using GPT-3), and a better one for participants with the "open"
training. These results suggest the efficiency of using LLMs to support
children in generating more curious questions, using a natural language
prompting approach that affords usability by teachers and other users not
specialists of AI techniques. Furthermore, results also show that open-ended
content may be more suitable for training curious question-asking skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06094">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated outstanding performance on a wide
range of tasks such as question answering and code generation. On a high level,
given an input, a language model can be used to automatically complete the
sequence in a statistically-likely way. Based on this, users prompt these
models with language instructions or examples, to implement a variety of
downstream tasks. Advanced prompting methods can even imply interaction between
the language model, a user, and external tools such as calculators. However, to
obtain state-of-the-art performance or adapt language models for specific
tasks, complex task- and model-specific programs have to be implemented, which
may still require ad-hoc interaction.
</p>
<p>Based on this, we present the novel idea of Language Model Programming (LMP).
LMP generalizes language model prompting from pure text prompts to an intuitive
combination of text prompting and scripting. Additionally, LMP allows
constraints to be specified over the language model output. This enables easy
adaption to many tasks while abstracting language model internals and providing
high-level semantics.
</p>
<p>To enable LMP, we implement LMQL(short for Language Model Query Language),
which leverages the constraints and control flow from an LMP prompt to generate
an efficient inference procedure that minimizes the number of expensive calls
to the underlying language model.
</p>
<p>We show that LMQL can capture a wide range of state-of-the-art prompting
methods in an intuitive way, especially facilitating interactive flows that are
challenging to implement with existing high-level APIs. Our evaluation shows
that we retain or increase the accuracy on several downstream tasks, while also
significantly reducing the required amount of computation or cost in the case
of pay-to-use APIs (26-85% cost savings).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08011">
<div class="article-summary-box-inner">
<span><p>Dialect differences caused by regional, social, and economic factors cause
performance discrepancies for many groups of language technology users.
Inclusive and equitable language technology must critically be dialect
invariant, meaning that performance remains constant over dialectal shifts.
Current systems often fall short of this ideal since they are designed and
tested on a single dialect: Standard American English (SAE). We introduce a
suite of resources for evaluating and achieving English dialect invariance. The
resource is called Multi-VALUE, a controllable rule-based translation system
spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE
maps SAE to synthetic forms of each dialect. First, we use this system to
stress tests question answering, machine translation, and semantic parsing.
Stress tests reveal significant performance disparities for leading models on
non-standard dialects. Second, we use this system as a data augmentation
technique to improve the dialect robustness of existing systems. Finally, we
partner with native speakers of Chicano and Indian English to release new
gold-standard variants of the popular CoQA task. To execute the transformation
code, run model checkpoints, and download both synthetic and gold-standard
dialectal benchmark datasets, see <a href="http://value-nlp.org.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09739">
<div class="article-summary-box-inner">
<span><p>Training learnable metrics using modern language models has recently emerged
as a promising method for the automatic evaluation of machine translation.
However, existing human evaluation datasets for text simplification have
limited annotations that are based on unitary or outdated models, making them
unsuitable for this approach. To address these issues, we introduce the
SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on
2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging
simplification benchmark consisting of over 1K human ratings of 360
simplifications including GPT-3.5 generated text. Training on SimpEval, we
present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive
empirical results show that LENS correlates much better with human judgment
than existing metrics, paving the way for future progress in the evaluation of
text simplification. We also introduce Rank and Rate, a human evaluation
framework that rates simplifications from several models in a list-wise manner
using an interactive interface, which ensures both consistency and accuracy in
the evaluation process and is used to create the SimpEval datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Embedder, Any Task: Instruction-Finetuned Text Embeddings. (arXiv:2212.09741v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09741">
<div class="article-summary-box-inner">
<span><p>We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Trajectories of Language Models Across Scales. (arXiv:2212.09803v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09803">
<div class="article-summary-box-inner">
<span><p>Scaling up language models has led to unprecedented performance gains, but
little is understood about how the training dynamics change as models get
larger. How do language models of different sizes learn during pre-training?
Why do larger language models demonstrate more desirable behaviors? In this
paper, we analyze the intermediate training checkpoints of differently sized
OPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token
prediction, sequence-level generation, and downstream tasks. We find that 1) at
a given perplexity and independent of model sizes, a similar subset of training
tokens see the most significant reduction in loss, with the rest stagnating or
showing double-descent behavior; 2) early in training, all models learn to
reduce the perplexity of grammatical sequences that contain hallucinations,
with small models halting at this suboptimal distribution and larger ones
eventually learning to assign these sequences lower probabilities; 3)
perplexity is a strong predictor of in-context learning performance on 74
multiple-choice tasks from BIG-Bench, and this holds independent of the model
size. Together, these results show that perplexity is more predictive of model
behaviors than model size or training computation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. (arXiv:2212.10511v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10511">
<div class="article-summary-box-inner">
<span><p>Despite their impressive performance on diverse tasks, large language models
(LMs) still struggle with tasks requiring rich world knowledge, implying the
limitations of relying solely on their parameters to encode a wealth of world
knowledge. This paper aims to understand LMs' strengths and limitations in
memorizing factual knowledge, by conducting large-scale knowledge probing
experiments of 10 models and 4 augmentation methods on PopQA, our new
open-domain QA dataset with 14k questions. We find that LMs struggle with less
popular factual knowledge, and that scaling fails to appreciably improve
memorization of factual knowledge in the long tail. We then show that
retrieval-augmented LMs largely outperform orders of magnitude larger LMs,
while unassisted LMs remain competitive in questions about high-popularity
entities. Based on those findings, we devise a simple, yet effective, method
for powerful and efficient retrieval-augmented LMs, which retrieves
non-parametric memories only when necessary. Experimental results show that
this significantly improves models' performance while reducing the inference
costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically-informed Hierarchical Event Modeling. (arXiv:2212.10547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10547">
<div class="article-summary-box-inner">
<span><p>Prior work has shown that coupling sequential latent variable models with
semantic ontological knowledge can improve the representational capabilities of
event modeling approaches. In this work, we present a novel, doubly
hierarchical, semi-supervised event modeling framework that provides structural
hierarchy while also accounting for ontological hierarchy. Our approach
consists of multiple layers of structured latent variables, where each
successive layer compresses and abstracts the previous layers. We guide this
compression through the injection of structured ontological knowledge that is
defined at the type level of events: importantly, our model allows for partial
injection of semantic knowledge and it does not depend on observing instances
at any particular level of the semantic ontology. Across two different datasets
and four different evaluation metrics, we demonstrate that our approach is able
to out-perform the previous state-of-the-art approaches by up to 8.5%,
demonstrating the benefits of structured and semantic hierarchical knowledge
for event modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ORCA: A Challenging Benchmark for Arabic Language Understanding. (arXiv:2212.10758v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10758">
<div class="article-summary-box-inner">
<span><p>Due to their crucial role in all NLP, several benchmarks have been proposed
to evaluate pretrained language models. In spite of these efforts, no public
benchmark of diverse nature currently exists for evaluation of Arabic. This
makes it challenging to measure progress for both Arabic and multilingual
language models. This challenge is compounded by the fact that any benchmark
targeting Arabic needs to take into account the fact that Arabic is not a
single language but rather a collection of languages and varieties. In this
work, we introduce ORCA, a publicly available benchmark for Arabic language
understanding evaluation. ORCA is carefully constructed to cover diverse Arabic
varieties and a wide range of challenging Arabic understanding tasks exploiting
60 different datasets across seven NLU task clusters. To measure current
progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between
18 multilingual and Arabic language models. We also provide a public
leaderboard with a unified single-number evaluation metric (ORCA score) to
facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders. (arXiv:2212.10818v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10818">
<div class="article-summary-box-inner">
<span><p>The network architecture of end-to-end (E2E) automatic speech recognition
(ASR) can be classified into several models, including connectionist temporal
classification (CTC), recurrent neural network transducer (RNN-T), attention
mechanism, and non-autoregressive mask-predict models. Since each of these
network architectures has pros and cons, a typical use case is to switch these
separate models depending on the application requirement, resulting in the
increased overhead of maintaining all models. Several methods for integrating
two of these complementary models to mitigate the overhead issue have been
proposed; however, if we integrate more models, we will further benefit from
these complementary models and realize broader applications with a single
system. This paper proposes four-decoder joint modeling (4D) of CTC, attention,
RNN-T, and mask-predict, which has the following three advantages: 1) The four
decoders are jointly trained so that they can be easily switched depending on
the application scenarios. 2) Joint training may bring model regularization and
improve the model robustness thanks to their complementary properties. 3) Novel
one-pass joint decoding methods using CTC, attention, and RNN-T further
improves the performance. The experimental results showed that the proposed
model consistently reduced the WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10823">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE), which has relied on structurally annotated corpora
for model training, has been particularly challenging in low-resource scenarios
and domains. Recent literature has tackled low-resource RE by self-supervised
learning, where the solution involves pretraining the entity pair embedding by
RE-based objective and finetuning on labeled data by classification-based
objective. However, a critical challenge to this approach is the gap in
objectives, which prevents the RE model from fully utilizing the knowledge in
pretrained representations. In this paper, we aim at bridging the gap and
propose to pretrain and finetune the RE model using consistent objectives of
contrastive learning. Since in this kind of representation learning paradigm,
one relation may easily form multiple clusters in the representation space, we
further propose a multi-center contrastive loss that allows one relation to
form multiple clusters to better align with pretraining. Experiments on two
document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness
of our method. Particularly, when using 1% end-task training data, our method
outperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning. (arXiv:2301.10915v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10915">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) is an important step in dialogue management to
keep track of users' beliefs. Existing works fine-tune all language model (LM)
parameters to tackle the DST task, which requires significant data and
computing resources for training and hosting. The cost grows exponentially in
the real-world deployment where dozens of fine-tuned LM are used for different
domains and tasks. To reduce parameter size and better utilize cross-task
shared information, we propose to use soft prompt token embeddings to learn
task properties. Without tuning LM parameters, our method drastically reduces
the number of parameters needed to less than 0.5% of prior works while achieves
better low-resource DST performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11716">
<div class="article-summary-box-inner">
<span><p>The gap between speech and text modalities is a major challenge in
speech-to-text translation (ST). Different methods have been proposed to reduce
this gap, but most of them require architectural changes in ST training. In
this work, we propose to mitigate this issue at the pre-training stage,
requiring no change in the ST model. First, we show that the connectionist
temporal classification (CTC) loss can reduce the modality gap by design. We
provide a quantitative comparison with the more common cross-entropy loss,
showing that pre-training with CTC consistently achieves better final ST
accuracy. Nevertheless, CTC is only a partial solution and thus, in our second
contribution, we propose a novel pre-training method combining CTC and optimal
transport to further reduce this gap. Our method pre-trains a Siamese-like
model composed of two encoders, one for acoustic inputs and the other for
textual inputs, such that they produce representations that are close to each
other in the Wasserstein space. Extensive experiments on the standard CoVoST-2
and MuST-C datasets show that our pre-training method applied to the vanilla
encoder-decoder Transformer achieves state-of-the-art performance under the
no-external-data setting, and performs on par with recent strong multi-task
learning systems trained with external data. Finally, our method can also be
applied on top of these multi-task systems, leading to further improvements for
these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Layer-wise Score Aggregation for Textual OOD Detection. (arXiv:2302.09852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09852">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection is a rapidly growing field due to new
robustness and security requirements driven by an increased number of AI-based
systems. Existing OOD textual detectors often rely on an anomaly score (e.g.,
Mahalanobis distance) computed on the embedding output of the last layer of the
encoder. In this work, we observe that OOD detection performance varies greatly
depending on the task and layer output. More importantly, we show that the
usual choice (the last layer) is rarely the best one for OOD detection and that
far better results could be achieved if the best layer were picked. To leverage
this observation, we propose a data-driven, unsupervised method to combine
layer-wise anomaly scores. In addition, we extend classical textual OOD
benchmarks by including classification tasks with a greater number of classes
(up to 77), which reflects more realistic settings. On this augmented
benchmark, we show that the proposed post-aggregation methods achieve robust
and consistent results while removing manual feature selection altogether.
Their performance achieves near oracle's best layer performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08127">
<div class="article-summary-box-inner">
<span><p>CB2 is a multi-agent platform to study collaborative natural language
interaction in a grounded task-oriented scenario. It includes a 3D game
environment, a backend server designed to serve trained models to human agents,
and various tools and processes to enable scalable studies. We deploy CB2 at
https://cb2.ai as a system demonstration with a learned instruction following
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Performance of GPT-3.5 and GPT-4 in Grammatical Error Correction. (arXiv:2303.14342v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14342">
<div class="article-summary-box-inner">
<span><p>GPT-3 and GPT-4 models are powerful, achieving high performance on a variety
of Natural Language Processing tasks. However, there is a relative lack of
detailed published analysis of their performance on the task of grammatical
error correction (GEC). To address this, we perform experiments testing the
capabilities of a GPT-3.5 model (text-davinci-003) and a GPT-4 model
(gpt-4-0314) on major GEC benchmarks. We compare the performance of different
prompts in both zero-shot and few-shot settings, analyzing intriguing or
problematic outputs encountered with different prompt formats. We report the
performance of our best prompt on the BEA-2019 and JFLEG datasets, finding that
the GPT models can perform well in a sentence-level revision setting, with
GPT-4 achieving a new high score on the JFLEG benchmark. Through human
evaluation experiments, we compare the GPT models' corrections to source, human
reference, and baseline GEC system sentences and observe differences in editing
strategies and how they are scored by human raters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContraSim -- A Similarity Measure Based on Contrastive Learning. (arXiv:2303.16992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16992">
<div class="article-summary-box-inner">
<span><p>Recent work has compared neural network representations via similarity-based
analyses to improve model interpretation. The quality of a similarity measure
is typically evaluated by its success in assigning a high score to
representations that are expected to be matched. However, existing similarity
measures perform mediocrely on standard benchmarks. In this work, we develop a
new similarity measure, dubbed ContraSim, based on contrastive learning. In
contrast to common closed-form similarity measures, ContraSim learns a
parameterized measure by using both similar and dissimilar examples. We perform
an extensive experimental evaluation of our method, with both language and
vision models, on the standard layer prediction benchmark and two new
benchmarks that we introduce: the multilingual benchmark and the image-caption
benchmark. In all cases, ContraSim achieves much higher accuracy than previous
similarity measures, even when presented with challenging examples. Finally,
ContraSim is more suitable for the analysis of neural networks, revealing new
insights not captured by previous measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03682">
<div class="article-summary-box-inner">
<span><p>Coreference Resolution is a well studied problem in NLP. While widely studied
for English and other resource-rich languages, research on coreference
resolution in Bengali largely remains unexplored due to the absence of relevant
datasets. Bengali, being a low-resource language, exhibits greater
morphological richness compared to English. In this article, we introduce a new
dataset, BenCoref, comprising coreference annotations for Bengali texts
gathered from four distinct domains. This relatively small dataset contains
5200 mention annotations forming 502 mention clusters within 48,569 tokens. We
describe the process of creating this dataset and report performance of
multiple models trained using BenCoref. We anticipate that our work sheds some
light on the variations in coreference phenomena across multiple domains in
Bengali and encourages the development of additional resources for Bengali.
Furthermore, we found poor crosslingual performance at zero-shot setting from
English, highlighting the need for more language-specific resources for this
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06795">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel Token-and-Duration Transducer (TDT)
architecture for sequence-to-sequence tasks. TDT extends conventional
RNN-Transducer architectures by jointly predicting both a token and its
duration, i.e. the number of input frames covered by the emitted token. This is
achieved by using a joint network with two outputs which are independently
normalized to generate distributions over tokens and durations. During
inference, TDT models can skip input frames guided by the predicted duration
output, which makes them significantly faster than conventional Transducers
which process the encoder output frame by frame. TDT models achieve both better
accuracy and significantly faster inference than conventional Transducers on
different sequence transduction tasks. TDT models for Speech Recognition
achieve better accuracy and up to 2.82X faster inference than conventional
Transducers. TDT models for Speech Translation achieve an absolute gain of over
1 BLEU on the MUST-C test compared with conventional Transducers, and its
inference is 2.27X faster. In Speech Intent Classification and Slot Filling
tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over
conventional Transducers, while running up to 1.28X faster. Our implementation
of the TDT model will be open-sourced with the NeMo
(https://github.com/NVIDIA/NeMo) toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. (arXiv:2304.09116v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09116">
<div class="article-summary-box-inner">
<span><p>Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild
datasets is important to capture the diversity in human speech such as speaker
identities, prosodies, and styles (e.g., singing). Current large TTS systems
usually quantize speech into discrete tokens and use language models to
generate these tokens one by one, which suffer from unstable prosody, word
skipping/repeating issue, and poor voice quality. In this paper, we develop
NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual
vector quantizers to get the quantized latent vectors and uses a diffusion
model to generate these latent vectors conditioned on text input. To enhance
the zero-shot capability that is important to achieve diverse speech synthesis,
we design a speech prompting mechanism to facilitate in-context learning in the
diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to
large-scale datasets with 44K hours of speech and singing data and evaluate its
voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS
systems by a large margin in terms of prosody/timbre similarity, robustness,
and voice quality in a zero-shot setting, and performs novel zero-shot singing
synthesis with only a speech prompt. Audio samples are available at
https://speechresearch.github.io/naturalspeech2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00217">
<div class="article-summary-box-inner">
<span><p>In a recent paper published in the Journal of Language Evolution, Kauhanen,
Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the
results presented in one of my papers (Koplenig, Royal Society Open Science, 6,
181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show
through a series of statistical analyses that large numbers of L2 (second
language) speakers do not seem to affect the (grammatical or statistical)
complexity of a language. To this end, I focus on the way in which the
Ethnologue assesses language status: a language is characterised as vehicular
if, in addition to being used by L1 (first language) speakers, it should also
have a significant number of L2 users. KEW criticise both the use of
vehicularity as a (binary) indicator of whether a language has a significant
number of L2 users and the idea of imputing a zero proportion of L2 speakers to
non-vehicular languages whenever a direct estimate of that proportion is
unavailable. While I recognise the importance of post-publication commentary on
published research, I show in this rejoinder that both points of criticism are
explicitly mentioned and analysed in my paper. In addition, I also comment on
other points raised by KEW and demonstrate that both alternative analyses
offered by KEW do not stand up to closer scrutiny.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00586">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as "The war lasted from the year 1732 to the year 17", and predict valid
two-digit end years (years &gt; 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02637">
<div class="article-summary-box-inner">
<span><p>As pointed out by several scholars, current research on hate speech (HS)
recognition is characterized by unsystematic data creation strategies and
diverging annotation schemata. Subsequently, supervised-learning models tend to
generalize poorly to datasets they were not trained on, and the performance of
the models trained on datasets labeled using different HS taxonomies cannot be
compared. To ease this problem, we propose applying extremely weak supervision
that only relies on the class name rather than on class samples from the
annotated data. We demonstrate the effectiveness of a state-of-the-art
weakly-supervised text classification model in various in-dataset and
cross-dataset settings. Furthermore, we conduct an in-depth quantitative and
qualitative analysis of the source of poor generalizability of HS
classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03111">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL parsing, which aims at converting natural language instructions
into executable SQLs, has gained increasing attention in recent years. In
particular, Codex and ChatGPT have shown impressive results in this task.
However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on
database schema with few rows of database contents leaving the gap between
academic study and real-world applications. To mitigate this gap, we present
Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks,
containing 12,751 pairs of text-to-SQL data and 95 databases with a total size
of 33.4 GB, spanning 37 professional domains. Our emphasis on database values
highlights the new challenges of dirty database contents, external knowledge
between NL questions and database contents, and SQL efficiency, particularly in
the context of massive databases. To solve these problems, text-to-SQL models
must feature database value comprehension in addition to semantic parsing. The
experimental results demonstrate the significance of database values in
generating accurate text-to-SQLs for big databases. Furthermore, even the most
effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution
accuracy, which is still far from the human result of 92.96%, proving that
challenges still stand. Besides, we also provide an efficiency analysis to
offer insights into generating text-to-efficient-SQLs that are beneficial to
industries. We believe that BIRD will contribute to advancing real-world
applications of text-to-SQL research. The leaderboard and source code are
available: https://bird-bench.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation. (arXiv:2305.03602v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03602">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) is a realistic but challenging task that
requires an agent to locate the target region using verbal and visual cues.
While significant advancements have been achieved recently, there are still two
broad limitations: (1) The explicit information mining for significant guiding
semantics concealed in both vision and language is still under-explored; (2)
The previously structured map method provides the average historical appearance
of visited nodes, while it ignores distinctive contributions of various images
and potent information retention in the reasoning process. This work proposes a
dual semantic-aware recurrent global-adaptive network (DSRG) to address the
above problems. First, DSRG proposes an instruction-guidance linguistic module
(IGL) and an appearance-semantics visual module (ASV) for boosting vision and
language semantic learning respectively. For the memory mechanism, a global
adaptive aggregation module (GAA) is devised for explicit panoramic observation
fusion, and a recurrent memory fusion module (RMF) is introduced to supply
implicit temporal hidden states. Extensive experimental results on the R2R and
REVERIE datasets demonstrate that our method achieves better performance than
existing methods. Code is available at https://github.com/CrystalSixone/DSRG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts. (arXiv:2305.04928v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04928">
<div class="article-summary-box-inner">
<span><p>Supervised named entity recognition (NER) in the biomedical domain depends on
large sets of annotated texts with the given named entities. The creation of
such datasets can be time-consuming and expensive, while extraction of new
entities requires additional annotation tasks and retraining the model. To
address these challenges, this paper proposes a method for zero- and few-shot
NER in the biomedical domain. The method is based on transforming the task of
multi-class token classification into binary token classification and
pre-training on a large amount of datasets and biomedical entities, which allow
the model to learn semantic relations between the given and potentially novel
named entity labels. We have achieved average F1 scores of 35.44% for zero-shot
NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot
NER on 9 diverse evaluated biomedical entities with fine-tuned PubMedBERT-based
model. The results demonstrate the effectiveness of the proposed method for
recognizing new biomedical entities with no or limited number of examples,
outperforming previous transformer-based methods, and being comparable to
GPT3-based models using models with over 1000 times fewer parameters. We make
models and developed code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05402">
<div class="article-summary-box-inner">
<span><p>The categorization of massive e-Commerce data is a crucial, well-studied
task, which is prevalent in industrial settings. In this work, we aim to
improve an existing product categorization model that is already in use by a
major web company, serving multiple applications. At its core, the product
categorization model is a text classification model that takes a product title
as an input and outputs the most suitable category out of thousands of
available candidates. Upon a closer inspection, we found inconsistencies in the
labeling of similar items. For example, minor modifications of the product
title pertaining to colors or measurements majorly impacted the model's output.
This phenomenon can negatively affect downstream recommendation or search
applications, leading to a sub-optimal user experience.
</p>
<p>To address this issue, we propose a new framework for consistent text
categorization. Our goal is to improve the model's consistency while
maintaining its production-level performance. We use a semi-supervised approach
for data augmentation and presents two different methods for utilizing
unlabeled samples. One method relies directly on existing catalogs, while the
other uses a generative model. We compare the pros and cons of each approach
and present our experimental results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation. (arXiv:2305.07393v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07393">
<div class="article-summary-box-inner">
<span><p>Dialogue systems for non-English languages have long been under-explored. In
this paper, we take the first step to investigate few-shot cross-lingual
transfer learning (FS-XLT) and multitask learning (MTL) in the context of
open-domain dialogue generation for non-English languages with limited data. We
observed catastrophic forgetting in both FS-XLT and MTL for all 6 languages in
our preliminary experiments. To mitigate the issue, we propose a simple yet
effective prompt learning approach that can preserve the multilinguality of
multilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging
the gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and
our hand-crafted prompts. Experimental results on all 6 languages in terms of
both automatic and human evaluations demonstrate the effectiveness of our
approach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07490">
<div class="article-summary-box-inner">
<span><p>In recent years, large language models (LLMs) have made significant progress
in natural language processing (NLP), with models like ChatGPT and GPT-4
achieving impressive capabilities in various linguistic tasks. However,
training models on such a large scale is challenging, and finding datasets that
match the model's scale is often difficult. Fine-tuning and training models
with fewer parameters using novel methods have emerged as promising approaches
to overcome these challenges. One such model is MiniGPT-4, which achieves
comparable vision-language understanding to GPT-4 by leveraging novel
pre-training models and innovative training strategies. However, the model
still faces some challenges in image understanding, particularly in artistic
pictures. A novel multimodal model called ArtGPT-4 has been proposed to address
these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100
device in just 2 hours, using only about 200 GB of data. The model can depict
images with an artistic flair and generate visual code, including aesthetically
pleasing HTML/CSS web pages. Furthermore, the article proposes novel benchmarks
for evaluating the performance of vision-language models. In the subsequent
evaluation methods, ArtGPT-4 scored more than 1 point higher than the current
\textbf{state-of-the-art} model and was only 0.25 points lower than artists on
a 6-point scale. Our code and pre-trained model are available at
\url{https://huggingface.co/Tyrannosaurus/ArtGPT-4}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08455">
<div class="article-summary-box-inner">
<span><p>We call on the Document AI (DocAI) community to reevaluate current
methodologies and embrace the challenge of creating more practically-oriented
benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to
remediate the halted research progress in understanding visually-rich documents
(VRDs). We present a new dataset with novelties related to types of questions,
answers, and document layouts based on multi-industry, multi-domain, and
multi-page VRDs of various origins, and dates. Moreover, we are pushing the
boundaries of current methods by creating multi-task and multi-domain
evaluation setups that more accurately simulate real-world situations where
powerful generalization and adaptation under low-resource settings are desired.
DUDE aims to set a new standard as a more practical, long-standing benchmark
for the community, and we hope that it will lead to future extensions and
contributions that address real-world challenges. Finally, our work illustrates
the importance of finding more efficient ways to model language, images, and
layout in DocAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09941">
<div class="article-summary-box-inner">
<span><p>Transgender and non-binary (TGNB) individuals disproportionately experience
discrimination and exclusion from daily life. Given the recent popularity and
adoption of language generation technologies, the potential to further
marginalize this population only grows. Although a multitude of NLP fairness
literature focuses on illuminating and addressing gender biases, assessing
gender harms for TGNB identities requires understanding how such identities
uniquely interact with societal gender norms and how they differ from gender
binary-centric perspectives. Such measurement frameworks inherently require
centering TGNB voices to help guide the alignment between gender-inclusive NLP
and whom they are intended to serve. Towards this goal, we ground our work in
the TGNB community and existing interdisciplinary literature to assess how the
social reality surrounding experienced marginalization by TGNB persons
contributes to and persists within Open Language Generation (OLG). By first
understanding their marginalization stressors, we evaluate (1) misgendering and
(2) harmful responses to gender disclosure. To do this, we introduce the TANGO
dataset, comprising of template-based text curated from real-world text within
a TGNB-oriented community. We discover a dominance of binary gender norms
within the models; LLMs least misgendered subjects in generated text when
triggered by prompts whose subjects used binary pronouns. Meanwhile,
misgendering was most prevalent when triggering generation with singular they
and neopronouns. When prompted with gender disclosures, LLM text contained
stigmatizing language and scored most toxic when triggered by TGNB gender
disclosure. Our findings warrant further research on how TGNB harms manifest in
LLMs and serve as a broader case study toward concretely grounding the design
of gender-inclusive AI in community voices and interdisciplinary literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10036">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated powerful capabilities in both
text understanding and generation. Companies have begun to offer Embedding as a
Service (EaaS) based on these LLMs, which can benefit various natural language
processing (NLP) tasks for customers. However, previous studies have shown that
EaaS is vulnerable to model extraction attacks, which can cause significant
losses for the owners of LLMs, as training these models is extremely expensive.
To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark
method called EmbMarker that implants backdoors on embeddings. Our method
selects a group of moderate-frequency words from a general text corpus to form
a trigger set, then selects a target embedding as the watermark, and inserts it
into the embeddings of texts containing trigger words as the backdoor. The
weight of insertion is proportional to the number of trigger words included in
the text. This allows the watermark backdoor to be effectively transferred to
EaaS-stealer's model for copyright verification while minimizing the adverse
impact on the original embeddings' utility. Our extensive experiments on
various datasets show that our method can effectively protect the copyright of
EaaS models without compromising service quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11541">
<div class="article-summary-box-inner">
<span><p>Large Language Model (LLM) has gained popularity and achieved remarkable
results in open-domain tasks, but its performance in real industrial
domain-specific scenarios is average since there is no specific knowledge in
it. This issue has attracted widespread attention, but there are few relevant
benchmarks available. In this paper, we provide a benchmark Question Answering
(QA) dataset named MSQA, which is about Microsoft products and IT technical
problems encountered by customers. This dataset contains industry
cloud-specific QA knowledge, which is not available for general LLM, so it is
well suited for evaluating methods aimed at improving domain-specific
capabilities of LLM. In addition, we propose a new model interaction paradigm
that can empower LLM to achieve better performance on domain-specific tasks
where it is not proficient. Extensive experiments demonstrate that the approach
following our model fusion framework outperforms the commonly used LLM with
retrieval methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Forward Tuning Boosts In-context Learning in Language Models. (arXiv:2305.13016v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13016">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have exhibited an emergent in-context learning
(ICL) ability. However, the ICL models that can solve ordinary cases are hardly
extended to solve more complex tasks by processing the demonstration examples
once. This single-turn ICL is incoordinate with the decision making process of
humans by learning from analogy. In this paper, we propose an effective and
efficient two-stage framework to boost ICL in LLMs by exploiting a dual form
between Transformer attention and gradient descent-based optimization.
Concretely, we divide the ICL process into "Deep-Thinking" and inference
stages. The "Deep-Thinking" stage performs iterative forward optimization of
demonstrations, which is expected to boost the reasoning abilities of LLMs at
test time by "thinking" demonstrations multiple times. It produces accumulated
meta-gradients by manipulating the Key-Value matrices in the self-attention
modules of the Transformer. Then, the inference stage only takes the test query
as input without concatenating demonstrations and applies the learned
meta-gradients through attention for output prediction. In this way,
demonstrations are not required during the inference stage since they are
already learned and stored in the definitive meta-gradients. LLMs can be
effectively and efficiently adapted to downstream tasks. Extensive experiments
on ten classification and multiple-choice datasets show that our method
achieves substantially better performance than standard ICL in terms of both
accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13108">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition systems based on deep learning are mainly
trained under empirical risk minimization (ERM). Since ERM utilizes the
averaged performance on the data samples regardless of a group such as healthy
or dysarthric speakers, ASR systems are unaware of the performance disparities
across the groups. This results in biased ASR systems whose performance
differences among groups are severe. In this study, we aim to improve the ASR
system in terms of group robustness for dysarthric speakers. To achieve our
goal, we present a novel approach, sample reweighting with sample affinity test
(Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given
data sample and then mitigates the bias by debiasing helpfulness-based sample
reweighting. Experimental results demonstrate that Re-SAT contributes to
improved ASR performance on dysarthric speech without performance degradation
on healthy speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Fragile is Relation Extraction under Entity Replacements?. (arXiv:2305.13551v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13551">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) aims to extract the relations between entity names
from the textual context. In principle, textual context determines the
ground-truth relation and the RE models should be able to correctly identify
the relations reflected by the textual context. However, existing work has
found that the RE models memorize the entity name patterns to make RE
predictions while ignoring the textual context. This motivates us to raise the
question: ``are RE models robust to the entity replacements?'' In this work, we
operate the random and type-constrained entity replacements over the RE
instances in TACRED and evaluate the state-of-the-art RE models under the
entity replacements. We observe the 30\% - 50\% F1 score drops on the
state-of-the-art RE models under entity replacements. These results suggest
that we need more efforts to develop effective RE models robust to entity
replacements. We release the source code at
https://github.com/wangywUST/RobustRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR. (arXiv:2305.13716v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13716">
<div class="article-summary-box-inner">
<span><p>The recently proposed serialized output training (SOT) simplifies
multi-talker automatic speech recognition (ASR) by generating speaker
transcriptions separated by a special token. However, frequent speaker changes
can make speaker change prediction difficult. To address this, we propose
boundary-aware serialized output training (BA-SOT), which explicitly
incorporates boundary knowledge into the decoder via a speaker change detection
task and boundary constraint loss. We also introduce a two-stage connectionist
temporal classification (CTC) strategy that incorporates token-level SOT CTC to
restore temporal context information. Besides typical character error rate
(CER), we introduce utterance-dependent character error rate (UD-CER) to
further measure the precision of speaker change prediction. Compared to
original SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a
pre-trained ASR model for BA-SOT model initialization further reduces
CER/UD-CER by 8.4%/19.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14571">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art models for natural language understanding require a
preprocessing step to convert raw text into discrete tokens. This process known
as tokenization relies on a pre-built vocabulary of words or sub-word
morphemes. This fixed vocabulary limits the model's robustness to spelling
errors and its capacity to adapt to new domains. In this work, we introduce a
novel open-vocabulary language model that adopts a hierarchical two-level
approach: one at the word level and another at the sequence level. Concretely,
we design an intra-word module that uses a shallow Transformer architecture to
learn word representations from their characters, and a deep inter-word
Transformer module that contextualizes each word representation by attending to
the entire word sequence. Our model thus directly operates on character
sequences with explicit awareness of word boundaries, but without biased
sub-word or word-level vocabulary. Experiments on various downstream tasks show
that our method outperforms strong baselines. We also demonstrate that our
hierarchical model is robust to textual corruption and domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16264">
<div class="article-summary-box-inner">
<span><p>The current trend of scaling language models involves increasing both
parameter count and training dataset size. Extrapolating this trend suggests
that training dataset size may soon be limited by the amount of text data
available on the internet. Motivated by this limit, we investigate scaling
language models in data-constrained regimes. Specifically, we run a large set
of experiments varying the extent of data repetition and compute budget,
ranging up to 900 billion training tokens and 9 billion parameter models. We
find that with constrained data for a fixed compute budget, training with up to
4 epochs of repeated data yields negligible changes to loss compared to having
unique data. However, with more repetition, the value of adding compute
eventually decays to zero. We propose and empirically validate a scaling law
for compute optimality that accounts for the decreasing value of repeated
tokens and excess parameters. Finally, we experiment with approaches mitigating
data scarcity, including augmenting the training dataset with code data or
removing commonly used filters. Models and datasets from our 400 training runs
are freely available at https://github.com/huggingface/datablations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16756">
<div class="article-summary-box-inner">
<span><p>Accurate and rapid situation analysis during humanitarian crises is critical
to delivering humanitarian aid efficiently and is fundamental to humanitarian
imperatives and the Leave No One Behind (LNOB) principle. This data analysis
can highly benefit from language processing systems, e.g., by classifying the
text data according to a humanitarian ontology. However, approaching this by
simply fine-tuning a generic large language model (LLM) involves considerable
practical and ethical issues, particularly the lack of effectiveness on
data-sparse and complex subdomains, and the encoding of societal biases and
unwanted associations. In this work, we aim to provide an effective and
ethically-aware system for humanitarian data analysis. We approach this by (1)
introducing a novel architecture adjusted to the humanitarian analysis
framework, (2) creating and releasing a novel humanitarian-specific LLM called
HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our
experiments' results show the better performance of our approach on zero-shot
and full-training settings in comparison with strong baseline models, while
also revealing the existence of biases in the resulting LLMs. Utilizing a
targeted counterfactual data augmentation approach, we significantly reduce
these biases without compromising performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16806">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose
language models capable of addressing many natural language generation or
understanding tasks. On the task of Machine Translation (MT), multiple works
have investigated few-shot prompting mechanisms to elicit better translations
from LLMs. However, there has been relatively little investigation on how such
translations differ qualitatively from the translations generated by standard
Neural Machine Translation (NMT) models. In this work, we investigate these
differences in terms of the literalness of translations produced by the two
systems. Using literalness measures involving word alignment and monotonicity,
we find that translations out of English (E-X) from GPTs tend to be less
literal, while exhibiting similar or better scores on MT quality metrics. We
demonstrate that this finding is borne out in human evaluations as well. We
then show that these differences are especially pronounced when translating
sentences that contain idiomatic expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16809">
<div class="article-summary-box-inner">
<span><p>When caregivers ask open--ended questions to motivate dialogue with children,
it facilitates the child's reading comprehension skills.Although there is scope
for use of technological tools, referred here as "intelligent tutoring
systems", to scaffold this process, it is currently unclear whether existing
intelligent systems that generate human--language like questions is beneficial.
Additionally, training data used in the development of these automated question
generation systems is typically sourced without attention to demographics, but
people with different cultural backgrounds may ask different questions. As a
part of a broader project to design an intelligent reading support app for
Latinx children, we crowdsourced questions from Latinx caregivers and
noncaregivers as well as caregivers and noncaregivers from other demographics.
We examine variations in question--asking within this dataset mediated by
individual, cultural, and contextual factors. We then design a system that
automatically extracts templates from this data to generate open--ended
questions that are representative of those asked by Latinx caregivers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation. (arXiv:2305.16938v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16938">
<div class="article-summary-box-inner">
<span><p>Few-shot fine-tuning and in-context learning are two alternative strategies
for task adaptation of pre-trained language models. Recently, in-context
learning has gained popularity over fine-tuning due to its simplicity and
improved out-of-domain generalization, and because extensive evidence shows
that fine-tuned models pick up on spurious correlations. Unfortunately,
previous comparisons of the two approaches were done using models of different
sizes. This raises the question of whether the observed weaker out-of-domain
generalization of fine-tuned models is an inherent property of fine-tuning or a
limitation of the experimental setup. In this paper, we compare the
generalization of few-shot fine-tuning and in-context learning to challenge
datasets, while controlling for the models used, the number of examples, and
the number of parameters, ranging from 125M to 30B. Our results show that
fine-tuned language models can in fact generalize well out-of-domain. We find
that both approaches generalize similarly; they exhibit large variation and
depend on properties such as model size and the number of examples,
highlighting that robust task adaptation remains a challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information. (arXiv:2305.16967v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16967">
<div class="article-summary-box-inner">
<span><p>The long-standing one-to-many issue of the open-domain dialogues poses
significant challenges for automatic evaluation methods, i.e., there may be
multiple suitable responses which differ in semantics for a given
conversational context. To tackle this challenge, we propose a novel
learning-based automatic evaluation metric (CMN), which can robustly evaluate
open-domain dialogues by augmenting Conditional Variational Autoencoders
(CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual
Information (MI) to model the semantic similarity of text in the latent space.
Experimental results on two open-domain dialogue datasets demonstrate the
superiority of our method compared with a wide range of baselines, especially
in handling responses which are distant to the golden reference responses in
semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model. (arXiv:2305.17116v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17116">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have made significant advancements in natural
language processing (NLP). Broad corpora capture diverse patterns but can
introduce irrelevance, while focused corpora enhance reliability by reducing
misleading information. Training LLMs on focused corpora poses computational
challenges. An alternative approach is to use a retrieval-augmentation (RetA)
method tested in a specific domain.
</p>
<p>To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a
custom RetA model were compared using 19 questions on diffuse large B-cell
lymphoma (DLBCL) disease. Eight independent reviewers assessed responses based
on accuracy, relevance, and readability (rated 1-3).
</p>
<p>The RetA model performed best in accuracy (12/19 3-point scores, total=47)
and relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4
received the highest readability scores (17/19, 55), followed by GPT-3 (15/19,
53) and the RetA model (11/19, 47). Prometheus underperformed in accuracy (34),
relevance (32), and readability (38).
</p>
<p>Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared
to the RetA model and Prometheus. Hallucinations were mostly associated with
non-existent references or fabricated efficacy data.
</p>
<p>These findings suggest that RetA models, supplemented with domain-specific
corpora, may outperform general-purpose LLMs in accuracy and relevance within
specific domains. However, this evaluation was limited to specific questions
and metrics and may not capture challenges in semantic search and other NLP
tasks. Further research will explore different LLM architectures, RetA
methodologies, and evaluation methods to assess strengths and limitations more
comprehensively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection. (arXiv:2305.17480v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17480">
<div class="article-summary-box-inner">
<span><p>Hyperbole and metaphor are common in day-to-day communication (e.g., "I am in
deep trouble": how does trouble have depth?), which makes their detection
important, especially in a conversational AI setting. Existing approaches to
automatically detect metaphor and hyperbole have studied these language
phenomena independently, but their relationship has hardly, if ever, been
explored computationally. In this paper, we propose a multi-task deep learning
framework to detect hyperbole and metaphor simultaneously. We hypothesize that
metaphors help in hyperbole detection, and vice-versa. To test this hypothesis,
we annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels.
Simultaneously, we annotate two metaphor datasets- TroFi and LCC- with
hyperbole labels. Experiments using these datasets give an improvement of the
state of the art of hyperbole detection by 12%. Additionally, our multi-task
learning (MTL) approach shows an improvement of up to 17% over single-task
learning (STL) for both hyperbole and metaphor detection, supporting our
hypothesis. To the best of our knowledge, ours is the first demonstration of
computational leveraging of linguistic intimacy between metaphor and hyperbole,
leading to showing the superiority of MTL over STL for hyperbole and metaphor
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Children: Improving Image-Caption Pretraining via Curriculum. (arXiv:2305.17540v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17540">
<div class="article-summary-box-inner">
<span><p>Image-caption pretraining has been quite successfully used for downstream
vision tasks like zero-shot image classification and object detection. However,
image-caption pretraining is still a hard problem -- it requires multiple
concepts (nouns) from captions to be aligned to several objects in images. To
tackle this problem, we go to the roots -- the best learner, children. We take
inspiration from cognitive science studies dealing with children's language
learning to propose a curriculum learning framework. The learning begins with
easy-to-align image caption pairs containing one concept per caption. The
difficulty is progressively increased with each new phase by adding one more
concept per caption. Correspondingly, the knowledge acquired in each learning
phase is utilized in subsequent phases to effectively constrain the learning
problem to aligning one new concept-object pair in each phase. We show that
this learning strategy improves over vanilla image-caption training in various
settings -- pretraining from scratch, using a pretrained image or/and
pretrained text encoder, low data regime etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application. (arXiv:2305.17701v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17701">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) learn not only natural text generation abilities
but also social biases against different demographic groups from real-world
data. This poses a critical risk when deploying LLM-based applications.
Existing research and resources are not readily applicable in South Korea due
to the differences in language and culture, both of which significantly affect
the biases and targeted demographic groups. This limitation requires localized
social bias datasets to ensure the safe and effective deployment of LLMs. To
this end, we present KO SB I, a new social bias dataset of 34k pairs of
contexts and sentences in Korean covering 72 demographic groups in 15
categories. We find that through filtering-based moderation, social biases in
generated content can be reduced by 16.47%p on average for HyperCLOVA (30B and
82B), and GPT-3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Large Language Models Know What They Don't Know?. (arXiv:2305.18153v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18153">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have a wealth of knowledge that allows them to
excel in various Natural Language Processing (NLP) tasks. Current research
focuses on enhancing their performance within their existing knowledge. Despite
their vast knowledge, LLMs are still limited by the amount of information they
can accommodate and comprehend. Therefore, the ability to understand their own
limitations on the unknows, referred to as self-knowledge, is of paramount
importance. This study aims to evaluate LLMs' self-knowledge by assessing their
ability to identify unanswerable or unknowable questions. We introduce an
automated methodology to detect uncertainty in the responses of these models,
providing a novel measure of their self-knowledge. We further introduce a
unique dataset, SelfAware, consisting of unanswerable questions from five
diverse categories and their answerable counterparts. Our extensive analysis,
involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an
intrinsic capacity for self-knowledge within these models. Moreover, we
demonstrate that in-context learning and instruction tuning can further enhance
this self-knowledge. Despite this promising insight, our findings also
highlight a considerable gap between the capabilities of these models and human
proficiency in recognizing the limits of their knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09782">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) usesimage processing algorithms to process
the image and natural language processing methods to understand and answer the
question. VQA is helpful to a visually impaired person, can be used for the
security surveillance system and online chatbots that learn from the web. It
uses NLP methods to learn the semantic of the question and to derive the
textual features. Computer vision techniques are used for generating image
representation in such a way that they can identify the objects about which
question is asked. The Attention model tries to mimic the human behavior of
giving attention to a different region of an image according to our
understanding of its context. This paper critically examines and reviews
methods of VQA algorithm such as generation of semantics of text,
identification of objects and answer classification techniques that use the
co-attention approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15937">
<div class="article-summary-box-inner">
<span><p>We propose a visually grounded speech model that acquires new words and their
visual depictions from just a few word-image example pairs. Given a set of test
images and a spoken query, we ask the model which image depicts the query word.
Previous work has simplified this problem by either using an artificial setting
with digit word-image pairs or by using a large number of examples per class.
We propose an approach that can work on natural word-image pairs but with less
examples, i.e. fewer shots. Our approach involves using the given word-image
example pairs to mine new unsupervised word-image training pairs from large
collections of unlabelled speech and images. Additionally, we use a
word-to-image attention mechanism to determine word-image similarity. With this
new model, we achieve better performance with fewer shots than any existing
approach.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-31 23:10:57.900349536 UTC">2023-05-31 23:10:57 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>