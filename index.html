<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-14T01:30:00Z">11-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Casual Conversations v2: Designing a large consent-driven dataset to measure algorithmic bias and robustness. (arXiv:2211.05809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05809">
<div class="article-summary-box-inner">
<span><p>Developing robust and fair AI systems require datasets with comprehensive set
of labels that can help ensure the validity and legitimacy of relevant
measurements. Recent efforts, therefore, focus on collecting person-related
datasets that have carefully selected labels, including sensitive
characteristics, and consent forms in place to use those attributes for model
testing and development. Responsible data collection involves several stages,
including but not limited to determining use-case scenarios, selecting
categories (annotations) such that the data are fit for the purpose of
measuring algorithmic bias for subgroups and most importantly ensure that the
selected categories/subcategories are robust to regional diversities and
inclusive of as many subgroups as possible.
</p>
<p>Meta, in a continuation of our efforts to measure AI algorithmic bias and
robustness
(https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set),
is working on collecting a large consent-driven dataset with a comprehensive
list of categories. This paper describes our proposed design of such categories
and subcategories for Casual Conversations v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The CRINGE Loss: Learning what language not to model. (arXiv:2211.05826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05826">
<div class="article-summary-box-inner">
<span><p>Standard language model training employs gold human documents or human-human
interaction data, and treats all training data as positive examples. Growing
evidence shows that even with very large amounts of positive training data,
issues remain that can be alleviated with relatively small amounts of negative
data -- examples of what the model should not do. In this work, we propose a
novel procedure to train with such data called the CRINGE loss (ContRastive
Iterative Negative GEneration). We show the effectiveness of this approach
across three different experiments on the tasks of safe generation,
contradiction avoidance, and open-domain dialogue. Our models outperform
multiple strong baselines and are conceptually simple, easy to train and
implement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Climate Policy Tracker: Pipeline for automated analysis of public climate policies. (arXiv:2211.05852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05852">
<div class="article-summary-box-inner">
<span><p>The number of standardized policy documents regarding climate policy and
their publication frequency is significantly increasing. The documents are long
and tedious for manual analysis, especially for policy experts, lawmakers, and
citizens who lack access or domain expertise to utilize data analytics tools.
Potential consequences of such a situation include reduced citizen governance
and involvement in climate policies and an overall surge in analytics costs,
rendering less accessibility for the public. In this work, we use a Latent
Dirichlet Allocation-based pipeline for the automatic summarization and
analysis of 10-years of national energy and climate plans (NECPs) for the
period from 2021 to 2030, established by 27 Member States of the European
Union. We focus on analyzing policy framing, the language used to describe
specific issues, to detect essential nuances in the way governments frame their
climate policies and achieve climate goals. The methods leverage topic modeling
and clustering for the comparative analysis of policy documents across
different countries. It allows for easier integration in potential
user-friendly applications for the development of theories and processes of
climate policy. This would further lead to better citizen governance and
engagement over climate policies and public policy research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Reliability of Large Language Models through Semantic Consistency. (arXiv:2211.05853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05853">
<div class="article-summary-box-inner">
<span><p>While large pretrained language models (PLMs) demonstrate incredible fluency
and performance on many natural language tasks, recent work has shown that
well-performing PLMs are very sensitive to what prompts are feed into them.
Even when prompts are semantically identical, language models may give very
different answers. When considering safe and trustworthy deployments of PLMs we
would like their outputs to be consistent under prompts that mean the same
thing or convey the same intent. While some work has looked into how
state-of-the-art PLMs address this need, they have been limited to only
evaluating lexical equality of single- or multi-word answers and do not address
consistency of generative text sequences. In order to understand consistency of
PLMs under text generation settings, we develop a measure of semantic
consistency that allows the comparison of open-ended text outputs. We implement
several versions of this consistency metric to evaluate the performance of a
number of PLMs on paraphrased versions of questions in the TruthfulQA dataset,
we find that our proposed metrics are considerably more consistent than
traditional metrics embodying lexical consistency, and also correlate with
human evaluation of output consistency to a higher degree.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding. (arXiv:2211.05869v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05869">
<div class="article-summary-box-inner">
<span><p>Collecting sufficient labeled data for spoken language understanding (SLU) is
expensive and time-consuming. Recent studies achieved promising results by
using pre-trained models in low-resource scenarios. Inspired by this, we aim to
ask: which (if any) pre-training strategies can improve performance across SLU
benchmarks? To answer this question, we employ four types of pre-trained models
and their combinations for SLU. We leverage self-supervised speech and language
models (LM) pre-trained on large quantities of unpaired data to extract strong
speech and text representations. We also explore using supervised models
pre-trained on larger external automatic speech recognition (ASR) or SLU
corpora. We conduct extensive experiments on the SLU Evaluation (SLUE)
benchmark and observe self-supervised pre-trained models to be more powerful,
with pre-trained LM and speech models being most beneficial for the Sentiment
Analysis and Named Entity Recognition task, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREATIVESUMM: Shared Task on Automatic Summarization for Creative Writing. (arXiv:2211.05886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05886">
<div class="article-summary-box-inner">
<span><p>This paper introduces the shared task of summarizing documents in several
creative domains, namely literary texts, movie scripts, and television scripts.
Summarizing these creative documents requires making complex literary
interpretations, as well as understanding non-trivial temporal dependencies in
texts containing varied styles of plot development and narrative structure.
This poses unique challenges and is yet underexplored for text summarization
systems. In this shared task, we introduce four sub-tasks and their
corresponding datasets, focusing on summarizing books, movie scripts, primetime
television scripts, and daytime soap opera scripts. We detail the process of
curating these datasets for the task, as well as the metrics used for the
evaluation of the submissions. As part of the CREATIVESUMM workshop at COLING
2022, the shared task attracted 18 submissions in total. We discuss the
submissions and the baselines for each sub-task in this paper, along with
directions for facilitating future work in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense. (arXiv:2211.05895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05895">
<div class="article-summary-box-inner">
<span><p>Visual commonsense understanding requires Vision Language (VL) models to not
only understand image and text but also cross-reference in-between to fully
integrate and achieve comprehension of the visual scene described. Recently,
various approaches have been developed and have achieved high performance on
visual commonsense benchmarks. However, it is unclear whether the models really
understand the visual scene and underlying commonsense knowledge due to limited
evaluation data resources. To provide an in-depth analysis, we present a
Multimodal Evaluation (ME) pipeline to automatically generate question-answer
pairs to test models' understanding of the visual scene, text, and related
knowledge. We then take a step further to show that training with the ME data
boosts the model's performance in standard VCR evaluation. Lastly, our in-depth
analysis and comparison reveal interesting findings: (1) semantically low-level
information can assist the learning of high-level information but not the
opposite; (2) visual information is generally under utilization compared with
text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breadth-First Pipeline Parallelism. (arXiv:2211.05953v1 [cs.DC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05953">
<div class="article-summary-box-inner">
<span><p>We introduce Breadth-First Pipeline Parallelism, a novel training schedule
which optimizes the combination of pipeline and data parallelism. Breadth-First
Pipeline Parallelism lowers training time, cost and memory usage by combining a
high GPU utilization with a small batch size per GPU, and by making use of
fully sharded data parallelism. Experimentally, we observed increases of up to
53% in training speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEE: A Novel Multilingual Event Extraction Dataset. (arXiv:2211.05955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05955">
<div class="article-summary-box-inner">
<span><p>Event Extraction (EE) is one of the fundamental tasks in Information
Extraction (IE) that aims to recognize event mentions and their arguments
(i.e., participants) from text. Due to its importance, extensive methods and
resources have been developed for Event Extraction. However, one limitation of
current research for EE involves the under-exploration for non-English
languages in which the lack of high-quality multilingual EE datasets for model
training and evaluation has been the main hindrance. To address this
limitation, we propose a novel Multilingual Event Extraction dataset (MEE) that
provides annotation for more than 50K event mentions in 8 typologically
different languages. MEE comprehensively annotates data for entity mentions,
event triggers and event arguments. We conduct extensive experiments on the
proposed dataset to reveal challenges and opportunities for multilingual EE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINION: a Large-Scale and Diverse Dataset for Multilingual Event Detection. (arXiv:2211.05958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05958">
<div class="article-summary-box-inner">
<span><p>Event Detection (ED) is the task of identifying and classifying trigger words
of event mentions in text. Despite considerable research efforts in recent
years for English text, the task of ED in other languages has been
significantly less explored. Switching to non-English languages, important
research questions for ED include how well existing ED models perform on
different languages, how challenging ED is in other languages, and how well ED
knowledge and annotation can be transferred across languages. To answer those
questions, it is crucial to obtain multilingual ED datasets that provide
consistent event annotation for multiple languages. There exist some
multilingual ED datasets; however, they tend to cover a handful of languages
and mainly focus on popular ones. Many languages are not covered in existing
multilingual ED datasets. In addition, the current datasets are often small and
not accessible to the public. To overcome those shortcomings, we introduce a
new large-scale multilingual dataset for ED (called MINION) that consistently
annotates events for 8 different languages; 5 of them have not been supported
by existing multilingual datasets. We also perform extensive experiments and
analysis to demonstrate the challenges and transferability of ED across
languages in MINION that in all call for more research effort in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Align, Write, Re-order: Explainable End-to-End Speech Translation via Operation Sequence Generation. (arXiv:2211.05967v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05967">
<div class="article-summary-box-inner">
<span><p>The black-box nature of end-to-end speech translation (E2E ST) systems makes
it difficult to understand how source language inputs are being mapped to the
target language. To solve this problem, we would like to simultaneously
generate automatic speech recognition (ASR) and ST predictions such that each
source language word is explicitly mapped to a target language word. A major
challenge arises from the fact that translation is a non-monotonic sequence
transduction task due to word ordering differences between languages -- this
clashes with the monotonic nature of ASR. Therefore, we propose to generate ST
tokens out-of-order while remembering how to re-order them later. We achieve
this by predicting a sequence of tuples consisting of a source word, the
corresponding target words, and post-editing operations dictating the correct
insertion points for the target word. We examine two variants of such operation
sequences which enable generation of monotonic transcriptions and non-monotonic
translations from the same speech input simultaneously. We apply our approach
to offline and real-time streaming models, demonstrating that we can provide
explainable translations without sacrificing quality or latency. In fact, the
delayed re-ordering ability of our approach improves performance during
streaming. As an added benefit, our method performs ASR and ST simultaneously,
making it faster than using two separate systems to perform these tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios. (arXiv:2211.05980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05980">
<div class="article-summary-box-inner">
<span><p>Domain adaptation is an effective solution to data scarcity in low-resource
scenarios. However, when applied to token-level tasks such as bioNER, domain
adaptation methods often suffer from the challenging linguistic characteristics
that clinical narratives possess, which leads to unsatisfactory performance. In
this paper, we present a simple yet effective hardness-guided domain adaptation
(HGDA) framework for bioNER tasks that can effectively leverage the domain
hardness information to improve the adaptability of the learnt model in
low-resource scenarios. Experimental results on biomedical datasets show that
our model can achieve significant performance improvement over the recently
published state-of-the-art (SOTA) MetaNER model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Getting the Most out of Simile Recognition. (arXiv:2211.05984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05984">
<div class="article-summary-box-inner">
<span><p>Simile recognition involves two subtasks: simile sentence classification that
discriminates whether a sentence contains simile, and simile component
extraction that locates the corresponding objects (i.e., tenors and vehicles).
Recent work ignores features other than surface strings. In this paper, we
explore expressive features for this task to achieve more effective data
utilization. Particularly, we study two types of features: 1) input-side
features that include POS tags, dependency trees and word definitions, and 2)
decoding features that capture the interdependence among various decoding
decisions. We further construct a model named HGSR, which merges the input-side
features as a heterogeneous graph and leverages decoding features via
distillation. Experiments show that HGSR significantly outperforms the current
state-of-the-art systems and carefully designed baselines, verifying the
effectiveness of introduced features. Our code is available at
https://github.com/DeepLearnXMU/HGSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinformation Detection using Persuasive Writing Strategies. (arXiv:2211.05985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05985">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation is a prominent problem in today's society, and
many researchers in academia and industry are trying to combat it. Due to the
vast amount of misinformation that is created every day, it is unrealistic to
leave this task to human fact-checkers. Data scientists and researchers have
been working on automated misinformation detection for years, and it is still a
challenging problem today. The goal of our research is to add a new level to
automated misinformation detection; classifying segments of text with
persuasive writing techniques in order to produce interpretable reasoning for
why an article can be marked as misinformation. To accomplish this, we present
a novel annotation scheme containing many common persuasive writing tactics,
along with a dataset with human annotations accordingly. For this task, we make
use of a RoBERTa model for text classification, due to its high performance in
NLP. We develop several language model-based baselines and present the results
of our persuasive strategy label predictions as well as the improvements these
intermediate labels make in detecting misinformation and producing
interpretable results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCPrompt: Counterfactual Contrastive Prompt-Tuning for Many-Class Classification. (arXiv:2211.05987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05987">
<div class="article-summary-box-inner">
<span><p>With the success of the prompt-tuning paradigm in Natural Language Processing
(NLP), various prompt templates have been proposed to further stimulate
specific knowledge for serving downstream tasks, e.g., machine translation,
text generation, relation extraction, and so on. Existing prompt templates are
mainly shared among all training samples with the information of task
description. However, training samples are quite diverse. The sharing task
description is unable to stimulate the unique task-related information in each
training sample, especially for tasks with the finite-label space. To exploit
the unique task-related information, we imitate the human decision process
which aims to find the contrastive attributes between the objective factual and
their potential counterfactuals. Thus, we propose the \textbf{C}ounterfactual
\textbf{C}ontrastive \textbf{Prompt}-Tuning (CCPrompt) approach for many-class
classification, e.g., relation classification, topic classification, and entity
typing. Compared with simple classification tasks, these tasks have more
complex finite-label spaces and are more rigorous for prompts. First of all, we
prune the finite label space to construct fact-counterfactual pairs. Then, we
exploit the contrastive attributes by projecting training instances onto every
fact-counterfactual pair. We further set up global prototypes corresponding
with all contrastive attributes for selecting valid contrastive attributes as
additional tokens in the prompt template. Finally, a simple Siamese
representation learning is employed to enhance the robustness of the model. We
conduct experiments on relation classification, topic classification, and
entity typing tasks in both fully supervised setting and few-shot setting. The
results indicate that our model outperforms former baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge-Enhanced Pre-trained Language Models. (arXiv:2211.05994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05994">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PLMs) which are trained on large text corpus
through the self-supervised learning method, have yielded promising performance
on various tasks in Natural Language Processing (NLP). However, though PLMs
with huge parameters can effectively possess rich knowledge learned from
massive training text and benefit downstream tasks at the fine-tuning stage,
they still have some limitations such as poor reasoning ability due to the lack
of external knowledge. Incorporating knowledge into PLMs has been tried to
tackle these issues. In this paper, we present a comprehensive review of
Knowledge-Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear
insight into this thriving field. We introduce appropriate taxonomies
respectively for Natural Language Understanding (NLU) and Natural Language
Generation (NLG) to highlight the focus of these two kinds of tasks. For NLU,
we take several types of knowledge into account and divide them into four
categories: linguistic knowledge, text knowledge, knowledge graph (KG), and
rule knowledge. The KE-PLMs for NLG are categorized into KG-based and
retrieval-based methods. Finally, we point out some promising future directions
of KE-PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-game Toxic Language Detection: Shared Task and Attention Residuals. (arXiv:2211.05995v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05995">
<div class="article-summary-box-inner">
<span><p>In-game toxic language becomes the hot potato in the gaming industry and
community. There have been several online game toxicity analysis frameworks and
models proposed. However, it is still challenging to detect toxicity due to the
nature of in-game chat, which has extremely short length. In this paper, we
describe how the in-game toxic language shared task has been established using
the real-world in-game chat data. In addition, we propose and introduce the
model/framework for toxic language token tagging (slot filling) from the
in-game chat. The data and code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction. (arXiv:2211.06014v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06014">
<div class="article-summary-box-inner">
<span><p>Information Extraction (IE) aims to extract structured information from
heterogeneous sources. IE from natural language texts include sub-tasks such as
Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction
(EE). Most IE systems require comprehensive understandings of sentence
structure, implied semantics, and domain knowledge to perform well; thus, IE
tasks always need adequate external resources and annotations. However, it
takes time and effort to obtain more human annotations. Low-Resource
Information Extraction (LRIE) strives to use unsupervised data, reducing the
required resources and human annotation. In practice, existing systems either
utilize self-training schemes to generate pseudo labels that will cause the
gradual drift problem, or leverage consistency regularization methods which
inevitably possess confirmation bias. To alleviate confirmation bias due to the
lack of feedback loops in existing LRIE learning paradigms, we develop a
Gradient Imitation Reinforcement Learning (GIRL) method to encourage
pseudo-labeled data to imitate the gradient descent direction on labeled data,
which can force pseudo-labeled data to achieve better optimization capabilities
similar to labeled data. Based on how well the pseudo-labeled data imitates the
instructive gradient descent direction obtained from labeled data, we design a
reward to quantify the imitation process and bootstrap the optimization
capability of pseudo-labeled data through trial and error. In addition to
learning paradigms, GIRL is not limited to specific sub-tasks, and we leverage
GIRL to solve all IE sub-tasks (named entity recognition, relation extraction,
and event extraction) in low-resource settings (semi-supervised IE and few-shot
IE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoRAL: a Context-aware Croatian Abusive Language Dataset. (arXiv:2211.06053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06053">
<div class="article-summary-box-inner">
<span><p>In light of unprecedented increases in the popularity of the internet and
social media, comment moderation has never been a more relevant task.
Semi-automated comment moderation systems greatly aid human moderators by
either automatically classifying the examples or allowing the moderators to
prioritize which comments to consider first. However, the concept of
inappropriate content is often subjective, and such content can be conveyed in
many subtle and indirect ways. In this work, we propose CoRAL -- a language and
culturally aware Croatian Abusive dataset covering phenomena of implicitness
and reliance on local and global context. We show experimentally that current
models degrade when comments are not explicit and further degrade when language
skill and context knowledge are required to interpret the comment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SceneFake: An Initial Dataset and Benchmarks for Scene Fake Audio Detection. (arXiv:2211.06073v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06073">
<div class="article-summary-box-inner">
<span><p>Previous databases have been designed to further the development of fake
audio detection. However, fake utterances are mostly generated by altering
timbre, prosody, linguistic content or channel noise of original audios. They
ignore a fake situation, in which the attacker manipulates an acoustic scene of
the original audio with another forgery one. It will pose a major threat to our
society if some people misuse the manipulated audio with malicious purpose.
Therefore, this motivates us to fill in the gap. This paper designs such a
dataset for scene fake audio detection (SceneFake). A manipulated audio in the
SceneFake dataset involves only tampering the acoustic scene of an utterance by
using speech enhancement technologies. We can not only detect fake utterances
on a seen test set but also evaluate the generalization of fake detection
models to unseen manipulation attacks. Some benchmark results are described on
the SceneFake dataset. Besides, an analysis of fake attacks with different
speech enhancement technologies and signal-to-noise ratios are presented on the
dataset. The results show that scene manipulated utterances can not be detected
reliably by the existing baseline models of ASVspoof 2019. Furthermore, the
detection of unseen scene manipulation audio is still challenging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Helping the Weak Makes You Strong: Simple Multi-Task Learning Improves Non-Autoregressive Translators. (arXiv:2211.06075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06075">
<div class="article-summary-box-inner">
<span><p>Recently, non-autoregressive (NAR) neural machine translation models have
received increasing attention due to their efficient parallel decoding.
However, the probabilistic framework of NAR models necessitates conditional
independence assumption on target sequences, falling short of characterizing
human language data. This drawback results in less informative learning signals
for NAR models under conventional MLE training, thereby yielding unsatisfactory
accuracy compared to their autoregressive (AR) counterparts. In this paper, we
propose a simple and model-agnostic multi-task learning framework to provide
more informative learning signals. During training stage, we introduce a set of
sufficiently weak AR decoders that solely rely on the information provided by
NAR decoder to make prediction, forcing the NAR decoder to become stronger or
else it will be unable to support its weak AR partners. Experiments on WMT and
IWSLT datasets show that our approach can consistently improve accuracy of
multiple NAR baselines without adding any additional decoding overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards automating Numerical Consistency Checks in Financial Reports. (arXiv:2211.06112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06112">
<div class="article-summary-box-inner">
<span><p>We introduce KPI-Check, a novel system that automatically identifies and
cross-checks semantically equivalent key performance indicators (KPIs), e.g.
"revenue" or "total costs", in real-world German financial reports. It combines
a financial named entity and relation extraction module with a BERT-based
filtering and text pair classification component to extract KPIs from
unstructured sentences before linking them to synonymous occurrences in the
balance sheet and profit &amp; loss statement. The tool achieves a high matching
performance of $73.00$% micro F$_1$ on a hold out test set and is currently
being deployed for a globally operating major auditing firm to assist the
auditing procedure of financial statements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Much Hate with #china? A Preliminary Analysis on China-related Hateful Tweets Two Years After the Covid Pandemic Began. (arXiv:2211.06116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06116">
<div class="article-summary-box-inner">
<span><p>Following the outbreak of a global pandemic, online content is filled with
hate speech. Donald Trump's ''Chinese Virus'' tweet shifted the blame for the
spread of the Covid-19 virus to China and the Chinese people, which triggered a
new round of anti-China hate both online and offline. This research intends to
examine China-related hate speech on Twitter during the two years following the
burst of the pandemic (2020 and 2021). Through Twitter's API, in total
2,172,333 tweets hashtagged #china posted during the time were collected. By
employing multiple state-of-the-art pretrained language models for hate speech
detection, we identify a wide range of hate of various types, resulting in an
automatically labeled anti-China hate speech dataset. We identify a hateful
rate in #china tweets of 2.5% in 2020 and 1.9% in 2021. This is well above the
average rate of online hate speech on Twitter at 0.6% identified in Gao et al.,
2017. We further analyzed the longitudinal development of #china tweets and
those identified as hateful in 2020 and 2021 through visualizing the daily
number and hate rate over the two years. Our keyword analysis of hate speech in
#china tweets reveals the most frequently mentioned terms in the hateful #china
tweets, which can be used for further social science studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings. (arXiv:2211.06127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06127">
<div class="article-summary-box-inner">
<span><p>Universal cross-lingual sentence embeddings map semantically similar
cross-lingual sentences into a shared embedding space. Aligning cross-lingual
sentence embeddings usually requires supervised cross-lingual parallel
sentences. In this work, we propose mSimCSE, which extends SimCSE to
multilingual settings and reveal that contrastive learning on English data can
surprisingly learn high-quality universal cross-lingual sentence embeddings
without any parallel data. In unsupervised and weakly supervised settings,
mSimCSE significantly improves previous sentence embedding methods on
cross-lingual retrieval and multilingual STS tasks. The performance of
unsupervised mSimCSE is comparable to fully supervised methods in retrieving
low-resource languages and multilingual STS. The performance can be further
enhanced when cross-lingual NLI data is available. Our code is publicly
available at https://github.com/yaushian/mSimCSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unimodal and Multimodal Representation Training for Relation Extraction. (arXiv:2211.06168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06168">
<div class="article-summary-box-inner">
<span><p>Multimodal integration of text, layout and visual information has achieved
SOTA results in visually rich document understanding (VrDU) tasks, including
relation extraction (RE). However, despite its importance, evaluation of the
relative predictive capacity of these modalities is less prevalent. Here, we
demonstrate the value of shared representations for RE tasks by conducting
experiments in which each data type is iteratively excluded during training. In
addition, text and layout data are evaluated in isolation. While a bimodal text
and layout approach performs best (F1=0.684), we show that text is the most
important single predictor of entity relations. Additionally, layout geometry
is highly predictive and may even be a feasible unimodal approach. Despite
being less effective, we highlight circumstances where visual information can
bolster performance. In total, our results demonstrate the efficacy of training
joint representations for RE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocuT5: Seq2seq SQL Generation with Table Documentation. (arXiv:2211.06193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06193">
<div class="article-summary-box-inner">
<span><p>Current SQL generators based on pre-trained language models struggle to
answer complex questions requiring domain context or understanding fine-grained
table structure. Humans would deal with these unknowns by reasoning over the
documentation of the tables. Based on this hypothesis, we propose DocuT5, which
uses off-the-shelf language model architecture and injects knowledge from
external `documentation' to improve domain generalization. We perform
experiments on the Spider family of datasets that contain complex questions
that are cross-domain and multi-table. Specifically, we develop a new
text-to-SQL failure taxonomy and find that 19.6% of errors are due to foreign
key mistakes, and 49.2% are due to a lack of domain knowledge. We proposed
DocuT5, a method that captures knowledge from (1) table structure context of
foreign keys and (2) domain knowledge through contextualizing tables and
columns. Both types of knowledge improve over state-of-the-art T5 with
constrained decoding on Spider, and domain knowledge produces state-of-the-art
comparable effectiveness on Spider-DK and Spider-SYN datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Factual Consistency in Summarization with Compression-Based Post-Editing. (arXiv:2211.06196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06196">
<div class="article-summary-box-inner">
<span><p>State-of-the-art summarization models still struggle to be factually
consistent with the input text. A model-agnostic way to address this problem is
post-editing the generated summaries. However, existing approaches typically
fail to remove entity errors if a suitable input entity replacement is not
available or may insert erroneous content. In our work, we focus on removing
extrinsic entity errors, or entities not in the source, to improve consistency
while retaining the summary's essential information and form. We propose to use
sentence-compression data to train the post-editing model to take a summary
with extrinsic entity errors marked with special tokens and output a
compressed, well-formed summary with those errors removed. We show that this
model improves factual consistency while maintaining ROUGE, improving entity
precision by up to 30% on XSum, and that this model can be applied on top of
another post-editor, improving entity precision by up to a total of 38%. We
perform an extensive comparison of post-editing approaches that demonstrate
trade-offs between factual consistency, informativeness, and grammaticality,
and we analyze settings where post-editors show the largest improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving word mover's distance by leveraging self-attention matrix. (arXiv:2211.06229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06229">
<div class="article-summary-box-inner">
<span><p>Measuring the semantic similarity between two sentences is still an important
task. The word mover's distance (WMD) computes the similarity via the optimal
alignment between the sets of word embeddings. However, WMD does not utilize
word order, making it difficult to distinguish sentences with large overlaps of
similar words, even if they are semantically very different. Here, we attempt
to improve WMD by incorporating the sentence structure represented by BERT's
self-attention matrix (SAM). The proposed method is based on the Fused
Gromov-Wasserstein distance, which simultaneously considers the similarity of
the word embedding and the SAM for calculating the optimal transport between
two sentences. Experiments on paraphrase identification and semantic textual
similarity show that the proposed method improves WMD and its variants. Our
code is available at https://github.com/ymgw55/WSMD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A hybrid entity-centric approach to Persian pronoun resolution. (arXiv:2211.06257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06257">
<div class="article-summary-box-inner">
<span><p>Pronoun resolution is a challenging subset of an essential field in natural
language processing called coreference resolution. Coreference resolution is
about finding all entities in the text that refers to the same real-world
entity. This paper presents a hybrid model combining multiple rulebased sieves
with a machine-learning sieve for pronouns. For this purpose, seven
high-precision rule-based sieves are designed for the Persian language. Then, a
random forest classifier links pronouns to the previous partial clusters. The
presented method demonstrates exemplary performance using pipeline design and
combining the advantages of machine learning and rulebased methods. This method
has solved some challenges in end-to-end models. In this paper, the authors
develop a Persian coreference corpus called Mehr in the form of 400 documents.
This corpus fixes some weaknesses of the previous corpora in the Persian
language. Finally, the efficiency of the presented system compared to the
earlier model in Persian is reported by evaluating the proposed method on the
Mehr and Uppsala test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Developer Discussions to Guide Fixing Bugs in Software. (arXiv:2211.06335v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06335">
<div class="article-summary-box-inner">
<span><p>Automatically fixing software bugs is a challenging task. While recent work
showed that natural language context is useful in guiding bug-fixing models,
the approach required prompting developers to provide this context, which was
simulated through commit messages written after the bug-fixing code changes
were made. We instead propose using bug report discussions, which are available
before the task is performed and are also naturally occurring, avoiding the
need for any additional information from developers. For this, we augment
standard bug-fixing datasets with bug report discussions. Using these newly
compiled datasets, we demonstrate that various forms of natural language
context derived from such discussions can aid bug-fixing, even leading to
improved performance over using commit messages corresponding to the oracle
bug-fixing commits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Male and Female Speakers' Word Choices in Public Speeches. (arXiv:2211.06366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06366">
<div class="article-summary-box-inner">
<span><p>The extent to which men and women use language differently has been
questioned previously. Finding clear and consistent gender differences in
language is not conclusive in general, and the research is heavily influenced
by the context and method employed to identify the difference. In addition, the
majority of the research was conducted in written form, and the sample was
collected in writing. Therefore, we compared the word choices of male and
female presenters in public addresses such as TED lectures. The frequency of
numerous types of words, such as parts of speech (POS), linguistic,
psychological, and cognitive terms were analyzed statistically to determine how
male and female speakers use words differently. Based on our data, we
determined that male speakers use specific types of linguistic, psychological,
cognitive, and social words in considerably greater frequency than female
speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach. (arXiv:2211.06398v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06398">
<div class="article-summary-box-inner">
<span><p>Double-blind peer review mechanism has become the skeleton of academic
research across multiple disciplines including computer science, yet several
studies have questioned the quality of peer reviews and raised concerns on
potential biases in the process. In this paper, we conduct a thorough and
rigorous study on fairness disparities in peer review with the help of large
language models (LMs). We collect, assemble, and maintain a comprehensive
relational database for the International Conference on Learning
Representations (ICLR) conference from 2017 to date by aggregating data from
OpenReview, Google Scholar, arXiv, and CSRanking, and extracting high-level
features using language models. We postulate and study fairness disparities on
multiple protective attributes of interest, including author gender, geography,
author, and institutional prestige. We observe that the level of disparity
differs and textual features are essential in reducing biases in the predictive
modeling. We distill several insights from our analysis on study the peer
review process with the help of large LMs. Our database also provides avenues
for studying new natural language processing (NLP) methods that facilitate the
understanding of the peer review mechanism. We study a concrete example towards
automatic machine review systems and provide baseline models for the review
generation and scoring tasks such that the database can be used as a benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Federated Approach to Predicting Emojis in Hindi Tweets. (arXiv:2211.06401v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06401">
<div class="article-summary-box-inner">
<span><p>The use of emojis affords a visual modality to, often private, textual
communication. The task of predicting emojis however provides a challenge for
machine learning as emoji use tends to cluster into the frequently used and the
rarely used emojis. Much of the machine learning research on emoji use has
focused on high resource languages and has conceptualised the task of
predicting emojis around traditional server-side machine learning approaches.
However, traditional machine learning approaches for private communication can
introduce privacy concerns, as these approaches require all data to be
transmitted to a central storage. In this paper, we seek to address the dual
concerns of emphasising high resource languages for emoji prediction and
risking the privacy of people's data. We introduce a new dataset of $118$k
tweets (augmented from $25$k unique tweets) for emoji prediction in Hindi, and
propose a modification to the federated learning algorithm, CausalFedGSD, which
aims to strike a balance between model performance and user privacy. We show
that our approach obtains comparative scores with more complex centralised
models while reducing the amount of data required to optimise the models and
minimising risks to user privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Architectural Bottleneck Principle. (arXiv:2211.06420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06420">
<div class="article-summary-box-inner">
<span><p>In this paper, we seek to measure how much information a component in a
neural network could extract from the representations fed into it. Our work
stands in contrast to prior probing work, most of which investigates how much
information a model's representations contain. This shift in perspective leads
us to propose a new principle for probing, the architectural bottleneck
principle: In order to estimate how much information a given component could
extract, a probe should look exactly like the component. Relying on this
principle, we estimate how much syntactic information is available to
transformers through our attentional probe, a probe that exactly resembles a
transformer's self-attention head. Experimentally, we find that, in three
models (BERT, ALBERT, and RoBERTa), a sentence's syntax tree is mostly
extractable by our probe, suggesting these models have access to syntactic
information while composing their contextual representations. Whether this
information is actually used by these models, however, remains an open
question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis. (arXiv:2104.06835v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06835">
<div class="article-summary-box-inner">
<span><p>Exploiting rich linguistic information in raw text is crucial for expressive
text-to-speech (TTS). As large scale pre-trained text representation develops,
bidirectional encoder representations from Transformers (BERT) has been proven
to embody semantic information and employed to TTS recently. However, original
or simply fine-tuned BERT embeddings still cannot provide sufficient semantic
knowledge that expressive TTS models should take into account. In this paper,
we propose a word-level semantic representation enhancing method based on
dependency structure and pre-trained BERT embedding. The BERT embedding of each
word is reprocessed considering its specific dependencies and related words in
the sentence, to generate more effective semantic representation for TTS. To
better utilize the dependency structure, relational gated graph network (RGGN)
is introduced to make semantic information flow and aggregate through the
dependency structure. The experimental results show that the proposed method
can further improve the naturalness and expressiveness of synthesized speeches
on both Mandarin and English datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialoging Resonance: How Users Perceive, Reciprocate and React to Chatbot's Self-Disclosure in Conversational Recommendations. (arXiv:2106.01666v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01666">
<div class="article-summary-box-inner">
<span><p>Using chatbots to deliver recommendations is increasingly popular. The design
of recommendation chatbots has primarily been taking an information-centric
approach by focusing on the recommended content per se. Limited attention is on
how social connection and relational strategies, such as self-disclosure from a
chatbot, may influence users' perception and acceptance of the recommendation.
In this work, we designed, implemented, and evaluated a social chatbot capable
of performing three different levels of self-disclosure: factual information
(low), cognitive opinions (medium), and emotions (high). In the evaluation, we
recruited 372 participants to converse with the chatbot on two topics: movies
and COVID-19 experiences. In each topic, the chatbot performed small talks and
made recommendations relevant to the topic. Participants were randomly assigned
to four experimental conditions where the chatbot used factual, cognitive,
emotional, and adaptive strategies to perform self-disclosures. By training a
text classifier to identify users' level of self-disclosure in real-time, the
adaptive chatbot can dynamically match its self-disclosure to the level of
disclosure exhibited by the users. Our results show that users reciprocate with
higher-level self-disclosure when a recommendation chatbot consistently
displays emotions throughout the conversation. Chatbot's emotional disclosure
also led to increased interactional enjoyment and more positive interpersonal
perception towards the bot, fostering a stronger human-chatbot relationship and
thus leading to increased recommendation effectiveness, including a higher
tendency to accept the recommendation. We discuss the understandings obtained
and implications to future design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regression Transformer: Concurrent sequence regression and generation for molecular language modeling. (arXiv:2202.01338v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01338">
<div class="article-summary-box-inner">
<span><p>Despite significant progress of generative models in the natural sciences,
their controllability remains challenging. One fundamentally missing aspect of
molecular or protein generative models is an inductive bias that can reflect
continuous properties of interest. To that end, we propose the Regression
Transformer (RT), a novel method that abstracts regression as a conditional
sequence modeling problem. This introduces a new paradigm of multitask language
models which seamlessly bridge sequence regression and conditional sequence
generation.
</p>
<p>We thoroughly demonstrate that, despite using a nominal-scale training
objective, the RT matches or surpasses the performance of conventional
regression models in property prediction tasks of small molecules, proteins and
chemical reactions. Critically, priming the same model with continuous
properties yields a highly competitive conditional generative model that
outperforms specialized approaches in a substructure-constrained,
property-driven molecule generation benchmark. Our dichotomous approach is
facilitated by a novel, alternating training scheme that enables the model to
decorate seed sequences by desired properties, e.g., to optimize reaction
yield.
</p>
<p>In sum, the RT is the first report of a multitask model that concurrently
excels at predictive and generative tasks in biochemistry. This finds
particular application in property-driven, local exploration of the chemical or
protein space and could pave the road toward foundation models in material
design.
</p>
<p>The code to reproduce all experiments of the paper is available at:
https://github.com/IBM/regression-transformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines. (arXiv:2204.13746v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13746">
<div class="article-summary-box-inner">
<span><p>Convincing people to get vaccinated against COVID-19 is a key societal
challenge in the present times. As a first step towards this goal, many prior
works have relied on social media analysis to understand the specific concerns
that people have towards these vaccines, such as potential side-effects,
ineffectiveness, political factors, and so on. Though there are datasets that
broadly classify social media posts into Anti-vax and Pro-Vax labels, there is
no dataset (to our knowledge) that labels social media posts according to the
specific anti-vaccine concerns mentioned in the posts. In this paper, we have
curated CAVES, the first large-scale dataset containing about 10k COVID-19
anti-vaccine tweets labelled into various specific anti-vaccine concerns in a
multi-label setting. This is also the first multi-label classification dataset
that provides explanations for each of the labels. Additionally, the dataset
also provides class-wise summaries of all the tweets. We also perform
preliminary experiments on the dataset and show that this is a very challenging
dataset for multi-label explainable classification and tweet summarization, as
is evident by the moderate scores achieved by some state-of-the-art models. Our
dataset and codes are available at: https://github.com/sohampoddar26/caves-data
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generalist Agent. (arXiv:2205.06175v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06175">
<div class="article-summary-box-inner">
<span><p>Inspired by progress in large-scale language modeling, we apply a similar
approach towards building a single generalist agent beyond the realm of text
outputs. The agent, which we refer to as Gato, works as a multi-modal,
multi-task, multi-embodiment generalist policy. The same network with the same
weights can play Atari, caption images, chat, stack blocks with a real robot
arm and much more, deciding based on its context whether to output text, joint
torques, button presses, or other tokens. In this report we describe the model
and the data, and document the current capabilities of Gato.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifiers are Better Experts for Controllable Text Generation. (arXiv:2205.07276v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07276">
<div class="article-summary-box-inner">
<span><p>This paper proposes a simple method for controllable text generation based on
weighting logits with a free-form classifier, namely CAIF sampling. Using an
arbitrary text classifier, we adjust a small part of a language model's logits
and guide text generation towards or away from classifier prediction. We
experimented with toxicity avoidance and sentiment control tasks and showed
that the proposed method significantly outperforms recent PPLM, GeDi, and
DExperts on PPL and task accuracy metrics based on the external classifier of
generated texts. In addition, compared to other approaches, it is easier to
implement and tune and has significantly fewer restrictions and requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation. (arXiv:2209.05451v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05451">
<div class="article-summary-box-inner">
<span><p>Transformers have revolutionized vision and natural language processing with
their ability to scale with large datasets. But in robotic manipulation, data
is both limited and expensive. Can manipulation still benefit from Transformers
with the right problem formulation? We investigate this question with PerAct, a
language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.
PerAct encodes language goals and RGB-D voxel observations with a Perceiver
Transformer, and outputs discretized actions by ``detecting the next best voxel
action''. Unlike frameworks that operate on 2D images, the voxelized 3D
observation and action space provides a strong structural prior for efficiently
learning 6-DoF actions. With this formulation, we train a single multi-task
Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks
(with 18 variations) from just a few demonstrations per task. Our results show
that PerAct significantly outperforms unstructured image-to-action agents and
3D ConvNet baselines for a wide range of tabletop tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Multi-Task Learning for Abstractive Text Summarization. (arXiv:2210.14606v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14606">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of multi-task learning and pre-finetuning for
natural language understanding, few works have studied the effects of task
families on abstractive text summarization. Task families are a form of task
grouping during the pre-finetuning stage to learn common skills, such as
reading comprehension. To close this gap, we analyze the influence of
multi-task learning strategies using task families for the English abstractive
text summarization task. We group tasks into one of three strategies, i.e.,
sequential, simultaneous, and continual multi-task learning, and evaluate
trained models through two downstream tasks. We find that certain combinations
of task families (e.g., advanced reading comprehension and natural language
inference) positively impact downstream performance. Further, we find that
choice and combinations of task families influence downstream performance more
than the training scheme, supporting the use of task families for abstractive
text summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Text Classification Data and Models Using Aggregated Input Salience. (arXiv:2211.05485v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05485">
<div class="article-summary-box-inner">
<span><p>Realizing when a model is right for a wrong reason is not trivial and
requires a significant effort by model developers. In some cases, an input
salience method, which highlights the most important parts of the input, may
reveal problematic reasoning. But scrutinizing highlights over many data
instances is tedious and often infeasible. Furthermore, analyzing examples in
isolation does not reveal general patterns in the data or in the model's
behavior. In this paper we aim to address these issues and go from
understanding single examples to understanding entire datasets and models. The
methodology we propose is based on aggregated salience maps. Using this
methodology we address multiple distinct but common model developer needs by
showing how problematic data and model behavior can be identified -- a
necessary first step for improving the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking. (arXiv:2211.05503v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05503">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) aims to convert the dialogue history into
dialogue states which consist of slot-value pairs. As condensed structural
information memorizing all history information, the dialogue state in the last
turn is typically adopted as the input for predicting the current state by DST
models. However, these models tend to keep the predicted slot values unchanged,
which is defined as state momentum in this paper. Specifically, the models
struggle to update slot values that need to be changed and correct wrongly
predicted slot values in the last turn. To this end, we propose MoNET to tackle
state momentum via noise-enhanced training. First, the previous state of each
turn in the training data is noised via replacing some of its slot values.
Then, the noised previous state is used as the input to learn to predict the
current state, improving the model's ability to update and correct slot values.
Furthermore, a contrastive context matching framework is designed to narrow the
representation distance between a state and its corresponding noised variant,
which reduces the impact of noised state and makes the model better understand
the dialogue history. Experimental results on MultiWOZ datasets show that MoNET
outperforms previous DST methods. Ablations and analysis verify the
effectiveness of MoNET in alleviating state momentum and improving anti-noise
ability.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-14 23:14:53.928377629 UTC">2022-11-14 23:14:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>