<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-08T01:30:00Z">09-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03219">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) embedding has been used to benefit the diagnosis of
animal diseases by analyzing electronic medical records (EMRs), such as notes
and veterinary records. However, learning representations to capture entities
and relations with literal information in KGs is challenging as the KGs show
heterogeneous properties and various types of literal information. Meanwhile,
the existing methods mostly aim to preserve graph structures surrounding target
nodes without considering different types of literals, which could also carry
significant information. In this paper, we propose a knowledge graph embedding
model for the efficient diagnosis of animal diseases, which could learn various
types of literal information and graph structure and fuse them into unified
representations, namely LiteralKG. Specifically, we construct a knowledge graph
that is built from EMRs along with literal information collected from various
animal hospitals. We then fuse different types of entities and node feature
information into unified vector representations through gate networks. Finally,
we propose a self-supervised learning task to learn graph structure in pretext
tasks and then towards various downstream tasks. Experimental results on link
prediction tasks demonstrate that our model outperforms the baselines that
consist of state-of-the-art models. The source code is available at
https://github.com/NSLab-CUK/LiteralKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach. (arXiv:2309.03223v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03223">
<div class="article-summary-box-inner">
<span><p>One of the most common things that a genealogist is tasked with is the
gathering of a person's initial family history, normally via in-person
interviews or with the use of a platform such as ancestry.com, as this can
provide a strong foundation upon which a genealogist may build. However, the
ability to conduct these interviews can often be hindered by both geographical
constraints and the technical proficiency of the interviewee, as the
interviewee in these types of interviews is most often an elderly person with a
lower than average level of technical proficiency. With this in mind, this
study presents what we believe, based on prior research, to be the first
chatbot geared entirely towards the gathering of family histories, and explores
the viability of utilising such a chatbot by comparing the performance and
usability of such a method with the aforementioned alternatives. With a
chatbot-based approach, we show that, though the average time taken to conduct
an interview may be longer than if the user had used ancestry.com or
participated in an in-person interview, the number of mistakes made and the
level of confusion from the user regarding the UI and process required is lower
than the other two methods. Note that the final metric regarding the user's
confusion is not applicable for the in-person interview sessions due to its
lack of a UI. With refinement, we believe this use of a chatbot could be a
valuable tool for genealogists, especially when dealing with interviewees who
are based in other countries where it is not possible to conduct an in-person
interview.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03227">
<div class="article-summary-box-inner">
<span><p>Drug repositioning-a promising strategy for discovering new therapeutic uses
for existing drugs-has been increasingly explored in the computational science
literature using biomedical databases. However, the technological potential of
drug repositioning candidates has often been overlooked. This study presents a
novel protocol to comprehensively analyse various sources such as
pharmaceutical patents and biomedical databases, and identify drug
repositioning candidates with both technological potential and scientific
evidence. To this end, first, we constructed a scientific biomedical knowledge
graph (s-BKG) comprising relationships between drugs, diseases, and genes
derived from biomedical databases. Our protocol involves identifying drugs that
exhibit limited association with the target disease but are closely located in
the s-BKG, as potential drug candidates. We constructed a patent-informed
biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information.
Finally, we developed a graph embedding protocol to ascertain the structure of
the p-BKG, thereby calculating the relevance scores of those candidates with
target disease-related patents to evaluate their technological potential. Our
case study on Alzheimer's disease demonstrates its efficacy and feasibility,
while the quantitative outcomes and systematic methods are expected to bridge
the gap between computational discoveries and successful market applications in
drug repositioning research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation. (arXiv:2309.03238v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03238">
<div class="article-summary-box-inner">
<span><p>Emotion recognition is a complex task due to the inherent subjectivity in
both the perception and production of emotions. The subjectivity of emotions
poses significant challenges in developing accurate and robust computational
models. This thesis examines critical facets of emotion recognition, beginning
with the collection of diverse datasets that account for psychological factors
in emotion production.
</p>
<p>To handle the challenge of non-representative training data, this work
collects the Multimodal Stressed Emotion dataset, which introduces controlled
stressors during data collection to better represent real-world influences on
emotion production. To address issues with label subjectivity, this research
comprehensively analyzes how data augmentation techniques and annotation
schemes impact emotion perception and annotator labels. It further handles
natural confounding variables and variations by employing adversarial networks
to isolate key factors like stress from learned emotion representations during
model training. For tackling concerns about leakage of sensitive demographic
variables, this work leverages adversarial learning to strip sensitive
demographic information from multimodal encodings. Additionally, it proposes
optimized sociological evaluation metrics aligned with cost-effective,
real-world needs for model testing.
</p>
<p>This research advances robust, practical emotion recognition through
multifaceted studies of challenges in datasets, labels, modeling, demographic
and membership variable encoding in representations, and evaluation. The
groundwork has been laid for cost-effective, generalizable emotion recognition
models that are less likely to encode sensitive demographic information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03241">
<div class="article-summary-box-inner">
<span><p>Previous studies have typically assumed that large language models are unable
to accurately perform arithmetic operations, particularly multiplication of &gt;8
digits, and operations involving decimals and fractions, without the use of
calculator tools. This paper aims to challenge this misconception. With
sufficient training data, a 2 billion-parameter language model can accurately
perform multi-digit arithmetic operations with almost 100% accuracy without
data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication
accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from
GLM-10B on a dataset with additional multi-step arithmetic operations and math
problems described in text, achieves similar performance to GPT-4 on a
5,000-samples Chinese math problem test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation. (arXiv:2309.03340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03340">
<div class="article-summary-box-inner">
<span><p>There has been significant research on developing pretrained transformer
architectures for multimodal-to-text generation tasks. Albeit performance
improvements, such models are frequently overparameterized, hence suffer from
hallucination and large memory footprint making them challenging to deploy on
edge devices. In this paper, we address both these issues for the application
of automated audio captioning. First, we propose a data augmentation technique
for generating hallucinated audio captions and show that similarity based on an
audio-text shared latent space is suitable for detecting hallucination. Then,
we propose a parameter efficient inference time faithful decoding algorithm
that enables smaller audio captioning models with performance equivalent to
larger models trained with more data. During the beam decoding step, the
smaller model utilizes an audio-text shared latent representation to
semantically align the generated text with corresponding input audio. Faithful
guidance is introduced into the beam probability by incorporating the cosine
similarity between latent representation projections of greedy rolled out
intermediate beams and audio clip. We show the efficacy of our algorithm on
benchmark datasets and evaluate the proposed scheme against baselines using
conventional audio captioning and semantic similarity metrics while
illustrating tradeoffs between performance and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoDia: A New Dataset for Romanian Dialect Identification from Speech. (arXiv:2309.03378v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03378">
<div class="article-summary-box-inner">
<span><p>Dialect identification is a critical task in speech processing and language
technology, enhancing various applications such as speech recognition, speaker
verification, and many others. While most research studies have been dedicated
to dialect identification in widely spoken languages, limited attention has
been given to dialect identification in low-resource languages, such as
Romanian. To address this research gap, we introduce RoDia, the first dataset
for Romanian dialect identification from speech. The RoDia dataset includes a
varied compilation of speech samples from five distinct regions of Romania,
covering both urban and rural environments, totaling 2 hours of manually
annotated speech data. Along with our dataset, we introduce a set of
competitive models to be used as baselines for future research. The top scoring
model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%,
indicating that the task is challenging. We thus believe that RoDia is a
valuable resource that will stimulate research aiming to address the challenges
of Romanian dialect identification. We publicly release our dataset and code at
https://github.com/codrut2/RoDia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03409">
<div class="article-summary-box-inner">
<span><p>Optimization is ubiquitous. While derivative-based algorithms have been
powerful tools for various problems, the absence of gradient imposes challenges
on many real-world applications. In this work, we propose Optimization by
PROmpting (OPRO), a simple and effective approach to leverage large language
models (LLMs) as optimizers, where the optimization task is described in
natural language. In each optimization step, the LLM generates new solutions
from the prompt that contains previously generated solutions with their values,
then the new solutions are evaluated and added to the prompt for the next
optimization step. We first showcase OPRO on linear regression and traveling
salesman problems, then move on to prompt optimization where the goal is to
find instructions that maximize the task accuracy. With a variety of LLMs, we
demonstrate that the best prompts optimized by OPRO outperform human-designed
prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models. (arXiv:2309.03412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03412">
<div class="article-summary-box-inner">
<span><p>Instruction tuning is essential for large language models (LLMs) to become
interactive. While many instruction tuning datasets exist in English, there is
a noticeable lack in other languages. Also, their effectiveness has not been
well verified in non-English languages. We construct a Japanese instruction
dataset by expanding and filtering existing datasets and apply the dataset to a
Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning
on both Japanese and English existing models using our instruction dataset. We
evaluated these models from both quantitative and qualitative perspectives. As
a result, the effectiveness of Japanese instruction datasets is confirmed. The
results also indicate that even with relatively small LLMs, performances in
downstream tasks would be improved through instruction tuning. Our instruction
dataset, tuned models, and implementation are publicly available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty. (arXiv:2309.03433v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03433">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OIE) task aims at extracting structured facts
from unstructured text, typically in the form of (subject, relation, object)
triples. Despite the potential of large language models (LLMs) like ChatGPT as
a general task solver, they lag behind state-of-the-art (supervised) methods in
OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant
context from relevant relations and generate structured output due to the
restrictions on fine-tuning the model. Second, LLMs generates responses
autoregressively based on probability, which makes the predicted relations lack
confidence. In this paper, we assess the capabilities of LLMs in improving the
OIE task. Particularly, we propose various in-context learning strategies to
enhance LLM's instruction-following ability and a demonstration uncertainty
quantification module to enhance the confidence of the generated relations. Our
experiments on three OIE benchmark datasets show that our approach holds its
own against established supervised methods, both quantitatively and
qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03450">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have become ubiquitous across various domains,
transforming the way we interact with information and conduct research.
However, most high-performing LLMs remain confined behind proprietary walls,
hindering scientific progress. Most open-source LLMs, on the other hand, are
limited in their ability to support longer sequence lengths, which is a key
requirement for many tasks that require inference over an input context. To
address this, we have trained XGen, a series of 7B parameter models on up to 8K
sequence length for up to 1.5T tokens. We have also finetuned the XGen models
on public-domain instructional data, creating their instruction-tuned
counterparts (XGen-Inst). We open-source our models for both research
advancements and commercial applications. Our evaluation on standard benchmarks
shows that XGen models achieve comparable or better results when compared with
state-of-the-art open-source LLMs. Our targeted evaluation on long sequence
modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence
open-source LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning for Tangible Effects: Natural Language Processing for Uncovering the Illicit Massage Industry & Computer Vision for Tactile Sensing. (arXiv:2309.03470v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03470">
<div class="article-summary-box-inner">
<span><p>I explore two questions in this thesis: how can computer science be used to
fight human trafficking? And how can computer vision create a sense of touch?
</p>
<p>I use natural language processing (NLP) to monitor the United States illicit
massage industry (IMI), a multi-billion dollar industry that offers not just
therapeutic massages but also commercial sexual services. Employees of this
industry are often immigrant women with few job opportunities, leaving them
vulnerable to fraud, coercion, and other facets of human trafficking.
Monitoring spatiotemporal trends helps prevent trafficking in the IMI. By
creating datasets with three publicly-accessible websites: Google Places,
Rubmaps, and AMPReviews, combined with NLP techniques such as bag-of-words and
Word2Vec, I show how to derive insights into the labor pressures and language
barriers that employees face, as well as the income, demographics, and societal
pressures affecting sex buyers. I include a call-to-action to other researchers
given these datasets. I also consider how to creating synthetic financial data,
which can aid with counter-trafficking in the banking sector. I use an
agent-based model to create both tabular and payee-recipient graph data.
</p>
<p>I then consider the role of computer vision in making tactile sensors. I
report on a novel sensor, the Digger Finger, that adapts the Gelsight sensor to
finding objects in granular media. Changes include using a wedge shape to
facilitate digging, replacing the internal lighting LEDs with fluorescent
paint, and adding a vibrator motor to counteract jamming. Finally, I also show
how to use a webcam and a printed reference marker, or fiducial, to create a
low-cost six-axis force-torque sensor. This sensor is up to a hundred times
less expensive than commercial sensors, allowing for a wider range of
applications. For this and earlier chapters I release design files and code as
open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Anchor Learning Approach for Citation Field Learning. (arXiv:2309.03559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03559">
<div class="article-summary-box-inner">
<span><p>Citation field learning is to segment a citation string into fields of
interest such as author, title, and venue. Extracting such fields from
citations is crucial for citation indexing, researcher profile analysis, etc.
User-generated resources like academic homepages and Curriculum Vitae, provide
rich citation field information. However, extracting fields from these
resources is challenging due to inconsistent citation styles, incomplete
sentence syntax, and insufficient training data. To address these challenges,
we propose a novel algorithm, CIFAL (citation field learning by anchor
learning), to boost the citation field learning performance. CIFAL leverages
the anchor learning, which is model-agnostic for any Pre-trained Language
Model, to help capture citation patterns from the data of different citation
styles. The experiments demonstrate that CIFAL outperforms state-of-the-art
methods in citation field learning, achieving a 2.83% improvement in
field-level F1-scores. Extensive analysis of the results further confirms the
effectiveness of CIFAL quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm. (arXiv:2309.03563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03563">
<div class="article-summary-box-inner">
<span><p>In intent detection tasks, leveraging meaningful semantic information from
intent labels can be particularly beneficial for few-shot scenarios. However,
existing few-shot intent detection methods either ignore the intent labels,
(e.g. treating intents as indices) or do not fully utilize this information
(e.g. only using part of the intent labels). In this work, we present an
end-to-end One-to-All system that enables the comparison of an input utterance
with all label candidates. The system can then fully utilize label semantics in
this way. Experiments on three few-shot intent detection tasks demonstrate that
One-to-All is especially effective when the training resource is extremely
scarce, achieving state-of-the-art performance in 1-, 3- and 5-shot settings.
Moreover, we present a novel pretraining strategy for our model that utilizes
indirect supervision from paraphrasing, enabling zero-shot cross-domain
generalization on intent detection tasks. Our code is at
https://github.com/jiangshdd/AllLablesTogethe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03564">
<div class="article-summary-box-inner">
<span><p>Large language models, particularly those akin to the rapidly progressing GPT
series, are gaining traction for their expansive influence. While there is keen
interest in their applicability within medical domains such as psychology,
tangible explorations on real-world data remain scant. Concurrently, users on
social media platforms are increasingly vocalizing personal sentiments; under
specific thematic umbrellas, these sentiments often manifest as negative
emotions, sometimes escalating to suicidal inclinations. Timely discernment of
such cognitive distortions and suicidal risks is crucial to effectively
intervene and potentially avert dire circumstances. Our study ventured into
this realm by experimenting on two pivotal tasks: suicidal risk and cognitive
distortion identification on Chinese social media platforms. Using supervised
learning as a baseline, we examined and contrasted the efficacy of large
language models via three distinct strategies: zero-shot, few-shot, and
fine-tuning. Our findings revealed a discernible performance gap between the
large language models and traditional supervised learning approaches, primarily
attributed to the models' inability to fully grasp subtle categories. Notably,
while GPT-4 outperforms its counterparts in multiple scenarios, GPT-3.5 shows
significant enhancement in suicide risk classification after fine-tuning. To
our knowledge, this investigation stands as the maiden attempt at gauging large
language models on Chinese social media tasks. This study underscores the
forward-looking and transformative implications of using large language models
in the field of psychology. It lays the groundwork for future applications in
psychological research and practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Loquacity and Visible Emotion: ChatGPT as a Policy Advisor. (arXiv:2309.03595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03595">
<div class="article-summary-box-inner">
<span><p>ChatGPT, a software seeking to simulate human conversational abilities, is
attracting increasing attention. It is sometimes portrayed as a groundbreaking
productivity aid, including for creative work. In this paper, we run an
experiment to assess its potential in complex writing tasks. We ask the
software to compose a policy brief for the Board of the Bank of Italy. We find
that ChatGPT can accelerate workflows by providing well-structured content
suggestions, and by producing extensive, linguistically correct text in a
matter of seconds. It does, however, require a significant amount of expert
supervision, which partially offsets productivity gains. If the app is used
naively, output can be incorrect, superficial, or irrelevant. Superficiality is
an especially problematic limitation in the context of policy advice intended
for high-level audiences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03613">
<div class="article-summary-box-inner">
<span><p>Recent popularity surrounds large AI language models due to their impressive
natural language capabilities. They contribute significantly to
language-related tasks, including prompt-based learning, making them valuable
for various specific tasks. This approach unlocks their full potential,
enhancing precision and generalization. Research communities are actively
exploring their applications, with ChatGPT receiving recognition. Despite
extensive research on large language models, their potential in recommendation
scenarios still needs to be explored. This study aims to fill this gap by
investigating ChatGPT's capabilities as a zero-shot recommender system. Our
goals include evaluating its ability to use user preferences for
recommendations, reordering existing recommendation lists, leveraging
information from similar users, and handling cold-start situations. We assess
ChatGPT's performance through comprehensive experiments using three datasets
(MovieLens Small, Last.FM, and Facebook Book). We compare ChatGPT's performance
against standard recommendation algorithms and other large language models,
such as GPT-3.5 and PaLM-2. To measure recommendation effectiveness, we employ
widely-used evaluation metrics like Mean Average Precision (MAP), Recall,
Precision, F1, normalized Discounted Cumulative Gain (nDCG), Item Coverage,
Expected Popularity Complement (EPC), Average Coverage of Long Tail (ACLT),
Average Recommendation Popularity (ARP), and Popularity-based Ranking-based
Equal Opportunity (PopREO). Through thoroughly exploring ChatGPT's abilities in
recommender systems, our study aims to contribute to the growing body of
research on the versatility and potential applications of large language
models. Our experiment code is available on the GitHub repository:
https://github.com/sisinflab/Recommender-ChatGPT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BNS-Net: A Dual-channel Sarcasm Detection Method Considering Behavior-level and Sentence-level Conflicts. (arXiv:2309.03658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03658">
<div class="article-summary-box-inner">
<span><p>Sarcasm detection is a binary classification task that aims to determine
whether a given utterance is sarcastic. Over the past decade, sarcasm detection
has evolved from classical pattern recognition to deep learning approaches,
where features such as user profile, punctuation and sentiment words have been
commonly employed for sarcasm detection. In real-life sarcastic expressions,
behaviors without explicit sentimental cues often serve as carriers of implicit
sentimental meanings. Motivated by this observation, we proposed a dual-channel
sarcasm detection model named BNS-Net. The model considers behavior and
sentence conflicts in two channels. Channel 1: Behavior-level Conflict Channel
reconstructs the text based on core verbs while leveraging the modified
attention mechanism to highlight conflict information. Channel 2:
Sentence-level Conflict Channel introduces external sentiment knowledge to
segment the text into explicit and implicit sentences, capturing conflicts
between them. To validate the effectiveness of BNS-Net, several comparative and
ablation experiments are conducted on three public sarcasm datasets. The
analysis and evaluation of experimental results demonstrate that the BNS-Net
effectively identifies sarcasm in text and achieves the state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring an LM to generate Prolog Predicates from Mathematics Questions. (arXiv:2309.03667v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03667">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a surge in interest in NLP driven by ChatGPT.
ChatGPT, a transformer-based generative language model of substantial scale,
exhibits versatility in performing various tasks based on natural language.
Nevertheless, large language models often exhibit poor performance in solving
mathematics questions that require reasoning. Prior research has demonstrated
the effectiveness of chain-of-thought prompting in enhancing reasoning
capabilities. Now, we aim to investigate whether fine-tuning a model for the
generation of Prolog codes, a logic language, and subsequently passing these
codes to a compiler can further improve accuracy. Consequently, we employ
chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other
fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code +
chain-of-thought, and chain-of-thought + Prolog code, respectively. The results
reveal that the Prolog generation model surpasses the baseline in performance,
while the combination generation models do not yield significant improvements.
The Prolog corpus based on GSM8K and the correspondingly finetuned Prolog
generation model based on LLaMA7B are released to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word segmentation granularity in Korean. (arXiv:2309.03713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03713">
<div class="article-summary-box-inner">
<span><p>This paper describes word {segmentation} granularity in Korean language
processing. From a word separated by blank space, which is termed an eojeol, to
a sequence of morphemes in Korean, there are multiple possible levels of word
segmentation granularity in Korean. For specific language processing and corpus
annotation tasks, several different granularity levels have been proposed and
utilized, because the agglutinative languages including Korean language have a
one-to-one mapping between functional morpheme and syntactic category. Thus, we
analyze these different granularity levels, presenting the examples of Korean
language processing systems for future reference. Interestingly, the
granularity by separating only functional morphemes including case markers and
verbal endings, and keeping other suffixes for morphological derivation results
in the optimal performance for phrase structure parsing. This contradicts
previous best practices for Korean language processing, which has been the de
facto standard for various applications that require separating all morphemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties. (arXiv:2309.03747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03747">
<div class="article-summary-box-inner">
<span><p>In this paper, we adopted a retrospective approach to examine and compare
five existing popular sentence encoders, i.e., Sentence-BERT, Universal
Sentence Encoder (USE), LASER, InferSent, and Doc2vec, in terms of their
performance on downstream tasks versus their capability to capture basic
semantic properties. Initially, we evaluated all five sentence encoders on the
popular SentEval benchmark and found that multiple sentence encoders perform
quite well on a variety of popular downstream tasks. However, being unable to
find a single winner in all cases, we designed further experiments to gain a
deeper understanding of their behavior. Specifically, we proposed four semantic
evaluation criteria, i.e., Paraphrasing, Synonym Replacement, Antonym
Replacement, and Sentence Jumbling, and evaluated the same five sentence
encoders using these criteria. We found that the Sentence-Bert and USE models
pass the paraphrasing criterion, with SBERT being the superior between the two.
LASER dominates in the case of the synonym replacement criterion.
Interestingly, all the sentence encoders failed the antonym replacement and
jumbling criteria. These results suggest that although these popular sentence
encoders perform quite well on the SentEval benchmark, they still struggle to
capture some basic semantic properties, thus, posing a daunting dilemma in NLP
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03748">
<div class="article-summary-box-inner">
<span><p>The latest advancements in AI and deep learning have led to a breakthrough in
large language model (LLM)-based agents such as GPT-4. However, many commercial
conversational agent development tools are pipeline-based and have limitations
in holding a human-like conversation. This paper investigates the capabilities
of LLMs to enhance pipeline-based conversational agents during two phases: 1)
in the design and development phase and 2) during operations. In 1) LLMs can
aid in generating training data, extracting entities and synonyms,
localization, and persona design. In 2) LLMs can assist in contextualization,
intent classification to prevent conversational breakdown and handle
out-of-scope questions, auto-correcting utterances, rephrasing responses,
formulating disambiguation questions, summarization, and enabling closed
question-answering capabilities. We conducted informal experiments with GPT-4
in the private banking domain to demonstrate the scenarios above with a
practical example. Companies may be hesitant to replace their pipeline-based
agents with LLMs entirely due to privacy concerns and the need for deep
integration within their existing ecosystems. A hybrid approach in which LLMs'
are integrated into the pipeline-based agents allows them to save time and
costs of building and running agents by capitalizing on the capabilities of
LLMs while retaining the integration and privacy safeguards of their existing
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">USA: Universal Sentiment Analysis Model & Construction of Japanese Sentiment Text Classification and Part of Speech Dataset. (arXiv:2309.03787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03787">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is a pivotal task in the domain of natural language
processing. It encompasses both text-level sentiment polarity classification
and word-level Part of Speech(POS) sentiment polarity determination. Such
analysis challenges models to understand text holistically while also
extracting nuanced information. With the rise of Large Language Models(LLMs),
new avenues for sentiment analysis have opened. This paper proposes enhancing
performance by leveraging the Mutual Reinforcement Effect(MRE) between
individual words and the overall text. It delves into how word polarity
influences the overarching sentiment of a passage. To support our research, we
annotated four novel Sentiment Text Classification and Part of Speech(SCPOS)
datasets, building upon existing sentiment classification datasets.
Furthermore, we developed a Universal Sentiment Analysis(USA) model, with a
7-billion parameter size. Experimental results revealed that our model
surpassed the performance of gpt-3.5-turbo across all four datasets,
underscoring the significance of MRE in sentiment analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03831">
<div class="article-summary-box-inner">
<span><p>Drift in machine learning refers to the phenomenon where the statistical
properties of data or context, in which the model operates, change over time
leading to a decrease in its performance. Therefore, maintaining a constant
monitoring process for machine learning model performance is crucial in order
to proactively prevent any potential performance regression. However,
supervised drift detection methods require human annotation and consequently
lead to a longer time to detect and mitigate the drift. In our proposed
unsupervised drift detection method, we follow a two step process. Our first
step involves encoding a sample of production data as the target distribution,
and the model training data as the reference distribution. In the second step,
we employ a kernel-based statistical test that utilizes the maximum mean
discrepancy (MMD) distance metric to compare the reference and target
distributions and estimate any potential drift. Our method also identifies the
subset of production data that is the root cause of the drift. The models
retrained using these identified high drift samples show improved performance
on online customer experience quality metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03852">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved remarkable success in NLP and
multimodal tasks. Despite these successes, their development faces two main
challenges: (i) high computational cost; and (ii) difficulty in conducting fair
and objective evaluations. LLMs are prohibitively expensive, making it feasible
for only a few major players to undertake their training, thereby constraining
both research and application opportunities. This underscores the importance of
cost-effective LLM training. In this paper, we utilize a growth strategy to
significantly reduce LLM training cost. We demonstrate that an LLM with 101B
parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a
systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to
existing evaluations that focus more on knowledge-oriented abilities. We
introduce our benchmark including evaluations on important aspects of
intelligence including symbolic mapping, itrule understanding, pattern mining,
and anti-interference. Such evaluations minimize the potential impact of
memorization. Experimental results show that our model FLM-101B, trained with a
budget of $100K, achieves comparable performance to powerful and well-known
models, eg GPT-3 and GLM-130B, especially in the IQ benchmark evaluations with
contexts unseen in training data. The checkpoint of FLM-101B will be
open-sourced at https://huggingface.co/CofeAI/FLM-101B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03876">
<div class="article-summary-box-inner">
<span><p>Instruction-tuned Large Language Models (LLMs) have recently showcased
remarkable ability to generate fitting responses to natural language
instructions. However, an open research question concerns the inherent biases
of trained models and their responses. For instance, if the data used to tune
an LLM is dominantly written by persons with a specific political bias, we
might expect generated answers to share this bias. Current research work seeks
to de-bias such models, or suppress potentially biased answers. With this
demonstration, we take a different view on biases in instruction-tuning: Rather
than aiming to suppress them, we aim to make them explicit and transparent. To
this end, we present OpinionGPT, a web demo in which users can ask questions
and select all biases they wish to investigate. The demo will answer this
question using a model fine-tuned on text representing each of the selected
biases, allowing side-by-side comparison. To train the underlying model, we
identified 11 different biases (political, geographic, gender, age) and derived
an instruction-tuning corpus in which each answer was written by members of one
of these demographics. This paper presents OpinionGPT, illustrates how we
trained the bias-aware model and showcases the web application (available at
https://opiniongpt.informatik.hu-berlin.de).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing "Forecast Utterance" for Conversational Data Science. (arXiv:2309.03877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03877">
<div class="article-summary-box-inner">
<span><p>Envision an intelligent agent capable of assisting users in conducting
forecasting tasks through intuitive, natural conversations, without requiring
in-depth knowledge of the underlying machine learning (ML) processes. A
significant challenge for the agent in this endeavor is to accurately
comprehend the user's prediction goals and, consequently, formulate precise ML
tasks. In this paper, we take a pioneering step towards this ambitious goal by
introducing a new concept called Forecast Utterance and then focus on the
automatic and accurate interpretation of users' prediction goals from these
utterances. Specifically, we frame the task as a slot-filling problem, where
each slot corresponds to a specific aspect of the goal prediction task. We then
employ two zero-shot methods for solving the slot-filling task, namely: 1)
Entity Extraction (EE), and 2) Question-Answering (QA) techniques. Our
experiments, conducted with three meticulously crafted data sets, validate the
viability of our ambitious goal and demonstrate the effectiveness of both EE
and QA techniques in interpreting Forecast Utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03882">
<div class="article-summary-box-inner">
<span><p>Multi-choice questions (MCQs) serve as a common yet important task format in
the research of large language models (LLMs). Our work shows that LLMs exhibit
an inherent "selection bias" in MCQs, which refers to LLMs' preferences to
select options located at specific positions (like "Option C"). This bias is
prevalent across various LLMs, making their performance vulnerable to option
position changes in MCQs. We identify that one primary cause resulting in
selection bias is option numbering, i.e., the ID symbols A/B/C/D associated
with the options. To mitigate selection bias, we propose a new method called
PriDe. PriDe first decomposes the observed model prediction distribution into
an intrinsic prediction over option contents and a prior distribution over
option IDs. It then estimates the prior by permutating option contents on a
small number of test samples, which is used to debias the subsequent test
samples. We demonstrate that, as a label-free, inference-time method, PriDe
achieves a more effective and computation-efficient debiasing than strong
baselines. We further show that the priors estimated by PriDe generalize well
across different domains, highlighting its practical potential in broader
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03883">
<div class="article-summary-box-inner">
<span><p>Despite their impressive capabilities, large language models (LLMs) are prone
to hallucinations, i.e., generating content that deviates from facts seen
during pretraining. We propose a simple decoding strategy for reducing
hallucinations with pretrained LLMs that does not require conditioning on
retrieved external knowledge nor additional fine-tuning. Our approach obtains
the next-token distribution by contrasting the differences in logits obtained
from projecting the later layers versus earlier layers to the vocabulary space,
exploiting the fact that factual knowledge in an LLMs has generally been shown
to be localized to particular transformer layers. We find that this Decoding by
Contrasting Layers (DoLa) approach is able to better surface factual knowledge
and reduce the generation of incorrect facts. DoLa consistently improves the
truthfulness across multiple choices tasks and open-ended generation tasks, for
example improving the performance of LLaMA family models on TruthfulQA by
12-17% absolute points, demonstrating its potential in making LLMs reliably
generate truthful facts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Audio Captioning via Audibility Guidance. (arXiv:2309.03884v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03884">
<div class="article-summary-box-inner">
<span><p>The task of audio captioning is similar in essence to tasks such as image and
video captioning. However, it has received much less attention. We propose
three desiderata for captioning audio -- (i) fluency of the generated text,
(ii) faithfulness of the generated text to the input audio, and the somewhat
related (iii) audibility, which is the quality of being able to be perceived
based only on audio. Our method is a zero-shot method, i.e., we do not learn to
perform captioning. Instead, captioning occurs as an inference process that
involves three networks that correspond to the three desired qualities: (i) A
Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A
model that provides a matching score between an audio file and a text, for
which we use a multimodal matching network called ImageBind, and (iii) A text
classifier, trained using a dataset we collected automatically by instructing
GPT-4 with prompts designed to direct the generation of both audible and
inaudible sentences. We present our results on the AudioCap dataset,
demonstrating that audibility guidance significantly enhances performance
compared to the baseline, which lacks this objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03886">
<div class="article-summary-box-inner">
<span><p>Labeling neural network submodules with human-legible descriptions is useful
for many downstream tasks: such descriptions can surface failures, guide
interventions, and perhaps even explain important model behaviors. To date,
most mechanistic descriptions of trained networks have involved small models,
narrowly delimited phenomena, and large amounts of human labor. Labeling all
human-interpretable sub-computations in models of increasing size and
complexity will almost certainly require tools that can generate and validate
descriptions automatically. Recently, techniques that use learned models
in-the-loop for labeling have begun to gain traction, but methods for
evaluating their efficacy are limited and ad-hoc. How should we validate and
compare open-ended labeling tools? This paper introduces FIND (Function
INterpretation and Description), a benchmark suite for evaluating the building
blocks of automated interpretability methods. FIND contains functions that
resemble components of trained neural networks, and accompanying descriptions
of the kind we seek to generate. The functions are procedurally constructed
across textual and numeric domains, and involve a range of real-world
complexities, including noise, composition, approximation, and bias. We
evaluate new and existing methods that use language models (LMs) to produce
code-based and language descriptions of function behavior. We find that an
off-the-shelf LM augmented with only black-box access to functions can
sometimes infer their structure, acting as a scientist by forming hypotheses,
proposing experiments, and updating descriptions in light of new data. However,
LM-based descriptions tend to capture global function behavior and miss local
corruptions. These results show that FIND will be useful for characterizing the
performance of more sophisticated interpretability methods before they are
applied to real-world models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03905">
<div class="article-summary-box-inner">
<span><p>We present ImageBind-LLM, a multi-modality instruction tuning method of large
language models (LLMs) via ImageBind. Existing works mainly focus on language
and image instruction tuning, different from which, our ImageBind-LLM can
respond to multi-modality conditions, including audio, 3D point clouds, video,
and their embedding-space arithmetic by only image-text alignment training.
During training, we adopt a learnable bind network to align the embedding space
between LLaMA and ImageBind's image encoder. Then, the image features
transformed by the bind network are added to word tokens of all layers in
LLaMA, which progressively injects visual instructions via an attention-free
and zero-initialized gating mechanism. Aided by the joint embedding of
ImageBind, the simple image-text training enables our model to exhibit superior
multi-modality instruction-following capabilities. During inference, the
multi-modality inputs are fed into the corresponding ImageBind encoders, and
processed by a proposed visual cache model for further cross-modal embedding
enhancement. The training-free cache model retrieves from three million image
features extracted by ImageBind, which effectively mitigates the
training-inference modality discrepancy. Notably, with our approach,
ImageBind-LLM can respond to instructions of diverse modalities and demonstrate
significant language generation quality. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12191">
<div class="article-summary-box-inner">
<span><p>The representation space of neural models for textual data emerges in an
unsupervised manner during training. Understanding how those representations
encode human-interpretable concepts is a fundamental problem. One prominent
approach for the identification of concepts in neural representations is
searching for a linear subspace whose erasure prevents the prediction of the
concept from the representations. However, while many linear erasure algorithms
are tractable and interpretable, neural networks do not necessarily represent
concepts in a linear manner. To identify non-linearly encoded concepts, we
propose a kernelization of a linear minimax game for concept erasure. We
demonstrate that it is possible to prevent specific non-linear adversaries from
predicting the concept. However, the protection does not transfer to different
nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded
concept remains an open problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph. (arXiv:2212.05798v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05798">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over textual resources remains a challenge,
particularly when dealing with nuanced relationships between multiple entities
expressed within natural-language sentences. To this end, curated knowledge
bases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely used
and gained great acceptance for question-answering (QA) applications in the
past decade. While these KBs offer a structured knowledge representation, they
lack the contextual diversity found in natural-language sources. To address
this limitation, BigText-QA introduces an integrated QA approach, which is able
to answer questions based on a more redundant form of a knowledge graph (KG)
that organizes both structured and unstructured (i.e., "hybrid") knowledge in a
unified graphical representation. Thereby, BigText-QA is able to combine the
best of both worlds$\unicode{x2013}$a canonical set of named entities, mapped
to a structured background KB (such as YAGO or Wikidata), as well as an open
set of textual clauses providing highly diversified relational paraphrases with
rich context information. Our experimental results demonstrate that BigText-QA
outperforms DrQA, a neural-network-based QA system, and achieves competitive
results to QUEST, a graph-based unsupervised QA system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Claim Optimization in Computational Argumentation. (arXiv:2212.08913v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08913">
<div class="article-summary-box-inner">
<span><p>An optimal delivery of arguments is key to persuasion in any debate, both for
humans and for AI systems. This requires the use of clear and fluent claims
relevant to the given debate. Prior work has studied the automatic assessment
of argument quality extensively. Yet, no approach actually improves the quality
so far. To fill this gap, this paper proposes the task of claim optimization:
to rewrite argumentative claims in order to optimize their delivery. As
multiple types of optimization are possible, we approach this task by first
generating a diverse set of candidate claims using a large language model, such
as BART, taking into account contextual information. Then, the best candidate
is selected using various quality metrics. In automatic and human evaluation on
an English-language corpus, our quality-based candidate selection outperforms
several baselines, improving 60% of all claims (worsening 16% only). Follow-up
analyses reveal that, beyond copy editing, our approach often specifies claims
with details, whereas it adds less evidence than humans do. Moreover, its
capabilities generalize well to other domains, such as instructional texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13592">
<div class="article-summary-box-inner">
<span><p>While code-mixing is a common linguistic practice in many parts of the world,
collecting high-quality and low-cost code-mixed data remains a challenge for
natural language processing (NLP) research. The recent proliferation of Large
Language Models (LLMs) compels one to ask: how capable are these systems in
generating code-mixed data? In this paper, we explore prompting multilingual
LLMs in a zero-shot manner to generate code-mixed data for seven languages in
South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,
Tamil, and Singlish. We find that publicly available multilingual
instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of
producing texts with phrases or clauses from different languages. ChatGPT
exhibits inconsistent capabilities in generating code-mixed texts, wherein its
performance varies depending on the prompt template and language pairing. For
instance, ChatGPT generates fluent and natural Singlish texts (an English-based
creole spoken in Singapore), but for English-Tamil language pair, the system
mostly produces grammatically incorrect or semantically meaningless utterances.
Furthermore, it may erroneously introduce languages not specified in the
prompt. Based on our investigation, existing multilingual LLMs exhibit a wide
range of proficiency in code-mixed data generation for SEA languages. As such,
we advise against using LLMs in this context without extensive human checks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Chain-of-Thought Prompting for Code Generation. (arXiv:2305.06599v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06599">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive
performance in code generation. LLMs take prompts as inputs, and
Chain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.
CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural
language reasoning steps) and then output the code. However, CoT prompting is
designed for natural language generation and has low accuracy in code
generation.
</p>
<p>In this paper, we propose Structured CoTs (SCoTs) and present a novel
prompting technique for code generation, named SCoT prompting. Our motivation
is source code contains rich structural information and any code can be
composed of three program structures (i.e., sequence, branch, and loop
structures). Intuitively, structured intermediate reasoning steps make for
structured source code. Thus, we ask LLMs to use program structures to build
CoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.
Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to think
about how to solve requirements from the view of source code and further the
performance of LLMs in code generation. We apply SCoT prompting to two LLMs
(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,
MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline
- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human
developers prefer programs from SCoT prompting. (3) SCoT prompting is robust to
examples and achieves substantial improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00526">
<div class="article-summary-box-inner">
<span><p>Layout-aware pre-trained models has achieved significant progress on document
image question answering. They introduce extra learnable modules into existing
language models to capture layout information within document images from text
bounding box coordinates obtained by OCR tools. However, extra modules
necessitate pre-training on extensive document images. This prevents these
methods from directly utilizing off-the-shelf instruction-tuning language
foundation models, which have recently shown promising potential in zero-shot
learning. Instead, in this paper, we find that instruction-tuning language
models like Claude and ChatGPT can understand layout by spaces and line breaks.
Based on this observation, we propose the LAyout and Task aware Instruction
Prompt (LATIN-Prompt), which consists of layout-aware document content and
task-aware instruction. Specifically, the former uses appropriate spaces and
line breaks to recover the layout information among text segments obtained by
OCR tools, and the latter ensures that generated answers adhere to formatting
requirements. Moreover, we propose the LAyout and Task aware Instruction Tuning
(LATIN-Tuning) to improve the performance of small instruction-tuning models
like Alpaca. Experimental results show that LATIN-Prompt enables zero-shot
performance of Claude and ChatGPT to be comparable to the fine-tuning
performance of SOTAs on document image question answering, and LATIN-Tuning
enhances the zero-shot performance of Alpaca significantly. For example,
LATIN-Prompt improves the performance of Claude and ChatGPT on DocVQA by 263%
and 20% respectively. LATIN-Tuning improves the performance of Alpaca on DocVQA
by 87.7%. Quantitative and qualitative analyses demonstrate the effectiveness
of LATIN-Prompt and LATIN-Tuning. We provide the code in supplementary and will
release it to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. (arXiv:2306.05301v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05301">
<div class="article-summary-box-inner">
<span><p>Enabling large language models to utilize real-world tools effectively is
crucial for achieving embodied intelligence. Existing approaches to tool
learning have either primarily relied on extremely large language models, such
as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or
utilized supervised learning to train limited scopes of tools on compact
models. However, it remains uncertain whether smaller language models can
achieve generalized tool-use abilities without tool-specific training. To
address this question, this paper introduces ToolAlpaca, a novel framework
designed to automatically generate a diverse tool-use corpus and learn
generalized tool-use abilities on compact language models with minimal human
intervention. Specifically, ToolAlpaca first automatically creates a highly
diversified tool-use corpus by building a multi-agent simulation environment.
The corpus contains 3938 tool-use instances from more than 400 real-world tool
APIs spanning 50 distinct categories. Subsequently, the constructed corpus is
employed to fine-tune compact language models, resulting in two models, namely
ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the
ability of these models to utilize previously unseen tools without specific
training. Experimental results demonstrate that ToolAlpaca achieves effective
generalized tool-use capabilities comparable to those of extremely large
language models like GPT-3.5, demonstrating that learning generalized tool-use
ability is feasible for compact language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Max-Margin Token Selection in Attention Mechanism. (arXiv:2306.13596v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13596">
<div class="article-summary-box-inner">
<span><p>Attention mechanism is a central component of the transformer architecture
which led to the phenomenal success of large language models. However, the
theoretical principles underlying the attention mechanism are poorly
understood, especially its nonconvex optimization dynamics. In this work, we
explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle
\boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where
$\boldsymbol{X}$ is the token sequence and
$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are trainable parameters. We
prove that running gradient descent on $\boldsymbol{p}$, or equivalently
$\boldsymbol{W}$, converges in direction to a max-margin solution that
separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly
formalizes attention as an optimal token selection mechanism. Remarkably, our
results are applicable to general data and precisely characterize
$\textit{optimality}$ of tokens in terms of the value embeddings
$\boldsymbol{Xv}$ and problem geometry. We also provide a broader
regularization path analysis that establishes the margin maximizing nature of
attention even for nonlinear prediction heads. When optimizing $\boldsymbol{v}$
and $\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions
under which the regularization paths directionally converge to their respective
hard-margin SVM solutions where $\boldsymbol{v}$ separates the input features
based on their labels. Interestingly, the SVM formulation of $\boldsymbol{p}$
is influenced by the support vector geometry of $\boldsymbol{v}$. Finally, we
verify our theoretical findings via numerical experiments and provide insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04014">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to restart the process over again once new data becomes available. A much
cheaper and more efficient solution would be to enable the continual
pre-training of these models, i.e. updating pre-trained models with new data
instead of re-training them from scratch. However, the distribution shift
induced by novel data typically results in degraded performance on past data.
Taking a step towards efficient continual pre-training, in this work, we
examine the effect of different warm-up strategies. Our hypothesis is that the
learning rate must be re-increased to improve compute efficiency when training
on a new dataset. We study the warmup phase of models pre-trained on the Pile
(upstream data, 300B tokens) as we continue to pre-train on SlimPajama
(downstream data, 297B tokens), following a linear warmup and cosine decay
schedule. We conduct all experiments on the Pythia 410M language model
architecture and evaluate performance through validation perplexity. We
experiment with different pre-training checkpoints, various maximum learning
rates, and various warmup lengths. Our results show that while rewarming models
first increases the loss on upstream and downstream data, in the longer run it
improves the downstream performance, outperforming models trained from
scratch$\unicode{x2013}$even for a large downstream dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11432">
<div class="article-summary-box-inner">
<span><p>Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11764">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZC3: Zero-Shot Cross-Language Code Clone Detection. (arXiv:2308.13754v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13754">
<div class="article-summary-box-inner">
<span><p>Developers introduce code clones to improve programming productivity. Many
existing studies have achieved impressive performance in monolingual code clone
detection. However, during software development, more and more developers write
semantically equivalent programs with different languages to support different
platforms and help developers translate projects from one language to another.
Considering that collecting cross-language parallel data, especially for
low-resource languages, is expensive and time-consuming, how designing an
effective cross-language model that does not rely on any parallel data is a
significant problem. In this paper, we propose a novel method named ZC3 for
Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive
snippet prediction to form an isomorphic representation space among different
programming languages. Based on this, ZC3 exploits domain-aware learning and
cycle consistency learning to further constrain the model to generate
representations that are aligned among different languages meanwhile are
diacritical for different types of clones. To evaluate our approach, we conduct
extensive experiments on four representative cross-language clone detection
datasets. Experimental results show that ZC3 outperforms the state-of-the-art
baselines by 67.12%, 51.39%, 14.85%, and 53.01% on the MAP score, respectively.
We further investigate the representational distribution of different languages
and discuss the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EditSum: A Retrieve-and-Edit Framework for Source Code Summarization. (arXiv:2308.13775v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13775">
<div class="article-summary-box-inner">
<span><p>Existing studies show that code summaries help developers understand and
maintain source code. Unfortunately, these summaries are often missing or
outdated in software projects. Code summarization aims to generate natural
language descriptions automatically for source code. Code summaries are highly
structured and have repetitive patterns. Besides the patternized words, a code
summary also contains important keywords, which are the key to reflecting the
functionality of the code. However, the state-of-the-art approaches perform
poorly on predicting the keywords, which leads to the generated summaries
suffering a loss in informativeness. To alleviate this problem, this paper
proposes a novel retrieve-and-edit approach named EditSum for code
summarization. Specifically, EditSum first retrieves a similar code snippet
from a pre-defined corpus and treats its summary as a prototype summary to
learn the pattern. Then, EditSum edits the prototype automatically to combine
the pattern in the prototype with the semantic information of input code. Our
motivation is that the retrieved prototype provides a good start-point for
post-generation because the summaries of similar code snippets often have the
same pattern. The post-editing process further reuses the patternized words in
the prototype and generates keywords based on the semantic information of input
code. We conduct experiments on a large-scale Java corpus and experimental
results demonstrate that EditSum outperforms the state-of-the-art approaches by
a substantial margin. The human evaluation also proves the summaries generated
by EditSum are more informative and useful. We also verify that EditSum
performs well on predicting the patternized words and keywords.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16137">
<div class="article-summary-box-inner">
<span><p>In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex tasks, they often face the
need to conduct longer reasoning processes or understand larger contexts. In
these situations, the length generalization failure of LLMs on long sequences
becomes more prominent. Most pre-training schemes truncate training sequences
to a fixed length. LLMs often struggle to generate fluent and coherent texts,
let alone carry out downstream tasks, after longer contexts, even with relative
positional encoding designed to cope with this problem. Common solutions such
as finetuning on longer corpora often involve daunting hardware and time costs
and require careful training process design. To more efficiently leverage the
generation capacity of existing LLMs, we theoretically and empirically
investigate the main out-of-distribution (OOD) factors contributing to this
problem. Inspired by this diagnosis, we propose a simple yet effective solution
for on-the-fly length generalization, LM-Infinite. It involves only a
$\Lambda$-shaped attention mask (to avoid excessive attended tokens) and a
distance limit (to avoid unseen distances) while requiring no parameter updates
or learning. We find it applicable to a variety of LLMs using relative-position
encoding methods. LM-Infinite is computationally efficient with $O(n)$ time and
space, and demonstrates consistent text generation fluency and quality to as
long as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding
speedup. On downstream tasks such as passkey retrieval, it continues to work on
inputs much longer than training lengths where vanilla models fail immediately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16763">
<div class="article-summary-box-inner">
<span><p>Stance detection aims to identify the attitude expressed in a document
towards a given target. Techniques such as Chain-of-Thought (CoT) prompting
have advanced this task, enhancing a model's reasoning capabilities through the
derivation of intermediate rationales. However, CoT relies primarily on a
model's pre-trained internal knowledge during reasoning, thereby neglecting the
valuable external information that is previously unknown to the model. This
omission, especially within the unsupervised reasoning process, can affect the
model's overall performance. Moreover, while CoT enhances Large Language Models
(LLMs), smaller LMs, though efficient operationally, face challenges in
delivering nuanced reasoning. In response to these identified gaps, we
introduce the Ladder-of-Thought (LoT) for the stance detection task.
Constructed through a dual-phase Progressive Optimization Framework, LoT
directs the small LMs to assimilate high-quality external knowledge, refining
the intermediate rationales produced. These bolstered rationales subsequently
serve as the foundation for more precise predictions - akin to how a ladder
facilitates reaching elevated goals. LoT achieves a balance between efficiency
and performance. Our empirical evaluations underscore LoT's efficacy, marking a
16% improvement over GPT-3.5 and a 10% enhancement compared to GPT-3.5 with CoT
on stance detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers as Support Vector Machines. (arXiv:2308.16898v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16898">
<div class="article-summary-box-inner">
<span><p>Since its inception in "Attention Is All You Need", transformer architecture
has led to revolutionary advancements in NLP. The attention layer within the
transformer admits a sequence of input tokens $X$ and makes them interact
through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where
$(K,Q)$ are the trainable key-query parameters. In this work, we establish a
formal equivalence between the optimization geometry of self-attention and a
hard-margin SVM problem that separates optimal input tokens from non-optimal
tokens using linear constraints on the outer-products of token pairs. This
formalism allows us to characterize the implicit bias of 1-layer transformers
optimized with gradient descent: (1) Optimizing the attention layer with
vanishing regularization, parameterized by $(K,Q)$, converges in direction to
an SVM solution minimizing the nuclear norm of the combined parameter
$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm
objective. We characterize this convergence, highlighting that it can occur
toward locally-optimal directions rather than global ones. (2) Complementing
this, we prove the local/global directional convergence of gradient descent
under suitable geometric conditions. Importantly, we show that
over-parameterization catalyzes global convergence by ensuring the feasibility
of the SVM problem and by guaranteeing a benign optimization landscape devoid
of stationary points. (3) While our theory applies primarily to linear
prediction heads, we propose a more general SVM equivalence that predicts the
implicit bias with nonlinear heads. Our findings are applicable to arbitrary
datasets and their validity is verified via experiments. We also introduce
several open problems and research directions. We believe these findings
inspire the interpretation of transformers as a hierarchy of SVMs that
separates and selects optimal tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00237">
<div class="article-summary-box-inner">
<span><p>The development of large language models tailored for handling patients'
clinical notes is often hindered by the limited accessibility and usability of
these notes due to strict privacy regulations. To address these challenges, we
first create synthetic large-scale clinical notes using publicly available case
reports extracted from biomedical literature. We then use these synthetic notes
to train our specialized clinical large language model, Asclepius. While
Asclepius is trained on synthetic data, we assess its potential performance in
real-world applications by evaluating it using real clinical notes. We
benchmark Asclepius against several other large language models, including
GPT-3.5-turbo and other open-source alternatives. To further validate our
approach using synthetic notes, we also compare Asclepius with its variants
trained on real clinical notes. Our findings convincingly demonstrate that
synthetic clinical notes can serve as viable substitutes for real ones when
constructing high-performing clinical language models. This conclusion is
supported by detailed evaluations conducted by both GPT-4 and medical
professionals. All resources including weights, codes, and data used in the
development of Asclepius are made publicly accessible for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot information extraction from radiological reports using ChatGPT. (arXiv:2309.01398v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01398">
<div class="article-summary-box-inner">
<span><p>Electronic health records contain an enormous amount of valuable information,
but many are recorded in free text. Information extraction is the strategy to
transform the sequence of characters into structured data, which can be
employed for secondary analysis. However, the traditional information
extraction components, such as named entity recognition and relation
extraction, require annotated data to optimize the model parameters, which has
become one of the major bottlenecks in building information extraction systems.
With the large language models achieving good performances on various
downstream NLP tasks without parameter tuning, it becomes possible to use large
language models for zero-shot information extraction. In this study, we aim to
explore whether the most popular large language model, ChatGPT, can extract
useful information from the radiological reports. We first design the prompt
template for the interested information in the CT reports. Then, we generate
the prompts by combining the prompt template with the CT reports as the inputs
of ChatGPT to obtain the responses. A post-processing module is developed to
transform the responses into structured extraction results. We conducted the
experiments with 847 CT reports collected from Peking University Cancer
Hospital. The experimental results indicate that ChatGPT can achieve
competitive performances for some extraction tasks compared with the baseline
information extraction system, but some limitations need to be further
improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!. (arXiv:2309.02110v2 [math.HO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02110">
<div class="article-summary-box-inner">
<span><p>Wordle is a popular, online word game offered by the New York Times
(nytimes.com). Currently there are some 2 million players of the English
version worldwide. Players have 6 attempts to guess the daily word (target
word) and after each attempt, the player receives color-coded information about
the correctness and position of each letter in the guess. After either a
successful completion of the puzzle or the final unsuccessful attempt, software
can assess the player's luck and skill using Information Theory and can display
data for the first, second, ..., sixth guesses of a random sample of all
players. Recently, I discovered that the latter data is presented in a format
that can easily be copied and pasted into a spreadsheet. I compiled data on
Wordle players' first guesses from May 2023 - August 2023 and inferred some
interesting information about Wordle players. A) Every day, about 0.2-0.5% of
players solve the puzzle in one attempt. Because the odds of guessing the one
of 2,315 possible target words at random is 0.043%, this implies that 4,000 -
10,000 players cheat by obtaining the target word outside of playing the game!
B) At least 1/3 of the players have a favorite starting word, or cycle through
several. And even though players should be aware that target words are never
repeated, most players appear to remain loyal to their starting word even after
its appearance as a target word. C) On August 15, 2023, about 30,000 players
abruptly changed their starting word, presumably based on a crossword puzzle
clue! Wordle players can be influenced! This study goes beyond social media
postings, surveys, and Google Trends to provide solid, quantitative evidence
about cheating in Wordle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02553">
<div class="article-summary-box-inner">
<span><p>Behavioral testing in NLP allows fine-grained evaluation of systems by
examining their linguistic capabilities through the analysis of input-output
behavior. Unfortunately, existing work on behavioral testing in Machine
Translation (MT) is currently restricted to largely handcrafted tests covering
a limited range of capabilities and languages. To address this limitation, we
propose to use Large Language Models (LLMs) to generate a diverse set of source
sentences tailored to test the behavior of MT models in a range of situations.
We can then verify whether the MT model exhibits the expected behavior through
matching candidate sets that are also generated using LLMs. Our approach aims
to make behavioral testing of MT systems practical while requiring only minimal
human effort. In our experiments, we apply our proposed evaluation framework to
assess multiple available MT systems, revealing that while in general
pass-rates follow the trends observable from traditional accuracy-based
metrics, our method was able to uncover several important differences and
potential bugs that go unnoticed when relying only on accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02706">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable
capabilities across a wide range of tasks, however, the attention given to
non-English languages has been limited in this field of research. To address
this gap and assess the proficiency of language models in the Korean language
and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary,
history, and general knowledge. Our evaluation of language models on this
benchmark highlights the potential advantages of employing Large
Language-Specific Models(LLSMs) over a comprehensive, universal model like
GPT-3.5. Remarkably, our study reveals that models approximately 13 times
smaller than GPT-3.5 can exhibit similar performance levels in terms of
language-specific knowledge retrieval. This observation underscores the
importance of homogeneous corpora for training professional-level
language-specific models. On the contrary, we also observe a perplexing
performance dip in these smaller LMs when they are tasked to generate
structured answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Large Language Models for Clinical Tasks. (arXiv:2309.02884v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02884">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable adaptability,
showcasing their capacity to excel in tasks for which they were not explicitly
trained. However, despite their impressive natural language processing (NLP)
capabilities, effective alignment of LLMs remains a crucial challenge when
deploying them for specific clinical applications. The ability to generate
responses with factually accurate content and to engage in non-trivial
reasoning steps are crucial for the LLMs to be eligible for applications in
clinical medicine. Employing a combination of techniques including
instruction-tuning and in-prompt strategies like few-shot and chain-of-thought
prompting has significantly enhanced the performance of LLMs. Our proposed
alignment strategy for medical question-answering, known as
'expand-guess-refine', offers a parameter and data-efficient solution. A
preliminary analysis of this method demonstrated outstanding performance,
achieving a score of 70.63% on a subset of questions sourced from the USMLE
dataset.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-08 23:10:34.273412168 UTC">2023-09-08 23:10:34 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>