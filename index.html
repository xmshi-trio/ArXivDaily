<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-10T01:30:00Z">11-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Discover, Explanation, Improvement: Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04476">
<div class="article-summary-box-inner">
<span><p>Current natural language processing (NLP) models such as BERT and RoBERTa
have achieved high overall performance, but they often make systematic errors
due to bias or certain difficult features to learn. Thus research on slice
detection models (SDM) which automatically identifies underperforming groups of
datapoints has gradually caught more attention, which aims at both
understanding model behaviors and providing insights for future model training
and designing. However, there is little systematic research on SDM and
quantitative evaluation of its assessment for NLP models. Our paper fills this
gap by proposing "Discover, Explanation, Improvement" framework that discovers
coherent and underperforming groups of datapoints and unites datapoints of each
slice under human-understandable concepts; it also provides comprehensive
evaluation tasks and the corresponding quantitative metrics, which enable
convenient comparison for future works. Results show that our framework can
accurately select error-prone datapoints with informative semantic features
that summarize error patterns, based on which it directly boosts model
performance by an average of 2.85 points based on trained models without tuning
any parameters across multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Example Selection for In-Context Learning. (arXiv:2211.04486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04486">
<div class="article-summary-box-inner">
<span><p>With a handful of demonstration examples, large-scale language models show
strong capability to perform various tasks by in-context learning from these
examples, without any fine-tuning. We demonstrate that in-context learning
performance can be highly unstable across samples of examples, indicating the
idiosyncrasies of how language models acquire information. We formulate example
selection for in-context learning as a sequential decision problem, and propose
a reinforcement learning algorithm for identifying generalizable policies to
select demonstration examples. For GPT-2, our learned policies demonstrate
strong abilities of generalizing to unseen tasks in training, with a $5.8\%$
improvement on average. Examples selected from our learned policies can even
achieve a small improvement on GPT-3 Ada. However, the improvement diminishes
on larger GPT-3 models, suggesting emerging capabilities of large language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations. (arXiv:2211.04508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04508">
<div class="article-summary-box-inner">
<span><p>We present SpeechMatrix, a large-scale multilingual corpus of
speech-to-speech translations mined from real speech of European Parliament
recordings. It contains speech alignments in 136 language pairs with a total of
418 thousand hours of speech. To evaluate the quality of this parallel speech,
we train bilingual speech-to-speech translation models on mined data only and
establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test
sets. Enabled by the multilinguality of SpeechMatrix, we also explore
multilingual speech-to-speech translation, a topic which was addressed by few
other works. We also demonstrate that model pre-training and sparse scaling
using Mixture-of-Experts bring large gains to translation performance. The
mined data and models are freely available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going for GOAL: A Resource for Grounded Football Commentaries. (arXiv:2211.04534v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04534">
<div class="article-summary-box-inner">
<span><p>Recent video+language datasets cover domains where the interaction is highly
structured, such as instructional videos, or where the interaction is scripted,
such as TV shows. Both of these properties can lead to spurious cues to be
exploited by models rather than learning to ground language. In this paper, we
present GrOunded footbAlL commentaries (GOAL), a novel dataset of football (or
`soccer') highlights videos with transcribed live commentaries in English. As
the course of a game is unpredictable, so are commentaries, which makes them a
unique resource to investigate dynamic language grounding. We also provide
state-of-the-art baselines for the following tasks: frame reordering, moment
retrieval, live commentary retrieval and play-by-play live commentary
generation. Results show that SOTA models perform reasonably well in most
tasks. We discuss the implications of these results and suggest new tasks for
which GOAL can be used. Our codebase is available at:
https://gitlab.com/grounded-sport-convai/goal-baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward a Neural Semantic Parsing System for EHR Question Answering. (arXiv:2211.04569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04569">
<div class="article-summary-box-inner">
<span><p>Clinical semantic parsing (SP) is an important step toward identifying the
exact information need (as a machine-understandable logical form) from a
natural language query aimed at retrieving information from electronic health
records (EHRs). Current approaches to clinical SP are largely based on
traditional machine learning and require hand-building a lexicon. The recent
advancements in neural SP show a promise for building a robust and flexible
semantic parser without much human effort. Thus, in this paper, we aim to
systematically assess the performance of two such neural SP models for EHR
question answering (QA). We found that the performance of these advanced neural
models on two clinical SP datasets is promising given their ease of application
and generalizability. Our error analysis surfaces the common types of errors
made by these models and has the potential to inform future research into
improving the performance of neural SP models for EHR QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Euphemisms with Literal Descriptions and Visual Imagery. (arXiv:2211.04576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04576">
<div class="article-summary-box-inner">
<span><p>This paper describes our two-stage system for the Euphemism Detection shared
task hosted by the 3rd Workshop on Figurative Language Processing in
conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive
or unpleasant issues like addiction and death. The ambiguous nature of
euphemistic words or expressions makes it challenging to detect their actual
meaning within a context. In the first stage, we seek to mitigate this
ambiguity by incorporating literal descriptions into input text prompts to our
baseline model. It turns out that this kind of direct supervision yields
remarkable performance improvement. In the second stage, we integrate visual
supervision into our system using visual imageries, two sets of images
generated by a text-to-image model by taking terms and descriptions as input.
Our experiments demonstrate that visual supervision also gives a statistically
significant performance boost. Our system achieved the second place with an F1
score of 87.2%, only about 0.9% worse than the best submission.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Follow Instructions in Text-Based Games. (arXiv:2211.04591v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04591">
<div class="article-summary-box-inner">
<span><p>Text-based games present a unique class of sequential decision making problem
in which agents interact with a partially observable, simulated environment via
actions and observations conveyed through natural language. Such observations
typically include instructions that, in a reinforcement learning (RL) setting,
can directly or indirectly guide a player towards completing reward-worthy
tasks. In this work, we study the ability of RL agents to follow such
instructions. We conduct experiments that show that the performance of
state-of-the-art text-based game agents is largely unaffected by the presence
or absence of such instructions, and that these agents are typically unable to
execute tasks to completion. To further study and address the task of
instruction following, we equip RL agents with an internal structured
representation of natural language instructions in the form of Linear Temporal
Logic (LTL), a formal language that is increasingly used for temporally
extended reward specification in RL. Our framework both supports and highlights
the benefit of understanding the temporal semantics of instructions and in
measuring progress towards achievement of such a temporally extended behaviour.
Experiments with 500+ games in TextWorld demonstrate the superior performance
of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects. (arXiv:2211.04604v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04604">
<div class="article-summary-box-inner">
<span><p>Robots operating in human environments must be able to rearrange objects into
semantically-meaningful configurations, even if these objects are previously
unseen. In this work, we focus on the problem of building physically-valid
structures without step-by-step instructions. We propose StructDiffusion, which
combines a diffusion model and an object-centric transformer to construct
structures out of a single RGB-D image based on high-level language goals, such
as "set the table." Our method shows how diffusion models can be used for
complex multi-step 3D planning tasks. StructDiffusion improves success rate on
assembling physically-valid structures out of unseen objects by on average 16%
over an existing multi-modal transformer model, while allowing us to use one
multi-task model to produce a wider range of different structures. We show
experiments on held-out objects in both simulation and on real-world
rearrangement tasks. For videos and additional results, check out our website:
<a href="http://weiyuliu.com/StructDiffusion/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepE: a deep neural network for knowledge graph embedding. (arXiv:2211.04620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04620">
<div class="article-summary-box-inner">
<span><p>Recently, neural network based methods have shown their power in learning
more expressive features on the task of knowledge graph embedding (KGE).
However, the performance of deep methods often falls behind the shallow ones on
simple graphs. One possible reason is that deep models are difficult to train,
while shallow models might suffice for accurately representing the structure of
the simple KGs.
</p>
<p>In this paper, we propose a neural network based model, named DeepE, to
address the problem, which stacks multiple building blocks to predict the tail
entity based on the head entity and the relation. Each building block is an
addition of a linear and a non-linear function. The stacked building blocks are
equivalent to a group of learning functions with different non-linear depth.
Hence, DeepE allows deep functions to learn deep features, and shallow
functions to learn shallow features. Through extensive experiments, we find
DeepE outperforms other state-of-the-art baseline methods. A major advantage of
DeepE is the robustness. DeepE achieves a Mean Rank (MR) score that is 6%, 30%,
65% lower than the best baseline methods on FB15k-237, WN18RR and YAGO3-10. Our
design makes it possible to train much deeper networks on KGE, e.g. 40 layers
on FB15k-237, and without scarifying precision on simple relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Method to Judge the Style of Classical Poetry Based on Pre-trained Model. (arXiv:2211.04657v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04657">
<div class="article-summary-box-inner">
<span><p>One of the important topics in the research field of Chinese classical poetry
is to analyze the poetic style. By examining the relevant works of previous
dynasties, researchers judge a poetic style mostly by their subjective
feelings, and refer to the previous evaluations that have become a certain
conclusion. Although this judgment method is often effective, there may be some
errors. This paper builds the most perfect data set of Chinese classical poetry
at present, trains a BART-poem pre -trained model on this data set, and puts
forward a generally applicable poetry style judgment method based on this
BART-poem model, innovatively introduces in-depth learning into the field of
computational stylistics, and provides a new research method for the study of
classical poetry. This paper attempts to use this method to solve the problem
of poetry style identification in the Tang and Song Dynasties, and takes the
poetry schools that are considered to have a relatively clear and consistent
poetic style, such as the Hongzheng Qizi and Jiajing Qizi, Jiangxi poetic
school and Tongguang poetic school, as the research object, and takes the poems
of their representative poets for testing. Experiments show that the judgment
results of the tested poetry work made by the model are basically consistent
with the conclusions given by critics of previous dynasties, verify some
avant-garde judgments of Mr. Qian Zhongshu, and better solve the task of poetry
style recognition in the Tang and Song dynasties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Label Prompt Selection. (arXiv:2211.04668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04668">
<div class="article-summary-box-inner">
<span><p>Natural language prompts have been shown to facilitate cross-task
generalization for large language models. However, with no or limited labeled
examples, the cross-task performance is highly sensitive to the choice of
prompts, while selecting a high-performing prompt is challenging given the
scarcity of labels. To address the issue, we propose a Zero-Label Prompt
Selection (ZPS) method that selects prompts without any labeled data or
gradient update. Specifically, given the candidate human-written prompts for a
task, ZPS labels a set of unlabeled data with a prompt ensemble and uses the
pseudo-labels for prompt selection. Experiments show that ZPS improves over
prior methods by a sizeable margin in zero-label performance. We also extend
ZPS to a few-shot setting and show its advantages over strong baselines such as
prompt tuning and model tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind. (arXiv:2211.04684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04684">
<div class="article-summary-box-inner">
<span><p>When reading a story, humans can rapidly understand new fictional characters
with a few observations, mainly by drawing analogy to fictional and real people
they met before in their lives. This reflects the few-shot and meta-learning
essence of humans' inference of characters' mental states, i.e., humans'
theory-of-mind (ToM), which is largely ignored in existing research. We fill
this gap with a novel NLP benchmark, TOM-IN-AMC, the first assessment of
models' ability of meta-learning of ToM in a realistic narrative understanding
scenario. Our benchmark consists of $\sim$1,000 parsed movie scripts for this
purpose, each corresponding to a few-shot character understanding task; and
requires models to mimic humans' ability of fast digesting characters with a
few starting scenes in a new movie. Our human study verified that humans can
solve our problem by inferring characters' mental states based on their
previously seen movies; while the state-of-the-art metric-learning and
meta-learning approaches adapted to our task lags 30% behind.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Extractive Summarization with Heterogeneous Graph Embeddings for Chinese Document. (arXiv:2211.04698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04698">
<div class="article-summary-box-inner">
<span><p>In the scenario of unsupervised extractive summarization, learning
high-quality sentence representations is essential to select salient sentences
from the input document. Previous studies focus more on employing statistical
approaches or pre-trained language models (PLMs) to extract sentence
embeddings, while ignoring the rich information inherent in the heterogeneous
types of interaction between words and sentences. In this paper, we are the
first to propose an unsupervised extractive summarizaiton method with
heterogeneous graph embeddings (HGEs) for Chinese document. A heterogeneous
text graph is constructed to capture different granularities of interactions by
incorporating graph structural information. Moreover, our proposed graph is
general and flexible where additional nodes such as keywords can be easily
integrated. Experimental results demonstrate that our method consistently
outperforms the strong baseline in three summarization datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FF2: A Feature Fusion Two-Stream Framework for Punctuation Restoration. (arXiv:2211.04699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04699">
<div class="article-summary-box-inner">
<span><p>To accomplish punctuation restoration, most existing methods focus on
introducing extra information (e.g., part-of-speech) or addressing the class
imbalance problem. Recently, large-scale transformer-based pre-trained language
models (PLMS) have been utilized widely and obtained remarkable success.
However, the PLMS are trained on the large dataset with marks, which may not
fit well with the small dataset without marks, causing the convergence to be
not ideal. In this study, we propose a Feature Fusion two-stream framework
(FF2) to bridge the gap. Specifically, one stream leverages a pre-trained
language model to capture the semantic feature, while another auxiliary module
captures the feature at hand. We also modify the computation of multi-head
attention to encourage communication among heads. Then, two features with
different perspectives are aggregated to fuse information and enhance context
awareness. Without additional data, the experimental results on the popular
benchmark IWSLT demonstrate that FF2 achieves new SOTA performance, which
verifies that our approach is effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition. (arXiv:2211.04717v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04717">
<div class="article-summary-box-inner">
<span><p>Noisy Student Training (NST) has recently demonstrated extremely strong
performance in Automatic Speech Recognition (ASR). In this paper, we propose a
data selection strategy named LM Filter to improve the performances of NST on
non-target domain data in ASR tasks. Hypothesis with and without Language Model
are generated and CER differences between them are utilized as a filter
threshold. Results reveal that significant improvements of 10.4% compared with
no data filtering baselines. We can achieve 3.31% CER in AISHELL-1 test set,
which is best result from our knowledge without any other supervised data. We
also perform evaluations on supervised 1000 hour AISHELL-2 dataset and
competitive results of 4.72% CER can be achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Named Entity Recognition from Medical Texts: An Adaptive Shared Network Architecture with Attentive CRF. (arXiv:2211.04759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04759">
<div class="article-summary-box-inner">
<span><p>Recognizing useful named entities plays a vital role in medical information
processing, which helps drive the development of medical area research. Deep
learning methods have achieved good results in medical named entity recognition
(NER). However, we find that existing methods face great challenges when
dealing with the nested named entities. In this work, we propose a novel
method, referred to as ASAC, to solve the dilemma caused by the nested
phenomenon, in which the core idea is to model the dependency between different
categories of entity recognition. The proposed method contains two key modules:
the adaptive shared (AS) part and the attentive conditional random field (ACRF)
module. The former part automatically assigns adaptive weights across each task
to achieve optimal recognition accuracy in the multi-layer network. The latter
module employs the attention operation to model the dependency between
different entities. In this way, our model could learn better entity
representations by capturing the implicit distinctions and relationships
between different categories of entities. Extensive experiments on public
datasets verify the effectiveness of our method. Besides, we also perform
ablation analyses to deeply understand our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution-based Emotion Recognition in Conversation. (arXiv:2211.04834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04834">
<div class="article-summary-box-inner">
<span><p>Automatic emotion recognition in conversation (ERC) is crucial for
emotion-aware conversational artificial intelligence. This paper proposes a
distribution-based framework that formulates ERC as a sequence-to-sequence
problem for emotion distribution estimation. The inherent ambiguity of emotions
and the subjectivity of human perception lead to disagreements in emotion
labels, which is handled naturally in our framework from the perspective of
uncertainty estimation in emotion distributions. A Bayesian training loss is
introduced to improve the uncertainty estimation by conditioning each emotional
state on an utterance-specific Dirichlet prior distribution. Experimental
results on the IEMOCAP dataset show that ERC outperformed the
single-utterance-based system, and the proposed distribution-based ERC methods
have not only better classification accuracy, but also show improved
uncertainty estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token. (arXiv:2211.04898v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04898">
<div class="article-summary-box-inner">
<span><p>The pre-training of masked language models (MLMs) consumes massive
computation to achieve good results on downstream NLP tasks, resulting in a
large carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as
placeholders and gather the contextualized information from unmasked tokens to
restore the corrupted information. It raises the question of whether we can
append [MASK]s at a later layer, to reduce the sequence length for earlier
layers and make the pre-training more efficient. We show: (1) [MASK]s can
indeed be appended at a later layer, being disentangled from the word
embedding; (2) The gathering of contextualized information from unmasked tokens
can be conducted with a few layers. By further increasing the masking rate from
15% to 50%, we can pre-train RoBERTa-base and RoBERTa-large from scratch with
only 78% and 68% of the original computational budget without any degradation
on the GLUE benchmark. When pre-training with the original budget, our method
outperforms RoBERTa for 6 out of 8 GLUE tasks, on average by 0.4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novel Chapter Abstractive Summarization using Spinal Tree Aware Sub-Sentential Content Selection. (arXiv:2211.04903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04903">
<div class="article-summary-box-inner">
<span><p>Summarizing novel chapters is a difficult task due to the input length and
the fact that sentences that appear in the desired summaries draw content from
multiple places throughout the chapter. We present a pipelined
extractive-abstractive approach where the extractive step filters the content
that is passed to the abstractive component. Extremely lengthy input also
results in a highly skewed dataset towards negative instances for extractive
summarization; we thus adopt a margin ranking loss for extraction to encourage
separation between positive and negative examples. Our extraction component
operates at the constituent level; our approach to this problem enriches the
text with spinal tree information which provides syntactic context (in the form
of constituents) to the extraction model. We show an improvement of 3.71
Rouge-1 points over best results reported in prior work on an existing novel
chapter dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings. (arXiv:2211.04928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04928">
<div class="article-summary-box-inner">
<span><p>This paper presents miCSE, a mutual information-based Contrastive learning
framework that significantly advances the state-of-the-art in few-shot sentence
embedding. The proposed approach imposes alignment between the attention
pattern of different views during contrastive learning. Learning sentence
embeddings with miCSE entails enforcing the syntactic consistency across
augmented views for every single sentence, making contrastive self-supervised
learning more sample efficient. As a result, the proposed approach shows strong
performance in the few-shot learning domain. While it achieves superior results
compared to state-of-the-art methods on multiple benchmarks in few-shot
learning, it is comparable in the full-shot scenario. The proposed approach is
conceptually simple, easy to implement and optimize, yet empirically powerful.
This study opens up avenues for efficient self-supervised learning methods that
are more robust than current contrastive methods for sentence embedding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoSA : A System to Accelerate Annotations on Business Documents with Human-in-the-Loop. (arXiv:2211.04934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04934">
<div class="article-summary-box-inner">
<span><p>Business documents come in a variety of structures, formats and information
needs which makes information extraction a challenging task. Due to these
variations, having a document generic model which can work well across all
types of documents and for all the use cases seems far-fetched. For
document-specific models, we would need customized document-specific labels. We
introduce DoSA (Document Specific Automated Annotations), which helps
annotators in generating initial annotations automatically using our novel
bootstrap approach by leveraging document generic datasets and models. These
initial annotations can further be reviewed by a human for correctness. An
initial document-specific model can be trained and its inference can be used as
feedback for generating more automated annotations. These automated annotations
can be reviewed by human-in-the-loop for the correctness and a new improved
model can be trained using the current model as pre-trained model before going
for the next iteration. In this paper, our scope is limited to Form like
documents due to limited availability of generic annotated datasets, but this
idea can be extended to a variety of other documents as more datasets are
built. An open-source ready-to-use implementation is made available on GitHub
https://github.com/neeleshkshukla/DoSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Speech Translation with Pre-trained Models. (arXiv:2211.04939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04939">
<div class="article-summary-box-inner">
<span><p>When building state-of-the-art speech translation models, the need for large
computational resources is a significant obstacle due to the large training
data size and complex models. The availability of pre-trained models is a
promising opportunity to build strong speech translation systems efficiently.
In a first step, we investigate efficient strategies to build cascaded and
end-to-end speech translation systems based on pre-trained models. Using this
strategy, we can train and apply the models on a single GPU. While the
end-to-end models show superior translation performance to cascaded ones, the
application of this technology has a limitation on the need for additional
end-to-end training data. In a second step, we proposed an additional
similarity loss to encourage the model to generate similar hidden
representations for speech and transcript. Using this technique, we can
increase the data efficiency and improve the translation quality by 6 BLEU
points in scenarios with limited end-to-end training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating and Improving Context Attention Distribution on Multi-Turn Response Generation using Self-Contained Distractions. (arXiv:2211.04943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04943">
<div class="article-summary-box-inner">
<span><p>Despite the rapid progress of open-domain generation-based conversational
agents, most deployed systems treat dialogue contexts as single-turns, while
systems dealing with multi-turn contexts are less studied. There is a lack of a
reliable metric for evaluating multi-turn modelling, as well as an effective
solution for improving it. In this paper, we focus on an essential component of
multi-turn generation-based conversational agents: context attention
distribution, i.e. how systems distribute their attention on dialogue's
context. For evaluation of this component, We introduce a novel
attention-mechanism-based metric: DAS ratio. To improve performance on this
component, we propose an optimization strategy that employs self-contained
distractions. Our experiments on the Ubuntu chatlogs dataset show that models
with comparable perplexity can be distinguished by their ability on context
attention distribution. Our proposed optimization strategy improves both
non-hierarchical and hierarchical models on the proposed metric by about 10%
from baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accountable and Explainable Methods for Complex Reasoning over Text. (arXiv:2211.04946v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04946">
<div class="article-summary-box-inner">
<span><p>A major concern of Machine Learning (ML) models is their opacity. They are
deployed in an increasing number of applications where they often operate as
black boxes that do not provide explanations for their predictions. Among
others, the potential harms associated with the lack of understanding of the
models' rationales include privacy violations, adversarial manipulations, and
unfair discrimination. As a result, the accountability and transparency of ML
models have been posed as critical desiderata by works in policy and law,
philosophy, and computer science.
</p>
<p>In computer science, the decision-making process of ML models has been
studied by developing accountability and transparency methods. Accountability
methods, such as adversarial attacks and diagnostic datasets, expose
vulnerabilities of ML models that could lead to malicious manipulations or
systematic faults in their predictions. Transparency methods explain the
rationales behind models' predictions gaining the trust of relevant
stakeholders and potentially uncovering mistakes and unfairness in models'
decisions. To this end, transparency methods have to meet accountability
requirements as well, e.g., being robust and faithful to the underlying
rationales of a model.
</p>
<p>This thesis presents my research that expands our collective knowledge in the
areas of accountability and transparency of ML models developed for complex
reasoning tasks over text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions. (arXiv:2211.04971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.04971">
<div class="article-summary-box-inner">
<span><p>Image captioning models tend to describe images in an object-centric way,
emphasising visible objects. But image descriptions can also abstract away from
objects and describe the type of scene depicted. In this paper, we explore the
potential of a state-of-the-art Vision and Language model, VinVL, to caption
images at the scene level using (1) a novel dataset which pairs images with
both object-centric and scene descriptions. Through (2) an in-depth analysis of
the effect of the fine-tuning, we show (3) that a small amount of curated data
suffices to generate scene descriptions without losing the capability to
identify object-level concepts in the scene; the model acquires a more holistic
view of the image compared to when object-centric descriptions are generated.
We discuss the parallels between these results and insights from computational
and cognitive science research on scene perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discord Questions: A Computational Approach To Diversity Analysis in News Coverage. (arXiv:2211.05007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05007">
<div class="article-summary-box-inner">
<span><p>There are many potential benefits to news readers accessing diverse sources.
Modern news aggregators do the hard work of organizing the news, offering
readers a plethora of source options, but choosing which source to read remains
challenging. We propose a new framework to assist readers in identifying source
differences and gaining an understanding of news coverage diversity. The
framework is based on the generation of Discord Questions: questions with a
diverse answer pool, explicitly illustrating source differences. To assemble a
prototype of the framework, we focus on two components: (1) discord question
generation, the task of generating questions answered differently by sources,
for which we propose an automatic scoring method, and create a model that
improves performance from current question generation (QG) methods by 5%, (2)
answer consolidation, the task of grouping answers to a question that are
semantically similar, for which we collect data and repurpose a method that
achieves 81% balanced accuracy on our realistic test set. We illustrate the
framework's feasibility through a prototype interface. Even though model
performance at discord QG still lags human performance by more than 15%,
generated questions are judged to be more interesting than factoid questions
and can reveal differences in the level of detail, sentiment, and reasoning of
sources in news coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes. (arXiv:2211.05015v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05015">
<div class="article-summary-box-inner">
<span><p>Providing better language tools for low-resource and endangered languages is
imperative for equitable growth. Recent progress with massively multilingual
pretrained models has proven surprisingly effective at performing zero-shot
transfer to a wide variety of languages. However, this transfer is not
universal, with many languages not currently understood by multilingual
approaches. It is estimated that only 72 languages possess a "small set of
labeled datasets" on which we could test a model's performance, the vast
majority of languages not having the resources available to simply evaluate
performances on. In this work, we attempt to clarify which languages do and do
not currently benefit from such transfer. To that end, we develop a general
approach that requires only unlabelled text to detect which languages are not
well understood by a cross-lingual model. Our approach is derived from the
hypothesis that if a model's understanding is insensitive to perturbations to
text in a language, it is likely to have a limited understanding of that
language. We construct a cross-lingual sentence similarity task to evaluate our
approach empirically on 350, primarily low-resource, languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Structure Matters Most in Most Languages. (arXiv:2211.05025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05025">
<div class="article-summary-box-inner">
<span><p>Many recent perturbation studies have found unintuitive results on what does
and does not matter when performing Natural Language Understanding (NLU) tasks
in English. Coding properties, such as the order of words, can often be removed
through shuffling without impacting downstream performances. Such insight may
be used to direct future research into English NLP models. As many improvements
in multilingual settings consist of wholesale adaptation of English approaches,
it is important to verify whether those studies replicate or not in
multilingual settings. In this work, we replicate a study on the importance of
local structure, and the relative unimportance of global structure, in a
multilingual setting. We find that the phenomenon observed on the English
language broadly translates to over 120 languages, with a few caveats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers. (arXiv:2211.05030v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05030">
<div class="article-summary-box-inner">
<span><p>Recent developments in natural language generation (NLG) using neural
language models have brought us closer than ever to the goal of building
AI-powered creative writing tools. However, most prior work on human-AI
collaboration in the creative writing domain has evaluated new systems with
amateur writers, typically in contrived user studies of limited scope. In this
work, we commissioned 13 professional, published writers from a diverse set of
creative writing backgrounds to craft stories using Wordcraft, a text editor
with built-in AI-powered writing assistance tools. Using interviews and
participant journals, we discuss the potential of NLG to have significant
impact in the creative writing domain--especially with respect to
brainstorming, generation of story details, world-building, and research
assistance. Experienced writers, more so than amateurs, typically have
well-developed systems and methodologies for writing, as well as distinctive
voices and target audiences. Our work highlights the challenges in building for
these writers; NLG technologies struggle to preserve style and authorial voice,
and they lack deep understanding of story contents. In order for AI-powered
writing assistants to realize their full potential, it is essential that they
take into account the diverse goals and expertise of human writers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Performance of Automatic Keyword Extraction (AKE) Methods Using PoS-Tagging and Enhanced Semantic-Awareness. (arXiv:2211.05031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05031">
<div class="article-summary-box-inner">
<span><p>Automatic keyword extraction (AKE) has gained more importance with the
increasing amount of digital textual data that modern computing systems
process. It has various applications in information retrieval (IR) and natural
language processing (NLP), including text summarisation, topic analysis and
document indexing. This paper proposes a simple but effective
post-processing-based universal approach to improve the performance of any AKE
methods, via an enhanced level of semantic-awareness supported by PoS-tagging.
To demonstrate the performance of the proposed approach, we considered word
types retrieved from a PoS-tagging step and two representative sources of
semantic information -- specialised terms defined in one or more
context-dependent thesauri, and named entities in Wikipedia. The above three
steps can be simply added to the end of any AKE methods as part of a
post-processor, which simply re-evaluate all candidate keywords following some
context-specific and semantic-aware criteria. For five state-of-the-art (SOTA)
AKE methods, our experimental results with 17 selected datasets showed that the
proposed approach improved their performances both consistently (up to 100\% in
terms of improved cases) and significantly (between 10.2\% and 53.8\%, with an
average of 25.8\%, in terms of F1-score and across all five methods),
especially when all the three enhancement steps are used. Our results have
profound implications considering the ease to apply our proposed approach to
any AKE methods and to further extend it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Contrastive Learning and Knowledge Graph Embeddings to develop medical word embeddings for the Italian language. (arXiv:2211.05035v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05035">
<div class="article-summary-box-inner">
<span><p>Word embeddings play a significant role in today's Natural Language
Processing tasks and applications. While pre-trained models may be directly
employed and integrated into existing pipelines, they are often fine-tuned to
better fit with specific languages or domains. In this paper, we attempt to
improve available embeddings in the uncovered niche of the Italian medical
domain through the combination of Contrastive Learning (CL) and Knowledge Graph
Embedding (KGE). The main objective is to improve the accuracy of semantic
similarity between medical terms, which is also used as an evaluation task.
Since the Italian language lacks medical texts and controlled vocabularies, we
have developed a specific solution by combining preexisting CL methods
(multi-similarity loss, contextualization, dynamic sampling) and the
integration of KGEs, creating a new variant of the loss. Although without
having outperformed the state-of-the-art, represented by multilingual models,
the obtained results are encouraging, providing a significant leap in
performance compared to the starting model, while using a significantly lower
amount of data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MACSum: Controllable Summarization with Mixed Attributes. (arXiv:2211.05041v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05041">
<div class="article-summary-box-inner">
<span><p>Controllable summarization allows users to generate customized summaries with
specified attributes. However, due to the lack of designated annotations of
controlled summaries, existing works have to craft pseudo datasets by adapting
generic summarization benchmarks. Furthermore, most research focuses on
controlling single attributes individually (e.g., a short summary or a highly
abstractive summary) rather than controlling a mix of attributes together
(e.g., a short and highly abstractive summary). In this paper, we propose
MACSum, the first human-annotated summarization dataset for controlling mixed
attributes. It contains source texts from two domains, news articles and
dialogues, with human-annotated summaries controlled by five designed
attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We
propose two simple and effective parameter-efficient approaches for the new
task of mixed controllable summarization based on hard prompt tuning and soft
prefix tuning. Results and analysis demonstrate that hard prompt models yield
the best performance on all metrics and human evaluations. However,
mixed-attribute control is still challenging for summarization tasks. Our
dataset and code are available at https://github.com/psunlpgroup/MACSum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is Wrong with Language Models that Can Not Tell a Story?. (arXiv:2211.05044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05044">
<div class="article-summary-box-inner">
<span><p>This paper argues that a deeper understanding of narrative and the successful
generation of longer subjectively interesting texts is a vital bottleneck that
hinders the progress in modern Natural Language Processing (NLP) and may even
be in the whole field of Artificial Intelligence. We demonstrate that there are
no adequate datasets, evaluation methods, and even operational concepts that
could be used to start working on narrative processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Transfer Learning for Check-worthy Claim Identification over Twitter. (arXiv:2211.05087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05087">
<div class="article-summary-box-inner">
<span><p>Misinformation spread over social media has become an undeniable infodemic.
However, not all spreading claims are made equal. If propagated, some claims
can be destructive, not only on the individual level, but to organizations and
even countries. Detecting claims that should be prioritized for fact-checking
is considered the first step to fight against spread of fake news. With
training data limited to a handful of languages, developing supervised models
to tackle the problem over lower-resource languages is currently infeasible.
Therefore, our work aims to investigate whether we can use existing datasets to
train models for predicting worthiness of verification of claims in tweets in
other languages. We present a systematic comparative study of six approaches
for cross-lingual check-worthiness estimation across pairs of five diverse
languages with the help of Multilingual BERT (mBERT) model. We run our
experiments using a state-of-the-art multilingual Twitter dataset. Our results
show that for some language pairs, zero-shot cross-lingual transfer is possible
and can perform as good as monolingual models that are trained on the target
language. We also show that in some languages, this approach outperforms (or at
least is comparable to) state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05100">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Scaling Transformer Inference. (arXiv:2211.05102v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05102">
<div class="article-summary-box-inner">
<span><p>We study the problem of efficient generative inference for Transformer
models, in one of its most challenging settings: large deep models, with tight
latency targets and long sequence lengths. Better understanding of the
engineering tradeoffs for inference for large Transformer-based models is
important as use cases of these models are growing rapidly throughout
application areas. We develop a simple analytical model for inference
efficiency to select the best multi-dimensional partitioning techniques
optimized for TPU v4 slices based on the application requirements. We combine
these with a suite of low-level optimizations to achieve a new Pareto frontier
on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter
models that outperforms the FasterTransformer suite of benchmarks. We further
show that with appropriate partitioning, the lower memory requirements of
multiquery attention (i.e. multiple query heads share single key/value head)
enables scaling up to 32x larger context lengths. Finally, we achieve a
low-batch-size latency of 29ms per token during generation (using int8 weight
quantization) and a 76% MFU during large-batch-size processing of input tokens,
while supporting a long 2048-token context length on the PaLM 540B parameter
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05103">
<div class="article-summary-box-inner">
<span><p>In this paper, we extend previous self-supervised approaches for language
identification by experimenting with Conformer based architecture in a
multilingual pre-training paradigm. We find that pre-trained speech models
optimally encode language discriminatory information in lower layers. Further,
we demonstrate that the embeddings obtained from these layers are significantly
robust to classify unseen languages and different acoustic environments without
additional training. After fine-tuning a pre-trained Conformer model on the
VoxLingua107 dataset, we achieve results similar to current state-of-the-art
systems for language identification. More, our model accomplishes this with 5x
less parameters. We open-source the model through the NVIDIA NeMo toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models with Controllable Working Memory. (arXiv:2211.05110v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05110">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have led to a series of breakthroughs in natural
language processing (NLP), owing to their excellent understanding and
generation abilities. Remarkably, what further sets these models apart is the
massive amounts of world knowledge they internalize during pretraining. While
many downstream applications provide the model with an informational context to
aid its performance on the underlying task, how the model's world knowledge
interacts with the factual information presented in the context remains under
explored. As a desirable behavior, an LLM should give precedence to the context
whenever it contains task-relevant information that conflicts with the model's
memorized knowledge. This enables model predictions to be grounded in the
context, which can then be used to update or correct specific model predictions
without frequent retraining. By contrast, when the context is irrelevant to the
task, the model should ignore it and fall back on its internal knowledge. In
this paper, we undertake a first joint study of the aforementioned two
properties, namely controllability and robustness, in the context of LLMs. We
demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)
could exhibit poor controllability and robustness, which do not scale with
increasing model size. As a solution, we propose a novel method - Knowledge
Aware FineTuning (KAFT) - to strengthen both controllability and robustness by
incorporating counterfactual and irrelevant contexts to standard supervised
datasets. Our comprehensive evaluation showcases the utility of KAFT across
model architectures and sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The challenges of temporal alignment on Twitter during crises. (arXiv:2104.08535v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08535">
<div class="article-summary-box-inner">
<span><p>Language use changes over time, and this impacts the effectiveness of NLP
systems. This phenomenon is even more prevalent in social media data during
crisis events where meaning and frequency of word usage may change over the
course of days. Contextual language models fail to adapt temporally,
emphasizing the need for temporal adaptation in models which need to be
deployed over an extended period of time. While existing approaches consider
data spanning large periods of time (from years to decades), shorter time spans
are critical for crisis data. We quantify temporal degradation for this
scenario and propose methods to cope with performance loss by leveraging
techniques from domain adaptation. To the best of our knowledge, this is the
first effort to explore effects of rapid language change driven by adversarial
adaptations, particularly during natural and human-induced disasters. Through
extensive experimentation on diverse crisis datasets, we analyze under what
conditions our approaches outperform strong baselines while highlighting the
current limitations of temporal adaptation methods in scenarios where access to
unlabeled data is scarce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering. (arXiv:2109.06122v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06122">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) is challenging not only because the model has
to handle multi-modal information, but also because it is just so hard to
collect sufficient training examples -- there are too many questions one can
ask about an image. As a result, a VQA model trained solely on human-annotated
examples could easily over-fit specific question styles or image contents that
are being asked, leaving the model largely ignorant about the sheer diversity
of questions. Existing methods address this issue primarily by introducing an
auxiliary task such as visual grounding, cycle consistency, or debiasing. In
this paper, we take a drastically different approach. We found that many of the
"unknowns" to the learned VQA model are indeed "known" in the dataset
implicitly. For instance, questions asking about the same object in different
images are likely paraphrases; the number of detected or annotated objects in
an image already provides the answer to the "how many" question, even if the
question has not been annotated for that image. Building upon these insights,
we present a simple data augmentation pipeline SimpleAug to turn this "known"
knowledge into training examples for VQA. We show that these augmented examples
can notably improve the learned VQA models' performance, not only on the VQA-CP
dataset with language prior shifts but also on the VQA v2 dataset without such
shifts. Our method further opens up the door to leverage weakly-labeled or
unlabeled images in a principled way to enhance VQA models. Our code and data
are publicly available at https://github.com/heendung/simpleAUG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. (arXiv:2111.07367v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07367">
<div class="article-summary-box-inner">
<span><p>Feature attribution a.k.a. input salience methods which assign an importance
score to a feature are abundant but may produce surprisingly different results
for the same model on the same input. While differences are expected if
disparate definitions of importance are assumed, most methods claim to provide
faithful attributions and point at the features most relevant for a model's
prediction. Existing work on faithfulness evaluation is not conclusive and does
not provide a clear answer as to how different methods are to be compared.
Focusing on text classification and the model debugging scenario, our main
contribution is a protocol for faithfulness evaluation that makes use of
partially synthetic data to obtain ground truth for feature importance ranking.
Following the protocol, we do an in-depth analysis of four standard salience
method classes on a range of datasets and shortcuts for BERT and LSTM models
and demonstrate that some of the most popular method configurations provide
poor results even for simplest shortcuts. We recommend following the protocol
for each new task and model combination to find the best method for identifying
shortcuts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-MELD: A Multilingual Multi-Party Dataset for Emotion Recognition in Conversations. (arXiv:2203.16799v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16799">
<div class="article-summary-box-inner">
<span><p>Expression of emotions is a crucial part of daily human communication.
Emotion recognition in conversations (ERC) is an emerging field of study, where
the primary task is to identify the emotion behind each utterance in a
conversation. Though a lot of work has been done on ERC in the past, these
works only focus on ERC in the English language, thereby ignoring any other
languages. In this paper, we present Multilingual MELD (M-MELD), where we
extend the Multimodal EmotionLines Dataset (MELD) \cite{poria2018meld} to 4
other languages beyond English, namely Greek, Polish, French, and Spanish.
Beyond just establishing strong baselines for all of these 4 languages, we also
propose a novel architecture, DiscLSTM, that uses both sequential and
conversational discourse context in a conversational dialogue for ERC. Our
proposed approach is computationally efficient, can transfer across languages
using just a cross-lingual encoder, and achieves better performance than most
uni-modal text approaches in the literature on both MELD and M-MELD. We make
our data and code publicly on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets. (arXiv:2205.06871v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06871">
<div class="article-summary-box-inner">
<span><p>Precisely assessing the progress in natural language generation (NLG) tasks
is challenging, and human evaluation to establish a preference in a model's
output over another is often necessary. However, human evaluation is usually
costly, difficult to reproduce, and non-reusable. In this paper, we propose a
new and simple automatic evaluation method for NLG called Near-Negative
Distinction (NND) that repurposes prior human annotations into NND tests. In an
NND test, an NLG model must place a higher likelihood on a high-quality output
candidate than on a near-negative candidate with a known error. Model
performance is established by the number of NND tests a model passes, as well
as the distribution over task-specific errors the model fails on. Through
experiments on three NLG tasks (question generation, question answering, and
summarization), we show that NND achieves a higher correlation with human
judgments than standard NLG evaluation metrics. We then illustrate NND
evaluation in four practical scenarios, for example performing fine-grain model
analysis, or studying model training dynamics. Our findings suggest that NND
can give a second life to human annotations and provide low-cost NLG
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Conversational Systems: A Review of Current Advances, Gaps, and Opportunities. (arXiv:2206.05017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05017">
<div class="article-summary-box-inner">
<span><p>Empathy is a vital factor that contributes to mutual understanding, and joint
problem-solving. In recent years, a growing number of studies have recognized
the benefits of empathy and started to incorporate empathy in conversational
systems. We refer to this topic as empathetic conversational systems. To
identify the critical gaps and future opportunities in this topic, this paper
examines this rapidly growing field using five review dimensions: (i)
conceptual empathy models and frameworks, (ii) adopted empathy-related
concepts, (iii) datasets and algorithmic techniques developed, (iv) evaluation
strategies, and (v) state-of-the-art approaches. The findings show that most
studies have centered on the use of the EMPATHETICDIALOGUES dataset, and the
text-based modality dominates research in this field. Studies mainly focused on
extracting features from the messages of the users and the conversational
systems, with minimal emphasis on user modeling and profiling. Notably, studies
that have incorporated emotion causes, external knowledge, and affect matching
in the response generation models, have obtained significantly better results.
For implementation in diverse real-world settings, we recommend that future
studies should address key gaps in areas of detecting and authenticating
emotions at the entity level, handling multimodal inputs, displaying more
nuanced empathetic behaviors, and encompassing additional dialogue system
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automatic Evaluation of the WMT22 General Machine Translation Task. (arXiv:2209.14172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14172">
<div class="article-summary-box-inner">
<span><p>This report presents an automatic evaluation of the general machine
translation task of the Seventh Conference on Machine Translation (WMT22). It
evaluates a total of 185 systems for 21 translation directions including
high-resource to low-resource language pairs and from closely related to
distant languages. This large-scale automatic evaluation highlights some of the
current limits of state-of-the-art machine translation systems. It also shows
how automatic metrics, namely chrF, BLEU, and COMET, can complement themselves
to mitigate their own limits in terms of interpretability and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code-Switching without Switching: Language Agnostic End-to-End Speech Translation. (arXiv:2210.01512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01512">
<div class="article-summary-box-inner">
<span><p>We propose a) a Language Agnostic end-to-end Speech Translation model (LAST),
and b) a data augmentation strategy to increase code-switching (CS)
performance. With increasing globalization, multiple languages are increasingly
used interchangeably during fluent speech. Such CS complicates traditional
speech recognition and translation, as we must recognize which language was
spoken first and then apply a language-dependent recognizer and subsequent
translation component to generate the desired target language output. Such a
pipeline introduces latency and errors. In this paper, we eliminate the need
for that, by treating speech recognition and translation as one unified
end-to-end speech translation problem. By training LAST with both input
languages, we decode speech into one target language, regardless of the input
language. LAST delivers comparable recognition and speech translation accuracy
in monolingual usage, while reducing latency and error rate considerably when
CS is observed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Causal Analysis of Mental Health on Social Media Data. (arXiv:2210.08430v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08430">
<div class="article-summary-box-inner">
<span><p>With recent developments in Social Computing, Natural Language Processing and
Clinical Psychology, the social NLP research community addresses the challenge
of automation in mental illness on social media. A recent extension to the
problem of multi-class classification of mental health issues is to identify
the cause behind the user's intention. However, multi-class causal
categorization for mental health issues on social media has a major challenge
of wrong prediction due to the overlapping problem of causal explanations.
There are two possible mitigation techniques to solve this problem: (i)
Inconsistency among causal explanations/ inappropriate human-annotated
inferences in the dataset, (ii) in-depth analysis of arguments and stances in
self-reported text using discourse analysis. In this research work, we
hypothesise that if there exists the inconsistency among F1 scores of different
classes, there must be inconsistency among corresponding causal explanations as
well. In this task, we fine tune the classifiers and find explanations for
multi-class causal categorization of mental illness on social media with LIME
and Integrated Gradient (IG) methods. We test our methods with CAMS dataset and
validate with annotated interpretations. A key contribution of this research
work is to find the reason behind inconsistency in accuracy of multi-class
causal categorization. The effectiveness of our methods is evident with the
results obtained having category-wise average scores of $81.29 \%$ and $0.906$
using cosine similarity and word mover's distance, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Too Brittle To Touch: Comparing the Stability of Quantization and Distillation Towards Developing Lightweight Low-Resource MT Models. (arXiv:2210.15184v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15184">
<div class="article-summary-box-inner">
<span><p>Leveraging shared learning through Massively Multilingual Models,
state-of-the-art machine translation models are often able to adapt to the
paucity of data for low-resource languages. However, this performance comes at
the cost of significantly bloated models which are not practically deployable.
Knowledge Distillation is one popular technique to develop competitive,
lightweight models: In this work, we first evaluate its use to compress MT
models focusing on languages with extremely limited training data. Through our
analysis across 8 languages, we find that the variance in the performance of
the distilled models due to their dependence on priors including the amount of
synthetic data used for distillation, the student architecture, training
hyperparameters and confidence of the teacher models, makes distillation a
brittle compression mechanism. To mitigate this, we explore the use of
post-training quantization for the compression of these models. Here, we find
that while distillation provides gains across some low-resource languages,
quantization provides more consistent performance trends for the entire range
of languages, especially the lowest-resource languages in our target set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DORE: Document Ordered Relation Extraction based on Generative Framework. (arXiv:2210.16064v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16064">
<div class="article-summary-box-inner">
<span><p>In recent years, there is a surge of generation-based information extraction
work, which allows a more direct use of pre-trained language models and
efficiently captures output dependencies. However, previous generative methods
using lexical representation do not naturally fit document-level relation
extraction (DocRE) where there are multiple entities and relational facts. In
this paper, we investigate the root cause of the underwhelming performance of
the existing generative DocRE models and discover that the culprit is the
inadequacy of the training paradigm, instead of the capacities of the models.
We propose to generate a symbolic and ordered sequence from the relation matrix
which is deterministic and easier for model to learn. Moreover, we design a
parallel row generation method to process overlong target sequences. Besides,
we introduce several negative sampling strategies to improve the performance
with balanced signals. Experimental results on four datasets show that our
proposed method can improve the performance of the generative DocRE models. We
have released our code at https://github.com/ayyyq/DORE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User-Entity Differential Privacy in Learning Natural Language Models. (arXiv:2211.01141v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01141">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel concept of user-entity differential
privacy (UeDP) to provide formal privacy protection simultaneously to both
sensitive entities in textual data and data owners in learning natural language
models (NLMs). To preserve UeDP, we developed a novel algorithm, called
UeDP-Alg, optimizing the trade-off between privacy loss and model utility with
a tight sensitivity bound derived from seamlessly combining user and sensitive
entity sampling processes. An extensive theoretical analysis and evaluation
show that our UeDP-Alg outperforms baseline approaches in model utility under
the same privacy budget consumption on several NLM tasks, using benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-Shot Code-Switched Speech Recognition. (arXiv:2211.01458v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01458">
<div class="article-summary-box-inner">
<span><p>In this work, we seek to build effective code-switched (CS) automatic speech
recognition systems (ASR) under the zero-shot setting where no transcribed CS
speech data is available for training. Previously proposed frameworks which
conditionally factorize the bilingual task into its constituent monolingual
parts are a promising starting point for leveraging monolingual data
efficiently. However, these methods require the monolingual modules to perform
language segmentation. That is, each monolingual module has to simultaneously
detect CS points and transcribe speech segments of one language while ignoring
those of other languages -- not a trivial task. We propose to simplify each
monolingual module by allowing them to transcribe all speech segments
indiscriminately with a monolingual script (i.e. transliteration). This simple
modification passes the responsibility of CS point detection to subsequent
bilingual modules which determine the final output by considering multiple
monolingual transliterations along with external language model information. We
apply this transliteration-based approach in an end-to-end differentiable
neural network and demonstrate its efficacy for zero-shot CS ASR on
Mandarin-English SEAME test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid-SD (H_SD): A new hybrid evaluation metric for automatic speech recognition tasks. (arXiv:2211.01722v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01722">
<div class="article-summary-box-inner">
<span><p>Many studies have examined the shortcomings of word error rate (WER) as an
evaluation metric for automatic speech recognition (ASR) systems, particularly
when used for spoken language understanding tasks such as intent recognition
and dialogue systems. In this paper, we propose Hybrid-SD (H_SD), a new hybrid
evaluation metric for ASR systems that takes into account both semantic
correctness and error rate. To generate sentence dissimilarity scores (SD), we
built a fast and lightweight SNanoBERT model using distillation techniques. Our
experiments show that the SNanoBERT model is 25.9x smaller and 38.8x faster
than SRoBERTa while achieving comparable results on well-known benchmarks.
Hence, making it suitable for deploying with ASR models on edge devices. We
also show that H_SD correlates more strongly with downstream tasks such as
intent recognition and named-entity recognition (NER).
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-10 23:16:30.859450367 UTC">2022-11-10 23:16:30 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>