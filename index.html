<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-20T01:30:00Z">10-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Alibaba-Translate China's Submission for WMT 2022 Quality Estimation Shared Task. (arXiv:2210.10049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10049">
<div class="article-summary-box-inner">
<span><p>In this paper, we present our submission to the sentence-level MQM benchmark
at Quality Estimation Shared Task, named UniTE (Unified Translation
Evaluation). Specifically, our systems employ the framework of UniTE, which
combined three types of input formats during training with a pre-trained
language model. First, we apply the pseudo-labeled data examples for the
continuously pre-training phase. Notably, to reduce the gap between
pre-training and fine-tuning, we use data pruning and a ranking-based score
normalization strategy. For the fine-tuning phase, we use both Direct
Assessment (DA) and Multidimensional Quality Metrics (MQM) data from past
years' WMT competitions. Finally, we collect the source-only evaluation
results, and ensemble the predictions generated by two UniTE models, whose
backbones are XLM-R and InfoXLM, respectively. Results show that our models
reach 1st overall ranking in the Multilingual and English-Russian settings, and
2nd overall ranking in English-German and Chinese-English settings, showing
relatively strong performances in this year's quality estimation competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and analyzing missing citations to published scientific entities. (arXiv:2210.10073v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10073">
<div class="article-summary-box-inner">
<span><p>Proper citation is of great importance in academic writing for it enables
knowledge accumulation and maintains academic integrity. However, citing
properly is not an easy task. For published scientific entities, the
ever-growing academic publications and over-familiarity of terms easily lead to
missing citations. To deal with this situation, we design a special method
Citation Recommendation for Published Scientific Entity (CRPSE) based on the
cooccurrences between published scientific entities and in-text citations in
the same sentences from previous researchers. Experimental outcomes show the
effectiveness of our method in recommending the source papers for published
scientific entities. We further conduct a statistical analysis on missing
citations among papers published in prestigious computer science conferences in
2020. In the 12,278 papers collected, 475 published scientific entities of
computer science and mathematics are found to have missing citations. Many
entities mentioned without citations are found to be well-accepted research
results. On a median basis, the papers proposing these published scientific
entities with missing citations were published 8 years ago, which can be
considered the time frame for a published scientific entity to develop into a
well-accepted concept. For published scientific entities, we appeal for
accurate and full citation of their source papers as required by academic
standards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler. (arXiv:2210.10105v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10105">
<div class="article-summary-box-inner">
<span><p>Numerical reasoning over text is a challenging task of Artificial
Intelligence (AI), requiring reading comprehension and numerical reasoning
abilities. Previous approaches use numerical reasoning programs to represent
the reasoning process. However, most works do not separate the generation of
operators and operands, which are key components of a numerical reasoning
program, thus limiting their ability to generate such programs for complicated
tasks. In this paper, we introduce the numEricaL reASoning with adapTive
symbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the
Encoder and a Compiler with four modules: Reasoning Manager, Operator
Generator, Operands Generator, and Memory Register. ELASTIC is robust when
conducting complicated reasoning. Also, it is domain agnostic by supporting the
expansion of diverse operators without caring about the number of operands it
contains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution
accuracy and program accuracy on the FinQA dataset and 83.00 program accuracy
on the MathQA dataset, outperforming previous state-of-the-art models
significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Active Learning for Natural Language Processing. (arXiv:2210.10109v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10109">
<div class="article-summary-box-inner">
<span><p>In this work, we provide a survey of active learning (AL) for its
applications in natural language processing (NLP). In addition to a
fine-grained categorization of query strategies, we also investigate several
other important aspects of applying AL to NLP problems. These include AL for
structured prediction tasks, annotation cost, model learning (especially with
deep neural models), and starting and stopping AL. Finally, we conclude with a
discussion of related topics and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs. (arXiv:2210.10144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10144">
<div class="article-summary-box-inner">
<span><p>The extraction of aspect terms is a critical step in fine-grained sentiment
analysis of text. Existing approaches for this task have yielded impressive
results when the training and testing data are from the same domain. However,
these methods show a drastic decrease in performance when applied to
cross-domain settings where the domain of the testing data differs from that of
the training data. To address this lack of extensibility and robustness, we
propose a novel approach for automatically constructing domain-specific
knowledge graphs that contain information relevant to the identification of
aspect terms. We introduce a methodology for injecting information from these
knowledge graphs into Transformer models, including two alternative mechanisms
for knowledge insertion: via query enrichment and via manipulation of attention
patterns. We demonstrate state-of-the-art performance on benchmark datasets for
cross-domain aspect term extraction using our approach and investigate how the
amount of external knowledge available to the Transformer impacts model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedCLIP: Contrastive Learning from Unpaired Medical Images and Text. (arXiv:2210.10163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10163">
<div class="article-summary-box-inner">
<span><p>Existing vision-text contrastive learning like CLIP aims to match the paired
image and caption embeddings while pushing others apart, which improves
representation transferability and supports zero-shot prediction. However,
medical image-text datasets are orders of magnitude below the general images
and captions from the internet. Moreover, previous methods encounter many false
negatives, i.e., images and reports from separate patients probably carry the
same semantics but are wrongly treated as negatives. In this paper, we decouple
images and texts for multimodal contrastive learning thus scaling the usable
training data in a combinatorial magnitude with low cost. We also propose to
replace the InfoNCE loss with semantic matching loss based on medical knowledge
to eliminate false negatives in contrastive learning. We prove that MedCLIP is
a simple yet effective framework: it outperforms state-of-the-art methods on
zero-shot prediction, supervised classification, and image-text retrieval.
Surprisingly, we observe that with only 20K pre-training data, MedCLIP wins
over the state-of-the-art method (using around 200K data). Our code is
available at https://github.com/RyanWangZf/MedCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering. (arXiv:2210.10176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10176">
<div class="article-summary-box-inner">
<span><p>Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a
two-stage framework that first retrieves external knowledge given the visual
question and then predicts the answer based on the retrieved content. However,
the retrieved knowledge is often inadequate. Retrievals are frequently too
general and fail to cover specific knowledge needed to answer the question.
Also, the naturally available supervision (whether the passage contains the
correct answer) is weak and does not guarantee question relevancy. To address
these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that
provides stronger supervision during training and recognizes question-relevant
entities to help retrieve more specific knowledge. Experiments show that our
EnFoRe model achieves superior retrieval performance on OK-VQA, the currently
largest outside-knowledge VQA dataset. We also combine the retrieved knowledge
with state-of-the-art VQA models, and achieve a new state-of-the-art
performance on OK-VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Unsupervised Speech Translation. (arXiv:2210.10191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10191">
<div class="article-summary-box-inner">
<span><p>The amount of labeled data to train models for speech tasks is limited for
most languages, however, the data scarcity is exacerbated for speech
translation which requires labeled data covering two different languages. To
address this issue, we study a simple and effective approach to build speech
translation systems without labeled data by leveraging recent advances in
unsupervised speech recognition, machine translation and speech synthesis,
either in a pipeline approach, or to generate pseudo-labels for training
end-to-end speech translation models. Furthermore, we present an unsupervised
domain adaptation technique for pre-trained speech models which improves the
performance of downstream unsupervised speech recognition, especially for
low-resource settings. Experiments show that unsupervised speech-to-text
translation outperforms the previous unsupervised state of the art by 3.2 BLEU
on the Libri-Trans benchmark, on CoVoST 2, our best systems outperform the best
supervised end-to-end models (without pre-training) from only two years ago by
an average of 5.0 BLEU over five X-En directions. We also report competitive
results on MuST-C and CVSS benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation. (arXiv:2210.10200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10200">
<div class="article-summary-box-inner">
<span><p>If one sees the place name Houston Mercer Dog Run in New York, how does one
know how to pronounce it? Assuming one knows that Houston in New York is
pronounced "how-ston" and not like the Texas city, then one can probably guess
that "how-ston" is also used in the name of the dog park. We present a novel
architecture that learns to use the pronunciations of neighboring names in
order to guess the pronunciation of a given target feature. Applied to Japanese
place names, we demonstrate the utility of the model to finding and proposing
corrections for errors in Google Maps.
</p>
<p>To demonstrate the utility of this approach to structurally similar problems,
we also report on an application to a totally different task: Cognate reflex
prediction in comparative historical linguistics. A version of the code has
been open-sourced
(https://github.com/google-research/google-research/tree/master/cognate_inpaint_neighbors).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10209">
<div class="article-summary-box-inner">
<span><p>Continual Learning (CL) methods mainly focus on avoiding catastrophic
forgetting and learning representations that are transferable to new tasks.
Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a
randomly initialized, fixed base network (model) and finds a supermask for each
new task that selectively keeps or removes each weight to produce a subnetwork.
They prevent forgetting as the network weights are not being updated. Although
there is no forgetting, the performance of the supermask is sub-optimal because
fixed weights restrict its representational power. Furthermore, there is no
accumulation or transfer of knowledge inside the model when new tasks are
learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training),
which performs exclusive and non-overlapping subnetwork weight training. This
avoids conflicting updates to the shared weights by subsequent tasks to improve
performance while still preventing forgetting. Furthermore, we propose a novel
KNN-based Knowledge Transfer (KKT) module that dynamically initializes a new
task's mask based on previous tasks for improving knowledge transfer. We
demonstrate that ExSSNeT outperforms SupSup and other strong previous methods
on both text classification and vision tasks while preventing forgetting.
Moreover, ExSSNeT is particularly advantageous for sparse masks that activate
2-10% of the model parameters, resulting in an average improvement of 8.3% over
SupSup. Additionally, ExSSNeT scales to a large number of tasks (100), and our
KKT module helps to learn new tasks faster while improving overall performance.
Our code is available at https://github.com/prateeky2806/exessnet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Slot Type Attentions to Improve Joint Intent Detection and Slot Filling. (arXiv:2210.10227v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10227">
<div class="article-summary-box-inner">
<span><p>Joint intent detection and slot filling is a key research topic in natural
language understanding (NLU). Existing joint intent and slot filling systems
analyze and compute features collectively for all slot types, and importantly,
have no way to explain the slot filling model decisions. In this work, we
propose a novel approach that: (i) learns to generate additional slot type
specific features in order to improve accuracy and (ii) provides explanations
for slot filling decisions for the first time in a joint NLU model. We perform
an additional constrained supervision using a set of binary classifiers for the
slot type specific feature learning, thus ensuring appropriate attention
weights are learned in the process to explain slot filling decisions for
utterances. Our model is inherently explainable and does not need any post-hoc
processing. We evaluate our approach on two widely used datasets and show
accuracy improvements. Moreover, a detailed analysis is also provided for the
exclusive slot explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker- and Age-Invariant Training for Child Acoustic Modeling Using Adversarial Multi-Task Learning. (arXiv:2210.10231v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10231">
<div class="article-summary-box-inner">
<span><p>One of the major challenges in acoustic modelling of child speech is the
rapid changes that occur in the children's articulators as they grow up, their
differing growth rates and the subsequent high variability in the same age
group. These high acoustic variations along with the scarcity of child speech
corpora have impeded the development of a reliable speech recognition system
for children. In this paper, a speaker- and age-invariant training approach
based on adversarial multi-task learning is proposed. The system consists of
one generator shared network that learns to generate speaker- and age-invariant
features connected to three discrimination networks, for phoneme, age, and
speaker. The generator network is trained to minimize the
phoneme-discrimination loss and maximize the speaker- and age-discrimination
losses in an adversarial multi-task learning fashion. The generator network is
a Time Delay Neural Network (TDNN) architecture while the three discriminators
are feed-forward networks. The system was applied to the OGI speech corpora and
achieved a 13% reduction in the WER of the ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Type-supervised sequence labeling based on the heterogeneous star graph for named entity recognition. (arXiv:2210.10240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10240">
<div class="article-summary-box-inner">
<span><p>Named entity recognition is a fundamental task in natural language
processing, identifying the span and category of entities in unstructured
texts. The traditional sequence labeling methodology ignores the nested
entities, i.e. entities included in other entity mentions. Many approaches
attempt to address this scenario, most of which rely on complex structures or
have high computation complexity. The representation learning of the
heterogeneous star graph containing text nodes and type nodes is investigated
in this paper. In addition, we revise the graph attention mechanism into a
hybrid form to address its unreasonableness in specific topologies. The model
performs the type-supervised sequence labeling after updating nodes in the
graph. The annotation scheme is an extension of the single-layer sequence
labeling and is able to cope with the vast majority of nested entities.
Extensive experiments on public NER datasets reveal the effectiveness of our
model in extracting both flat and nested entities. The method achieved
state-of-the-art performance on both flat and nested datasets. The significant
improvement in accuracy reflects the superiority of the multi-layer labeling
strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Driven Investigation of Noise-Adaptive Utterance Generation with Linguistic Modification. (arXiv:2210.10252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10252">
<div class="article-summary-box-inner">
<span><p>In noisy environments, speech can be hard to understand for humans. Spoken
dialog systems can help to enhance the intelligibility of their output, either
by modifying the speech synthesis (e.g., imitate Lombard speech) or by
optimizing the language generation. We here focus on the second type of
approach, by which an intended message is realized with words that are more
intelligible in a specific noisy environment. By conducting a speech perception
experiment, we created a dataset of 900 paraphrases in babble noise, perceived
by native English speakers with normal hearing. We find that careful selection
of paraphrases can improve intelligibility by 33% at SNR -5 dB. Our analysis of
the data shows that the intelligibility differences between paraphrases are
mainly driven by noise-robust acoustic cues. Furthermore, we propose an
intelligibility-aware paraphrase ranking model, which outperforms baseline
models with a relative improvement of 31.37% at SNR -5 dB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continued Pretraining for Better Zero- and Few-Shot Promptability. (arXiv:2210.10258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10258">
<div class="article-summary-box-inner">
<span><p>Recently introduced language model prompting methods can achieve high
accuracy in zero- and few-shot settings while requiring few to no learned
task-specific parameters. Nevertheless, these methods still often trail behind
full model finetuning. In this work, we investigate if a dedicated continued
pretraining stage could improve "promptability", i.e., zero-shot performance
with natural language prompts or few-shot performance with prompt tuning. We
reveal settings where existing continued pretraining methods lack
promptability. We also identify current methodological gaps, which we fill with
thorough large-scale experiments. We demonstrate that a simple recipe,
continued pretraining that incorporates a trainable prompt during multi-task
learning, leads to improved promptability in both zero- and few-shot settings
compared to existing methods, up to 31% relative. On the other hand, we find
that continued pretraining using MAML-style meta-learning, a method that
directly optimizes few-shot promptability, yields subpar performance. We
validate our findings with two prompt tuning methods, and, based on our
results, we provide concrete recommendations to optimize promptability for
different use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10260">
<div class="article-summary-box-inner">
<span><p>Named entity recognition is a traditional task in natural language
processing. In particular, nested entity recognition receives extensive
attention for the widespread existence of the nesting scenario. The latest
research migrates the well-established paradigm of set prediction in object
detection to cope with entity nesting. However, the manual creation of query
vectors, which fail to adapt to the rich semantic information in the context,
limits these approaches. An end-to-end entity detection approach with proposer
and regressor is presented in this paper to tackle the issues. First, the
proposer utilizes the feature pyramid network to generate high-quality entity
proposals. Then, the regressor refines the proposals for generating the final
prediction. The model adopts encoder-only architecture and thus obtains the
advantages of the richness of query semantics, high precision of entity
localization, and easiness for model training. Moreover, we introduce the novel
spatially modulated attention and progressive refinement for further
improvement. Extensive experiments demonstrate that our model achieves advanced
performance in flat and nested NER, achieving a new state-of-the-art F1 score
of 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models. (arXiv:2210.10289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10289">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its
variants, have led to significant improvements on various NLP tasks in past
years. However, a theoretical framework for studying their relationships is
still missing. In this paper, we fill this gap by investigating the linear
dependency between pre-trained LMs. The linear dependency of LMs is defined
analogously to the linear dependency of vectors. We propose Language Model
Decomposition (LMD) to represent a LM using a linear combination of other LMs
as basis, and derive the closed-form solution. A goodness-of-fit metric for LMD
similar to the coefficient of determination is defined and used to measure the
linear dependency of a set of LMs. In experiments, we find that BERT and eleven
(11) BERT-like LMs are 91% linearly dependent. This observation suggests that
current state-of-the-art (SOTA) LMs are highly "correlated". To further advance
SOTA we need more diverse and novel LMs that are less dependent on existing
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation. (arXiv:2210.10291v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10291">
<div class="article-summary-box-inner">
<span><p>Recently, aspect sentiment quad prediction (ASQP) has become a popular task
in the field of aspect-level sentiment analysis. Previous work utilizes a
predefined template to paraphrase the original sentence into a structure target
sequence, which can be easily decoded as quadruplets of the form (aspect
category, aspect term, opinion term, sentiment polarity). The template involves
the four elements in a fixed order. However, we observe that this solution
contradicts with the order-free property of the ASQP task, since there is no
need to fix the template order as long as the quadruplet is extracted
correctly. Inspired by the observation, we study the effects of template orders
and find that some orders help the generative model achieve better performance.
It is hypothesized that different orders provide various views of the
quadruplet. Therefore, we propose a simple but effective method to identify the
most proper orders, and further combine multiple proper templates as data
augmentation to improve the ASQP task. Specifically, we use the pre-trained
language model to select the orders with minimal entropy. By fine-tuning the
pre-trained language model with these template orders, our approach improves
the performance of quad prediction, and outperforms state-of-the-art methods
significantly in low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning. (arXiv:2210.10293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10293">
<div class="article-summary-box-inner">
<span><p>Multiple pre-training objectives fill the vacancy of the understanding
capability of single-objective language modeling, which serves the ultimate
purpose of pre-trained language models (PrLMs), generalizing well on a mass of
scenarios. However, learning multiple training objectives in a single model is
challenging due to the unknown relative significance as well as the potential
contrariety between them. Empirical studies have shown that the current
objective sampling in an ad-hoc manual setting makes the learned language
representation barely converge to the desired optimum. Thus, we propose
\textit{MOMETAS}, a novel adaptive sampler based on meta-learning, which learns
the latent sampling pattern on arbitrary pre-training objectives. Such a design
is lightweight with negligible additional training overhead. To validate our
approach, we adopt five objectives and conduct continual pre-training with
BERT-base and BERT-large models, where MOMETAS demonstrates universal
performance gain over other rule-based sampling strategies on 14 natural
language processing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss. (arXiv:2210.10305v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10305">
<div class="article-summary-box-inner">
<span><p>For readability assessment, traditional methods mainly employ machine
learning classifiers with hundreds of linguistic features. Although the deep
learning model has become the prominent approach for almost all NLP tasks, it
is less explored for readability assessment. In this paper, we propose a
BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)
for readability assessment. Specially, we present a new difficulty knowledge
guided semi-supervised method to extract topic features to complement the
traditional linguistic features. From the linguistic features, we employ
projection filtering to extract orthogonal features to supplement BERT
representations. Furthermore, we design a new length-balanced loss to handle
the greatly varying length distribution of data. Our model achieves
state-of-the-art performances on two English benchmark datasets and one dataset
of Chinese textbooks, and also achieves the near-perfect accuracy of 99\% on
one English dataset. Moreover, our proposed model obtains comparable results
with human experts in consistency test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking. (arXiv:2210.10320v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10320">
<div class="article-summary-box-inner">
<span><p>Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors. Recent researches start from the pretrained knowledge of language
models and take multimodal information into CSC models to improve the
performance. However, they overlook the rich knowledge in the dictionary, the
reference book where one can learn how one character should be pronounced,
written, and used. In this paper, we propose the LEAD framework, which renders
the CSC model to learn heterogeneous knowledge from the dictionary in terms of
phonetics, vision, and meaning. LEAD first constructs positive and negative
samples according to the knowledge of character phonetics, glyphs, and
definitions in the dictionary. Then a unified contrastive learning-based
training scheme is employed to refine the representations of the CSC models.
Extensive experiments and detailed analyses on the SIGHAN benchmark datasets
demonstrate the effectiveness of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping. (arXiv:2210.10325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10325">
<div class="article-summary-box-inner">
<span><p>Fine-tuning over large pretrained language models (PLMs) has established many
state-of-the-art results. Despite its superior performance, such fine-tuning
can be unstable, resulting in significant variance in performance and potential
risks for practical applications. Previous works have attributed such
instability to the catastrophic forgetting problem in the top layers of PLMs,
which indicates iteratively that fine-tuning layers in a top-down manner is a
promising solution. In this paper, we first point out that this method does not
always work out due to the different convergence speeds of different
layers/modules. Inspired by this observation, we propose a simple
component-wise gradient norm clipping method to adjust the convergence speed
for different components. Experiment results demonstrate that our method
achieves consistent improvements in terms of generalization performance,
convergence speed, and training stability. The codebase can be found at
https://github.com/yangalan123/FineTuningStability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10329">
<div class="article-summary-box-inner">
<span><p>Transformer-based Language Models (LMs) achieve remarkable performances on a
variety of NLU tasks, but are also prone to generating toxic texts such as
insults, threats, and profanities which limit their adaptations to the
real-world applications. To overcome this issue, a few text generation
approaches aim to detoxify toxic texts with additional LMs or perturbations.
However, previous methods require excessive memory, computations, and time
which are serious bottlenecks in their real-world application. To address such
limitations, we propose an effective yet efficient method for language
detoxification using an attribute-discriminative latent space. Specifically, we
project the latent space of an original Transformer LM to a discriminative
latent space on which the texts are well-separated by their attributes, with
the help of a projection block and a discriminator. This allows the LM to
control the text generation to be non-toxic with minimal memory and computation
overhead. We validate our model, Attribute-Discriminative Language Model (ADLM)
on detoxified language and dialogue generation tasks, on which our method
significantly outperforms baselines both in performance and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revision Transformers: Getting RiT of No-Nos. (arXiv:2210.10332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10332">
<div class="article-summary-box-inner">
<span><p>Current transformer language models (LM) are large-scale models with billions
of parameters. They have been shown to provide high performances on a variety
of tasks but are also prone to shortcut learning and bias. Addressing such
incorrect model behavior via parameter adjustments is very costly. This is
particularly problematic for updating dynamic concepts, such as moral values,
which vary culturally or interpersonally. In this work, we question the current
common practice of storing all information in the model parameters and propose
the Revision Transformer (RiT) employing information retrieval to facilitate
easy model updating. The specific combination of a large-scale pre-trained LM
that inherently but also diffusely encodes world knowledge with a
clear-structured revision engine makes it possible to update the model's
knowledge with little effort and the help of user interaction. We exemplify RiT
on a moral dataset and simulate user feedback demonstrating strong performance
in model revision even with small data. This way, users can easily design a
model regarding their preferences, paving the way for more transparent and
personalized AI models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Devil in Linear Transformer. (arXiv:2210.10340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10340">
<div class="article-summary-box-inner">
<span><p>Linear transformers aim to reduce the quadratic space-time complexity of
vanilla transformers. However, they usually suffer from degraded performances
on various tasks and corpus. In this paper, we examine existing kernel-based
linear transformers and identify two key issues that lead to such performance
gaps: 1) unbounded gradients in the attention computation adversely impact the
convergence of linear transformer models; 2) attention dilution which trivially
distributes attention scores over long sequences while neglecting neighbouring
structures. To address these issues, we first identify that the scaling of
attention matrices is the devil in unbounded gradients, which turns out
unnecessary in linear attention as we show theoretically and empirically. To
this end, we propose a new linear attention that replaces the scaling operation
with a normalization to stabilize gradients. For the issue of attention
dilution, we leverage a diagonal attention to confine attention to only
neighbouring tokens in early layers. Benefiting from the stable gradients and
improved attention, our new linear transformer model, transNormer, demonstrates
superior performance on text classification and language modeling tasks, as
well as on the challenging Long-Range Arena benchmark, surpassing vanilla
transformer and existing linear variants by a clear margin while being
significantly more space-time efficient. The code is available at
https://github.com/OpenNLPLab/Transnormer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10341">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EnTDA: Entity-to-Text based Data Augmentation Approach for Named Entity Recognition Tasks. (arXiv:2210.10343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10343">
<div class="article-summary-box-inner">
<span><p>Data augmentation techniques have been used to improve the generalization
capability of models in the named entity recognition (NER) tasks. Existing
augmentation methods either manipulate the words in the original text that
require hand-crafted in-domain knowledge, or leverage generative models which
solicit dependency order among entities. To alleviate the excessive reliance on
the dependency order among entities in existing augmentation paradigms, we
develop an entity-to-text instead of text-to-entity based data augmentation
method named: EnTDA to decouple the dependencies between entities by adding,
deleting, replacing and swapping entities, and adopt these augmented data to
bootstrap the generalization ability of the NER model. Furthermore, we
introduce a diversity beam search to increase the diversity of the augmented
data. Experiments on thirteen NER datasets across three tasks (flat NER, nested
NER, and discontinuous NER) and two settings (full data NER and low resource
NER) show that EnTDA could consistently outperform the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation. (arXiv:2210.10349v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10349">
<div class="article-summary-box-inner">
<span><p>Symbolic music generation aims to generate music scores automatically. A
recent trend is to use Transformer or its variants in music generation, which
is, however, suboptimal, because the full attention cannot efficiently model
the typically long music sequences (e.g., over 10,000 tokens), and the existing
models have shortcomings in generating musical repetition structures. In this
paper, we propose Museformer, a Transformer with a novel fine- and
coarse-grained attention for music generation. Specifically, with the
fine-grained attention, a token of a specific bar directly attends to all the
tokens of the bars that are most relevant to music structures (e.g., the
previous 1st, 2nd, 4th and 8th bars, selected via similarity statistics); with
the coarse-grained attention, a token only attends to the summarization of the
other bars rather than each token of them so as to reduce the computational
cost. The advantages are two-fold. First, it can capture both music
structure-related correlations via the fine-grained attention, and other
contextual information via the coarse-grained attention. Second, it is
efficient and can model over 3X longer music sequences compared to its
full-attention counterpart. Both objective and subjective experimental results
demonstrate its ability to generate long music sequences with high quality and
better structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuGER$^2$: Multi-Granularity Evidence Retrieval and Reasoning for Hybrid Question Answering. (arXiv:2210.10350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10350">
<div class="article-summary-box-inner">
<span><p>Hybrid question answering (HQA) aims to answer questions over heterogeneous
data, including tables and passages linked to table cells. The heterogeneous
data can provide different granularity evidence to HQA models, e.t., column,
row, cell, and link. Conventional HQA models usually retrieve coarse- or
fine-grained evidence to reason the answer. Through comparison, we find that
coarse-grained evidence is easier to retrieve but contributes less to the
reasoner, while fine-grained evidence is the opposite. To preserve the
advantage and eliminate the disadvantage of different granularity evidence, we
propose MuGER$^2$, a Multi-Granularity Evidence Retrieval and Reasoning
approach. In evidence retrieval, a unified retriever is designed to learn the
multi-granularity evidence from the heterogeneous data. In answer reasoning, an
evidence selector is proposed to navigate the fine-grained evidence for the
answer reader based on the learned multi-granularity evidence. Experiment
results on the HybridQA dataset show that MuGER$^2$ significantly boosts the
HQA performance. Further ablation analysis verifies the effectiveness of both
the retrieval and reasoning designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging a New Spanish Corpus for Multilingual and Crosslingual Metaphor Detection. (arXiv:2210.10358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10358">
<div class="article-summary-box-inner">
<span><p>The lack of wide coverage datasets annotated with everyday metaphorical
expressions for languages other than English is striking. This means that most
research on supervised metaphor detection has been published only for that
language. In order to address this issue, this work presents the first corpus
annotated with naturally occurring metaphors in Spanish large enough to develop
systems to perform metaphor detection. The presented dataset, CoMeta, includes
texts from various domains, namely, news, political discourse, Wikipedia and
reviews. In order to label CoMeta, we apply the MIPVU method, the guidelines
most commonly used to systematically annotate metaphor on real data. We use our
newly created dataset to provide competitive baselines by fine-tuning several
multilingual and monolingual state-of-the-art large language models.
Furthermore, by leveraging the existing VUAM English data in addition to
CoMeta, we present the, to the best of our knowledge, first cross-lingual
experiments on supervised metaphor detection. Finally, we perform a detailed
error analysis that explores the seemingly high transfer of everyday metaphor
across these two languages and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling. (arXiv:2210.10369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10369">
<div class="article-summary-box-inner">
<span><p>Recent joint multiple intent detection and slot filling models employ label
embeddings to achieve the semantics-label interactions. However, they treat all
labels and label embeddings as uncorrelated individuals, ignoring the
dependencies among them. Besides, they conduct the decoding for the two tasks
independently, without leveraging the correlations between them. Therefore, in
this paper, we first construct a Heterogeneous Label Graph (HLG) containing two
kinds of topologies: (1) statistical dependencies based on labels'
co-occurrence patterns and hierarchies in slot labels; (2) rich relations among
the label nodes. Then we propose a novel model termed ReLa-Net. It can capture
beneficial correlations among the labels from HLG. The label correlations are
leveraged to enhance semantic-label interactions. Moreover, we also propose the
label-aware inter-dependent decoding mechanism to further exploit the label
correlations for decoding. Experiment results show that our ReLa-Net
significantly outperforms previous models. Remarkably, ReLa-Net surpasses the
previous best model by over 20\% in terms of overall accuracy on MixATIS
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs. (arXiv:2210.10375v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10375">
<div class="article-summary-box-inner">
<span><p>Recent graph-based models for joint multiple intent detection and slot
filling have obtained promising results through modeling the guidance from the
prediction of intents to the decoding of slot filling. However, existing
methods (1) only model the \textit{unidirectional guidance} from intent to
slot; (2) adopt \textit{homogeneous graphs} to model the interactions between
the slot semantics nodes and intent label nodes, which limit the performance.
In this paper, we propose a novel model termed Co-guiding Net, which implements
a two-stage framework achieving the \textit{mutual guidances} between the two
tasks. In the first stage, the initial estimated labels of both tasks are
produced, and then they are leveraged in the second stage to model the mutual
guidances. Specifically, we propose two \textit{heterogeneous graph attention
networks} working on the proposed two \textit{heterogeneous semantics-label
graphs}, which effectively represent the relations among the semantics nodes
and label nodes. Experiment results show that our model outperforms existing
models by a large margin, obtaining a relative improvement of 19.3\% over the
previous best model on MixATIS dataset in overall accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tourist Guidance Robot Based on HyperCLOVA. (arXiv:2210.10400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10400">
<div class="article-summary-box-inner">
<span><p>This paper describes our system submitted to Dialogue Robot Competition 2022.
Our proposed system is a combined model of rule-based and generation-based
dialog systems. The system utilizes HyperCLOVA, a Japanese foundation model,
not only to generate responses but also summarization, search information, etc.
We also used our original speech recognition system, which was fine-tuned for
this dialog task. As a result, our system ranked second in the preliminary
round and moved on to the finals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid-Regressive Neural Machine Translation. (arXiv:2210.10416v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10416">
<div class="article-summary-box-inner">
<span><p>In this work, we empirically confirm that non-autoregressive translation with
an iterative refinement mechanism (IR-NAT) suffers from poor acceleration
robustness because it is more sensitive to decoding batch size and computing
device setting than autoregressive translation (AT). Inspired by it, we attempt
to investigate how to combine the strengths of autoregressive and
non-autoregressive translation paradigms better. To this end, we demonstrate
through synthetic experiments that prompting a small number of AT's predictions
can promote one-shot non-autoregressive translation to achieve the equivalent
performance of IR-NAT. Following this line, we propose a new two-stage
translation prototype called hybrid-regressive translation (HRT). Specifically,
HRT first generates discontinuous sequences via autoregression (e.g., make a
prediction every k tokens, k&gt;1) and then fills in all previously skipped tokens
at once in a non-autoregressive manner. We also propose a bag of techniques to
effectively and efficiently train HRT without adding any model parameters. HRT
achieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is
at least 1.5x faster than AT, regardless of batch size and device. In addition,
another bonus of HRT is that it successfully inherits the good characteristics
of AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to
the vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed
of HRT with a 12-layer encoder and 1-layer decoder is further doubled on both
GPU and CPU without BLEU loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Linguistic Investigation of Machine Learning based Contradiction Detection Models: An Empirical Analysis and Future Perspectives. (arXiv:2210.10434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10434">
<div class="article-summary-box-inner">
<span><p>We analyze two Natural Language Inference data sets with respect to their
linguistic features. The goal is to identify those syntactic and semantic
properties that are particularly hard to comprehend for a machine learning
model. To this end, we also investigate the differences between a
crowd-sourced, machine-translated data set (SNLI) and a collection of text
pairs from internet sources. Our main findings are, that the model has
difficulty recognizing the semantic importance of prepositions and verbs,
emphasizing the importance of linguistically aware pre-training tasks.
Furthermore, it often does not comprehend antonyms and homonyms, especially if
those are depending on the context. Incomplete sentences are another problem,
as well as longer paragraphs and rare words or phrases. The study shows that
automated language understanding requires a more informed approach, utilizing
as much external knowledge as possible throughout the training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation. (arXiv:2210.10436v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10436">
<div class="article-summary-box-inner">
<span><p>Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which
is the core step of bridging and integrating multi-source KGs. In this paper,
we argue that existing GNN-based EA methods inherit the inborn defects from
their neural network lineage: weak scalability and poor interpretability.
Inspired by recent studies, we reinvent the Label Propagation algorithm to
effectively run on KGs and propose a non-neural EA framework -- LightEA,
consisting of three efficient components: (i) Random Orthogonal Label
Generation, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn
Iteration. According to the extensive experiments on public datasets, LightEA
has impressive scalability, robustness, and interpretability. With a mere tenth
of time consumption, LightEA achieves comparable results to state-of-the-art
methods across all datasets and even surpasses them on many.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Rules-Based Corpus Generation for Native Chinese Grammatical Error Correction. (arXiv:2210.10442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10442">
<div class="article-summary-box-inner">
<span><p>Chinese Grammatical Error Correction (CGEC) is both a challenging NLP task
and a common application in human daily life. Recently, many data-driven
approaches are proposed for the development of CGEC research. However, there
are two major limitations in the CGEC field: First, the lack of high-quality
annotated training corpora prevents the performance of existing CGEC models
from being significantly improved. Second, the grammatical errors in widely
used test sets are not made by native Chinese speakers, resulting in a
significant gap between the CGEC models and the real application. In this
paper, we propose a linguistic rules-based approach to construct large-scale
CGEC training corpora with automatically generated grammatical errors.
Additionally, we present a challenging CGEC benchmark derived entirely from
errors made by native Chinese speakers in real-world scenarios. Extensive
experiments and detailed analyses not only demonstrate that the training data
constructed by our method effectively improves the performance of CGEC models,
but also reflect that our benchmark is an excellent resource for further
development of the CGEC field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing. (arXiv:2210.10449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10449">
<div class="article-summary-box-inner">
<span><p>A lack of large-scale human-annotated data has hampered the hierarchical
discourse parsing of Chinese. In this paper, we present GCDT, the largest
hierarchical discourse treebank for Mandarin Chinese in the framework of
Rhetorical Structure Theory (RST). GCDT covers over 60K tokens across five
genres of freely available text, using the same relation inventory as
contemporary RST treebanks for English. We also report on this dataset's
parsing experiments, including state-of-the-art (SOTA) scores for Chinese RST
parsing and RST parsing on the English GUM dataset, using cross-lingual
training in Chinese and English with multilingual embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10488">
<div class="article-summary-box-inner">
<span><p>Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called "neural texts"), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing. (arXiv:2210.10543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10543">
<div class="article-summary-box-inner">
<span><p>Recently, a number of articles have argued that deep learning models such as
GPT could also capture key aspects of language processing in the human mind and
brain. However, I will argue that these models are not suitable as neural
models of human language. Firstly, because they fail on fundamental boundary
conditions, such as the amount of learning they require. This would in fact
imply that the mechanisms of GPT and brain language processing are
fundamentally different. Secondly, because they do not possess the logistics of
access needed for compositional and productive human language processing.
Neural architectures could possess logistics of access based on small-world
like network structures, in which processing does not consist of symbol
manipulation but of controlling the flow of activation. In this view, two
complementary approaches would be needed to investigate the relation between
brain and cognition. Investigating learning methods could reveal how 'learned
cognition' as found in deep learning could develop in the brain. However,
neural architectures with logistics of access should also be developed to
account for 'productive cognition' as required for natural or artificial human
language processing. Later on, these approaches could perhaps be combined to
see how such architectures could develop by learning and development from a
simpler basis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Does More Than Describe: On The Lack Of Figurative Speech in Text-To-Image Models. (arXiv:2210.10578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10578">
<div class="article-summary-box-inner">
<span><p>The impressive capacity shown by recent text-to-image diffusion models to
generate high-quality pictures from textual input prompts has leveraged the
debate about the very definition of art. Nonetheless, these models have been
trained using text data collected from content-based labelling protocols that
focus on describing the items and actions in an image but neglect any
subjective appraisal. Consequently, these automatic systems need rigorous
descriptions of the elements and the pictorial style of the image to be
generated, otherwise failing to deliver. As potential indicators of the actual
artistic capabilities of current generative models, we characterise the
sentimentality, objectiveness and degree of abstraction of publicly available
text data used to train current text-to-image diffusion models. Considering the
sharp difference observed between their language style and that typically
employed in artistic contexts, we suggest generative models should incorporate
additional sources of subjective information in their training in order to
overcome (or at least to alleviate) some of their current limitations, thus
effectively unleashing a truly artistic and creative generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEntRE: A paragraph-level Chinese dataset for Relation Extraction among Enterprises. (arXiv:2210.10581v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10581">
<div class="article-summary-box-inner">
<span><p>Enterprise relation extraction aims to detect pairs of enterprise entities
and identify the business relations between them from unstructured or
semi-structured text data, and it is crucial for several real-world
applications such as risk analysis, rating research and supply chain security.
However, previous work mainly focuses on getting attribute information about
enterprises like personnel and corporate business, and pays little attention to
enterprise relation extraction. To encourage further progress in the research,
we introduce the CEntRE, a new dataset constructed from publicly available
business news data with careful human annotation and intelligent data
processing. Extensive experiments on CEntRE with six excellent models
demonstrate the challenges of our proposed dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced vectors for top-k document retrieval in Question Answering. (arXiv:2210.10584v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10584">
<div class="article-summary-box-inner">
<span><p>Modern day applications, especially information retrieval webapps that
involve "search" as their use cases are gradually moving towards "answering"
modules. Conversational chatbots which have been proved to be more engaging to
users, use Question Answering as their core. Since, precise answering is
computationally expensive, several approaches have been developed to prefetch
the most relevant documents/passages from the database that contain the answer.
We propose a different approach that retrieves the evidence documents
efficiently and accurately, making sure that the relevant document for a given
user query is not missed. We do so by assigning each document (or passage in
our case), a unique identifier and using them to create dense vectors which can
be efficiently indexed. More precisely, we use the identifier to predict
randomly sampled context window words of the relevant question corresponding to
the passage along with the words of passage itself. This naturally embeds the
passage identifier into the vector space in such a way that the embedding is
closer to the question without compromising he information content. This
approach enables efficient creation of real-time query vectors in ~4
milliseconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Graph Masking Pre-training for Graph-to-Text Generation. (arXiv:2210.10599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10599">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text
(G2T) generation by processing the linearised version of a graph. However, the
linearisation is known to ignore the structural information. Additionally, PLMs
are typically pre-trained on free text which introduces domain mismatch between
pre-training and downstream G2T generation tasks. To address these
shortcomings, we propose graph masking pre-training strategies that neither
require supervision signals nor adjust the architecture of the underlying
pre-trained encoder-decoder model. When used with a pre-trained T5, our
approach achieves new state-of-the-art results on WebNLG+2020 and
EventNarrative G2T generation datasets. Our method also shows to be very
effective in the low-resource setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NGEP: A Graph-based Event Planning Framework for Story Generation. (arXiv:2210.10602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10602">
<div class="article-summary-box-inner">
<span><p>To improve the performance of long text generation, recent studies have
leveraged automatically planned event structures (i.e. storylines) to guide
story generation. Such prior works mostly employ end-to-end neural generation
models to predict event sequences for a story. However, such generation models
struggle to guarantee the narrative coherence of separate events due to the
hallucination problem, and additionally the generated event sequences are often
hard to control due to the end-to-end nature of the models. To address these
challenges, we propose NGEP, an novel event planning framework which generates
an event sequence by performing inference on an automatically constructed event
graph and enhances generalisation ability through a neural event advisor. We
conduct a range of experiments on multiple criteria, and the results
demonstrate that our graph-based neural framework outperforms the
state-of-the-art (SOTA) event planning approaches, considering both the
performance of event sequence generation and the effectiveness on the
downstream task of story generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models. (arXiv:2210.10606v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10606">
<div class="article-summary-box-inner">
<span><p>We study the way DALLE-2 maps symbols (words) in the prompt to their
references (entities or properties of entities in the generated image). We show
that in stark contrast to the way human process language, DALLE-2 does not
follow the constraint that each word has a single role in the interpretation,
and sometimes re-use the same symbol for different purposes. We collect a set
of stimuli that reflect the phenomenon: we show that DALLE-2 depicts both
senses of nouns with multiple senses at once; and that a given word can modify
the properties of two distinct entities in the image, or can be depicted as one
object and also modify the properties of another object, creating a semantic
leakage of properties between entities. Taken together, our study highlights
the differences between DALLE-2 and human language processing and opens an
avenue for future study on the inductive biases of text-to-image models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Chinese Story Generation via Awareness of Syntactic Dependencies and Semantics. (arXiv:2210.10618v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10618">
<div class="article-summary-box-inner">
<span><p>Story generation aims to generate a long narrative conditioned on a given
input. In spite of the success of prior works with the application of
pre-trained models, current neural models for Chinese stories still struggle to
generate high-quality long text narratives. We hypothesise that this stems from
ambiguity in syntactically parsing the Chinese language, which does not have
explicit delimiters for word segmentation. Consequently, neural models suffer
from the inefficient capturing of features in Chinese narratives. In this
paper, we present a new generation framework that enhances the feature
capturing mechanism by informing the generation model of dependencies between
words and additionally augmenting the semantic representation learning through
synonym denoising training. We conduct a range of experiments, and the results
demonstrate that our framework outperforms the state-of-the-art Chinese
generation models on all evaluation metrics, demonstrating the benefits of
enhanced dependency and semantic representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses. (arXiv:2210.10634v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10634">
<div class="article-summary-box-inner">
<span><p>Recently, substantial progress has been made in text ranking based on
pretrained language models such as BERT. However, there are limited studies on
how to leverage more powerful sequence-to-sequence models such as T5. Existing
attempts usually formulate text ranking as classification and rely on
postprocessing to obtain a ranked list. In this paper, we propose RankT5 and
study two T5-based ranking model structures, an encoder-decoder and an
encoder-only one, so that they not only can directly output ranking scores for
each query-document pair, but also can be fine-tuned with "pairwise" or
"listwise" ranking losses to optimize ranking performances. Our experiments
show that the proposed models with ranking losses can achieve substantial
ranking performance gains on different public text ranking data sets. Moreover,
when fine-tuned with listwise ranking losses, the ranking model appears to have
better zero-shot ranking performance on out-of-domain data sets compared to the
model fine-tuned with classification losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-Best Hypotheses Reranking for Text-To-SQL Systems. (arXiv:2210.10668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10668">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL task maps natural language utterances to structured queries that
can be issued to a database. State-of-the-art (SOTA) systems rely on finetuning
large, pre-trained language models in conjunction with constrained decoding
applying a SQL parser. On the well established Spider dataset, we begin with
Oracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's
10-best list, yields a $7.7\%$ absolute improvement in both exact match (EM)
and execution (EX) accuracy, showing significant potential improvements with
reranking. Identifying coherence and correctness as reranking approaches, we
design a model generating a query plan and propose a heuristic schema linking
algorithm. Combining both approaches, with T5-Large, we obtain a consistent
$1\% $ improvement in EM accuracy, and a $~2.5\%$ improvement in EX,
establishing a new SOTA for this task. Our comprehensive error studies on DEV
data show the underlying difficulty in making progress on this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10669">
<div class="article-summary-box-inner">
<span><p>Social media platforms are currently the main channel for political
messaging, allowing politicians to target specific demographics and adapt based
on their reactions. However, making this communication transparent is
challenging, as the messaging is tightly coupled with its intended audience and
often echoed by multiple stakeholders interested in advancing specific
policies. Our goal in this paper is to take a first step towards understanding
these highly decentralized settings. We propose a weakly supervised approach to
identify the stance and issue of political ads on Facebook and analyze how
political campaigns use some kind of demographic targeting by location, gender,
or age. Furthermore, we analyze the temporal dynamics of the political ads on
election polls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arabic Word-level Readability Visualization for Assisted Text Simplification. (arXiv:2210.10672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10672">
<div class="article-summary-box-inner">
<span><p>This demo paper presents a Google Docs add-on for automatic Arabic word-level
readability visualization. The add-on includes a lemmatization component that
is connected to a five-level readability lexicon and Arabic WordNet-based
substitution suggestions. The add-on can be used for assessing the reading
difficulty of a text and identifying difficult words as part of the task of
manual text simplification. We make our add-on and its code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10678">
<div class="article-summary-box-inner">
<span><p>This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP. (arXiv:2210.10683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10683">
<div class="article-summary-box-inner">
<span><p>Textual adversarial samples play important roles in multiple subfields of NLP
research, including security, evaluation, explainability, and data
augmentation. However, most work mixes all these roles, obscuring the problem
definitions and research goals of the security role that aims to reveal the
practical concerns of NLP models. In this paper, we rethink the research
paradigm of textual adversarial samples in security scenarios. We discuss the
deficiencies in previous work and propose our suggestions that the research on
the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their
methods on security tasks to demonstrate the real-world concerns; (2) consider
real-world attackers' goals, instead of developing impractical methods. To this
end, we first collect, process, and release a security datasets collection
Advbench. Then, we reformalize the task and adjust the emphasis on different
goals in SoadNLP. Next, we propose a simple method based on heuristic rules
that can easily fulfill the actual adversarial goals to simulate real-world
attack methods. We conduct experiments on both the attack and the defense sides
on Advbench. Experimental results show that our method has higher practical
value, indicating that the research paradigm in SoadNLP may start from our new
benchmark. All the code and data of Advbench can be obtained at
\url{https://github.com/thunlp/Advbench}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Understand Us, Poorly. (arXiv:2210.10684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10684">
<div class="article-summary-box-inner">
<span><p>Some claim language models understand us. Others won't hear it. To clarify, I
investigate three views of human language understanding: as-mapping,
as-reliability and as-representation. I argue that while behavioral reliability
is necessary for understanding, internal representations are sufficient; they
climb the right hill. I review state-of-the-art language and multi-modal
models: they are pragmatically challenged by under-specification of form. I
question the Scaling Paradigm: limits on resources may prohibit scaled-up
models from approaching understanding. Last, I describe how as-representation
advances a science of understanding. We need work which probes model internals,
adds more of human language, and measures what models can learn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Procedural Fairness: Uncovering Biases in How a Toxic Language Classifier Uses Sentiment Information. (arXiv:2210.10689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10689">
<div class="article-summary-box-inner">
<span><p>Previous works on the fairness of toxic language classifiers compare the
output of models with different identity terms as input features but do not
consider the impact of other important concepts present in the context. Here,
besides identity terms, we take into account high-level latent features learned
by the classifier and investigate the interaction between these features and
identity terms. For a multi-class toxic language classifier, we leverage a
concept-based explanation framework to calculate the sensitivity of the model
to the concept of sentiment, which has been used before as a salient feature
for toxic language detection. Our results show that although for some classes,
the classifier has learned the sentiment information as expected, this
information is outweighed by the influence of identity terms as input features.
This work is a step towards evaluating procedural fairness, where unfair
processes lead to unfair outcomes. The produced knowledge can guide debiasing
techniques to ensure that important concepts besides identity terms are
well-represented in training datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced African Languages. (arXiv:2210.10692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10692">
<div class="article-summary-box-inner">
<span><p>We participated in the WMT 2022 Large-Scale Machine Translation Evaluation
for the African Languages Shared Task. This work describes our approach, which
is based on filtering the given noisy data using a sentence-pair classifier
that was built by fine-tuning a pre-trained language model. To train the
classifier, we obtain positive samples (i.e. high-quality parallel sentences)
from a gold-standard curated dataset and extract negative samples (i.e.
low-quality parallel sentences) from automatically aligned parallel data by
choosing sentences with low alignment scores. Our final machine translation
model was then trained on filtered data, instead of the entire noisy dataset.
We empirically validate our approach by evaluating on two common datasets and
show that data filtering generally improves overall translation quality, in
some cases even significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness of Demonstration-based Learning Under Limited Data Scenario. (arXiv:2210.10693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10693">
<div class="article-summary-box-inner">
<span><p>Demonstration-based learning has shown great potential in stimulating
pretrained language models' ability under limited data scenario. Simply
augmenting the input with some demonstrations can significantly improve
performance on few-shot NER. However, why such demonstrations are beneficial
for the learning process remains unclear since there is no explicit alignment
between the demonstrations and the predictions. In this paper, we design
pathological demonstrations by gradually removing intuitively useful
information from the standard ones to take a deep dive of the robustness of
demonstration-based sequence labeling and show that (1) demonstrations composed
of random tokens still make the model a better few-shot learner; (2) the length
of random demonstrations and the relevance of random tokens are the main
factors affecting the performance; (3) demonstrations increase the confidence
of model predictions on captured superficial patterns. We have publicly
released our code at https://github.com/SALT-NLP/RobustDemo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking. (arXiv:2210.10695v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10695">
<div class="article-summary-box-inner">
<span><p>Pairing a lexical retriever with a neural re-ranking model has set
state-of-the-art performance on large-scale information retrieval datasets.
This pipeline covers scenarios like question answering or navigational queries,
however, for information-seeking scenarios, users often provide information on
whether a document is relevant to their query in form of clicks or explicit
feedback. Therefore, in this work, we explore how relevance feedback can be
directly integrated into neural re-ranking models by adopting few-shot and
parameter-efficient learning techniques. Specifically, we introduce a kNN
approach that re-ranks documents based on their similarity with the query and
the documents the user considers relevant. Further, we explore Cross-Encoder
models that we pre-train using meta-learning and subsequently fine-tune for
each query, training only on the feedback documents. To evaluate our different
integration strategies, we transform four existing information retrieval
datasets into the relevance feedback scenario. Extensive experiments
demonstrate that integrating relevance feedback directly in neural re-ranking
models improves their performance, and fusing lexical ranking with our best
performing neural re-ranker outperforms all other methods by 5.2 nDCG@20.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction. (arXiv:2210.10709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10709">
<div class="article-summary-box-inner">
<span><p>Information Extraction, which aims to extract structural relational triple or
event from unstructured texts, often suffers from data scarcity issues. With
the development of pre-trained language models, many prompt-based approaches to
data-efficient information extraction have been proposed and achieved
impressive performance. However, existing prompt learning methods for
information extraction are still susceptible to several potential limitations:
(i) semantic gap between natural language and output structure knowledge with
pre-defined schema; (ii) representation learning with locally individual
instances limits the performance given the insufficient features. In this
paper, we propose a novel approach of schema-aware Reference As Prompt (RAP),
which dynamically leverage schema and knowledge inherited from global
(few-shot) training data for each sample. Specifically, we propose a
schema-aware reference store, which unifies symbolic schema and relevant
textual instances. Then, we employ a dynamic reference integration module to
retrieve pertinent knowledge from the datastore as prompts during training and
inference. Experimental results demonstrate that RAP can be plugged into
various existing models and outperforms baselines in low-resource settings on
five datasets of relational triple extraction and event extraction. In
addition, we provide comprehensive empirical ablations and case analysis
regarding different types and scales of knowledge in order to better understand
the mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniNL: Aligning Representation Learning with Scoring Function for OOD Detection via Unified Neighborhood Learning. (arXiv:2210.10722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10722">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-domain (OOD) intents from user queries is essential for
avoiding wrong operations in task-oriented dialogue systems. The key challenge
is how to distinguish in-domain (IND) and OOD intents. Previous methods ignore
the alignment between representation learning and scoring function, limiting
the OOD detection performance. In this paper, we propose a unified neighborhood
learning framework (UniNL) to detect OOD intents. Specifically, we design a
K-nearest neighbor contrastive learning (KNCL) objective for representation
learning and introduce a KNN-based scoring function for OOD detection. We aim
to align representation learning with scoring function. Experiments and
analysis on two benchmark datasets show the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10723">
<div class="article-summary-box-inner">
<span><p>We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05126">
<div class="article-summary-box-inner">
<span><p>Existing research studies on cross-sentence relation extraction in long-form
multi-party conversations aim to improve relation extraction without
considering the explainability of such methods. This work addresses that gap by
focusing on extracting explanations that indicate that a relation exists while
using only partially labeled data. We propose our model-agnostic framework,
D-REX, a policy-guided semi-supervised algorithm that explains and ranks
relations. We frame relation extraction as a re-ranking task and include
relation- and entity-specific explanations as an intermediate step of the
inference process. We find that about 90% of the time, human annotators prefer
D-REX's explanations over a strong BERT-based joint relation extraction and
explanation model. Finally, our evaluations on a dialogue relation extraction
dataset show that our method is simple yet effective and achieves a
state-of-the-art F1 score on relation extraction, improving upon existing
methods by 13.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation. (arXiv:2109.09519v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09519">
<div class="article-summary-box-inner">
<span><p>To explore the limit of dialogue generation pre-training, we present the
models of PLATO-XL with up to 11 billion parameters, trained on both Chinese
and English social media conversations. To train such large models, we adopt
the architecture of unified transformer with high computation and parameter
efficiency. In addition, we carry out multi-party aware pre-training to better
distinguish the characteristic information in social media conversations. With
such designs, PLATO-XL successfully achieves superior performances as compared
to other approaches in both Chinese and English chitchat. We further explore
the capacity of PLATO-XL on other conversational tasks, such as knowledge
grounded dialogue and task-oriented conversation. The experimental results
indicate that PLATO-XL obtains state-of-the-art results across multiple
conversational tasks, verifying its potential as a foundation model of
conversational AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. (arXiv:2110.08247v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08247">
<div class="article-summary-box-inner">
<span><p>Backdoor attacks are a kind of emergent security threat in deep learning.
After being injected with a backdoor, a deep neural model will behave normally
on standard inputs but give adversary-specified predictions once the input
contains specific backdoor triggers. In this paper, we find two simple tricks
that can make existing textual backdoor attacks much more harmful. The first
trick is to add an extra training task to distinguish poisoned and clean data
during the training of the victim model, and the second one is to use all the
clean training data rather than remove the original clean data corresponding to
the poisoned data. These two tricks are universally applicable to different
attack models. We conduct experiments in three tough situations including clean
data fine-tuning, low-poisoning-rate, and label-consistent attacks.
Experimental results show that the two tricks can significantly improve attack
performance. This paper exhibits the great potential harmfulness of backdoor
attacks. All the code and data can be obtained at
\url{https://github.com/thunlp/StyleAttack}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10962">
<div class="article-summary-box-inner">
<span><p>Knowledge-enhanced language representation learning has shown promising
results across various knowledge-intensive NLP tasks. However, prior methods
are limited in efficient utilization of multilingual knowledge graph (KG) data
for language model (LM) pretraining. They often train LMs with KGs in indirect
ways, relying on extra entity/relation embeddings to facilitate knowledge
injection. In this work, we explore methods to make better use of the
multilingual annotation and language agnostic property of KG triples, and
present novel knowledge based multilingual language models (KMLMs) trained
directly on the knowledge triples. We first generate a large amount of
multilingual synthetic sentences using the Wikidata KG triples. Then based on
the intra- and inter-sentence structures of the generated data, we design
pretraining tasks to enable the LMs to not only memorize the factual knowledge
but also learn useful logical patterns. Our pretrained KMLMs demonstrate
significant performance improvements on a wide range of knowledge-intensive
cross-lingual tasks, including named entity recognition (NER), factual
knowledge retrieval, relation classification, and a newly designed logical
reasoning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VIRT: Improving Representation-based Models for Text Matching through Virtual Interaction. (arXiv:2112.04195v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04195">
<div class="article-summary-box-inner">
<span><p>With the booming of pre-trained transformers, representation-based models
based on Siamese transformer encoders have become mainstream techniques for
efficient text matching. However, these models suffer from severe performance
degradation due to the lack of interaction between the text pair, compared with
interaction-based models. Prior arts attempt to address this through performing
extra interaction for Siamese encoded representations, while the interaction
during encoding is still ignored. To remedy this, we propose a \textit{Virtual}
InteRacTion mechanism (VIRT) to transfer interactive knowledge from
interaction-based models into Siamese encoders through attention map
distillation. As a train-time-only component, VIRT could completely maintain
the high efficiency of the Siamese structure and brings no extra computation
cost during inference. To fully utilize the learned interactive knowledge, we
further design a VIRT-adapted interaction strategy. Experimental results on
multiple text matching datasets demonstrate that our method outperforms
state-of-the-art representation-based models. What's more, VIRT can be easily
integrated into existing representation-based methods to achieve further
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Video Summarization via Multimodal Self-supervised Learning. (arXiv:2201.02494v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02494">
<div class="article-summary-box-inner">
<span><p>Modern video summarization methods are based on deep neural networks that
require a large amount of annotated data for training. However, existing
datasets for video summarization are small-scale, easily leading to
over-fitting of the deep models. Considering that the annotation of large-scale
datasets is time-consuming, we propose a multimodal self-supervised learning
framework to obtain semantic representations of videos, which benefits the
video summarization task. Specifically, the self-supervised learning is
conducted by exploring the semantic consistency between the videos and text in
both coarse-grained and fine-grained fashions, as well as recovering masked
frames in the videos. The multimodal framework is trained on a newly-collected
dataset that consists of video-text pairs. Additionally, we introduce a
progressive video summarization method, where the important content in a video
is pinpointed progressively to generate better summaries. Extensive experiments
have proved the effectiveness and superiority of our method in rank correlation
coefficients and F-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COLD: A Benchmark for Chinese Offensive Language Detection. (arXiv:2201.06025v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06025">
<div class="article-summary-box-inner">
<span><p>Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Arabic Transformer Models. (arXiv:2201.07434v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07434">
<div class="article-summary-box-inner">
<span><p>Arabic is a Semitic language which is widely spoken with many dialects. Given
the success of pre-trained language models, many transformer models trained on
Arabic and its dialects have surfaced. While these models have been compared
with respect to downstream NLP tasks, no evaluation has been carried out to
directly compare the internal representations. We probe how linguistic
information is encoded in Arabic pretrained models, trained on different
varieties of Arabic language. We perform a layer and neuron analysis on the
models using three intrinsic tasks: two morphological tagging tasks based on
MSA (modern standard Arabic) and dialectal POS-tagging and a dialectal
identification task. Our analysis enlightens interesting findings such as: i)
word morphology is learned at the lower and middle layers ii) dialectal
identification necessitate more knowledge and hence preserved even in the final
layers, iii) despite a large overlap in their vocabulary, the MSA-based models
fail to capture the nuances of Arabic dialects, iv) we found that neurons in
embedding layers are polysemous in nature, while the neurons in middle layers
are exclusive to specific properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning. (arXiv:2201.08520v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08520">
<div class="article-summary-box-inner">
<span><p>We present a two-step hybrid reinforcement learning (RL) policy that is
designed to generate interpretable and robust hierarchical policies on the RL
problem with graph-based input. Unlike prior deep reinforcement learning
policies parameterized by an end-to-end black-box graph neural network, our
approach disentangles the decision-making process into two steps. The first
step is a simplified classification problem that maps the graph input to an
action group where all actions share a similar semantic meaning. The second
step implements a sophisticated rule-miner that conducts explicit one-hop
reasoning over the graph and identifies decisive edges in the graph input
without the necessity of heavy domain knowledge. This two-step hybrid policy
presents human-friendly interpretations and achieves better performance in
terms of generalization and robustness. Extensive experimental studies on four
levels of complex text-based games have demonstrated the superiority of the
proposed method compared to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search. (arXiv:2201.10866v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10866">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose the CodeRetriever model, which learns the
function-level code semantic representations through large-scale code-text
contrastive pre-training. We adopt two contrastive learning schemes in
CodeRetriever: unimodal contrastive learning and bimodal contrastive learning.
For unimodal contrastive learning, we design an unsupervised learning approach
to build semantic-related code pairs based on the documentation and function
name. For bimodal contrastive learning, we leverage the documentation and
in-line comments of code to build code-text pairs. Both contrastive objectives
can fully leverage large-scale code corpus for pre-training. Extensive
experimental results show that CodeRetriever achieves new state-of-the-art with
significant improvement over existing code pre-trained models, on eleven
domain/language-specific code search tasks with six programming languages in
different code granularity (function-level, snippet-level and statement-level).
These results demonstrate the effectiveness and robustness of CodeRetriever.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings. (arXiv:2201.12093v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12093">
<div class="article-summary-box-inner">
<span><p>Learning sentence embeddings in an unsupervised manner is fundamental in
natural language processing. Recent common practice is to couple pre-trained
language models with unsupervised contrastive learning, whose success relies on
augmenting a sentence with a semantically-close positive instance to construct
contrastive pairs. Nonetheless, existing approaches usually depend on a
mono-augmenting strategy, which causes learning shortcuts towards the
augmenting biases and thus corrupts the quality of sentence embeddings. A
straightforward solution is resorting to more diverse positives from a
multi-augmenting strategy, while an open question remains about how to
unsupervisedly learn from the diverse positives but with uneven augmenting
qualities in the text field. As one answer, we propose a novel Peer-Contrastive
Learning (PCL) with diverse augmentations. PCL constructs diverse contrastive
positives and negatives at the group level for unsupervised sentence
embeddings. PCL performs peer-positive contrast as well as peer-network
cooperation, which offers an inherent anti-bias ability and an effective way to
learn from diverse augmentations. Experiments on STS benchmarks verify the
effectiveness of PCL against its competitors in unsupervised sentence
embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings. (arXiv:2202.06671v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06671">
<div class="article-summary-box-inner">
<span><p>Learning scientific document representations can be substantially improved
through contrastive learning objectives, where the challenge lies in creating
positive and negative training samples that encode the desired similarity
semantics. Prior work relies on discrete citation relations to generate
contrast samples. However, discrete citations enforce a hard cut-off to
similarity. This is counter-intuitive to similarity-based learning, and ignores
that scientific papers can be very similar despite lacking a direct citation -
a core problem of finding related research. Instead, we use controlled nearest
neighbor sampling over citation graph embeddings for contrastive learning. This
control allows us to learn continuous similarity, to sample hard-to-learn
negatives and positives, and also to avoid collisions between negative and
positive samples by controlling the sampling margin between them. The resulting
method SciNCL outperforms the state-of-the-art on the SciDocs benchmark.
Furthermore, we demonstrate that it can train (or tune) models
sample-efficiently, and that it can be combined with recent training-efficient
methods. Perhaps surprisingly, even training a general-domain language model
this way outperforms baselines pretrained in-domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saving Dense Retriever from Shortcut Dependency in Conversational Search. (arXiv:2202.07280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07280">
<div class="article-summary-box-inner">
<span><p>Conversational search (CS) needs a holistic understanding of conversational
inputs to retrieve relevant passages. In this paper, we demonstrate the
existence of a retrieval shortcut in CS, which causes models to retrieve
passages solely relying on partial history while disregarding the latest
question. With in-depth analysis, we first show that naively trained dense
retrievers heavily exploit the shortcut and hence perform poorly when asked to
answer history-independent questions. To build more robust models against
shortcut dependency, we explore various hard negative mining strategies.
Experimental results show that training with the model-based hard negatives
effectively mitigates the dependency on the shortcut, significantly improving
dense retrievers on recent CS benchmarks. In particular, our retriever
outperforms the previous state-of-the-art model by 11.0 in Recall@10 on QReCC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphology Without Borders: Clause-Level Morphology. (arXiv:2202.12832v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12832">
<div class="article-summary-box-inner">
<span><p>Morphological tasks use large multi-lingual datasets that organize words into
inflection tables, which then serve as training and evaluation data for various
tasks. However, a closer inspection of these data reveals profound
cross-linguistic inconsistencies, that arise from the lack of a clear
linguistic and operational definition of what is a word, and that severely
impair the universality of the derived tasks. To overcome this deficiency, we
propose to view morphology as a clause-level phenomenon, rather than
word-level. It is anchored in a fixed yet inclusive set of features, that
encapsulates all functions realized in a saturated clause. We deliver
MightyMorph, a novel dataset for clause-level morphology covering 4
typologically-different languages: English, German, Turkish and Hebrew. We use
this dataset to derive 3 clause-level morphological tasks: inflection,
reinflection and analysis. Our experiments show that the clause-level tasks are
substantially harder than the respective word-level tasks, while having
comparable complexity across languages. Furthermore, redefining morphology to
the clause-level provides a neat interface with contextualized language models
(LMs) and allows assessing the morphological knowledge encoded in these models
and their usability for morphological tasks. Taken together, this work opens up
new horizons in the study of computational morphology, leaving ample space for
studying neural morphology cross-linguistically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03897">
<div class="article-summary-box-inner">
<span><p>Pre-trained large-scale models provide a transferable embedding, and they
show promising performance on diverse downstream tasks. However, the analysis
of learned embedding has not been explored well, and the transferability for
cross-modal tasks can be improved. This paper provides a perspective to
understand multi-modal embedding in terms of uniformity and alignment. We newly
find that the representation learned by multi-modal learning models such as
CLIP has two separated embedding spaces for each heterogeneous dataset with
less alignment. Besides, there are unexplored large intermediate areas between
the two modalities with less uniformity. As a result, lack of alignment and
uniformity might restrict the robustness and transferability of the
representation for the downstream task. To this end, we provide a new
end-to-end fine-tuning method for robust representation that encourages better
uniformity and alignment score. First, we propose a \textit{Geodesic
Multi-Modal Mixup} that mixes the representation of image and text to generate
the hard negative samples on the hyperspherical embedding space. Second, we
fine-tune the multi-modal model on hard negative samples as well as normal
negatives and positive samples with contrastive loss. Through extensive
experiments on retrieval, classification, and structure-awareness task, we
demonstrate that our geodesic multi-modal Mixup learns a robust representation
and provides improved performance on various downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04392">
<div class="article-summary-box-inner">
<span><p>Pretrained language models can be effectively stimulated by textual prompts
or demonstrations, especially in low-data scenarios. Recent works have focused
on automatically searching discrete or continuous prompts or optimized
verbalizers, yet studies for the demonstration are still limited. Concretely,
the demonstration examples are crucial for an excellent final performance of
prompt-tuning. In this paper, we propose a novel pluggable, extensible, and
efficient approach named contrastive demonstration tuning, which is free of
demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged
into any previous prompt-tuning approaches; (ii) Extended to widespread
classification tasks with a large number of categories. Experimental results on
16 datasets illustrate that our method integrated with previous approaches
LM-BFF and P-tuning can yield better performance. Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical N-Gram Framework for Zero-Shot Link Prediction. (arXiv:2204.10293v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10293">
<div class="article-summary-box-inner">
<span><p>Due to the incompleteness of knowledge graphs (KGs), zero-shot link
prediction (ZSLP) which aims to predict unobserved relations in KGs has
attracted recent interest from researchers. A common solution is to use textual
features of relations (e.g., surface name or textual descriptions) as auxiliary
information to bridge the gap between seen and unseen relations. Current
approaches learn an embedding for each word token in the text. These methods
lack robustness as they suffer from the out-of-vocabulary (OOV) problem.
Meanwhile, models built on character n-grams have the capability of generating
expressive representations for OOV words. Thus, in this paper, we propose a
Hierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which
considers the dependencies among character n-grams of the relation surface name
for ZSLP. Our approach works by first constructing a hierarchical n-gram graph
on the surface name to model the organizational structure of n-grams that leads
to the surface name. A GramTransformer, based on the Transformer is then
presented to model the hierarchical n-gram graph to construct the relation
embedding for ZSLP. Experimental results show the proposed HNZSLP achieved
state-of-the-art performance on two ZSLP datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures. (arXiv:2204.13692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13692">
<div class="article-summary-box-inner">
<span><p>Being able to rank the similarity of short text segments is an interesting
bonus feature of neural machine translation. Translation-based similarity
measures include direct and pivot translation probability, as well as
translation cross-likelihood, which has not been studied so far. We analyze
these measures in the common framework of multilingual NMT, releasing the
NMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared
to baselines such as sentence embeddings, translation-based measures prove
competitive in paraphrase identification and are more robust against
adversarial or multilingual input, especially if proper normalization is
applied. When used for reference-based evaluation of data-to-text generation in
2 tasks and 17 languages, translation-based measures show a relatively high
correlation to human judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying the Convergences in Multilingual Neural Machine Translation. (arXiv:2205.01620v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01620">
<div class="article-summary-box-inner">
<span><p>Although all-in-one-model multilingual neural machine translation
(multilingual NMT) has achieved remarkable progress, the convergence
inconsistency in the joint training is ignored, i.e., different language pairs
reaching convergence in different epochs. This leads to the trained MNMT model
over-fitting low-resource language translations while under-fitting
high-resource ones. In this paper, we propose a novel training strategy named
LSSD (Language-Specific Self-Distillation), which can alleviate the convergence
inconsistency and help MNMT models achieve the best performance on each
language pair simultaneously. Specifically, LSSD picks up language-specific
best checkpoints for each language pair to teach the current model on the fly.
Furthermore, we systematically explore three sample-level manipulations of
knowledge transferring. Experimental results on three datasets show that LSSD
obtains consistent improvements towards all language pairs and achieves the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outliers Dimensions that Disrupt Transformers Are Driven by Frequency. (arXiv:2205.11380v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11380">
<div class="article-summary-box-inner">
<span><p>While Transformer-based language models are generally very robust to pruning,
there is the recently discovered outlier phenomenon: disabling only 48 out of
110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We
replicate the original evidence for the outlier phenomenon and we link it to
the geometry of the embedding space. We find that in both BERT and RoBERTa the
magnitude of hidden state coefficients corresponding to outlier dimensions
correlates with the frequency of encoded tokens in pre-training data, and it
also contributes to the "vertical" self-attention pattern enabling the model to
focus on the special tokens. This explains the drop in performance from
disabling the outliers, and it suggests that to decrease anisotropicity in
future models we need pre-training schemas that would better take into account
the skewed token distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuned Language Models are Continual Learners. (arXiv:2205.12393v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12393">
<div class="article-summary-box-inner">
<span><p>Recent work on large language models relies on the intuition that most
natural language processing tasks can be described via natural language
instructions. Language models trained on these instructions show strong
zero-shot performance on several standard datasets. However, these models even
though impressive still perform poorly on a wide range of tasks outside of
their respective training and evaluation sets. To address this limitation, we
argue that a model should be able to keep extending its knowledge and
abilities, without forgetting previous skills. In spite of the limited success
of Continual Learning we show that Language Models can be continual learners.
We empirically investigate the reason for this success and conclude that
Continual Learning emerges from self-supervision pre-training. Our resulting
model Continual-T0 (CT0) is able to learn diverse new tasks, while still
maintaining good performance on previous tasks, spanning remarkably through 70
datasets in total. Finally, we show that CT0 is able to combine instructions in
ways it was never trained for, demonstrating some compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12986">
<div class="article-summary-box-inner">
<span><p>Sentence scoring aims at measuring the likelihood score of a sentence and is
widely used in many natural language processing scenarios, like reranking,
which is to select the best sentence from multiple candidates. Previous works
on sentence scoring mainly adopted either causal language modeling (CLM) like
GPT or masked language modeling (MLM) like BERT, which have some limitations:
1) CLM only utilizes unidirectional information for the probability estimation
of a sentence without considering bidirectional context, which affects the
scoring quality; 2) MLM can only estimate the probability of partial tokens at
a time and thus requires multiple forward passes to estimate the probability of
the whole sentence, which incurs large computation and time cost. In this
paper, we propose \textit{Transcormer} -- a Transformer model with a novel
\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,
our SLM adopts a triple-stream self-attention mechanism to estimate the
probability of all tokens in a sentence with bidirectional context and only
requires a single forward pass. SLM can avoid the limitations of CLM (only
unidirectional context) and MLM (multiple forward passes) and inherit their
advantages, and thus achieve high effectiveness and efficiency in scoring.
Experimental results on multiple tasks demonstrate that our method achieves
better performance than other language modelings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Generator-Ranker Learning for Natural Language Generation. (arXiv:2206.13974v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13974">
<div class="article-summary-box-inner">
<span><p>Generate-then-rank is a widely used mechanism for text generation, where a
generator produces multiple candidates and a ranker chooses the best one.
However, existing methods usually train the generator and the ranker
separately, which causes a lack of mutual feedback and a misalignment of their
objectives. This results in suboptimal generation quality. To address this
issue, we propose JGR, a novel joint training algorithm that integrates the
generator and the ranker in a single framework. JGR optimizes the generator
with a hybrid objective that combines data likelihood and ranker reward, and
trains the ranker with a contrastive loss that compares the generator outputs.
By alternately updating the generator and the ranker, JGR can effectively
harmonize their learning and enhance their quality jointly. We evaluate JGR on
various text generation tasks and demonstrate that it surpasses existing
methods on four public datasets across three common generation scenarios. Our
code, data, and models are available at https://github.com/microsoft/AdvNLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoeticTTS -- Controllable Poetry Reading for Literary Studies. (arXiv:2207.05549v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05549">
<div class="article-summary-box-inner">
<span><p>Speech synthesis for poetry is challenging due to specific intonation
patterns inherent to poetic speech. In this work, we propose an approach to
synthesise poems with almost human like naturalness in order to enable literary
scholars to systematically examine hypotheses on the interplay between text,
spoken realisation, and the listener's perception of poems. To meet these
special requirements for literary studies, we resynthesise poems by cloning
prosodic values from a human reference recitation, and afterwards make use of
fine-grained prosody control to manipulate the synthetic speech in a
human-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We
find that finetuning our TTS model on poetry captures poetic intonation
patterns to a large extent which is beneficial for prosody cloning and
manipulation and verify the success of our approach both in an objective
evaluation as well as in human studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing Pre-trained Language Models' Understanding of Distributivity via Causal Mediation Analysis. (arXiv:2209.04761v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.04761">
<div class="article-summary-box-inner">
<span><p>To what extent do pre-trained language models grasp semantic knowledge
regarding the phenomenon of distributivity? In this paper, we introduce
DistNLI, a new diagnostic dataset for natural language inference that targets
the semantic difference arising from distributivity, and employ the causal
mediation analysis framework to quantify the model behavior and explore the
underlying mechanism in this semantically-related task. We find that the extent
of models' understanding is associated with model size and vocabulary size. We
also provide insights into how models encode such high-level semantic
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Distributional Lens for Multi-Aspect Controllable Text Generation. (arXiv:2210.02889v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02889">
<div class="article-summary-box-inner">
<span><p>Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs. (arXiv:2210.04490v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04490">
<div class="article-summary-box-inner">
<span><p>Answering factual questions with temporal intent over knowledge graphs
(temporal KGQA) attracts rising attention in recent years. In the generation of
temporal queries, existing KGQA methods ignore the fact that some intrinsic
connections between events can make them temporally related, which may limit
their capability. We systematically analyze the possible interpretation of
temporal constraints and conclude the interpretation structures as the Semantic
Framework of Temporal Constraints, SF-TCons. Based on the semantic framework,
we propose a temporal question answering method, SF-TQA, which generates query
graphs by exploring the relevant facts of mentioned entities, where the
exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA
significantly outperforms existing methods on two benchmarks over different
knowledge graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network. (arXiv:2210.05499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05499">
<div class="article-summary-box-inner">
<span><p>Long document question answering is a challenging task due to its demands for
complex reasoning over long text. Previous works usually take long documents as
non-structured flat texts or only consider the local structure in long
documents. However, these methods usually ignore the global structure of the
long document, which is essential for long-range understanding. To tackle this
problem, we propose Compressive Graph Selector Network (CGSN) to capture the
global structure in a compressive and iterative manner. The proposed model
mainly focuses on the evidence selection phase of long document question
answering. Specifically, it consists of three modules: local graph network,
global graph network and evidence memory network. Firstly, the local graph
network builds the graph structure of the chunked segment in token, sentence,
paragraph and segment levels to capture the short-term dependency of the text.
Secondly, the global graph network selectively receives the information of each
level from the local graph, compresses them into the global graph nodes and
applies graph attention to the global graph nodes to build the long-range
reasoning over the entire text in an iterative way. Thirdly, the evidence
memory network is designed to alleviate the redundancy problem in the evidence
selection by saving the selected result in the previous steps. Extensive
experiments show that the proposed model outperforms previous methods on two
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MTet: Multi-domain Translation for English and Vietnamese. (arXiv:2210.05610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05610">
<div class="article-summary-box-inner">
<span><p>We introduce MTet, the largest publicly available parallel corpus for
English-Vietnamese translation. MTet consists of 4.2M high-quality training
sentence pairs and a multi-domain test set refined by the Vietnamese research
community. Combining with previous works on English-Vietnamese translation, we
grow the existing parallel dataset to 6.2M sentence pairs. We also release the
first pretrained model EnViT5 for English and Vietnamese languages. Combining
both resources, our model significantly outperforms previous state-of-the-art
results by up to 2 points in translation BLEU score, while being 1.6 times
smaller.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foundation Transformers. (arXiv:2210.06423v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06423">
<div class="article-summary-box-inner">
<span><p>A big convergence of model architectures across language, vision, speech, and
multimodal is emerging. However, under the same name "Transformers", the above
areas use different implementations for better performance, e.g.,
Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We
call for the development of Foundation Transformer for true general-purpose
modeling, which serves as a go-to architecture for various tasks and modalities
with guaranteed training stability. In this work, we introduce a Transformer
variant, named Magneto, to fulfill the goal. Specifically, we propose
Sub-LayerNorm for good expressivity, and the initialization strategy
theoretically derived from DeepNet for stable scaling up. Extensive experiments
demonstrate its superior performance and better stability than the de facto
Transformer variants designed for various applications, including language
modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e.,
BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller. (arXiv:2210.06694v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06694">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new task of sub-event generation for an unseen
process to evaluate the understanding of the coherence of sub-event actions and
objects. To solve the problem, we design SubeventWriter, a sub-event sequence
generation framework with a coherence controller. Given an unseen process, the
framework can iteratively construct the sub-event sequence by generating one
sub-event at each iteration. We also design a very effective coherence
controller to decode more coherent sub-events. As our extensive experiments and
analysis indicate, SubeventWriter can generate more reliable and meaningful
sub-event sequences for unseen processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithms for Weighted Pushdown Automata. (arXiv:2210.06884v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06884">
<div class="article-summary-box-inner">
<span><p>Weighted pushdown automata (WPDAs) are at the core of many natural language
processing tasks, like syntax-based statistical machine translation and
transition-based dependency parsing. As most existing dynamic programming
algorithms are designed for context-free grammars (CFGs), algorithms for PDAs
often resort to a PDA-to-CFG conversion. In this paper, we develop novel
algorithms that operate directly on WPDAs. Our algorithms are inspired by
Lang's algorithm, but use a more general definition of pushdown automaton and
either reduce the space requirements by a factor of $|\Gamma|$ (the size of the
stack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the
number of states). When run on the same class of PDAs as Lang's algorithm, our
algorithm is both more space-efficient by a factor of $|\Gamma|$ and more
time-efficient by a factor of $|Q| \cdot |\Gamma|$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Character-Centric Story Visualization via Visual Planning and Token Alignment. (arXiv:2210.08465v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08465">
<div class="article-summary-box-inner">
<span><p>Story visualization advances the traditional text-to-image generation by
enabling multiple image generation based on a complete story. This task
requires machines to 1) understand long text inputs and 2) produce a globally
consistent image sequence that illustrates the contents of the story. A key
challenge of consistent story visualization is to preserve characters that are
essential in stories. To tackle the challenge, we propose to adapt a recent
work that augments Vector-Quantized Variational Autoencoders (VQ-VAE) with a
text-tovisual-token (transformer) architecture. Specifically, we modify the
text-to-visual-token module with a two-stage framework: 1) character token
planning model that predicts the visual tokens for characters only; 2) visual
token completion model that generates the remaining visual token sequence,
which is sent to VQ-VAE for finalizing image generations. To encourage
characters to appear in the images, we further train the two-stage framework
with a character-token alignment objective. Extensive experiments and
evaluations demonstrate that the proposed method excels at preserving
characters and can produce higher quality image sequences compared with the
strong baselines. Codes can be found in https://github.com/sairin1202/VP-CSV
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling. (arXiv:2210.08753v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08753">
<div class="article-summary-box-inner">
<span><p>Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbol Guided Hindsight Priors for Reward Learning from Human Preferences. (arXiv:2210.09151v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09151">
<div class="article-summary-box-inner">
<span><p>Specifying rewards for reinforcement learned (RL) agents is challenging.
Preference-based RL (PbRL) mitigates these challenges by inferring a reward
from feedback over sets of trajectories. However, the effectiveness of PbRL is
limited by the amount of feedback needed to reliably recover the structure of
the target reward. We present the PRIor Over Rewards (PRIOR) framework, which
incorporates priors about the structure of the reward function and the
preference feedback into the reward learning process. Imposing these priors as
soft constraints on the reward learning objective reduces the amount of
feedback required by half and improves overall reward recovery. Additionally,
we demonstrate that using an abstract state space for the computation of the
priors further improves the reward learning and the agent's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Bidirectional Language-Knowledge Graph Pretraining. (arXiv:2210.09338v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09338">
<div class="article-summary-box-inner">
<span><p>Pretraining a language model (LM) on text has been shown to help various
downstream NLP tasks. Recent works show that a knowledge graph (KG) can
complement text data, offering structured background knowledge that provides a
useful scaffold for reasoning. However, these works are not pretrained to learn
a deep fusion of the two modalities at scale, limiting the potential to acquire
fully joint representations of text and KG. Here we propose DRAGON (Deep
Bidirectional Language-Knowledge Graph Pretraining), a self-supervised approach
to pretraining a deeply joint language-knowledge foundation model from text and
KG at scale. Specifically, our model takes pairs of text segments and relevant
KG subgraphs as input and bidirectionally fuses information from both
modalities. We pretrain this model by unifying two self-supervised reasoning
tasks, masked language modeling and KG link prediction. DRAGON outperforms
existing LM and LM+KG models on diverse downstream tasks including question
answering across general and biomedical domains, with +5% absolute gain on
average. In particular, DRAGON achieves notable performance on complex
reasoning about language and knowledge (+10% on questions involving long
contexts or multi-step reasoning) and low-resource QA (+8% on OBQA and
RiddleSense), and new state-of-the-art results on various BioNLP tasks. Our
code and trained models are available at
https://github.com/michiyasunaga/dragon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-granularity Argument Mining in Legal Texts. (arXiv:2210.09472v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09472">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore legal argument mining using multiple levels of
granularity. Argument mining has usually been conceptualized as a sentence
classification problem. In this work, we conceptualize argument mining as a
token-level (i.e., word-level) classification problem. We use a Longformer
model to classify the tokens. Results show that token-level text classification
identifies certain legal argument elements more accurately than sentence-level
text classification. Token-level classification also provides greater
flexibility to analyze legal texts and to gain more insight into what the model
focuses on when processing a large amount of input data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis. (arXiv:2210.09803v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.09803">
<div class="article-summary-box-inner">
<span><p>Most existing pre-trained language representation models (PLMs) are
sub-optimal in sentiment analysis tasks, as they capture the sentiment
information from word-level while under-considering sentence-level information.
In this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained
language model with combined Word-level and Sentence-level Pre-training tasks.
The word level pre-training task detects replaced sentiment words, via a
generator-discriminator framework, to enhance the PLM's knowledge about
sentiment words. The sentence level pre-training task further strengthens the
discriminator via a contrastive learning framework, with similar sentences as
negative samples, to encode sentiments in a sentence. Extensive experimental
results show that SentiWSP achieves new state-of-the-art performance on various
sentence-level and aspect-level sentiment classification benchmarks. We have
made our code and model publicly available at
https://github.com/XMUDM/SentiWSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning. (arXiv:2210.10041v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10041">
<div class="article-summary-box-inner">
<span><p>While transferring a pretrained language model, common approaches
conventionally attach their task-specific classifiers to the top layer and
adapt all the pretrained layers. We investigate whether one could make a
task-specific selection on which subset of the layers to adapt and where to
place the classifier. The goal is to reduce the computation cost of transfer
learning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its
performance.
</p>
<p>We propose to select layers based on the variability of their hidden states
given a task-specific corpus. We say a layer is already "well-specialized" in a
task if the within-class variability of its hidden states is low relative to
the between-class variability. Our variability metric is cheap to compute and
doesn't need any training or hyperparameter tuning. It is robust to data
imbalance and data scarcity. Extensive experiments on the GLUE benchmark
demonstrate that selecting layers based on our metric can yield significantly
stronger performance than using the same number of top layers and often match
the performance of fine-tuning or adapter-tuning the entire language model.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-20 23:22:26.036169621 UTC">2022-10-20 23:22:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>