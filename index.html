<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-24T01:30:00Z">08-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to generate and corr- uh I mean repair language in real-time. (arXiv:2308.11683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11683">
<div class="article-summary-box-inner">
<span><p>In conversation, speakers produce language incrementally, word by word, while
continuously monitoring the appropriateness of their own contribution in the
dynamically unfolding context of the conversation; and this often leads them to
repair their own utterance on the fly. This real-time language processing
capacity is furthermore crucial to the development of fluent and natural
conversational AI. In this paper, we use a previously learned Dynamic Syntax
grammar and the CHILDES corpus to develop, train and evaluate a probabilistic
model for incremental generation where input to the model is a purely semantic
generation goal concept in Type Theory with Records (TTR). We show that the
model's output exactly matches the gold candidate in 78% of cases with a
ROUGE-l score of 0.86. We further do a zero-shot evaluation of the ability of
the same model to generate self-repairs when the generation goal changes
mid-utterance. Automatic evaluation shows that the model can generate
self-repairs correctly in 85% of cases. A small human evaluation confirms the
naturalness and grammaticality of the generated self-repairs. Overall, these
results further highlight the generalisation power of grammar-based models and
lay the foundations for more controllable, and naturally interactive
conversational AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11696">
<div class="article-summary-box-inner">
<span><p>The increasing versatility of language models LMs has given rise to a new
class of benchmarks that comprehensively assess a broad range of capabilities.
Such benchmarks are associated with massive computational costs reaching
thousands of GPU hours per model. However the efficiency aspect of these
evaluation efforts had raised little discussion in the literature. In this work
we present the problem of Efficient Benchmarking namely intelligently reducing
the computation costs of LM evaluation without compromising reliability. Using
the HELM benchmark as a test case we investigate how different benchmark design
choices affect the computation-reliability tradeoff. We propose to evaluate the
reliability of such decisions by using a new measure Decision Impact on
Reliability DIoR for short. We find for example that the current leader on HELM
may change by merely removing a low-ranked model from the benchmark and observe
that a handful of examples suffice to obtain the correct benchmark ranking.
Conversely a slightly different choice of HELM scenarios varies ranking widely.
Based on our findings we outline a set of concrete recommendations for more
efficient benchmark design and utilization practices leading to dramatic cost
savings with minimal loss of benchmark reliability often reducing computation
by x100 or more.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion. (arXiv:2308.11720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11720">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) is a pivotal task in automatically extracting
structured information from unstructured text. In this paper, we present a
multi-faceted approach that integrates representative examples and through
co-set expansion. The primary goal of our method is to enhance relation
classification accuracy and mitigating confusion between contrastive classes.
</p>
<p>Our approach begins by seeding each relationship class with representative
examples. Subsequently, our co-set expansion algorithm enriches training
objectives by incorporating similarity measures between target pairs and
representative pairs from the target class. Moreover, the co-set expansion
process involves a class ranking procedure that takes into account exemplars
from contrastive classes. Contextual details encompassing relation mentions are
harnessed via context-free Hearst patterns to ascertain contextual similarity.
</p>
<p>Empirical evaluation demonstrates the efficacy of our co-set expansion
approach, resulting in a significant enhancement of relation classification
performance. Our method achieves an observed margin of at least 1 percent
improvement in accuracy in most settings, on top of existing fine-tuning
approaches. To further refine our approach, we conduct an in-depth analysis
that focuses on tuning contrastive examples. This strategic selection and
tuning effectively reduce confusion between classes sharing similarities,
leading to a more precise classification process.
</p>
<p>Experimental results underscore the effectiveness of our proposed framework
for relation extraction. The synergy between co-set expansion and context-aware
prompt tuning substantially contributes to improved classification accuracy.
Furthermore, the reduction in confusion between contrastive classes through
contrastive examples tuning validates the robustness and reliability of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11730">
<div class="article-summary-box-inner">
<span><p>The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LM-guided graph traverser that
navigates across nodes and gathers supporting passages assisting LLMs in MD-QA.
The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the LM-guided traverser acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code is at https://github.com/YuWVandy/KG-LLM-MDQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. (arXiv:2308.11761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11761">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated impressive impact in the field
of natural language processing, but they still struggle with several issues
regarding, such as completeness, timeliness, faithfulness and adaptability.
While recent efforts have focuses on connecting LLMs with external knowledge
sources, the integration of knowledge bases (KBs) remains understudied and
faces several challenges. In this paper, we introduce KnowledGPT, a
comprehensive framework to bridge LLMs with various knowledge bases,
facilitating both the retrieval and storage of knowledge. The retrieval process
employs the program of thought prompting, which generates search language for
KBs in code format with pre-defined functions for KB operations. Besides
retrieval, KnowledGPT offers the capability to store knowledge in a
personalized KB, catering to individual user demands. With extensive
experiments, we show that by integrating LLMs with KBs, KnowledGPT properly
answers a broader range of questions requiring world knowledge compared with
vanilla LLMs, utilizing both knowledge existing in widely-known KBs and
extracted into personalized KBs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11764">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11767">
<div class="article-summary-box-inner">
<span><p>ChatGPT is becoming a new reality. In this paper, we show how to distinguish
ChatGPT-generated publications from counterparts produced by scientists. Using
a newly designed supervised Machine Learning algorithm, we demonstrate how to
detect machine-generated publications from those produced by scientists. The
algorithm was trained using 100 real publication abstracts, followed by a
10-fold calibration approach to establish a lower-upper bound range of
acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT
contributed merely 23\% of the bigram content, which is less than 50\% of any
of the other 10 calibrating folds. This analysis highlights a significant
disparity in technical terms where ChatGPT fell short of matching real science.
When categorizing the individual articles, the xFakeBibs algorithm accurately
identified 98 out of 100 publications as fake, with 2 articles incorrectly
classified as real publications. Though this work introduced an algorithmic
approach that detected the ChatGPT-generated fake science with a high degree of
accuracy, it remains challenging to detect all fake records. This work is
indeed a step in the right direction to counter fake science and
misinformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model. (arXiv:2308.11773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11773">
<div class="article-summary-box-inner">
<span><p>Language use has been shown to correlate with depression, but large-scale
validation is needed. Traditional methods like clinic studies are expensive.
So, natural language processing has been employed on social media to predict
depression, but limitations remain-lack of validated labels, biased user
samples, and no context. Our study identified 29 topics in 3919
smartphone-collected speech recordings from 265 participants using the Whisper
tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal
to 10 were regarded as risk topics for depression: No Expectations, Sleep,
Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic
emergence and associations with depression, we compared behavioral (from
wearables) and linguistic characteristics across identified topics. The
correlation between topic shifts and changes in depression severity over time
was also investigated, indicating the importance of longitudinally monitoring
language use. We also tested the BERTopic model on a similar smaller dataset
(356 speech recordings from 57 participants), obtaining some consistent
results. In summary, our findings demonstrate specific speech topics may
indicate depression severity. The presented data-driven workflow provides a
practical approach to collecting and analyzing large-scale speech data from
real-world settings for digital health research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11780">
<div class="article-summary-box-inner">
<span><p>Most current methods for detecting anomalies in text concentrate on
constructing models solely relying on unlabeled data. These models operate on
the presumption that no labeled anomalous examples are available, which
prevents them from utilizing prior knowledge of anomalies that are typically
present in small numbers in many real-world applications. Furthermore, these
models prioritize learning feature embeddings rather than optimizing anomaly
scores directly, which could lead to suboptimal anomaly scoring and inefficient
use of data during the learning process. In this paper, we introduce FATE, a
deep few-shot learning-based framework that leverages limited anomaly examples
and learns anomaly scores explicitly in an end-to-end method using deviation
learning. In this approach, the anomaly scores of normal examples are adjusted
to closely resemble reference scores obtained from a prior distribution.
Conversely, anomaly samples are forced to have anomalous scores that
considerably deviate from the reference score in the upper tail of the prior.
Additionally, our model is optimized to learn the distinct behavior of
anomalies by utilizing a multi-head self-attention layer and multiple instance
learning approaches. Comprehensive experiments on several benchmark datasets
demonstrate that our proposed approach attains a new level of state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards an On-device Agent for Text Rewriting. (arXiv:2308.11807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11807">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated impressive capabilities for
text rewriting. Nonetheless, the large sizes of these models make them
impractical for on-device inference, which would otherwise allow for enhanced
privacy and economical inference. Creating a smaller yet potent language model
for text rewriting presents a formidable challenge because it requires
balancing the need for a small size with the need to retain the emergent
capabilities of the LLM, that requires costly data collection. To address the
above challenge, we introduce a new instruction tuning approach for building a
mobile-centric text rewriting model. Our strategies enable the generation of
high quality training data without any human labeling. In addition, we propose
a heuristic reinforcement learning framework which substantially enhances
performance without requiring preference data. To further bridge the
performance gap with the larger server-side model, we propose an effective
approach that combines the mobile rewrite agent with the server model using a
cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce
MessageRewriteEval, a benchmark that focuses on text rewriting for messages
through natural language instructions. Through empirical experiments, we
demonstrate that our on-device model surpasses the current state-of-the-art
LLMs in text rewriting while maintaining a significantly reduced model size.
Notably, we show that our proposed cascading approach improves model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11827">
<div class="article-summary-box-inner">
<span><p>Large language models such as Open AI's Generative Pre-trained Transformer
(GPT) models are proficient at answering questions, but their knowledge is
confined to the information present in their training data. This limitation
renders them ineffective when confronted with questions about recent
developments or non-public documents. Our research proposes a method that
enables GPT models to answer questions by employing context from an information
source not previously included in their training data. The methodology includes
preprocessing of contextual information, the embedding of contexts and queries,
constructing prompt through the integration of context embeddings, and
generating answers using GPT models. We applied this method in a controlled
test scenario using the California Driver's Handbook as the information source.
The GPT-3 model achieved a 96% passing score on a set of 50 sample driving
knowledge test questions. In contrast, without context, the model's passing
score fell to 82%. However, the model still fails to answer some questions
correctly even with providing library of context, highlighting room for
improvement. The research also examined the impact of prompt length and context
format, on the model's performance. Overall, the study provides insights into
the limitations and potential improvements for GPT models in question-answering
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11878">
<div class="article-summary-box-inner">
<span><p>The strategy of training the model from scratch in a specific language or
domain serves two essential purposes: i) enhancing performance in the
particular linguistic or domain context, and ii) ensuring effective
tokenization. The main limitation inherent to this approach lies in the
associated cost, which can reach six to seven-digit dollar values, depending on
the model size and the number of parameters involved.
</p>
<p>The main solution to overcome the cost challenge is to rely on available
pre-trained models, which, despite recent advancements such as the LLaMA and
LLaMA-2 models, still demonstrate inefficiency for certain specific domain
problems or prove ineffective in scenarios involving conversational memory
resources, given the large number of tokens required to represent text.
</p>
<p>To overcome this issue, we present a methodology named Cabrita, which, as our
research demonstrates, successfully addresses the performance and efficient
tokenization problem, all at an affordable cost. We believe that this
methodology can be applied to any transformer-like architecture model. To
validate the study, we conducted continuous pre-training exclusively using
Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in
a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer
that results in a significant reduction in the number of tokens required to
represent the text. In our assessment, for few-shot learning tasks, we achieved
similar results with this 3B model compared to a traditional continuous
pre-training approach as well as to 7B models English pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap: Deciphering Tabular Data Using Large Language Model. (arXiv:2308.11891v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11891">
<div class="article-summary-box-inner">
<span><p>In the realm of natural language processing, the understanding of tabular
data has perpetually stood as a focal point of scholarly inquiry. The emergence
of expansive language models, exemplified by the likes of ChatGPT, has ushered
in a wave of endeavors wherein researchers aim to harness these models for
tasks related to table-based question answering. Central to our investigative
pursuits is the elucidation of methodologies that amplify the aptitude of such
large language models in discerning both the structural intricacies and
inherent content of tables, ultimately facilitating their capacity to provide
informed responses to pertinent queries. To this end, we have architected a
distinctive module dedicated to the serialization of tables for seamless
integration with expansive language models. Additionally, we've instituted a
corrective mechanism within the model to rectify potential inaccuracies.
Experimental results indicate that, although our proposed method trails the
SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about
1.2% in tests on specific datasets. This research marks the first application
of large language models to table-based question answering tasks, enhancing the
model's comprehension of both table structures and content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement. (arXiv:2308.11923v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11923">
<div class="article-summary-box-inner">
<span><p>We proposed Audio Difference Captioning (ADC) as a new extension task of
audio captioning for describing the semantic differences between input pairs of
similar but slightly different audio clips. The ADC solves the problem that
conventional audio captioning sometimes generates similar captions for similar
audio clips, failing to describe the difference in content. We also propose a
cross-attention-concentrated transformer encoder to extract differences by
comparing a pair of audio clips and a similarity-discrepancy disentanglement to
emphasize the difference in the latent space. To evaluate the proposed methods,
we built an AudioDiffCaps dataset consisting of pairs of similar but slightly
different audio clips with human-annotated descriptions of their differences.
The experiment with the AudioDiffCaps dataset showed that the proposed methods
solve the ADC task effectively and improve the attention weights to extract the
difference by visualizing them in the transformer encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11940">
<div class="article-summary-box-inner">
<span><p>Text-based audio generation models have limitations as they cannot encompass
all the information in audio, leading to restricted controllability when
relying solely on text. To address this issue, we propose a novel model that
enhances the controllability of existing pre-trained text-to-audio models by
incorporating additional conditions including content (timestamp) and style
(pitch contour and energy contour) as supplements to the text. This approach
achieves fine-grained control over the temporal order, pitch, and energy of
generated audio. To preserve the diversity of generation, we employ a trainable
control condition encoder that is enhanced by a large language model and a
trainable Fusion-Net to encode and fuse the additional conditions while keeping
the weights of the pre-trained text-to-audio model frozen. Due to the lack of
suitable datasets and evaluation metrics, we consolidate existing datasets into
a new dataset comprising the audio and corresponding conditions and use a
series of evaluation metrics to evaluate the controllability performance.
Experimental results demonstrate that our model successfully achieves
fine-grained control to accomplish controllable audio generation. Audio samples
and our dataset are publicly available at
https://conditionaudiogen.github.io/conditionaudiogen/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11971">
<div class="article-summary-box-inner">
<span><p>Building scalable vision-language models to learn from diverse, multimodal
data remains an open challenge. In this paper, we introduce an Efficient
Vision-languagE foundation model, namely EVE, which is one unified multimodal
Transformer pre-trained solely by one unified pre-training task. Specifically,
EVE encodes both vision and language within a shared Transformer network
integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which
capture modality-specific information by selectively switching to different
experts. To unify pre-training tasks of vision and language, EVE performs
masked signal modeling on image-text pairs to reconstruct masked signals, i.e.,
image pixels and text tokens, given visible signals. This simple yet effective
pre-training objective accelerates training by 3.5x compared to the model
pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing
to the combination of the unified architecture and pre-training task, EVE is
easy to scale up, enabling better downstream performance with fewer resources
and faster training speed. Despite its simplicity, EVE achieves
state-of-the-art performance on various vision-language downstream tasks,
including visual question answering, visual reasoning, and image-text
retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. (arXiv:2308.11995v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11995">
<div class="article-summary-box-inner">
<span><p>Building socialbots that can have deep, engaging open-domain conversations
with humans is one of the grand challenges of artificial intelligence (AI). To
this end, bots need to be able to leverage world knowledge spanning several
domains effectively when conversing with humans who have their own world
knowledge. Existing knowledge-grounded conversation datasets are primarily
stylized with explicit roles for conversation partners. These datasets also do
not explore depth or breadth of topical coverage with transitions in
conversations. We introduce Topical-Chat, a knowledge-grounded human-human
conversation dataset where the underlying knowledge spans 8 broad topics and
conversation partners don't have explicitly defined roles, to help further
research in open-domain conversational AI. We also train several
state-of-the-art encoder-decoder conversational models on Topical-Chat and
perform automated and human evaluation for benchmarking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature. (arXiv:2308.12008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12008">
<div class="article-summary-box-inner">
<span><p>Intertextual allusions hold a pivotal role in Classical Philology, with Latin
authors frequently referencing Ancient Greek texts. Until now, the automatic
identification of these intertextual references has been constrained to
monolingual approaches, seeking parallels solely within Latin or Greek texts.
In this study, we introduce SPhilBERTa, a trilingual Sentence-RoBERTa model
tailored for Classical Philology, which excels at cross-lingual semantic
comprehension and identification of identical sentences across Ancient Greek,
Latin, and English. We generate new training data by automatically translating
English texts into Ancient Greek. Further, we present a case study,
demonstrating SPhilBERTa's capability to facilitate automated detection of
intertextual parallels. Our models and resources are available at
https://github.com/Heidelberg-NLP/ancient-language-models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12014">
<div class="article-summary-box-inner">
<span><p>Big models, exemplified by Large Language Models (LLMs), are models typically
pre-trained on massive data and comprised of enormous parameters, which not
only obtain significantly improved performance across diverse tasks but also
present emergent capabilities absent in smaller models. However, the growing
intertwining of big models with everyday human lives poses potential risks and
might cause serious social harm. Therefore, many efforts have been made to
align LLMs with humans to make them better follow user instructions and satisfy
human preferences. Nevertheless, `what to align with' has not been fully
discussed, and inappropriate alignment goals might even backfire. In this
paper, we conduct a comprehensive survey of different alignment goals in
existing work and trace their evolution paths to help identify the most
essential goal. Particularly, we investigate related works from two
perspectives: the definition of alignment goals and alignment evaluation. Our
analysis encompasses three distinct levels of alignment goals and reveals a
goal transformation from fundamental abilities to value orientation, indicating
the potential of intrinsic human values as the alignment goal for enhanced
LLMs. Based on such results, we further discuss the challenges of achieving
such intrinsic value alignment and provide a collection of available resources
for future research on the alignment of big models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information. (arXiv:2308.12022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12022">
<div class="article-summary-box-inner">
<span><p>Passage reranking is a crucial task in many applications, particularly when
dealing with large-scale documents. Traditional neural architectures are
limited in retrieving the best passage for a question because they usually
match the question to each passage separately, seldom considering contextual
information in other passages that can provide comparison and reference
information. This paper presents a list-context attention mechanism to augment
the passage representation by incorporating the list-context information from
other candidates. The proposed coarse-to-fine (C2F) neural retriever addresses
the out-of-memory limitation of the passage attention mechanism by dividing the
list-context modeling process into two sub-processes, allowing for efficient
encoding of context information from a large number of candidate answers. This
method can be generally used to encode context information from any number of
candidate answers in one pass. Different from most multi-stage information
retrieval architectures, this model integrates the coarse and fine rankers into
the joint optimization process, allowing for feedback between the two layers to
update the model simultaneously. Experiments demonstrate the effectiveness of
the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization. (arXiv:2308.12025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12025">
<div class="article-summary-box-inner">
<span><p>The Biomedical Entity Normalization (BEN) task aims to align raw,
unstructured medical entities to standard entities, thus promoting data
coherence and facilitating better downstream medical applications. Recently,
prompt learning methods have shown promising results in this task. However,
existing research falls short in tackling the more complex Chinese BEN task,
especially in the few-shot scenario with limited medical data, and the vast
potential of the external medical knowledge base has yet to be fully harnessed.
To address these challenges, we propose a novel Knowledge-injected Prompt
Learning (PL-Knowledge) method. Specifically, our approach consists of five
stages: candidate entity matching, knowledge extraction, knowledge encoding,
knowledge injection, and prediction output. By effectively encoding the
knowledge items contained in medical entities and incorporating them into our
tailor-made knowledge-injected templates, the additional knowledge enhances the
model's ability to capture latent relationships between medical entities, thus
achieving a better match with the standard entities. We extensively evaluate
our model on a benchmark dataset in both few-shot and full-scale scenarios. Our
method outperforms existing baselines, with an average accuracy boost of
12.96\% in few-shot and 0.94\% in full-data cases, showcasing its excellence in
the BEN task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12030">
<div class="article-summary-box-inner">
<span><p>Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted
great attention given their surprising improvement and performance. Length
controlled generation of LLMs emerges as an important topic, which also enables
users to fully leverage the capability of LLMs in more real-world scenarios
like generating a proper answer or essay of a desired length. In addition, the
autoregressive generation in LLMs is extremely time-consuming, while the
ability of controlling this generated length can arbitrarily reduce the
inference cost by limiting the length, and thus satisfy different needs.
Therefore, we aim to propose a prompt-based length control method to achieve
this length controlled generation, which can also be widely applied in
GPT-style LLMs. In particular, we adopt reinforcement learning with the reward
signal given by either trainable or rule-based reward model, which further
affects the generation of LLMs via rewarding a pre-defined target length.
Experiments show that our method significantly improves the accuracy of
prompt-based length control for summarization task on popular datasets like
CNNDM and NYT. We believe this length-controllable ability can provide more
potentials towards the era of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12032">
<div class="article-summary-box-inner">
<span><p>In the realm of Large Language Models, the balance between instruction data
quality and quantity has become a focal point. Recognizing this, we introduce a
self-guided methodology for LLMs to autonomously discern and select cherry
samples from vast open-source datasets, effectively minimizing manual curation
and potential cost for instruction tuning an LLM. Our key innovation, the
Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to
identify discrepancies between a model's expected responses and its autonomous
generation prowess. Through the adept application of IFD, cherry samples are
pinpointed, leading to a marked uptick in model training efficiency. Empirical
validations on renowned datasets like Alpaca and WizardLM underpin our
findings; with a mere 10% of conventional data input, our strategy showcases
improved results. This synthesis of self-guided cherry-picking and the IFD
metric signifies a transformative leap in the optimization of LLMs, promising
both efficiency and resource-conscious advancements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine. (arXiv:2308.12033v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12033">
<div class="article-summary-box-inner">
<span><p>As an effective tool for eliciting the power of Large Language Models (LLMs),
prompting has recently demonstrated unprecedented abilities across a variety of
complex tasks. To further improve the performance, prompt ensemble has
attracted substantial interest for tackling the hallucination and instability
of LLMs. However, existing methods usually adopt a two-stage paradigm, which
requires a pre-prepared set of prompts with substantial manual effort, and is
unable to perform directed optimization for different weak learners. In this
paper, we propose a simple, universal, and automatic method named PREFER (Pompt
Ensemble learning via Feedback-Reflect-Refine) to address the stated
limitations. Specifically, given the fact that weak learners are supposed to
focus on hard examples during boosting, PREFER builds a feedback mechanism for
reflecting on the inadequacies of existing weak learners. Based on this, the
LLM is required to automatically synthesize new prompts for iterative
refinement. Moreover, to enhance stability of the prompt effect evaluation, we
propose a novel prompt bagging method involving forward and backward thinking,
which is superior to majority voting and is beneficial for both feedback and
weight calculation in boosting. Extensive experiments demonstrate that our
PREFER achieves state-of-the-art performance in multiple types of tasks by a
significant margin. We have made our code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12038">
<div class="article-summary-box-inner">
<span><p>Recently there has been a significant surge in multimodal learning in terms
of both image-to-text and text-to-image generation. However, the success is
typically limited to English, leaving other languages largely behind. Building
a competitive counterpart in other languages is highly challenging due to the
low-resource nature of non-English multimodal data (i.e., lack of large-scale,
high-quality image-text data). In this work, we propose MPM, an effective
training paradigm for training large multimodal models in low-resource
languages. MPM demonstrates that Multilingual language models can Pivot
zero-shot Multimodal learning across languages. Specifically, based on a strong
multilingual large language model, multimodal models pretrained on English-only
image-text data can well generalize to other languages in a zero-shot manner
for both image-to-text and text-to-image generation, even surpassing models
trained on image-text data in native languages. Taking Chinese as a practice of
MPM, we build large multimodal models VisCPM in image-to-text and text-to-image
generation, which achieve state-of-the-art (open-source) performance in
Chinese. To facilitate future research, we open-source codes and model weights
at https://github.com/OpenBMB/VisCPM.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track. (arXiv:2308.12039v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12039">
<div class="article-summary-box-inner">
<span><p>Large-scale text retrieval technology has been widely used in various
practical business scenarios. This paper presents our systems for the TREC 2022
Deep Learning Track. We explain the hybrid text retrieval and multi-stage text
ranking method adopted in our solution. The retrieval stage combined the two
structures of traditional sparse retrieval and neural dense retrieval. In the
ranking stage, in addition to the full interaction-based ranking model built on
large pre-trained language model, we also proposes a lightweight sub-ranking
module to further enhance the final text ranking performance. Evaluation
results demonstrate the effectiveness of our proposed approach. Our models
achieve the 1st and 4th rank on the test set of passage ranking and document
ranking respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning. (arXiv:2308.12043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12043">
<div class="article-summary-box-inner">
<span><p>With the increasing size of pre-trained language models (PLMs), fine-tuning
all the parameters in the model is not efficient, especially when there are a
large number of downstream tasks, which incur significant training and storage
costs. Many parameter-efficient fine-tuning (PEFT) approaches have been
proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach
that injects trainable rank decomposition matrices into every target module.
Yet LoRA ignores the importance of parameters in different modules. To address
this problem, many works have been proposed to prune the parameters of LoRA.
However, under limited training conditions, the upper bound of the rank of the
pruned parameter matrix is still affected by the preset values. We, therefore,
propose IncreLoRA, an incremental parameter allocation method that adaptively
adds trainable parameters during training based on the importance scores of
each module. This approach is different from the pruning method as it is not
limited by the initial number of training parameters, and each parameter matrix
has a higher rank upper bound for the same training overhead. We conduct
extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA.
The results show that our method owns higher parameter efficiency, especially
when under the low-resource settings where our method significantly outperforms
the baselines. Our code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CgT-GAN: CLIP-guided Text GAN for Image Captioning. (arXiv:2308.12045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12045">
<div class="article-summary-box-inner">
<span><p>The large-scale visual-language pre-trained model, Contrastive Language-Image
Pre-training (CLIP), has significantly improved image captioning for scenarios
without human-annotated image-caption pairs. Recent advanced CLIP-based image
captioning without human annotations follows a text-only training paradigm,
i.e., reconstructing text from shared embedding space. Nevertheless, these
approaches are limited by the training/inference gap or huge storage
requirements for text embeddings. Given that it is trivial to obtain images in
the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates
images into the training process to enable the model to "see" real visual
modality. Particularly, we use adversarial training to teach CgT-GAN to mimic
the phrases of an external text corpus and CLIP-based reward to provide
semantic guidance. The caption generator is jointly rewarded based on the
caption naturalness to human language calculated from the GAN's discriminator
and the semantic guidance reward computed by the CLIP-based reward module. In
addition to the cosine similarity as the semantic guidance reward (i.e.,
CLIP-cos), we further introduce a novel semantic guidance reward called
CLIP-agg, which aligns the generated caption with a weighted text embedding by
attentively aggregating the entire corpus. Experimental results on three
subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms
state-of-the-art methods significantly across all metrics. Code is available at
https://github.com/Lihr747/CgtGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Language Models with Offline Reinforcement Learning from Human Feedback. (arXiv:2308.12050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12050">
<div class="article-summary-box-inner">
<span><p>Learning from human preferences is crucial for language models (LMs) to
effectively cater to human needs and societal values. Previous research has
made notable progress by leveraging human feedback to follow instructions.
However, these approaches rely primarily on online reinforcement learning (RL)
techniques like Proximal Policy Optimization (PPO), which have been proven
unstable and challenging to tune for language models. Moreover, PPO requires
complex distributed system implementation, hindering the efficiency of
large-scale distributed training. In this study, we propose an offline
reinforcement learning from human feedback (RLHF) framework to align LMs using
pre-generated samples without interacting with RL environments. Specifically,
we explore maximum likelihood estimation (MLE) with filtering, reward-weighted
regression (RWR), and Decision Transformer (DT) to align language models to
human preferences. By employing a loss function similar to supervised
fine-tuning, our methods ensure more stable model training than PPO with a
simple machine learning system~(MLSys) and much fewer (around 12.3\%) computing
resources. Experimental results demonstrate the DT alignment outperforms other
Offline RLHF methods and is better than PPO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12060">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) is a critical yet challenging task
due to the vast number of entities within knowledge bases and the diversity of
natural language questions posed by users. Unfortunately, the performance of
most KBQA models tends to decline significantly in real-world scenarios where
high-quality annotated data is insufficient. To mitigate the burden associated
with manual annotation, we introduce FlexKBQA by utilizing Large Language
Models (LLMs) as program translators for addressing the challenges inherent in
the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms
to sample diverse programs, such as SPARQL queries, from the knowledge base,
which are subsequently converted into natural language questions via LLMs. This
synthetic dataset facilitates training a specialized lightweight model for the
KB. Additionally, to reduce the barriers of distribution shift between
synthetic data and real user questions, FlexKBQA introduces an executionguided
self-training method to iterative leverage unlabeled user questions.
Furthermore, we explore harnessing the inherent reasoning capability of LLMs to
enhance the entire framework. Consequently, FlexKBQA delivers substantial
flexibility, encompassing data annotation, deployment, and being domain
agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we
observe that under the few-shot even the more challenging zero-shot scenarios,
FlexKBQA achieves impressive results with a few annotations, surpassing all
previous baselines and even approaching the performance of supervised models,
achieving a remarkable 93% performance relative to the fully-supervised models.
We posit that FlexKBQA represents a significant advancement towards exploring
better integration of large and lightweight models. The code is open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12067">
<div class="article-summary-box-inner">
<span><p>Multimodal large language models acquire their instruction-following
capabilities through a two-stage training process: pre-training on image-text
pairs and fine-tuning on supervised vision-language instruction data. Recent
studies have shown that large language models can achieve satisfactory results
even with a limited amount of high-quality instruction-following data. In this
paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset
comprising only 200 examples, amounting to approximately 6% of the
instruction-following data used in the alignment dataset for MiniGPT-4. We
first propose several metrics to access the quality of multimodal instruction
data. Based on these metrics, we present a simple and effective data selector
to automatically identify and filter low-quality vision-language data. By
employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on
various evaluations (e.g., visual question answering, GPT-4 preference).
Overall, our findings demonstrate that less but high-quality instruction tuning
data is efficient to enable multimodal large language models to generate better
output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments. (arXiv:2308.12086v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12086">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have gained widespread popularity across diverse
domains involving text generation, summarization, and various natural language
processing tasks. Despite their inherent limitations, LLM-based designs have
shown promising capabilities in planning and navigating open-world scenarios.
This paper introduces a novel application of pre-trained LLMs as agents within
cybersecurity network environments, focusing on their utility for sequential
decision-making processes.
</p>
<p>We present an approach wherein pre-trained LLMs are leveraged as attacking
agents in two reinforcement learning environments. Our proposed agents
demonstrate similar or better performance against state-of-the-art agents
trained for thousands of episodes in most scenarios and configurations. In
addition, the best LLM agents perform similarly to human testers of the
environment without any additional training process. This design highlights the
potential of LLMs to efficiently address complex decision-making tasks within
cybersecurity.
</p>
<p>Furthermore, we introduce a new network security environment named
NetSecGame. The environment is designed to eventually support complex
multi-agent scenarios within the network security domain. The proposed
environment mimics real network attacks and is designed to be highly modular
and adaptable for various scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Position Matters in Sequence Generation with Large Language Models. (arXiv:2308.12097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12097">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are capable of performing conditional sequence
generation tasks, such as translation or summarization, through instruction
fine-tuning. The fine-tuning data is generally sequentially concatenated from a
specific task instruction, an input sentence, and the corresponding response.
Considering the locality modeled by the self-attention mechanism of LLMs, these
models face the risk of instruction forgetting when generating responses for
long input sentences. To mitigate this issue, we propose enhancing the
instruction-following capability of LLMs by shifting the position of task
instructions after the input sentences. Theoretical analysis suggests that our
straightforward method can alter the model's learning focus, thereby
emphasizing the training of instruction-following capabilities. Concurrently,
experimental results demonstrate that our approach consistently outperforms
traditional settings across various model scales (1B / 7B / 13B) and different
sequence generation tasks (translation and summarization), without any
additional data or annotation costs. Notably, our method significantly improves
the zero-shot performance on conditional sequence generation, e.g., up to 9.7
BLEU points on WMT zero-shot translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Change Detection for the Romanian Language. (arXiv:2308.12131v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12131">
<div class="article-summary-box-inner">
<span><p>Automatic semantic change methods try to identify the changes that appear
over time in the meaning of words by analyzing their usage in diachronic
corpora. In this paper, we analyze different strategies to create static and
contextual word embedding models, i.e., Word2Vec and ELMo, on real-world
English and Romanian datasets. To test our pipeline and determine the
performance of our models, we first evaluate both word embedding models on an
English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a
Romanian dataset, and we underline different aspects of semantic changes in
this low-resource language, such as meaning acquisition and loss. The
experimental results show that, depending on the corpus, the most important
factors to consider are the choice of model and the distance to calculate a
score for detecting semantic change.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Faithfulness Using the Longest Supported Subsequence. (arXiv:2308.12157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12157">
<div class="article-summary-box-inner">
<span><p>As increasingly sophisticated language models emerge, their trustworthiness
becomes a pivotal issue, especially in tasks such as summarization and
question-answering. Ensuring their responses are contextually grounded and
faithful is challenging due to the linguistic diversity and the myriad of
possible answers. In this paper, we introduce a novel approach to evaluate
faithfulness of machine-generated text by computing the longest noncontinuous
substring of the claim that is supported by the context, which we refer to as
the Longest Supported Subsequence (LSS). Using a new human-annotated dataset,
we finetune a model to generate LSS. We introduce a new method of evaluation
and demonstrate that these metrics correlate better with human ratings when LSS
is employed, as opposed to when it is not. Our proposed metric demonstrates an
18% enhancement over the prevailing state-of-the-art metric for faithfulness on
our dataset. Our metric consistently outperforms other metrics on a
summarization dataset across six different models. Finally, we compare several
popular Large Language Models (LLMs) for faithfulness using this metric. We
release the human-annotated dataset built for predicting LSS and our fine-tuned
model for evaluating faithfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum Learning with Adam: The Devil Is in the Wrong Details. (arXiv:2308.12202v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12202">
<div class="article-summary-box-inner">
<span><p>Curriculum learning (CL) posits that machine learning models -- similar to
humans -- may learn more efficiently from data that match their current
learning progress. However, CL methods are still poorly understood and, in
particular for natural language processing (NLP), have achieved only limited
success. In this paper, we explore why. Starting from an attempt to replicate
and extend a number of recent curriculum methods, we find that their results
are surprisingly brittle when applied to NLP. A deep dive into the
(in)effectiveness of the curricula in some scenarios shows us why: when
curricula are employed in combination with the popular Adam optimisation
algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation
parameters for this algorithm. We present a number of different case studies
with different common hand-crafted and automated CL approaches to illustrate
this phenomenon, and we find that none of them outperforms optimisation with
only Adam with well-chosen hyperparameters. As such, our results contribute to
understanding why CL methods work, but at the same time urge caution when
claiming positive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12215">
<div class="article-summary-box-inner">
<span><p>We examine the disconnect between scholarship and practice in applying
machine learning to trust and safety problems, using misinformation detection
as a case study. We systematize literature on automated detection of
misinformation across a corpus of 270 well-cited papers in the field. We then
examine subsets of papers for data and code availability, design missteps,
reproducibility, and generalizability. We find significant shortcomings in the
literature that call into question claimed performance and practicality.
Detection tasks are often meaningfully distinct from the challenges that online
services actually face. Datasets and model evaluation are often
non-representative of real-world contexts, and evaluation frequently is not
independent of model training. Data and code availability is poor. Models do
not generalize well to out-of-domain data. Based on these results, we offer
recommendations for evaluating machine learning applications to trust and
safety problems. Our aim is for future work to avoid the pitfalls that we
identify.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12219">
<div class="article-summary-box-inner">
<span><p>The recent surge of generative AI has been fueled by the generative power of
diffusion probabilistic models and the scalable capabilities of large language
models. Despite their potential, it remains elusive whether diffusion language
models can solve general language tasks comparable to their autoregressive
counterparts. This paper demonstrates that scaling diffusion models w.r.t.
data, sizes, and tasks can effectively make them strong language learners. We
build competent diffusion language models at scale by first acquiring knowledge
from massive data via masked language modeling pretraining thanks to their
intrinsic connections. We then reprogram pretrained masked language models into
diffusion language models via diffusive adaptation, wherein task-specific
finetuning and instruction finetuning are explored to unlock their versatility
in solving general language tasks. Experiments show that scaling diffusion
language models consistently improves performance across downstream language
tasks. We further discover that instruction finetuning can elicit zero-shot and
few-shot in-context learning abilities that help tackle many unseen tasks by
following natural language instructions, and show promise in advanced and
challenging abilities such as reasoning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12247">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) and generative AI have played a transformative
role in computer research and applications. Controversy has arisen as to
whether these models output copyrighted data, which can occur if the data the
models are trained on is copyrighted. LLMs are built on the transformer neural
network architecture, which in turn relies on a mathematical computation called
Attention that uses the softmax function.
</p>
<p>In this paper, we show that large language model training and optimization
can be seen as a softmax regression problem. We then establish a method of
efficiently performing softmax regression, in a way that prevents the
regression function from generating copyright data. This establishes a
theoretical method of training large language models in a way that avoids
generating copyright data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt2Model: Generating Deployable Models from Natural Language Instructions. (arXiv:2308.12261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12261">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) enable system builders today to create competent
NLP systems through prompting, where they only need to describe the task in
natural language and provide a few examples. However, in other ways, LLMs are a
step backward from traditional special-purpose NLP models; they require
extensive computational resources for deployment and can be gated behind APIs.
In this paper, we propose Prompt2Model, a general-purpose method that takes a
natural language task description like the prompts provided to LLMs, and uses
it to train a special-purpose model that is conducive to deployment. This is
done through a multi-step process of retrieval of existing datasets and
pretrained models, dataset generation using LLMs, and supervised fine-tuning on
these retrieved and generated datasets. Over three tasks, we demonstrate that
given the same few-shot prompt as input, Prompt2Model trains models that
outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%
while being up to 700 times smaller. We also show that this data can be used to
obtain reliable performance estimates of model performance, enabling model
developers to assess model reliability before deployment. Prompt2Model is
available open-source at https://github.com/neulab/prompt2model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models. (arXiv:2308.12272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12272">
<div class="article-summary-box-inner">
<span><p>Foundational Language Models (FLMs) have advanced natural language processing
(NLP) research. Current researchers are developing larger FLMs (e.g., XLNet,
T5) to enable contextualized language representation, classification, and
generation. While developing larger FLMs has been of significant advantage, it
is also a liability concerning hallucination and predictive uncertainty.
Fundamentally, larger FLMs are built on the same foundations as smaller FLMs
(e.g., BERT); hence, one must recognize the potential of smaller FLMs which can
be realized through an ensemble. In the current research, we perform a reality
check on FLMs and their ensemble on benchmark and real-world datasets. We
hypothesize that the ensembling of FLMs can influence the individualistic
attention of FLMs and unravel the strength of coordination and cooperation of
different FLMs. We utilize BERT and define three other ensemble techniques:
{Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a
knowledge-guided reinforcement learning approach. We discovered that the
suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by
a factor of many times using datasets that show the usefulness of NLP in
sensitive fields, such as mental health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D4: Improving LLM Pretraining via Document De-Duplication and Diversification. (arXiv:2308.12284v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12284">
<div class="article-summary-box-inner">
<span><p>Over recent years, an increasing amount of compute and data has been poured
into training large language models (LLMs), usually by doing one-pass learning
on as many tokens as possible randomly selected from large-scale web corpora.
While training on ever-larger portions of the internet leads to consistent
performance improvements, the size of these improvements diminishes with scale,
and there has been little work exploring the effect of data selection on
pre-training and downstream performance beyond simple de-duplication methods
such as MinHash. Here, we show that careful data selection (on top of
de-duplicated data) via pre-trained model embeddings can speed up training (20%
efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up
to 2%) at the 6.7B model scale. Furthermore, we show that repeating data
intelligently consistently outperforms baseline training (while repeating
random data performs worse than baseline training). Our results indicate that
clever data selection can significantly improve LLM pre-training, calls into
question the common practice of training for a single epoch on as much data as
possible, and demonstrates a path to keep improving our models past the limits
of randomly sampling web data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dive into Deep Learning. (arXiv:2106.11342v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11342">
<div class="article-summary-box-inner">
<span><p>This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Structured Span Selector. (arXiv:2205.03977v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03977">
<div class="article-summary-box-inner">
<span><p>Many natural language processing tasks, e.g., coreference resolution and
semantic role labeling, require selecting text spans and making decisions about
them. A typical approach to such tasks is to score all possible spans and
greedily select spans for task-specific downstream processing. This approach,
however, does not incorporate any inductive bias about what sort of spans ought
to be selected, e.g., that selected spans tend to be syntactic constituents. In
this paper, we propose a novel grammar-based structured span selection model
which learns to make use of the partial span-level annotation provided for such
problems. Compared to previous approaches, our approach gets rid of the
heuristic greedy span selection scheme, allowing us to model the downstream
task on an optimal set of spans. We evaluate our model on two popular span
prediction tasks: coreference resolution and semantic role labeling. We show
empirical improvements on both.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making first order linear logic a generating grammar. (arXiv:2206.08955v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08955">
<div class="article-summary-box-inner">
<span><p>It is known that different categorial grammars have surface representation in
a fragment of first order multiplicative linear logic (MLL1). We show that the
fragment of interest is equivalent to the recently introduced extended tensor
type calculus (ETTC). ETTC is a calculus of specific typed terms, which
represent tuples of strings, more precisely bipartite graphs decorated with
strings. Types are derived from linear logic formulas, and rules correspond to
concrete operations on these string-labeled graphs, so that they can be
conveniently visualized. This provides the above mentioned fragment of MLL1
that is relevant for language modeling not only with some alternative syntax
and intuitive geometric representation, but also with an intrinsic deductive
system, which has been absent.
</p>
<p>In this work we consider a non-trivial notationally enriched variation of the
previously introduced {\bf ETTC}, which allows more concise and transparent
computations. We present both a cut-free sequent calculus and a natural
deduction formalism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment Analysis. (arXiv:2208.01368v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.01368">
<div class="article-summary-box-inner">
<span><p>The advancement of aspect-based sentiment analysis (ABSA) has urged the lack
of a user-friendly framework that can largely lower the difficulty of
reproducing state-of-the-art ABSA performance, especially for beginners. To
meet the demand, we present \our, a modularized framework built on PyTorch for
reproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA
subtasks, including aspect term extraction, aspect sentiment classification,
and end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates
29 models and 26 datasets. With just a few lines of code, the result of a model
on a specific dataset can be reproduced. With a modularized design, PyABSA can
also be flexibly extended to considered models, datasets, and other related
tasks. Besides, PyABSA highlights its data augmentation and annotation
features, which significantly address data scarcity. All are welcome to have a
try at \url{https://github.com/yangheng95/PyABSA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be Imitated?. (arXiv:2212.08986v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08986">
<div class="article-summary-box-inner">
<span><p>Authorship style transfer involves altering text to match the style of a
target author whilst preserving the original meaning. Existing unsupervised
approaches like STRAP have largely focused on style transfer to target authors
with many examples of their writing style in books, speeches, or other
published works. This high-resource training data requirement (often greater
than 100,000 words) makes these approaches primarily useful for style transfer
to published authors, politicians, or other well-known figures and authorship
styles, while style transfer to non-famous authors has not been well-studied.
We introduce the \textit{low-resource authorship style transfer} task, a more
challenging class of authorship style transfer where only a limited amount of
text in the target author's style may exist. In our experiments, we
specifically choose source and target authors from Reddit and style transfer
their Reddit posts, limiting ourselves to just 16 posts (on average ~500 words)
of the target author's style. Style transfer accuracy is typically measured by
how often a classifier or human judge will classify an output as written by the
target author. Recent authorship representations models excel at authorship
identification even with just a few writing samples, making automatic
evaluation of this task possible for the first time through evaluation metrics
we propose. Our results establish an in-context learning technique we develop
as the strongest baseline, though we find current approaches do not yet achieve
mastery of this challenging task. We release our data and implementations to
encourage further investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11004">
<div class="article-summary-box-inner">
<span><p>Interactions among humans on social media often convey intentions behind
their actions, yielding a psychological language resource for Mental Health
Analysis (MHA) of online users. The success of Computational Intelligence
Techniques (CIT) for inferring mental illness from such social media resources
points to NLP as a lens for causal analysis and perception mining. However, we
argue that more consequential and explainable research is required for optimal
impact on clinical psychology practice and personalized mental healthcare. To
bridge this gap, we posit two significant dimensions: (1) Causal analysis to
illustrate a cause and effect relationship in the user generated text; (2)
Perception mining to infer psychological perspectives of social effects on
online users intentions. Within the scope of Natural Language Processing (NLP),
we further explore critical areas of inquiry associated with these two
dimensions, specifically through recent advancements in discourse analysis.
This position paper guides the community to explore solutions in this space and
advance the state of practice in developing conversational agents for inferring
mental health from social media. We advocate for a more explainable approach
toward modeling computational psychology problems through the lens of language
as we observe an increased number of research contributions in dataset and
problem formulation for causal relation extraction and perception enhancements
while inferring mental states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models. (arXiv:2303.02206v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02206">
<div class="article-summary-box-inner">
<span><p>Answering questions over domain-specific graphs requires a tailored approach
due to the limited number of relations and the specific nature of the domain.
Our approach integrates classic logical programming languages into large
language models (LLMs), enabling the utilization of logical reasoning
capabilities to tackle the KGQA task. By representing the questions as Prolog
queries, which are readable and near close to natural language in
representation, we facilitate the generation of programmatically derived
answers. To validate the effectiveness of our approach, we evaluate it using a
well-known benchmark dataset, MetaQA. Our experimental results demonstrate that
our method achieves accurate identification of correct answer entities for all
test questions, even when trained on a small fraction of annotated data.
Overall, our work presents a promising approach to addressing question
answering over domain-specific graphs, offering an explainable and robust
solution by incorporating logical programming languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction. (arXiv:2306.14122v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14122">
<div class="article-summary-box-inner">
<span><p>Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction
(MRE) necessitate the fundamental reasoning capacity for intricate linguistic
and multimodal comprehension. In this study, we explore distilling the
reasoning ability of large language models (LLMs) into a more compact student
model by generating a \textit{chain of thought} (CoT) -- a sequence of
intermediate reasoning steps. Specifically, we commence by exemplifying the
elicitation of such reasoning ability from LLMs through CoT prompts covering
multi-grain (noun, sentence, multimodality) and data-augmentation (style,
entity, image) dimensions. Subsequently, we present a novel conditional prompt
distillation method to assimilate the commonsense reasoning ability from LLMs,
thereby enhancing the utility of the student model in addressing text-only
inputs without the requisite addition of image and CoT knowledge. Extensive
experiments reveal that our approach attains state-of-the-art accuracy and
manifests a plethora of advantages concerning interpretability, data
efficiency, and cross-domain generalization on MNER and MRE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Machine Learning Methods for Assigning Software Issues to Team Members. (arXiv:2307.00009v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00009">
<div class="article-summary-box-inner">
<span><p>Software issues contain units of work to fix, improve, or create new threads
during the development and facilitate communication among the team members.
Assigning an issue to the most relevant team member and determining a category
of an issue is a tedious and challenging task. Wrong classifications cause
delays and rework in the project and trouble among the team members. This paper
proposes a set of carefully curated linguistic features for shallow machine
learning methods and compares the performance of shallow and ensemble methods
with deep language models. Unlike the state-of-the-art, we assign issues to
four roles (designer, developer, tester, and leader) rather than to specific
individuals or teams to contribute to the generality of our solution. We also
consider the level of experience of the developers to reflect the industrial
practices in our solution formulation. We collect and annotate five industrial
data sets from one of the top three global television producers to evaluate our
proposal and compare it with deep language models. Our data sets contain 5324
issues in total. We show that an ensemble classifier of shallow techniques
achieves 0.92 for issue assignment in accuracy which is statistically
comparable to the state-of-the-art deep language models. The contributions
include the public sharing of five annotated industrial issue data sets, the
development of a clear and comprehensive feature set, the introduction of a
novel label set, and the validation of the efficacy of an ensemble classifier
of shallow machine learning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-consistency for open-ended generations. (arXiv:2307.06857v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06857">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) can exhibit considerable variation in the
quality of their sampled outputs. Reranking and selecting the best generation
from the sampled set is a popular way of obtaining strong gains in generation
quality. In this paper, we present a novel approach for reranking LLM
generations. Unlike other techniques that might involve additional inferences
or training a specialized reranker, our approach relies on easy to compute
pairwise statistics between the generations that have minimal compute overhead.
We show that our approach can be formalized as an extension of self-consistency
and analyze its performance in that framework, theoretically as well as via
simulations. We show strong improvements for selecting the best $k$ generations
for code generation tasks as well as robust improvements for best generation
for the tasks of autoformalization, and summarization. While our approach only
assumes black-box access to LLMs, we show that additional access to token
probabilities can improve performance even further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10652">
<div class="article-summary-box-inner">
<span><p>As an efficient approach to understand, generate, and process natural
language texts, research in natural language processing (NLP) has exhibited a
rapid spread and wide adoption in recent years. Given the increasing research
work in this area, several NLP-related approaches have been surveyed in the
research community. However, a comprehensive study that categorizes established
topics, identifies trends, and outlines areas for future research remains
absent. Contributing to closing this gap, we have systematically classified and
analyzed research papers in the ACL Anthology. As a result, we present a
structured overview of the research landscape, provide a taxonomy of fields of
study in NLP, analyze recent developments in NLP, summarize our findings, and
highlight directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16680">
<div class="article-summary-box-inner">
<span><p>Diffusion models and large language models have emerged as leading-edge
generative models and have sparked a revolutionary impact on various aspects of
human life. However, the practical implementation of these models has also
exposed inherent risks, highlighting their dual nature and raising concerns
regarding their trustworthiness. Despite the abundance of literature on this
subject, a comprehensive survey specifically delving into the intersection of
large-scale generative models and their trustworthiness remains largely absent.
To bridge this gap, This paper investigates both the long-standing and emerging
threats associated with these models across four fundamental dimensions:
privacy, security, fairness, and responsibility. In this way, we construct an
extensive map outlining the trustworthiness of these models, while also
providing practical recommendations and identifying future directions. These
efforts are crucial for promoting the trustworthy deployment of these models,
ultimately benefiting society as a whole.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forward-Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07758">
<div class="article-summary-box-inner">
<span><p>Chain-of-Though (CoT) prompting has shown promising performance in various
reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency}
proposes to sample a diverse set of reasoning chains which may lead to
different answers while the answer that receives the most votes is selected. In
this paper, we propose a novel method to use backward reasoning in verifying
candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM
to predict the masked token when a candidate answer is provided by \textit{a
simple template}, i.e., "\textit{\textbf{If we know the answer of the above
question is \{a candidate answer\}, what is the value of unknown variable ${\bf
x}$?}}" Intuitively, the LLM is expected to predict the masked token
successfully if the provided candidate answer is correct. We further propose
FOBAR to combine forward and backward reasoning for estimating the probability
of candidate answers. We conduct extensive experiments on six data sets and
three LLMs. Experimental results demonstrate that FOBAR achieves
state-of-the-art performance on various reasoning benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08239">
<div class="article-summary-box-inner">
<span><p>We propose MemoChat, a pipeline for refining instructions that enables large
language models (LLMs) to effectively employ self-composed memos for
maintaining consistent long-range open-domain conversations. We demonstrate a
long-range open-domain conversation through iterative
"memorization-retrieval-response" cycles. This requires us to carefully design
tailored tuning instructions for each distinct stage. The instructions are
reconstructed from a collection of public datasets to teach the LLMs to
memorize and retrieve past dialogues with structured memos, leading to enhanced
consistency when participating in future conversations. We invite experts to
manually annotate a test set designed to evaluate the consistency of long-range
conversations questions. Experiments on three testing scenarios involving both
open-source and API-accessible chatbots at scale verify the efficacy of
MemoChat, which outperforms strong baselines. Our codes, data and models are
available here: https://github.com/LuJunru/MemoChat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09662">
<div class="article-summary-box-inner">
<span><p>Larger language models (LLMs) have taken the world by storm with their
massive multi-tasking capabilities simply by optimizing over a next-word
prediction objective. With the emergence of their properties and encoded
knowledge, the risk of LLMs producing harmful outputs increases, making them
unfit for scalable deployment for the public. In this work, we propose a new
safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that
even widely deployed models are susceptible to the Chain of Utterances-based
(CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and
ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We
also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in
generating harmful responses in more than 86% of the red-teaming attempts.
Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It
constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting,
we collect a dataset that consists of 1.9K harmful questions covering a wide
range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2)
SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the
safety alignment of LLMs by minimizing the negative log-likelihood over helpful
responses and penalizing over harmful responses by gradient accent over sample
loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely
aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the
utility of the baseline models (TruthfulQA, MMLU, and BBH).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Good Are Large Language Models at Out-of-Distribution Detection?. (arXiv:2308.10261v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10261">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection plays a vital role in enhancing the
reliability of machine learning (ML) models. The emergence of large language
models (LLMs) has catalyzed a paradigm shift within the ML community,
showcasing their exceptional capabilities across diverse natural language
processing tasks. While existing research has probed OOD detection with
relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark
differences in scales, pre-training objectives, and inference paradigms call
into question the applicability of these findings to LLMs. This paper embarks
on a pioneering empirical investigation of OOD detection in the domain of LLMs,
focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate
commonly-used OOD detectors, scrutinizing their performance in both zero-grad
and fine-tuning scenarios. Notably, we alter previous discriminative
in-distribution fine-tuning into generative fine-tuning, aligning the
pre-training objective of LLMs with downstream tasks. Our findings unveil that
a simple cosine distance OOD detector demonstrates superior efficacy,
outperforming other OOD detectors. We provide an intriguing explanation for
this phenomenon by highlighting the isotropic nature of the embedding spaces of
LLMs, which distinctly contrasts with the anisotropic property observed in
smaller BERT family models. The new insight enhances our understanding of how
LLMs detect OOD data, thereby enhancing their adaptability and reliability in
dynamic environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability. (arXiv:2308.10380v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10380">
<div class="article-summary-box-inner">
<span><p>This paper outlines a natural conversational approach to solving personalized
energy-related problems using large language models (LLMs). We focus on
customizable optimization problems that necessitate repeated solving with
slight variations in modeling and are user-specific, hence posing a challenge
to devising a one-size-fits-all model. We put forward a strategy that augments
an LLM with an optimization solver, enhancing its proficiency in understanding
and responding to user specifications and preferences while providing nonlinear
reasoning capabilities. Our approach pioneers the novel concept of human-guided
optimization autoformalism, translating a natural language task specification
automatically into an optimization instance. This enables LLMs to analyze,
explain, and tackle a variety of instance-specific energy-related problems,
pushing beyond the limits of current prompt-based techniques.
</p>
<p>Our research encompasses various commonplace tasks in the energy sector, from
electric vehicle charging and Heating, Ventilation, and Air Conditioning (HVAC)
control to long-term planning problems such as cost-benefit evaluations for
installing rooftop solar photovoltaics (PVs) or heat pumps. This pilot study
marks an essential stride towards the context-based formulation of optimization
using LLMs, with the potential to democratize optimization processes. As a
result, stakeholders are empowered to optimize their energy consumption,
promoting sustainable energy practices customized to personal needs and
preferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content from Wykop.pl web service. (arXiv:2308.10592v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10592">
<div class="article-summary-box-inner">
<span><p>Advances in automated detection of offensive language online, including hate
speech and cyberbullying, require improved access to publicly available
datasets comprising social media content. In this paper, we introduce BAN-PL,
the first open dataset in the Polish language that encompasses texts flagged as
harmful and subsequently removed by professional moderators. The dataset
encompasses a total of 691,662 pieces of content from a popular social
networking service, Wykop, often referred to as the "Polish Reddit", including
both posts and comments, and is evenly distributed into two distinct classes:
"harmful" and "neutral". We provide a comprehensive description of the data
collection and preprocessing procedures, as well as highlight the linguistic
specificity of the data. The BAN-PL dataset, along with advanced preprocessing
scripts for, i.a., unmasking profanities, will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SONAR: Sentence-Level Multimodal and Language-Agnostic Representations. (arXiv:2308.11466v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11466">
<div class="article-summary-box-inner">
<span><p>We introduce SONAR, a new multilingual and multimodal fixed-size sentence
embedding space. Our single text encoder, covering 200 languages, substantially
outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim
and xsim++ multilingual similarity search tasks. Speech segments can be
embedded in the same SONAR embedding space using language-specific speech
encoders trained in a teacher-student setting on speech transcription data. Our
encoders outperform existing speech encoders on similarity search tasks. We
also provide a text decoder for 200 languages, which allows us to perform
text-to-text and speech-to-text machine translation, including for zero-shot
language and modality combinations. Our text-to-text results are competitive
compared to the state-of-the-art NLLB~1B model, despite the fixed-size
bottleneck representation. Our zero-shot speech-to-text translation results
compare favorably with strong supervised baselines such as Whisper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model as a User Simulator. (arXiv:2308.11534v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11534">
<div class="article-summary-box-inner">
<span><p>The unparalleled performance of closed-sourced ChatGPT has sparked efforts
towards its democratization, with notable strides made by leveraging real user
and ChatGPT conversations, as evidenced by Vicuna. However, while current
endeavors like Baize and UltraChat aim to auto-generate conversational data due
to challenges in gathering human participation, they primarily rely on ChatGPT
to simulate human behaviors based on directives rather than genuine human
learning. This results in a limited scope, diminished diversity, and an absence
of genuine multi-round conversational dynamics. To address the above issues, we
innovatively target human questions extracted from genuine human-machine
conversations as a learning goal and train a user simulator, UserGPT, to
produce a high-quality human-centric synthetic conversation dataset, RealChat.
Subsequently, this dataset trains our assistant model, ReaLM. Experimentally,
ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
comparison when considering equivalent training set sizes, and manual
evaluation also shows that our model is highly competitive. Impressively, when
fine-tuned with the latest LLaMA 2 model, ReaLM secured a leading score of 6.33
in the MT-Bench, outshining the contemporary same-scale models, including the
LLaMA-2-7B-chat model. Further in-depth analysis demonstrates the scalability
and transferability of our approach. A preliminary exploration into the
interplay between training set data quality and resultant model performance is
also undertaken, laying a robust groundwork for future investigations. The code
is available at https://github.com/FreedomIntelligence/ReaLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models. (arXiv:2308.11601v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11601">
<div class="article-summary-box-inner">
<span><p>The introduction of the transformer architecture and the self-attention
mechanism has led to an explosive production of language models trained on
specific downstream tasks and data domains. With over 200, 000 models in the
Hugging Face ecosystem, users grapple with selecting and optimizing models to
suit multifaceted workflows and data domains while addressing computational,
security, and recency concerns. There is an urgent need for machine learning
frameworks that can eliminate the burden of model selection and customization
and unleash the incredible power of the vast emerging model library for end
users. Here, we propose a context-aware routing system, Tryage, that leverages
a language model router for optimal selection of expert models from a model
library based on analysis of individual input prompts. Inspired by the thalamic
router in the brain, Tryage employs a perceptive router to predict down-stream
model performance on prompts and, then, makes a routing decision using an
objective function that integrates performance predictions with user goals and
constraints that are incorporated through flags (e.g., model size, model
recency). Tryage allows users to explore a Pareto front and automatically
trade-off between task accuracy and secondary goals including minimization of
model size, recency, security, verbosity, and readability. Across heterogeneous
data sets that include code, text, clinical data, and patents, the Tryage
framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection
identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by
GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how
routing models can be applied to program and control the behavior of
multi-model LLM systems to maximize efficient use of the expanding and evolving
language model ecosystem.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-24 23:10:56.298876161 UTC">2023-08-24 23:10:56 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>