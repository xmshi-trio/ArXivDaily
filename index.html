<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-24T01:30:00Z">10-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.05957">
<div class="article-summary-box-inner">
<span><p>Evaluating the readability of a text can significantly facilitate the precise
expression of information in written form. The formulation of text readability
assessment involves the identification of meaningful properties of the text
regardless of its length. Sophisticated features and models are used to
evaluate the comprehensibility of texts accurately. Despite this, the problem
of assessing texts' readability efficiently remains relatively untouched. The
efficiency of state-of-the-art text readability assessment models can be
further improved using deep reinforcement learning models. Using a hard
attention-based active inference technique, the proposed approach makes
efficient use of input text and computational resources. Through the use of
semi-supervised signals, the reinforcement learning model uses the minimum
amount of text in order to determine text's readability. A comparison of the
model on Weebit and Cambridge Exams with state-of-the-art models, such as the
BERT text readability model, shows that it is capable of achieving
state-of-the-art accuracy with a significantly smaller amount of input text
than other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Compound PCFGs. (arXiv:2103.02298v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02298">
<div class="article-summary-box-inner">
<span><p>Compound probabilistic context-free grammars (C-PCFGs) have recently
established a new state of the art for unsupervised phrase-structure grammar
induction. However, due to the high space and time complexities of chart-based
representation and inference, it is difficult to investigate C-PCFGs
comprehensively. In this work, we rely on a fast implementation of C-PCFGs to
conduct an evaluation complementary to that of~\citet{kim-etal-2019-compound}.
We start by analyzing and ablating C-PCFGs on English treebanks. Our findings
suggest that (1) C-PCFGs are data-efficient and can generalize to unseen
sentence/constituent lengths; and (2) C-PCFGs make the best use of
sentence-level information in generating preterminal rule probabilities. We
further conduct a multilingual evaluation of C-PCFGs. The experimental results
show that the best configurations of C-PCFGs, which are tuned on English, do
not always generalize to morphology-rich languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval. (arXiv:2112.05917v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05917">
<div class="article-summary-box-inner">
<span><p>Article comprehension is an important challenge in natural language
processing with many applications such as article generation or
image-to-article retrieval. Prior work typically encodes all tokens in articles
uniformly using pretrained language models. However, in many applications, such
as understanding news stories, these articles are based on real-world events
and may reference many named entities that are difficult to accurately
recognize and predict by language models. To address this challenge, we propose
an ENtity-aware article GeneratIoN and rEtrieval (ENGINE) framework, to
explicitly incorporate named entities into language models. ENGINE has two main
components: a named-entity extraction module to extract named entities from
both metadata and embedded images associated with articles, and an entity-aware
mechanism that enhances the model's ability to recognize and predict entity
names. We conducted experiments on three public datasets: GoodNews, VisualNews,
and WikiText, where our results demonstrate that our model can boost both
article generation and article retrieval performance, with a 4-5 perplexity
improvement in article generation and a 3-4% boost in recall@1 in article
retrieval. We release our implementation at
https://github.com/Zhongping-Zhang/ENGINE .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. (arXiv:2203.09313v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09313">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-training has shown remarkable performance in building
open-domain dialogue systems. However, previous works mainly focus on showing
and evaluating the conversational performance of the released dialogue model,
ignoring the discussion of some key factors towards a powerful human-like
chatbot, especially in Chinese scenarios. In this paper, we conduct extensive
experiments to investigate these under-explored factors, including data quality
control, model architecture designs, training approaches, and decoding
strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese
dialogue model with 2.8 billion parameters, and will make our models and codes
publicly available. Automatic and human evaluations show that EVA2.0
significantly outperforms other open-source counterparts. We also discuss the
limitations of this work by presenting some failure cases and pose some future
research directions on large-scale Chinese open-domain dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL. (arXiv:2205.12422v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12422">
<div class="article-summary-box-inner">
<span><p>Can non-programmers annotate natural language utterances with complex
programs that represent their meaning? We introduce APEL, a framework in which
non-programmers select among candidate programs generated by a seed semantic
parser (e.g., Codex). Since they cannot understand the candidate programs, we
ask them to select indirectly by examining the programs' input-ouput examples.
For each utterance, APEL actively searches for a simple input on which the
candidate programs tend to produce different outputs. It then asks the
non-programmers only to choose the appropriate output, thus allowing us to
infer which program is correct and could be used to fine-tune the parser. As a
first case study, we recruited human non-programmers to use APEL to re-annotate
SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation
accuracy as the original expert annotators (75%) and exposed many subtle errors
in the original annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin. (arXiv:2206.00648v2 [q-fin.ST] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00648">
<div class="article-summary-box-inner">
<span><p>Bitcoin, with its ever-growing popularity, has demonstrated extreme price
volatility since its origin. This volatility, together with its decentralised
nature, make Bitcoin highly subjective to speculative trading as compared to
more traditional assets. In this paper, we propose a multimodal model for
predicting extreme price fluctuations. This model takes as input a variety of
correlated assets, technical indicators, as well as Twitter content. In an
in-depth study, we explore whether social media discussions from the general
public on Bitcoin have predictive power for extreme price movements. A dataset
of 5,000 tweets per day containing the keyword `Bitcoin' was collected from
2015 to 2021. This dataset, called PreBit, is made available online. In our
hybrid model, we use sentence-level FinBERT embeddings, pretrained on financial
lexicons, so as to capture the full contents of the tweets and feed it to the
model in an understandable way. By combining these embeddings with a
Convolutional Neural Network, we built a predictive model for significant
market movements. The final multimodal ensemble model includes this NLP model
together with a model based on candlestick data, technical indicators and
correlated asset prices. In an ablation study, we explore the contribution of
the individual modalities. Finally, we propose and backtest a trading strategy
based on the predictions of our models with varying prediction threshold and
show that it can used to build a profitable trading strategy with a reduced
risk over a `hold' or moving average strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08012">
<div class="article-summary-box-inner">
<span><p>Human beings use compositionality to generalise from past experiences to
novel experiences. We assume a separation of our experiences into fundamental
atomic components that can be recombined in novel ways to support our ability
to engage with novel experiences. We frame this as the ability to learn to
generalise compositionally, and we will refer to behaviours making use of this
ability as compositional learning behaviours (CLBs). A central problem to
learning CLBs is the resolution of a binding problem (BP). While it is another
feat of intelligence that human beings perform with ease, it is not the case
for state-of-the-art artificial agents. Thus, in order to build artificial
agents able to collaborate with human beings, we propose to develop a novel
benchmark to investigate agents' abilities to exhibit CLBs by solving a
domain-agnostic version of the BP. We take inspiration from the language
emergence and grounding framework of referential games and propose a
meta-learning extension of referential games, entitled Meta-Referential Games,
and use this framework to build our benchmark, the Symbolic Behaviour Benchmark
(S2B). We provide baseline results and error analysis showing that our
benchmark is a compelling challenge that we hope will spur the research
community towards developing more capable artificial agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting. (arXiv:2208.08374v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08374">
<div class="article-summary-box-inner">
<span><p>Many real-world tasks involve a mixed-initiative setup, wherein humans and AI
systems collaboratively perform a task. While significant work has been
conducted towards enabling humans to specify, through language, exactly how an
agent should complete a task (i.e., low-level specification), prior work lacks
on interpreting the high-level strategic intent of the human commanders.
Parsing strategic intent from language will allow autonomous systems to
independently operate according to the user's plan without frequent guidance or
instruction. In this paper, we build a computational interface capable of
translating unstructured language strategies into actionable intent in the form
of goals and constraints. Leveraging a game environment, we collect a dataset
of over 1000 examples, mapping language strategies to the corresponding goals
and constraints, and show that our model, trained on this dataset,
significantly outperforms human interpreters in inferring strategic intent
(i.e., goals and constraints) from language (p &lt; 0.05). Furthermore, we show
that our model (125M parameters) significantly outperforms ChatGPT for this
task (p &lt; 0.05) in a low-data setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05261">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have achieved great success on sentence pair
modeling tasks, such as answer selection and natural language inference (NLI).
These models generally perform cross-attention over input pairs, leading to
prohibitive computational costs. Recent studies propose dual-encoder and late
interaction architectures for faster computation. However, the balance between
the expressive of cross-attention and computation speedup still needs better
coordinated. To this end, this paper introduces a novel paradigm MixEncoder for
efficient sentence pair modeling. MixEncoder involves a light-weight
cross-attention mechanism. It conducts query encoding only once while modeling
the query-candidate interaction in parallel. Extensive experiments conducted on
four tasks demonstrate that our MixEncoder can speed up sentence pairing by
over 113x while achieving comparable performance as the more expensive
cross-attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions. (arXiv:2210.07440v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07440">
<div class="article-summary-box-inner">
<span><p>Debiasing methods in NLP models traditionally focus on isolating information
related to a sensitive attribute (e.g., gender or race). We instead argue that
a favorable debiasing method should use sensitive information 'fairly,' with
explanations, rather than blindly eliminating it. This fair balance is often
subjective and can be challenging to achieve algorithmically. We explore two
interactive setups with a frozen predictive model and show that users able to
provide feedback can achieve a better and fairer balance between task
performance and bias mitigation. In one setup, users, by interacting with test
examples, further decreased bias in the explanations (5-8%) while maintaining
the same prediction accuracy. In the other setup, human feedback was able to
disentangle associated bias and predictive information from the input leading
to superior bias mitigation and improved task performance (4-5%)
simultaneously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12770">
<div class="article-summary-box-inner">
<span><p>Fine-tuning Large Language Models (LLMs) pre-trained from general or related
domain data to a specific domain and task using a limited amount of resources
available in the new task has been a popular practice in NLP fields. In this
work, we re-visit this assumption, and carry out investigation in clinical NLP,
specifically named-entity recognition on Drugs and their related Attributes. We
compare Transformer models that are learned from scratch to fine-tuning
BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also
investigate the comparison of such models and their extended models with a CRF
layer for continuous learning. We use n2c2-2018 shared task data for model
development and evaluations. The experimental outcomes show that 1) the CRF
layer makes a difference for all neural models; 2) on BIO-strict span level
evaluation using macro-average F1, while the fine-tuned LLMs achieved scores
0.83+, the TransformerCRF model learned from scratch achieved 0.78+
demonstrating comparable performances but using much less cost, e.g. 39.80\%
less training parameters; 3) on BIO-strict span level evaluation using
weighted-average F1, the score gaps are even smaller (97.59\%, 97.44\%,
96.84\%) for models (ClinicalBERT-CRF, BERT-CRF, TransformerCRF). 4) efficient
training using down-sampling for better data-distribution (SamBD) further
reduced the data for model learning but producing similar outcomes around 0.02
points lower than the full set model training. Our models including source
codes will be hosted at \url{https://github.com/HECTA-UoM/TransformerCRF}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Data Perspectivism and Personalization: An Application to Social Norms. (arXiv:2210.14531v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14531">
<div class="article-summary-box-inner">
<span><p>Instead of using a single ground truth for language processing tasks, several
recent studies have examined how to represent and predict the labels of the set
of annotators. However, often little or no information about annotators is
known, or the set of annotators is small. In this work, we examine a corpus of
social media posts about conflict from a set of 13k annotators and 210k
judgements of social norms. We provide a novel experimental setup that applies
personalization methods to the modeling of annotators and compare their
effectiveness for predicting the perception of social norms. We further provide
an analysis of performance across subsets of social situations that vary by the
closeness of the relationship between parties in conflict, and assess where
personalization helps the most.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15500">
<div class="article-summary-box-inner">
<span><p>As language models become increasingly integrated into our digital lives,
Personalized Text Generation (PTG) has emerged as a pivotal component with a
wide range of applications. However, the bias inherent in user written text,
often used for PTG model training, can inadvertently associate different levels
of linguistic quality with users' protected attributes. The model can inherit
the bias and perpetuate inequality in generating text w.r.t. users' protected
attributes, leading to unfair treatment when serving users. In this work, we
investigate fairness of PTG in the context of personalized explanation
generation for recommendations. We first discuss the biases in generated
explanations and their fairness implications. To promote fairness, we introduce
a general framework to achieve measure-specific counterfactual fairness in
explanation generation. Extensive experiments and human evaluations demonstrate
the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15631">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed great strides in self-supervised learning (SSL)
on the speech processing. The SSL model is normally pre-trained on a great
variety of unlabelled data and a large model size is preferred to increase the
modeling capacity. However, this might limit its potential applications due to
the expensive computation and memory costs introduced by the oversize model.
Miniaturization for SSL models has become an important research direction of
practical value. To this end, we explore the effective distillation of
HuBERT-based SSL models for automatic speech recognition (ASR). First, in order
to establish a strong baseline, a comprehensive study on different student
model structures is conducted. On top of this, as a supplement to the
regression loss widely adopted in previous works, a discriminative loss is
introduced for HuBERT to enhance the distillation performance, especially in
low-resource scenarios. In addition, we design a simple and effective algorithm
to distill the front-end input from waveform to Fbank feature, resulting in 17%
parameter reduction and doubling inference speed, at marginal performance
degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense. (arXiv:2211.05895v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05895">
<div class="article-summary-box-inner">
<span><p>Visual commonsense understanding requires Vision Language (VL) models to not
only understand image and text but also cross-reference in-between to fully
integrate and achieve comprehension of the visual scene described. Recently,
various approaches have been developed and have achieved high performance on
visual commonsense benchmarks. However, it is unclear whether the models really
understand the visual scene and underlying commonsense knowledge due to limited
evaluation data resources. To provide an in-depth analysis, we present a
Multimodal Evaluation (ME) pipeline to automatically generate question-answer
pairs to test models' understanding of the visual scene, text, and related
knowledge. We then take a step further to show that training with the ME data
boosts the model's performance in standard VCR evaluation. Lastly, our in-depth
analysis and comparison reveal interesting findings: (1) semantically low-level
information can assist the learning of high-level information but not the
opposite; (2) visual information is generally under utilization compared with
text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Contrastive Learning and Numerical Evidence for Confusing Legal Judgment Prediction. (arXiv:2211.08238v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08238">
<div class="article-summary-box-inner">
<span><p>Given the fact description text of a legal case, legal judgment prediction
(LJP) aims to predict the case's charge, law article and penalty term. A core
problem of LJP is how to distinguish confusing legal cases, where only subtle
text differences exist. Previous studies fail to distinguish different
classification errors with a standard cross-entropy classification loss, and
ignore the numbers in the fact description for predicting the term of penalty.
To tackle these issues, in this work, first, we propose a moco-based supervised
contrastive learning to learn distinguishable representations, and explore the
best strategy to construct positive example pairs to benefit all three subtasks
of LJP simultaneously. Second, in order to exploit the numbers in legal cases
for predicting the penalty terms of certain cases, we further enhance the
representation of the fact description with extracted crime amounts which are
encoded by a pre-trained numeracy model. Extensive experiments on public
benchmarks show that the proposed method achieves new state-of-the-art results,
especially on confusing legal cases. Ablation studies also demonstrate the
effectiveness of each component.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09935">
<div class="article-summary-box-inner">
<span><p>Extracting commonsense knowledge from a large language model (LLM) offers a
path to designing intelligent robots. Existing approaches that leverage LLMs
for planning are unable to recover when an action fails and often resort to
retrying failed actions, without resolving the error's underlying cause.
</p>
<p>We propose a novel approach (CAPE) that attempts to propose corrective
actions to resolve precondition errors during planning. CAPE improves the
quality of generated plans by leveraging few-shot reasoning from action
preconditions. Our approach enables embodied agents to execute more tasks than
baseline methods while ensuring semantic correctness and minimizing
re-prompting. In VirtualHome, CAPE generates executable plans while improving a
human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our
improvements transfer to a Boston Dynamics Spot robot initialized with a set of
skills (specified in language) and associated preconditions, where CAPE
improves the correctness metric of the executed task plans by 76.49% compared
to SayCan. Our approach enables the robot to follow natural language commands
and robustly recover from failures, which baseline approaches largely cannot
resolve or address inefficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VER: Unifying Verbalizing Entities and Relations. (arXiv:2211.11093v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11093">
<div class="article-summary-box-inner">
<span><p>Entities and relationships between entities are vital in the real world.
Essentially, we understand the world by understanding entities and relations.
For instance, to understand a field, e.g., computer science, we need to
understand the relevant concepts, e.g., machine learning, and the relationships
between concepts, e.g., machine learning and artificial intelligence. To
understand a person, we should first know who he/she is and how he/she is
related to others. To understand entities and relations, humans may refer to
natural language descriptions. For instance, when learning a new scientific
term, people usually start by reading its definition in dictionaries or
encyclopedias. To know the relationship between two entities, humans tend to
create a sentence to connect them. In this paper, we propose VER: a unified
model for Verbalizing Entities and Relations. Specifically, we attempt to build
a system that takes any entity or entity set as input and generates a sentence
to represent entities and relations. Extensive experiments demonstrate that our
model can generate high-quality sentences describing entities and entity
relationships and facilitate various tasks on entities and relations, including
definition modeling, relation modeling, and generative commonsense reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12588">
<div class="article-summary-box-inner">
<span><p>Recently, there has been significant progress in teaching language models to
perform step-by-step reasoning to solve complex numerical reasoning tasks.
Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these
tasks. CoT uses language models to perform both reasoning and computation in
the multi-step `thought' process. To disentangle computation from reasoning, we
propose `Program of Thoughts' (PoT), which uses language models (mainly Codex)
to express the reasoning process as a program. The computation is relegated to
an external computer, which executes the generated programs to derive the
answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,
TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)
for both few-shot and zero-shot setups. Under both few-shot and zero-shot
settings, PoT can show an average performance gain over CoT by around 12\%
across all the evaluated datasets. By combining PoT with self-consistency
decoding, we can achieve SoTA performance on all math problem datasets and
near-SoTA performance on financial datasets. All of our data and code are
released in Github https://github.com/wenhuchen/Program-of-Thoughts
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13308">
<div class="article-summary-box-inner">
<span><p>Learned representations of scientific documents can serve as valuable input
features for downstream tasks without further fine-tuning. However, existing
benchmarks for evaluating these representations fail to capture the diversity
of relevant tasks. In response, we introduce SciRepEval, the first
comprehensive benchmark for training and evaluating scientific document
representations. It includes 24 challenging and realistic tasks, 8 of which are
new, across four formats: classification, regression, ranking and search. We
then use this benchmark to study and improve the generalization ability of
scientific document representation models. We show how state-of-the-art models
like SPECTER and SciNCL struggle to generalize across the task formats, and
that simple multi-task training fails to improve them. However, a new approach
that learns multiple embeddings per document, each tailored to a different
format, can improve performance. We experiment with task-format-specific
control codes and adapters and find they outperform the existing
single-embedding state-of-the-art by over 2 points absolute. We release the
resulting family of multi-format models, called SPECTER2, for the community to
use and build on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16031">
<div class="article-summary-box-inner">
<span><p>Syntax is a latent hierarchical structure which underpins the robust and
compositional nature of human language. In this work, we explore the hypothesis
that syntactic dependencies can be represented in language model attention
distributions and propose a new method to induce these structures
theory-agnostically. Instead of modeling syntactic relations as defined by
annotation schemata, we model a more general property implicit in the
definition of dependency relations, syntactic substitutability. This property
captures the fact that words at either end of a dependency can be substituted
with words from the same category. Substitutions can be used to generate a set
of syntactically invariant sentences whose representations are then used for
parsing. We show that increasing the number of substitutions used improves
parsing accuracy on natural data. On long-distance subject-verb agreement
constructions, our method achieves 79.5% recall compared to 8.9% using a
previous method. Our method also provides improvements when transferred to a
different parsing setup, demonstrating that it generalizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection. (arXiv:2212.00482v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00482">
<div class="article-summary-box-inner">
<span><p>The task of response selection in multi-turn dialogue is to find the best
option from all candidates. In order to improve the reasoning ability of the
model, previous studies pay more attention to using explicit algorithms to
model the dependencies between utterances, which are deterministic, limited and
inflexible. In addition, few studies consider differences between the options
before and after reasoning. In this paper, we propose an Implicit Relational
Reasoning Graph Network to address these issues, which consists of the
Utterance Relational Reasoner (URR) and the Option Dual Comparator (ODC). URR
aims to implicitly extract dependencies between utterances, as well as
utterances and options, and make reasoning with relational graph convolutional
networks. ODC focuses on perceiving the difference between the options through
dual comparison, which can eliminate the interference of the noise options.
Experimental results on two multi-turn dialogue reasoning benchmark datasets
MuTual and MuTual+ show that our method significantly improves the baseline of
four pretrained language models and achieves state-of-the-art performance. The
model surpasses human performance for the first time on the MuTual dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning. (arXiv:2212.02851v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02851">
<div class="article-summary-box-inner">
<span><p>Dialogue State Tracking (DST), a key component of task-oriented conversation
systems, represents user intentions by determining the values of pre-defined
slots in an ongoing dialogue. Existing approaches use hand-crafted templates
and additional slot information to fine-tune and prompt large pre-trained
language models and elicit slot values from the dialogue context. Significant
manual effort and domain knowledge is required to design effective prompts,
limiting the generalizability of these approaches to new domains and tasks. In
this work, we propose DiSTRICT, a generalizable in-context tuning approach for
DST that retrieves highly relevant training examples for a given dialogue to
fine-tune the model without any hand-crafted templates. Experiments with the
MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches
in various zero-shot and few-shot settings using a much smaller model, thereby
providing an important advantage for real-world deployments that often have
limited resource availability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09702">
<div class="article-summary-box-inner">
<span><p>As information extraction (IE) systems have grown more adept at processing
whole documents, the classic task of template filling has seen renewed interest
as benchmark for document-level IE. In this position paper, we call into
question the suitability of template filling for this purpose. We argue that
the task demands definitive answers to thorny questions of event individuation
-- the problem of distinguishing distinct events -- about which even human
experts disagree. Through an annotation study and error analysis, we show that
this raises concerns about the usefulness of template filling metrics, the
quality of datasets for the task, and the ability of models to learn it.
Finally, we consider possible solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. (arXiv:2212.09724v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09724">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) link prediction aims to infer new facts based on
existing facts in the KG. Recent studies have shown that using the graph
neighborhood of a node via graph neural networks (GNNs) provides more useful
information compared to just using the query information. Conventional GNNs for
KG link prediction follow the standard message-passing paradigm on the entire
KG, which leads to superfluous computation, over-smoothing of node
representations, and also limits their expressive power. On a large scale, it
becomes computationally expensive to aggregate useful information from the
entire KG for inference. To address the limitations of existing KG link
prediction frameworks, we propose a novel retrieve-and-read framework, which
first retrieves a relevant subgraph context for the query and then jointly
reasons over the context and the query with a high-capacity reader. As part of
our exemplar instantiation for the new framework, we propose a novel
Transformer-based GNN as the reader, which incorporates graph-based attention
structure and cross-attention between query and context for deep fusion. This
simple yet effective design enables the model to focus on salient context
information relevant to the query. Empirical results on two standard KG link
prediction datasets demonstrate the competitive performance of the proposed
method. Furthermore, our analysis yields valuable insights for designing
improved retrievers within the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models. (arXiv:2212.09873v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09873">
<div class="article-summary-box-inner">
<span><p>There is growing interest in incorporating eye-tracking data and other
implicit measures of human language processing into natural language processing
(NLP) pipelines. The data from human language processing contain unique insight
into human linguistic understanding that could be exploited by language models.
However, many unanswered questions remain about the nature of this data and how
it can best be utilized in downstream NLP tasks. In this paper, we present
eyeStyliency, an eye-tracking dataset for human processing of stylistic text
(e.g., politeness). We develop a variety of methods to derive style saliency
scores over text using the collected eye dataset. We further investigate how
this saliency data compares to both human annotation methods and model-based
interpretability metrics. We find that while eye-tracking data is unique, it
also intersects with both human annotations and model-based importance scores,
providing a possible bridge between human- and machine-based perspectives. We
propose utilizing this type of data to evaluate the cognitive plausibility of
models that interpret style. Our eye-tracking data and processing code are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALCAP: Alignment-Augmented Music Captioner. (arXiv:2212.10901v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10901">
<div class="article-summary-box-inner">
<span><p>Music captioning has gained significant attention in the wake of the rising
prominence of streaming media platforms. Traditional approaches often
prioritize either the audio or lyrics aspect of the music, inadvertently
ignoring the intricate interplay between the two. However, a comprehensive
understanding of music necessitates the integration of both these elements. In
this study, we delve into this overlooked realm by introducing a method to
systematically learn multimodal alignment between audio and lyrics through
contrastive learning. This not only recognizes and emphasizes the synergy
between audio and lyrics but also paves the way for models to achieve deeper
cross-modal coherence, thereby producing high-quality captions. We provide both
theoretical and empirical results demonstrating the advantage of the proposed
method, which achieves new state-of-the-art on two music captioning datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis. (arXiv:2212.11680v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.11680">
<div class="article-summary-box-inner">
<span><p>Developed to alleviate prohibitive labeling costs, active learning (AL)
methods aim to reduce label complexity in supervised learning. While recent
work has demonstrated the benefit of using AL in combination with large
pre-trained language models (PLMs), it has often overlooked the practical
challenges that hinder the effectiveness of AL. We address these challenges by
leveraging representation smoothness analysis to ensure AL is feasible, that
is, both effective and practicable. Firstly, we propose an early stopping
technique that does not require a validation set -- often unavailable in
realistic AL conditions -- and observe significant improvements over random
sampling across multiple datasets and AL methods. Further, we find that task
adaptation improves AL, whereas standard short fine-tuning in AL does not
provide improvements over random sampling. Our work demonstrates the usefulness
of representation smoothness analysis for AL and introduces an AL stopping
criterion that reduces label complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Natural Language Models: Recent Advances and Future Directions. (arXiv:2301.09112v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09112">
<div class="article-summary-box-inner">
<span><p>Recent developments in deep learning have led to great success in various
natural language processing (NLP) tasks. However, these applications may
involve data that contain sensitive information. Therefore, how to achieve good
performance while also protecting the privacy of sensitive data is a crucial
challenge in NLP. To preserve privacy, Differential Privacy (DP), which can
prevent reconstruction attacks and protect against potential side knowledge, is
becoming a de facto technique for private data analysis. In recent years, NLP
in DP models (DP-NLP) has been studied from different perspectives, which
deserves a comprehensive review. In this paper, we provide the first systematic
review of recent advances in DP deep learning models in NLP. In particular, we
first discuss some differences and additional challenges of DP-NLP compared
with the standard DP deep learning. Then, we investigate some existing work on
DP-NLP and present its recent developments from three aspects: gradient
perturbation based methods, embedding vector perturbation based methods, and
ensemble model based methods. We also discuss some challenges and future
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12609">
<div class="article-summary-box-inner">
<span><p>Originally proposed as a method for knowledge transfer from one model to
another, some recent studies have suggested that knowledge distillation (KD) is
in fact a form of regularization. Perhaps the strongest support of all for this
new perspective comes from its apparent similarities with label smoothing (LS).
Here we re-examine this stated equivalence between the two methods by comparing
the predictive confidences of the models they train. Experiments on four text
classification tasks involving models of different sizes show that: (a) In most
settings, KD and LS drive model confidence in completely opposite directions,
and (b) In KD, the student inherits not only its knowledge but also its
confidence from the teacher, reinforcing the classical knowledge transfer view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using In-Context Learning to Improve Dialogue Safety. (arXiv:2302.00871v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00871">
<div class="article-summary-box-inner">
<span><p>While large neural-based conversational models have become increasingly
proficient dialogue agents, recent work has highlighted safety issues with
these systems. For example, these systems can be goaded into generating toxic
content, which often perpetuates social biases or stereotypes. We investigate a
retrieval-based method for reducing bias and toxicity in responses from
chatbots. It uses in-context learning to steer a model towards safer
generations. Concretely, to generate a response to an unsafe dialogue context,
we retrieve demonstrations of safe responses to similar dialogue contexts. We
find our method performs competitively with strong baselines without requiring
training. For instance, using automatic evaluation, we find our best fine-tuned
baseline only generates safe responses to unsafe dialogue contexts from
DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking
procedure which can further improve response safeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03693">
<div class="article-summary-box-inner">
<span><p>This paper concerns the structure of learned representations in text-guided
generative models, focusing on score-based models. A key property of such
models is that they can compose disparate concepts in a `disentangled' manner.
This suggests these models have internal representations that encode concepts
in a `disentangled' manner. Here, we focus on the idea that concepts are
encoded as subspaces of some representation space. We formalize what this
means, show there's a natural choice for the representation, and develop a
simple method for identifying the part of the representation corresponding to a
given concept. In particular, this allows us to manipulate the concepts
expressed by the model through algebraic manipulation of the representation. We
demonstrate the idea with examples using Stable Diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. (arXiv:2302.04012v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04012">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) for automatic code generation have achieved
breakthroughs in several programming tasks. Their advances in competition-level
programming problems have made them an essential pillar of AI-assisted pair
programming, and tools such as GitHub Copilot have emerged as part of the daily
programming workflow used by millions of developers. The training data for
these models is usually collected from the Internet (e.g., from open-source
repositories) and is likely to contain faults and security vulnerabilities.
This unsanitized training data can cause the language models to learn these
vulnerabilities and propagate them during the code generation procedure. While
these models have been extensively assessed for their ability to produce
functionally correct programs, there remains a lack of comprehensive
investigations and benchmarks addressing the security aspects of these models.
</p>
<p>In this work, we propose a method to systematically study the security issues
of code language models to assess their susceptibility to generating vulnerable
code. To this end, we introduce the first approach to automatically find
generated code that contains vulnerabilities in black-box code generation
models. To achieve this, we present an approach to approximate inversion of the
black-box code generation models based on few-shot prompting. We evaluate the
effectiveness of our approach by examining code language models in generating
high-risk security weaknesses. Furthermore, we establish a collection of
diverse non-secure prompts for various vulnerability scenarios using our
method. This dataset forms a benchmark for evaluating and comparing the
security weaknesses in code language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04858">
<div class="article-summary-box-inner">
<span><p>Augmenting pretrained language models (LMs) with a vision encoder (e.g.,
Flamingo) has obtained the state-of-the-art results in image-to-text
generation. However, these models store all the knowledge within their
parameters, thus often requiring enormous model parameters to model the
abundant visual concepts and very rich textual descriptions. Additionally, they
are inefficient in incorporating new data, requiring a computational-expensive
fine-tuning process. In this work, we introduce a Retrieval-augmented Visual
Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the
relevant knowledge from the external database for zero and in-context few-shot
image-to-text generations. By storing certain knowledge explicitly in the
external database, our approach reduces the number of model parameters and can
easily accommodate new data during evaluation by simply updating the database.
We also construct an interleaved image and text data that facilitates
in-context few-shot learning capabilities. We demonstrate that Re-ViLM
significantly boosts performance for image-to-text generation tasks, especially
for zero-shot and few-shot generation in out-of-domain settings with 4 times
less parameters compared with baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Agile Text Classifiers for Everyone. (arXiv:2302.06541v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06541">
<div class="article-summary-box-inner">
<span><p>Text-based safety classifiers are widely used for content moderation and
increasingly to tune generative language model behavior - a topic of growing
concern for the safety of digital assistants and chatbots. However, different
policies require different classifiers, and safety policies themselves improve
from iteration and adaptation. This paper introduces and evaluates methods for
agile text classification, whereby classifiers are trained using small,
targeted datasets that can be quickly developed for a particular policy.
Experimenting with 7 datasets from three safety-related domains, comprising 15
annotation schemes, led to our key finding: prompt-tuning large language
models, like PaLM 62B, with a labeled dataset of as few as 80 examples can
achieve state-of-the-art performance. We argue that this enables a paradigm
shift for text classification, especially for models supporting safer online
discourse. Instead of collecting millions of examples to attempt to create
universal safety classifiers over months or years, classifiers could be tuned
using small datasets, created by individuals or small organizations, tailored
for specific use cases, and iterated on and adapted in the time-span of a day.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v5 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07268">
<div class="article-summary-box-inner">
<span><p>A rapidly increasing amount of human conversation occurs online. But
divisiveness and conflict can fester in text-based interactions on social media
platforms, in messaging apps, and on other digital forums. Such toxicity
increases polarization and, importantly, corrodes the capacity of diverse
societies to develop efficient solutions to complex social problems that impact
everyone. Scholars and civil society groups promote interventions that can make
interpersonal conversations less divisive or more productive in offline
settings, but scaling these efforts to the amount of discourse that occurs
online is extremely challenging. We present results of a large-scale experiment
that demonstrates how online conversations about divisive topics can be
improved with artificial intelligence tools. Specifically, we employ a large
language model to make real-time, evidence-based recommendations intended to
improve participants' perception of feeling understood in conversations. We
find that these interventions improve the reported quality of the conversation,
reduce political divisiveness, and improve the tone, without systematically
changing the content of the conversation or moving people's policy attitudes.
These findings have important implications for future research on social media,
political deliberation, and the growing community of scholars interested in the
place of artificial intelligence within computational social science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WiCE: Real-World Entailment for Claims in Wikipedia. (arXiv:2303.01432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.01432">
<div class="article-summary-box-inner">
<span><p>Textual entailment models are increasingly applied in settings like
fact-checking, presupposition verification in question answering, or summary
evaluation. However, these represent a significant domain shift from existing
entailment datasets, and models underperform as a result. We propose WiCE, a
new fine-grained textual entailment dataset built on natural claim and evidence
pairs extracted from Wikipedia. In addition to standard claim-level entailment,
WiCE provides entailment judgments over sub-sentence units of the claim, and a
minimal subset of evidence sentences that support each subclaim. To support
this, we propose an automatic claim decomposition strategy using GPT-3.5 which
we show is also effective at improving entailment models' performance on
multiple datasets at test time. Finally, we show that real claims in our
dataset involve challenging verification and retrieval problems that existing
models fail to address.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!. (arXiv:2303.08559v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08559">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made remarkable strides in various tasks.
Whether LLMs are competitive few-shot solvers for information extraction (IE)
tasks, however, remains an open problem. In this work, we aim to provide a
thorough answer to this question. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate that current advanced LLMs
consistently exhibit inferior performance, higher latency, and increased budget
requirements compared to fine-tuned SLMs under most settings. Therefore, we
conclude that LLMs are not effective few-shot information extractors in
general. Nonetheless, we illustrate that with appropriate prompting strategies,
LLMs can effectively complement SLMs and tackle challenging samples that SLMs
struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm
to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as
filters and LLMs serve as rerankers. By prompting LLMs to rerank a small
portion of difficult samples identified by SLMs, our preliminary system
consistently achieves promising improvements (2.4% F1-gain on average) on
various IE tasks, with an acceptable time and cost investment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-faithful Prompting for Large Language Models. (arXiv:2303.11315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11315">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) encode parametric knowledge about world facts
and have shown remarkable performance in knowledge-driven NLP tasks. However,
their reliance on parametric knowledge may cause them to overlook contextual
cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,
knowledge acquisition tasks). In this paper, we seek to assess and enhance
LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction
with abstention. We demonstrate that LLMs' faithfulness can be significantly
improved using carefully designed prompting strategies. In particular, we
identify opinion-based prompts and counterfactual demonstrations as the most
effective methods. Opinion-based prompts reframe the context as a narrator's
statement and inquire about the narrator's opinions, while counterfactual
demonstrations use instances containing false facts to improve faithfulness in
knowledge conflict situations. Neither technique requires additional training.
We conduct experiments on three datasets of two standard NLP tasks, machine
reading comprehension and relation extraction, and the results demonstrate
significant improvement in faithfulness to contexts. Code and data are released
at https://github.com/wzhouad/context-faithful-llm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12314">
<div class="article-summary-box-inner">
<span><p>Prompt tuning is a parameter-efficient method, which learns soft prompts and
conditions frozen language models to perform specific downstream tasks. Though
effective, prompt tuning under few-shot settings on the one hand heavily relies
on a good initialization of soft prompts. On the other hand, it can easily
overfit to few-shot training samples, thereby undermining generalizability.
Existing works leverage pre-training or supervised meta-learning to initialize
soft prompts but they fail to data-efficiently generalize to unseen downstream
tasks. To address the above problems, this paper proposes a novel
Self-sUpervised meta-Prompt learning framework with MEta-gradient
Regularization for few-shot generalization (SUPMER). SUPMER leverages
self-supervised meta-learning with a diverse set of well-designed meta-training
tasks to learn a universal prompt initialization for efficient adaptation using
only unlabeled data. Additionally, it jointly meta-learns a gradient
regularization function to transform raw gradients into a domain-generalizable
direction, thus alleviating the problem of overfitting. Extensive experiments
show that SUPMER achieves better performance for different few-shot downstream
tasks, and also exhibits a stronger domain generalization ability. The code for
SUPMER will be available at https://github.com/beepkh/SUPMER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12528">
<div class="article-summary-box-inner">
<span><p>Generative AI models have shown impressive performance on many Natural
Language Processing tasks such as language understanding, reasoning, and
language generation. An important question being asked by the AI community
today is about the capabilities and limits of these models, and it is clear
that evaluating generative AI is very challenging. Most studies on generative
LLMs have been restricted to English and it is unclear how capable these models
are at understanding and generating text in other languages. We present the
first comprehensive benchmarking of generative LLMs - MEGA, which evaluates
models on standard NLP benchmarks, covering 16 NLP datasets across 70
typologically diverse languages. We compare the performance of generative LLMs
including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive
models on these tasks to determine how well generative models perform compared
to the previous generation of LLMs. We present a thorough analysis of the
performance of models across languages and tasks and discuss challenges in
improving the performance of generative LLMs on low-resource languages. We
create a framework for evaluating generative LLMs in the multilingual setting
and provide directions for future progress in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08315">
<div class="article-summary-box-inner">
<span><p>Dual use, the intentional, harmful reuse of technology and scientific
artefacts, is a problem yet to be well-defined within the context of Natural
Language Processing (NLP). However, as NLP technologies continue to advance and
become increasingly widespread in society, their inner workings have become
increasingly opaque. Therefore, understanding dual use concerns and potential
ways of limiting them is critical to minimising the potential harms of research
and development. In this paper, we conduct a survey of NLP researchers and
practitioners to understand the depth and their perspective of the problem as
well as to assess existing available support. Based on the results of our
survey, we offer a definition of dual use that is tailored to the needs of the
NLP community. The survey revealed that a majority of researchers are concerned
about the potential dual use of their research but only take limited action
toward it. In light of the survey results, we discuss the current state and
potential means for mitigating dual use in NLP and propose a checklist that can
be integrated into existing conference ethics-frameworks, e.g., the ACL ethics
checklist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09145">
<div class="article-summary-box-inner">
<span><p>Post-training quantization~(PTQ) of transformer language models faces
significant challenges due to the existence of detrimental outliers in
activations. We observe that these outliers are concentrated in specific
channels and are asymmetric across channels. To address this issue, we propose
the Outlier Suppression+~(OS+) framework, which contains the channel-wise
shifting for asymmetry and channel-wise scaling for concentration. We show that
these operations can be seamlessly migrated into subsequent modules while
maintaining equivalence. Second, we propose a fast and stable scheme to
calculate effective shifting and scaling values. The channel-wise shifting
aligns the center of each channel for removal of outlier asymmetry. The
channel-wise scaling quantitatively evaluates changes brought by migration and
quantization for better quantization burden balance. We validate our OS+ under
both standard and fine-grained quantization settings with models including
BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks
demonstrate the superiority of our approach. Especially, with standard
quantization, OS+ can achieve near-floating-point performance on both small
models and large language models on 8-bit and 6-bit. Besides, we establish a
new state-of-the-art for 4-bit BERT with 15.5\% improvement. Our code is
available at \url{https://github.com/ModelTC/Outlier_Suppression_Plus}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11389">
<div class="article-summary-box-inner">
<span><p>Recent psycholinguistic studies have drawn conflicting conclusions about the
relationship between the quality of a language model and the ability of its
surprisal estimates to predict human reading times, which has been speculated
to be due to the large gap in both the amount of training data and model
capacity across studies. The current work aims to consolidate these findings by
evaluating surprisal estimates from Transformer-based language model variants
that vary systematically in the amount of training data and model capacity on
their ability to predict human reading times. The results show that surprisal
estimates from most variants with contemporary model capacities provide the
best fit after seeing about two billion training tokens, after which they begin
to diverge from humanlike expectations. Additionally, newly-trained smaller
model variants reveal a 'tipping point' at convergence, after which the
decrease in language model perplexity begins to result in poorer fits to human
reading times. These results suggest that the massive amount of training data
is mainly responsible for the poorer fit achieved by surprisal from larger
pre-trained language models, and that a certain degree of model capacity is
necessary for Transformer-based language models to capture humanlike
expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. (arXiv:2305.00118v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00118">
<div class="article-summary-box-inner">
<span><p>In this work, we carry out a data archaeology to infer books that are known
to ChatGPT and GPT-4 using a name cloze membership inference query. We find
that OpenAI models have memorized a wide collection of copyrighted materials,
and that the degree of memorization is tied to the frequency with which
passages of those books appear on the web. The ability of these models to
memorize an unknown set of books complicates assessments of measurement
validity for cultural analytics by contaminating test data; we show that models
perform much better on memorized books than on non-memorized books for
downstream tasks. We argue that this supports a case for open models whose
training data is known.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01219">
<div class="article-summary-box-inner">
<span><p>The prompt-based learning paradigm, which bridges the gap between
pre-training and fine-tuning, achieves state-of-the-art performance on several
NLP tasks, particularly in few-shot settings. Despite being widely applied,
prompt-based learning is vulnerable to backdoor attacks. Textual backdoor
attacks are designed to introduce targeted vulnerabilities into models by
poisoning a subset of training samples through trigger injection and label
modification. However, they suffer from flaws such as abnormal natural language
expressions resulting from the trigger and incorrect labeling of poisoned
samples. In this study, we propose ProAttack, a novel and efficient method for
performing clean-label backdoor attacks based on the prompt, which uses the
prompt itself as a trigger. Our method does not require external triggers and
ensures correct labeling of poisoned samples, improving the stealthy nature of
the backdoor attack. With extensive experiments on rich-resource and few-shot
text classification tasks, we empirically validate ProAttack's competitive
performance in textual backdoor attacks. Notably, in the rich-resource setting,
ProAttack achieves state-of-the-art attack success rates in the clean-label
backdoor attack benchmark without external triggers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation. (arXiv:2305.01498v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01498">
<div class="article-summary-box-inner">
<span><p>We present PeerSum, a novel dataset for generating meta-reviews of scientific
papers. The meta-reviews can be interpreted as abstractive summaries of
reviews, multi-turn discussions and the paper abstract. These source documents
have rich inter-document relationships with an explicit hierarchical
conversational structure, cross-references and (occasionally) conflicting
information. To introduce the structural inductive bias into pre-trained
language models, we introduce Rammer ( Relationship-aware Multi-task
Meta-review Generator), a model that uses sparse attention based on the
conversational structure and a multi-task training objective that predicts
metadata features (e.g., review ratings). Our experimental results show that
Rammer outperforms other strong baseline models in terms of a suite of
automatic evaluation metrics. Further analyses, however, reveal that RAMMER and
other models struggle to handle conflicts in source documents of PeerSum,
suggesting meta-review generation is a challenging task and a promising avenue
for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02105">
<div class="article-summary-box-inner">
<span><p>In spite of the potential for ground-breaking achievements offered by large
language models (LLMs) (e.g., GPT-3), they still lag significantly behind
fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).
This is due to the two major shortcomings of LLMs in RE: (1) low relevance
regarding entity and relation in retrieved demonstrations for in-context
learning; and (2) the strong inclination to wrongly classify NULL examples into
other pre-defined labels.
</p>
<p>In this paper, we propose GPT-RE to bridge the gap between LLMs and
fully-supervised baselines. GPT-RE successfully addresses the aforementioned
issues by (1) incorporating task-specific entity representations in
demonstration retrieval; and (2) enriching the demonstrations with gold
label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE
datasets, and observe that GPT-RE achieves improvements over not only existing
GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE
achieves SOTA performances on the Semeval and SciERC datasets, and competitive
performances on the TACRED and ACE05 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity. (arXiv:2305.02176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02176">
<div class="article-summary-box-inner">
<span><p>Mixture-of-experts (MoE) models that employ sparse activation have
demonstrated effectiveness in significantly increasing the number of parameters
while maintaining low computational requirements per token. However, recent
studies have established that MoE models are inherently parameter-inefficient
as the improvement in performance diminishes with an increasing number of
experts. We hypothesize this parameter inefficiency is a result of all experts
having equal capacity, which may not adequately meet the varying complexity
requirements of different tokens or tasks. In light of this, we propose
Stratified Mixture of Experts (SMoE) models, which feature a stratified
structure and can assign dynamic capacity to different tokens. We demonstrate
the effectiveness of SMoE on three multilingual machine translation benchmarks,
containing 4, 15, and 94 language pairs, respectively. We show that SMoE
outperforms multiple state-of-the-art MoE models with the same or fewer
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02239">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have improved zero-shot text classification by
allowing the transfer of semantic knowledge from the training data in order to
classify among specific label sets in downstream tasks. We propose a simple way
to further improve zero-shot accuracies with minimal effort. We curate small
finetuning datasets intended to describe the labels for a task. Unlike typical
finetuning data, which has texts annotated with labels, our data simply
describes the labels in language, e.g., using a few related terms,
dictionary/encyclopedia entries, and short templates. Across a range of topic
and sentiment datasets, our method is more accurate than zero-shot by 17-19%
absolute. It is also more robust to choices required for zero-shot
classification, such as patterns for prompting the model to classify and
mappings from labels to tokens in the model's vocabulary. Furthermore, since
our data merely describes the labels but does not use input texts, finetuning
on it yields a model that performs strongly on multiple text domains for a
given label set, even improving over few-shot out-of-domain classification in
multiple settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re$^3$Dial: Retrieve, Reorganize and Rescale Dialogue Corpus for Long-Turn Open-Domain Dialogue Pre-training. (arXiv:2305.02606v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02606">
<div class="article-summary-box-inner">
<span><p>Pre-training on large-scale open-domain dialogue data can substantially
improve the performance of dialogue models. However, the pre-trained dialogue
model's ability to utilize long-range context is limited due to the scarcity of
long-turn dialogue sessions. Most dialogues in existing pre-training corpora
contain fewer than three turns of dialogue. To alleviate this issue, we propose
the Retrieve, Reorganize and Rescale framework (Re$^3$Dial), which can
automatically construct billion-scale long-turn dialogues by reorganizing
existing short-turn ones. Given a short-turn session, Re$^3$Dial first employs
a session retriever to retrieve coherent consecutive sessions. To this end, we
train the retriever to capture semantic and discourse relations within
multi-turn dialogues through contrastive training. Next, Re$^3$Dial samples a
session from retrieved results following a diversity sampling strategy, which
is designed to penalize repetitive or generic sessions. A longer session is
then derived by concatenating the original session and the sampled session. By
repeating the above process, Re$^3$Dial can yield a coherent long-turn
dialogue. Extensive experiments on multiple multi-turn dialogue benchmarks
demonstrate that Re$^3$Dial significantly improves the dialogue model's ability
to utilize long-range context and thus generate more sensible and informative
responses. Finally, we build a toolkit for efficiently rescaling conversations
with Re$^3$Dial, which enables us to construct a corpus containing 1B Chinese
dialogue sessions with 11.3 turns on average (5$\times$ longer than the
original corpus). Our retriever model, code, and data is publicly available at
\url{https://github.com/thu-coai/Re3Dial}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition. (arXiv:2305.02996v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02996">
<div class="article-summary-box-inner">
<span><p>Cross-encoder models, which jointly encode and score a query-item pair, are
prohibitively expensive for direct k-nearest neighbor (k-NN) search.
Consequently, k-NN search typically employs a fast approximate retrieval (e.g.
using BM25 or dual-encoder vectors), followed by reranking with a
cross-encoder; however, the retrieval approximation often has detrimental
recall regret. This problem is tackled by ANNCUR (Yadav et al., 2022), a recent
work that employs a cross-encoder only, making search efficient using a
relatively small number of anchor items, and a CUR matrix factorization. While
ANNCUR's one-time selection of anchors tends to approximate the cross-encoder
distances on average, doing so forfeits the capacity to accurately estimate
distances to items near the query, leading to regret in the crucial end-task:
recall of top-k items. In this paper, we propose ADACUR, a method that
adaptively, iteratively, and efficiently minimizes the approximation error for
the practically important top-k neighbors. It does so by iteratively performing
k-NN search using the anchors available so far, then adding these retrieved
nearest neighbors to the anchor set for the next round. Empirically, on
multiple datasets, in comparison to previous traditional and state-of-the-art
methods such as ANNCUR and dual-encoder-based retrieve-and-rerank, our proposed
approach ADACUR consistently reduces recall error-by up to 70% on the important
k = 1 setting-while using no more compute than its competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expository Text Generation: Imitate, Retrieve, Paraphrase. (arXiv:2305.03276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03276">
<div class="article-summary-box-inner">
<span><p>Expository documents are vital resources for conveying complex information to
readers. Despite their usefulness, writing expository text by hand is a
challenging process that requires careful content planning, obtaining facts
from multiple sources, and the ability to clearly synthesize these facts. To
ease these burdens, we propose the task of expository text generation, which
seeks to automatically generate an accurate and stylistically consistent
expository text for a topic by intelligently searching a knowledge source. We
solve our task by developing IRP, a framework that overcomes the limitations of
retrieval-augmented models and iteratively performs content planning, fact
retrieval, and rephrasing. Through experiments on three diverse,
newly-collected datasets, we show that IRP produces factual and organized
expository texts that accurately inform readers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05471">
<div class="article-summary-box-inner">
<span><p>With the recent advances in natural language processing (NLP), a vast number
of applications have emerged across various use cases. Among the plethora of
NLP applications, many academic researchers are motivated to do work that has a
positive social impact, in line with the recent initiatives of NLP for Social
Good (NLP4SG). However, it is not always obvious to researchers how their
research efforts are tackling today's big social problems. Thus, in this paper,
we introduce NLP4SG Papers, a scientific dataset with three associated tasks
that can help identify NLP4SG papers and characterize the NLP4SG landscape by:
(1) identifying the papers that address a social problem, (2) mapping them to
the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying
the task they are solving and the methods they are using. Using
state-of-the-art NLP models, we address each of these tasks and use them on the
entire ACL Anthology, resulting in a visualization workspace that gives
researchers a comprehensive overview of the field of NLP4SG. Our website is
available at https://nlp4sg.vercel.app. We released our data at
https://huggingface.co/datasets/feradauto/NLP4SGPapers and code at
https://github.com/feradauto/nlp4sg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Retrieval Augmented Generation. (arXiv:2305.06983v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06983">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable ability of large language models (LMs) to comprehend
and generate language, they have a tendency to hallucinate and create factually
inaccurate output. Augmenting LMs by retrieving information from external
knowledge resources is one promising solution. Most existing retrieval
augmented LMs employ a retrieve-and-generate setup that only retrieves
information once based on the input. This is limiting, however, in more general
scenarios involving generation of long texts, where continually gathering
information throughout generation is essential. In this work, we provide a
generalized view of active retrieval augmented generation, methods that
actively decide when and what to retrieve across the course of the generation.
We propose Forward-Looking Active REtrieval augmented generation (FLARE), a
generic method which iteratively uses a prediction of the upcoming sentence to
anticipate future content, which is then utilized as a query to retrieve
relevant documents to regenerate the sentence if it contains low-confidence
tokens. We test FLARE along with baselines comprehensively over 4 long-form
knowledge-intensive generation tasks/datasets. FLARE achieves superior or
competitive performance on all tasks, demonstrating the effectiveness of our
method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. (arXiv:2305.07004v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07004">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) demonstrate impressive multilingual capability,
but their performance varies substantially across different languages. In this
work, we introduce a simple yet effective method, called cross-lingual-thought
prompting (XLT), to systematically improve the multilingual capability of LLMs.
Specifically, XLT is a generic template prompt that stimulates cross-lingual
and logical reasoning skills to enhance task performance across languages. We
conduct comprehensive evaluations on 7 typical benchmarks related to reasoning,
understanding, and generation tasks, covering both high-resource and
low-resource languages. Experimental results show that XLT not only remarkably
enhances the performance of various multilingual tasks but also significantly
reduces the gap between the average performance and the best performance of
each task in different languages. Notably, XLT brings over 10 points of average
improvement in arithmetic reasoning and open-domain question-answering tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZARA: Improving Few-Shot Self-Rationalization for Small Language Models. (arXiv:2305.07355v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07355">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) that jointly generate end-task answers as well as
free-text rationales are known as self-rationalization models. Recent works
demonstrate great performance gain for self-rationalization by few-shot
prompting LMs with rationale-augmented exemplars. However, the ability to
benefit from explanations only emerges with large-scale LMs, which have poor
accessibility. In this work, we explore the less-studied setting of leveraging
explanations for small LMs to improve few-shot self-rationalization. We first
revisit the relationship between rationales and answers. Inspired by the
implicit mental process of how human beings assess explanations, we present a
novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to
automatically construct pseudo-parallel data for self-training by reducing the
problem of plausibility judgement to natural language inference. Experimental
results show ZARA achieves SOTA performance on the FEB benchmark, for both the
task accuracy and the explanation metric. In addition, we conduct human and
quantitative evaluation validating ZARA's ability to automatically identify
plausible and accurate rationale-answer pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruct Before Summarize: An Efficient Two-Step Framework for Condensing and Summarizing Meeting Transcripts. (arXiv:2305.07988v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07988">
<div class="article-summary-box-inner">
<span><p>Meetings typically involve multiple participants and lengthy conversations,
resulting in redundant and trivial content. To overcome these challenges, we
propose a two-step framework, Reconstruct before Summarize (RbS), for effective
and efficient meeting summarization. RbS first leverages a self-supervised
paradigm to annotate essential contents by reconstructing the meeting
transcripts. Secondly, we propose a relative positional bucketing (RPB)
algorithm to equip (conventional) summarization models to generate the summary.
Despite the additional reconstruction process, our proposed RPB significantly
compressed the input, leading to faster processing and reduced memory
consumption compared to traditional summarization methods. We validate the
effectiveness and efficiency of our method through extensive evaluations and
analysis. On two meeting summarization datasets, AMI and ICSI, our approach
outperforms previous state-of-the-art approaches without relying on large-scale
pre-training or expert-grade annotating tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructGPT: A General Framework for Large Language Model to Reason over Structured Data. (arXiv:2305.09645v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09645">
<div class="article-summary-box-inner">
<span><p>In this paper, we study how to improve the zero-shot reasoning ability of
large language models~(LLMs) over structured data in a unified way. Inspired by
the study on tool augmentation for LLMs, we develop an \emph{Iterative
Reading-then-Reasoning~(IRR)} approach for solving question answering tasks
based on structured data, called \textbf{StructGPT}. In our approach, we
construct the specialized function to collect relevant evidence from structured
data (\ie \emph{reading}), and let LLMs concentrate the reasoning task based on
the collected information (\ie \emph{reasoning}). Specially, we propose an
\emph{invoking-linearization-generation} procedure to support LLMs in reasoning
on the structured data with the help of the external interfaces. By iterating
this procedures with provided interfaces, our approach can gradually approach
the target answer to a given query. Extensive experiments conducted on three
types of structured data demonstrate the effectiveness of our approach, which
can significantly boost the performance of ChatGPT and achieve comparable
performance against the full-data supervised-tuning baselines. Our codes and
data are publicly available at~\url{https://github.com/RUCAIBox/StructGPT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mirages: On Anthropomorphism in Dialogue Systems. (arXiv:2305.09800v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09800">
<div class="article-summary-box-inner">
<span><p>Automated dialogue or conversational systems are anthropomorphised by
developers and personified by users. While a degree of anthropomorphism may be
inevitable due to the choice of medium, conscious and unconscious design
choices can guide users to personify such systems to varying degrees.
Encouraging users to relate to automated systems as if they were human can lead
to high risk scenarios caused by over-reliance on their outputs. As a result,
natural language processing researchers have investigated the factors that
induce personification and develop resources to mitigate such effects. However,
these efforts are fragmented, and many aspects of anthropomorphism have yet to
be explored. In this paper, we discuss the linguistic factors that contribute
to the anthropomorphism of dialogue systems and the harms that can arise,
including reinforcing gender stereotypes and notions of acceptable language. We
recommend that future efforts towards developing dialogue systems take
particular care in their design, development, release, and description; and
attend to the many linguistic cues that can elicit personification by users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10163">
<div class="article-summary-box-inner">
<span><p>Generative Pre-Training (GPT) models like ChatGPT have demonstrated
exceptional performance in various Natural Language Processing (NLP) tasks.
Although ChatGPT has been integrated into the overall workflow to boost
efficiency in many domains, the lack of flexibility in the finetuning process
hinders its applications in areas that demand extensive domain expertise and
semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on
the China National Medical Licensing Examination (CNMLE) and propose a novel
approach to improve ChatGPT from two perspectives: integrating medical domain
knowledge and enabling few-shot learning. By using a simple but effective
retrieval method, medical background knowledge is extracted as semantic
instructions to guide the inference of ChatGPT. Similarly, relevant medical
questions are identified and fed as demonstrations to ChatGPT. Experimental
results show that directly applying ChatGPT fails to qualify the CNMLE at a
score of 51 (i.e., only 51\% of questions are answered correctly). While our
knowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which not
only passes the qualification but also surpasses the average score of humans
(61). This research demonstrates the potential of knowledge-enhanced ChatGPT to
serve as versatile medical assistants, capable of analyzing real-world medical
problems in a more accessible, user-friendly, and adaptable manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10434">
<div class="article-summary-box-inner">
<span><p>Visual text evokes an image in a person's mind, while non-visual text fails
to do so. A method to automatically detect visualness in text will enable
text-to-image retrieval and generation models to augment text with relevant
images. This is particularly challenging with long-form text as text-to-image
generation and retrieval models are often triggered for text that is designed
to be explicitly visual in nature, whereas long-form text could contain many
non-visual sentences. To this end, we curate a dataset of 3,620 English
sentences and their visualness scores provided by multiple human annotators. We
also propose a fine-tuning strategy that adapts large vision-language models
like CLIP by modifying the model's contrastive learning objective to map text
identified as non-visual to a common NULL image while matching visual text to
their corresponding images in the document. We evaluate the proposed approach
on its ability to (i) classify visual and non-visual text accurately, and (ii)
attend over words that are identified as visual in psycholinguistic studies.
Empirical evaluation indicates that our approach performs better than several
heuristics and baseline models for the proposed task. Furthermore, to highlight
the importance of modeling the visualness of text, we conduct qualitative
analyses of text-to-image generation systems like DALL-E. Project webpage:
https://gaurav22verma.github.io/text-visualness/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paxion: Patching Action Knowledge in Video-Language Foundation Models. (arXiv:2305.10683v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10683">
<div class="article-summary-box-inner">
<span><p>Action knowledge involves the understanding of textual, visual, and temporal
aspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)
containing two carefully designed probing tasks: Action Antonym and Video
Reversal, which targets multimodal alignment capabilities and temporal
understanding skills of the model, respectively. Despite recent video-language
models' (VidLM) impressive performance on various benchmark tasks, our
diagnostic tasks reveal their surprising deficiency (near-random performance)
in action knowledge, suggesting that current models rely on object recognition
abilities as a shortcut for action understanding. To remedy this, we propose a
novel framework, Paxion, along with a new Discriminative Video Dynamics
Modeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher
network to encode new action knowledge and a Knowledge Fuser component to
integrate the Patcher into frozen VidLMs without compromising their existing
capabilities. Due to limitations of the widely-used Video-Text Contrastive
(VTC) loss for learning action knowledge, we introduce the DVDM objective to
train the Knowledge Patcher. DVDM forces the model to encode the correlation
between the action text and the correct ordering of video frames. Our extensive
analyses show that Paxion and DVDM together effectively fill the gap in action
knowledge understanding (~50% to 80%), while maintaining or improving
performance on a wide spectrum of both object- and action-centric downstream
tasks. The code and data will be made publicly available for research purposes
at https://github.com/MikeWangWZHL/Paxion.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10713">
<div class="article-summary-box-inner">
<span><p>With growing capabilities of large language models, prompting them has become
the dominant way to access them. This has motivated the development of
strategies for automatically selecting effective language prompts. In this
paper, we introduce prompt flatness, a new metric to quantify the expected
utility of a language prompt. This metric is inspired by flatness
regularization in statistical learning that quantifies the robustness of the
model towards its parameter perturbations. We provide theoretical foundations
for this metric and its relationship with other prompt selection metrics,
providing a comprehensive understanding of existing methods. Empirically, we
show that combining prompt flatness with existing metrics improves both
performance and sample efficiency. Our metric outperforms the previous prompt
selection metrics with an average increase of 5% in accuracy and 10% in Pearson
correlation across 6 classification benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10786">
<div class="article-summary-box-inner">
<span><p>Prior studies diagnose the anisotropy problem in sentence representations
from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis
reveals that the sentence embeddings from BERT suffer from a bias towards
uninformative words, limiting the performance in semantic textual similarity
(STS) tasks. To address this bias, we propose a simple and efficient
unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words
with model-based importance estimations and computes the weighted average of
word representations from pre-trained models as sentence embeddings. Ditto can
be easily applied to any pre-trained language model as a postprocessing
operation. Compared to prior sentence embedding approaches, Ditto does not add
parameters nor requires any learning. Empirical evaluations demonstrate that
our proposed Ditto can alleviate the anisotropy problem and improve various
pre-trained models on STS tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11550">
<div class="article-summary-box-inner">
<span><p>We argue that translation quality alone is not a sufficient metric for
measuring knowledge transfer in multilingual neural machine translation. To
support this claim, we introduce Representational Transfer Potential (RTP),
which measures representational similarities between languages. We show that
RTP can measure both positive and negative transfer (interference), and find
that RTP is strongly correlated with changes in translation quality, indicating
that transfer does occur. Furthermore, we investigate data and language
characteristics that are relevant for transfer, and find that multi-parallel
overlap is an important yet under-explored feature. Based on this, we develop a
novel training scheme, which uses an auxiliary similarity loss that encourages
representations to be more invariant across languages by taking advantage of
multi-parallel data. We show that our method yields increased translation
quality for low- and mid-resource languages across multiple data and model
setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11747">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as ChatGPT, are prone to generate
hallucinations, i.e., content that conflicts with the source or cannot be
verified by the factual knowledge. To understand what types of content and to
which extent LLMs are apt to hallucinate, we introduce the Hallucination
Evaluation benchmark for Large Language Models (HaluEval), a large collection
of generated and human-annotated hallucinated samples for evaluating the
performance of LLMs in recognizing hallucination. To generate these samples, we
propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.
Besides, we also hire some human labelers to annotate the hallucinations in
ChatGPT responses. The empirical results suggest that ChatGPT is likely to
generate hallucinated content in specific topics by fabricating unverifiable
information (i.e., about $19.5\%$ responses). Moreover, existing LLMs face
great challenges in recognizing the hallucinations in texts. However, our
experiments also prove that providing external knowledge or adding reasoning
steps can help LLMs recognize hallucinations. Our benchmark can be accessed at
https://github.com/RUCAIBox/HaluEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11862">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable performance in
various tasks and gained significant attention. LLMs are also used for local
sequence transduction tasks, including grammatical error correction (GEC) and
formality style transfer, where most tokens in a source text are kept
unchanged. However, the models that generate all target tokens in such tasks
have a tendency to simply copy the input text as is, without making needed
changes, because the difference between input and output texts is minimal in
the training data. This is also inefficient because the computational cost
grows quadratically with the target sequence length with Transformer. This
paper proposes predicting edit spans for the source text for local sequence
transduction tasks. Representing an edit span with a position of the source
text and corrected tokens, we can reduce the length of the target sequence and
the computational cost for inference. We apply instruction tuning for LLMs on
the supervision data of edit spans. Experiments show that the proposed method
achieves comparable performance to the baseline in four tasks, paraphrasing,
formality style transfer, GEC, and text simplification, despite reducing the
length of the target text by as small as 21%. Furthermore, we report that the
task-specific fine-tuning with the proposed method achieved state-of-the-art
performance in the four tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Automated Topic Model Evaluation with Large Language Models. (arXiv:2305.12152v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12152">
<div class="article-summary-box-inner">
<span><p>Topic models are used to make sense of large text collections. However,
automatically evaluating topic model output and determining the optimal number
of topics both have been longstanding challenges, with no effective automated
solutions to date. This paper proposes using large language models to evaluate
such output. We find that large language models appropriately assess the
resulting topics, correlating more strongly with human judgments than existing
automated metrics. We then investigate whether we can use large language models
to automatically determine the optimal number of topics. We automatically
assign labels to documents and choosing configurations with the most pure
labels returns reasonable values for the optimal number of topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Open-QA Evaluation. (arXiv:2305.12421v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12421">
<div class="article-summary-box-inner">
<span><p>This study focuses on the evaluation of the Open Question Answering (Open-QA)
task, which can directly estimate the factuality of large language models
(LLMs). Current automatic evaluation methods have shown limitations, indicating
that human evaluation still remains the most reliable approach. We introduce a
new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset
EVOUNA, designed to assess the accuracy of AI-generated answers in relation to
standard answers within Open-QA. Our evaluation of these methods utilizes
human-annotated results to measure their performance. Specifically, the work
investigates methods that show high correlation with human evaluations, deeming
them more reliable. We also discuss the pitfalls of current methods and methods
to improve LLM-based evaluators. We believe this new QA-Eval task and
corresponding dataset EVOUNA will facilitate the development of more effective
automatic evaluation tools and prove valuable for future research in this area.
All resources are available at \url{https://github.com/wangcunxiang/QA-Eval}
and it is under the Apache-2.0 License.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12517">
<div class="article-summary-box-inner">
<span><p>While instruction-tuned Large Language Models (LLMs) excel at extracting
information from text, they are not suitable for locating texts conforming to a
given description in a large document collection (semantic retrieval).
Similarity search over embedding vectors does allow to perform retrieval by
query, but the similarity reflected in the embedding is ill-defined and
non-consistent, and is sub-optimal for many use cases. What, then, is a good
query representation for effective retrieval?
</p>
<p>We identify the well defined and consistent task of retrieving sentences
based on abstract descriptions of their content. We demonstrate the inadequacy
of current text embeddings and propose an alternative model that significantly
improves when used in standard nearest neighbor search. The model is trained
using positive and negative pairs sourced through prompting a LLM. While it is
easy to source the training material from an LLM, the retrieval task cannot be
performed by the LLM directly. This demonstrates that data from LLMs can be
used not only for distilling more efficient specialized models than the
original LLM, but also for creating new capabilities not immediately possible
using the original model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Few-shot Classification with Instruction-Finetuned Language Models. (arXiv:2305.12576v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12576">
<div class="article-summary-box-inner">
<span><p>A particularly successful class of approaches for few-shot learning combines
language models with prompts -- hand-crafted task descriptions that complement
data samples. However, designing prompts by hand for each task commonly
requires domain knowledge and substantial guesswork. We observe, in the context
of classification tasks, that instruction finetuned language models exhibit
remarkable prompt robustness, and we subsequently propose a simple method to
eliminate the need for handcrafted prompts, named AuT-Few. This approach
consists of (i) a prompt retrieval module that selects suitable task
instructions from the instruction-tuning knowledge base, and (ii) the
generation of two distinct, semantically meaningful, class descriptions and a
selection mechanism via cross-validation. Over $12$ datasets, spanning $8$
classification tasks, we show that AuT-Few outperforms current state-of-the-art
few-shot learning methods. Moreover, AuT-Few is the best ranking method across
datasets on the RAFT few-shot benchmark. Notably, these results are achieved
without task-specific handcrafted prompts on unseen tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture. (arXiv:2305.12710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12710">
<div class="article-summary-box-inner">
<span><p>Real-world domain experts (e.g., doctors) rarely annotate only a decision
label in their day-to-day workflow without providing explanations. Yet,
existing low-resource learning techniques, such as Active Learning (AL), that
aim to support human annotators mostly focus on the label while neglecting the
natural language explanation of a data point. This work proposes a novel AL
architecture to support experts' real-world need for label and explanation
annotations in low-resource scenarios. Our AL architecture leverages an
explanation-generation model to produce explanations guided by human
explanations, a prediction model that utilizes generated explanations toward
prediction faithfully, and a novel data diversity-based AL sampling strategy
that benefits from the explanation annotations. Automated and human evaluations
demonstrate the effectiveness of incorporating explanations into AL sampling
and the improved human annotation efficiency and trustworthiness with our AL
architecture. Additional ablation studies illustrate the potential of our AL
architecture for transfer learning, generalizability, and integration with
large language models (LLMs). While LLMs exhibit exceptional
explanation-generation capabilities for relatively simple tasks, their
effectiveness in complex real-world tasks warrants further in-depth study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models. (arXiv:2305.13085v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13085">
<div class="article-summary-box-inner">
<span><p>This study investigates machine translation between related languages i.e.,
languages within the same family that share linguistic characteristics such as
word order and lexical similarity. Machine translation through few-shot
prompting leverages a small set of translation pair examples to generate
translations for test sentences. This procedure requires the model to learn how
to generate translations while simultaneously ensuring that token ordering is
maintained to produce a fluent and accurate translation. We propose that for
related languages, the task of machine translation can be simplified by
leveraging the monotonic alignment characteristic of such languages. We
introduce DecoMT, a novel approach of few-shot prompting that decomposes the
translation process into a sequence of word chunk translations. Through
automatic and human evaluation conducted on multiple related language pairs
across various language families, we demonstrate that our proposed approach of
decomposed prompting surpasses multiple established few-shot baseline
approaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM
model with an average improvement of 8 chrF++ scores across the examined
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13186">
<div class="article-summary-box-inner">
<span><p>Current scientific fact-checking benchmarks exhibit several shortcomings,
such as biases arising from crowd-sourced claims and an over-reliance on
text-based evidence. We present SCITAB, a challenging evaluation dataset
consisting of 1.2K expert-verified scientific claims that 1) originate from
authentic scientific publications and 2) require compositional reasoning for
verification. The claims are paired with evidence-containing scientific tables
annotated with labels. Through extensive evaluations, we demonstrate that
SCITAB poses a significant challenge to state-of-the-art models, including
table-based pretraining models and large language models. All models except
GPT-4 achieved performance barely above random guessing. Popular prompting
techniques, such as Chain-of-Thought, do not achieve much performance gains on
SCITAB. Our analysis uncovers several unique challenges posed by SCITAB,
including table grounding, claim ambiguity, and compositional reasoning. Our
codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting is not a substitute for probability measurements in large language models. (arXiv:2305.13264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13264">
<div class="article-summary-box-inner">
<span><p>Prompting is now a dominant method for evaluating the linguistic knowledge of
large language models (LLMs). While other methods directly read out models'
probability distributions over strings, prompting requires models to access
this internal information by processing linguistic input, thereby implicitly
testing a new type of emergent ability: metalinguistic judgment. In this study,
we compare metalinguistic prompting and direct probability measurements as ways
of measuring models' linguistic knowledge. Broadly, we find that LLMs'
metalinguistic judgments are inferior to quantities directly derived from
representations. Furthermore, consistency gets worse as the prompt query
diverges from direct measurements of next-word probabilities. Our findings
suggest that negative results relying on metalinguistic prompts cannot be taken
as conclusive evidence that an LLM lacks a particular linguistic
generalization. Our results also highlight the value that is lost with the move
to closed APIs where access to probability distributions is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules. (arXiv:2305.13406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13406">
<div class="article-summary-box-inner">
<span><p>Existing large language models (LLMs) that mainly focus on Standard American
English (SAE) often lead to significantly worse performance when being applied
to other English dialects. While existing mitigations tackle discrepancies for
individual target dialects, they assume access to high-accuracy dialect
identification systems. The boundaries between dialects are inherently
flexible, making it difficult to categorize language into discrete predefined
categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic
Aggregation), a modular approach to imbue SAE-trained models with
multi-dialectal robustness by composing adapters which handle specific
linguistic features. The compositional architecture of DADA allows for both
targeted adaptation to specific dialect variants and simultaneous adaptation to
various dialects. We show that DADA is effective for both single task and
instruction finetuned language models, offering an extensible and interpretable
framework for adapting existing LLMs to different English dialects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAILEX: Email Event and Argument Extraction. (arXiv:2305.13469v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13469">
<div class="article-summary-box-inner">
<span><p>In this work, we present the first dataset, MailEx, for performing event
extraction from conversational email threads. To this end, we first proposed a
new taxonomy covering 10 event types and 76 arguments in the email domain. Our
final dataset includes 1.5K email threads and ~4K emails, which are annotated
with totally ~8K event instances. To understand the task challenges, we
conducted a series of experiments comparing three types of approaches, i.e.,
fine-tuned sequence labeling, fine-tuned generative extraction, and few-shot
in-context learning. Our results showed that the task of email event extraction
is far from being addressed, due to challenges lying in, e.g., extracting
non-continuous, shared trigger spans, extracting non-named entity arguments,
and modeling the email conversational history. Our work thus suggests more
future investigations in this domain-specific event extraction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look-back Decoding for Open-Ended Text Generation. (arXiv:2305.13477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13477">
<div class="article-summary-box-inner">
<span><p>Given a prefix (context), open-ended generation aims to decode texts that are
coherent, which do not abruptly drift from previous topics, and informative,
which do not suffer from undesired repetitions. In this paper, we propose
Look-back, an improved decoding algorithm that leverages the Kullback-Leibler
divergence to track the distribution distance between current and historical
decoding steps. Thus Look-back can automatically predict potential repetitive
phrase and topic drift, and remove tokens that may cause the failure modes,
restricting the next token probability distribution within a plausible distance
to the history. We perform decoding experiments on document continuation and
story generation, and demonstrate that Look-back is able to generate more
fluent and coherent text, outperforming other strong decoding methods
significantly in both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks. (arXiv:2305.13547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13547">
<div class="article-summary-box-inner">
<span><p>Text classification tasks often encounter few shot scenarios with limited
labeled data, and addressing data scarcity is crucial. Data augmentation with
mixup has shown to be effective on various text classification tasks. However,
most of the mixup methods do not consider the varying degree of learning
difficulty in different stages of training and generate new samples with one
hot labels, resulting in the model over confidence. In this paper, we propose a
self evolution learning (SE) based mixup approach for data augmentation in text
classification, which can generate more adaptive and model friendly pesudo
samples for the model training. SE focuses on the variation of the model's
learning ability. To alleviate the model confidence, we introduce a novel
instance specific label smoothing approach, which linearly interpolates the
model's output and one hot labels of the original samples to generate new soft
for label mixing up. Through experimental analysis, in addition to improving
classification accuracy, we demonstrate that SE also enhances the model's
generalize ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDIS: Entity-Driven Image Search over Multimodal Web Content. (arXiv:2305.13631v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13631">
<div class="article-summary-box-inner">
<span><p>Making image retrieval methods practical for real-world search applications
requires significant progress in dataset scales, entity comprehension, and
multimodal information fusion. In this work, we introduce
\textbf{E}ntity-\textbf{D}riven \textbf{I}mage \textbf{S}earch (EDIS), a
challenging dataset for cross-modal image search in the news domain. EDIS
consists of 1 million web images from actual search engine results and curated
datasets, with each image paired with a textual description. Unlike datasets
that assume a small set of single-modality candidates, EDIS reflects real-world
web image search scenarios by including a million multimodal image-text pairs
as candidates. EDIS encourages the development of retrieval models that
simultaneously address cross-modal information fusion and matching. To achieve
accurate ranking results, a model must: 1) understand named entities and events
from text queries, 2) ground entities onto images or text descriptions, and 3)
effectively fuse textual and visual representations. Our experimental results
show that EDIS challenges state-of-the-art methods with dense entities and a
large-scale candidate set. The ablation study also proves that fusing textual
features with visual features is critical in improving retrieval results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13735">
<div class="article-summary-box-inner">
<span><p>Aligning large language models (LLMs) to human values has become increasingly
important as it enables sophisticated steering of LLMs. However, it requires
significant human demonstrations and feedback or distillation from proprietary
LLMs such as ChatGPT. In this work, we propose a novel alignment learning
framework with synthetic feedback not dependent on extensive human annotations
and proprietary LLMs. First, we perform reward modeling (RM) with synthetic
feedback by contrasting responses from vanilla LLMs with various sizes and
prompts. Then, we use the RM to simulate high-quality demonstrations to train a
supervised policy and further optimize the model with reinforcement learning.
Our resulting model, Aligned Language Model with Synthetic Training dataset
(ALMoST), outperforms recent open-sourced models, which are trained on the
outputs of InstructGPT or human-annotated demonstrations, in alignment
benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2,
55.0% and 58.5% of the time, respectively. Further analyses demonstrate the
efficacy and importance of synthetic feedback in our framework. The code is
available at https://github.com/naver-ai/almost
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning. (arXiv:2305.13971v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13971">
<div class="article-summary-box-inner">
<span><p>Despite their impressive performance, large language models (LMs) still
struggle with reliably generating complex output structures when not finetuned
to follow the required output format exactly. To address this issue,
grammar-constrained decoding (GCD) can be used to control the generation of
LMs, guaranteeing that the output follows a given structure. Most existing GCD
methods are, however, limited to specific tasks, such as parsing or code
generation. In this work, we demonstrate that formal grammars can describe the
output space for a much wider range of tasks and argue that GCD can serve as a
unified framework for structured NLP tasks in general. For increased
flexibility, we introduce input-dependent grammars, which allow the grammar to
depend on the input and thus enable the generation of different output
structures for different inputs. We then empirically demonstrate the power and
flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity
disambiguation, and (3) constituency parsing. Our results indicate that
grammar-constrained LMs substantially outperform unconstrained LMs or even beat
task-specific finetuned models. Grammar constraints thus hold great promise for
harnessing off-the-shelf LMs for a wide range of structured NLP tasks,
especially where training data is scarce or finetuning is expensive. Code and
data: https://github.com/epfl-dlab/GCD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Condensing Multilingual Knowledge with Lightweight Language-Specific Modules. (arXiv:2305.13993v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13993">
<div class="article-summary-box-inner">
<span><p>Incorporating language-specific (LS) modules is a proven method to boost
performance in multilingual machine translation. This approach bears similarity
to Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the
scalability of this approach to hundreds of languages (experts) tends to be
unmanageable due to the prohibitive number of parameters introduced by
full-rank matrices in fully-connected layers. In this work, we introduce the
Language-Specific Matrix Synthesis (LMS) method. This approach constructs LS
modules by generating low-rank matrices from two significantly smaller matrices
to approximate the full-rank matrix. Furthermore, we condense multilingual
knowledge from multiple LS modules into a single shared module with the Fuse
Distillation (FD) technique to improve the efficiency of inference and model
serialization. We show that our LMS method significantly outperforms previous
LS methods and MoE methods with the same amount of extra parameters, e.g., 1.73
BLEU points over the Switch Transformer on many-to-many multilingual machine
translation. Importantly, LMS is able to have comparable translation
performance with much fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13999">
<div class="article-summary-box-inner">
<span><p>Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE)
have proven effective in scaling up Transformers model size for
\textit{pretraining} large language models. By only activating part of the FFN
parameters conditioning on input, S-FFN improves generalization performance
while keeping training and inference costs (in FLOPs) fixed. In this work, we
analyzed two major design choices of S-FFN: the memory block (a.k.a. expert)
size and the memory block selection method under a general conceptual framework
of sparse neural memory. Using this unified framework, we compare several S-FFN
architectures for language modeling and provide insights into their relative
efficacy and efficiency. We found a simpler selection method --
\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated
hidden states, achieving lower perplexity in language model pretraining
compared to existing MoE architectures including Switch Transformer (Fedus et
al., 2021) and HashLayer (Roller et al., 2021).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation. (arXiv:2305.14105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14105">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated the capability to perform on machine
translation when the input is prompted with a few examples (in-context
learning). Translation quality depends on various features of the selected
examples, such as their quality and relevance, but previous work has
predominantly focused on individual features in isolation. In this paper, we
propose a general framework for combining different features influencing
example selection. We learn a regression model, CTQ Scorer (Contextual
Translation Quality), that selects examples based on multiple features in order
to maximize the translation quality. On multiple language pairs and language
models, we show that CTQ Scorer helps significantly outperform random selection
as well as strong single-factor baselines reported in the literature. We also
see an improvement of over 2.5 COMET points on average with respect to a strong
BM25 retrieval-based baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models. (arXiv:2305.14214v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14214">
<div class="article-summary-box-inner">
<span><p>While many languages possess processes of joining two or more words to create
compound words, previous studies have been typically limited only to languages
with excessively productive compound formation (e.g., German, Dutch) and there
is no public dataset containing compound and non-compound words across a large
number of languages. In this work, we systematically study decompounding, the
task of splitting compound words into their constituents, at a wide scale. We
first address the data gap by introducing a dataset of 255k compound and
non-compound words across 56 diverse languages obtained from Wiktionary. We
then use this dataset to evaluate an array of Large Language Models (LLMs) on
the decompounding task. We find that LLMs perform poorly, especially on words
which are tokenized unfavorably by subword tokenization. We thus introduce a
novel methodology to train dedicated models for decompounding. The proposed
two-stage procedure relies on a fully self-supervised objective in the first
stage, while the second, supervised learning stage optionally fine-tunes the
model on the annotated Wiktionary data. Our self-supervised models outperform
the prior best unsupervised decompounding models by 13.9% accuracy on average.
Our fine-tuned models outperform all prior (language-specific) decompounding
tools. Furthermore, we use our models to leverage decompounding during the
creation of a subword tokenizer, which we refer to as CompoundPiece.
CompoundPiece tokenizes compound words more favorably on average, leading to
improved performance on decompounding over an otherwise equivalent model using
SentencePiece tokenization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Large Language Models Are Not (Yet) Code-Switchers. (arXiv:2305.14235v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14235">
<div class="article-summary-box-inner">
<span><p>Multilingual Large Language Models (LLMs) have recently shown great
capabilities in a wide range of tasks, exhibiting state-of-the-art performance
through zero-shot or few-shot prompting methods. While there have been
extensive studies on their abilities in monolingual tasks, the investigation of
their potential in the context of code-switching (CSW), the practice of
alternating languages within an utterance, remains relatively uncharted. In
this paper, we provide a comprehensive empirical analysis of various
multilingual LLMs, benchmarking their performance across four tasks: sentiment
analysis, machine translation, summarization and word-level language
identification. Our results indicate that despite multilingual LLMs exhibiting
promising outcomes in certain tasks using zero or few-shot prompting, they
still underperform in comparison to fine-tuned models of much smaller scales.
We argue that current "multilingualism" in LLMs does not inherently imply
proficiency with code-switching texts, calling for future research to bridge
this discrepancy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14257">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) struggle on processing complicated observations
in interactive decision making tasks. To alleviate this issue, we propose a
simple hierarchical prompting approach. Diverging from previous prompting
approaches that always put the \emph{full} observation~(\eg a web page) to the
prompt, we propose to first construct an action-aware observation which is more
\emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor
prompt then predicts the next action based on the summarized observation. While
our method has broad applicability, we particularly demonstrate its efficacy in
the complex domain of web navigation where a full observation often contains
redundant and irrelevant information. Our approach outperforms the previous
state-of-the-art prompting mechanis by 6.2\% on task success rate,
demonstrating its potential on interactive decision making tasks with long
observation traces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Rewriting for Retrieval-Augmented Large Language Models. (arXiv:2305.14283v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14283">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) play powerful, black-box readers in the
retrieve-then-read pipeline, making remarkable progress in knowledge-intensive
tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of
the previous retrieve-then-read for the retrieval-augmented LLMs from the
perspective of the query rewriting. Unlike prior studies focusing on adapting
either the retriever or the reader, our approach pays attention to the
adaptation of the search query itself, for there is inevitably a gap between
the input text and the needed knowledge in retrieval. We first prompt an LLM to
generate the query, then use a web search engine to retrieve contexts.
Furthermore, to better align the query to the frozen modules, we propose a
trainable scheme for our pipeline. A small language model is adopted as a
trainable rewriter to cater to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader by reinforcement learning.
Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice
QA. Experiments results show consistent performance improvement, indicating
that our framework is proven effective and scalable, and brings a new framework
for retrieval-augmented LLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-powered Data Augmentation for Enhanced Cross-lingual Performance. (arXiv:2305.14288v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14288">
<div class="article-summary-box-inner">
<span><p>This paper explores the potential of leveraging Large Language Models (LLMs)
for data augmentation in multilingual commonsense reasoning datasets where the
available training data is extremely limited. To achieve this, we utilise
several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment
three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate
the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR,
using the synthesised data. We compare the performance of training with data
generated in English and target languages, as well as translated
English-generated data, revealing the overall advantages of incorporating data
generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best
case. Furthermore, we conduct a human evaluation by asking native speakers to
assess the naturalness and logical coherence of the generated examples across
different languages. The results of the evaluation indicate that LLMs such as
ChatGPT and GPT-4 excel at producing natural and coherent text in most
languages, however, they struggle to generate meaningful text in certain
languages like Tamil. We also observe that ChatGPT falls short in generating
plausible alternatives compared to the original dataset, whereas examples from
GPT-4 exhibit competitive logical consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TalkUp: Paving the Way for Understanding Empowering Language. (arXiv:2305.14326v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14326">
<div class="article-summary-box-inner">
<span><p>Empowering language is important in many real-world contexts, from education
to workplace dynamics to healthcare. Though language technologies are growing
more prevalent in these contexts, empowerment has seldom been studied in NLP,
and moreover, it is inherently challenging to operationalize because of its
implicit nature. This work builds from linguistic and social psychology
literature to explore what characterizes empowering language. We then
crowdsource a novel dataset of Reddit posts labeled for empowerment, reasons
why these posts are empowering to readers, and the social relationships between
posters and readers. Our preliminary analyses show that this dataset, which we
call TalkUp, can be used to train language models that capture empowering and
disempowering language. More broadly, TalkUp provides an avenue to explore
implication, presuppositions, and how social context influences the meaning of
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Model Selection with Large Language Models for Reasoning. (arXiv:2305.14333v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14333">
<div class="article-summary-box-inner">
<span><p>Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two
distinct reasoning methods, each with its own strengths. CoT employs natural
language, offering flexibility and interpretability, while PAL utilizes
programming language, yielding more structured and rigorous logic. We introduce
a model selection method to combine the best of both worlds by employing a
large language model (LLM) to dynamically select between them. Our theoretical
analysis underscores the feasibility of this method, which is further
corroborated by empirical results. Our proposed method demonstrates significant
performance improvements across eight reasoning datasets with Codex, ChatGPT,
and GPT-4. Additionally, our method is complementary to self-consistency; when
integrated, it can further enhance performance while significantly reducing
computation costs. Moreover, we achieve new state-of-the-art results on GSM8K
and SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and
prompts are available at https://github.com/XuZhao0/Model-Selection-Reasoning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14458">
<div class="article-summary-box-inner">
<span><p>Large language models (e.g., GPT-4) are uniquely capable of producing highly
rated text simplification, yet current human evaluation methods fail to provide
a clear understanding of systems' specific strengths and weaknesses. To address
this limitation, we introduce SALSA, an edit-based human annotation framework
that enables holistic and fine-grained text simplification evaluation. We
develop twenty one linguistically grounded edit types, covering the full
spectrum of success and failure across dimensions of conceptual, syntactic and
lexical simplicity. Using SALSA, we collect 19K edit annotations on 840
simplifications, revealing discrepancies in the distribution of simplification
strategies performed by fine-tuned models, prompted LLMs and humans, and find
GPT-3.5 performs more quality edits than humans, but still exhibits frequent
errors. Using our fine-grained annotations, we develop LENS-SALSA, a
reference-free automatic simplification metric, trained to predict sentence-
and word-level quality simultaneously. Additionally, we introduce word-level
quality estimation for simplification and report promising baseline results.
Our data, new metric, and annotation toolkit are available at
https://salsa-eval.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment. (arXiv:2305.14492v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14492">
<div class="article-summary-box-inner">
<span><p>Designing systems that can reason across cultures requires that they are
grounded in the norms of the contexts in which they operate. However, current
research on developing computational models of social norms has primarily
focused on American society. Here, we propose a novel approach to discover and
compare descriptive social norms across Chinese and American cultures. We
demonstrate our approach by leveraging discussions on a Chinese Q&amp;A platform
(Zhihu) and the existing SocialChemistry dataset as proxies for contrasting
cultural axes, align social situations cross-culturally, and extract social
norms from texts using in-context learning. Embedding Chain-of-Thought
prompting in a human-AI collaborative framework, we build a high-quality
dataset of 3,069 social norms aligned with social situations across Chinese and
American cultures alongside corresponding free-text explanations. To test the
ability of models to reason about social norms across cultures, we introduce
the task of explainable social norm entailment, showing that existing models
under 3B parameters have significant room for improvement in both automatic and
human evaluation. Further analysis of cross-cultural norm differences based on
our dataset shows empirical alignment with the social orientations framework,
revealing several situational and descriptive nuances in norms across these
cultures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14499">
<div class="article-summary-box-inner">
<span><p>Neural document rerankers are extremely effective in terms of accuracy.
However, the best models require dedicated hardware for serving, which is
costly and often not feasible. To avoid this serving-time requirement, we
present a method of capturing up to 86% of the gains of a Transformer
cross-attention model with a lexicalized scoring function that only requires
10-6% of the Transformer's FLOPs per document and can be served using commodity
CPUs. When combined with a BM25 retriever, this approach matches the quality of
a state-of-the art dual encoder retriever, that still requires an accelerator
for query encoding. We introduce NAIL (Non-Autoregressive Indexing with
Language models) as a model architecture that is compatible with recent
encoder-decoder and decoder-only large language models, such as T5, GPT-3 and
PaLM. This model architecture can leverage existing pre-trained checkpoints and
can be fine-tuned for efficiently constructing document representations that do
not require neural processing of queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14536">
<div class="article-summary-box-inner">
<span><p>While automatic dialogue tutors hold great potential in making education
personalized and more accessible, research on such systems has been hampered by
a lack of sufficiently large and high-quality datasets. Collecting such
datasets remains challenging, as recording tutoring sessions raises privacy
concerns and crowdsourcing leads to insufficient data quality. To address this,
we propose a framework to generate such dialogues by pairing human teachers
with a Large Language Model (LLM) prompted to represent common student errors.
We describe how we use this framework to collect MathDial, a dataset of 3k
one-to-one teacher-student tutoring dialogues grounded in multi-step math
reasoning problems. While models like GPT-3 are good problem solvers, they fail
at tutoring because they generate factually incorrect feedback or are prone to
revealing solutions to students too early. To overcome this, we let teachers
provide learning opportunities to students by guiding them using various
scaffolding questions according to a taxonomy of teacher moves. We demonstrate
MathDial and its extensive annotations can be used to finetune models to be
more effective tutors (and not just solvers). We confirm this by automatic and
human evaluation, notably in an interactive setting that measures the trade-off
between student solving success and telling solutions. The dataset is released
publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sources of Hallucination by Large Language Models on Inference Tasks. (arXiv:2305.14552v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14552">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are claimed to be capable of Natural Language
Inference (NLI), necessary for applied tasks like question answering and
summarization. We present a series of behavioral studies on several LLM
families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled
experiments. We establish two biases originating from pretraining which predict
much of their behavior, and show that these are major sources of hallucination
in generative LLMs. First, memorization at the level of sentences: we show
that, regardless of the premise, models falsely label NLI test samples as
entailing when the hypothesis is attested in training data, and that entities
are used as ``indices'' to access the memorized data. Second, statistical
patterns of usage learned at the level of corpora: we further show a similar
effect when the premise predicate is less frequent than that of the hypothesis
in the training data, a bias following from previous studies. We demonstrate
that LLMs perform significantly worse on NLI test samples which do not conform
to these biases than those which do, and we offer these as valuable controls
for future LLM evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings. (arXiv:2305.14576v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14576">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have ignited a surge in demand for
effective fine-tuning techniques, particularly in low-resource domains and
languages. Active learning (AL), a set of algorithms designed to decrease
labeling costs by minimizing label complexity, has shown promise in confronting
the labeling bottleneck. In parallel, adapter modules designed for
parameter-efficient fine-tuning (PEFT) have demonstrated notable potential in
low-resource settings. However, the interplay between AL and adapter-based PEFT
remains unexplored. We present an empirical study of PEFT behavior with AL in
low-resource settings for text classification tasks. Our findings affirm the
superiority of PEFT over full-fine tuning (FFT) in low-resource settings and
demonstrate that this advantage persists in AL setups. We further examine the
properties of PEFT and FFT through the lens of forgetting dynamics and
instance-level representations, where we find that PEFT yields more stable
representations of early and middle layers compared to FFT. Our research
underscores the synergistic potential of AL and PEFT in low-resource settings,
paving the way for advancements in efficient and effective fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14617">
<div class="article-summary-box-inner">
<span><p>Understanding the speaker's intended meaning often involves drawing
commonsense inferences to reason about what is not stated explicitly. In
multi-event sentences, it requires understanding the relationships between
events based on contextual knowledge. We propose COMET-M (Multi-Event), an
event-centric commonsense model capable of generating commonsense inferences
for a target event within a complex sentence. COMET-M builds upon COMET
(Bosselut et al., 2019), which excels at generating event-centric inferences
for simple sentences, but struggles with the complexity of multi-event
sentences prevalent in natural text. To overcome this limitation, we curate a
multi-event inference dataset of 35K human-written inferences. We trained
COMET-M on the human-written inferences and also created baselines using
automatically labeled examples. Experimental results demonstrate the
significant performance improvement of COMET-M over COMET in generating
multi-event inferences. Moreover, COMET-M successfully produces distinct
inferences for each target event, taking the complete context into
consideration. COMET-M holds promise for downstream tasks involving natural
text such as coreference resolution, dialogue, and story understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Are What You Annotate: Towards Better Models through Annotator Representations. (arXiv:2305.14663v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14663">
<div class="article-summary-box-inner">
<span><p>Annotator disagreement is ubiquitous in natural language processing (NLP)
tasks. There are multiple reasons for such disagreements, including the
subjectivity of the task, difficult cases, unclear guidelines, and so on.
Rather than simply aggregating labels to obtain data annotations, we instead
try to directly model the diverse perspectives of the annotators, and
explicitly account for annotators' idiosyncrasies in the modeling process by
creating representations for each annotator (annotator embeddings) and also
their annotations (annotation embeddings). In addition, we propose TID-8, The
Inherent Disagreement - 8 dataset, a benchmark that consists of eight existing
language understanding datasets that have inherent annotator disagreement. We
test our approach on TID-8 and show that our approach helps models learn
significantly better from disagreements on six different datasets in TID-8
while increasing model size by fewer than 1% parameters. By capturing the
unique tendencies and subjectivity of individual annotators through embeddings,
our representations prime AI models to be inclusive of diverse viewpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Biases in Automatic Evaluation Metrics for Image Captioning. (arXiv:2305.14711v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14711">
<div class="article-summary-box-inner">
<span><p>Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have
demonstrated decent correlations with human judgments in various language
generation tasks. However, their impact on fairness remains largely unexplored.
It is widely recognized that pretrained models can inadvertently encode
societal biases, thus employing these models for evaluation purposes may
inadvertently perpetuate and amplify biases. For example, an evaluation metric
may favor the caption "a woman is calculating an account book" over "a man is
calculating an account book," even if the image only shows male accountants. In
this paper, we conduct a systematic study of gender biases in model-based
automatic evaluation metrics for image captioning tasks. We start by curating a
dataset comprising profession, activity, and object concepts associated with
stereotypical gender associations. Then, we demonstrate the negative
consequences of using these biased metrics, including the inability to
differentiate between biased and unbiased generations, as well as the
propagation of biases to generation models through reinforcement learning.
Finally, we present a simple and effective way to mitigate the metric bias
without hurting the correlations with human judgments. Our dataset and
framework lay the foundation for understanding the potential harm of
model-based evaluation metrics, and facilitate future works to develop more
inclusive evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14735">
<div class="article-summary-box-inner">
<span><p>The impact of AI models on marginalized communities has traditionally been
measured by identifying performance differences between specified demographic
subgroups. Though this approach aims to center vulnerable groups, it risks
obscuring patterns of harm faced by intersectional subgroups or shared across
multiple groups. To address this, we draw on theories of marginalization from
disability studies and related disciplines, which state that people farther
from the norm face greater adversity, to consider the "margins" in the domain
of toxicity detection. We operationalize the "margins" of a dataset by
employing outlier detection to identify text about people with demographic
attributes distant from the "norm". We find that model performance is
consistently worse for demographic outliers, with mean squared error (MSE)
between outliers and non-outliers up to 70.4% worse across toxicity types. It
is also worse for text outliers, with a MSE up to 68.4% higher for outliers
than non-outliers. We also find text and demographic outliers to be
particularly susceptible to errors in the classification of severe toxicity and
identity attacks. Compared to analysis of disparities using traditional
demographic breakdowns, we find that our outlier analysis frequently surfaces
greater harms faced by a larger, more intersectional group, which suggests that
outlier analysis is particularly beneficial for identifying harms against those
groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECHo: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric Reasoning. (arXiv:2305.14740v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14740">
<div class="article-summary-box-inner">
<span><p>We introduce ECHo (Event Causality Inference via Human-Centric Reasoning), a
diagnostic dataset of event causality inference grounded in visio-linguistic
social scenarios. ECHo employs real-world human-centric deductive information
building on a television crime drama. ECHo requires the Theory-of-Mind (ToM)
ability to understand and reason about social interactions based on multimodal
information. Using ECHo, we propose a unified Chain-of-Thought (CoT) framework
to assess the reasoning capability of current AI systems. Our ToM-enhanced CoT
pipeline accommodates various large foundation models in both zero-shot and
few-shot visio-linguistic reasoning. We use this framework to scrutinize recent
large foundation models such as InstructGPT and MiniGPT-4 on three diagnostic
human-centric tasks. Further analysis demonstrates ECHo as a challenging
dataset to expose imperfections and inconsistencies in reasoning. Our data and
code are publicly available at https://github.com/YuxiXie/ECHo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14755">
<div class="article-summary-box-inner">
<span><p>Most existing stylistic text rewriting methods and evaluation metrics operate
on a sentence level, but ignoring the broader context of the text can lead to
preferring generic, ambiguous, and incoherent rewrites. In this paper, we
investigate integrating the preceding textual context into both the
$\textit{rewriting}$ and $\textit{evaluation}$ stages of stylistic text
rewriting, and introduce a new composite contextual evaluation metric
$\texttt{CtxSimFit}$ that combines similarity to the original sentence with
contextual cohesiveness. We comparatively evaluate non-contextual and
contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our
experiments show that humans significantly prefer contextual rewrites as more
fitting and natural over non-contextual ones, yet existing sentence-level
automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences
($\rho$=0--0.3). In contrast, human preferences are much better reflected by
both our novel $\texttt{CtxSimFit}$ ($\rho$=0.7--0.9) as well as proposed
context-infused versions of common metrics ($\rho$=0.4--0.7). Overall, our
findings highlight the importance of integrating context into the generation
and especially the evaluation stages of stylistic text rewriting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization. (arXiv:2305.14760v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14760">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have achieved remarkable success in natural
language understanding. However, fine-tuning pretrained models on limited
training data tends to overfit and thus diminish performance. This paper
presents Bi-Drop, a fine-tuning strategy that selectively updates model
parameters using gradients from various sub-nets dynamically generated by
dropout. The sub-net estimation of Bi-Drop is performed in an in-batch manner,
so it overcomes the problem of hysteresis in sub-net updating, which is
possessed by previous methods that perform asynchronous sub-net estimation.
Also, Bi-Drop needs only one mini-batch to estimate the sub-net so it achieves
higher utility of training data. Experiments on the GLUE benchmark demonstrate
that Bi-Drop consistently outperforms previous fine-tuning methods.
Furthermore, empirical results also show that Bi-Drop exhibits excellent
generalization ability and robustness for domain transfer, data imbalance, and
low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14794">
<div class="article-summary-box-inner">
<span><p>Recent advances in weakly supervised text classification mostly focus on
designing sophisticated methods to turn high-level human heuristics into
quality pseudo-labels. In this paper, we revisit the seed matching-based
method, which is arguably the simplest way to generate pseudo-labels, and show
that its power was greatly underestimated. We show that the limited performance
of seed matching is largely due to the label bias injected by the simple
seed-match rule, which prevents the classifier from learning reliable
confidence for selecting high-quality pseudo-labels. Interestingly, simply
deleting the seed words present in the matched input texts can mitigate the
label bias and help learn better confidence. Subsequently, the performance
achieved by seed matching can be improved significantly, making it on par with
or even better than the state-of-the-art. Furthermore, to handle the case when
the seed words are not made known, we propose to simply delete the word tokens
in the input text randomly with a high deletion ratio. Remarkably, seed
matching equipped with this random deletion method can often achieve even
better performance than that with seed deletion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory. (arXiv:2305.14889v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14889">
<div class="article-summary-box-inner">
<span><p>We address a fundamental challenge in Natural Language Generation (NLG) model
evaluation -- the design and evaluation of evaluation metrics. Recognizing the
limitations of existing automatic metrics and noises from how current human
evaluation was conducted, we propose MetricEval, a framework informed by
measurement theory, the foundation of educational test design, for
conceptualizing and evaluating the reliability and validity of NLG evaluation
metrics. The framework formalizes the source of measurement error and offers
statistical tools for evaluating evaluation metrics based on empirical data.
With our framework, one can quantify the uncertainty of the metrics to better
interpret the result. To exemplify the use of our framework in practice, we
analyzed a set of evaluation metrics for summarization and identified issues
related to conflated validity structure in human-eval and reliability in
LLM-based metrics. Through MetricEval, we aim to promote the design,
evaluation, and interpretation of valid and reliable metrics to advance robust
and effective NLG models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Self-Adaptive Prompting. (arXiv:2305.14926v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14926">
<div class="article-summary-box-inner">
<span><p>A hallmark of modern large language models (LLMs) is their impressive general
zero-shot and few-shot abilities, often elicited through in-context learning
(ICL) via prompting. However, while highly coveted and being the most general,
zero-shot performances in LLMs are still typically weaker due to the lack of
guidance and the difficulty of applying existing automatic prompt design
methods in general tasks when ground-truth labels are unavailable. In this
study, we address this by presenting Universal Self-Adaptive Prompting (USP),
an automatic prompt design approach specifically tailored for zero-shot
learning (while compatible with few-shot). Requiring only a small amount of
unlabeled data and an inference-only LLM, USP is highly versatile: to achieve
universal prompting, USP categorizes a possible NLP task into one of the three
possible task types and then uses a corresponding selector to select the most
suitable queries and zero-shot model-generated responses as
pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a
fully automated way. We evaluate USP with PaLM and PaLM 2 models and
demonstrate performances that are considerably stronger than standard zero-shot
baselines and often comparable to or even superior to few-shot baselines across
more than 40 natural language understanding, natural language generation, and
reasoning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP. (arXiv:2305.14976v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14976">
<div class="article-summary-box-inner">
<span><p>ChatGPT's emergence heralds a transformative phase in NLP, particularly
demonstrated through its excellent performance on many English benchmarks.
However, the model's efficacy across diverse linguistic contexts remains
largely uncharted territory. This work aims to bridge this knowledge gap, with
a primary focus on assessing ChatGPT's capabilities on Arabic languages and
dialectal varieties. Our comprehensive study conducts a large-scale automated
and human evaluation of ChatGPT, encompassing 44 distinct language
understanding and generation tasks on over 60 different datasets. To our
knowledge, this marks the first extensive performance analysis of ChatGPT's
deployment in Arabic NLP. Our findings indicate that, despite its remarkable
performance in English, ChatGPT is consistently surpassed by smaller models
that have undergone finetuning on Arabic. We further undertake a meticulous
comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal
Arabic (DA), unveiling the relative shortcomings of both models in handling
Arabic dialects compared to MSA. Although we further explore and confirm the
utility of employing GPT-4 as a potential alternative for human evaluation, our
work adds to a growing body of research underscoring the limitations of
ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning with Language Model is Planning with World Model. (arXiv:2305.14992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14992">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown remarkable reasoning capabilities,
especially when prompted to generate intermediate reasoning steps (e.g.,
Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are
easy for humans, such as generating action plans for executing tasks in a given
environment, or performing complex math, logical, and commonsense reasoning.
The deficiency stems from the key fact that LLMs lack an internal
$\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment
status, intermediate variable values) and simulate long-term outcomes of
actions. This prevents LLMs from performing deliberate planning akin to human
brains, which involves exploring alternative reasoning paths, anticipating
future states and rewards, and iteratively refining existing reasoning steps.
To overcome the limitations, we propose a new LLM reasoning framework,
$\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning
$\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning
agent, and incorporates a principled planning algorithm (based on Monto Carlo
Tree Search) for strategic exploration in the vast reasoning space. During
reasoning, the LLM (as agent) incrementally builds a reasoning tree under the
guidance of the LLM (as world model) and task-specific rewards, and obtains a
high-reward reasoning path efficiently with a proper balance between
exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of
challenging reasoning problems including plan generation, math reasoning, and
logical inference. Empirical results on these tasks demonstrate the superiority
of RAP over various strong baselines, including CoT and least-to-most prompting
with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%
relative improvement in a plan generation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems. (arXiv:2305.15017v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15017">
<div class="article-summary-box-inner">
<span><p>Despite outstanding performance in many tasks, language models are
notoriously inclined to make factual errors in tasks requiring arithmetic
computation. We address this deficiency by creating Calc-X, a collection of
datasets that demonstrates the appropriate use of a calculator in reasoning
chains. Calc-X is suitable for teaching language models to offload computations
to a symbolic system. We survey and unify several existing chain-of-thought
datasets into a proposed format, resulting in a standard collection of over
300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X
collection to train open-source calculator-using models we call Calcformers and
show that these models approximately double the accuracy of generating correct
results compared to vanilla language model baselines. We make all Calc-X
datasets, source code and Calcformers models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories. (arXiv:2305.15028v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15028">
<div class="article-summary-box-inner">
<span><p>Recently, Large Language Models (LLMs) have been serving as general-purpose
interfaces, posing a significant demand for comprehensive visual knowledge.
However, it remains unclear how well current LLMs and their visually augmented
counterparts (VaLMs) can master visual commonsense knowledge. To investigate
this, we propose ImageNetVC, a human-annotated dataset specifically designed
for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet
categories. Utilizing ImageNetVC, we benchmark the fundamental visual
commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze
the factors affecting the visual commonsense knowledge of large-scale models,
providing insights into the development of language models enriched with visual
commonsense knowledge. Our code and dataset are available at
https://github.com/hemingkx/ImageNetVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations. (arXiv:2305.15035v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15035">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have exhibited striking in-context learning
(ICL) ability to adapt to target tasks with a few input-output demonstrations.
For better ICL, different methods are proposed to select representative
demonstrations from existing training corpora. However, such settings are not
aligned with real-world practices, as end-users usually query LMs without
access to demonstration pools. In this work, we introduce Self-ICL -- a simple
framework which bootstraps LMs' intrinsic capabilities to perform zero-shot
ICL. Given a test input, Self-ICL first prompts the model to generate
pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via
zero-shot prompting. Finally, we perform ICL for the test input with the
pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard
tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy
and head-to-head comparison. Moreover, with zero-shot chain-of-thought,
Self-ICL achieves results comparable to using real demonstrations.
Additionally, we conduct a range of analyses to validate Self-ICL's
effectiveness and provide insights for its behaviors under different settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is GPT-4 a Good Data Analyst?. (arXiv:2305.15038v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15038">
<div class="article-summary-box-inner">
<span><p>As large language models (LLMs) have demonstrated their powerful capabilities
in plenty of domains and tasks, including context understanding, code
generation, language generation, data storytelling, etc., many data analysts
may raise concerns if their jobs will be replaced by artificial intelligence
(AI). This controversial topic has drawn great attention in public. However, we
are still at a stage of divergent opinions without any definitive conclusion.
Motivated by this, we raise the research question of "is GPT-4 a good data
analyst?" in this work and aim to answer it by conducting head-to-head
comparative studies. In detail, we regard GPT-4 as a data analyst to perform
end-to-end data analysis with databases from a wide range of domains. We
propose a framework to tackle the problems by carefully designing the prompts
for GPT-4 to conduct experiments. We also design several task-specific
evaluation metrics to systematically compare the performance between several
professional human data analysts and GPT-4. Experimental results show that
GPT-4 can achieve comparable performance to humans. We also provide in-depth
discussions about our results to shed light on further studies before reaching
the conclusion that GPT-4 can replace data analysts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models. (arXiv:2305.15064v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15064">
<div class="article-summary-box-inner">
<span><p>Recent large language models (LLMs) are promising for making decisions in
grounded environments. However, LLMs frequently fail in complex decision-making
tasks due to the misalignment between the pre-trained knowledge in LLMs and the
actual rules in the environment. Existing methods require either costly
gradient computation or lengthy in-context demonstrations. In this paper, we
propose AutoPlan, an approach to guide LLM-based agents to accomplish
interactive decision-making tasks. AutoPlan augments the LLM prompt with a
task-solving plan and optimizes it through iterative experience collection and
reflection. Our experiments show that AutoPlan, though using no in-context
demonstrations, achieves success rates on par with the baselines using
human-written demonstrations on ALFWorld and even outperforms them by 8% on
HotpotQA. The code is available at https://github.com/owaski/AutoPlan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15074">
<div class="article-summary-box-inner">
<span><p>The performance of large language models (LLMs) on existing reasoning
benchmarks has significantly improved over the past years. In response, we
present JEEBench, a considerably more challenging benchmark dataset for
evaluating the problem solving abilities of LLMs. We curate 515 challenging
pre-engineering mathematics, physics and chemistry problems from the highly
competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep
in-domain knowledge is essential for solving problems in this benchmark. Our
evaluation on various open-source and proprietary models reveals that the
highest performance, even after using techniques like self-consistency,
self-refinement and chain-of-thought prompting, is less than 40%. The typical
failure modes of GPT-4, the best model, are errors in algebraic manipulation,
difficulty in grounding abstract concepts into mathematical equations
accurately and failure in retrieving relevant domain-specific concepts. We also
observe that by mere prompting, GPT-4 is unable to assess risk introduced by
negative marking for incorrect answers. For this, we develop a post-hoc
confidence-thresholding method over self-consistency, which enables effective
response selection. We hope that our challenging benchmark will guide future
re-search in problem-solving using LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Learning Online Adaptation of Language Models. (arXiv:2305.15076v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15076">
<div class="article-summary-box-inner">
<span><p>Large language models encode impressively broad world knowledge in their
parameters. However, the knowledge in static language models falls out of date,
limiting the model's effective "shelf life." While online fine-tuning can
reduce this degradation, we find that naively fine-tuning on a stream of
documents leads to a low level of information uptake. We hypothesize that
online fine-tuning does not sufficiently attend to important information. That
is, the gradient signal from important tokens representing factual information
is drowned out by the gradient from inherently noisy tokens, suggesting that a
dynamic, context-aware learning rate may be beneficial. We therefore propose
learning which tokens to upweight. We meta-train a small, autoregressive model
to reweight the language modeling loss for each token during online
fine-tuning, with the objective of maximizing the out-of-date base
question-answering model's ability to answer questions about a document after a
single weighted gradient step. We call this approach Context-aware Meta-learned
Loss Scaling (CaMeLS). Across three different distributions of documents, our
experiments find that CaMeLS provides substantially improved information uptake
on streams of thousands of documents compared with standard fine-tuning and
baseline heuristics for reweighting token losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. (arXiv:2305.15294v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15294">
<div class="article-summary-box-inner">
<span><p>Large language models are powerful text processors and reasoners, but are
still subject to limitations including outdated knowledge and hallucinations,
which necessitates connecting them to the world. Retrieval-augmented large
language models have raised extensive attention for grounding model generation
on external knowledge. However, retrievers struggle to capture relevance,
especially for queries with complex information needs. Recent work has proposed
to improve relevance modeling by having large language models actively involved
in retrieval, i.e., to improve retrieval with generation. In this paper, we
show that strong performance can be achieved by a method we call Iter-RetGen,
which synergizes retrieval and generation in an iterative manner. A model
output shows what might be needed to finish a task, and thus provides an
informative context for retrieving more relevant knowledge which in turn helps
generate a better output in the next iteration. Compared with recent work which
interleaves retrieval with generation when producing an output, Iter-RetGen
processes all retrieved knowledge as a whole and largely preserves the
flexibility in generation without structural constraints. We evaluate
Iter-RetGen on multi-hop question answering, fact verification, and commonsense
reasoning, and show that it can flexibly leverage parametric knowledge and
non-parametric knowledge, and is superior to or competitive with
state-of-the-art retrieval-augmented baselines while causing fewer overheads of
retrieval and generation. We can further improve performance via
generation-augmented retrieval adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16340">
<div class="article-summary-box-inner">
<span><p>Transformers have shown dominant performance across a range of domains
including language and vision. However, their computational cost grows
quadratically with the sequence length, making their usage prohibitive for
resource-constrained applications. To counter this, our approach is to divide
the whole sequence into segments and apply attention to the individual
segments. We propose a segmented recurrent transformer (SRformer) that combines
segmented (local) attention with recurrent attention. The loss caused by
reducing the attention window length is compensated by aggregating information
across segments with recurrent attention. SRformer leverages Recurrent
Accumulate-and-Fire (RAF) neurons' inherent memory to update the cumulative
product of keys and values. The segmented attention and lightweight RAF neurons
ensure the efficiency of the proposed transformer. Such an approach leads to
models with sequential processing capability at a lower computation/memory
cost. We apply the proposed method to T5 and BART transformers. The modified
models are tested on summarization datasets including CNN-dailymail, XSUM,
ArXiv, and MediaSUM. Notably, using segmented inputs of varied sizes, the
proposed model achieves $6-22\%$ higher ROUGE1 scores than a segmented
transformer and outperforms other recurrent transformer approaches.
Furthermore, compared to full attention, the proposed model reduces the
computational complexity of cross attention by around $40\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models. (arXiv:2305.16426v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16426">
<div class="article-summary-box-inner">
<span><p>Vector space models of word meaning all share the assumption that words
occurring in similar contexts have similar meanings. In such models, words that
are similar in their topical associations but differ in their logical force
tend to emerge as semantically close, creating well-known challenges for NLP
applications that involve logical reasoning. Modern pretrained language models,
such as BERT, RoBERTa and GPT-3 hold the promise of performing better on
logical tasks than classic static word embeddings. However, reports are mixed
about their success. In the current paper, we advance this discussion through a
systematic study of scalar adverbs, an under-explored class of words with
strong logical force. Using three different tasks, involving both naturalistic
social media data and constructed examples, we investigate the extent to which
BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these
common words. We ask: 1) Do the models distinguish amongst the three semantic
categories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicit
representations of full scales from maximally negative to maximally positive?
3) How do word frequency and contextual factors impact model performance? We
find that despite capturing some aspects of logical meaning, the models fall
far short of human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections. (arXiv:2305.18287v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18287">
<div class="article-summary-box-inner">
<span><p>Recently, large-scale pre-trained Vision and Language (VL) models have set a
new state-of-the-art (SOTA) in zero-shot visual classification enabling
open-vocabulary recognition of potentially unlimited set of categories defined
as simple language prompts. However, despite these great advances, the
performance of these zeroshot classifiers still falls short of the results of
dedicated (closed category set) classifiers trained with supervised fine
tuning. In this paper we show, for the first time, how to reduce this gap
without any labels and without any paired VL data, using an unlabeled image
collection and a set of texts auto-generated using a Large Language Model (LLM)
describing the categories of interest and effectively substituting labeled
visual instances of those categories. Using our label-free approach, we are
able to attain significant performance improvements over the zero-shot
performance of the base VL model and other contemporary methods and baselines
on a wide variety of datasets, demonstrating absolute improvement of up to
11.7% (3.8% on average) in the label-free setting. Moreover, despite our
approach being label-free, we observe 1.3% average gains over leading few-shot
prompting baselines that do use 5-shot supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18396">
<div class="article-summary-box-inner">
<span><p>The community explored to build private inference frameworks for
transformer-based large language models (LLMs) in a server-client setting,
where the server holds the model parameters and the client inputs its private
data (or prompt) for inference. However, these frameworks impose significant
overhead when the private inputs are forward propagated through the original
LLMs. In this paper, we show that substituting the computation- and
communication-heavy operators in the transformer architecture with
privacy-computing friendly approximations can greatly reduce the private
inference costs while incurring very minor impact on model performance.
Compared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing
friendly model inference pipeline achieves a $5\times$ acceleration in
computation and an 80% reduction in communication overhead, while retaining
nearly identical accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset. (arXiv:2306.03030v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03030">
<div class="article-summary-box-inner">
<span><p>Recent advancements in large language models (LLMs) have transformed the
field of question answering (QA). However, evaluating LLMs in the medical field
is challenging due to the lack of standardized and comprehensive datasets. To
address this gap, we introduce CMExam, sourced from the Chinese National
Medical Licensing Examination. CMExam consists of 60K+ multiple-choice
questions for standardized and objective evaluations, as well as solution
explanations for model reasoning evaluation in an open-ended manner. For
in-depth analyses of LLMs, we invited medical professionals to label five
additional question-wise annotations, including disease groups, clinical
departments, medical disciplines, areas of competency, and question difficulty
levels. Alongside the dataset, we further conducted thorough experiments with
representative LLMs and QA algorithms on CMExam. The results show that GPT-4
had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results
highlight a great disparity when compared to human accuracy, which stood at
71.6%. For explanation tasks, while LLMs could generate relevant reasoning and
demonstrate improved performance after finetuning, they fall short of a desired
standard, indicating ample room for improvement. To the best of our knowledge,
CMExam is the first Chinese medical exam dataset to provide comprehensive
medical annotations. The experiments and findings of LLM evaluation also
provide valuable insights into the challenges and potential solutions in
developing Chinese medical QA systems and LLM evaluation pipelines. The dataset
and relevant code are available at https://github.com/williamliujl/CMExam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models. (arXiv:2306.06815v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06815">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are progressively being utilized as machine
learning services and interface tools for various applications. However, the
security implications of LLMs, particularly in relation to adversarial and
Trojan attacks, remain insufficiently examined. In this paper, we propose
TrojLLM, an automatic and black-box framework to effectively generate universal
and stealthy triggers. When these triggers are incorporated into the input
data, the LLMs' outputs can be maliciously manipulated. Moreover, the framework
also supports embedding Trojans within discrete prompts, enhancing the overall
effectiveness and precision of the triggers' attacks. Specifically, we propose
a trigger discovery algorithm for generating universal triggers for various
inputs by querying victim LLM-based APIs using few-shot data samples.
Furthermore, we introduce a novel progressive Trojan poisoning algorithm
designed to generate poisoned prompts that retain efficacy and transferability
across a diverse range of models. Our experiments and results demonstrate
TrojLLM's capacity to effectively insert Trojans into text prompts in
real-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining
exceptional performance on clean test sets. Our work sheds light on the
potential security risks in current models and offers a potential defensive
approach. The source code of TrojLLM is available at
https://github.com/UCF-ML-Research/TrojLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.13089">
<div class="article-summary-box-inner">
<span><p>Molecule property prediction has gained significant attention in recent
years. The main bottleneck is the label insufficiency caused by expensive lab
experiments. In order to alleviate this issue and to better leverage textual
knowledge for tasks, this study investigates the feasibility of employing
natural language instructions to accomplish molecule-related tasks in a
zero-shot setting. We discover that existing molecule-text models perform
poorly in this setting due to inadequate treatment of instructions and limited
capacity for graphs. To overcome these issues, we propose GIMLET, which unifies
language models for both graph and text data. By adopting generalized position
embedding, our model is extended to encode both graph structures and
instruction text without additional graph encoding modules. GIMLET also
decouples encoding of the graph from tasks instructions in the attention
mechanism, enhancing the generalization of graph features across novel tasks.
We construct a dataset consisting of more than two thousand molecule tasks with
corresponding instructions derived from task descriptions. We pretrain GIMLET
on the molecule tasks along with instructions, enabling the model to transfer
effectively to a broad range of tasks. Experimental results demonstrate that
GIMLET significantly outperforms molecule-text baselines in instruction-based
zero-shot learning, even achieving closed results to supervised GNN models on
tasks such as toxcast and muv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17020">
<div class="article-summary-box-inner">
<span><p>The task of determining crime types based on criminal behavior facts has
become a very important and meaningful task in social science. But the problem
facing the field now is that the data samples themselves are unevenly
distributed, due to the nature of the crime itself. At the same time, data sets
in the judicial field are less publicly available, and it is not practical to
produce large data sets for direct training. This article proposes a new
training model to solve this problem through NLP processing methods. We first
propose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the
defects of uneven data set distribution by generating new samples. Then we use
a large open source dataset (CAIL-big) as our pretraining dataset and a small
dataset collected by ourselves for Fine-tuning, giving it good generalization
ability to unfamiliar small datasets. At the same time, we use the improved
Bert model with dynamic masking to improve the model. Experiments show that the
proposed method achieves state-of-the-art results on the present dataset. At
the same time, the effectiveness of module CFDPM is proved by experiments. This
article provides a valuable methodology contribution for classifying social
science texts such as criminal behaviors. Extensive experiments on public
benchmarks show that the proposed method achieves new state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09384">
<div class="article-summary-box-inner">
<span><p>As the popularity of voice assistants continues to surge, conversational
search has gained increased attention in Information Retrieval. However, data
sparsity issues in conversational search significantly hinder the progress of
supervised conversational search methods. Consequently, researchers are
focusing more on zero-shot conversational search approaches. Nevertheless,
existing zero-shot methods face three primary limitations: they are not
universally applicable to all retrievers, their effectiveness lacks sufficient
explainability, and they struggle to resolve common conversational ambiguities
caused by omission. To address these limitations, we introduce a novel
Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based
on previous dialogue contexts without requiring supervision from conversational
search data. Specifically, our framework utilizes language models designed for
machine reading comprehension tasks to explicitly resolve two common
ambiguities: coreference and omission, in raw queries. In comparison to
existing zero-shot methods, our approach is universally applicable to any
retriever without additional adaptation or indexing. It also provides greater
explainability and effectively enhances query intent understanding because
ambiguities are explicitly and proactively resolved. Through extensive
experiments on four TREC conversational datasets, we demonstrate the
effectiveness of our method, which consistently outperforms state-of-the-art
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topics, Authors, and Networks in Large Language Model Research: Trends from a Survey of 17K arXiv Papers. (arXiv:2307.10700v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10700">
<div class="article-summary-box-inner">
<span><p>Large language model (LLM) research is dramatically impacting society, making
it essential to understand the topics and values it prioritizes, the authors
and institutions driving it, and its networks of collaboration. Due to the
recent growth of the field, many of these fundamental attributes lack
systematic description. We gather, annotate, and analyze a new dataset of
16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. We
show that LLM research increasingly focuses on societal impacts: the Computers
and Society sub-arXiv has seen 20x growth in its proportion of LLM-related
papers in 2023. This change is driven in part by an influx of new authors: a
majority of 2023 papers are first-authored by researchers who have not
previously written an LLM-related paper, and these papers focus particularly on
applications and societal considerations. While a handful of companies hold
outsize influence, academia publishes a much larger fraction of papers than
industry overall, and this gap widens in 2023. LLM research is also being
shaped by social dynamics: there are gender and academic/industry differences
in the topics authors prioritize, and a stark U.S./China schism in the
collaboration network. Overall, our analysis documents how LLM research both
shapes and is shaped by society, attesting to the necessity of sociotechnical
lenses; we discuss implications for researchers and policymakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes. (arXiv:2307.15455v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15455">
<div class="article-summary-box-inner">
<span><p>Query auto-completion (QAC) aims to suggest plausible completions for a given
query prefix. Traditionally, QAC systems have leveraged tries curated from
historical query logs to suggest most popular completions. In this context,
there are two specific scenarios that are difficult to handle for any QAC
system: short prefixes (which are inherently ambiguous) and unseen prefixes.
Recently, personalized Natural Language Generation (NLG) models have been
proposed to leverage previous session queries as context for addressing these
two challenges. However, such NLG models suffer from two drawbacks: (1) some of
the previous session queries could be noisy and irrelevant to the user intent
for the current prefix, and (2) NLG models cannot directly incorporate
historical query popularity. This motivates us to propose a novel NLG model for
QAC, Trie-NLG, which jointly leverages popularity signals from trie and
personalization signals from previous session queries. We train the Trie-NLG
model by augmenting the prefix with rich context comprising of recent session
queries and top trie completions. This simple modeling approach overcomes the
limitations of trie-based and NLG-based approaches and leads to
state-of-the-art performance. We evaluate the Trie-NLG model using two large
QAC datasets. On average, our model achieves huge ~57% and ~14% boost in MRR
over the popular trie-based lookup and the strong BART-based baseline methods,
respectively. We make our code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01684">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) demonstrate remarkable performance on a variety
of natural language understanding (NLU) tasks, primarily due to their
in-context learning ability. This ability could be applied to building babylike
models, i.e. models at small scales, improving training efficiency. In this
paper, we propose a "CoThought" pipeline, which efficiently trains smaller
"baby" language models (BabyLMs) by leveraging the Chain of Thought prompting
of LLMs. Our pipeline restructures a dataset of less than 100M in size using
GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that
are comparable to the school texts for language learners. The BabyLM is then
pretrained on this restructured dataset in a RoBERTa fashion. In evaluations
across 4 benchmarks, our BabyLM outperforms the vanilla RoBERTa in 10
linguistic, NLU, and question-answering tasks by more than 3 points, showing a
superior ability to extract contextual information. These results suggest that
compact LMs pretrained on small, LLM-restructured data can better understand
tasks and achieve improved performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03415">
<div class="article-summary-box-inner">
<span><p>The challenge of low-latency speech translation has recently draw significant
interest in the research community as shown by several publications and shared
tasks. Therefore, it is essential to evaluate these different approaches in
realistic scenarios. However, currently only specific aspects of the systems
are evaluated and often it is not possible to compare different approaches.
</p>
<p>In this work, we propose the first framework to perform and evaluate the
various aspects of low-latency speech translation under realistic conditions.
The evaluation is carried out in an end-to-end fashion. This includes the
segmentation of the audio as well as the run-time of the different components.
</p>
<p>Secondly, we compare different approaches to low-latency speech translation
using this framework. We evaluate models with the option to revise the output
as well as methods with fixed output. Furthermore, we directly compare
state-of-the-art cascaded as well as end-to-end systems. Finally, the framework
allows to automatically evaluate the translation quality as well as latency and
also provides a web interface to show the low-latency model outputs to the
user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. (arXiv:2308.10848v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10848">
<div class="article-summary-box-inner">
<span><p>Autonomous agents empowered by Large Language Models (LLMs) have undergone
significant improvements, enabling them to generalize across a broad spectrum
of tasks. However, in real-world scenarios, cooperation among individuals is
often required to enhance the efficiency and effectiveness of task
accomplishment. Hence, inspired by human group dynamics, we propose a
multi-agent framework \framework that can collaboratively and dynamically
adjust its composition as a greater-than-the-sum-of-its-parts system. Our
experiments demonstrate that \framework framework can effectively deploy
multi-agent groups that outperform a single agent. Furthermore, we delve into
the emergence of social behaviors among individual agents within a group during
collaborative task accomplishment. In view of these behaviors, we discuss some
possible strategies to leverage positive ones and mitigate negative ones for
improving the collaborative potential of multi-agent groups. Our codes for
\framework will soon be released at
\url{https://github.com/OpenBMB/AgentVerse}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13137">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have revolutionized natural language processing
tasks. However, their practical deployment is hindered by their immense memory
and computation requirements. Although recent post-training quantization (PTQ)
methods are effective in reducing memory footprint and improving the
computational efficiency of LLM, they hand-craft quantization parameters, which
leads to low performance and fails to deal with extremely low-bit quantization.
To tackle this issue, we introduce an Omnidirectionally calibrated Quantization
(OmniQuant) technique for LLMs, which achieves good performance in diverse
quantization settings while maintaining the computational efficiency of PTQ by
efficiently optimizing various quantization parameters. OmniQuant comprises two
innovative components including Learnable Weight Clipping (LWC) and Learnable
Equivalent Transformation (LET). LWC modulates the extreme values of weights by
optimizing the clipping threshold. Meanwhile, LET tackles activation outliers
by shifting the challenge of quantization from activations to weights through a
learnable equivalent transformation. Operating within a differentiable
framework using block-wise error minimization, OmniQuant can optimize the
quantization process efficiently for both weight-only and weight-activation
quantization. For instance, the LLaMA-2 model family with the size of 7-70B can
be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using
128 samples. Extensive experiments validate OmniQuant's superior performance
across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16,
and W2A16. Additionally, OmniQuant demonstrates effectiveness in
instruction-tuned models and delivers notable improvements in inference speed
and memory reduction on real devices. Codes and models are available at
\url{https://github.com/OpenGVLab/OmniQuant}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Wide Feedforward is All You Need. (arXiv:2309.01826v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01826">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture has two main non-embedding components: Attention
and the Feed Forward Network (FFN). Attention captures interdependencies
between words regardless of their position, while the FFN non-linearly
transforms each input token independently. In this work we explore the role of
the FFN, and find that despite taking up a significant fraction of the model's
parameters, it is highly redundant. Concretely, we are able to substantially
reduce the number of parameters with only a modest drop in accuracy by removing
the FFN on the decoder layers and sharing a single FFN across the encoder.
Finally we scale this architecture back to its original size by increasing the
hidden dimension of the shared FFN, achieving substantial gains in both
accuracy and latency with respect to the original Transformer Big.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL. (arXiv:2309.06553v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06553">
<div class="article-summary-box-inner">
<span><p>In this study, we aim to enhance the arithmetic reasoning ability of Large
Language Models (LLMs) through zero-shot prompt optimization. We identify a
previously overlooked objective of query dependency in such optimization and
elucidate two ensuing challenges that impede the successful and economical
design of prompt optimization techniques. One primary issue is the absence of
an effective method to evaluate prompts during inference when the golden answer
is unavailable. Concurrently, learning via interactions with the LLMs to
navigate the expansive natural language prompting space proves to be
resource-intensive. To address this, we introduce Prompt-OIRL, which harnesses
offline inverse reinforcement learning to draw insights from offline prompting
demonstration data. Such data exists as by-products when diverse prompts are
benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent
prompt optimization objective is achieved by first learning an offline reward
model. This model can evaluate any query-prompt pairs without accessing LLMs.
Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.
Our experimental evaluations across various LLM scales and arithmetic reasoning
datasets underscore both the efficacy and economic viability of the proposed
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07430">
<div class="article-summary-box-inner">
<span><p>Sifting through vast textual data and summarizing key information imposes a
substantial burden on how clinicians allocate their time. Although large
language models (LLMs) have shown immense promise in natural language
processing (NLP) tasks, their efficacy on a diverse range of clinical
summarization tasks has not yet been rigorously demonstrated. In this work, we
apply domain adaptation methods to eight LLMs, spanning six datasets and four
distinct clinical summarization tasks: radiology reports, patient questions,
progress notes, and doctor-patient dialogue. Our thorough quantitative
assessment reveals trade-offs between models and adaptation methods in addition
to instances where recent advances in LLMs may not improve results. Further, in
a clinical reader study with ten physicians, we show that summaries from our
best-adapted LLMs are preferable to human summaries in terms of completeness
and correctness. Our ensuing qualitative analysis highlights challenges faced
by both LLMs and human experts. Lastly, we correlate traditional quantitative
NLP metrics with reader study scores to enhance our understanding of how these
metrics align with physician preferences. Our research marks the first evidence
of LLMs outperforming human experts in clinical text summarization across
multiple tasks. This implies that integrating LLMs into clinical workflows
could alleviate documentation burden, empowering clinicians to focus more on
personalized patient care and the inherently human aspects of medicine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11668">
<div class="article-summary-box-inner">
<span><p>Resolving semantic ambiguity has long been recognised as a central challenge
in the field of Machine Translation. Recent work on benchmarking translation
performance on ambiguous sentences has exposed the limitations of conventional
Neural Machine Translation (NMT) systems, which fail to handle many such cases.
Large language models (LLMs) have emerged as a promising alternative,
demonstrating comparable performance to traditional NMT models while
introducing new paradigms for controlling the target outputs. In this paper, we
study the capabilities of LLMs to translate "ambiguous sentences" - i.e. those
containing highly polysemous words and/or rare word senses. We also propose two
ways to improve their disambiguation capabilities, through a) in-context
learning and b) fine-tuning on carefully curated ambiguous datasets.
Experiments show that our methods can match or outperform state-of-the-art
systems such as DeepL and NLLB in four out of five language directions. Our
research provides valuable insights into effectively adapting LLMs to become
better disambiguators during Machine Translation. We release our curated
disambiguation corpora and resources at
https://data.statmt.org/ambiguous-europarl.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Tuned Embedding Classification for Multi-Label Industry Sector Allocation. (arXiv:2309.12075v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12075">
<div class="article-summary-box-inner">
<span><p>Prompt Tuning is emerging as a scalable and cost-effective method to
fine-tune Pretrained Language Models (PLMs), which are often referred to as
Large Language Models (LLMs). This study benchmarks the performance and
computational efficiency of Prompt Tuning and baselines for multi-label text
classification. This is applied to the challenging task of classifying
companies into an investment firm's proprietary industry taxonomy, supporting
their thematic investment strategy. Text-to-text classification is frequently
reported to outperform task-specific classification heads, but has several
limitations when applied to a multi-label classification problem where each
label consists of multiple tokens: (a) Generated labels may not match any label
in the label taxonomy; (b) The fine-tuning process lacks permutation invariance
and is sensitive to the order of the provided labels; (c) The model provides
binary decisions rather than appropriate confidence scores. Limitation (a) is
addressed by applying constrained decoding using Trie Search, which slightly
improves classification performance. All limitations (a), (b), and (c) are
addressed by replacing the PLM's language head with a classification head,
which is referred to as Prompt Tuned Embedding Classification (PTEC). This
improves performance significantly, while also reducing computational costs
during inference. In our industrial application, the training data is skewed
towards well-known companies. We confirm that the model's performance is
consistent across both well-known and less-known companies. Our overall results
indicate the continuing need to adapt state-of-the-art methods to
domain-specific tasks, even in the era of PLMs with strong generalization
abilities. We release our codebase and a benchmarking dataset at
https://github.com/EQTPartners/PTEC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.16583">
<div class="article-summary-box-inner">
<span><p>With the rapid advancement of large language models (LLMs), there is a
pressing need for a comprehensive evaluation suite to assess their capabilities
and limitations. Existing LLM leaderboards often reference scores reported in
other papers without consistent settings and prompts, which may inadvertently
encourage cherry-picking favored settings and prompts for better results. In
this work, we introduce GPT-Fathom, an open-source and reproducible LLM
evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+
leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across
7 capability categories, all under aligned settings. Our retrospective study on
OpenAI's earlier models offers valuable insights into the evolutionary path
from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3
progressively improves to GPT-4, including technical details like whether
adding code data improves LLM's reasoning capability, which aspects of LLM
capability can be improved by SFT and RLHF, how much is the alignment tax, etc.
Our analysis sheds light on many of these questions, aiming to improve the
transparency of advanced LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Dialogue Management: Quality Datasets vs Models. (arXiv:2310.01339v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01339">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems (TODS) have become crucial for users to
interact with machines and computers using natural language. One of its key
components is the dialogue manager, which guides the conversation towards a
good goal for the user by providing the best possible response. Previous works
have proposed rule-based systems (RBS), reinforcement learning (RL), and
supervised learning (SL) as solutions for the correct dialogue management; in
other words, select the best response given input by the user. However, this
work argues that the leading cause of DMs not achieving maximum performance
resides in the quality of the datasets rather than the models employed thus
far; this means that dataset errors, like mislabeling, originate a large
percentage of failures in dialogue management. We studied the main errors in
the most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate this
hypothesis. To do this, we have designed a synthetic dialogue generator to
fully control the amount and type of errors introduced in the dataset. Using
this generator, we demonstrated that errors in the datasets contribute
proportionally to the performance of the models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02374">
<div class="article-summary-box-inner">
<span><p>Conversational Health Agents (CHAs) are interactive systems designed to
enhance personal healthcare services by engaging in empathetic conversations
and processing multimodal data. While current CHAs, especially those utilizing
Large Language Models (LLMs), primarily focus on conversation, they often need
more comprehensive agent capabilities. This limitation includes accessing
personal user health data from wearables, ubiquitous data collection sources,
and electronic health records, integrating the latest published health
insights, and connecting with established multimodal data analysis tools. In
this paper, we propose an LLM-powered framework to empower CHAs to generate a
personalized response for users' healthcare queries. This framework provides
critical thinking, knowledge acquisition, and problem-solving abilities by
integrating healthcare data sources, enabling multilingual and multimodal
conversations, and interacting with various user data analysis tools. We
illustrate the framework's proficiency in handling complex healthcare tasks via
a case study on stress level estimation, showcasing the agent's cognitive and
operational capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03368">
<div class="article-summary-box-inner">
<span><p>In this paper, we establish a benchmark named HalluQA (Chinese Hallucination
Question-Answering) to measure the hallucination phenomenon in Chinese large
language models. HalluQA contains 450 meticulously designed adversarial
questions, spanning multiple domains, and takes into account Chinese historical
culture, customs, and social phenomena. During the construction of HalluQA, we
consider two types of hallucinations: imitative falsehoods and factual errors,
and we construct adversarial samples based on GLM-130B and ChatGPT. For
evaluation, we design an automated evaluation method using GPT-4 to judge
whether a model output is hallucinated. We conduct extensive experiments on 24
large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk
and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than
50%. This indicates that HalluQA is highly challenging. We analyze the primary
types of hallucinations in different types of models and their causes.
Additionally, we discuss which types of hallucinations should be prioritized
for different types of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04445">
<div class="article-summary-box-inner">
<span><p>It has been shown that Large Language Model (LLM) alignments can be
circumvented by appending specially crafted attack suffixes with harmful
queries to elicit harmful responses. To conduct attacks against private target
models whose characterization is unknown, public models can be used as proxies
to fashion the attack, with successful attacks being transferred from public
proxies to private target models. The success rate of attack depends on how
closely the proxy model approximates the private model. We hypothesize that for
attacks to be transferrable, it is sufficient if the proxy can approximate the
target model in the neighborhood of the harmful query. Therefore, in this
paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning
proxy models on similar queries that lie in the lexico-semantic neighborhood of
harmful queries to decrease the divergence between the proxy and target models.
First, we demonstrate three approaches to prompt private target models to
obtain similar queries given harmful queries. Next, we obtain data for local
fine-tuning by eliciting responses from target models for the generated similar
queries. Then, we optimize attack suffixes to generate attack prompts and
evaluate the impact of our local fine-tuning on the attack's success rate.
Experiments show that local fine-tuning of proxy models improves attack
transferability and increases attack success rate by $39\%$, $7\%$, and $0.5\%$
(absolute) on target models ChatGPT, GPT-4, and Claude respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU. (arXiv:2310.04928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04928">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) are often pre-trained on large-scale
multilingual texts, their reasoning abilities and real-world knowledge are
mainly evaluated based on English datasets. Assessing LLM capabilities beyond
English is increasingly vital but hindered due to the lack of suitable
datasets. In this work, we introduce IndoMMLU, the first multi-task language
understanding benchmark for Indonesian culture and languages, which consists of
questions from primary school to university entrance exams in Indonesia. By
employing professional teachers, we obtain 14,981 questions across 64 tasks and
education levels, with 46% of the questions focusing on assessing proficiency
in the Indonesian language and knowledge of nine local languages and cultures
in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass
the Indonesian primary school level, with limited knowledge of local Indonesian
languages and culture. Other smaller models such as BLOOMZ and Falcon perform
at even lower levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guideline Learning for In-context Information Extraction. (arXiv:2310.05066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05066">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can perform a new task by merely conditioning on
task instructions and a few input-output examples, without optimizing any
parameters. This is called In-Context Learning (ICL). In-context Information
Extraction (IE) has recently garnered attention in the research community.
However, the performance of In-context IE generally lags behind the
state-of-the-art supervised expert models. We highlight a key reason for this
shortfall: underspecified task description. The limited-length context
struggles to thoroughly express the intricate IE task instructions and various
edge cases, leading to misalignment in task comprehension with humans. In this
paper, we propose a Guideline Learning (GL) framework for In-context IE which
reflectively learns and follows guidelines. During the learning phrase, GL
automatically synthesizes a set of guidelines based on a few error cases, and
during inference, GL retrieves helpful guidelines for better ICL. Moreover, we
propose a self-consistency-based active learning method to enhance the
efficiency of GL. Experiments on event extraction and relation extraction show
that GL can significantly improve the performance of in-context IE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05074">
<div class="article-summary-box-inner">
<span><p>Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the
reasoning capabilities of Large Language Models (LLMs) with at least 100
billion parameters. However, it is ineffective or even detrimental when applied
to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion
parameters. To address this limitation, we introduce Dialogue-guided
Chain-of-Thought (DialCoT) which employs a dialogue format to generate
intermediate reasoning steps, guiding the model toward the final answer.
Additionally, we optimize the model's reasoning path selection using the
Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning
capabilities. Our method offers several advantages compared to previous
approaches. Firstly, we transform the process of solving complex reasoning
questions by breaking them down into a series of simpler sub-questions,
significantly reducing the task difficulty and making it more suitable for
SLMs. Secondly, we optimize the model's reasoning path selection through the
PPO algorithm. We conduct comprehensive experiments on four arithmetic
reasoning datasets, demonstrating that our method achieves significant
performance improvements compared to state-of-the-art competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05280">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models empower them to follow freeform
instructions, including imitating generic or specific demographic personas in
conversations. We define generic personas to represent demographic groups, such
as "an Asian person", whereas specific personas may take the form of specific
popular Asian names like "Yumi". While the adoption of personas enriches user
experiences by making dialogue systems more engaging and approachable, it also
casts a shadow of potential risk by exacerbating social biases within model
responses, thereby causing societal harm through interactions with users. In
this paper, we systematically study "persona biases", which we define to be the
sensitivity of dialogue models' harmful behaviors contingent upon the personas
they adopt. We categorize persona biases into biases in harmful expression and
harmful agreement, and establish a comprehensive evaluation framework to
measure persona biases in five aspects: Offensiveness, Toxic Continuation,
Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to
investigate persona biases by experimenting with UNIVERSALPERSONA, a
systematically constructed persona dataset encompassing various types of both
generic and specific model personas. Through benchmarking on four different
models -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers
significant persona biases in dialogue systems. Our findings also underscore
the pressing need to revisit the use of personas in dialogue agents to ensure
safe application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Long-form Text Generation Efficacy with Task-adaptive Tokenization. (arXiv:2310.05317v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05317">
<div class="article-summary-box-inner">
<span><p>We propose task-adaptive tokenization as a way to adapt the generation
pipeline to the specifics of a downstream task and enhance long-form generation
in mental health. Inspired by insights from cognitive science, our
task-adaptive tokenizer samples variable segmentations from multiple outcomes,
with sampling probabilities optimized based on task-specific data. We introduce
a strategy for building a specialized vocabulary and introduce a vocabulary
merging protocol that allows for the integration of task-specific tokens into
the pre-trained model's tokenization step. Through extensive experiments on
psychological question-answering tasks in both Chinese and English, we find
that our task-adaptive tokenization approach brings a significant improvement
in generation performance while using up to 60% fewer tokens. Preliminary
experiments point to promising results when using our tokenization approach
with very large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Establishing Trustworthiness: Rethinking Tasks and Model Evaluation. (arXiv:2310.05442v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05442">
<div class="article-summary-box-inner">
<span><p>Language understanding is a multi-faceted cognitive capability, which the
Natural Language Processing (NLP) community has striven to model
computationally for decades. Traditionally, facets of linguistic intelligence
have been compartmentalized into tasks with specialized model architectures and
corresponding evaluation protocols. With the advent of large language models
(LLMs) the community has witnessed a dramatic shift towards general purpose,
task-agnostic approaches powered by generative models. As a consequence, the
traditional compartmentalized notion of language tasks is breaking down,
followed by an increasing challenge for evaluation and analysis. At the same
time, LLMs are being deployed in more real-world scenarios, including
previously unforeseen zero-shot setups, increasing the need for trustworthy and
reliable systems. Therefore, we argue that it is time to rethink what
constitutes tasks and model evaluation in NLP, and pursue a more holistic view
on language, placing trustworthiness at the center. Towards this goal, we
review existing compartmentalized approaches for understanding the origins of a
model's functional capacity, and provide recommendations for more multi-faceted
evaluation protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations. (arXiv:2310.05592v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05592">
<div class="article-summary-box-inner">
<span><p>While recently developed NLP explainability methods let us open the black box
in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is
an interactive tool offering a conversational interface. Such a dialogue system
can help users explore datasets and models with explanations in a
contextualized manner, e.g. via clarification or follow-up questions, and
through a natural language interface. We adapt the conversational explanation
framework TalkToModel (Slack et al., 2022) to the NLP domain, add new
NLP-specific operations such as free-text rationalization, and illustrate its
generalizability on three NLP tasks (dialogue act classification, question
answering, hate speech detection). To recognize user queries for explanations,
we evaluate fine-tuned and few-shot prompting models and implement a novel
Adapter-based approach. We then conduct two user studies on (1) the perceived
correctness and helpfulness of the dialogues, and (2) the simulatability, i.e.
how objectively helpful dialogical explanations are for humans in figuring out
the model's predicted label when it's not shown. We found rationalization and
feature attribution were helpful in explaining the model behavior. Moreover,
users could more reliably predict the model outcome based on an explanation
dialogue rather than one-off explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance. (arXiv:2310.05597v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05597">
<div class="article-summary-box-inner">
<span><p>While analogies are a common way to evaluate word embeddings in NLP, it is
also of interest to investigate whether or not analogical reasoning is a task
in itself that can be learned. In this paper, we test several ways to learn
basic analogical reasoning, specifically focusing on analogies that are more
typical of what is used to evaluate analogical reasoning in humans than those
in commonly used NLP benchmarks. Our experiments find that models are able to
learn analogical reasoning, even with a small amount of data. We additionally
compare our models to a dataset with a human baseline, and find that after
training, models approach human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05703">
<div class="article-summary-box-inner">
<span><p>Despite the success of Siamese encoder models such as sentence transformers
(ST), little is known about the aspects of inputs they pay attention to. A
barrier is that their predictions cannot be attributed to individual features,
as they compare two inputs rather than processing a single one. This paper
derives a local attribution method for Siamese encoders by generalizing the
principle of integrated gradients to models with multiple inputs. The solution
takes the form of feature-pair attributions, and can be reduced to a
token-token matrix for STs. Our method involves the introduction of integrated
Jacobians and inherits the advantageous formal properties of integrated
gradients: it accounts for the model's full computation graph and is guaranteed
to converge to the actual prediction. A pilot study shows that in an ST few
token-pairs can often explain large fractions of predictions, and it focuses on
nouns and verbs. For accurate predictions, it however needs to attend to the
majority of tokens and parts of speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models. (arXiv:2310.06374v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06374">
<div class="article-summary-box-inner">
<span><p>Keyphrase Generation (KPG) is a longstanding task in NLP with widespread
applications. The advent of sequence-to-sequence (seq2seq) pre-trained language
models (PLMs) has ushered in a transformative era for KPG, yielding promising
performance improvements. However, many design decisions remain unexplored and
are often made arbitrarily. This paper undertakes a systematic analysis of the
influence of model selection and decoding strategies on PLM-based KPG. We begin
by elucidating why seq2seq PLMs are apt for KPG, anchored by an
attention-driven hypothesis. We then establish that conventional wisdom for
selecting seq2seq PLMs lacks depth: (1) merely increasing model size or
performing task-specific adaptation is not parameter-efficient; (2) although
combining in-domain pre-training with task adaptation benefits KPG, it does
partially hinder generalization. Regarding decoding, we demonstrate that while
greedy search achieves strong F1 scores, it lags in recall compared with
sampling-based methods. Based on these insights, we propose DeSel, a
likelihood-based decode-select algorithm for seq2seq PLMs. DeSel improves
greedy search by an average of 4.7% semantic F1 across five datasets. Our
collective findings pave the way for deeper future investigations into
PLM-based KPG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06404">
<div class="article-summary-box-inner">
<span><p>A common practice in knowledge-grounded dialogue generation is to explicitly
utilize intermediate steps (e.g., web-search, memory retrieval) with modular
approaches. However, data for such steps are often inaccessible compared to
those of dialogue responses as they are unobservable in an ordinary dialogue.
To fill in the absence of these data, we develop a self-improving method to
improve the generative performances of intermediate steps without the ground
truth data. In particular, we propose a novel bootstrapping scheme with a
guided prompt and a modified loss function to enhance the diversity of
appropriate self-generated responses. Through experiments on various benchmark
datasets, we empirically demonstrate that our method successfully leverages a
self-improving mechanism in generating intermediate and final responses and
improves the performances on the task of knowledge-grounded dialogue
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Humans and language models diverge when predicting repeating text. (arXiv:2310.06408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06408">
<div class="article-summary-box-inner">
<span><p>Language models that are trained on the next-word prediction task have been
shown to accurately model human behavior in word prediction and reading speed.
In contrast with these findings, we present a scenario in which the performance
of humans and LMs diverges. We collected a dataset of human next-word
predictions for five stimuli that are formed by repeating spans of text. Human
and GPT-2 LM predictions are strongly aligned in the first presentation of a
text span, but their performance quickly diverges when memory (or in-context
learning) begins to play a role. We traced the cause of this divergence to
specific attention heads in a middle layer. Adding a power-law recency bias to
these attention heads yielded a model that performs much more similarly to
humans. We hope that this scenario will spur future work in bringing LMs closer
to human behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06918">
<div class="article-summary-box-inner">
<span><p>The recent success of SimCSE has greatly advanced state-of-the-art sentence
representations. However, the original formulation of SimCSE does not fully
exploit the potential of hard negative samples in contrastive learning. This
study introduces an unsupervised contrastive learning framework that combines
SimCSE with hard negative mining, aiming to enhance the quality of sentence
embeddings. The proposed focal-InfoNCE function introduces self-paced
modulation terms in the contrastive objective, downweighting the loss
associated with easy negatives and encouraging the model focusing on hard
negatives. Experimentation on various STS benchmarks shows that our method
improves sentence embeddings in terms of Spearman's correlation and
representation alignment and uniformity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting. (arXiv:2310.07081v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07081">
<div class="article-summary-box-inner">
<span><p>Idioms are common in everyday language, but often pose a challenge to
translators because their meanings do not follow from the meanings of their
parts. Despite significant advances, machine translation systems still struggle
to translate idiomatic expressions. We provide a simple characterization of
idiomatic translation and related issues. This allows us to conduct a synthetic
experiment revealing a tipping point at which transformer-based machine
translation models correctly default to idiomatic translations. To expand
multilingual resources, we compile a dataset of ~4k natural sentences
containing idiomatic expressions in French, Finnish, and Japanese. To improve
translation of natural idioms, we introduce two straightforward yet effective
techniques: the strategic upweighting of training loss on potentially idiomatic
sentences, and using retrieval-augmented models. This not only improves the
accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in
absolute accuracy, but also holds potential benefits for non-idiomatic
sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction. (arXiv:2310.07155v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07155">
<div class="article-summary-box-inner">
<span><p>Social media has become a major driver of social change, by facilitating the
formation of online social movements. Automatically understanding the
perspectives driving the movement and the voices opposing it, is a challenging
task as annotated data is difficult to obtain. We propose a weakly supervised
graph-based approach that explicitly models perspectives in
#BackLivesMatter-related tweets. Our proposed approach utilizes a
social-linguistic representation of the data. We convert the text to a graph by
breaking it into structured elements and connect it with the social network of
authors, then structured prediction is done over the elements for identifying
perspectives. Our approach uses a small seed set of labeled examples. We
experiment with large language models for generating artificial training
examples, compare them to manual annotation, and find that it achieves
comparable performance. We perform quantitative and qualitative analyses using
a human-annotated test set. Our model outperforms multitask baselines by a
large margin, successfully characterizing the perspectives supporting and
opposing #BLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models. (arXiv:2310.07611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07611">
<div class="article-summary-box-inner">
<span><p>The dominance of proprietary LLMs has led to restricted access and raised
information privacy concerns. High-performing open-source alternatives are
crucial for information-sensitive and high-volume applications but often lag
behind in performance. To address this gap, we propose (1) A untargeted variant
of iterative self-critique and self-refinement devoid of external influence.
(2) A novel ranking metric - Performance, Refinement, and Inference Cost Score
(PeRFICS) - to find the optimal model for a given task considering refined
performance and cost. Our experiments show that SoTA open source models of
varying sizes from 7B - 65B, on average, improve 8.2% from their baseline
performance. Strikingly, even models with extremely small memory footprints,
such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39%
improvement in high-creativity, open ended tasks on the Vicuna benchmark.
Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement.
This work has profound implications for resource-constrained and
information-sensitive environments seeking to leverage LLMs without incurring
prohibitive costs, compromising on performance and privacy. The domain-agnostic
self-refinement process coupled with our novel ranking metric facilitates
informed decision-making in model selection, thereby reducing costs and
democratizing access to high-performing language models, as evidenced by case
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08130">
<div class="article-summary-box-inner">
<span><p>General-purpose text decoding approaches are usually adopted for dialogue
response generation. Although the quality of the generated responses can be
improved with dialogue-specific encoding methods, conversational decoding
methods are still under-explored. Inspired by \citet{wu2023learning} that a
good dialogue feature space should follow the rules of locality and isotropy,
we present a fine-grained conversational decoding method, termed
\textit{isotropic and proximal search (IPS)}. Our method is designed to
generate the semantic-concentrated response, while still maintaining
informativeness and discrimination against the context. Experiments show that
our approach outperforms existing decoding strategies in the dialogue field
across both automatic and human evaluation metrics. More in-depth analyses
further confirm the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation. (arXiv:2310.08395v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08395">
<div class="article-summary-box-inner">
<span><p>The task of Question Generation over Knowledge Bases (KBQG) aims to convert a
logical form into a natural language question. For the sake of expensive cost
of large-scale question annotation, the methods of KBQG under low-resource
scenarios urgently need to be developed. However, current methods heavily rely
on annotated data for fine-tuning, which is not well-suited for few-shot
question generation. The emergence of Large Language Models (LLMs) has shown
their impressive generalization ability in few-shot tasks. Inspired by
Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for
reasoning, we formulate KBQG task as a reasoning problem, where the generation
of a complete question is splitted into a series of sub-question generation.
Our proposed prompting method KQG-CoT first retrieves supportive logical forms
from the unlabeled data pool taking account of the characteristics of the
logical form. Then, we write a prompt to explicit the reasoning chain of
generating complicated questions based on the selected demonstrations. To
further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the
logical forms by their complexity. We conduct extensive experiments over three
public KBQG datasets. The results demonstrate that our prompting method
consistently outperforms other prompting baselines on the evaluated datasets.
Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of
the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4,
METEOR, and ROUGE-L, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08659">
<div class="article-summary-box-inner">
<span><p>Quantization is an indispensable technique for serving Large Language Models
(LLMs) and has recently found its way into LoRA fine-tuning. In this work we
focus on the scenario where quantization and LoRA fine-tuning are applied
together on a pre-trained model. In such cases it is common to observe a
consistent gap in the performance on downstream tasks between full fine-tuning
and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ
(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that
simultaneously quantizes an LLM and finds a proper low-rank initialization for
LoRA fine-tuning. Such an initialization alleviates the discrepancy between the
quantized and full-precision model and significantly improves the
generalization in downstream tasks. We evaluate our method on natural language
understanding, question answering, summarization, and natural language
generation tasks. Experiments show that our method is highly effective and
outperforms existing quantization methods, especially in the challenging 2-bit
and 2/4-bit mixed precision regimes. We will release our code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Zero-Shot Language Agent for Computer Control with Structured Reflection. (arXiv:2310.08740v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08740">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown increasing capacity at planning and
executing a high-level goal in a live computer environment (e.g. MiniWoB++). To
perform a task, recent works often require a model to learn from trace examples
of the task via either supervised learning or few/many-shot prompting. Without
these trace examples, it remains a challenge how an agent can autonomously
learn and improve its control on a computer, which limits the ability of an
agent to perform a new task. We approach this problem with a zero-shot agent
that requires no given expert traces. Our agent plans for executable actions on
a partially observed environment, and iteratively progresses a task by
identifying and learning from its mistakes via self-reflection and structured
thought management. On the easy tasks of MiniWoB++, we show that our zero-shot
agent often outperforms recent SoTAs, with more efficient reasoning. For tasks
with more complexity, our reflective agent performs on par with prior best
models, even though previous works had the advantages of accessing expert
traces or additional screen information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents. (arXiv:2310.09343v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09343">
<div class="article-summary-box-inner">
<span><p>Human-like chatbots necessitate the use of commonsense reasoning in order to
effectively comprehend and respond to implicit information present within
conversations. Achieving such coherence and informativeness in responses,
however, is a non-trivial task. Even for large language models (LLMs), the task
of identifying and aggregating key evidence within a single hop presents a
substantial challenge. This complexity arises because such evidence is
scattered across multiple turns in a conversation, thus necessitating
integration over multiple hops. Hence, our focus is to facilitate such
multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought
(CoT) reasoning. To this end, we propose a knowledge distillation framework
that leverages LLMs as unreliable teachers and selectively distills consistent
and helpful rationales via alignment filters. We further present DOCTOR, a
DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for
response generation. We conduct extensive experiments to show that enhancing
dialogue agents with high-quality rationales from DOCTOR significantly improves
the quality of their responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Merging Experts into One: Improving Computational Efficiency of Mixture of Experts. (arXiv:2310.09832v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09832">
<div class="article-summary-box-inner">
<span><p>Scaling the size of language models usually leads to remarkable advancements
in NLP tasks. But it often comes with a price of growing computational cost.
Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a
small subset of parameters (e.g., one expert) for each input, its computation
escalates significantly if increasing the number of activated experts, limiting
its practical utility. Can we retain the advantages of adding more experts
without substantially increasing the computational costs? In this paper, we
first demonstrate the superiority of selecting multiple experts and then
propose a computation-efficient approach called \textbf{\texttt{Merging Experts
into One}} (MEO), which reduces the computation cost to that of a single
expert. Extensive experiments show that MEO significantly improves
computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G
(MEO). Moreover, we propose a token-level attention block that further enhances
the efficiency and performance of token-level MEO, e.g., 83.3\% (MEO) vs.
82.6\% (vanilla MoE) average score on the GLUE benchmark. Our code will be
released upon acceptance. Code will be released at:
\url{https://github.com/Shwai-He/MEO}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Learning with Iterative Demonstration Selection. (arXiv:2310.09881v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09881">
<div class="article-summary-box-inner">
<span><p>Spurred by advancements in scale, large language models (LLMs) have
demonstrated strong few-shot learning ability via in-context learning (ICL).
However, the performance of ICL has been shown to be highly sensitive to the
selection of few-shot demonstrations. Selecting the most suitable examples as
context remains an ongoing challenge and an open problem. Existing literature
has highlighted the importance of selecting examples that are diverse or
semantically similar to the test sample while ignoring the fact that the
optimal selection dimension, i.e., diversity or similarity, is task-specific.
Leveraging the merits of both dimensions, we propose Iterative Demonstration
Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT),
IDS iteratively selects examples that are diverse but still strongly correlated
with the test sample as ICL demonstrations. Specifically, IDS applies
Zero-shot-CoT to the test sample before demonstration selection. The output
reasoning path is then used to choose demonstrations that are prepended to the
test sample for inference. The generated answer is accompanied by its
corresponding reasoning path for extracting a new set of demonstrations in the
next iteration. After several iterations, IDS adopts majority voting to obtain
the final result. Through extensive experiments on tasks including commonsense
reasoning, question answering, topic classification, and sentiment analysis, we
demonstrate that IDS can consistently outperform existing ICL demonstration
selection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaLomo: Low-memory Optimization with Adaptive Learning Rate. (arXiv:2310.10195v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10195">
<div class="article-summary-box-inner">
<span><p>Large language models have achieved remarkable success, but their extensive
parameter size necessitates substantial memory for training, thereby setting a
high threshold. While the recently proposed low-memory optimization (LOMO)
reduces memory footprint, its optimization technique, akin to stochastic
gradient descent, is sensitive to hyper-parameters and exhibits suboptimal
convergence, failing to match the performance of the prevailing optimizer for
large language models, AdamW. Through empirical analysis of the Adam optimizer,
we found that, compared to momentum, the adaptive learning rate is more
critical for bridging the gap. Building on this insight, we introduce the
low-memory optimization with adaptive learning rate (AdaLomo), which offers an
adaptive learning rate for each parameter. To maintain memory efficiency, we
employ non-negative matrix factorization for the second-order moment estimation
in the optimizer state. Additionally, we suggest the use of a grouped update
normalization to stabilize convergence. Our experiments with instruction-tuning
and further pre-training demonstrate that AdaLomo achieves results on par with
AdamW, while significantly reducing memory requirements, thereby lowering the
hardware barrier to training large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10378">
<div class="article-summary-box-inner">
<span><p>Multilingual large-scale Pretrained Language Models (PLMs) have been shown to
store considerable amounts of factual knowledge, but large variations are
observed across languages. With the ultimate goal of ensuring that users with
different language backgrounds obtain consistent feedback from the same model,
we study the cross-lingual consistency (CLC) of factual knowledge in various
multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)
metric to evaluate knowledge consistency across languages independently from
accuracy. Using this metric, we conduct an in-depth analysis of the determining
factors for CLC, both at model level and at language-pair level. Among other
results, we find that increasing model size leads to higher factual probing
accuracy in most languages, but does not improve cross-lingual consistency.
Finally, we conduct a case study on CLC when new factual associations are
inserted in the PLMs via model editing. Results on a small sample of facts
inserted in English reveal a clear pattern whereby the new piece of knowledge
transfers only to languages with which English has a high RankC score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling. (arXiv:2310.10567v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10567">
<div class="article-summary-box-inner">
<span><p>Retrieval-augmented language models show promise in addressing issues like
outdated information and hallucinations in language models (LMs). However,
current research faces two main problems: 1) determining what information to
retrieve, and 2) effectively combining retrieved information during generation.
We argue that valuable retrieved information should not only be related to the
current source text but also consider the future target text, given the nature
of LMs that model future tokens. Moreover, we propose that aggregation using
latent variables derived from a compact latent space is more efficient than
utilizing explicit raw text, which is limited by context length and susceptible
to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model
built upon the variational auto-encoder (VAE). It encodes the text corpus into
a latent space, capturing current and future information from both source and
target text. Additionally, we leverage the VAE to initialize the latent space
and adopt the probabilistic form of the retrieval generation paradigm by
expanding the Gaussian prior distribution into a Gaussian mixture distribution.
Theoretical analysis provides an optimizable upper bound for RegaVAE.
Experimental results on various datasets demonstrate significant improvements
in text generation quality and hallucination removal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10638">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs'performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation. (arXiv:2310.10698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10698">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have showcased remarkable prowess in code
generation. However, automated code generation is still challenging since it
requires a high-level semantic mapping between natural language requirements
and codes. Most existing LLMs-based approaches for code generation rely on
decoder-only causal language models often treate codes merely as plain text
tokens, i.e., feeding the requirements as a prompt input, and outputing code as
flat sequence of tokens, potentially missing the rich semantic features
inherent in source code. To bridge this gap, this paper proposes the "Semantic
Chain-of-Thought" approach to intruduce semantic information of code, named
SeCoT. Our motivation is that the semantic information of the source code (\eg
data flow and control flow) describes more precise program execution behavior,
intention and function. By guiding LLM consider and integrate semantic
information, we can achieve a more granular understanding and representation of
code, enhancing code generation accuracy. Meanwhile, while traditional
techniques leveraging such semantic information require complex static or
dynamic code analysis to obtain features such as data flow and control flow,
SeCoT demonstrates that this process can be fully automated via the intrinsic
capabilities of LLMs (i.e., in-context learning), while being generalizable and
applicable to challenging domains. While SeCoT can be applied with different
LLMs, this paper focuses on the powerful GPT-style models: ChatGPT(close-source
model) and WizardCoder(open-source model). The experimental study on three
popular DL benchmarks (i.e., HumanEval, HumanEval-ET and MBPP) shows that SeCoT
can achieves state-of-the-art performance, greatly improving the potential for
large models and code generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10701">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10765">
<div class="article-summary-box-inner">
<span><p>Rapid progress has been made in instruction-learning for image editing with
natural-language instruction, as exemplified by InstructPix2Pix. In
biomedicine, such methods can be applied to counterfactual image generation,
which helps differentiate causal structure from spurious correlation and
facilitate robust image interpretation for disease progression modeling.
However, generic image-editing models are ill-suited for the biomedical domain,
and counterfactual biomedical image generation is largely underexplored. In
this paper, we present BiomedJourney, a novel method for counterfactual
biomedical image generation by instruction-learning from multimodal patient
journeys. Given a patient with two biomedical images taken at different time
points, we use GPT-4 to process the corresponding imaging reports and generate
a natural language description of disease progression. The resulting triples
(prior image, progression description, new image) are then used to train a
latent diffusion model for counterfactual biomedical image generation. Given
the relative scarcity of image time series data, we introduce a two-stage
curriculum that first pretrains the denoising network using the much more
abundant single image-report pairs (with dummy prior image), and then continues
training using the counterfactual triples. Experiments using the standard
MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive
battery of tests on counterfactual medical image generation, BiomedJourney
substantially outperforms prior state-of-the-art methods in instruction image
editing and medical image generation such as InstructPix2Pix and RoentGen. To
facilitate future study in counterfactual medical generation, we plan to
release our instruction-learning code and pretrained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT. (arXiv:2310.10903v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10903">
<div class="article-summary-box-inner">
<span><p>The rapid proliferation of ChatGPT has incited debates regarding its impact
on human writing. Amid concerns about declining writing standards, this study
investigates the role of ChatGPT in facilitating academic writing, especially
among language learners. Using a case study approach, this study examines the
experiences of Kailing, a doctoral student, who integrates ChatGPT throughout
their academic writing process. The study employs activity theory as a lens for
understanding writing with generative AI tools and data analyzed includes
semi-structured interviews, writing samples, and GPT logs. Results indicate
that Kailing effectively collaborates with ChatGPT across various writing
stages while preserving her distinct authorial voice and agency. This
underscores the potential of AI tools such as ChatGPT to enhance academic
writing for language learners without overshadowing individual authenticity.
This case study offers a critical exploration of how ChatGPT is utilized in the
academic writing process and the preservation of a student's authentic voice
when engaging with the tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11069">
<div class="article-summary-box-inner">
<span><p>Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11368">
<div class="article-summary-box-inner">
<span><p>Recognizing vulnerability is crucial for understanding and implementing
targeted support to empower individuals in need. This is especially important
at the European Court of Human Rights (ECtHR), where the court adapts
Convention standards to meet actual individual needs and thus ensures effective
human rights protection. However, the concept of vulnerability remains elusive
at the ECtHR and no prior NLP research has dealt with it. To enable future
research in this area, we present VECHR, a novel expert-annotated multi-label
dataset comprising of vulnerability type classification and explanation
rationale. We benchmark the performance of state-of-the-art models on VECHR
from both prediction and explainability perspectives. Our results demonstrate
the challenging nature of the task with lower prediction performance and
limited agreement between models and experts. Further, we analyze the
robustness of these models in dealing with out-of-domain (OOD) data and observe
overall limited performance. Our dataset poses unique challenges offering
significant room for improvement regarding performance, explainability, and
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11772">
<div class="article-summary-box-inner">
<span><p>Topic segmentation is critical for obtaining structured documents and
improving downstream tasks such as information retrieval. Due to its ability of
automatically exploring clues of topic shift from abundant labeled data, recent
supervised neural models have greatly promoted the development of long document
topic segmentation, but leaving the deeper relationship between coherence and
topic segmentation underexplored. Therefore, this paper enhances the ability of
supervised models to capture coherence from both logical structure and semantic
similarity perspectives to further improve the topic segmentation performance,
proposing Topic-aware Sentence Structure Prediction (TSSP) and Contrastive
Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to
force the model to comprehend structural information by learning the original
relations between adjacent sentences in a disarrayed document, which is
constructed by jointly disrupting the original document at topic and sentence
levels. Moreover, we utilize inter- and intra-topic information to construct
contrastive samples and design the CSSL objective to ensure that the sentences
representations in the same topic have higher similarity, while those in
different topics are less similar. Extensive experiments show that the
Longformer with our approach significantly outperforms old state-of-the-art
(SOTA) methods. Our approach improve $F_1$ of old SOTA by 3.42 (73.74 -&gt; 77.16)
and reduces $P_k$ by 1.11 points (15.0 -&gt; 13.89) on WIKI-727K and achieves an
average relative reduction of 4.3% on $P_k$ on WikiSection. The average
relative $P_k$ drop of 8.38% on two out-of-domain datasets also demonstrates
the robustness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification. (arXiv:2310.11878v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11878">
<div class="article-summary-box-inner">
<span><p>In legal NLP, Case Outcome Classification (COC) must not only be accurate but
also trustworthy and explainable. Existing work in explainable COC has been
limited to annotations by a single expert. However, it is well-known that
lawyers may disagree in their assessment of case facts. We hence collect a
novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two
experts in the domain of international human rights law, for whom we observe
weak agreement. We study their disagreements and build a two-level
task-independent taxonomy, supplemented with COC-specific subcategories. To our
knowledge, this is the first work in the legal NLP that focuses on human label
variation. We quantitatively assess different taxonomy categories and find that
disagreements mainly stem from underspecification of the legal context, which
poses challenges given the typically limited granularity and noise in COC
metadata. We further assess the explainablility of SOTA COC models on RAVE and
observe limited agreement between models and experts. Overall, our case study
reveals hitherto underappreciated complexities in creating benchmark datasets
in legal NLP that revolve around identifying aspects of a case's facts
supposedly relevant to its outcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11960">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have achieved state-of-the-art performance in many
areas. However, the quadratic complexity of self-attention with respect to the
input length hinders the applicability of Transformer-based models to long
sequences. To address this, we present Fast Multipole Attention, a new
attention mechanism that uses a divide-and-conquer strategy to reduce the time
and memory complexity of attention for sequences of length $n$ from
$\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a
global receptive field. The hierarchical approach groups queries, keys, and
values into $\mathcal{O}( \log n)$ levels of resolution, where groups at
greater distances are increasingly larger in size and the weights to compute
group quantities are learned. As such, the interaction between tokens far from
each other is considered in lower resolution in an efficient hierarchical
manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$
or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled
or not. This multi-level divide-and-conquer strategy is inspired by fast
summation methods from $n$-body physics and the Fast Multipole Method. We
perform evaluation on autoregressive and bidirectional language modeling tasks
and compare our Fast Multipole Attention model with other efficient attention
variants on medium-size datasets. We find empirically that the Fast Multipole
Transformer performs much better than other efficient transformers in terms of
memory size and accuracy. The Fast Multipole Attention mechanism has the
potential to empower large language models with much greater sequence lengths,
taking the full context into account in an efficient, naturally hierarchical
manner during training and when generating long sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation. (arXiv:2310.12020v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12020">
<div class="article-summary-box-inner">
<span><p>The convergence of embodied agents and large language models (LLMs) has
brought significant advancements to embodied instruction following.
Particularly, the strong reasoning capabilities of LLMs make it possible for
robots to perform long-horizon tasks without expensive annotated
demonstrations. However, public benchmarks for testing the long-horizon
reasoning capabilities of language-conditioned robots in various scenarios are
still missing. To fill this gap, this work focuses on the tabletop manipulation
task and releases a simulation benchmark, \textit{LoHoRavens}, which covers
various long-horizon reasoning aspects spanning color, size, space, arithmetics
and reference. Furthermore, there is a key modality bridging problem for
long-horizon manipulation tasks with LLMs: how to incorporate the observation
feedback during robot execution for the LLM's closed-loop planning, which is
however less studied by prior work. We investigate two methods of bridging the
modality gap: caption generation and learnable interface for incorporating
explicit and implicit observation feedback to the LLM, respectively. These
methods serve as the two baselines for our proposed benchmark. Experiments show
that both methods struggle to solve some tasks, indicating long-horizon
manipulation tasks are still challenging for current popular models. We expect
the proposed public benchmark and baselines can help the community develop
better models for long-horizon tabletop manipulation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12059">
<div class="article-summary-box-inner">
<span><p>In this paper, we evaluate the ability of large language models (LLMs) to
perform multiple choice symbol binding (MCSB) for multiple choice question
answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus
on Vietnamese, with fewer challenging MCQA datasets than in English. The two
existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent
research in Vietnamese natural language processing (NLP) has focused on the
Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to
2023 to evaluate ChatGPT. However, these studies have mainly focused on how
ChatGPT solves the VNHSGE step by step. We aim to create a novel and
high-quality dataset by providing structured guidelines for typing LaTeX
formulas for mathematics, physics, chemistry, and biology. This dataset can be
used to evaluate the MCSB ability of LLMs and smaller language models (LMs)
because it is typed in a strict LaTeX style. We focus on predicting the
character (A, B, C, or D) that is the most likely answer to a question, given
the context of the question. Our evaluation of six well-known LLMs, namely
BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the
ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising
results on the MCSB ability of LLMs for Vietnamese. The dataset is available
for research purposes only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures. (arXiv:2310.12074v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12074">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new IncidentAI dataset for safety prevention.
Different from prior corpora that usually contain a single task, our dataset
comprises three tasks: named entity recognition, cause-effect extraction, and
information retrieval. The dataset is annotated by domain experts who have at
least six years of practical experience as high-pressure gas conservation
managers. We validate the contribution of the dataset in the scenario of safety
prevention. Preliminary results on the three tasks show that NLP techniques are
beneficial for analyzing incident reports to prevent future failures. The
dataset facilitates future research in NLP and incident management communities.
The access to the dataset is also provided (the IncidentAI dataset is available
at: https://github.com/Cinnamon/incident-ai-dataset).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining Language Models with Text-Attributed Heterogeneous Graphs. (arXiv:2310.12580v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12580">
<div class="article-summary-box-inner">
<span><p>In many real-world scenarios (e.g., academic networks, social platforms),
different types of entities are not only associated with texts but also
connected by various relationships, which can be abstracted as Text-Attributed
Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models
(LMs) primarily focus on separately learning the textual information of each
entity and overlook the crucial aspect of capturing topological connections
among entities in TAHGs. In this paper, we present a new pretraining framework
for LMs that explicitly considers the topological and heterogeneous information
in TAHGs. Firstly, we define a context graph as neighborhoods of a target node
within specific orders and propose a topology-aware pretraining task to predict
nodes involved in the context graph by jointly optimizing an LM and an
auxiliary heterogeneous graph neural network. Secondly, based on the
observation that some nodes are text-rich while others have little text, we
devise a text augmentation strategy to enrich textless nodes with their
neighbors' texts for handling the imbalance issue. We conduct link prediction
and node classification tasks on three datasets from various domains.
Experimental results demonstrate the superiority of our approach over existing
methods and the rationality of each design. Our code is available at
https://github.com/Hope-Rita/THLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Real-World Streaming Speech Translation for Code-Switched Speech. (arXiv:2310.12648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12648">
<div class="article-summary-box-inner">
<span><p>Code-switching (CS), i.e. mixing different languages in a single sentence, is
a common phenomenon in communication and can be challenging in many Natural
Language Processing (NLP) settings. Previous studies on CS speech have shown
promising results for end-to-end speech translation (ST), but have been limited
to offline scenarios and to translation to one of the languages present in the
source (\textit{monolingual transcription}).
</p>
<p>In this paper, we focus on two essential yet unexplored areas for real-world
CS speech translation: streaming settings, and translation to a third language
(i.e., a language not included in the source). To this end, we extend the
Fisher and Miami test and validation datasets to include new targets in Spanish
and German. Using this data, we train a model for both offline and streaming ST
and we establish baseline results for the two settings mentioned earlier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12823">
<div class="article-summary-box-inner">
<span><p>Open large language models (LLMs) with great performance in various tasks
have significantly advanced the development of LLMs. However, they are far
inferior to commercial models such as ChatGPT and GPT-4 when acting as agents
to tackle complex tasks in the real world. These agent tasks employ LLMs as the
central controller responsible for planning, memorization, and tool
utilization, necessitating both fine-grained prompting methods and robust LLMs
to achieve satisfactory performance. Though many prompting methods have been
proposed to complete particular agent tasks, there is lack of research focusing
on improving the agent capabilities of LLMs themselves without compromising
their general abilities. In this work, we present AgentTuning, a simple and
general method to enhance the agent abilities of LLMs while maintaining their
general LLM capabilities. We construct AgentInstruct, a lightweight
instruction-tuning dataset containing high-quality interaction trajectories. We
employ a hybrid instruction-tuning strategy by combining AgentInstruct with
open-source instructions from general domains. AgentTuning is used to
instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show
that AgentTuning enables LLMs' agent capabilities without compromising general
abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent
tasks, demonstrating generalized agent capabilities. We open source the
AgentInstruct and AgentLM-7B, 13B, and 70B models at
https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to
commercial LLMs for agent tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding. (arXiv:2310.12874v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12874">
<div class="article-summary-box-inner">
<span><p>Analogy-making between narratives is crucial for human reasoning. In this
paper, we evaluate the ability to identify and generate analogies by
constructing a first-of-its-kind large-scale story-level analogy corpus,
\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with
human annotations on two similarities from the extended Structure-Mapping
Theory. We design a set of tests on \textsc{StoryAnalogy}, presenting the first
evaluation of story-level analogy identification and generation. Interestingly,
we find that the analogy identification tasks are incredibly difficult not only
for sentence embedding models but also for the recent large language models
(LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around
30% accuracy in multiple-choice questions (compared to over 85% accuracy for
humans). Furthermore, we observe that the data in \textsc{StoryAnalogy} can
improve the quality of analogy generation in LLMs, where a fine-tuned
FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models. (arXiv:2310.12936v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12936">
<div class="article-summary-box-inner">
<span><p>Various types of social biases have been reported with pretrained Masked
Language Models (MLMs) in prior work. However, multiple underlying factors are
associated with an MLM such as its model size, size of the training data,
training objectives, the domain from which pretraining data is sampled,
tokenization, and languages present in the pretrained corpora, to name a few.
It remains unclear as to which of those factors influence social biases that
are learned by MLMs. To study the relationship between model factors and the
social biases learned by an MLM, as well as the downstream task performance of
the model, we conduct a comprehensive study over 39 pretrained MLMs covering
different model sizes, training objectives, tokenization methods, training data
domains and languages. Our results shed light on important factors often
neglected in prior literature, such as tokenization or model objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12942">
<div class="article-summary-box-inner">
<span><p>This work investigates the computational expressivity of language models
(LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992)
famously showed that RNNs with rational weights and hidden states and unbounded
computation time are Turing complete. However, LMs define weightings over
strings in addition to just (unweighted) language membership and the analysis
of the computational power of RNN LMs (RLMs) should reflect this. We extend the
Turing completeness result to the probabilistic case, showing how a rationally
weighted RLM with unbounded computation time can simulate any probabilistic
Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a
symbol at every time step, we treat the above result as an upper bound on the
expressivity of RLMs. We also provide a lower bound by showing that under the
restriction to real-time computation, such models can simulate deterministic
real-time rational PTMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13012">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) represent a revolution in AI. However, they also
pose many significant risks, such as the presence of biased, private,
copyrighted or harmful text. For this reason we need open, transparent and safe
solutions. We introduce a complete open-source ecosystem for developing and
testing LLMs. The goal of this project is to boost open alternatives to
closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs of
diverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUI
designed for efficient fine-tuning, evaluation, and deployment of LLMs using
the most recent state-of-the-art techniques. Our code and models are fully
open-source. We believe this work helps to boost AI development and make it
more accessible, efficient and trustworthy. The demo is available at:
https://gpt.h2o.ai/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories. (arXiv:1910.07115v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.07115">
<div class="article-summary-box-inner">
<span><p>GitHub has become an important platform for code sharing and scientific
exchange. With the massive number of repositories available, there is a
pressing need for topic-based search. Even though the topic label functionality
has been introduced, the majority of GitHub repositories do not have any
labels, impeding the utility of search and topic-based analysis. This work
targets the automatic repository classification problem as keyword-driven
hierarchical classification. Specifically, users only need to provide a label
hierarchy with keywords to supply as supervision. This setting is flexible,
adaptive to the users' needs, accounts for the different granularity of topic
labels and requires minimal human effort. We identify three key challenges of
this problem, namely (1) the presence of multi-modal signals; (2) supervision
scarcity and bias; (3) supervision format mismatch. In recognition of these
challenges, we propose the HiGitClass framework, comprising of three modules:
heterogeneous information network embedding; keyword enrichment; topic modeling
and pseudo document generation. Experimental results on two GitHub repository
collections confirm that HiGitClass is superior to existing weakly-supervised
and dataless hierarchical classification methods, especially in its ability to
integrate both structured and unstructured data for repository classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimally Supervised Categorization of Text with Metadata. (arXiv:2005.00624v3 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00624">
<div class="article-summary-box-inner">
<span><p>Document categorization, which aims to assign a topic label to each document,
plays a fundamental role in a wide variety of applications. Despite the success
of existing studies in conventional supervised document classification, they
are less concerned with two real problems: (1) the presence of metadata: in
many domains, text is accompanied by various additional information such as
authors and tags. Such metadata serve as compelling topic indicators and should
be leveraged into the categorization framework; (2) label scarcity: labeled
training samples are expensive to obtain in some cases, where categorization
needs to be performed using only a small set of annotated data. In recognition
of these two challenges, we propose MetaCat, a minimally supervised framework
to categorize text with metadata. Specifically, we develop a generative process
describing the relationships between words, documents, labels, and metadata.
Guided by the generative model, we embed text and metadata into the same
semantic space to encode heterogeneous signals. Then, based on the same
generative process, we synthesize training samples to address the bottleneck of
label scarcity. We conduct a thorough evaluation on a wide range of datasets.
Experimental results prove the effectiveness of MetaCat over many competitive
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Metadata-Aware Document Categorization under Weak Supervision. (arXiv:2010.13556v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13556">
<div class="article-summary-box-inner">
<span><p>Categorizing documents into a given label hierarchy is intuitively appealing
due to the ubiquity of hierarchical topic structures in massive text corpora.
Although related studies have achieved satisfying performance in fully
supervised hierarchical document classification, they usually require massive
human-annotated training data and only utilize text information. However, in
many domains, (1) annotations are quite expensive where very few training
samples can be acquired; (2) documents are accompanied by metadata information.
Hence, this paper studies how to integrate the label hierarchy, metadata, and
text signals for document categorization under weak supervision. We develop
HiMeCat, an embedding-based generative framework for our task. Specifically, we
propose a novel joint representation learning module that allows simultaneous
modeling of category dependencies, metadata information and textual semantics,
and we introduce a data augmentation module that hierarchically synthesizes
training documents to complement the original, small-scale training set. Our
experiments demonstrate a consistent improvement of HiMeCat over competitive
baselines and validate the contribution of our representation learning and data
augmentation modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MATCH: Metadata-Aware Text Classification in A Large Hierarchy. (arXiv:2102.07349v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07349">
<div class="article-summary-box-inner">
<span><p>Multi-label text classification refers to the problem of assigning each given
document its most relevant labels from the label set. Commonly, the metadata of
the given documents and the hierarchy of the labels are available in real-world
applications. However, most existing studies focus on only modeling the text
information, with a few attempts to utilize either metadata or hierarchy
signals, but not both of them. In this paper, we bridge the gap by formalizing
the problem of metadata-aware text classification in a large label hierarchy
(e.g., with tens of thousands of labels). To address this problem, we present
the MATCH solution -- an end-to-end framework that leverages both metadata and
hierarchy information. To incorporate metadata, we pre-train the embeddings of
text and metadata in the same space and also leverage the fully-connected
attentions to capture the interrelations between them. To leverage the label
hierarchy, we propose different ways to regularize the parameters and output
probability of each child label by its parents. Extensive experiments on two
massive text datasets with large-scale label hierarchies demonstrate the
effectiveness of MATCH over state-of-the-art deep learning baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v3 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04022">
<div class="article-summary-box-inner">
<span><p>We study the problem of weakly supervised text classification, which aims to
classify text documents into a set of pre-defined categories with category
surface names only and without any annotated training document provided. Most
existing classifiers leverage textual information in each document. However, in
many domains, documents are accompanied by various types of metadata (e.g.,
authors, venue, and year of a research paper). These metadata and their
combinations may serve as strong category indicators in addition to textual
contents. In this paper, we explore the potential of using metadata to help
weakly supervised text classification. To be specific, we model the
relationships between documents and metadata via a heterogeneous information
network. To effectively capture higher-order structures in the network, we use
motifs to describe metadata combinations. We propose a novel framework, named
MotifClass, which (1) selects category-indicative motif instances, (2)
retrieves and generates pseudo-labeled training samples based on category names
and indicative motif instances, and (3) trains a text classifier using the
pseudo training data. Extensive experiments on real-world datasets demonstrate
the superior performance of MotifClass to existing weakly supervised text
classification approaches. Further analysis shows the benefit of considering
higher-order metadata information in our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation. (arXiv:2111.06515v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06515">
<div class="article-summary-box-inner">
<span><p>Real-time location inference of social media users is the fundamental of some
spatial applications such as localized search and event detection. While tweet
text is the most commonly used feature in location estimation, most of the
prior works suffer from either the noise or the sparsity of textual features.
In this paper, we aim to tackle these two problems. We use topic modeling as a
building block to characterize the geographic topic variation and lexical
variation so that "one-hot" encoding vectors will no longer be directly used.
We also incorporate other features which can be extracted through the Twitter
streaming API to overcome the noise problem. Experimental results show that our
RATE algorithm outperforms several benchmark methods, both in the precision of
region classification and the mean distance error of latitude and longitude
regression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05932">
<div class="article-summary-box-inner">
<span><p>Large-scale multi-label text classification (LMTC) aims to associate a
document with its relevant labels from a large candidate set. Most existing
LMTC approaches rely on massive human-annotated training data, which are often
costly to obtain and suffer from a long-tailed label distribution (i.e., many
labels occur only a few times in the training set). In this paper, we study
LMTC under the zero-shot setting, which does not require any annotated
documents with labels and only relies on label surface names and descriptions.
To train a classifier that calculates the similarity score between a document
and a label, we propose a novel metadata-induced contrastive learning (MICoL)
method. Different from previous text-based contrastive learning techniques,
MICoL exploits document metadata (e.g., authors, venues, and references of
research papers), which are widely available on the Web, to derive similar
document-document pairs. Experimental results on two large-scale datasets show
that: (1) MICoL significantly outperforms strong zero-shot text classification
and contrastive learning baselines; (2) MICoL is on par with the
state-of-the-art supervised metadata-aware LMTC method trained on 10K-200K
labeled documents; and (3) MICoL tends to predict more infrequent labels than
supervised methods, thus alleviates the deteriorated performance on long-tailed
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds. (arXiv:2205.01845v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01845">
<div class="article-summary-box-inner">
<span><p>Discovering latent topics from text corpora has been studied for decades.
Many existing topic models adopt a fully unsupervised setting, and their
discovered topics may not cater to users' particular interests due to their
inability of leveraging user guidance. Although there exist seed-guided topic
discovery approaches that leverage user-provided seeds to discover
topic-representative terms, they are less concerned with two factors: (1) the
existence of out-of-vocabulary seeds and (2) the power of pre-trained language
models (PLMs). In this paper, we generalize the task of seed-guided topic
discovery to allow out-of-vocabulary seeds. We propose a novel framework, named
SeeTopic, wherein the general knowledge of PLMs and the local semantics learned
from the input corpus can mutually benefit each other. Experiments on three
real datasets from different domains demonstrate the effectiveness of SeeTopic
in terms of topic coherence, accuracy, and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study. (arXiv:2302.03341v1 [cs.DL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03341">
<div class="article-summary-box-inner">
<span><p>Due to the exponential growth of scientific publications on the Web, there is
a pressing need to tag each paper with fine-grained topics so that researchers
can track their interested fields of study rather than drowning in the whole
literature. Scientific literature tagging is beyond a pure multi-label text
classification task because papers on the Web are prevalently accompanied by
metadata information such as venues, authors, and references, which may serve
as additional signals to infer relevant tags. Although there have been studies
making use of metadata in academic paper classification, their focus is often
restricted to one or two scientific fields (e.g., computer science and
biomedicine) and to one specific model. In this work, we systematically study
the effect of metadata on scientific literature tagging across 19 fields. We
select three representative multi-label classifiers (i.e., a bag-of-words
model, a sequence-based model, and a pre-trained language model) and explore
their performance change in scientific literature tagging when metadata are fed
to the classifiers as additional features. We observe some ubiquitous patterns
of metadata's effects across all fields (e.g., venues are consistently
beneficial to paper tagging in almost all cases), as well as some unique
patterns in fields other than computer science and biomedicine, which are not
explored in previous studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. (arXiv:2305.14232v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14232">
<div class="article-summary-box-inner">
<span><p>Scientific literature understanding tasks have gained significant attention
due to their potential to accelerate scientific discovery. Pre-trained language
models (LMs) have shown effectiveness in these tasks, especially when tuned via
contrastive learning. However, jointly utilizing pre-training data across
multiple heterogeneous tasks (e.g., extreme multi-label paper classification,
citation prediction, and literature search) remains largely unexplored. To
bridge this gap, we propose a multi-task contrastive learning framework,
SciMult, with a focus on facilitating common knowledge sharing across different
scientific literature understanding tasks while preventing task-specific skills
from interfering with each other. To be specific, we explore two techniques --
task-aware specialization and instruction tuning. The former adopts a
Mixture-of-Experts Transformer architecture with task-aware sub-layers; the
latter prepends task-specific instructions to the input text so as to produce
task-aware outputs. Extensive experiments on a comprehensive collection of
benchmark datasets verify the effectiveness of our task-aware specialization
strategy, where we outperform state-of-the-art scientific pre-trained LMs.
Code, datasets, and pre-trained models can be found at
https://scimult.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14003">
<div class="article-summary-box-inner">
<span><p>Instead of relying on human-annotated training samples to build a classifier,
weakly supervised scientific paper classification aims to classify papers only
using category descriptions (e.g., category names, category-indicative
keywords). Existing studies on weakly supervised paper classification are less
concerned with two challenges: (1) Papers should be classified into not only
coarse-grained research topics but also fine-grained themes, and potentially
into multiple themes, given a large and fine-grained label space; and (2) full
text should be utilized to complement the paper title and abstract for
classification. Moreover, instead of viewing the entire paper as a long linear
sequence, one should exploit the structural information such as citation links
across papers and the hierarchy of sections and paragraphs in each paper. To
tackle these challenges, in this study, we propose FUTEX, a framework that uses
the cross-paper network structure and the in-paper hierarchy structure to
classify full-text scientific papers under weak supervision. A network-aware
contrastive fine-tuning module and a hierarchy-aware aggregation module are
designed to leverage the two types of structural signals, respectively.
Experiments on two benchmark datasets demonstrate that FUTEX significantly
outperforms competitive baselines and is on par with fully supervised
classifiers that use 1,000 to 60,000 ground-truth training samples.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-24 23:11:10.629344933 UTC">2023-10-24 23:11:10 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>