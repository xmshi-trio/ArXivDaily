<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-23T01:30:00Z">10-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Health Data Interoperability with Large Language Models: A FHIR Study. (arXiv:2310.12989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12989">
<div class="article-summary-box-inner">
<span><p>In this study, we investigated the ability of the large language model (LLM)
to enhance healthcare data interoperability. We leveraged the LLM to convert
clinical texts into their corresponding FHIR resources. Our experiments,
conducted on 3,671 snippets of clinical text, demonstrated that the LLM not
only streamlines the multi-step natural language processing and human
calibration processes but also achieves an exceptional accuracy rate of over
90% in exact matches when compared to human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-Level Relation Extraction with Relation Correlation Enhancement. (arXiv:2310.13000v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13000">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (DocRE) is a task that focuses on
identifying relations between entities within a document. However, existing
DocRE models often overlook the correlation between relations and lack a
quantitative analysis of relation correlations. To address this limitation and
effectively capture relation correlations in DocRE, we propose a relation graph
method, which aims to explicitly exploit the interdependency among relations.
Firstly, we construct a relation graph that models relation correlations using
statistical co-occurrence information derived from prior relation knowledge.
Secondly, we employ a re-weighting scheme to create an effective relation
correlation matrix to guide the propagation of relation information.
Furthermore, we leverage graph attention networks to aggregate relation
embeddings. Importantly, our method can be seamlessly integrated as a
plug-and-play module into existing models. Experimental results demonstrate
that our approach can enhance the performance of multi-relation extraction,
highlighting the effectiveness of considering relation correlations in DocRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13001">
<div class="article-summary-box-inner">
<span><p>With the exponential growth in large language models (LLMs), leveraging their
emergent properties for specialized domains like finance merits exploration.
However, regulated fields such as finance pose unique constraints, requiring
domain-optimized frameworks. We present ConFIRM, an LLM-based conversational
financial information retrieval model tailored for query intent classification
and knowledge base labeling.
</p>
<p>ConFIRM comprises two modules:
</p>
<p>1) a method to synthesize finance domain-specific question-answer pairs, and
</p>
<p>2) evaluation of parameter efficient fine-tuning approaches for the query
classification task. We generate a dataset of over 4000 samples, assessing
accuracy on a separate test set.
</p>
<p>ConFIRM achieved over 90% accuracy, essential for regulatory compliance.
ConFIRM provides a data-efficient solution to extract precise query intent for
financial dialog systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Geospatially Knowledgeable?. (arXiv:2310.13002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13002">
<div class="article-summary-box-inner">
<span><p>Despite the impressive performance of Large Language Models (LLM) for various
natural language processing tasks, little is known about their comprehension of
geographic data and related ability to facilitate informed geospatial
decision-making. This paper investigates the extent of geospatial knowledge,
awareness, and reasoning abilities encoded within such pretrained LLMs. With a
focus on autoregressive language models, we devise experimental approaches
related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge,
(ii) using geospatial and non-geospatial prepositions to gauge their geospatial
awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to
assess the models' geospatial reasoning capabilities and to determine locations
of cities based on prompting. Our results confirm that it does not only take
larger, but also more sophisticated LLMs to synthesize geospatial knowledge
from textual information. As such, this research contributes to understanding
the potential and limitations of LLMs in dealing with geospatial information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoBaSS: Gauging Learnability in Supervised Fine-tuning Data. (arXiv:2310.13008v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13008">
<div class="article-summary-box-inner">
<span><p>Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large
Language Models (LLMs) to specific task prerequisites. The selection of
fine-tuning data profoundly influences the model's performance, whose principle
is traditionally grounded in data quality and distribution. In this paper, we
introduce a new dimension in SFT data selection: learnability. This new
dimension is motivated by the intuition that SFT unlocks capabilities acquired
by a LLM during the pretraining phase. Given that different pretrained models
have disparate capabilities, the SFT data appropriate for one may not suit
another. Thus, we introduce the term learnability to define the suitability of
data for effective learning by the model. We present the Loss Based SFT Data
Selection (LoBaSS) method, utilizing data learnability as the principal
criterion for the selection SFT data. This method provides a nuanced approach,
allowing the alignment of data selection with inherent model capabilities,
ensuring optimal compatibility and learning efficiency. In experimental
comparisons involving 7B and 13B models, our LoBaSS method is able to surpass
full-data fine-tuning at merely 6% of the total training data. When employing
16.7% of the data, LoBaSS harmonizes the model's capabilities across
conversational and mathematical domains, proving its efficacy and adaptability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13011">
<div class="article-summary-box-inner">
<span><p>As language models (LMs) become more capable, it is increasingly important to
align them with human preferences. However, the dominant paradigm for training
Preference Models (PMs) for that purpose suffers from fundamental limitations,
such as lack of transparency and scalability, along with susceptibility to
overfitting the preference dataset. We propose Compositional Preference Models
(CPMs), a novel PM framework that decomposes one global preference assessment
into several interpretable features, obtains scalar scores for these features
from a prompted LM, and aggregates these scores using a logistic regression
classifier. CPMs allow to control which properties of the preference data are
used to train the preference model and to build it based on features that are
believed to underlie the human preference judgment. Our experiments show that
CPMs not only improve generalization and are more robust to overoptimization
than standard PMs, but also that best-of-n samples obtained using CPMs tend to
be preferred over samples obtained using conventional PMs. Overall, our
approach demonstrates the benefits of endowing PMs with priors about which
features determine human preferences while relying on LM capabilities to
extract those features in a scalable and robust way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13012">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) represent a revolution in AI. However, they also
pose many significant risks, such as the presence of biased, private,
copyrighted or harmful text. For this reason we need open, transparent and safe
solutions. We introduce a complete open-source ecosystem for developing and
testing LLMs. The goal of this project is to boost open alternatives to
closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7
to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and
no-code GUI designed for efficient fine-tuning, evaluation, and deployment of
LLMs using the most recent state-of-the-art techniques. Our code and models are
licensed under fully permissive Apache 2.0 licenses. We believe open-source
language models help to boost AI development and make it more accessible and
trustworthy. The demo is available at: https://gpt.h2o.ai/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative error correction for code-switching speech recognition using large language models. (arXiv:2310.13013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13013">
<div class="article-summary-box-inner">
<span><p>Code-switching (CS) speech refers to the phenomenon of mixing two or more
languages within the same sentence. Despite the recent advances in automatic
speech recognition (ASR), CS-ASR is still a challenging task ought to the
grammatical structure complexity of the phenomenon and the data scarcity of
specific training corpus. In this work, we propose to leverage large language
models (LLMs) and lists of hypotheses generated by an ASR to address the CS
problem. Specifically, we first employ multiple well-trained ASR models for
N-best hypotheses generation, with the aim of increasing the diverse and
informative elements in the set of hypotheses. Next, we utilize the LLMs to
learn the hypotheses-to-transcription (H2T) mapping by adding a trainable
low-rank adapter. Such a generative error correction (GER) method directly
predicts the accurate transcription according to its expert linguistic
knowledge and N-best hypotheses, resulting in a paradigm shift from the
traditional language model rescoring or error correction techniques.
Experimental evidence demonstrates that GER significantly enhances CS-ASR
accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show
remarkable data efficiency for H2T learning, providing a potential solution to
the data scarcity problem of CS-ASR in low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. (arXiv:2310.13014v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13014">
<div class="article-summary-box-inner">
<span><p>Accurately predicting the future would be an important milestone in the
capabilities of artificial intelligence. However, research on the ability of
large language models to provide probabilistic predictions about future events
remains nascent. To empirically test this ability, we enrolled OpenAI's
state-of-the-art large language model, GPT-4, in a three-month forecasting
tournament hosted on the Metaculus platform. The tournament, running from July
to October 2023, attracted 843 participants and covered diverse topics
including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict.
Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are
significantly less accurate than the median human-crowd forecasts. We find that
GPT-4's forecasts did not significantly differ from the no-information
forecasting strategy of assigning a 50% probability to every question. We
explore a potential explanation, that GPT-4 might be predisposed to predict
probabilities close to the midpoint of the scale, but our data do not support
this hypothesis. Overall, we find that GPT-4 significantly underperforms in
real-world predictive tasks compared to median human-crowd forecasts. A
potential explanation for this underperformance is that in real-world
forecasting tournaments, the true answers are genuinely unknown at the time of
prediction; unlike in other benchmark tasks like professional exams or time
series forecasting, where strong performance may at least partly be due to the
answers being memorized from the training data. This makes real-world
forecasting tournaments an ideal environment for testing the generalized
reasoning and prediction capabilities of artificial intelligence going forward.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition. (arXiv:2310.13015v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13015">
<div class="article-summary-box-inner">
<span><p>Adapters are an efficient, composable alternative to full fine-tuning of
pre-trained models and help scale the deployment of large ASR models to many
tasks. In practice, a task ID is commonly prepended to the input during
inference to route to single-task adapters for the specified task. However, one
major limitation of this approach is that the task ID may not be known during
inference, rendering it unsuitable for most multi-task settings. To address
this, we propose three novel task-ID-free methods to combine single-task
adapters in multi-task ASR and investigate two learning algorithms for
training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and
show that our methods are non-destructive and parameter-efficient. While only
updating 17% of the model parameters, our methods can achieve an 8% mean WER
improvement relative to full fine-tuning and are on-par with task-ID adapter
routing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position Interpolation Improves ALiBi Extrapolation. (arXiv:2310.13017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13017">
<div class="article-summary-box-inner">
<span><p>Linear position interpolation helps pre-trained models using rotary position
embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using
linear position interpolation to extend the extrapolation range of models using
Attention with Linear Biases (ALiBi). We find position interpolation
significantly improves extrapolation capability on upstream language modelling
and downstream summarization and retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding. (arXiv:2310.13022v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13022">
<div class="article-summary-box-inner">
<span><p>The recent success of large pre-trained language models (PLMs) heavily hinges
on massive labeled data, which typically produces inferior performance in
low-resource scenarios. To remedy this dilemma, we study self-training as one
of the predominant semi-supervised learning (SSL) approaches, which utilizes
large-scale unlabeled data to generate synthetic examples. However, too many
noisy labels will hurt the model performance, and the self-training procedure
requires multiple training iterations making it more expensive if all the model
parameters of the PLM are updated. This paper presents UPET, a novel
Uncertainty-aware Parameter-Efficient self-Training framework to effectively
and efficiently address the labeled data scarcity issue. Specifically, we
incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to
perform uncertainty estimation for the teacher model and then judiciously
select reliable pseudo-labeled examples based on confidence and certainty.
During the student training, we introduce multiple parameter-efficient learning
(PEL) paradigms that allow the optimization of only a small percentage of
parameters. We also propose a novel Easy-Hard Contrastive Tuning to enhance the
robustness and generalization. Extensive experiments over multiple downstream
tasks demonstrate that UPET achieves a substantial improvement in terms of
performance and efficiency. Our codes and data are released at https:
//github.com/wjn1996/UPET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13023">
<div class="article-summary-box-inner">
<span><p>Graph Neural Networks (GNNs) have advanced graph structure understanding via
recursive information exchange and aggregation among graph nodes. To improve
model robustness, self-supervised learning (SSL) has emerged as a promising
approach for data augmentation. However, existing methods for generating
pre-trained graph embeddings often rely on fine-tuning with specific downstream
task labels, which limits their usability in scenarios where labeled data is
scarce or unavailable. To address this, our research focuses on advancing the
generalization capabilities of graph models in challenging zero-shot learning
scenarios. Inspired by the success of large language models (LLMs), we aim to
develop a graph-oriented LLM that can achieve high generalization across
diverse downstream datasets and tasks, even without any information available
from the downstream graph data. In this work, we present the GraphGPT framework
that aligns LLMs with graph structural knowledge with a graph instruction
tuning paradigm. Our framework incorporates a text-graph grounding component to
establish a connection between textual information and graph structures.
Additionally, we propose a dual-stage instruction tuning paradigm, accompanied
by a lightweight graph-text alignment projector. This paradigm explores
self-supervised graph structural signals and task-specific graph instructions,
to guide LLMs in understanding complex graph structures and improving their
adaptability across different downstream tasks. Our framework is evaluated on
supervised and zero-shot graph learning tasks, demonstrating superior
generalization and outperforming state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt. (arXiv:2310.13024v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13024">
<div class="article-summary-box-inner">
<span><p>Continual pre-training has been urgent for adapting a pre-trained model to a
multitude of domains and tasks in the fast-evolving world. In practice, a
continually pre-trained model is expected to demonstrate not only greater
capacity when fine-tuned on pre-trained domains but also a non-decreasing
performance on unseen ones. In this work, we first investigate such anytime
fine-tuning effectiveness of existing continual pre-training approaches,
concluding with unanimously decreased performance on unseen domains. To this
end, we propose a prompt-guided continual pre-training method, where we train a
hypernetwork to generate domain-specific prompts by both agreement and
disagreement losses. The agreement loss maximally preserves the generalization
of a pre-trained model to new domains, and the disagreement one guards the
exclusiveness of the generated hidden states for each domain. Remarkably,
prompts by the hypernetwork alleviate the domain identity when fine-tuning and
promote knowledge transfer across domains. Our method achieved improvements of
3.57% and 3.4% on two real-world datasets (including domain shift and temporal
shift), respectively, demonstrating its efficacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Powerset multi-class cross entropy loss for neural speaker diarization. (arXiv:2310.13025v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13025">
<div class="article-summary-box-inner">
<span><p>Since its introduction in 2019, the whole end-to-end neural diarization
(EEND) line of work has been addressing speaker diarization as a frame-wise
multi-label classification problem with permutation-invariant training. Despite
EEND showing great promise, a few recent works took a step back and studied the
possible combination of (local) supervised EEND diarization with (global)
unsupervised clustering. Yet, these hybrid contributions did not question the
original multi-label formulation. We propose to switch from multi-label (where
any two speakers can be active at the same time) to powerset multi-class
classification (where dedicated classes are assigned to pairs of overlapping
speakers). Through extensive experiments on 9 different benchmarks, we show
that this formulation leads to significantly better performance (mostly on
overlapping speech) and robustness to domain mismatch, while eliminating the
detection threshold hyperparameter, critical for the multi-label formulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13028">
<div class="article-summary-box-inner">
<span><p>The rapid growth of computer science has led to a proliferation of research
presented at academic conferences, fostering global scholarly communication.
Researchers consistently seek accurate, current information about these events
at all stages. This data surge necessitates an intelligent question-answering
system to efficiently address researchers' queries and ensure awareness of the
latest advancements. The information of conferences is usually published on
their official website, organized in a semi-structured way with a lot of text.
To address this need, we have developed the ConferenceQA dataset for 7 diverse
academic conferences with human annotations. Firstly, we employ a combination
of manual and automated methods to organize academic conference data in a
semi-structured JSON format. Subsequently, we annotate nearly 100
question-answer pairs for each conference. Each pair is classified into four
different dimensions. To ensure the reliability of the data, we manually
annotate the source of each answer. In light of recent advancements, Large
Language Models (LLMs) have demonstrated impressive performance in various NLP
tasks. They have demonstrated impressive capabilities in information-seeking
question answering after instruction fine-tuning, and as such, we present our
conference QA study based on LLM. Due to hallucination and outdated knowledge
of LLMs, we adopt retrieval based methods to enhance LLMs' question-answering
abilities. We have proposed a structure-aware retrieval method, specifically
designed to leverage inherent structural information during the retrieval
process. Empirical validation on the ConferenceQA dataset has demonstrated the
effectiveness of this method. The dataset and code are readily accessible on
https://github.com/zjukg/ConferenceQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem. (arXiv:2310.13031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13031">
<div class="article-summary-box-inner">
<span><p>One of the most important challenges for modern search engines is to retrieve
relevant web content based on user queries. In order to achieve this challenge,
search engines have a module to rewrite user queries. That is why modern web
search engines utilize some statistical and neural models used in the natural
language processing domain. Statistical machine translation is a well-known NLP
method among them. The paper proposes a query rewriting pipeline based on a
monolingual machine translation model that learns to rewrite Arabic user search
queries. This paper also describes preprocessing steps to create a mapping
between user queries and web page titles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13032">
<div class="article-summary-box-inner">
<span><p>In many text-generation problems, users may prefer not only a single
response, but a diverse range of high-quality outputs from which to choose.
Quality-diversity (QD) search algorithms aim at such outcomes, by continually
improving and diversifying a population of candidates. However, the
applicability of QD to qualitative domains, like creative writing, has been
limited by the difficulty of algorithmically specifying measures of quality and
diversity. Interestingly, recent developments in language models (LMs) have
enabled guiding search through AI feedback, wherein LMs are prompted in natural
language to evaluate qualitative aspects of text. Leveraging this development,
we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an
evolutionary algorithm applies LMs to both generate variation and evaluate the
quality and diversity of candidate text. When assessed on creative writing
domains, QDAIF covers more of a specified search space with high-quality
samples than do non-QD controls. Further, human evaluation of QDAIF-generated
creative texts validates reasonable agreement between AI and human evaluation.
Our results thus highlight the potential of AI feedback to guide open-ended
search for creative and original solutions, providing a recipe that seemingly
generalizes to many domains and modalities. In this way, QDAIF is a step
towards AI systems that can independently search, diversify, evaluate, and
improve, which are among the core skills underlying human society's capacity
for innovation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GARI: Graph Attention for Relative Isomorphism of Arabic Word Embeddings. (arXiv:2310.13068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13068">
<div class="article-summary-box-inner">
<span><p>Bilingual Lexical Induction (BLI) is a core challenge in NLP, it relies on
the relative isomorphism of individual embedding spaces. Existing attempts
aimed at controlling the relative isomorphism of different embedding spaces
fail to incorporate the impact of semantically related words in the model
training objective. To address this, we propose GARI that combines the
distributional training objectives with multiple isomorphism losses guided by
the graph attention network. GARI considers the impact of semantical variations
of words in order to define the relative isomorphism of the embedding spaces.
Experimental evaluation using the Arabic language data set shows that GARI
outperforms the existing research by improving the average P@1 by a relative
score of up to 40.95% and 76.80% for in-domain and domain mismatch settings
respectively. We release the codes for GARI at
https://github.com/asif6827/GARI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues. (arXiv:2310.13080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13080">
<div class="article-summary-box-inner">
<span><p>Understanding emotions during conversation is a fundamental aspect of human
communication, driving NLP research for Emotion Recognition in Conversation
(ERC). While considerable research has focused on discerning emotions of
individual speakers in monolingual dialogues, understanding the emotional
dynamics in code-mixed conversations has received relatively less attention.
This motivates our undertaking of ERC for code-mixed conversations in this
study. Recognizing that emotional intelligence encompasses a comprehension of
worldly knowledge, we propose an innovative approach that integrates
commonsense information with dialogue context to facilitate a deeper
understanding of emotions. To achieve this, we devise an efficient pipeline
that extracts relevant commonsense from existing knowledge graphs based on the
code-mixed input. Subsequently, we develop an advanced fusion technique that
seamlessly combines the acquired commonsense information with the dialogue
representation obtained from a dedicated dialogue understanding module. Our
comprehensive experimentation showcases the substantial performance improvement
obtained through the systematic incorporation of commonsense in ERC. Both
quantitative assessments and qualitative analyses further corroborate the
validity of our hypothesis, reaffirming the pivotal role of commonsense
integration in enhancing ERC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Language Models Learn about Legal Entity Types during Pretraining?. (arXiv:2310.13092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13092">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have proven their ability to acquire diverse linguistic
knowledge during the pretraining phase, potentially serving as a valuable
source of incidental supervision for downstream tasks. However, there has been
limited research conducted on the retrieval of domain-specific knowledge, and
specifically legal knowledge. We propose to explore the task of Entity Typing,
serving as a proxy for evaluating legal knowledge as an essential aspect of
text comprehension, and a foundational task to numerous downstream legal NLP
applications. Through systematic evaluation and analysis and two types of
prompting (cloze sentences and QA-based templates) and to clarify the nature of
these acquired cues, we compare diverse types and lengths of entities both
general and domain-specific entities, semantics or syntax signals, and
different LM pretraining corpus (generic and legal-oriented) and architectures
(encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2
performs well on certain entities and exhibits potential for substantial
improvement with optimized prompt templates, (2) law-oriented LMs show
inconsistent performance, possibly due to variations in their training corpus,
(3) LMs demonstrate the ability to type entities even in the case of
multi-token entities, (4) all models struggle with entities belonging to
sub-domains of the law (5) Llama2 appears to frequently overlook syntactic
cues, a shortcoming less present in BERT-based architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network. (arXiv:2310.13099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13099">
<div class="article-summary-box-inner">
<span><p>We introduce a simple yet efficient sentence-level attack on black-box
toxicity detector models. By adding several positive words or sentences to the
end of a hateful message, we are able to change the prediction of a neural
network and pass the toxicity detection system check. This approach is shown to
be working on seven languages from three different language families. We also
describe the defence mechanism against the aforementioned attack and discuss
its limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model. (arXiv:2310.13106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13106">
<div class="article-summary-box-inner">
<span><p>Question generation is a widely used data augmentation approach with
extensive applications, and extracting qualified candidate answers from context
passages is a critical step for most question generation systems. However,
existing methods for candidate answer extraction are reliant on linguistic
rules or annotated data that face the partial annotation issue and challenges
in generalization. To overcome these limitations, we propose a novel
unsupervised candidate answer extraction approach that leverages the inherent
structure of context passages through a Differentiable Masker-Reconstructor
(DMR) Model with the enforcement of self-consistency for picking up salient
information tokens. We curated two datasets with exhaustively-annotated answers
and benchmark a comprehensive set of supervised and unsupervised candidate
answer extraction methods. We demonstrate the effectiveness of the DMR model by
showing its performance is superior among unsupervised methods and comparable
to supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models. (arXiv:2310.13127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13127">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can perform a wide range of tasks by following
natural language instructions, without the necessity of task-specific
fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by
the quality of these instructions, and manually writing effective instructions
for each task is a laborious and subjective process. In this paper, we
introduce Auto-Instruct, a novel method to automatically improve the quality of
instructions provided to LLMs. Our method leverages the inherent generative
ability of LLMs to produce diverse candidate instructions for a given task, and
then ranks them using a scoring model trained on a variety of 575 existing NLP
tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both
human-written instructions and existing baselines of LLM-generated
instructions. Furthermore, our method exhibits notable generalizability even
with other LLMs that are not incorporated into its training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries. (arXiv:2310.13132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13132">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are transforming the ways the general public
accesses and consumes information. Their influence is particularly pronounced
in pivotal sectors like healthcare, where lay individuals are increasingly
appropriating LLMs as conversational agents for everyday queries. While LLMs
demonstrate impressive language understanding and generation proficiencies,
concerns regarding their safety remain paramount in these high-stake domains.
Moreover, the development of LLMs is disproportionately focused on English. It
remains unclear how these LLMs perform in the context of non-English languages,
a gap that is critical for ensuring equity in the real-world use of these
systems.This paper provides a framework to investigate the effectiveness of
LLMs as multi-lingual dialogue systems for healthcare queries. Our
empirically-derived framework XlingEval focuses on three fundamental criteria
for evaluating LLM responses to naturalistic human-authored health-related
questions: correctness, consistency, and verifiability. Through extensive
experiments on four major global languages, including English, Spanish,
Chinese, and Hindi, spanning three expert-annotated large health Q&amp;A datasets,
and through an amalgamation of algorithmic and human-evaluation strategies, we
found a pronounced disparity in LLM responses across these languages,
indicating a need for enhanced cross-lingual capabilities. We further propose
XlingHealth, a cross-lingual benchmark for examining the multilingual
capabilities of LLMs in the healthcare context. Our findings underscore the
pressing need to bolster the cross-lingual capacities of these models, and to
provide an equitable information ecosystem accessible to all.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain. (arXiv:2310.13146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13146">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical
domain Question-answering task. The testbed includes 7.5k high-quality question
answering samples to provide a diverse and reliable benchmark. We performed a
comprehensive experimental study and evaluated several QA deep-learning models
under the proposed testbed. Despite impressive results on the original test
set, the performance degrades when applied to new test sets, which shows the
distribution shift. Our findings emphasize the need for and the potential for
increasing the robustness of clinical domain models under distributional
shifts. The testbed offers one way to track progress in that direction. It also
highlights the necessity of adopting evaluation metrics that consider
robustness to natural distribution shifts. We plan to expand the corpus by
adding more samples and model results. The full paper and the updated benchmark
are available at github.com/openlifescience-ai/clift
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13183">
<div class="article-summary-box-inner">
<span><p>It is widely acknowledged that large and sparse models have higher accuracy
than small and dense models under the same model size constraints. This
motivates us to train a large model and then remove its redundant neurons or
weights by pruning. Most existing works pruned the networks in a deterministic
way, the performance of which solely depends on a single pruning criterion and
thus lacks variety. Instead, in this paper, we propose a model pruning strategy
that first generates several pruning masks in a designed random way.
Subsequently, along with an effective mask-selection rule, the optimal mask is
chosen from the pool of mask candidates. To further enhance efficiency, we
introduce an early mask evaluation strategy, mitigating the overhead associated
with training multiple masks. Our extensive experiments demonstrate that this
approach achieves state-of-the-art performance across eight datasets from GLUE,
particularly excelling at high levels of sparsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Accurate Factual Inconsistency Detection Over Long Documents. (arXiv:2310.13189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13189">
<div class="article-summary-box-inner">
<span><p>Generative AI models exhibit remarkable potential; however, hallucinations
across various tasks present a significant challenge, particularly for longer
inputs that current approaches struggle to address effectively. We introduce
SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a
task-agnostic model for detecting factual inconsistencies using a novel
chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI)
based model that uses large text chunks to condition over long texts. This
approach achieves state-of-the-art performance in factual inconsistency
detection for diverse tasks and long inputs. Additionally, we leverage the
chunking mechanism and employ a novel algorithm to explain SCALE's decisions
through relevant source sentence retrieval. Our evaluations reveal that SCALE
outperforms existing methods on both standard benchmarks and a new long-form
dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses
competitive systems in efficiency and model explanation evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13191">
<div class="article-summary-box-inner">
<span><p>The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer's
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NameGuess: Column Name Expansion for Tabular Data. (arXiv:2310.13196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13196">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models have revolutionized many sectors,
including the database industry. One common challenge when dealing with large
volumes of tabular data is the pervasive use of abbreviated column names, which
can negatively impact performance on various data search, access, and
understanding tasks. To address this issue, we introduce a new task, called
NameGuess, to expand column names (used in database schema) as a natural
language generation problem. We create a training dataset of 384K
abbreviated-expanded column pairs using a new data fabrication method and a
human-annotated evaluation benchmark that includes 9.2K examples from
real-world tables. To tackle the complexities associated with polysemy and
ambiguity in NameGuess, we enhance auto-regressive language models by
conditioning on table content and column header names -- yielding a fine-tuned
model (with 2.7B parameters) that matches human performance. Furthermore, we
conduct a comprehensive analysis (on multiple LLMs) to validate the
effectiveness of table content in NameGuess and identify promising future
opportunities. Code has been made available at
https://github.com/amazon-science/nameguess.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Primacy Effect of ChatGPT. (arXiv:2310.13206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13206">
<div class="article-summary-box-inner">
<span><p>Instruction-tuned large language models (LLMs), such as ChatGPT, have led to
promising zero-shot performance in discriminative natural language
understanding (NLU) tasks. This involves querying the LLM using a prompt
containing the question, and the candidate labels to choose from. The
question-answering capabilities of ChatGPT arise from its pre-training on large
amounts of human-written text, as well as its subsequent fine-tuning on human
preferences, which motivates us to ask: Does ChatGPT also inherits humans'
cognitive biases? In this paper, we study the primacy effect of ChatGPT: the
tendency of selecting the labels at earlier positions as the answer. We have
two main findings: i) ChatGPT's decision is sensitive to the order of labels in
the prompt; ii) ChatGPT has a clearly higher chance to select the labels at
earlier positions as the answer. We hope that our experiments and analyses
provide additional insights into building more reliable ChatGPT-based
solutions. We release the source code at
https://github.com/wangywUST/PrimacyEffectGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition. (arXiv:2310.13213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13213">
<div class="article-summary-box-inner">
<span><p>We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition
covering 33 entity classes across 12 languages, in both monolingual and
multilingual settings. This dataset aims to tackle the following practical
challenges in NER: (i) effective handling of fine-grained classes that include
complex entities like movie titles, and (ii) performance degradation due to
noise generated from typing mistakes or OCR errors. The dataset is compiled
from open resources like Wikipedia and Wikidata, and is publicly available.
Evaluation based on the XLM-RoBERTa baseline highlights the unique challenges
posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the
scores are low with macro-F1=0.63 (across all languages), and (ii) the
corruption strategy significantly impairs performance, with entity corruption
resulting in 9% lower performance relative to non-entity corruptions across all
languages. This highlights the greater impact of entity noise in contrast to
context noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering. (arXiv:2310.13226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13226">
<div class="article-summary-box-inner">
<span><p>Blockchain technology has revolutionized the financial landscape, with
cryptocurrencies gaining widespread adoption for their decentralized and
transparent nature. As the sentiment expressed on social media platforms can
significantly influence cryptocurrency discussions and market movements,
sentiment analysis has emerged as a crucial tool for understanding public
opinion and predicting market trends. Motivated by the aim to enhance sentiment
analysis accuracy in the cryptocurrency domain, this paper investigates
fine-tuning techniques on large language models. This paper also investigates
the efficacy of supervised fine-tuning and instruction-based fine-tuning on
large language models for unseen tasks. Experimental results demonstrate a
significant average zero-shot performance gain of 40% after fine-tuning,
highlighting the potential of this technique in optimizing pre-trained language
model efficiency. Additionally, the impact of instruction tuning on models of
varying scales is examined, revealing that larger models benefit from
instruction tuning, achieving the highest average accuracy score of 75.16%. In
contrast, smaller-scale models may experience reduced generalization due to the
complete utilization of model capacity. To gain deeper insight about how
instruction works with these language models, this paper presents an
experimental investigation into the response of an instruction-based model
under different instruction tuning setups. The investigation demonstrates that
the model achieves an average accuracy score of 72.38% for short and simple
instructions. This performance significantly outperforms its accuracy under
long and complex instructions by over 12%, thereby effectively highlighting the
profound significance of instruction characteristics in maximizing model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search. (arXiv:2310.13227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13227">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated powerful decision-making and
planning capabilities in solving complicated real-world problems. LLM-based
autonomous agents can interact with diverse tools (e.g., functional APIs) and
generate solution plans that execute a series of API function calls in a
step-by-step manner. The multitude of candidate API function calls
significantly expands the action space, amplifying the critical need for
efficient action space navigation. However, existing methods either struggle
with unidirectional exploration in expansive action spaces, trapped into a
locally optimal solution, or suffer from exhaustively traversing all potential
actions, causing inefficient navigation. To address these issues, we propose
ToolChain*, an efficient tree search-based planning algorithm for LLM-based
agents. It formulates the entire action space as a decision tree, where each
node represents a possible API function call involved in a solution plan. By
incorporating the A* search algorithm with task-specific cost function design,
it efficiently prunes high-cost branches that may involve incorrect actions,
identifying the most low-cost valid path as the solution. Extensive experiments
on multiple tool-use and reasoning tasks demonstrate that ToolChain*
efficiently balances exploration and exploitation within an expansive action
space. It outperforms state-of-the-art baselines on planning and reasoning
tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Less the Merrier? Investigating Language Representation in Multilingual Models. (arXiv:2310.13228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13228">
<div class="article-summary-box-inner">
<span><p>Multilingual Language Models offer a way to incorporate multiple languages in
one model and utilize cross-language transfer learning to improve performance
for different Natural Language Processing (NLP) tasks. Despite progress in
multilingual models, not all languages are supported as well, particularly in
low-resource settings. In this work, we investigate the linguistic
representation of different languages in multilingual models. We start by
asking the question which languages are supported in popular multilingual
models and which languages are left behind. Then, for included languages, we
look at models' learned representations based on language family and dialect
and try to understand how models' learned representations for~(1) seen and~(2)
unseen languages vary across different language groups. In addition, we test
and analyze performance on downstream tasks such as text generation and Named
Entity Recognition. We observe from our experiments that community-centered
models -- models that focus on languages of a given family or geographical
location and are built by communities who speak them -- perform better at
distinguishing between languages in the same family for low-resource languages.
Our paper contributes to the literature in understanding multilingual models
and their shortcomings and offers insights on potential ways to improve them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Contrastive Learning for Script-based Character Understanding. (arXiv:2310.13231v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13231">
<div class="article-summary-box-inner">
<span><p>In this work, we tackle the scenario of understanding characters in scripts,
which aims to learn the characters' personalities and identities from their
utterances. We begin by analyzing several challenges in this scenario, and then
propose a multi-level contrastive learning framework to capture characters'
global information in a fine-grained manner. To validate the proposed
framework, we conduct extensive experiments on three character understanding
sub-tasks by comparing with strong pre-trained language models, including
SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate
that our method improves the performances by a considerable margin. Through
further in-depth analysis, we show the effectiveness of our method in
addressing the challenges and provide more hints on the scenario of character
understanding. We will open-source our work on github at
https://github.com/David-Li0406/Script-based-Character-Understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking. (arXiv:2310.13243v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13243">
<div class="article-summary-box-inner">
<span><p>In the field of information retrieval, Query Likelihood Models (QLMs) rank
documents based on the probability of generating the query given the content of
a document. Recently, advanced large language models (LLMs) have emerged as
effective QLMs, showcasing promising ranking capabilities. This paper focuses
on investigating the genuine zero-shot ranking effectiveness of recent LLMs,
which are solely pre-trained on unstructured text data without supervised
instruction fine-tuning. Our findings reveal the robust zero-shot ranking
ability of such LLMs, highlighting that additional instruction fine-tuning may
hinder effectiveness unless a question generation task is present in the
fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking
system that integrates LLM-based QLMs with a hybrid zero-shot retriever,
demonstrating exceptional effectiveness in both zero-shot and few-shot
scenarios. We make our codebase publicly available at
https://github.com/ielab/llm-qlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches. (arXiv:2310.13247v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13247">
<div class="article-summary-box-inner">
<span><p>Anomaly detection in command shell sessions is a critical aspect of computer
security. Recent advances in deep learning and natural language processing,
particularly transformer-based models, have shown great promise for addressing
complex security challenges. In this paper, we implement a comprehensive
approach to detect anomalies in Unix shell sessions using a pretrained
DistilBERT model, leveraging both unsupervised and supervised learning
techniques to identify anomalous activity while minimizing data labeling. The
unsupervised method captures the underlying structure and syntax of Unix shell
commands, enabling the detection of session deviations from normal behavior.
Experiments on a large-scale enterprise dataset collected from production
systems demonstrate the effectiveness of our approach in detecting anomalous
behavior in Unix shell sessions. This work highlights the potential of
leveraging recent advances in transformers to address important computer
security challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Grounding Helps Learn Word Meanings in Low-Data Regimes. (arXiv:2310.13257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13257">
<div class="article-summary-box-inner">
<span><p>Modern neural language models (LMs) are powerful tools for modeling human
sentence production and comprehension, and their internal representations are
remarkably well-aligned with representations of language in the human brain.
But to achieve these results, LMs must be trained in distinctly un-human-like
ways -- requiring orders of magnitude more language data than children receive
during development, and without any of the accompanying grounding in
perception, action, or social behavior. Do models trained more naturalistically
-- with grounded supervision -- exhibit more human-like language learning? We
investigate this question in the context of word learning, a key sub-task in
language acquisition. We train a diverse set of LM architectures, with and
without auxiliary supervision from image captioning tasks, on datasets of
varying scales. We then evaluate these models on a broad set of benchmarks
characterizing models' learning of syntactic categories, lexical relations,
semantic features, semantic similarity, and alignment with human neural
representations. We find that visual supervision can indeed improve the
efficiency of word learning. However, these improvements are limited: they are
present almost exclusively in the low-data regime, and sometimes canceled out
by the inclusion of rich distributional signals from text. The information
conveyed by text and images is not redundant -- we find that models mainly
driven by visual information yield qualitatively different from those mainly
driven by word co-occurrences. However, our results suggest that current
multi-modal modeling approaches fail to effectively leverage visual information
to build more human-like word representations from human-sized datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Quality-based Syntactic Template Retriever for Syntactically-controlled Paraphrase Generation. (arXiv:2310.13262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13262">
<div class="article-summary-box-inner">
<span><p>Existing syntactically-controlled paraphrase generation (SPG) models perform
promisingly with human-annotated or well-chosen syntactic templates. However,
the difficulty of obtaining such templates actually hinders the practical
application of SPG models. For one thing, the prohibitive cost makes it
unfeasible to manually design decent templates for every source sentence. For
another, the templates automatically retrieved by current heuristic methods are
usually unreliable for SPG models to generate qualified paraphrases. To escape
this dilemma, we propose a novel Quality-based Syntactic Template Retriever
(QSTR) to retrieve templates based on the quality of the to-be-generated
paraphrases. Furthermore, for situations requiring multiple paraphrases for
each source sentence, we design a Diverse Templates Search (DTS) algorithm,
which can enhance the diversity between paraphrases without sacrificing
quality. Experiments demonstrate that QSTR can significantly surpass existing
retrieval methods in generating high-quality paraphrases and even perform
comparably with human-annotated templates in terms of reference-free metrics.
Additionally, human evaluation and the performance on downstream tasks using
our generated paraphrases for data augmentation showcase the potential of our
QSTR and DTS algorithm in practical scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model. (arXiv:2310.13265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13265">
<div class="article-summary-box-inner">
<span><p>Multi-modal open-domain question answering typically requires evidence
retrieval from databases across diverse modalities, such as images, tables,
passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this
task. To enable LLMs to tackle the task in a zero-shot manner, we introduce
MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer
strategy that bypasses intricate multi-modality ranking, our framework can
accommodate new modalities and seamlessly transition to new models for the
task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each
modality separately, then fuses this multi-modal information using LLMs to
produce a final answer. Our methodology boosts performance on the MMCoQA
dataset, improving F1 by +37.91 points and EM by +34.07 points over the
supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the
zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and
significantly closes the gap with supervised methods. Our codebase is available
at https://github.com/lezhang7/MOQAGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Language Encoder of Contrastive Cross-modal Models. (arXiv:2310.13267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13267">
<div class="article-summary-box-inner">
<span><p>Contrastive cross-modal models such as CLIP and CLAP aid various
vision-language (VL) and audio-language (AL) tasks. However, there has been
limited investigation of and improvement in their language encoder, which is
the central component of encoding natural language descriptions of image/audio
into vector representations. We extensively evaluate how unsupervised and
supervised sentence embedding training affect language encoder quality and
cross-modal task performance. In VL pretraining, we found that sentence
embedding training language encoder quality and aids in cross-modal tasks,
improving contrastive VL models such as CyCLIP. In contrast, AL pretraining
benefits less from sentence embedding training, which may result from the
limited amount of pretraining data. We analyze the representation spaces to
understand the strengths of sentence embedding training, and find that it
improves text-space uniformity, at the cost of decreased cross-modal alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13276">
<div class="article-summary-box-inner">
<span><p>Over recent decades, significant advancements in cross-modal retrieval are
mainly driven by breakthroughs in visual and linguistic modeling. However, a
recent study shows that multi-modal data representations tend to cluster within
a limited convex cone (as representation degeneration problem), which hinders
retrieval performance due to the inseparability of these representations. In
our study, we first empirically validate the presence of the representation
degeneration problem across multiple cross-modal benchmarks and methods. Next,
to address it, we introduce a novel method, called InvGC, a post-processing
technique inspired by graph convolution and average pooling. Specifically,
InvGC defines the graph topology within the datasets and then applies graph
convolution in a subtractive manner. This method effectively separates
representations by increasing the distances between data points. To improve the
efficiency and effectiveness of InvGC, we propose an advanced graph topology,
LocalAdj, which only aims to increase the distances between each data point and
its nearest neighbors. To understand why InvGC works, we present a detailed
theoretical analysis, proving that the lower bound of recall will be improved
after deploying InvGC. Extensive empirical results show that InvGC and InvGC
w/LocalAdj significantly mitigate the representation degeneration problem,
thereby enhancing retrieval performance.
</p>
<p>Our code is available at
https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SALMONN: Towards Generic Hearing Abilities for Large Language Models. (arXiv:2310.13289v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13289">
<div class="article-summary-box-inner">
<span><p>Hearing is arguably an essential ability of artificial intelligence (AI)
agents in the physical world, which refers to the perception and understanding
of general auditory information consisting of at least three types of sounds:
speech, audio events, and music. In this paper, we propose SALMONN, a speech
audio language music open neural network, built by integrating a pre-trained
text-based large language model (LLM) with speech and audio encoders into a
single multimodal model. SALMONN enables the LLM to directly process and
understand general audio inputs and achieve competitive performances on a
number of speech and audio tasks used in training, such as automatic speech
recognition and translation, auditory-information-based question answering,
emotion recognition, speaker verification, and music and audio captioning
\textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in
the training, which includes but is not limited to speech translation to
untrained languages, speech-based slot filling, spoken-query-based question
answering, audio-based storytelling, and speech audio co-reasoning
\textit{etc}. The presence of the cross-modal emergent abilities is studied,
and a novel few-shot activation tuning approach is proposed to activate such
abilities of SALMONN. To our knowledge, SALMONN is the first model of its type
and can be regarded as a step towards AI with generic hearing abilities. An
interactive demo of SALMONN is available at
\texttt{\url{https://github.com/bytedance/SALMONN}}, and the training code and
model checkpoints will be released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Indirect Answers to Yes-No Questions in Multiple Languages. (arXiv:2310.13290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13290">
<div class="article-summary-box-inner">
<span><p>Yes-no questions expect a yes or no for an answer, but people often skip
polar keywords. Instead, they answer with long explanations that must be
interpreted. In this paper, we focus on this challenging problem and release
new benchmarks in eight languages. We present a distant supervision approach to
collect training data. We also demonstrate that direct answers (i.e., with
polar keywords) are useful to train models to interpret indirect answers (i.e.,
without polar keywords). Experimental results demonstrate that monolingual
fine-tuning is beneficial if training data can be obtained via distant
supervision for the language of interest (5 languages). Additionally, we show
that cross-lingual fine-tuning is always beneficial (8 languages).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks. (arXiv:2310.13291v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13291">
<div class="article-summary-box-inner">
<span><p>Large language models have revolutionized the field of NLP by achieving
state-of-the-art performance on various tasks. However, there is a concern that
these models may disclose information in the training data. In this study, we
focus on the summarization task and investigate the membership inference (MI)
attack: given a sample and black-box access to a model's API, it is possible to
determine if the sample was part of the training data. We exploit text
similarity and the model's resistance to document modifications as potential MI
signals and evaluate their effectiveness on widely used datasets. Our results
demonstrate that summarization models are at risk of exposing data membership,
even in cases where the reference summary is not available. Furthermore, we
discuss several safeguards for training summarization models to protect against
MI attacks and discuss the inherent trade-off between privacy and utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting. (arXiv:2310.13297v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13297">
<div class="article-summary-box-inner">
<span><p>Automatic response forecasting for news media plays a crucial role in
enabling content producers to efficiently predict the impact of news releases
and prevent unexpected negative outcomes such as social conflict and moral
injury. To effectively forecast responses, it is essential to develop measures
that leverage the social dynamics and contextual information surrounding
individuals, especially in cases where explicit profiles or historical actions
of the users are limited (referred to as lurkers). As shown in a previous
study, 97% of all tweets are produced by only the most active 25% of users.
However, existing approaches have limited exploration of how to best process
and utilize these important features. To address this gap, we propose a novel
framework, named SocialSense, that leverages a large language model to induce a
belief-centered graph on top of an existent social network, along with
graph-based propagation to capture social dynamics. We hypothesize that the
induced graph that bridges the gap between distant users who share similar
beliefs allows the model to effectively capture the response patterns. Our
method surpasses existing state-of-the-art in experimental evaluations for both
zero-shot and supervised settings, demonstrating its effectiveness in response
forecasting. Moreover, the analysis reveals the framework's capability to
effectively handle unseen user and lurker scenarios, further highlighting its
robustness and practical applicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-Time Self-Adaptive Small Language Models for Question Answering. (arXiv:2310.13307v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13307">
<div class="article-summary-box-inner">
<span><p>Recent instruction-finetuned large language models (LMs) have achieved
notable performances in various tasks, such as question-answering (QA).
However, despite their ability to memorize a vast amount of general knowledge
across diverse tasks, they might be suboptimal on specific tasks due to their
limited capacity to transfer and adapt knowledge to target tasks. Moreover,
further finetuning LMs with labeled datasets is often infeasible due to their
absence, but it is also questionable if we can transfer smaller LMs having
limited knowledge only with unlabeled test data. In this work, we show and
investigate the capabilities of smaller self-adaptive LMs, only with unlabeled
test data. In particular, we first stochastically generate multiple answers,
and then ensemble them while filtering out low-quality samples to mitigate
noise from inaccurate labels. Our proposed self-adaption strategy demonstrates
significant performance improvements on benchmark QA datasets with higher
robustness across diverse prompts, enabling LMs to stay stable. Code is
available at: https://github.com/starsuzi/T-SAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models. (arXiv:2310.13312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13312">
<div class="article-summary-box-inner">
<span><p>Over the past few years, various domain-specific pretrained language models
(PLMs) have been proposed and have outperformed general-domain PLMs in
specialized areas such as biomedical, scientific, and clinical domains. In
addition, financial PLMs have been studied because of the high economic impact
of financial data analysis. However, we found that financial PLMs were not
pretrained on sufficiently diverse financial data. This lack of diverse
training data leads to a subpar generalization performance, resulting in
general-purpose PLMs, including BERT, often outperforming financial PLMs on
many downstream tasks. To address this issue, we collected a broad range of
financial corpus and trained the Financial Language Model (FiLM) on these
diverse datasets. Our experimental results confirm that FiLM outperforms not
only existing financial PLMs but also general domain PLMs. Furthermore, we
provide empirical evidence that this improvement can be achieved even for
unseen corpus groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models. (arXiv:2310.13315v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13315">
<div class="article-summary-box-inner">
<span><p>Quantization is a promising approach for reducing memory overhead and
accelerating inference, especially in large pre-trained language model (PLM)
scenarios. While having no access to original training data due to security and
privacy concerns has emerged the demand for zero-shot quantization. Most of the
cutting-edge zero-shot quantization methods primarily 1) apply to computer
vision tasks, and 2) neglect of overfitting problem in the generative
adversarial learning process, leading to sub-optimal performance. Motivated by
this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)
framework for the zero-shot quantization of various PLMs. The key algorithm in
solving ZSAQ is the SAM-SGA optimization, which aims to improve the
quantization accuracy and model generalization via optimizing a minimax
problem. We theoretically prove the convergence rate for the minimax
optimization problem and this result can be applied to other nonconvex-PL
minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate
that our method brings consistent and significant performance gains on both
discriminative and generative PLMs, i.e., up to +6.98 average score.
Furthermore, we empirically validate that our method can effectively improve
the model generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Dual Encoders are Better Frame Identification Learners. (arXiv:2310.13316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13316">
<div class="article-summary-box-inner">
<span><p>Frame identification aims to find semantic frames associated with target
words in a sentence. Recent researches measure the similarity or matching score
between targets and candidate frames by modeling frame definitions. However,
they either lack sufficient representation learning of the definitions or face
challenges in efficiently selecting the most suitable frame from over 1000
candidate frames. Moreover, commonly used lexicon filtering ($lf$) to obtain
candidate frames for the target may ignore out-of-vocabulary targets and cause
inadequate frame modeling. In this paper, we propose CoFFTEA, a
$\underline{Co}$arse-to-$\underline{F}$ine $\underline{F}$rame and
$\underline{T}$arget $\underline{E}$ncoders $\underline{A}$rchitecture. With
contrastive learning and dual encoders, CoFFTEA efficiently and effectively
models the alignment between frames and targets. By employing a coarse-to-fine
curriculum learning procedure, CoFFTEA gradually learns to differentiate frames
with varying degrees of similarity. Experimental results demonstrate that
CoFFTEA outperforms previous models by 0.93 overall scores and 1.53 R@1 without
$lf$. Further analysis suggests that CoFFTEA can better model the relationships
between frame and frame, as well as target and target. The code for our
approach is available at https://github.com/pkunlp-icler/COFFTEA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting. (arXiv:2310.13321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13321">
<div class="article-summary-box-inner">
<span><p>Recent studies have revealed that grammatical error correction methods in the
sequence-to-sequence paradigm are vulnerable to adversarial attack, and simply
utilizing adversarial examples in the pre-training or post-training process can
significantly enhance the robustness of GEC models to certain types of attack
without suffering too much performance loss on clean data. In this paper, we
further conduct a thorough robustness evaluation of cutting-edge GEC methods
for four different types of adversarial attacks and propose a simple yet very
effective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the
augmenting data from the GEC models themselves in the post-training process and
introducing regularization data for cycle training, our proposed method can
effectively improve the model robustness of well-trained GEC models with only a
few more training epochs as an extra cost. More concretely, further training on
the regularization data can prevent the GEC models from over-fitting on
easy-to-learn samples and thus can improve the generalization capability and
robustness towards unseen data (adversarial noise/samples). Meanwhile, the
self-augmented data can provide more high-quality pseudo pairs to improve model
performance on the original testing data. Experiments on four benchmark
datasets and seven strong models indicate that our proposed training method can
significantly enhance the robustness of four types of attacks without using
purposely built adversarial examples in training. Evaluation results on clean
data further confirm that our proposed CSA method significantly improves the
performance of four baselines and yields nearly comparable results with other
state-of-the-art models. Our code is available at
https://github.com/ZetangForward/CSA-GEC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratizing Reasoning Ability: Tailored Learning from Large Language Model. (arXiv:2310.13332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13332">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) exhibit impressive emergent abilities in natural
language processing, but their democratization is hindered due to huge
computation requirements and closed-source nature. Recent research on advancing
open-source smaller LMs by distilling knowledge from black-box LLMs has
obtained promising results in the instruction-following ability. However, the
reasoning ability which is more challenging to foster, is relatively rarely
explored. In this paper, we propose a tailored learning approach to distill
such reasoning ability to smaller LMs to facilitate the democratization of the
exclusive reasoning ability. In contrast to merely employing LLM as a data
annotator, we exploit the potential of LLM as a reasoning teacher by building
an interactive multi-round learning paradigm. This paradigm enables the student
to expose its deficiencies to the black-box teacher who then can provide
customized training data in return. Further, to exploit the reasoning potential
of the smaller LM, we propose self-reflection learning to motivate the student
to learn from self-made mistakes. The learning from self-reflection and LLM are
all tailored to the student's learning status, thanks to the seamless
integration with the multi-round learning paradigm. Comprehensive experiments
and analysis on mathematical and commonsense reasoning tasks demonstrate the
effectiveness of our method. The code will be available at
https://github.com/Raibows/Learn-to-Reason.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets. (arXiv:2310.13340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13340">
<div class="article-summary-box-inner">
<span><p>Opinion summarization is expected to digest larger review sets and provide
summaries from different perspectives. However, most existing solutions are
deficient in epitomizing extensive reviews and offering opinion summaries from
various angles due to the lack of designs for information selection. To this
end, we propose SUBSUMM, a supervised summarization framework for large-scale
multi-perspective opinion summarization. SUBSUMM consists of a review sampling
strategy set and a two-stage training scheme. The sampling strategies take
sentiment orientation and contrastive information value into consideration,
with which the review subsets from different perspectives and quality levels
can be selected. Subsequently, the summarizer is encouraged to learn from the
sub-optimal and optimal subsets successively in order to capitalize on the
massive input. Experimental results on AmaSum and Rotten Tomatoes datasets
demonstrate that SUBSUMM is adept at generating pros, cons, and verdict
summaries from hundreds of input reviews. Furthermore, our in-depth analysis
verifies that the advanced selection of review subsets and the two-stage
training scheme are vital to boosting the summarization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs). (arXiv:2310.13343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13343">
<div class="article-summary-box-inner">
<span><p>With the development of large language models (LLMs) like the GPT series,
their widespread use across various application scenarios presents a myriad of
challenges. This review initially explores the issue of domain specificity,
where LLMs may struggle to provide precise answers to specialized questions
within niche fields. The problem of knowledge forgetting arises as these LLMs
might find it hard to balance old and new information. The knowledge repetition
phenomenon reveals that sometimes LLMs might deliver overly mechanized
responses, lacking depth and originality. Furthermore, knowledge illusion
describes situations where LLMs might provide answers that seem insightful but
are actually superficial, while knowledge toxicity focuses on harmful or biased
information outputs. These challenges underscore problems in the training data
and algorithmic design of LLMs. To address these issues, it's suggested to
diversify training data, fine-tune models, enhance transparency and
interpretability, and incorporate ethics and fairness training. Future
technological trends might lean towards iterative methodologies, multimodal
learning, model personalization and customization, and real-time learning and
feedback mechanisms. In conclusion, future LLMs should prioritize fairness,
transparency, and ethics, ensuring they uphold high moral and ethical standards
when serving humanity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Cognitive Plausibility of Subword Tokenization. (arXiv:2310.13348v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13348">
<div class="article-summary-box-inner">
<span><p>Subword tokenization has become the de-facto standard for tokenization,
although comparative evaluations of subword vocabulary quality across languages
are scarce. Existing evaluation studies focus on the effect of a tokenization
algorithm on the performance in downstream tasks, or on engineering criteria
such as the compression rate. We present a new evaluation paradigm that focuses
on the cognitive plausibility of subword tokenization. We analyze the
correlation of the tokenizer output with the response time and accuracy of
human performance on a lexical decision task. We compare three tokenization
algorithms across several languages and vocabulary sizes. Our results indicate
that the UnigramLM algorithm yields less cognitively plausible tokenization
behavior and a worse coverage of derivational morphemes, in contrast with prior
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation. (arXiv:2310.13361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13361">
<div class="article-summary-box-inner">
<span><p>Multimodal machine translation (MMT) simultaneously takes the source sentence
and a relevant image as input for translation. Since there is no paired image
available for the input sentence in most cases, recent studies suggest
utilizing powerful text-to-image generation models to provide image inputs.
Nevertheless, synthetic images generated by these models often follow different
distributions compared to authentic images. Consequently, using authentic
images for training and synthetic images for inference can introduce a
distribution shift, resulting in performance degradation during inference. To
tackle this challenge, in this paper, we feed synthetic and authentic images to
the MMT model, respectively. Then we minimize the gap between the synthetic and
authentic images by drawing close the input image representations of the
Transformer Encoder and the output distributions of the Transformer Decoder.
Therefore, we mitigate the distribution disparity introduced by the synthetic
images during inference, thereby freeing the authentic images from the
inference process.Experimental results show that our approach achieves
state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while
remaining independent of authentic images during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards General Error Diagnosis via Behavioral Testing in Machine Translation. (arXiv:2310.13362v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13362">
<div class="article-summary-box-inner">
<span><p>Behavioral testing offers a crucial means of diagnosing linguistic errors and
assessing capabilities of NLP models. However, applying behavioral testing to
machine translation (MT) systems is challenging as it generally requires human
efforts to craft references for evaluating the translation quality of such
systems on newly generated test cases. Existing works in behavioral testing of
MT systems circumvent this by evaluating translation quality without
references, but this restricts diagnosis to specific types of errors, such as
incorrect translation of single numeric or currency words. In order to diagnose
general errors, this paper proposes a new Bilingual Translation Pair Generation
based Behavior Testing (BTPGBT) framework for conducting behavioral testing of
MT systems. The core idea of BTPGBT is to employ a novel bilingual translation
pair generation (BTPG) approach that automates the construction of high-quality
test cases and their pseudoreferences. Experimental results on various MT
systems demonstrate that BTPGBT could provide comprehensive and accurate
behavioral testing results for general error diagnosis, which further leads to
several insightful findings. Our code and data are available at https:
//github.com/wujunjie1998/BTPGBT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training. (arXiv:2310.13377v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13377">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel human-robot interaction setup for robot and human
learning of symbolic language for identifying robot homeostatic needs. The
robot and human learn to use and respond to the same language symbols that
convey homeostatic needs and the stimuli that satisfy the homeostatic needs,
respectively. We adopted a differential outcomes training (DOT) protocol
whereby the robot provides feedback specific (differential) to its internal
needs (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We
found evidence that DOT can enhance the human's learning efficiency, which in
turn enables more efficient robot language acquisition. The robot used in the
study has a vocabulary similar to that of a human infant in the linguistic
``babbling'' phase. The robot software architecture is built upon a model for
affect-grounded language acquisition where the robot associates vocabulary with
internal needs (hunger, thirst, curiosity) through interactions with the human.
The paper presents the results of an initial pilot study conducted with the
interactive setup, which reveal that the robot's language acquisition achieves
higher convergence rate in the DOT condition compared to the non-DOT control
condition. Additionally, participants reported positive affective experiences,
feeling of being in control, and an empathetic connection with the robot. This
mutual learning (teacher-student learning) approach offers a potential
contribution of facilitating cognitive interventions with DOT (e.g. for people
with dementia) through increased therapy adherence as a result of engaging
humans more in training tasks by taking an active teaching-learning role. The
homeostatic motivational grounding of the robot's language acquisition has
potential to contribute to more ecologically valid and social
(collaborative/nurturing) interactions with robots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection. (arXiv:2310.13380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13380">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-domain (OOD) intents from user queries is essential for a
task-oriented dialogue system. Previous OOD detection studies generally work on
the assumption that plenty of labeled IND intents exist. In this paper, we
focus on a more practical few-shot OOD setting where there are only a few
labeled IND data and massive unlabeled mixed data that may belong to IND or
OOD. The new scenario carries two key challenges: learning discriminative
representations using limited IND data and leveraging unlabeled mixed data.
Therefore, we propose an adaptive prototypical pseudo-labeling (APP) method for
few-shot OOD detection, including a prototypical OOD detection framework
(ProtoOOD) to facilitate low-resource OOD detection using limited IND data, and
an adaptive pseudo-labeling method to produce high-quality pseudo OOD\&amp;IND
labels. Extensive experiments and analysis demonstrate the effectiveness of our
method for few-shot OOD detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tuna: Instruction Tuning using Feedback from Large Language Models. (arXiv:2310.13385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13385">
<div class="article-summary-box-inner">
<span><p>Instruction tuning of open-source large language models (LLMs) like LLaMA,
using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4,
has proven to be a cost-effective way to align model behaviors with human
preferences. However, the instruction-tuned model has only seen one response
per instruction, lacking the knowledge of potentially better responses. In this
paper, we propose finetuning an instruction-tuned LLM using our novel
\textit{probabilistic ranking} and \textit{contextual ranking} approaches to
increase the likelihood of generating better responses. Probabilistic ranking
enables the instruction-tuned model to inherit the relative rankings of
high-quality and low-quality responses from the teacher LLM. On the other hand,
learning with contextual ranking allows the model to refine its own response
distribution using the contextual understanding ability of stronger LLMs.
Furthermore, we apply probabilistic ranking and contextual ranking sequentially
to the instruction-tuned LLM. The resulting model, which we call \textbf{Tuna},
consistently improves the performance on Super Natural Instructions (119 test
tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results
than several strong reinforcement learning baselines. Our code and data are
available at \url{ https://github.com/microsoft/LMOps}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POSQA: Probe the World Models of LLMs with Size Comparisons. (arXiv:2310.13394v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13394">
<div class="article-summary-box-inner">
<span><p>Embodied language comprehension emphasizes that language understanding is not
solely a matter of mental processing in the brain but also involves
interactions with the physical and social environment. With the explosive
growth of Large Language Models (LLMs) and their already ubiquitous presence in
our daily lives, it is becoming increasingly necessary to verify their
real-world understanding. Inspired by cognitive theories, we propose POSQA: a
Physical Object Size Question Answering dataset with simple size comparison
questions to examine the extremity and analyze the potential mechanisms of the
embodied comprehension of the latest LLMs.
</p>
<p>We show that even the largest LLMs today perform poorly under the zero-shot
setting. We then push their limits with advanced prompting techniques and
external knowledge augmentation. Furthermore, we investigate whether their
real-world comprehension primarily derives from contextual information or
internal weights and analyse the impact of prompt formats and report bias of
different objects. Our results show that real-world understanding that LLMs
shaped from textual data can be vulnerable to deception and confusion by the
surface form of prompts, which makes it less aligned with human behaviours.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models. (arXiv:2310.13395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13395">
<div class="article-summary-box-inner">
<span><p>Prompting Large Language Models (LLMs) performs impressively in zero- and
few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot
afford the cost of creating large task-specific training datasets, but also the
cost of pretraining their own LLMs, are increasingly turning to third-party
services that allow them to prompt LLMs. However, such services currently
require a payment per call, which becomes a significant operating expense
(OpEx). Furthermore, customer inputs are often very similar over time, hence
SMEs end-up prompting LLMs with very similar instances. We propose a framework
that allows reducing the calls to LLMs by caching previous LLM responses and
using them to train a local inexpensive model on the SME side. The framework
includes criteria for deciding when to trust the local model or call the LLM,
and a methodology to tune the criteria and measure the tradeoff between
performance and cost. For experimental purposes, we instantiate our framework
with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN
classifier or a Multi-Layer Perceptron, using two common business tasks, intent
recognition and sentiment analysis. Experimental results indicate that
significant OpEx savings can be obtained with only slightly lower performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading. (arXiv:2310.13409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13409">
<div class="article-summary-box-inner">
<span><p>Conversational Machine Reading (CMR) requires answering a user's initial
question through multi-turn dialogue interactions based on a given document.
Although there exist many effective methods, they largely neglected the
alignment between the document and the user-provided information, which
significantly affects the intermediate decision-making and subsequent follow-up
question generation. To address this issue, we propose a pipeline framework
that (1) aligns the aforementioned two sides in an explicit way, (2)makes
decisions using a lightweight many-to-many entailment reasoning module, and (3)
directly generates follow-up questions based on the document and previously
asked questions. Our proposed method achieves state-of-the-art in
micro-accuracy and ranks the first place on the public leaderboard of the CMR
benchmark dataset ShARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Enhancing Relational Rules for Knowledge Graph Link Prediction. (arXiv:2310.13411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13411">
<div class="article-summary-box-inner">
<span><p>Graph neural networks (GNNs) have shown promising performance for knowledge
graph reasoning. A recent variant of GNN called progressive relational graph
neural network (PRGNN), utilizes relational rules to infer missing knowledge in
relational digraphs and achieves notable results. However, during reasoning
with PRGNN, two important properties are often overlooked: (1) the
sequentiality of relation composition, where the order of combining different
relations affects the semantics of the relational rules, and (2) the lagged
entity information propagation, where the transmission speed of required
information lags behind the appearance speed of new entities. Ignoring these
properties leads to incorrect relational rule learning and decreased reasoning
accuracy. To address these issues, we propose a novel knowledge graph reasoning
approach, the Relational rUle eNhanced Graph Neural Network (RUN-GNN).
Specifically, RUN-GNN employs a query related fusion gate unit to model the
sequentiality of relation composition and utilizes a buffering update mechanism
to alleviate the negative effect of lagged entity information propagation,
resulting in higher-quality relational rule learning. Experimental results on
multiple datasets demonstrate the superiority of RUN-GNN is superior on both
transductive and inductive link prediction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations. (arXiv:2310.13420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13420">
<div class="article-summary-box-inner">
<span><p>In the field of natural language processing, open-domain chatbots have
emerged as an important research topic. However, a major limitation of existing
open-domain chatbot research is its singular focus on short single-session
dialogue, neglecting the potential need for understanding contextual
information in multiple consecutive sessions that precede an ongoing dialogue.
Among the elements that compose the context in multi-session conversation
settings, the time intervals between sessions and the relationships between
speakers would be particularly important. Despite their importance, current
research efforts have not sufficiently addressed these dialogical components.
In this paper, we introduce a new 1M multi-session dialogue dataset, called
Conversation Chronicles, for implementing a long-term conversation setup in
which time intervals and fine-grained speaker relationships are incorporated.
Following recent works, we exploit a large language model to produce the data.
The extensive human evaluation shows that dialogue episodes in Conversation
Chronicles reflect those properties while maintaining coherent and consistent
interactions across all the sessions. We also propose a dialogue model, called
ReBot, which consists of chronological summarization and dialogue generation
modules using only around 630M parameters. When trained on Conversation
Chronicles, ReBot demonstrates long-term context understanding with a high
human engagement score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Consistency of Large Language Models under Ambiguity. (arXiv:2310.13439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13439">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) that do not give consistent answers across
contexts are problematic when used for tasks with expectations of consistency,
e.g., question-answering, explanations, etc. Our work presents an evaluation
benchmark for self-consistency in cases of under-specification where two or
more answers can be correct. We conduct a series of behavioral experiments on
the OpenAI model suite using an ambiguous integer sequence completion task. We
find that average consistency ranges from 67\% to 82\%, far higher than would
be predicted if a model's consistency was random, and increases as model
capability improves. Furthermore, we show that models tend to maintain
self-consistency across a series of robustness checks, including prompting
speaker changes and sequence length changes. These results suggest that
self-consistency arises as an emergent capability without specifically training
for it. Despite this, we find that models are uncalibrated when judging their
own consistency, with models displaying both over- and under-confidence. We
also propose a nonparametric test for determining from token output
distribution whether a model assigns non-trivial probability to alternative
answers. Using this test, we find that despite increases in self-consistency,
models usually place significant weight on alternative, inconsistent answers.
This distribution of probability mass provides evidence that even highly
self-consistent models internally compute multiple possible responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Past, Present, and Future of Typological Databases in NLP. (arXiv:2310.13440v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13440">
<div class="article-summary-box-inner">
<span><p>Typological information has the potential to be beneficial in the development
of NLP models, particularly for low-resource languages. Unfortunately, current
large-scale typological databases, notably WALS and Grambank, are inconsistent
both with each other and with other sources of typological information, such as
linguistic grammars. Some of these inconsistencies stem from coding errors or
linguistic variation, but many of the disagreements are due to the discrete
categorical nature of these databases. We shed light on this issue by
systematically exploring disagreements across typological databases and
resources, and their uses in NLP, covering the past and present. We next
investigate the future of such work, offering an argument that a continuous
view of typological features is clearly beneficial, echoing recommendations
from linguistics. We propose that such a view of typology has significant
potential in the future, including in language modeling in low-resource
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13447">
<div class="article-summary-box-inner">
<span><p>Within the multimodal field, the key to integrating vision and language lies
in establishing a good alignment strategy. Recently, benefiting from the
success of self-supervised learning, significant progress has been made in
multimodal semantic representation based on pre-trained models for vision and
language. However, there is still room for improvement in visual semantic
representation. The lack of spatial semantic coherence and vulnerability to
noise makes it challenging for current pixel or patch-based methods to
accurately extract complex scene boundaries. To this end, this paper develops
superpixel as a comprehensive compact representation of learnable image data,
which effectively reduces the number of visual primitives for subsequent
processing by clustering perceptually similar pixels. To mine more precise
topological relations, we propose a Multiscale Difference Graph Convolutional
Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical
structure of constituent visual patterns, and captures multiscale features by
progressively merging adjacent superpixels as graph nodes. Moreover, we predict
the differences between adjacent nodes through the graph structure,
facilitating key information aggregation of graph nodes to reason actual
semantic relations. Afterward, we design a multi-level fusion rule in a
bottom-up manner to avoid understanding deviation by learning complementary
spatial information at different regional scales. Our proposed method can be
well applied to multiple downstream task learning. Extensive experiments
demonstrate that our method is competitive with other state-of-the-art methods
in visual reasoning. Our code will be released upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning. (arXiv:2310.13448v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13448">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are a promising avenue for machine translation
(MT). However, current LLM-based MT systems are brittle: their effectiveness
highly depends on the choice of few-shot examples and they often require extra
post-processing due to overgeneration. Alternatives such as finetuning on
translation instructions are computationally expensive and may weaken
in-context learning capabilities, due to overspecialization. In this paper, we
provide a closer look at this problem. We start by showing that adapter-based
finetuning with LoRA matches the performance of traditional finetuning while
reducing the number of training parameters by a factor of 50. This method also
outperforms few-shot prompting and eliminates the need for post-processing or
in-context examples. However, we show that finetuning generally degrades
few-shot performance, hindering adaptation capabilities. Finally, to obtain the
best of both worlds, we propose a simple approach that incorporates few-shot
examples during finetuning. Experiments on 10 language pairs show that our
proposed approach recovers the original few-shot capabilities while keeping the
added benefits of finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13469">
<div class="article-summary-box-inner">
<span><p>Transformer models have demonstrated remarkable performance in neural machine
translation (NMT). However, their vulnerability to noisy input poses a
significant challenge in practical implementation, where generating clean
output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used
as a benchmark for evaluating the robustness of NMT models against noisy input.
Nevertheless, its utility is limited due to the presence of noise in both the
source and target sentences. To address this limitation, we focus on cleaning
the noise from the target sentences in MTNT, making it more suitable as a
benchmark for noise evaluation. Leveraging the capabilities of large language
models (LLMs), we observe their impressive abilities in noise removal. For
example, they can remove emojis while considering their semantic meaning.
Additionally, we show that LLM can effectively rephrase slang, jargon, and
profanities. The resulting datasets, called C-MTNT, exhibit significantly less
noise in the target sentences while preserving the semantic integrity of the
original sentences. Our human and GPT-4 evaluations also lead to a consistent
conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT
showcased its effectiveness in evaluating the robustness of NMT models,
highlighting the potential of advanced language models for data cleaning and
emphasizing C-MTNT as a valuable resource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. (arXiv:2310.13486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13486">
<div class="article-summary-box-inner">
<span><p>Finding the best way of adapting pre-trained language models to a task is a
big challenge in current NLP. Just like the previous generation of task-tuned
models (TT), models that are adapted to tasks via in-context-learning (ICL) are
robust in some setups but not in others. Here, we present a detailed analysis
of which design choices cause instabilities and inconsistencies in LLM
predictions. First, we show how spurious correlations between input
distributions and labels -- a known issue in TT models -- form only a minor
problem for prompted models. Then, we engage in a systematic, holistic
evaluation of different factors that have been found to influence predictions
in a prompting setup. We test all possible combinations of a range of factors
on both vanilla and instruction-tuned (IT) LLMs of different scale and
statistically analyse the results to show which factors are the most
influential, interactive or stable. Our results show which factors can be used
without precautions and which should be avoided or handled with care in most
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DistillCSE: Distilled Contrastive Learning for Sentence Embeddings. (arXiv:2310.13499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13499">
<div class="article-summary-box-inner">
<span><p>This paper proposes the DistillCSE framework, which performs contrastive
learning under the self-training paradigm with knowledge distillation. The
potential advantage of DistillCSE is its self-enhancing feature: using a base
model to provide additional supervision signals, a stronger model may be
learned through knowledge distillation. However, the vanilla DistillCSE through
the standard implementation of knowledge distillation only achieves marginal
improvements due to severe overfitting. The further quantitative analyses
demonstrate the reason that the standard knowledge distillation exhibits a
relatively large variance of the teacher model's logits due to the essence of
contrastive learning. To mitigate the issue induced by high variance, this
paper accordingly proposed two simple yet effective solutions for knowledge
distillation: a Group-P shuffling strategy as an implicit regularization and
the averaging logits from multiple teacher components. Experiments on standard
benchmarks demonstrate that the proposed DistillCSE outperforms many strong
baseline methods and yields a new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analogical Proportions and Creativity: A Preliminary Study. (arXiv:2310.13500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13500">
<div class="article-summary-box-inner">
<span><p>Analogical proportions are statements of the form "$a$ is to $b$ as $c$ is to
$d$", which expresses that the comparisons of the elements in pair $(a, b)$ and
in pair $(c, d)$ yield similar results. Analogical proportions are creative in
the sense that given 3 distinct items, the representation of a 4th item $d$,
distinct from the previous items, which forms an analogical proportion with
them can be calculated, provided certain conditions are met. After providing an
introduction to analogical proportions and their properties, the paper reports
the results of an experiment made with a database of animal descriptions and
their class, where we try to "create" new animals from existing ones,
retrieving rare animals such as platypus. We perform a series of experiments
using word embeddings as well as Boolean features in order to propose novel
animals based on analogical proportions, showing that word embeddings obtain
better results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13505">
<div class="article-summary-box-inner">
<span><p>Models for conversational question answering (ConvQA) over knowledge graphs
(KGs) are usually trained and tested on benchmarks of gold QA pairs. This
implies that training is limited to surface forms seen in the respective
datasets, and evaluation is on a small set of held-out questions. Through our
proposed framework REIGN, we take several steps to remedy this restricted
learning setup. First, we systematically generate reformulations of training
questions to increase robustness of models to surface form variations. This is
a particularly challenging problem, given the incomplete nature of such
questions. Second, we guide ConvQA models towards higher performance by feeding
it only those reformulations that help improve their answering quality, using
deep reinforcement learning. Third, we demonstrate the viability of training
major model components on one benchmark and applying them zero-shot to another.
Finally, for a rigorous evaluation of robustness for trained models, we use and
release large numbers of diverse reformulations generated by prompting GPT for
benchmark test sets (resulting in 20x increase in sizes). Our findings show
that ConvQA models with robust training via reformulations, significantly
outperform those with standard training from gold QA pairs only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Interactions Between Text Spans. (arXiv:2310.13506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13506">
<div class="article-summary-box-inner">
<span><p>Reasoning over spans of tokens from different parts of the input is essential
for natural language understanding (NLU) tasks such as fact-checking (FC),
machine reading comprehension (MRC) or natural language inference (NLI).
However, existing highlight-based explanations primarily focus on identifying
individual important tokens or interactions only between adjacent tokens or
tuples of tokens. Most notably, there is a lack of annotations capturing the
human decision-making process w.r.t. the necessary interactions for informed
decision-making in such tasks. To bridge this gap, we introduce SpanEx, a
multi-annotator dataset of human span interaction explanations for two NLU
tasks: NLI and FC. We then investigate the decision-making processes of
multiple fine-tuned large language models in terms of the employed connections
between spans in separate parts of the input and compare them to the human
reasoning processes. Finally, we present a novel community detection based
unsupervised method to extract such interaction explanations from a model's
inner workings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Question Generation with Multi-level Content Planning. (arXiv:2310.13512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13512">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of generating questions from a given context
and an answer, specifically focusing on questions that require multi-hop
reasoning across an extended context. Previous studies have suggested that key
phrase selection is essential for question generation (QG), yet it is still
challenging to connect such disjointed phrases into meaningful questions,
particularly for long context. To mitigate this issue, we propose MultiFactor,
a novel QG framework based on multi-level content planning. Specifically,
MultiFactor includes two components: FA-model, which simultaneously selects key
phrases and generates full answers, and Q-model which takes the generated full
answer as an additional input to generate questions. Here, full answer
generation is introduced to connect the short answer with the selected key
phrases, thus forming an answer-aware summary to facilitate QG. Both FA-model
and Q-model are formalized as simple-yet-effective Phrase-Enhanced
Transformers, our joint model for phrase selection and text generation.
Experimental results show that our method outperforms strong baselines on two
popular QG datasets. Our code is available at
https://github.com/zeaver/MultiFactor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Language Models to Self-Improve through Interactive Demonstrations. (arXiv:2310.13522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13522">
<div class="article-summary-box-inner">
<span><p>The self-improving ability of large language models (LLMs), enabled by
prompting them to analyze and revise their own outputs, has garnered
significant interest in recent research. However, this ability has been shown
to be absent and difficult to learn for smaller models, thus widening the
performance gap between state-of-the-art LLMs and more cost-effective and
faster ones. To reduce this gap, we introduce TriPosT, a training algorithm
that endows smaller models with such self-improvement ability, and show that
our approach can improve a LLaMA-7b's performance on math and reasoning tasks
by up to 7.13%. In contrast to prior work, we achieve this by using the smaller
model to interact with LLMs to collect feedback and improvements on its own
generations. We then replay this experience to train the small model. Our
experiments on four math and reasoning datasets show that the interactive
experience of learning from and correcting its own mistakes is crucial for
small models to improve their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Randomness Improves the Performance of Transformer Models. (arXiv:2310.13526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13526">
<div class="article-summary-box-inner">
<span><p>During the pre-training step of natural language models, the main objective
is to learn a general representation of the pre-training dataset, usually
requiring large amounts of textual data to capture the complexity and diversity
of natural language. Contrasting this, in most cases, the size of the data
available to solve the specific downstream task is often dwarfed by the
aforementioned pre-training dataset, especially in domains where data is
scarce. We introduce controlled randomness, i.e. noise, into the training
process to improve fine-tuning language models and explore the performance of
targeted noise in addition to the parameters of these models. We find that
adding such noise can improve the performance in our two downstream tasks of
joint named entity recognition and relation extraction and text summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Diachronic Perspective on User Trust in AI under Uncertainty. (arXiv:2310.13544v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13544">
<div class="article-summary-box-inner">
<span><p>In a human-AI collaboration, users build a mental model of the AI system
based on its reliability and how it presents its decision, e.g. its
presentation of system confidence and an explanation of the output. Modern NLP
systems are often uncalibrated, resulting in confidently incorrect predictions
that undermine user trust. In order to build trustworthy AI, we must understand
how user trust is developed and how it can be regained after potential
trust-eroding events. We study the evolution of user trust in response to these
trust-eroding events using a betting game. We find that even a few incorrect
instances with inaccurate confidence estimates damage user trust and
performance, with very slow recovery. We also show that this degradation in
trust reduces the success of human-AI collaboration and that different types of
miscalibration -- unconfidently correct and confidently incorrect -- have
different negative effects on user trust. Our findings highlight the importance
of calibration in user-facing AI applications and shed light on what aspects
help users decide whether to trust the AI system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13548">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning from human feedback (RLHF) is a popular technique for
training high-quality AI assistants. However, RLHF may also encourage model
responses that match user beliefs over truthful responses, a behavior known as
sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models
and whether human preference judgements are responsible. We first demonstrate
that five state-of-the-art AI assistants consistently exhibit sycophantic
behavior across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior of RLHF models, we
analyze existing human preference data. We find that when a response matches a
user's views, it is more likely to be preferred. Moreover, both humans and
preference models (PMs) prefer convincingly-written sycophantic responses over
correct ones a negligible fraction of the time. Optimizing model outputs
against PMs also sometimes sacrifices truthfulness in favor of sycophancy.
Overall, our results indicate that sycophancy is a general behavior of RLHF
models, likely driven in part by human preference judgements favoring
sycophantic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Perils & Promises of Fact-checking with Large Language Models. (arXiv:2310.13549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13549">
<div class="article-summary-box-inner">
<span><p>Autonomous fact-checking, using machine learning to verify claims, has grown
vital as misinformation spreads beyond human fact-checking capacity. Large
Language Models (LLMs) like GPT-4 are increasingly trusted to verify
information and write academic papers, lawsuits, and news articles, emphasizing
their role in discerning truth from falsehood and the importance of being able
to verify their outputs. Here, we evaluate the use of LLM agents in
fact-checking by having them phrase queries, retrieve contextual data, and make
decisions. Importantly, in our framework, agents explain their reasoning and
cite the relevant sources from the retrieved context. Our results show the
enhanced prowess of LLMs when equipped with contextual information. GPT-4
outperforms GPT-3, but accuracy varies based on query language and claim
veracity. While LLMs show promise in fact-checking, caution is essential due to
inconsistent accuracy. Our investigation calls for further research, fostering
a deeper comprehension of when agents succeed and when they fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning. (arXiv:2310.13552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13552">
<div class="article-summary-box-inner">
<span><p>In open-domain question-answering (ODQA), most existing questions require
single-hop reasoning on commonsense. To further extend this task, we officially
introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop
questions with explicit reasoning steps in open-domain setting. Recently, large
language models (LLMs) have found significant utility in facilitating ODQA
without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts
the reasoning capability of LLMs to a greater extent with manual or automated
paradigms. However, existing automated methods lack of quality assurance, while
manual approaches suffer from limited scalability and poor diversity, hindering
the capabilities of LLMs. In this paper, we propose Self-prompted
Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality
CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation
pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT
selection and self-prompted inference via in-context learning. Extensive
experiments on four multi-hop question-answering benchmarks show that our
proposed SP-CoT not only significantly surpasses the previous SOTA methods on
large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of
small-scale (13B) LLMs. Further analysis reveals the remarkable capability of
SP-CoT to elicit direct and concise intermediate reasoning steps by recalling
$\sim$50\% of intermediate answers on MuSiQue-Ans dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cache & Distil: Optimising API Calls to Large Language Models. (arXiv:2310.13561v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13561">
<div class="article-summary-box-inner">
<span><p>Large-scale deployment of generative AI tools often depends on costly API
calls to a Large Language Model (LLM) to fulfil user queries. To curtail the
frequency of these calls, one can employ a smaller language model -- a student
-- which is continuously trained on the responses of the LLM. This student
gradually gains proficiency in independently handling an increasing number of
user requests, a process we term neural caching. The crucial element in neural
caching is a policy that decides which requests should be processed by the
student alone and which should be redirected to the LLM, subsequently aiding
the student's learning. In this study, we focus on classification tasks, and we
consider a range of classic active learning-based selection criteria as the
policy. Our experiments suggest that Margin Sampling and Query by Committee
bring consistent benefits across tasks and budgets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring. (arXiv:2310.13566v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13566">
<div class="article-summary-box-inner">
<span><p>Constructing responses in task-oriented dialogue systems typically relies on
information sources such the current dialogue state or external databases. This
paper presents a novel approach to knowledge-grounded response generation that
combines retrieval-augmented language models with logical reasoning. The
approach revolves around a knowledge graph representing the current dialogue
state and background information, and proceeds in three steps. The knowledge
graph is first enriched with logically derived facts inferred using
probabilistic logical programming. A neural model is then employed at each turn
to score the conversational relevance of each node and edge of this extended
graph. Finally, the elements with highest relevance scores are converted to a
natural language form, and are integrated into the prompt for the neural
conversational model employed to generate the system response.
</p>
<p>We investigate the benefits of the proposed approach on two datasets (KVRET
and GraphWOZ) along with a human evaluation. Experimental results show that the
combination of (probabilistic) logical reasoning with conversational relevance
scoring does increase both the factuality and fluency of the responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Can Large Language Models Generate Correct Chain-of-Thoughts?. (arXiv:2310.13571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13571">
<div class="article-summary-box-inner">
<span><p>This paper delves into the capabilities of large language models (LLMs),
specifically focusing on advancing the theoretical comprehension of
chain-of-thought prompting. We investigate how LLMs can be effectively induced
to generate a coherent chain of thoughts. To achieve this, we introduce a
two-level hierarchical graphical model tailored for natural language
generation. Within this framework, we establish a compelling geometrical
convergence rate that gauges the likelihood of an LLM-generated chain of
thoughts compared to those originating from the true language. Our findings
provide a theoretical justification for the ability of LLMs to produce the
correct sequence of thoughts (potentially) explaining performance gains in
tasks demanding reasoning skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Decomposition of Question and SQL for Text-to-SQL Parsing. (arXiv:2310.13575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13575">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain
and complex queries. Recent research has employed a question decomposition
strategy to enhance the parsing of complex SQL queries. However, this strategy
encounters two major obstacles: (1) existing datasets lack question
decomposition; (2) due to the syntactic complexity of SQL, most complex queries
cannot be disentangled into sub-queries that can be readily recomposed. To
address these challenges, we propose a new modular Query Plan Language (QPL)
that systematically decomposes SQL queries into simple and regular sub-queries.
We develop a translator from SQL to QPL by leveraging analysis of SQL server
query optimization plans, and we augment the Spider dataset with QPL programs.
Experimental results demonstrate that the modular nature of QPL benefits
existing semantic-parsing architectures, and training text-to-QPL parsers is
more effective than text-to-SQL parsing for semantically equivalent queries.
The QPL approach offers two additional advantages: (1) QPL programs can be
paraphrased as simple questions, which allows us to create a dataset of
(complex question, decomposed questions). Training on this dataset, we obtain a
Question Decomposer for data retrieval that is sensitive to database schemas.
(2) QPL is more accessible to non-experts for complex queries, leading to more
interpretable output from the semantic parser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering. (arXiv:2310.13583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13583">
<div class="article-summary-box-inner">
<span><p>Despite the impressive growth of the abilities of multilingual language
models, such as XLM-R and mT5, it has been shown that they still face
difficulties when tackling typologically-distant languages, particularly in the
low-resource setting. One obstacle for effective cross-lingual transfer is
variability in word-order patterns. It can be potentially mitigated via source-
or target-side word reordering, and numerous approaches to reordering have been
proposed. However, they rely on language-specific rules, work on the level of
POS tags, or only target the main clause, leaving subordinate clauses intact.
To address these limitations, we present a new powerful reordering method,
defined in terms of Universal Dependencies, that is able to learn fine-grained
word-order patterns conditioned on the syntactic context from a small amount of
annotated data and can be applied at all levels of the syntactic tree. We
conduct experiments on a diverse set of tasks and show that our method
consistently outperforms strong baselines over different language pairs and
model architectures. This performance advantage holds true in both zero-shot
and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Machine Translation with Tailored Reference. (arXiv:2310.13588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13588">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) generates translation while reading
the whole source sentence. However, existing SiMT models are typically trained
using the same reference disregarding the varying amounts of available source
information at different latency. Training the model with ground-truth at low
latency may introduce forced anticipations, whereas utilizing reference
consistent with the source word order at high latency results in performance
degradation. Consequently, it is crucial to train the SiMT model with
appropriate reference that avoids forced anticipations during training while
maintaining high quality. In this paper, we propose a novel method that
provides tailored reference for the SiMT models trained at different latency by
rephrasing the ground-truth. Specifically, we introduce the tailor, induced by
reinforcement learning, to modify ground-truth to the tailored reference. The
SiMT model is trained with the tailored reference and jointly optimized with
the tailor to enhance performance. Importantly, our method is applicable to a
wide range of current SiMT approaches. Experiments on three translation tasks
demonstrate that our method achieves state-of-the-art performance in both fixed
and adaptive policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarineGPT: Unlocking Secrets of Ocean to the Public. (arXiv:2310.13596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13596">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be
powerful tools in promoting the user experience as an AI assistant. The
continuous works are proposing multi-modal large language models (MLLM),
empowering LLMs with the ability to sense multiple modality inputs through
constructing a joint semantic space (e.g. visual-text space). Though
significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in
domain-specific applications that required domain-specific knowledge and
expertise has been less conducted, especially for \textbf{marine domain}.
Different from general-purpose MLLMs, the marine-specific MLLM is required to
yield much more \textbf{sensitive}, \textbf{informative}, and
\textbf{scientific} responses. In this work, we demonstrate that the existing
MLLMs optimized on huge amounts of readily available general-purpose training
data show a minimal ability to understand domain-specific intents and then
generate informative and satisfactory responses. To address these issues, we
propose \textbf{MarineGPT}, the first vision-language model specially designed
for the marine domain, unlocking the secrets of the ocean to the public. We
present our \textbf{Marine-5M} dataset with more than 5 million marine
image-text pairs to inject domain-specific marine knowledge into our model and
achieve better marine vision and language alignment. Our MarineGPT not only
pushes the boundaries of marine understanding to the general public but also
offers a standard protocol for adapting a general-purpose assistant to
downstream domain-specific experts. We pave the way for a wide range of marine
applications while setting valuable data and pre-trained models for future
research in both academic and industrial communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark. (arXiv:2310.13606v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13606">
<div class="article-summary-box-inner">
<span><p>There is a lack of research into capabilities of recent LLMs to generate
convincing text in languages other than English and into performance of
detectors of machine-generated text in multilingual settings. This is also
reflected in the available benchmarks which lack authentic texts in languages
other than English and predominantly cover older generators. To fill this gap,
we introduce MULTITuDE, a novel benchmarking dataset for multilingual
machine-generated text detection comprising of 74,081 authentic and
machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru,
uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare
the performance of zero-shot (statistical and black-box) and fine-tuned
detectors. Considering the multilinguality, we evaluate 1) how these detectors
generalize to unseen languages (linguistically similar as well as dissimilar)
and unseen LLMs and 2) whether the detectors improve their performance when
trained on multiple languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making. (arXiv:2310.13610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13610">
<div class="article-summary-box-inner">
<span><p>Explaining black-box model behavior with natural language has achieved
impressive results in various NLP tasks. Recent research has explored the
utilization of subsequences from the input text as a rationale, providing users
with evidence to support the model decision. Although existing frameworks excel
in generating high-quality rationales while achieving high task performance,
they neglect to account for the unreliable link between the generated rationale
and model decision. In simpler terms, a model may make correct decisions while
attributing wrong rationales, or make poor decisions while attributing correct
rationales. To mitigate this issue, we propose a unified two-stage framework
known as Self-Attribution and Decision-Making (SADM). Through extensive
experiments on five reasoning datasets from the ERASER benchmark, we
demonstrate that our framework not only establishes a more reliable link
between the generated rationale and model decision but also achieves
competitive results in task performance and the quality of rationale.
Furthermore, we explore the potential of our framework in semi-supervised
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hunayn: Elevating Translation Beyond the Literal. (arXiv:2310.13613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13613">
<div class="article-summary-box-inner">
<span><p>This project introduces an advanced English-to-Arabic translator surpassing
conventional tools. Leveraging the Helsinki transformer (MarianMT), our
approach involves fine-tuning on a self-scraped, purely literary Arabic
dataset. Evaluations against Google Translate show consistent outperformance in
qualitative assessments. Notably, it excels in cultural sensitivity and context
accuracy. This research underscores the Helsinki transformer's superiority for
English-to-Arabic translation using a Fusha dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning. (arXiv:2310.13615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13615">
<div class="article-summary-box-inner">
<span><p>Due to the remarkable language understanding and generation abilities of
large language models (LLMs), their use in educational applications has been
explored. However, little work has been done on investigating the pedagogical
ability of LLMs in helping students to learn mathematics. In this position
paper, we discuss the challenges associated with employing LLMs to enhance
students' mathematical problem-solving skills by providing adaptive feedback.
Apart from generating the wrong reasoning processes, LLMs can misinterpret the
meaning of the question, and also exhibit difficulty in understanding the given
questions' rationales when attempting to correct students' answers. Three
research questions are formulated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised multimodal coreference resolution in image narrations. (arXiv:2310.13619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13619">
<div class="article-summary-box-inner">
<span><p>In this paper, we study multimodal coreference resolution, specifically where
a longer descriptive text, i.e., a narration is paired with an image. This
poses significant challenges due to fine-grained image-text alignment, inherent
ambiguity present in narrative language, and unavailability of large annotated
training sets. To tackle these challenges, we present a data efficient
semi-supervised approach that utilizes image-narration pairs to resolve
coreferences and narrative grounding in a multimodal context. Our approach
incorporates losses for both labeled and unlabeled data within a cross-modal
framework. Our evaluation shows that the proposed approach outperforms strong
baselines both quantitatively and qualitatively, for the tasks of coreference
resolution and narrative grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Information-Theoretic and Geometric Compression in Language Models. (arXiv:2310.13620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.13620">
<div class="article-summary-box-inner">
<span><p>For a language model (LM) to faithfully model human language, it must
compress vast, potentially infinite information into relatively few dimensions.
We propose analyzing compression in (pre-trained) LMs from two points of view:
geometric and information-theoretic. We demonstrate that the two views are
highly correlated, such that the intrinsic geometric dimension of linguistic
data predicts their coding length under the LM. We then show that, in turn,
high compression of a linguistic dataset predicts rapid adaptation to that
dataset, confirming that being able to compress linguistic information is an
important part of successful LM performance. As a practical byproduct of our
analysis, we evaluate a battery of intrinsic dimension estimators for the first
time on linguistic data, showing that only some encapsulate the relationship
between information-theoretic compression, geometric compression, and
ease-of-adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01777">
<div class="article-summary-box-inner">
<span><p>We propose a novel interpretable deep neural network for text classification,
called ProtoryNet, based on a new concept of prototype trajectories. Motivated
by the prototype theory in modern linguistics, ProtoryNet makes a prediction by
finding the most similar prototype for each sentence in a text sequence and
feeding an RNN backbone with the proximity of each sentence to the
corresponding active prototype. The RNN backbone then captures the temporal
pattern of the prototypes, which we refer to as prototype trajectories.
Prototype trajectories enable intuitive and fine-grained interpretation of the
reasoning process of the RNN model, in resemblance to how humans analyze texts.
We also design a prototype pruning procedure to reduce the total number of
prototypes used by the model for better interpretability. Experiments on
multiple public data sets show that ProtoryNet is more accurate than the
baseline prototype-based deep neural net and reduces the performance gap
compared to state-of-the-art black-box models. In addition, after prototype
pruning, the resulting ProtoryNet models only need less than or around 20
prototypes for all datasets, which significantly benefits interpretability.
Furthermore, we report a survey result indicating that human users find
ProtoryNet more intuitive and easier to understand than other prototype-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06969">
<div class="article-summary-box-inner">
<span><p>Pre-trained models (PTMs) have been widely used in various downstream tasks.
The parameters of PTMs are distributed on the Internet and may suffer backdoor
attacks. In this work, we demonstrate the universal vulnerability of PTMs,
where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary
downstream tasks. Specifically, attackers can add a simple pre-training task,
which restricts the output representations of trigger instances to pre-defined
vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor
functionality is not eliminated during fine-tuning, the triggers can make the
fine-tuned model predict fixed labels by pre-defined vectors. In the
experiments of both natural language processing (NLP) and computer vision (CV),
we show that NeuBA absolutely controls the predictions for trigger instances
without any knowledge of downstream tasks. Finally, we apply several defense
methods to NeuBA and find that model pruning is a promising direction to resist
NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the
wide use of PTMs. Our source code and models are available at
\url{https://github.com/thunlp/NeuBA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting degree and polarity: An artificial language learning study. (arXiv:2109.06333v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06333">
<div class="article-summary-box-inner">
<span><p>We investigate a new linguistic generalization in pre-trained language models
(taking BERT (Devlin et al., 2019) as a case study). We focus on degree
modifiers (expressions like slightly, very, rather, extremely) and test the
hypothesis that the degree expressed by a modifier (low, medium or high degree)
is related to the modifier's sensitivity to sentence polarity (whether it shows
preference for affirmative or negative sentences or neither). To probe this
connection, we apply the Artificial Language Learning experimental paradigm
from psycholinguistics to a neural language model. Our experimental results
suggest that BERT generalizes in line with existing linguistic observations
that relate degree semantics to polarity sensitivity, including the main one:
low degree semantics is associated with preference towards positive polarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uniform Complexity for Text Generation. (arXiv:2204.05185v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05185">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown promising results in a wide array of
generative NLP tasks, such as summarization and machine translation. In the
context of narrative generation, however, existing models still do not capture
factors that contribute to producing consistent text. For instance, it is
logical that a piece of text or a story should be uniformly readable throughout
and that this form of complexity should be controllable. As such, if the
complexity of an input text prompt is rated first-grade reading level in the
Flesch Reading Ease test, then the generated text continuing the plot should
also be within this range of complexity. With this in mind, we introduce
Uniform Complexity for Text Generation (UCTG), a new benchmark test which
raises the challenge of making generative models observe uniform linguistic
properties with respect to prompts. We experiment with over 150+ linguistically
and cognitively motivated features for evaluating text complexity in humans and
generative models. From our results, we find that models such as GPT-2 struggle
to preserve the complexity of input prompts used in its generations, even if
finetuned with professionally written texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and Translation Using Neural Transducers. (arXiv:2211.02809v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02809">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) and speech translation (ST) can both use
neural transducers as the model structure. It is thus possible to use a single
transducer model to perform both tasks. In real-world applications, such joint
ASR and ST models may need to be streaming and do not require source language
identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a
streaming language-agnostic multilingual speech recognition and translation
model using neural transducers. Based on the transducer model structure, we
propose four methods, a unified joint and prediction network for multilingual
output, a clustered multilingual encoder, target language identification for
encoder, and connectionist temporal classification regularization. Experimental
results show that LAMASSU not only drastically reduces the model size but also
reaches the performances of monolingual ASR and bilingual ST models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Frequency Distortion of Word Embeddings and Its Impact on Bias Metrics. (arXiv:2211.08203v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08203">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that static word embeddings can encode word
frequency information. However, little has been studied about this phenomenon
and its effects on downstream tasks. In the present work, we systematically
study the association between frequency and semantic similarity in several
static word embeddings. We find that Skip-gram, GloVe and FastText embeddings
tend to produce higher semantic similarity between high-frequency words than
between other frequency combinations. We show that the association between
frequency and similarity also appears when words are randomly shuffled. This
proves that the patterns found are not due to real semantic associations
present in the texts, but are an artifact produced by the word embeddings.
Finally, we provide an example of how word frequency can strongly impact the
measurement of gender bias with embedding-based metrics. In particular, we
carry out a controlled experiment that shows that biases can even change sign
or reverse their order by manipulating word frequencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model. (arXiv:2211.11363v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11363">
<div class="article-summary-box-inner">
<span><p>Continual pretraining is a popular way of building a domain-specific
pretrained language model from a general-domain language model. In spite of its
high efficiency, continual pretraining suffers from catastrophic forgetting,
which may harm the model's performance in downstream tasks. To alleviate the
issue, in this paper, we propose a continual pretraining method for the
BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a
small number of attention heads and hidden units inside each self-attention
layer and feed-forward network. Furthermore, we train a domain-specific
language model named AF Adapter based RoBERTa for the Chinese biomedical
domain. In experiments, models are applied to downstream tasks for evaluation.
The results demonstrate that with only about 17% of model parameters trained,
AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong
baselines. Further experimental results show that our method alleviates the
catastrophic forgetting problem by 11% compared to the fine-tuning method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning. (arXiv:2211.16773v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16773">
<div class="article-summary-box-inner">
<span><p>In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train
a model to directly optimize response for task-related metrics. However, RL
needs to perform exploration, which can be time-consuming due to the slow
auto-regressive sequence generation process. We investigate an approach to
create a more efficient RL-based algorithm to improve TOD performance in an
offline setting. First, we use a faster generation procedure that samples from
independent next-word distributions after training the language model (LM) with
supervised learning. We then introduce a fine-grained reward function to help
the model focus on learning key information in a dialog, by measuring the
importance and semantic closeness of each generated token. Experiments on the
MultiWoZ dataset show our new training algorithm, Keywords Reinforcement
Learning with Next-word Sampling (KRLS), achieves state-of-the-art performance
on the end-to-end response generation task, with a 15% training time reduction
compared to a standard RL algorithm using auto-regressive generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning. (arXiv:2212.10341v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10341">
<div class="article-summary-box-inner">
<span><p>Machine-Generated Text (MGT) detection, a task that discriminates MGT from
Human-Written Text (HWT), plays a crucial role in preventing misuse of text
generative models, which excel in mimicking human writing style recently.
Latest proposed detectors usually take coarse text sequences as input and
fine-tune pretrained models with standard cross-entropy loss. However, these
methods fail to consider the linguistic structure of texts. Moreover, they lack
the ability to handle the low-resource problem which could often happen in
practice considering the enormous amount of textual data online. In this paper,
we present a coherence-based contrastive learning model named CoCo to detect
the possible MGT under low-resource scenario. To exploit the linguistic
feature, we encode coherence information in form of graph into text
representation. To tackle the challenges of low data resource, we employ a
contrastive learning framework and propose an improved contrastive loss for
preventing performance degradation brought by simple samples. The experiment
results on two public datasets and two self-constructed datasets prove our
approach outperforms the state-of-art methods significantly. Also, we
surprisingly find that MGTs originated from up-to-date language models could be
easier to detect than these from previous models, in our experiments. And we
propose some preliminary explanations for this counter-intuitive phenomena. All
the codes and datasets are open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logic Mill -- A Knowledge Navigation System. (arXiv:2301.00200v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00200">
<div class="article-summary-box-inner">
<span><p>Logic Mill is a scalable and openly accessible software system that
identifies semantically similar documents within either one domain-specific
corpus or multi-domain corpora. It uses advanced Natural Language Processing
(NLP) techniques to generate numerical representations of documents. Currently
it leverages a large pre-trained language model to generate these document
representations. The system focuses on scientific publications and patent
documents and contains more than 200 million documents. It is easily accessible
via a simple Application Programming Interface (API) or via a web interface.
Moreover, it is continuously being updated and can be extended to text corpora
from other domains. We see this system as a general-purpose tool for future
research applications in the social sciences and other domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00407">
<div class="article-summary-box-inner">
<span><p>Lemmatization is a natural language processing (NLP) task which consists of
producing, from a given inflected word, its canonical form or lemma.
Lemmatization is one of the basic tasks that facilitate downstream NLP
applications, and is of particular importance for high-inflected languages.
Given that the process to obtain a lemma from an inflected word can be
explained by looking at its morphosyntactic category, including fine-grained
morphosyntactic information to train contextual lemmatizers has become common
practice, without considering whether that is the optimum in terms of
downstream performance. In order to address this issue, in this paper we
empirically investigate the role of morphological information to develop
contextual lemmatizers in six languages within a varied spectrum of
morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English.
Furthermore, and unlike the vast majority of previous work, we also evaluate
lemmatizers in out-of-domain settings, which constitutes, after all, their most
common application use. The results of our study are rather surprising. It
turns out that providing lemmatizers with fine-grained morphological features
during training is not that beneficial, not even for agglutinative languages.
In fact, modern contextual word representations seem to implicitly encode
enough morphological information to obtain competitive contextual lemmatizers
without seeing any explicit morphological signal. Moreover, our experiments
suggest that the best lemmatizers out-of-domain are those using simple UPOS
tags or those trained without morphology and, finally, that current evaluation
practices for lemmatization are not adequate to clearly discriminate between
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12343">
<div class="article-summary-box-inner">
<span><p>We propose CHiLL (Crafting High-Level Latents), an approach for
natural-language specification of features for linear models. CHiLL prompts
LLMs with expert-crafted queries to generate interpretable features from health
records. The resulting noisy labels are then used to train a simple linear
classifier. Generating features based on queries to an LLM can empower
physicians to use their domain expertise to craft features that are clinically
meaningful for a downstream task of interest, without having to manually
extract these from raw EHR. We are motivated by a real-world risk prediction
task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and
standard predictive tasks (e.g., 30-day readmission) to evaluate this approach.
We find that linear models using automatically extracted features are
comparably performant to models using reference features, and provide greater
interpretability than linear models using "Bag-of-Words" features. We verify
that learned feature weights align well with clinical expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12570">
<div class="article-summary-box-inner">
<span><p>The task of repository-level code completion is to continue writing the
unfinished code based on a broader context of the repository. While for
automated code completion tools, it is difficult to utilize the useful
information scattered in different files. We propose RepoCoder, a simple,
generic, and effective framework to address the challenge. It streamlines the
repository-level code completion process by incorporating a similarity-based
retriever and a pre-trained code language model in an iterative
retrieval-generation pipeline. RepoCoder makes effective utilization of
repository-level information for code completion and has the ability to
generate code at various levels of granularity. Moreover, we propose a new
benchmark RepoEval, which consists of the latest and high-quality real-world
repositories covering line, API invocation, and function body completion
scenarios. Experimental results indicate that RepoCoder significantly improves
the In-File completion baseline by over 10% in all settings and consistently
outperforms the vanilla retrieval-augmented code completion approach.
Furthermore, we validate the effectiveness of RepoCoder through comprehensive
analysis, providing valuable insights for future research. Our source code and
benchmark are publicly available:
https://github.com/microsoft/CodeT/tree/main/RepoCoder
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13780">
<div class="article-summary-box-inner">
<span><p>ChatGPT shows remarkable capabilities for machine translation (MT). Several
prior studies have shown that it achieves comparable results to commercial
systems for high-resource languages, but lags behind in complex tasks, e.g.,
low-resource and distant-language-pairs translation. However, they usually
adopt simple prompts which can not fully elicit the capability of ChatGPT. In
this paper, we aim to further mine ChatGPT's translation ability by revisiting
several aspects: temperature, task information, and domain information, and
correspondingly propose an optimal temperature setting and two (simple but
effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts
(DSP). We show that: 1) The performance of ChatGPT depends largely on
temperature, and a lower temperature usually can achieve better performance; 2)
Emphasizing the task information can further improve ChatGPT's performance,
particularly in complex MT tasks; 3) Introducing domain information can elicit
ChatGPT's generalization ability and improve its performance in the specific
domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT
tasks, which can be partially addressed by our proposed prompts but still need
to be highlighted for the MT/NLP community. We also explore the effects of
advanced in-context learning strategies and find a (negative but interesting)
observation: the powerful chain-of-thought prompt leads to word-by-word
translation behavior, thus bringing significant translation degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Did You Mean...? Confidence-based Trade-offs in Semantic Parsing. (arXiv:2303.16857v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16857">
<div class="article-summary-box-inner">
<span><p>We illustrate how a calibrated model can help balance common trade-offs in
task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show
that well-calibrated confidence scores allow us to balance cost with annotator
load, improving accuracy with a small number of interactions. We then examine
how confidence scores can help optimize the trade-off between usability and
safety. We show that confidence-based thresholding can substantially reduce the
number of incorrect low-confidence programs executed; however, this comes at a
cost to usability. We propose the DidYouMean system which better balances
usability and safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Textbooks with Visuals from the Web for Improved Learning. (arXiv:2304.08931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08931">
<div class="article-summary-box-inner">
<span><p>Textbooks are one of the main mediums for delivering high-quality education
to students. In particular, explanatory and illustrative visuals play a key
role in retention, comprehension and general transfer of knowledge. However,
many textbooks lack these interesting visuals to support student learning. In
this paper, we investigate the effectiveness of vision-language models to
automatically enhance textbooks with images from the web. We collect a dataset
of e-textbooks in the math, science, social science and business domains. We
then set up a text-image matching task that involves retrieving and
appropriately assigning web images to textbooks, which we frame as a matching
optimization problem. Through a crowd-sourced evaluation, we verify that (1)
while the original textbook images are rated higher, automatically assigned
ones are not far behind, and (2) the precise formulation of the optimization
problem matters. We release the dataset of textbooks with an associated image
bank to inspire further research in this intersectional area of computer vision
and NLP for education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09145">
<div class="article-summary-box-inner">
<span><p>Post-training quantization~(PTQ) of transformer language models faces
significant challenges due to the existence of detrimental outliers in
activations. We observe that these outliers are concentrated in specific
channels and are asymmetric across channels. To address this issue, we propose
the Outlier Suppression+~(OS+) framework, which contains the channel-wise
shifting for asymmetry and channel-wise scaling for concentration. We show that
these operations can be seamlessly migrated into subsequent modules while
maintaining equivalence. Second, we propose a fast and stable scheme to
calculate effective shifting and scaling values. The channel-wise shifting
aligns the center of each channel for removal of outlier asymmetry. The
channel-wise scaling quantitatively evaluates changes brough by migration and
quantization for better quantization burden balance. We validate our OS+ under
both standard and fine-grained quantization settings with models including
BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks
demonstrate the superiority of our approach. Especially, with standard
quantization, OS+ can achieve near-floating-point performance on both small
models and large language models on 8-bit and 6-bit. Besides, we establish a
new state-of-the-art for 4-bit BERT with 15.5\% improvement. Our code is
available at \url{https://github.com/ModelTC/Outlier_Suppression_Plus}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14108">
<div class="article-summary-box-inner">
<span><p>Multimodal datasets are a critical component in recent breakthroughs such as
Stable Diffusion and GPT-4, yet their design does not receive the same research
attention as model architectures or training algorithms. To address this
shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset
experiments centered around a new candidate pool of 12.8 billion image-text
pairs from Common Crawl. Participants in our benchmark design new filtering
techniques or curate new data sources and then evaluate their new dataset by
running our standardized CLIP training code and testing the resulting model on
38 downstream test sets. Our benchmark consists of multiple compute scales
spanning four orders of magnitude, which enables the study of scaling trends
and makes the benchmark accessible to researchers with varying resources. Our
baseline experiments show that the DataComp workflow leads to better training
sets. In particular, our best baseline, DataComp-1B, enables training a CLIP
ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming
OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training
procedure and compute. We release DataComp and all accompanying code at
www.datacomp.ai.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">We're Afraid Language Models Aren't Modeling Ambiguity. (arXiv:2304.14399v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14399">
<div class="article-summary-box-inner">
<span><p>Ambiguity is an intrinsic feature of natural language. Managing ambiguity is
a key part of human language understanding, allowing us to anticipate
misunderstanding as communicators and revise our interpretations as listeners.
As language models (LMs) are increasingly employed as dialogue interfaces and
writing aids, handling ambiguous language is critical to their success. We
characterize ambiguity in a sentence by its effect on entailment relations with
another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645
examples with diverse kinds of ambiguity. We design a suite of tests based on
AmbiEnt, presenting the first evaluation of pretrained LMs to recognize
ambiguity and disentangle possible meanings. We find that the task remains
extremely challenging, including for GPT-4, whose generated disambiguations are
considered correct only 32% of the time in human evaluation, compared to 90%
for disambiguations in our dataset. Finally, to illustrate the value of
ambiguity-sensitive tools, we show that a multilabel NLI model can flag
political claims in the wild that are misleading due to ambiguity. We encourage
the field to rediscover the importance of ambiguity for NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Based Evaluation of Political Bias in Automatic Summarization. (arXiv:2305.02321v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02321">
<div class="article-summary-box-inner">
<span><p>Growing literature has shown that NLP systems may encode social biases;
however, the political bias of summarization models remains relatively unknown.
In this work, we use an entity replacement method to investigate the portrayal
of politicians in automatically generated summaries of news articles. We
develop an entity-based computational framework to assess the sensitivities of
several extractive and abstractive summarizers to the politicians Donald Trump
and Joe Biden. We find consistent differences in these summaries upon entity
replacement, such as reduced emphasis of Trump's presence in the context of the
same article and a more individualistic representation of Trump with respect to
the collective US government (i.e., administration). These summary
dissimilarities are most prominent when the entity is heavily featured in the
source article. Our characterization provides a foundation for future studies
of bias in summarization and for normative discussions on the ideal qualities
of automatic summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. (arXiv:2305.03668v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03668">
<div class="article-summary-box-inner">
<span><p>Webpages have been a rich, scalable resource for vision-language and language
only tasks. Yet only pieces of webpages are kept in existing datasets:
image-caption pairs, long text articles, or raw HTML, never all in one place.
Webpage tasks have resultingly received little attention and structured
image-text data left underused. To study multimodal webpage understanding, we
introduce the Wikipedia Webpage suite (WikiWeb2M) containing 2M pages with all
of the associated image, text, and structure data. We verify its utility on
three generative tasks: page description generation, section summarization, and
contextual image captioning. We design a novel attention mechanism Prefix
Global, which selects the most relevant image and text content as global tokens
to attend to the rest of the webpage for context. By using page structure to
separate such tokens, it performs better than full attention with lower
computational complexity. Extensive experiments show that the new data in
WikiWeb2M improves task performance compared to prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns. (arXiv:2305.04812v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04812">
<div class="article-summary-box-inner">
<span><p>Social cognitive theory explains how people learn and acquire knowledge
through observing others. Recent years have witnessed the rapid development of
large language models (LLMs), which suggests their potential significance as
agents in the society. LLMs, as AI agents, can observe external information,
which shapes their cognition and behaviors. However, the extent to which
external information influences LLMs' cognition and behaviors remains unclear.
This study investigates how external statements and opinions influence LLMs'
thoughts and behaviors from a social cognitive perspective. Three experiments
were conducted to explore the effects of external information on LLMs'
memories, opinions, and social media behavioral decisions. Sociocognitive
factors, including source authority, social identity, and social role, were
analyzed to investigate their moderating effects. Results showed that external
information can significantly shape LLMs' memories, opinions, and behaviors,
with these changes mirroring human social cognitive patterns such as authority
bias, in-group bias, emotional positivity, and emotion contagion. This
underscores the challenges in developing safe and unbiased LLMs, and emphasizes
the importance of understanding the susceptibility of LLMs to external
influences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models. (arXiv:2305.06677v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06677">
<div class="article-summary-box-inner">
<span><p>A salient characteristic of pre-trained language models (PTLMs) is a
remarkable improvement in their generalization capability and emergence of new
capabilities with increasing model capacity and pre-training dataset size.
Consequently, we are witnessing the development of enormous models pushing the
state-of-the-art. It is, however, imperative to realize that this inevitably
leads to prohibitively long training times, extortionate computing costs, and a
detrimental environmental impact. Significant efforts are underway to make PTLM
training more efficient through innovations in model architectures, training
pipelines, and loss function design, with scant attention being paid to
optimizing the utility of training data. The key question that we ask is
whether it is possible to train PTLMs by employing only highly informative
subsets of the training data while maintaining downstream performance? Building
upon the recent progress in informative data subset selection, we show how we
can employ submodular optimization to select highly representative subsets of
the training corpora and demonstrate that the proposed framework can be applied
to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using only a
fraction of data. Further, we perform a rigorous empirical evaluation to show
that the resulting models achieve up to $\sim99\%$ of the performance of the
fully-trained models. We made our framework publicly available at
https://github.com/Efficient-AI/ingenious.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08503">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have achieved outstanding achievements in
abstractive single-document summarization (SDS). However, such benefits may not
fully extend to multi-document summarization (MDS), where the handling of
cross-document information is more complex. Previous works either design new
MDS architectures or apply PLMs bluntly with concatenated source documents as a
reformulated SDS task. While the former does not utilize previous pre-training
efforts and may not generalize well across different domains, the latter may
not sufficiently attend to the intricate cross-document relationships unique to
MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to
better utilize a PLM to facilitate multi-document interactions for the MDS
task. Across 10 MDS benchmarks from various domains, our method outperforms or
is competitive with the previous best models, including those with additional
MDS pre-training or with more parameters. It outperforms its corresponding PLM
backbone by up to 3 Rouge-L and is favored by humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India. (arXiv:2305.08828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08828">
<div class="article-summary-box-inner">
<span><p>This paper introduces PMIndiaSum, a multilingual and massively parallel
summarization corpus focused on languages in India. Our corpus provides a
training and testing ground for four language families, 14 languages, and the
largest to date with 196 language pairs. We detail our construction workflow
including data acquisition, processing, and quality assurance. Furthermore, we
publish benchmarks for monolingual, cross-lingual, and multilingual
summarization by fine-tuning, prompting, as well as translate-and-summarize.
Experimental results confirm the crucial role of our data in aiding
summarization between Indian languages. Our dataset is publicly available and
can be freely modified and re-distributed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10387">
<div class="article-summary-box-inner">
<span><p>Automated text simplification, a technique useful for making text more
accessible to people such as children and emergent bilinguals, is often thought
of as a monolingual translation task from complex sentences to simplified
sentences using encoder-decoder models. This view fails to account for
elaborative simplification, where new information is added into the simplified
text. This paper proposes to view elaborative simplification through the lens
of the Question Under Discussion (QUD) framework, providing a robust way to
investigate what writers elaborate upon, how they elaborate, and how
elaborations fit into the discourse context by viewing elaborations as explicit
answers to implicit questions. We introduce ElabQUD, consisting of 1.3K
elaborations accompanied with implicit QUDs, to study these phenomena. We show
that explicitly modeling QUD (via question generation) not only provides
essential understanding of elaborative simplification and how the elaborations
connect with the rest of the discourse, but also substantially improves the
quality of elaboration generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10613">
<div class="article-summary-box-inner">
<span><p>Temporal knowledge graph (TKG) forecasting benchmarks challenge models to
predict future facts using knowledge of past facts. In this paper, we apply
large language models (LLMs) to these benchmarks using in-context learning
(ICL). We investigate whether and to what extent LLMs can be used for TKG
forecasting, especially without any fine-tuning or explicit modules for
capturing structural and temporal information. For our experiments, we present
a framework that converts relevant historical facts into prompts and generates
ranked predictions using token probabilities. Surprisingly, we observe that
LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully
designed and trained for TKG forecasting. Our extensive evaluation presents
performances across several models and datasets with different characteristics,
compares alternative heuristics for preparing contextual information, and
contrasts to prominent TKG methods and simple frequency and recency baselines.
We also discover that using numerical indices instead of entity/relation names,
i.e., hiding semantic information, does not significantly affect the
performance ($\pm$0.4\% Hit@1). This shows that prior semantic knowledge is
unnecessary; instead, LLMs can leverage the existing patterns in the context to
achieve such performance. Our analysis also reveals that ICL enables LLMs to
learn irregular patterns from the historical context, going beyond simple
predictions based on common or recent information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11596">
<div class="article-summary-box-inner">
<span><p>Modern NLP models are often trained over large untrusted datasets, raising
the potential for a malicious adversary to compromise model behaviour. For
instance, backdoors can be implanted through crafting training instances with a
specific textual trigger and a target label. This paper posits that backdoor
poisoning attacks exhibit \emph{spurious correlation} between simple text
features and classification labels, and accordingly, proposes methods for
mitigating spurious correlation as means of defence. Our empirical study
reveals that the malicious triggers are highly correlated to their target
labels; therefore such correlations are extremely distinguishable compared to
those scores of benign features, and can be used to filter out potentially
problematic instances. Compared with several existing defences, our defence
method significantly reduces attack success rates across backdoor attacks, and
in the case of insertion-based attacks, our method provides a near-perfect
defence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11707">
<div class="article-summary-box-inner">
<span><p>In Natural Language Generation (NLG) tasks, for any input, multiple
communicative goals are plausible, and any goal can be put into words, or
produced, in multiple ways. We characterise the extent to which human
production varies lexically, syntactically, and semantically across four NLG
tasks, connecting human production variability to aleatoric or data
uncertainty. We then inspect the space of output strings shaped by a generation
system's predicted probability distribution and decoding algorithm to probe its
uncertainty. For each test input, we measure the generator's calibration to
human production variability. Following this instance-level approach, we
analyse NLG models and decoding strategies, demonstrating that probing a
generator with multiple samples and, when possible, multiple references,
provides the level of detail necessary to gain understanding of a model's
representation of uncertainty. Code available at
https://github.com/dmg-illc/nlg-uncertainty-probes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12027">
<div class="article-summary-box-inner">
<span><p>Entity linking methods based on dense retrieval are an efficient and widely
used solution in large-scale applications, but they fall short of the
performance of generative models, as they are sensitive to the structure of the
embedding space. In order to address this issue, this paper introduces DUCK, an
approach to infusing structural information in the space of entity
representations, using prior knowledge of entity types. Inspired by duck typing
in programming languages, we propose to define the type of an entity based on
the relations that it has with other entities in a knowledge graph. Then,
porting the concept of box embeddings to spherical polar coordinates, we
propose to represent relations as boxes on the hypersphere. We optimize the
model to cluster entities of similar type by placing them inside the boxes
corresponding to their relations. Our experiments show that our method sets new
state-of-the-art results on standard entity-disambiguation benchmarks, it
improves the performance of the model by up to 7.9 F1 points, outperforms other
type-aware approaches, and matches the results of generative models with 18
times more parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining. (arXiv:2305.12074v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12074">
<div class="article-summary-box-inner">
<span><p>Many text mining models are constructed by fine-tuning a large deep
pre-trained language model (PLM) in downstream tasks. However, a significant
challenge nowadays is maintaining performance when we use a lightweight model
with limited labelled samples. We present DisCo, a semi-supervised learning
(SSL) framework for fine-tuning a cohort of small student models generated from
a large PLM using knowledge distillation. Our key insight is to share
complementary knowledge among distilled student cohorts to promote their SSL
effectiveness. DisCo employs a novel co-training technique to optimize a cohort
of multiple small student models by promoting knowledge sharing among students
under diversified views: model views produced by different distillation
strategies and data views produced by various input augmentations. We evaluate
DisCo on both semi-supervised text classification and extractive summarization
tasks. Experimental results show that DisCo can produce student models that are
7.6 times smaller and 4.8 times faster in inference than the baseline PLMs
while maintaining comparable performance. We also show that DisCo-generated
student models outperform the similar-sized models elaborately tuned in
distinct tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness. (arXiv:2305.12947v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12947">
<div class="article-summary-box-inner">
<span><p>The emergence of generative large language models (LLMs) raises the question:
what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been
used for acquiring solutions to a wide variety of human-intelligence tasks,
including ones involving text generation, modification or evaluation. For some
of these tasks, models like ChatGPT can potentially substitute human workers.
In this study, we investigate whether this is the case for the task of
paraphrase generation for intent classification. We apply data collection
methodology of an existing crowdsourcing study (similar scale, prompts and seed
data) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases
are more diverse and lead to at least as robust models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13062">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are becoming attractive as few-shot reasoners to
solve Natural Language (NL)-related tasks. However, there is still much to
learn about how well LLMs understand structured data, such as tables. While it
is true that tables can be used as inputs to LLMs with serialization, there
lack of comprehensive studies examining whether LLMs can truly comprehend such
data. In this paper, we try to understand this by designing a benchmark to
evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark
we create includes seven tasks, each with its own unique challenges, \eg, cell
lookup, row retrieval, and size detection. We run a series of evaluations on
GPT-3.5 and GPT-4. We discover that the performance varied depending on a
number of input choices, including table input format, content order, role
prompting, and partition marks. Drawing from the insights gained through the
benchmark evaluations, we then propose \textit{self-augmentation} for effective
structural prompting, \eg, critical value / range identification using LLMs'
internal knowledge. When combined with carefully chosen input choices, these
structural prompting methods lead to promising improvements in LLM performance
on a variety of tabular tasks, \eg, TabFact($\uparrow2.31\%$),
HybridQA($\uparrow2.13\%$), SQA($\uparrow2.72\%$), Feverous($\uparrow0.84\%$),
and ToTTo($\uparrow5.68\%$). We believe that our benchmark and proposed
prompting methods can serve as a simple yet generic selection for future
research. The code and data are released in
\url{https://anonymous.4open.science/r/StructuredLLM-76F3}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization. (arXiv:2305.13091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13091">
<div class="article-summary-box-inner">
<span><p>With the recent undeniable advancement in reasoning abilities in large
language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for
using LLMs on various tasks. One area where LLMs can be employed is as an
alternative evaluation metric for complex generative tasks, which generally
demands expensive human judges to complement the traditional automatic metrics
for various evaluation dimensions such as fluency and consistency. In this
work, we conduct extensive analysis to investigate the stability and
reliability of LLMs as automatic evaluators for abstractive summarization. We
found that while ChatGPT and GPT-4 outperform the commonly used automatic
metrics, they are not ready as human replacements due to significant
limitations. That is, LLM evaluators rate each candidate system inconsistently
and are dimension-dependent. They also struggle to compare candidates with
close performance and become more unreliable with higher-quality summaries by
obtaining a lower correlation with humans. In other words, with better
abstractive summarization systems being introduced at a fast pace, LLMs may
result in misleading and unreliable evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13186">
<div class="article-summary-box-inner">
<span><p>Current scientific fact-checking benchmarks exhibit several shortcomings,
such as biases arising from crowd-sourced claims and an over-reliance on
text-based evidence. We present SCITAB, a challenging evaluation dataset
consisting of 1.2K expert-verified scientific claims that 1) originate from
authentic scientific publications and 2) require compositional reasoning for
verification. The claims are paired with evidence-containing scientific tables
annotated with labels. Through extensive evaluations, we demonstrate that
SCITAB poses a significant challenge to state-of-the-art models, including
table-based pretraining models and large language models. All models except
GPT-4 achieved performance barely above random guessing. Popular prompting
techniques, such as Chain-of-Thought, do not achieve much performance gains on
SCITAB. Our analysis uncovers several unique challenges posed by SCITAB,
including table grounding, claim ambiguity, and compositional reasoning. Our
codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives. (arXiv:2305.13192v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13192">
<div class="article-summary-box-inner">
<span><p>This paper improves contrastive learning for sentence embeddings from two
perspectives: handling dropout noise and addressing feature corruption.
Specifically, for the first perspective, we identify that the dropout noise
from negative pairs affects the model's performance. Therefore, we propose a
simple yet effective method to deal with such type of noise. Secondly, we
pinpoint the rank bottleneck of current solutions to feature corruption and
propose a dimension-wise contrastive learning objective to address this issue.
Both proposed methods are generic and can be applied to any contrastive
learning based models for sentence embeddings. Experimental results on standard
benchmarks demonstrate that combining both proposed methods leads to a gain of
1.8 points compared to the strong baseline SimCSE configured with BERT base.
Furthermore, applying the proposed method to DiffCSE, another strong
contrastive learning based baseline, results in a gain of 1.4 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents. (arXiv:2305.13303v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13303">
<div class="article-summary-box-inner">
<span><p>Automatically highlighting words that cause semantic differences between two
documents could be useful for a wide range of applications. We formulate
recognizing semantic differences (RSD) as a token-level regression task and
study three unsupervised approaches that rely on a masked language model. To
assess the approaches, we begin with basic English sentences and gradually move
to more complex, cross-lingual document pairs. Our results show that an
approach based on word alignment and sentence-level contrastive learning has a
robust correlation to gold labels. However, all unsupervised approaches still
leave a large margin of improvement. Code to reproduce our experiments is
available at https://github.com/ZurichNLP/recognizing-semantic-differences
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can LLMs facilitate interpretation of pre-trained language models?. (arXiv:2305.13386v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13386">
<div class="article-summary-box-inner">
<span><p>Work done to uncover the knowledge encoded within pre-trained language models
rely on annotated corpora or human-in-the-loop methods. However, these
approaches are limited in terms of scalability and the scope of interpretation.
We propose using a large language model, ChatGPT, as an annotator to enable
fine-grained interpretation analysis of pre-trained language models. We
discover latent concepts within pre-trained language models by applying
agglomerative hierarchical clustering over contextualized representations and
then annotate these concepts using ChatGPT. Our findings demonstrate that
ChatGPT produces accurate and semantically richer annotations compared to
human-annotated concepts. Additionally, we showcase how GPT-based annotations
empower interpretation analysis methodologies of which we demonstrate two:
probing frameworks and neuron interpretation. To facilitate further exploration
and experimentation in the field, we make available a substantial ConceptNet
dataset (TCN) comprising 39,000 annotated concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance. (arXiv:2305.13395v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13395">
<div class="article-summary-box-inner">
<span><p>Timely and accurate extraction of Adverse Drug Events (ADE) from biomedical
literature is paramount for public safety, but involves slow and costly manual
labor. We set out to improve drug safety monitoring (pharmacovigilance, PV)
through the use of Natural Language Processing (NLP). We introduce BioDEX, a
large-scale resource for Biomedical adverse Drug Event Extraction, rooted in
the historical output of drug safety reporting in the U.S. BioDEX consists of
65k abstracts and 19k full-text biomedical papers with 256k associated
document-level safety reports created by medical experts. The core features of
these reports include the reported weight, age, and biological sex of a
patient, a set of drugs taken by the patient, the drug dosages, the reactions
experienced, and whether the reaction was life threatening. In this work, we
consider the task of predicting the core information of the report given its
originating paper. We estimate human performance to be 72.0% F1, whereas our
best model achieves 62.3% F1, indicating significant headroom on this task. We
also begin to explore ways in which these models could help professional PV
reviewers. Our code and data are available: https://github.com/KarelDO/BioDEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue. (arXiv:2305.13602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13602">
<div class="article-summary-box-inner">
<span><p>Incorporating visual knowledge into text-only dialogue systems has become a
potential direction to imitate the way humans think, imagine, and communicate.
However, existing multimodal dialogue systems are either confined by the scale
and quality of available datasets or the coarse concept of visual knowledge. To
address these issues, we provide a new paradigm of constructing multimodal
dialogues as well as two datasets extended from text-only dialogues under such
paradigm (ReSee-WoW, ReSee-DD). We propose to explicitly split the visual
knowledge into finer granularity (``turn-level'' and ``entity-level''). To
further boost the accuracy and diversity of augmented visual information, we
retrieve them from the Internet or a large image dataset. To demonstrate the
superiority and universality of the provided visual knowledge, we propose a
simple but effective framework ReSee to add visual representation into vanilla
dialogue models by modality concatenations. We also conduct extensive
experiments and ablations w.r.t. different model configurations and visual
knowledge settings. Empirical, encouraging results not only demonstrate the
effectiveness of introducing visual knowledge at both entity and turn level but
also verify the proposed model ReSee outperforms several state-of-the-art
methods on automatic and human evaluations. By leveraging text and vision
knowledge, ReSee can produce informative responses with real-world visual
concepts. Our code is available at https://github.com/ImKeTT/ReSee.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning. (arXiv:2305.13660v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13660">
<div class="article-summary-box-inner">
<span><p>Planning for goal-oriented dialogue often requires simulating future dialogue
interactions and estimating task progress. Many approaches thus consider
training neural networks to perform look-ahead search algorithms such as A*
search and Monte Carlo Tree Search (MCTS). However, this training often
requires abundant annotated data, which creates challenges when faced with
noisy annotations or low-resource settings. We introduce GDP-Zero, an approach
using Open-Loop MCTS to perform goal-oriented dialogue policy planning without
any model training. GDP-Zero prompts a large language model to act as a policy
prior, value function, user simulator, and system model during the tree search.
We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that
its responses are preferred over ChatGPT up to 59.32% of the time, and are
rated more persuasive than ChatGPT during interactive evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training. (arXiv:2305.13723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13723">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised text classification trains a classifier using the label
name of each target class as the only supervision, which largely reduces human
annotation efforts. Most existing methods first use the label names as static
keyword-based features to generate pseudo labels, which are then used for final
classifier training. While reasonable, such a commonly adopted framework
suffers from two limitations: (1) keywords can have different meanings in
different contexts and some text may not have any keyword, so keyword matching
can induce noisy and inadequate pseudo labels; (2) the errors made in the
pseudo label generation stage will directly propagate to the classifier
training stage without a chance of being corrected. In this paper, we propose a
new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition
module that uses zero-shot prompting of pre-trained language models (PLM) to
get pseudo labels based on contextualized text understanding beyond static
keyword matching, and (2) a noise-robust iterative ensemble training module
that iteratively trains classifiers and updates pseudo labels by utilizing two
PLM fine-tuning methods that regularize each other. Extensive experiments show
that PIEClass achieves overall better performance than existing strong
baselines on seven benchmark datasets and even achieves similar performance to
fully-supervised classifiers on sentiment classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation. (arXiv:2305.13785v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13785">
<div class="article-summary-box-inner">
<span><p>Training or finetuning large-scale language models (LLMs) such as GPT-3
requires substantial computation resources, motivating recent efforts to
explore parameter-efficient adaptation to downstream tasks. One practical area
of research is to treat these models as black boxes and interact with them
through their inference APIs. In this paper, we investigate how to optimize
few-shot text classification without accessing the gradients of the LLMs. To
achieve this, we treat the black-box model as a feature extractor and train a
classifier with the augmented text data. Data augmentation is performed using
prompt-based finetuning on an auxiliary language model with a much smaller
parameter size than the black-box model. Through extensive experiments on eight
text classification datasets, we show that our approach, dubbed BT-Classifier,
significantly outperforms state-of-the-art black-box few-shot learners and
performs on par with methods that rely on full-model tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question Answering as Programming for Solving Time-Sensitive Questions. (arXiv:2305.14221v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14221">
<div class="article-summary-box-inner">
<span><p>Question answering plays a pivotal role in human daily life because it
involves our acquisition of knowledge about the world. However, due to the
dynamic and ever-changing nature of real-world facts, the answer can be
completely different when the time constraint in the question changes.
Recently, Large Language Models (LLMs) have shown remarkable intelligence in
question answering, while our experiments reveal that the aforementioned
problems still pose a significant challenge to existing LLMs. This can be
attributed to the LLMs' inability to perform rigorous reasoning based on
surface-level text semantics. To overcome this limitation, rather than
requiring LLMs to directly answer the question, we propose a novel approach
where we reframe the $\textbf{Q}$uestion $\textbf{A}$nswering task
$\textbf{a}$s $\textbf{P}$rogramming ($\textbf{QAaP}$). Concretely, by
leveraging modern LLMs' superior capability in understanding both natural
language and programming language, we endeavor to harness LLMs to represent
diversely expressed text as well-structured code and select the best matching
answer from multiple candidates through programming. We evaluate our QAaP
framework on several time-sensitive question answering datasets and achieve
decent improvement, up to $14.5$% over strong baselines. Our codes and data are
available at https://github.com/TianHongZXY/qaap
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Rewriting for Retrieval-Augmented Large Language Models. (arXiv:2305.14283v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14283">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) play powerful, black-box readers in the
retrieve-then-read pipeline, making remarkable progress in knowledge-intensive
tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of
the previous retrieve-then-read for the retrieval-augmented LLMs from the
perspective of the query rewriting. Unlike prior studies focusing on adapting
either the retriever or the reader, our approach pays attention to the
adaptation of the search query itself, for there is inevitably a gap between
the input text and the needed knowledge in retrieval. We first prompt an LLM to
generate the query, then use a web search engine to retrieve contexts.
Furthermore, to better align the query to the frozen modules, we propose a
trainable scheme for our pipeline. A small language model is adopted as a
trainable rewriter to cater to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader by reinforcement learning.
Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice
QA. Experiments results show consistent performance improvement, indicating
that our framework is proven effective and scalable, and brings a new framework
for retrieval-augmented LLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14600">
<div class="article-summary-box-inner">
<span><p>Semantic role labeling (SRL) has multiple disjoint label sets, e.g., VerbNet
and PropBank. Creating these datasets is challenging, therefore a natural
question is how to use each one to help the other. Prior work has shown that
cross-task interaction helps, but only explored multitask learning so far. A
common issue with multi-task setup is that argument sequences are still
separately decoded, running the risk of generating structurally inconsistent
label sequences (as per lexicons like Semlink). In this paper, we eliminate
such issue with a framework that jointly models VerbNet and PropBank labels as
one sequence. In this setup, we show that enforcing Semlink constraints during
decoding constantly improves the overall F1. With special input constructions,
our joint model infers VerbNet arguments from given PropBank arguments with
over 99 F1. For learning, we propose a constrained marginal model that learns
with knowledge defined in Semlink to further benefit from the large amounts of
PropBank-only data. On the joint benchmark based on CoNLL05, our models achieve
state-of-the-art F1's, outperforming the prior best in-domain model by 3.5
(VerbNet) and 0.8 (PropBank). For out-of-domain generalization, our models
surpass the prior best by 3.4 (VerbNet) and 0.2 (PropBank).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Getting MoRE out of Mixture of Language Model Reasoning Experts. (arXiv:2305.14628v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14628">
<div class="article-summary-box-inner">
<span><p>While recent large language models (LLMs) improve on various question
answering (QA) datasets, it remains difficult for a single model to generalize
across question types that require distinct reasoning abilities. We provide
empirical evidence that state-of-the-art LLMs suffer from poor generalizability
on reasoning types beyond those seen in the prompt. To remedy this, we propose
a Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse
specialized language models. We specialize the backbone language model with
prompts optimized for different reasoning categories, including factual,
multihop, mathematical, and commonsense reasoning. Our key insight is to
leverage agreement among the specialized experts to select the best answer for
each question, or to abstain from answering. This gives MoRE higher accuracy
than any single specialized model on a collection of 12 QA datasets from four
reasoning types. Beyond generalizability, the interpretable design of MoRE
improves selective question answering results compared to baselines without
incorporating inter-expert agreement. This framework is also more interpretable
and useful to human consumers of QA outputs. Our human study confirms that
presenting expert predictions and the answer selection process helps annotators
more accurately calibrate when to trust the system's output. We release all
code and data to facilitate future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering. (arXiv:2305.14869v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14869">
<div class="article-summary-box-inner">
<span><p>The task of zero-shot commonsense question answering evaluates models on
their capacity to reason about general scenarios beyond those presented in
specific datasets. Existing approaches for tackling this task leverage external
knowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on
synthetic QA pairs constructed from CSKBs. In these approaches, negative
examples (distractors) are formulated by randomly sampling from CSKBs using
fairly primitive keyword constraints. However, two bottlenecks limit these
approaches: the inherent incompleteness of CSKBs limits the semantic coverage
of synthetic QA pairs, and the lack of human annotations makes the sampled
negative examples potentially uninformative and contradictory. To tackle these
limitations above, we propose Conceptualization-Augmented Reasoner (CAR), a
zero-shot commonsense question-answering framework that fully leverages the
power of conceptualization. Specifically, CAR abstracts a commonsense knowledge
triple to many higher-level instances, which increases the coverage of CSKB and
expands the ground-truth answer space, reducing the likelihood of selecting
false-negative distractors. Extensive experiments demonstrate that CAR more
robustly generalizes to answering questions about zero-shot commonsense
scenarios than existing methods, including large language models, such as
GPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at
https://github.com/HKUST-KnowComp/CAR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis. (arXiv:2305.15054v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15054">
<div class="article-summary-box-inner">
<span><p>Mathematical reasoning in large language models (LMs) has garnered
significant attention in recent work, but there is a limited understanding of
how these models process and store information related to arithmetic tasks
within their architecture. In order to improve our understanding of this aspect
of language models, we present a mechanistic interpretation of
Transformer-based LMs on arithmetic questions using a causal mediation analysis
framework. By intervening on the activations of specific model components and
measuring the resulting changes in predicted probabilities, we identify the
subset of parameters responsible for specific predictions. This provides
insights into how information related to arithmetic is processed by LMs. Our
experimental results indicate that LMs process the input by transmitting the
information relevant to the query from mid-sequence early layers to the final
token using the attention mechanism. Then, this information is processed by a
set of MLP modules, which generate result-related information that is
incorporated into the residual stream. To assess the specificity of the
observed activation dynamics, we compare the effects of different model
components on arithmetic queries with other tasks, including number retrieval
from prompts and factual knowledge questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM. (arXiv:2305.15255v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15255">
<div class="article-summary-box-inner">
<span><p>We present a novel approach to adapting pre-trained large language models
(LLMs) to perform question answering (QA) and speech continuation. By endowing
the LLM with a pre-trained speech encoder, our model becomes able to take
speech inputs and generate speech outputs. The entire system is trained
end-to-end and operates directly on spectrograms, simplifying our architecture.
Key to our approach is a training objective that jointly supervises speech
recognition, text continuation, and speech synthesis using only paired
speech-text pairs, enabling a `cross-modal' chain-of-thought within a single
decoding pass. Our method surpasses existing spoken language models in speaker
preservation and semantic coherence. Furthermore, the proposed model improves
upon direct initialization in retaining the knowledge of the original LLM as
demonstrated through spoken QA datasets. Audio samples can be found at
https://michelleramanovich.github.io/spectron/spectron
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15269">
<div class="article-summary-box-inner">
<span><p>Given the intractably large size of the space of proofs, any model that is
capable of general deductive reasoning must generalize to proofs of greater
complexity. Recent studies have shown that large language models (LLMs) possess
some abstract deductive reasoning ability given chain-of-thought prompts.
However, they have primarily been tested on proofs using modus ponens or of a
specific size, and from the same distribution as the in-context examples. To
measure the general deductive reasoning ability of LLMs, we test on a broad set
of deduction rules and measure their ability to generalize to more complex
proofs from simpler demonstrations from multiple angles: depth-, width-, and
compositional generalization. To facilitate systematic exploration, we
construct a new synthetic and programmable reasoning dataset that enables
control over deduction rules and proof complexity. Our experiments on four LLMs
of various sizes and training objectives show that they are able to generalize
to compositional proofs. However, they have difficulty generalizing to longer
proofs, and they require explicit demonstrations to produce hypothetical
subproofs, specifically in proof by cases and proof by contradiction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15425">
<div class="article-summary-box-inner">
<span><p>Recent language models have shown impressive multilingual performance, even
when not explicitly trained for it. Despite this, there are concerns about the
quality of their outputs across different languages. In this paper, we show how
disparity in the treatment of different languages arises at the tokenization
stage, well before a model is even invoked. The same text translated into
different languages can have drastically different tokenization lengths, with
differences up to 15 times in some cases. These disparities persist even for
tokenizers that are intentionally trained for multilingual support.
Character-level and byte-level models also exhibit over 4 times the difference
in the encoding length for some language pairs. This induces unfair treatment
for some language communities in regard to the cost of accessing commercial
language services, the processing time and latency, as well as the amount of
content that can be provided as context to the models. Therefore, we make the
case that we should train future language models using multilingually fair
subword tokenizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16337">
<div class="article-summary-box-inner">
<span><p>Labels noise refers to errors in training labels caused by cheap data
annotation methods, such as web scraping or crowd-sourcing, which can be
detrimental to the performance of supervised classifiers. Several methods have
been proposed to counteract the effect of random label noise in supervised
classification, and some studies have shown that BERT is already robust against
high rates of randomly injected label noise. However, real label noise is not
random; rather, it is often correlated with input features or other
annotator-specific factors. In this paper, we evaluate BERT in the presence of
two types of realistic label noise: feature-dependent label noise, and
synthetic label noise from annotator disagreements. We show that the presence
of these types of noise significantly degrades BERT classification performance.
To improve robustness, we evaluate different types of ensembles and
noise-cleaning methods and compare their effectiveness against label noise
across different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03341">
<div class="article-summary-box-inner">
<span><p>We introduce Inference-Time Intervention (ITI), a technique designed to
enhance the "truthfulness" of large language models (LLMs). ITI operates by
shifting model activations during inference, following a set of directions
across a limited number of attention heads. This intervention significantly
improves the performance of LLaMA models on the TruthfulQA benchmark. On an
instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from
32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and
demonstrate how to balance it by tuning the intervention strength. ITI is
minimally invasive and computationally inexpensive. Moreover, the technique is
data efficient: while approaches like RLHF require extensive annotations, ITI
locates truthful directions using only few hundred examples. Our findings
suggest that LLMs may have an internal representation of the likelihood of
something being true, even as they produce falsehoods on the surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. (arXiv:2306.08877v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08877">
<div class="article-summary-box-inner">
<span><p>Text-conditioned image generation models often generate incorrect
associations between entities and their visual attributes. This reflects an
impaired mapping between linguistic binding of entities and modifiers in the
prompt and visual binding of the corresponding elements in the generated image.
As one notable example, a query like "a pink sunflower and a yellow flamingo"
may incorrectly produce an image of a yellow sunflower and a pink flamingo. To
remedy this issue, we propose SynGen, an approach which first syntactically
analyses the prompt to identify entities and their modifiers, and then uses a
novel loss function that encourages the cross-attention maps to agree with the
linguistic binding reflected by the syntax. Specifically, we encourage large
overlap between attention maps of entities and their modifiers, and small
overlap with other entities and modifier words. The loss is optimized during
inference, without retraining or fine-tuning the model. Human evaluation on
three datasets, including one new and challenging set, demonstrate significant
improvements of SynGen compared with current state of the art methods. This
work highlights how making use of sentence structure during inference can
efficiently and substantially improve the faithfulness of text-to-image
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09927">
<div class="article-summary-box-inner">
<span><p>Attention-based neural networks such as transformers have demonstrated a
remarkable ability to exhibit in-context learning (ICL): Given a short prompt
sequence of tokens from an unseen task, they can formulate relevant per-token
and next-token predictions without any parameter updates. By embedding a
sequence of labeled training data and unlabeled test data as a prompt, this
allows for transformers to behave like supervised learning algorithms. Indeed,
recent work has shown that when training transformer architectures over random
instances of linear regression problems, these models' predictions mimic those
of ordinary least squares.
</p>
<p>Towards understanding the mechanisms underlying this phenomenon, we
investigate the dynamics of ICL in transformers with a single linear
self-attention layer trained by gradient flow on linear regression tasks. We
show that despite non-convexity, gradient flow with a suitable random
initialization finds a global minimum of the objective function. At this global
minimum, when given a test prompt of labeled examples from a new prediction
task, the transformer achieves prediction error competitive with the best
linear predictor over the test prompt distribution. We additionally
characterize the robustness of the trained transformer to a variety of
distribution shifts and show that although a number of shifts are tolerated,
shifts in the covariate distribution of the prompts are not. Motivated by this,
we consider a generalized ICL setting where the covariate distributions can
vary across prompts. We show that although gradient flow succeeds at finding a
global minimum in this setting, the trained transformer is still brittle under
mild covariate shifts. We complement this finding with experiments on large,
nonlinear transformer architectures which we show are more robust under
covariate shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Emotion Diarization: Which Emotion Appears When?. (arXiv:2306.12991v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12991">
<div class="article-summary-box-inner">
<span><p>Speech Emotion Recognition (SER) typically relies on utterance-level
solutions. However, emotions conveyed through speech should be considered as
discrete speech events with definite temporal boundaries, rather than
attributes of the entire utterance. To reflect the fine-grained nature of
speech emotions, we propose a new task: Speech Emotion Diarization (SED). Just
as Speaker Diarization answers the question of "Who speaks when?", Speech
Emotion Diarization answers the question of "Which emotion appears when?". To
facilitate the evaluation of the performance and establish a common benchmark
for researchers, we introduce the Zaion Emotion Dataset (ZED), an openly
accessible speech emotion dataset that includes non-acted emotions recorded in
real-life conditions, along with manually-annotated boundaries of emotion
segments within the utterance. We provide competitive baselines and open-source
the code and the pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11224">
<div class="article-summary-box-inner">
<span><p>Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating textual inputs into numerical representations,
capturing the semantics of the text. These models excel in applications like
dense retrieval and semantic textual similarity. This paper details the
development of Jina Embeddings, starting with the creation of high-quality
pairwise and triplet datasets. It underlines the crucial role of data cleaning
in dataset preparation, offers in-depth insights into the model training
process, and concludes with a comprehensive performance evaluation using the
Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model's
awareness of grammatical negation, we construct a novel training and evaluation
dataset of negated and non-negated statements, which we make publicly available
to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11760">
<div class="article-summary-box-inner">
<span><p>Emotional intelligence significantly impacts our daily behaviors and
interactions. Although Large Language Models (LLMs) are increasingly viewed as
a stride toward artificial general intelligence, exhibiting impressive
performance in numerous tasks, it is still uncertain if LLMs can genuinely
grasp psychological emotional stimuli. Understanding and responding to
emotional cues gives humans a distinct advantage in problem-solving. In this
paper, we take the first step towards exploring the ability of LLMs to
understand emotional stimuli. To this end, we first conduct automatic
experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,
Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative
applications that represent comprehensive evaluation scenarios. Our automatic
experiments show that LLMs have a grasp of emotional intelligence, and their
performance can be improved with emotional prompts (which we call
"EmotionPrompt" that combines the original prompt with emotional stimuli),
e.g., 8.00% relative performance improvement in Instruction Induction and 115%
in BIG-Bench. In addition to those deterministic tasks that can be
automatically evaluated using existing metrics, we conducted a human study with
106 participants to assess the quality of generative tasks using both vanilla
and emotional prompts. Our human study results demonstrate that EmotionPrompt
significantly boosts the performance of generative tasks (10.9% average
improvement in terms of performance, truthfulness, and responsibility metrics).
We provide an in-depth discussion regarding why EmotionPrompt works for LLMs
and the factors that may influence its performance. We posit that EmotionPrompt
heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs
interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Specification of MiniDemographicABM.jl: A simplified agent-based demographic model of the UK. (arXiv:2307.16548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16548">
<div class="article-summary-box-inner">
<span><p>This documentation specifies a simplified non-calibrated demographic
agent-based model of the UK, a largely simplified version of the Lone Parent
Model presented in [Gostolil and Silverman 2020]. In the presented model,
individuals of an initial population are subject to ageing, deaths, births,
divorces and marriages throughout a simplified map of towns of the UK. The
specification employs the formal terminology presented in [Elsheikh 2023a]. The
main purpose of the model is to explore and exploit capabilities of the
state-of-the-art Agents.jl Julia package [Datseris2022] in the context of
demographic modeling applications. Implementation is provided via the Julia
package MiniDemographicABM.jl [Elsheikh 2023b]. A specific simulation is
progressed with a user-defined simulation fixed step size on a hourly, daily,
weekly, monthly basis or even an arbitrary user-defined clock rate. The model
can serve for comparative studies if implemented in other agent-based modelling
frameworks and programming languages. Moreover, the model serves as a base
implementation to be adjusted to realistic large-scale socio-economics,
pandemics or immigration studies mainly within a demographic context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations. (arXiv:2308.13081v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13081">
<div class="article-summary-box-inner">
<span><p>This document presents adequate formal terminology for the mathematical
specification of a subset of Agent Based Models (ABMs) in the field of
Demography. The simulation of the targeted ABMs follows a fixedstep
single-clocked pattern. The proposed terminology further improves the model
understanding and can act as a stand-alone protocol for the specification and
optionally the documentation of a significant set of (demographic) ABMs.
Nevertheless, it is imaginable the this terminology can serve as an inspiring
basis for further improvement to the largely-informal widely-used model
documentation and communication O.D.D. protocol [Grimm and et al., 2020,
Amouroux et al., 2010] to reduce many sources of ambiguity which hinder model
replications by other modelers. A published demographic model documentation,
largely simplified version of the Lone Parent Model [Gostoli and Silverman,
2020] is separately published in [Elsheikh, 2023c] as illustration for the
formal terminology presented here. The model was implemented in the Julia
language [Elsheikh, 2023b] based on the Agents.jl julia package [Datseris et
al., 2022].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio Contrastive based Fine-tuning. (arXiv:2309.11895v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.11895">
<div class="article-summary-box-inner">
<span><p>Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12273">
<div class="article-summary-box-inner">
<span><p>Rapid and accurate identification of Venous thromboembolism (VTE), a severe
cardiovascular condition including deep vein thrombosis (DVT) and pulmonary
embolism (PE), is important for effective treatment. Leveraging Natural
Language Processing (NLP) on radiology reports, automated methods have shown
promising advancements in identifying VTE events from retrospective data
cohorts or aiding clinical experts in identifying VTE events from radiology
reports. However, effectively training Deep Learning (DL) and the NLP models is
challenging due to limited labeled medical text data, the complexity and
heterogeneity of radiology reports, and data imbalance. This study proposes
novel method combinations of DL methods, along with data augmentation, adaptive
pre-trained NLP model selection, and a clinical expert NLP rule-based
classifier, to improve the accuracy of VTE identification in unstructured
(free-text) radiology reports. Our experimental results demonstrate the model's
efficacy, achieving an impressive 97\% accuracy and 97\% F1 score in predicting
DVT, and an outstanding 98.3\% accuracy and 98.4\% F1 score in predicting PE.
These findings emphasize the model's robustness and its potential to
significantly contribute to VTE research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the definition of toxicity in NLP. (arXiv:2310.02357v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02357">
<div class="article-summary-box-inner">
<span><p>The fundamental problem in toxicity detection task lies in the fact that the
toxicity is ill-defined. This causes us to rely on subjective and vague data in
models' training, which results in non-robust and non-accurate results: garbage
in - garbage out.
</p>
<p>This work suggests a new, stress-level-based definition of toxicity designed
to be objective and context-aware. On par with it, we also describe possible
ways of applying this new definition to dataset creation and model training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02954">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing, primarily propelled by Large
Language Models (LLMs), have showcased their remarkable capabilities grounded
in in-context learning. A promising avenue for guiding LLMs in intricate
reasoning tasks involves the utilization of intermediate reasoning steps within
the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies
in the effective selection of exemplars for facilitating in-context learning.
In this study, we introduce a framework that leverages Dual Queries and
Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars
for in-context learning. Dual Queries first query LLM to obtain LLM-generated
knowledge such as CoT, then query the retriever to obtain the final exemplars
via both question and the knowledge. Moreover, for the second query, LoRe
employs dimensionality reduction techniques to refine exemplar selection,
ensuring close alignment with the input question's knowledge. Through extensive
experiments, we demonstrate that DQ-LoRe significantly outperforms prior
state-of-the-art methods in the automatic selection of exemplars for GPT-4,
enhancing performance from 92.5% to 94.2%. Our comprehensive analysis further
reveals that DQ-LoRe consistently outperforms retrieval-based approaches in
terms of both performance and adaptability, especially in scenarios
characterized by distribution shifts. DQ-LoRe pushes the boundaries of
in-context learning and opens up new avenues for addressing complex reasoning
challenges. We will release the code soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. (arXiv:2310.05253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05253">
<div class="article-summary-box-inner">
<span><p>Claim verification plays a crucial role in combating misinformation. While
existing works on claim verification have shown promising results, a crucial
piece of the puzzle that remains unsolved is to understand how to verify claims
without relying on human-annotated data, which is expensive to create at a
large scale. Additionally, it is important for models to provide comprehensive
explanations that can justify their decisions and assist human fact-checkers.
This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)
Reasoning that can verify complex claims and generate explanations without the
need for annotated evidence using Large Language Models (LLMs). FOLK leverages
the in-context learning ability of LLMs to translate the claim into a
First-Order-Logic (FOL) clause consisting of predicates, each corresponding to
a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning
over a set of knowledge-grounded question-and-answer pairs to make veracity
predictions and generate explanations to justify its decision-making process.
This process makes our model highly explanatory, providing clear explanations
of its reasoning process in human-readable form. Our experiment results
indicate that FOLK outperforms strong baselines on three datasets encompassing
various claim verification challenges. Our code and data are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance. (arXiv:2310.05991v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.05991">
<div class="article-summary-box-inner">
<span><p>Document-level event argument extraction poses new challenges of long input
and cross-sentence inference compared to its sentence-level counterpart.
However, most prior works focus on capturing the relations between candidate
arguments and the event trigger in each event, ignoring two crucial points: a)
non-argument contextual clue information; b) the relevance among argument
roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling
and latent Role Guidance) model, which contains two novel and effective modules
for the above problem. The Span-Trigger-based Contextual Pooling(STCP)
adaptively selects and aggregates the information of non-argument clue words
based on the context attention weights of specific argument-trigger pairs from
pre-trained model. The Role-based Latent Information Guidance (RLIG) module
constructs latent role representations, makes them interact through
role-interactive encoding to capture semantic relevance, and merges them into
candidate arguments. Both STCP and RLIG introduce no more than 1% new
parameters compared with the base model and can be easily applied to other
event extraction models, which are compact and transplantable. Experiments on
two public datasets show that our SCPRG outperforms previous state-of-the-art
methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents
respectively. Further analyses illustrate the interpretability of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA. (arXiv:2310.06675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06675">
<div class="article-summary-box-inner">
<span><p>Question answering over hybrid contexts is a complex task, which requires the
combination of information extracted from unstructured texts and structured
tables in various ways. Recently, In-Context Learning demonstrated significant
performance advances for reasoning tasks. In this paradigm, a large language
model performs predictions based on a small set of supporting exemplars. The
performance of In-Context Learning depends heavily on the selection procedure
of the supporting exemplars, particularly in the case of HybridQA, where
considering the diversity of reasoning chains and the large size of the hybrid
contexts becomes crucial. In this work, we present Selection of ExEmplars for
hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that
is both representative and diverse. The key novelty of SEER is that it
formulates exemplar selection as a Knapsack Integer Linear Program. The
Knapsack framework provides the flexibility to incorporate diversity
constraints that prioritize exemplars with desirable attributes, and capacity
constraints that ensure that the prompt size respects the provided capacity
budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two
real-world benchmarks for HybridQA, where it outperforms previous exemplar
selection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07328">
<div class="article-summary-box-inner">
<span><p>The success of ChatGPT validates the potential of large language models
(LLMs) in artificial general intelligence (AGI). Subsequently, the release of
LLMs has sparked the open-source community's interest in instruction-tuning,
which is deemed to accelerate ChatGPT's replication process. However, research
on instruction-tuning LLMs in Chinese, the world's most spoken language, is
still in its early stages. Therefore, this paper makes an in-depth empirical
study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that
provides valuable findings for effectively customizing LLMs that can better
respond to Chinese instructions. Specifically, we systematically explore the
impact of LLM bases, parameter-efficient methods, instruction data types, which
are the three most important elements for instruction-tuning. Besides, we also
conduct experiment to study the impact of other factors, e.g., chain-of-thought
data and human-value alignment. We hope that this empirical study can make a
modest contribution to the open Chinese version of ChatGPT. This paper will
release a powerful Chinese LLMs that is comparable to ChatGLM. The code and
data are available at https://github.com/PhoebusSi/Alpaca-CoT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue. (arXiv:2310.07659v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07659">
<div class="article-summary-box-inner">
<span><p>Accurate knowledge selection is critical in knowledge-grounded dialogue
systems. Towards a closer look at it, we offer a novel perspective to organize
existing literature, i.e., knowledge selection coupled with, after, and before
generation. We focus on the third under-explored category of study, which can
not only select knowledge accurately in advance, but has the advantage to
reduce the learning, adjustment, and interpretation burden of subsequent
response generation models, especially LLMs. We propose GATE, a
generator-agnostic knowledge selection method, to prepare knowledge for
subsequent response generation models by selecting context-related knowledge
among different knowledge structures and variable knowledge requirements.
Experimental results demonstrate the superiority of GATE, and indicate that
knowledge selection before generation is a lightweight yet effective way to
facilitate LLMs (e.g., ChatGPT) to generate more informative responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Approach Towards Autoformalization. (arXiv:2310.07957v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07957">
<div class="article-summary-box-inner">
<span><p>Verifying mathematical proofs is difficult, but can be automated with the
assistance of a computer. Autoformalization is the task of automatically
translating natural language mathematics into a formal language that can be
verified by a program. This is a challenging task, and especially for
higher-level mathematics found in research papers. Research paper mathematics
requires large amounts of background and context. In this paper, we propose an
avenue towards tackling autoformalization for research-level mathematics, by
breaking the task into easier and more approachable subtasks: unlinked
formalization (formalization with unlinked definitions and theorems), entity
linking (linking to the proper theorems and definitions), and finally adjusting
types so it passes the type checker. In addition, we present arXiv2Formal, a
benchmark dataset for unlinked formalization consisting of 50 theorems
formalized for the Lean theorem prover sampled from papers on arXiv.org. We
welcome any contributions from the community to future versions of this
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08130">
<div class="article-summary-box-inner">
<span><p>General-purpose text decoding approaches are usually adopted for dialogue
response generation. Although the quality of the generated responses can be
improved with dialogue-specific encoding methods, conversational decoding
methods are still under-explored. Inspired by \citet{wu2023learning} that a
good dialogue feature space should follow the rules of locality and isotropy,
we present a fine-grained conversational decoding method, termed
\textit{isotropic and proximal search (IPS)}. Our method is designed to
generate the semantic-concentrated response, while still maintaining
informativeness and discrimination against the context. Experiments show that
our approach outperforms existing decoding strategies in the dialogue field
across both automatic and human evaluation metrics. More in-depth analyses
further confirm the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization. (arXiv:2310.08394v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08394">
<div class="article-summary-box-inner">
<span><p>Despite recent advances, evaluating how well large language models (LLMs)
follow user instructions remains an open problem. While evaluation methods of
language models have seen a rise in prompt-based approaches, limited work on
the correctness of these methods has been conducted. In this work, we perform a
meta-evaluation of a variety of metrics to quantify how accurately they measure
the instruction-following abilities of LLMs. Our investigation is performed on
grounded query-based summarization by collecting a new short-form, real-world
dataset riSum, containing 300 document-instruction pairs with 3 answers each.
All 900 answers are rated by 3 human annotators. Using riSum, we analyze the
agreement between evaluation methods and human judgment. Finally, we propose
new LLM-based reference-free evaluation methods that improve upon established
baselines and perform on par with costly reference-based metrics that require
high-quality summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Zero-Shot Language Agent for Computer Control with Structured Reflection. (arXiv:2310.08740v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08740">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown increasing capacity at planning and
executing a high-level goal in a live computer environment (e.g. MiniWoB++). To
perform a task, recent works often require a model to learn from trace examples
of the task via either supervised learning or few/many-shot prompting. Without
these trace examples, it remains a challenge how an agent can autonomously
learn and improve its control on a computer, which limits the ability of an
agent to perform a new task. We approach this problem with a zero-shot agent
that requires no given expert traces. Our agent plans for executable actions on
a partially observed environment, and iteratively progresses a task by
identifying and learning from its mistakes via self-reflection and structured
thought management. On the easy tasks of MiniWoB++, we show that our zero-shot
agent often outperforms recent SoTAs, with more efficient reasoning. For tasks
with more complexity, our reflective agent performs on par with prior best
models, even though previous works had the advantages of accessing expert
traces or additional screen information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System. (arXiv:2310.08877v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08877">
<div class="article-summary-box-inner">
<span><p>Developing an efficient retriever to retrieve knowledge from a large-scale
knowledge base (KB) is critical for task-oriented dialogue systems to
effectively handle localized and specialized tasks. However, widely used
generative models such as T5 and ChatGPT often struggle to differentiate subtle
differences among the retrieved KB records when generating responses, resulting
in suboptimal quality of generated responses. In this paper, we propose the
application of maximal marginal likelihood to train a perceptive retriever by
utilizing signals from response generation for supervision. In addition, our
approach goes beyond considering solely retrieved entities and incorporates
various meta knowledge to guide the generator, thus improving the utilization
of knowledge. We evaluate our approach on three task-oriented dialogue datasets
using T5 and ChatGPT as the backbone models. The results demonstrate that when
combined with meta knowledge, the response generator can effectively leverage
high-quality knowledge records from the retriever and enhance the quality of
generated responses. The codes and models of this paper are available at
https://github.com/shenwzh3/MK-TOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling. (arXiv:2310.09135v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09135">
<div class="article-summary-box-inner">
<span><p>In task-oriented dialogue scenarios, cross-domain zero-shot slot filling
plays a vital role in leveraging source domain knowledge to learn a model with
high generalization ability in unknown target domain where annotated data is
unavailable. However, the existing state-of-the-art zero-shot slot filling
methods have limited generalization ability in target domain, they only show
effective knowledge transfer on seen slots and perform poorly on unseen slots.
To alleviate this issue, we present a novel Hierarchical Contrastive Learning
Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarse-
to fine-grained contrastive learning based on Gaussian-distributed embedding to
learn the generalized deep semantic relations between utterance-tokens, by
optimizing inter- and intra-token distribution distance. This encourages HiCL
to generalize to the slot types unseen at training phase. Furthermore, we
present a new iterative label set semantics inference method to unbiasedly and
separately evaluate the performance of unseen slot types which entangled with
their counterparts (i.e., seen slot types) in the previous zero-shot slot
filling evaluation methods. The extensive empirical experiments on four
datasets demonstrate that the proposed method achieves comparable or even
better performance than the current state-of-the-art zero-shot slot filling
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09886">
<div class="article-summary-box-inner">
<span><p>Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10477">
<div class="article-summary-box-inner">
<span><p>The rapid advancement of large language models (LLMs) presents both
opportunities and challenges, particularly concerning unintentional generation
of harmful and toxic responses. While the traditional alignment methods strive
to steer LLMs towards desired performance and shield them from malicious
content, this study proposes a novel alignment strategy rooted in mistake
analysis by exposing LLMs to flawed outputs purposefully and then conducting a
thorough assessment to fully comprehend internal reasons via natural language
analysis. Thus, toxic responses can be transformed into instruction tuning
corpus for model alignment, and LLMs can not only be deterred from generating
flawed responses but also trained to self-criticize, leveraging its innate
ability to discriminate toxic content. Experimental results demonstrate that
the proposed method outperforms conventional alignment techniques for safety
instruction following, while maintaining superior efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11097">
<div class="article-summary-box-inner">
<span><p>The Italian Digital Media Observatory (IDMO) project, part of a European
initiative, focuses on countering disinformation and fake news. This report
outlines contributions from Rai-CRITS to the project, including: (i) the
creation of novel datasets for testing technologies (ii) development of an
automatic model for categorizing Pagella Politica verdicts to facilitate
broader analysis (iii) creation of an automatic model for recognizing textual
entailment with exceptional accuracy on the FEVER dataset (iv) assessment using
GPT-4 to identify textual entailmen (v) a game to raise awareness about fake
news at national events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11638">
<div class="article-summary-box-inner">
<span><p>Previous studies have relied on existing question-answering benchmarks to
evaluate the knowledge stored in large language models (LLMs). However, this
approach has limitations regarding factual knowledge coverage, as it mostly
focuses on generic domains which may overlap with the pretraining data. This
paper proposes a framework to systematically assess the factual knowledge of
LLMs by leveraging knowledge graphs (KGs). Our framework automatically
generates a set of questions and expected answers from the facts stored in a
given KG, and then evaluates the accuracy of LLMs in answering these questions.
We systematically evaluate the state-of-the-art LLMs with KGs in generic and
specific domains. The experiment shows that ChatGPT is consistently the top
performer across all domains. We also find that LLMs performance depends on the
instruction finetuning, domain and question complexity and is prone to
adversarial context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11710">
<div class="article-summary-box-inner">
<span><p>Aphasia, a language disorder resulting from brain damage, requires accurate
identification of specific aphasia types, such as Broca's and Wernicke's
aphasia, for effective treatment. However, little attention has been paid to
developing methods to detect different types of aphasia. Recognizing the
importance of analyzing co-speech gestures for distinguish aphasia types, we
propose a multimodal graph neural network for aphasia type detection using
speech and corresponding gesture patterns. By learning the correlation between
the speech and gesture modalities for each aphasia type, our model can generate
textual representations sensitive to gesture information, leading to accurate
aphasia type detection. Extensive experiments demonstrate the superiority of
our approach over existing methods, achieving state-of-the-art results (F1
84.2\%). We also show that gesture features outperform acoustic features,
highlighting the significance of gesture expression in detecting aphasia types.
We provide the codes for reproducibility purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11772">
<div class="article-summary-box-inner">
<span><p>Topic segmentation is critical for obtaining structured long documents and
improving downstream tasks like information retrieval. Due to its ability of
automatically exploring clues of topic shift from a large amount of labeled
data, recent supervised neural models have greatly promoted the development of
long document topic segmentation, but leaving the deeper relationship of
semantic coherence and topic segmentation underexplored. Therefore, this paper
enhances the supervised model's ability to capture coherence from both
structure and similarity perspectives to further improve the topic segmentation
performance, including the Topic-aware Sentence Structure Prediction (TSSP) and
Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is
proposed to force the model to comprehend structural information by learning
the original relations of adjacent sentences in a disarrayed document, which is
constructed by jointly disrupting the original document at the topic and
sentence levels. In addition, we utilize inter- and intra-topic information to
construct contrastive samples and design the CSSL objective to ensure that the
sentences representations in the same topic have higher semantic similarity,
while those in different topics are less similar. Extensive experiments show
that the Longformer with our approach significantly outperforms old
state-of-the-art (SOTA) methods. Our approach improves $F_{1}$ of old SOTA by
3.42 (73.74 -&gt; 77.16) and reduces $P_{k}$ by 1.11 points (15.0 -&gt; 13.89) on
WIKI-727K and achieves an average reduction of 0.83 points on $P_{k}$ on
WikiSection. The average $P_{k}$ drop of 2.82 points on the two out-of-domain
datasets also illustrates the robustness of our approach
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations. (arXiv:2310.12489v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12489">
<div class="article-summary-box-inner">
<span><p>Zero-shot classification enables text to be classified into classes not seen
during training. In this research, we investigate the effectiveness of
pre-trained language models to accurately classify responses from Doctors and
AI in health consultations through zero-shot learning. Our study aims to
determine whether these models can effectively detect if a text originates from
human or AI models without specific corpus training. We collect responses from
doctors to patient inquiries about their health and pose the same
question/response to AI models. While zero-shot language models show a good
understanding of language in general, they have limitations in classifying
doctor and AI responses in healthcare consultations. This research lays the
groundwork for further research into this field of medical text classification,
informing the development of more effective approaches to accurately classify
doctor-generated and AI-generated text in health consultations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12531">
<div class="article-summary-box-inner">
<span><p>Most multilingual vision-and-language (V&amp;L) research aims to accomplish
multilingual and multimodal capabilities within one model. However, the
scarcity of multilingual captions for images has hindered the development. To
overcome this obstacle, we propose ICU, Image Caption Understanding, which
divides a V&amp;L task into two stages: a V&amp;L model performs image captioning in
English, and a multilingual language model (mLM), in turn, takes the caption as
the alt text and performs crosslingual language understanding. The burden of
multilingual processing is lifted off V&amp;L model and placed on mLM. Since the
multilingual text data is relatively of higher abundance and quality, ICU can
facilitate the conquering of language barriers for V&amp;L models. In experiments
on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can
achieve new state-of-the-art results for five languages, and comparable results
for the rest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.12821">
<div class="article-summary-box-inner">
<span><p>Current gesture recognition systems primarily focus on identifying gestures
within a predefined set, leaving a gap in connecting these gestures to
interactive GUI elements or system functions (e.g., linking a 'thumb-up'
gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture
understanding and grounding framework leveraging large language models (LLMs).
Gesture descriptions are formulated based on hand landmark coordinates from
gesture videos and fed into our dual-agent dialogue system. A gesture agent
deciphers these descriptions and queries about the interaction context (e.g.,
interface, history, gaze data), which a context agent organizes and provides.
Following iterative exchanges, the gesture agent discerns user intent,
grounding it to an interactive function. We validated the gesture description
module using public first-view and third-view gesture datasets and tested the
whole system in two real-world settings: video streaming and smart home IoT
control. The highest zero-shot Top-5 grounding accuracies are 80.11% for video
streaming and 90.78% for smart home tasks, showing potential of the new gesture
understanding paradigm.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-23 23:11:43.158165132 UTC">2023-10-23 23:11:43 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>