<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-30T01:30:00Z">11-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CoNAL: Anticipating Outliers with Large Language Models. (arXiv:2211.15718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15718">
<div class="article-summary-box-inner">
<span><p>In many task settings, text classification models are likely to encounter
examples from novel classes on which they cannot predict correctly. Selective
prediction, in which models abstain on low-confidence examples, provides a
possible solution, but existing models are often overly confident on OOD
examples. To remedy this overconfidence, we introduce Contrastive
Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD
examples representative of novel classes, then trains to decrease confidence on
them. First, we generate OOD examples by prompting a large language model
twice: we prompt it to enumerate relevant novel labels, then generate examples
from each novel class matching the task format. Second, we train our classifier
with a novel contrastive objective that encourages lower confidence on
generated OOD examples than training examples. When trained with CoNAL,
classifiers improve in their ability to detect and abstain on OOD examples over
prior methods by an average of 2.3% AUAC and 5.5% AUROC across 4 NLP datasets,
with no cost to in-distribution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Language Generation for Language Learning Items. (arXiv:2211.15731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15731">
<div class="article-summary-box-inner">
<span><p>This work aims to employ natural language generation (NLG) to rapidly
generate items for English language learning applications: this requires both
language models capable of generating fluent, high-quality English, and to
control the output of the generation to match the requirements of the relevant
items. We experiment with deep pretrained models for this task, developing
novel methods for controlling items for factors relevant in language learning:
diverse sentences for different proficiency levels and argument structure to
test grammar. Human evaluation demonstrates high grammatically scores for all
models (3.4 and above out of 4), and higher length (24%) and complexity (9%)
over the baseline for the advanced proficiency model. Our results show that we
can achieve strong performance while adding additional control to ensure
diverse, tailored content for individual users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mathematically Modeling the Lexicon Entropy of Emergent Language. (arXiv:2211.15783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15783">
<div class="article-summary-box-inner">
<span><p>We formulate a stochastic process, FiLex, as a mathematical model of lexicon
entropy in deep learning-based emergent language systems. Defining a model
mathematically allows it to generate clear predictions which can be directly
and decisively tested. We empirically verify across four different environments
that FiLex predicts the correct correlation between hyperparameters (training
steps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax
temperature) and the emergent language's entropy in 20 out of 20
environment-hyperparameter combinations. Furthermore, our experiments reveal
that different environments show diverse relationships between their
hyperparameters and entropy which demonstrates the need for a model which can
make well-defined predictions at a precise level of granularity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Neural Entity Alignment with Compatibility. (arXiv:2211.15833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15833">
<div class="article-summary-box-inner">
<span><p>Entity Alignment (EA) aims to find equivalent entities between two Knowledge
Graphs (KGs). While numerous neural EA models have been devised, they are
mainly learned using labelled data only. In this work, we argue that different
entities within one KG should have compatible counterparts in the other KG due
to the potential dependencies among the entities. Making compatible predictions
thus should be one of the goals of training an EA model along with fitting the
labelled data: this aspect however is neglected in current methods. To power
neural EA models with compatibility, we devise a training framework by
addressing three problems: (1) how to measure the compatibility of an EA model;
(2) how to inject the property of being compatible into an EA model; (3) how to
optimise parameters of the compatibility model. Extensive experiments on
widely-used datasets demonstrate the advantages of integrating compatibility
within EA models. In fact, state-of-the-art neural EA models trained within our
framework using just 5\% of the labelled data can achieve comparable
effectiveness with supervised training using 20\% of the labelled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs. (arXiv:2211.15845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15845">
<div class="article-summary-box-inner">
<span><p>Existing knowledge graph (KG) embedding models have primarily focused on
static KGs. However, real-world KGs do not remain static, but rather evolve and
grow in tandem with the development of KG applications. Consequently, new facts
and previously unseen entities and relations continually emerge, necessitating
an embedding model that can quickly learn and transfer new knowledge through
growth. Motivated by this, we delve into an expanding field of KG embedding in
this paper, i.e., lifelong KG embedding. We consider knowledge transfer and
retention of the learning on growing snapshots of a KG without having to learn
embeddings from scratch. The proposed model includes a masked KG autoencoder
for embedding learning and update, with an embedding transfer strategy to
inject the learned knowledge into the new entity and relation embeddings, and
an embedding regularization method to avoid catastrophic forgetting. To
investigate the impacts of different aspects of KG growth, we construct four
datasets to evaluate the performance of lifelong KG embedding. Experimental
results show that the proposed model outperforms the state-of-the-art inductive
and lifelong embedding baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClueWeb22: 10 Billion Web Documents with Rich Information. (arXiv:2211.15848v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15848">
<div class="article-summary-box-inner">
<span><p>ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10
billion web pages affiliated with rich information. Its design was influenced
by the need for a high quality, large scale web corpus to support a range of
academic and industry research, for example, in information systems,
retrieval-augmented AI systems, and model pretraining. Compared with earlier
ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of
higher-quality, and aligned with the document distributions in commercial web
search. Besides raw HTML, ClueWeb22 includes rich information about the web
pages provided by industry-standard document understanding systems, including
the visual representation of pages rendered by a web browser, parsed HTML
structure information from a neural network parser, and pre-processed cleaned
document text to lower the barrier to entry. Many of these signals have been
widely used in industry but are available to the research community for the
first time at this scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Opinion Summarization with GPT-3. (arXiv:2211.15914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15914">
<div class="article-summary-box-inner">
<span><p>Very large language models such as GPT-3 have shown impressive performance
across a wide variety of tasks, including text summarization. In this paper, we
show that this strong performance extends to opinion summarization. We explore
several pipeline methods for applying GPT-3 to summarize a large collection of
user reviews in a zero-shot fashion, notably approaches based on recursive
summarization and selecting salient content to summarize through supervised
clustering or extraction. On two datasets, an aspect-oriented summarization
dataset of hotel reviews and a generic summarization dataset of Amazon and Yelp
reviews, we show that the GPT-3 models achieve very strong performance in human
evaluation. We argue that standard evaluation metrics do not reflect this, and
evaluate against several new measures targeting faithfulness, factuality, and
genericity to contrast these different methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BotSIM: An End-to-End Bot Simulation Toolkit for Commercial Task-Oriented Dialog Systems. (arXiv:2211.15916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15916">
<div class="article-summary-box-inner">
<span><p>We introduce BotSIM, a modular, open-source Bot SIMulation environment with
dialog generation, user simulation and conversation analytics capabilities.
BotSIM aims to serve as a one-stop solution for large-scale data-efficient
end-to-end evaluation, diagnosis and remediation of commercial task-oriented
dialog (TOD) systems to significantly accelerate commercial bot development and
evaluation, reduce cost and time-to-market. BotSIM adopts a layered design
comprising the infrastructure layer, the adaptor layer and the application
layer. The infrastructure layer hosts key models and components to support
BotSIM's major functionalities via a streamlined
"generation-simulation-remediation" pipeline. The adaptor layer is used to
extend BotSIM to accommodate new bot platforms. The application layer provides
a suite of command line tools and a Web App to significantly lower the entry
barrier for BotSIM users such as bot admins or practitioners. In this report,
we focus on the technical designs of various system components. A detailed case
study using Einstein BotBuilder is also presented to show how to apply BotSIM
pipeline for bot evaluation and remediation. The detailed system descriptions
can be found in our system demo paper. The toolkit is available at:
https://github.com/salesforce/BotSIM .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compressing Cross-Lingual Multi-Task Models at Qualtrics. (arXiv:2211.15927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15927">
<div class="article-summary-box-inner">
<span><p>Experience management is an emerging business area where organizations focus
on understanding the feedback of customers and employees in order to improve
their end-to-end experiences. This results in a unique set of machine learning
problems to help understand how people feel, discover issues they care about,
and find which actions need to be taken on data that are different in content
and distribution from traditional NLP domains. In this paper, we present a case
study of building text analysis applications that perform multiple
classification tasks efficiently in 12 languages in the nascent business area
of experience management. In order to scale up modern ML methods on experience
data, we leverage cross lingual and multi-task modeling techniques to
consolidate our models into a single deployment to avoid overhead. We also make
use of model compression and model distillation to reduce overall inference
latency and hardware cost to the level acceptable for business needs while
maintaining model prediction quality. Our findings show that multi-task
modeling improves task performance for a subset of experience management tasks
in both XLM-R and mBert architectures. Among the compressed architectures we
explored, we found that MiniLM achieved the best compression/performance
tradeoff. Our case study demonstrates a speedup of up to 15.61x with 2.60%
average task degradation (or 3.29x speedup with 1.71% degradation) and
estimated savings of 44% over using the original full-size model. These results
demonstrate a successful scaling up of text classification for the challenging
new area of ML for experience management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending the Subwording Model of Multilingual Pretrained Models for New Languages. (arXiv:2211.15965v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15965">
<div class="article-summary-box-inner">
<span><p>Multilingual pretrained models are effective for machine translation and
cross-lingual processing because they contain multiple languages in one model.
However, they are pretrained after their tokenizers are fixed; therefore it is
difficult to change the vocabulary after pretraining. When we extend the
pretrained models to new languages, we must modify the tokenizers
simultaneously. In this paper, we add new subwords to the SentencePiece
tokenizer to apply a multilingual pretrained model to new languages (Inuktitut
in this paper). In our experiments, we segmented Inuktitut sentences into
subwords without changing the segmentation of already pretrained languages, and
applied the mBART-50 pretrained model to English-Inuktitut translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratizing Machine Learning for Interdisciplinary Scholars: Report on Organizing the NLP+CSS Online Tutorial Series. (arXiv:2211.15971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15971">
<div class="article-summary-box-inner">
<span><p>Many scientific fields -- including biology, health, education, and the
social sciences -- use machine learning (ML) to help them analyze data at an
unprecedented scale. However, ML researchers who develop advanced methods
rarely provide detailed tutorials showing how to apply these methods. Existing
tutorials are often costly to participants, presume extensive programming
knowledge, and are not tailored to specific application fields. In an attempt
to democratize ML methods, we organized a year-long, free, online tutorial
series targeted at teaching advanced natural language processing (NLP) methods
to computational social science (CSS) scholars. Two organizers worked with
fifteen subject matter experts to develop one-hour presentations with hands-on
Python code for a range of ML methods and use cases, from data pre-processing
to analyzing temporal variation of language change. Although live participation
was more limited than expected, a comparison of pre- and post-tutorial surveys
showed an increase in participants' perceived knowledge of almost one point on
a 7-point Likert scale. Furthermore, participants asked thoughtful questions
during tutorials and engaged readily with tutorial content afterwards, as
demonstrated by 10K~total views of posted tutorial recordings. In this report,
we summarize our organizational efforts and distill five principles for
democratizing ML+X tutorials. We hope future organizers improve upon these
principles and continue to lower barriers to developing ML skills for
researchers of all fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Neural Discourse Deixis Resolution in Dialogue. (arXiv:2211.15980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15980">
<div class="article-summary-box-inner">
<span><p>We adapt Lee et al.'s (2018) span-based entity coreference model to the task
of end-to-end discourse deixis resolution in dialogue, specifically by
proposing extensions to their model that exploit task-specific characteristics.
The resulting model, dd-utt, achieves state-of-the-art results on the four
datasets in the CODI-CRAC 2021 shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalized Open Information Extraction. (arXiv:2211.15987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15987">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OpenIE) facilitates the open-domain discovery of
textual facts. However, the prevailing solutions evaluate OpenIE models on
in-domain test sets aside from the training corpus, which certainly violates
the initial task principle of domain-independence. In this paper, we propose to
advance OpenIE towards a more realistic scenario: generalizing over unseen
target domains with different data distributions from the source training
domains, termed Generalized OpenIE. For this purpose, we first introduce GLOBE,
a large-scale human-annotated multi-domain OpenIE benchmark, to examine the
robustness of recent OpenIE models to domain shifts, and the relative
performance degradation of up to 70% implies the challenges of generalized
OpenIE. Then, we propose DragonIE, which explores a minimalist graph expression
of textual fact: directed acyclic graph, to improve the OpenIE generalization.
Extensive experiments demonstrate that DragonIE beats the previous methods in
both in-domain and out-of-domain settings by as much as 6.0% in F1 score
absolutely, but there is still ample room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffG-RL: Leveraging Difference between State and Common Sense. (arXiv:2211.16002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16002">
<div class="article-summary-box-inner">
<span><p>Taking into account background knowledge as the context has always been an
important part of solving tasks that involve natural language. One
representative example of such tasks is text-based games, where players need to
make decisions based on both description text previously shown in the game, and
their own background knowledge about the language and common sense. In this
work, we investigate not simply giving common sense, as can be seen in prior
research, but also its effective usage. We assume that a part of the
environment states different from common sense should constitute one of the
grounds for action selection. We propose a novel agent, DiffG-RL, which
constructs a Difference Graph that organizes the environment states and common
sense by means of interactive objects with a dedicated graph encoder. DiffG-RL
also contains a framework for extracting the appropriate amount and
representation of common sense from the source to support the construction of
the graph. We validate DiffG-RL in experiments with text-based games that
require common sense and show that it outperforms baselines by 17% of scores.
The code is available at https://github.com/ibm/diffg-rl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Enhanced Contrastive Learning for Solving Math Word Problems. (arXiv:2211.16022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16022">
<div class="article-summary-box-inner">
<span><p>Solving math word problems is the task that analyses the relation of
quantities and requires an accurate understanding of contextual natural
language information. Recent studies show that current models rely on shallow
heuristics to predict solutions and could be easily misled by small textual
perturbations. To address this problem, we propose a Textual Enhanced
Contrastive Learning framework, which enforces the models to distinguish
semantically similar examples while holding different mathematical logic. We
adopt a self-supervised manner strategy to enrich examples with subtle textual
variance by textual reordering or problem re-construction. We then retrieve the
hardest to differentiate samples from both equation and textual perspectives
and guide the model to learn their representations. Experimental results show
that our method achieves state-of-the-art on both widely used benchmark
datasets and also exquisitely designed challenge datasets in English and
Chinese. \footnote{Our code and data is available at
\url{https://github.com/yiyunya/Textual_CL_MWP}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Multi-Answer Retrieval with Determinantal Point Processes. (arXiv:2211.16029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16029">
<div class="article-summary-box-inner">
<span><p>Often questions provided to open-domain question answering systems are
ambiguous. Traditional QA systems that provide a single answer are incapable of
answering ambiguous questions since the question may be interpreted in several
ways and may have multiple distinct answers. In this paper, we address
multi-answer retrieval which entails retrieving passages that can capture
majority of the diverse answers to the question. We propose a re-ranking based
approach using Determinantal point processes utilizing BERT as kernels. Our
method jointly considers query-passage relevance and passage-passage
correlation to retrieve passages that are both query-relevant and diverse.
Results demonstrate that our re-ranking technique outperforms state-of-the-art
method on the AmbigQA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16031">
<div class="article-summary-box-inner">
<span><p>Syntax is a latent hierarchical structure which underpins the robust and
compositional nature of human language. An active line of inquiry is whether
large pretrained language models (LLMs) are able to acquire syntax by training
on text alone; understanding a model's syntactic capabilities is essential to
understanding how it processes and makes use of language. In this paper, we
propose a new method, SSUD, which allows for the induction of syntactic
structures without supervision from gold-standard parses. Instead, we seek to
define formalism-agnostic, model-intrinsic syntactic parses by using a property
of syntactic relations: syntactic substitutability. We demonstrate both
quantitative and qualitative gains on dependency parsing tasks using SSUD, and
induce syntactic structures which we hope provide clarity into LLMs and
linguistic representations, alike.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Extraction Attack against Self-supervised Speech Models. (arXiv:2211.16044v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16044">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) speech models generate meaningful
representations of given clips and achieve incredible performance across
various downstream tasks. Model extraction attack (MEA) often refers to an
adversary stealing the functionality of the victim model with only query
access. In this work, we study the MEA problem against SSL speech model with a
small number of queries. We propose a two-stage framework to extract the model.
In the first stage, SSL is conducted on the large-scale unlabeled corpus to
pre-train a small speech model. Secondly, we actively sample a small portion of
clips from the unlabeled corpus and query the target model with these clips to
acquire their representations as labels for the small model's second-stage
training. Experiment results show that our sampling methods can effectively
extract the target model without knowing any information about its model
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating and reducing the distance between synthetic and real speech distributions. (arXiv:2211.16049v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16049">
<div class="article-summary-box-inner">
<span><p>While modern Text-to-Speech (TTS) systems can produce speech rated highly in
terms of subjective evaluation, the distance between real and synthetic speech
distributions remains understudied, where we use the term \textit{distribution}
to mean the sample space of all possible real speech recordings from a given
set of speakers; or of the synthetic samples that could be generated for the
same set of speakers. We evaluate the distance of real and synthetic speech
distributions along the dimensions of the acoustic environment, speaker
characteristics and prosody using a range of speech processing measures and the
respective Wasserstein distances of their distributions. We reduce these
distribution distances along said dimensions by providing utterance-level
information derived from the measures to the model and show they can be
generated at inference time. The improvements to the dimensions translate to
overall distribution distance reduction approximated using Automatic Speech
Recognition (ASR) by evaluating the fitness of the synthetic data as training
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering. (arXiv:2211.16093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16093">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) models are shown to be insensitive to large
perturbations to inputs; that is, they make correct and confident predictions
even when given largely perturbed inputs from which humans can not correctly
derive answers. In addition, QA models fail to generalize to other domains and
adversarial test sets, while humans maintain high accuracy. Based on these
observations, we assume that QA models do not use intended features necessary
for human reading but rely on spurious features, causing the lack of
generalization ability. Therefore, we attempt to answer the question: If the
overconfident predictions of QA models for various types of perturbations are
penalized, will the out-of-distribution (OOD) generalization be improved? To
prevent models from making confident predictions on perturbed inputs, we first
follow existing studies and maximize the entropy of the output probability for
perturbed inputs. However, we find that QA models trained to be sensitive to a
certain perturbation type are often insensitive to unseen types of
perturbations. Thus, we simultaneously maximize the entropy for the four
perturbation types (i.e., word- and sentence-level shuffling and deletion) to
further close the gap between models and humans. Contrary to our expectations,
although models become sensitive to the four types of perturbations, we find
that the OOD generalization is not improved. Moreover, the OOD generalization
is sometimes degraded after entropy maximization. Making unconfident
predictions on largely perturbed inputs per se may be beneficial to gaining
human trust. However, our negative results suggest that researchers should pay
attention to the side effect of entropy maximization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency-aware Self-training for Entity Alignment. (arXiv:2211.16101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16101">
<div class="article-summary-box-inner">
<span><p>Entity Alignment (EA), which aims to detect entity mappings (i.e. equivalent
entity pairs) in different Knowledge Graphs (KGs), is critical for KG fusion.
Neural EA methods dominate current EA research but still suffer from their
reliance on labelled mappings. To solve this problem, a few works have explored
boosting the training of EA models with self-training, which adds confidently
predicted mappings into the training data iteratively. Though the effectiveness
of self-training can be glimpsed in some specific settings, we still have very
limited knowledge about it. One reason is the existing works concentrate on
devising EA models and only treat self-training as an auxiliary tool. To fill
this knowledge gap, we change the perspective to self-training to shed light on
it. In addition, the existing self-training strategies have limited impact
because they introduce either much False Positive noise or a low quantity of
True Positive pseudo mappings. To improve self-training for EA, we propose
exploiting the dependencies between entities, a particularity of EA, to
suppress the noise without hurting the recall of True Positive mappings.
Through extensive experiments, we show that the introduction of dependency
makes the self-training strategy for EA reach a new level. The value of
self-training in alleviating the reliance on annotation is actually much higher
than what has been realised. Furthermore, we suggest future study on smart data
annotation to break the ceiling of EA performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Query-Focused Summarization with Prefix-Merging. (arXiv:2211.16164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16164">
<div class="article-summary-box-inner">
<span><p>Query-focused summarization has been considered as an important extension for
text summarization. It aims to generate a concise highlight for a given query.
Different from text summarization, query-focused summarization has long been
plagued by the problem of lacking high-quality large-scale datasets. In this
paper, we investigate the idea that whether we can integrate and transfer the
knowledge of text summarization and question answering to assist the few-shot
learning in query-focused summarization. Here, we propose prefix-merging, a
prefix-based pretraining strategy for few-shot learning in query-focused
summarization. Drawn inspiration from prefix-tuning, we are allowed to
integrate the task knowledge from text summarization and question answering
into a properly designed prefix and apply the merged prefix to query-focused
summarization. With only a small amount of trainable parameters, prefix-merging
outperforms fine-tuning on query-focused summarization. We further discuss the
influence of different prefix designs and propose a visualized explanation for
how prefix-merging works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnings from Technological Interventions in a Low Resource Language: Enhancing Information Access in Gondi. (arXiv:2211.16172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16172">
<div class="article-summary-box-inner">
<span><p>The primary obstacle to developing technologies for low-resource languages is
the lack of representative, usable data. In this paper, we report the
deployment of technology-driven data collection methods for creating a corpus
of more than 60,000 translations from Hindi to Gondi, a low-resource vulnerable
language spoken by around 2.3 million tribal people in south and central India.
During this process, we help expand information access in Gondi across 2
different dimensions (a) The creation of linguistic resources that can be used
by the community, such as a dictionary, children's stories, Gondi translations
from multiple sources and an Interactive Voice Response (IVR) based mass
awareness platform; (b) Enabling its use in the digital domain by developing a
Hindi-Gondi machine translation model, which is compressed by nearly 4 times to
enable it's edge deployment on low-resource edge devices and in areas of little
to no internet connectivity. We also present preliminary evaluations of
utilizing the developed machine translation model to provide assistance to
volunteers who are involved in collecting more data for the target language.
Through these interventions, we not only created a refined and evaluated corpus
of 26,240 Hindi-Gondi translations that was used for building the translation
model but also engaged nearly 850 community members who can help take Gondi
onto the internet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUNI Submission in WMT22 General Task. (arXiv:2211.16174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16174">
<div class="article-summary-box-inner">
<span><p>We present the CUNI-Bergamot submission for the WMT22 General translation
task. We compete in English$\rightarrow$Czech direction. Our submission further
explores block backtranslation techniques. Compared to the previous work, we
measure performance in terms of COMET score and named entities translation
accuracy. We evaluate performance of MBR decoding compared to traditional mixed
backtranslation training and we show a possible synergy when using both of the
techniques simultaneously. The results show that both approaches are effective
means of improving translation quality and they yield even better results when
combined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16198">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free "name-only transfer" in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoCAD: Automatically Generating Counterfactuals for Mitigating Shortcut Learning. (arXiv:2211.16202v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16202">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown the impressive efficacy of counterfactually
augmented data (CAD) for reducing NLU models' reliance on spurious features and
improving their generalizability. However, current methods still heavily rely
on human efforts or task-specific designs to generate counterfactuals, thereby
impeding CAD's applicability to a broad range of NLU tasks. In this paper, we
present AutoCAD, a fully automatic and task-agnostic CAD generation framework.
AutoCAD first leverages a classifier to unsupervisedly identify rationales as
spans to be intervened, which disentangles spurious and causal features. Then,
AutoCAD performs controllable generation enhanced by unlikelihood training to
produce diverse counterfactuals. Extensive evaluations on multiple
out-of-domain and challenge benchmarks demonstrate that AutoCAD consistently
and significantly boosts the out-of-distribution performance of powerful
pre-trained models across different NLU tasks, which is comparable or even
better than previous state-of-the-art human-in-the-loop or task-specific CAD
methods. The code is publicly available at https://github.com/thu-coai/AutoCAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Shortcut Solution Do Question Answering Models Prefer to Learn?. (arXiv:2211.16220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16220">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) models for reading comprehension tend to learn
shortcut solutions rather than the solutions intended by QA datasets. QA models
that have learned shortcut solutions can achieve human-level performance in
shortcut examples where shortcuts are valid, but these same behaviors degrade
generalization potential on anti-shortcut examples where shortcuts are invalid.
Various methods have been proposed to mitigate this problem, but they do not
fully take the characteristics of shortcuts themselves into account. We assume
that the learnability of shortcuts, i.e., how easy it is to learn a shortcut,
is useful to mitigate the problem. Thus, we first examine the learnability of
the representative shortcuts on extractive and multiple-choice QA datasets.
Behavioral tests using biased training sets reveal that shortcuts that exploit
answer positions and word-label correlations are preferentially learned for
extractive and multiple-choice QA, respectively. We find that the more
learnable a shortcut is, the flatter and deeper the loss landscape is around
the shortcut solution in the parameter space. We also find that the
availability of the preferred shortcuts tends to make the task easier to
perform from an information-theoretic viewpoint. Lastly, we experimentally show
that the learnability of shortcuts can be utilized to construct an effective QA
training set; the more learnable a shortcut is, the smaller the proportion of
anti-shortcut examples required to achieve comparable performance on shortcut
and anti-shortcut examples. We claim that the learnability of shortcuts should
be considered when designing mitigation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Measuring Tools: An Automatic Evaluation of Semantic Metrics for Text Corpora. (arXiv:2211.16259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16259">
<div class="article-summary-box-inner">
<span><p>The ability to compare the semantic similarity between text corpora is
important in a variety of natural language processing applications. However,
standard methods for evaluating these metrics have yet to be established. We
propose a set of automatic and interpretable measures for assessing the
characteristics of corpus-level semantic similarity metrics, allowing sensible
comparison of their behavior. We demonstrate the effectiveness of our
evaluation measures in capturing fundamental characteristics by evaluating them
on a collection of classical and state-of-the-art metrics. Our measures
revealed that recently-developed metrics are becoming better in identifying
semantic distributional mismatch while classical metrics are more sensitive to
perturbations in the surface text levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation. (arXiv:2211.16270v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16270">
<div class="article-summary-box-inner">
<span><p>The neural transducer is an end-to-end model for automatic speech recognition
(ASR). While the model is well-suited for streaming ASR, the training process
remains challenging. During training, the memory requirements may quickly
exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence
lengths. In this work, we analyze the time and space complexity of a typical
transducer training setup. We propose a memory-efficient training method that
computes the transducer loss and gradients sample by sample. We present
optimizations to increase the efficiency and parallelism of the sample-wise
method. In a set of thorough benchmarks, we show that our sample-wise method
significantly reduces memory usage, and performs at competitive speed when
compared to the default batched computation. As a highlight, we manage to
compute the transducer loss and gradients for a batch size of 1024, and audio
length of 40 seconds, using only 6 GB of memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches. (arXiv:2211.16285v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16285">
<div class="article-summary-box-inner">
<span><p>Text classification of unseen classes is a challenging Natural Language
Processing task and is mainly attempted using two different types of
approaches. Similarity-based approaches attempt to classify instances based on
similarities between text document representations and class description
representations. Zero-shot text classification approaches aim to generalize
knowledge gained from a training task by assigning appropriate labels of
unknown classes to text documents. Although existing studies have already
investigated individual approaches to these categories, the experiments in
literature do not provide a consistent comparison. This paper addresses this
gap by conducting a systematic evaluation of different similarity-based and
zero-shot approaches for text classification of unseen classes. Different
state-of-the-art approaches are benchmarked on four text classification
datasets, including a new dataset from the medical domain. Additionally, novel
SimCSE and SBERT-based baselines are proposed, as other baselines used in
existing work yield weak classification results and are easily outperformed.
Finally, the novel similarity-based Lbl2TransformerVec approach is presented,
which outperforms previous state-of-the-art approaches in unsupervised text
classification. Our experiments show that similarity-based approaches
significantly outperform zero-shot approaches in most cases. Additionally,
using SimCSE or SBERT embeddings instead of simpler text representations
increases similarity-based classification results even further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable speech synthesis by learning discrete phoneme-level prosodic representations. (arXiv:2211.16307v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16307">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel method for phoneme-level prosody control of
F0 and duration using intuitive discrete labels. We propose an unsupervised
prosodic clustering process which is used to discretize phoneme-level F0 and
duration features from a multispeaker speech dataset. These features are fed as
an input sequence of prosodic labels to a prosody encoder module which augments
an autoregressive attention-based text-to-speech model. We utilize various
methods in order to improve prosodic control range and coverage, such as
augmentation, F0 normalization, balanced clustering for duration and
speaker-independent clustering. The final model enables fine-grained
phoneme-level prosody control for all speakers contained in the training set,
while maintaining the speaker identity. Instead of relying on reference
utterances for inference, we introduce a prior prosody encoder which learns the
style of each speaker and enables speech synthesis without the requirement of
reference audio. We also fine-tune the multispeaker model to unseen speakers
with limited amounts of data, as a realistic application scenario and show that
the prosody control capabilities are maintained, verifying that the
speaker-independent prosodic clustering is effective. Experimental results show
that the model has high output speech quality and that the proposed method
allows efficient prosody control within each speaker's range despite the
variability that a multispeaker setting introduces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Evaluation Metrics for Code-Switching Automatic Speech Recognition. (arXiv:2211.16319v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16319">
<div class="article-summary-box-inner">
<span><p>Code-switching poses a number of challenges and opportunities for
multilingual automatic speech recognition. In this paper, we focus on the
question of robust and fair evaluation metrics. To that end, we develop a
reference benchmark data set of code-switching speech recognition hypotheses
with human judgments. We define clear guidelines for minimal editing of
automatic hypotheses. We validate the guidelines using 4-way inter-annotator
agreement. We evaluate a large number of metrics in terms of correlation with
human judgments. The metrics we consider vary in terms of representation
(orthographic, phonological, semantic), directness (intrinsic vs extrinsic),
granularity (e.g. word, character), and similarity computation method. The
highest correlation to human judgment is achieved using transliteration
followed by text normalization. We release the first corpus for human
acceptance of code-switching speech recognition results in dialectal
Arabic/English conversation speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chaining Simultaneous Thoughts for Numerical Reasoning. (arXiv:2211.16482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16482">
<div class="article-summary-box-inner">
<span><p>Given that rich information is hidden behind ubiquitous numbers in text,
numerical reasoning over text should be an essential skill of AI systems. To
derive precise equations to solve numerical reasoning problems, previous work
focused on modeling the structures of equations, and has proposed various
structured decoders. Though structure modeling proves to be effective, these
structured decoders construct a single equation in a pre-defined autoregressive
order, potentially placing an unnecessary restriction on how a model should
grasp the reasoning process. Intuitively, humans may have numerous pieces of
thoughts popping up in no pre-defined order; thoughts are not limited to the
problem at hand, and can even be concerned with other related problems. By
comparing diverse thoughts and chaining relevant pieces, humans are less prone
to errors. In this paper, we take this inspiration and propose CANTOR, a
numerical reasoner that models reasoning steps using a directed acyclic graph
where we produce diverse reasoning steps simultaneously without pre-defined
decoding dependencies, and compare and chain relevant ones to reach a solution.
Extensive experiments demonstrated the effectiveness of CANTOR under both
fully-supervised and weakly-supervised settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coder Reviewer Reranking for Code Generation. (arXiv:2211.16490v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16490">
<div class="article-summary-box-inner">
<span><p>Sampling diverse programs from a code language model and reranking with model
likelihood is a popular method for code generation but it is prone to
preferring degenerate solutions. Inspired by collaborative programming, we
propose Coder-Reviewer reranking. We augment Coder language models from past
work, which generate programs given language instructions, with Reviewer
models, which evaluate the likelihood of the instruction given the generated
programs. We perform an extensive study across six datasets with eight models
from three model families. Experimental results show that Coder-Reviewer
reranking leads to consistent and significant improvement (up to 17% absolute
accuracy gain) over reranking with the Coder model only. When combined with
executability filtering, Coder-Reviewer reranking can often outperform the
minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by
prompting, can generalize to different programming languages, and works well
with off-the-shelf hyperparameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstract Visual Reasoning with Tangram Shapes. (arXiv:2211.16492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16492">
<div class="article-summary-box-inner">
<span><p>We introduce KiloGram, a resource for studying abstract visual reasoning in
humans and machines. Drawing on the history of tangram puzzles as stimuli in
cognitive science, we build a richly annotated dataset that, with &gt;1k distinct
stimuli, is orders of magnitude larger and more diverse than prior resources.
It is both visually and linguistically richer, moving beyond whole shape
descriptions to include segmentation maps and part labels. We use this resource
to evaluate the abstract visual reasoning capacities of recent multi-modal
models. We observe that pre-trained weights demonstrate limited abstract
reasoning, which dramatically improves with fine-tuning. We also observe that
explicitly describing parts aids abstract reasoning for both humans and models,
especially when jointly encoding the linguistic and visual inputs. KiloGram is
available at https://lil.nlp.cornell.edu/kilogram .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages. (arXiv:2211.16496v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16496">
<div class="article-summary-box-inner">
<span><p>We study politeness phenomena in nine typologically diverse languages.
Politeness is an important facet of communication and is sometimes argued to be
cultural-specific, yet existing computational linguistic study is limited to
English. We create TyDiP, a dataset containing three-way politeness annotations
for 500 examples in each language, totaling 4.5K examples. We evaluate how well
multilingual models can identify politeness levels -- they show a fairly robust
zero-shot transfer ability, yet fall short of estimated human accuracy
significantly. We further study mapping the English politeness strategy lexicon
into nine languages via automatic translation and lexicon induction, analyzing
whether each strategy's impact stays consistent across languages. Lastly, we
empirically study the complicated relationship between formality and politeness
through transfer experiments. We hope our dataset will support various research
questions and applications, from evaluating multilingual models to constructing
polite multilingual agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03842">
<div class="article-summary-box-inner">
<span><p>Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the popular NAR models adopted in neural machine translation and
text edition by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00084">
<div class="article-summary-box-inner">
<span><p>Collaborative software development is an integral part of the modern software
development life cycle, essential to the success of large-scale software
projects. When multiple developers make concurrent changes around the same
lines of code, a merge conflict may occur. Such conflicts stall pull requests
and continuous integration pipelines for hours to several days, seriously
hurting developer productivity. To address this problem, we introduce
MergeBERT, a novel neural program merge framework based on token-level
three-way differencing and a transformer encoder model. By exploiting the
restricted nature of merge conflict resolutions, we reformulate the task of
generating the resolution sequence as a classification task over a set of
primitive merge patterns extracted from real-world merge commit data. Our model
achieves 63-68% accuracy for merge resolution synthesis, yielding nearly a 3x
performance improvement over existing semi-structured, and 2x improvement over
neural program merge tools. Finally, we demonstrate that MergeBERT is
sufficiently flexible to work with source code files in Java, JavaScript,
TypeScript, and C# programming languages. To measure the practical use of
MergeBERT, we conduct a user study to evaluate MergeBERT suggestions with 25
developers from large OSS projects on 122 real-world conflicts they
encountered. Results suggest that in practice, MergeBERT resolutions would be
accepted at a higher rate than estimated by automatic metrics for precision and
accuracy. Additionally, we use participant feedback to identify future avenues
for improvement of MergeBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14420">
<div class="article-summary-box-inner">
<span><p>Error correction is widely used in automatic speech recognition (ASR) to
post-process the generated sentence, and can further reduce the word error rate
(WER). Although multiple candidates are generated by an ASR system through beam
search, current error correction approaches can only correct one sentence at a
time, failing to leverage the voting effect from multiple candidates to better
detect and correct error tokens. In this work, we propose FastCorrect 2, an
error correction model that takes multiple ASR candidates as input for better
correction accuracy. FastCorrect 2 adopts non-autoregressive generation for
fast inference, which consists of an encoder that processes multiple source
sentences and a decoder that generates the target sentence in parallel from the
adjusted source sentence, where the adjustment is based on the predicted
duration of each source token. However, there are some issues when handling
multiple source sentences. First, it is non-trivial to leverage the voting
effect from multiple source sentences since they usually vary in length. Thus,
we propose a novel alignment algorithm to maximize the degree of token
alignment among multiple sentences in terms of token and pronunciation
similarity. Second, the decoder can only take one adjusted source sentence as
input, while there are multiple source sentences. Thus, we develop a candidate
predictor to detect the most suitable candidate for the decoder. Experiments on
our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce
the WER over the previous correction model with single candidate by 3.2% and
2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR
error correction. FastCorrect 2 achieves better performance than the cascaded
re-scoring and correction pipeline and can serve as a unified post-processing
module for ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. (arXiv:2110.07178v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07178">
<div class="article-summary-box-inner">
<span><p>The common practice for training commonsense models has gone
from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in
order to train commonsense models. In this work, we investigate an alternative,
from-machine-to-corpus-to-machine: general language models author these
commonsense knowledge graphs to train commonsense models. Our study leads to a
new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge
Distillation (Hinton et al., 2015), our approach uses larger models to teach
smaller models. A key difference is that we distill knowledge symbolically-as
text-in addition to the neural model. We also distill only one aspect-the
commonsense of a general language model teacher, allowing the student to be a
different type, a commonsense model. Altogether, we show that careful prompt
engineering and a separately trained critic model allow us to selectively
distill high-quality causal commonsense from GPT-3, a general language model.
Empirical results demonstrate that, for the first time, a human-authored
commonsense knowledge graph is surpassed by our automatically distilled variant
in all three criteria: quantity, quality, and diversity. In addition, it
results in a neural commonsense model that surpasses the teacher model's
commonsense capabilities despite its 100x smaller size. We apply this to the
ATOMIC resource, and share our new symbolic knowledge graph and commonsense
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Model Compression and Acceleration for Pretrained Language Models. (arXiv:2202.07105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07105">
<div class="article-summary-box-inner">
<span><p>Despite achieving state-of-the-art performance on many NLP tasks, the high
energy cost and long inference delay prevent Transformer-based pretrained
language models (PLMs) from seeing broader adoption including for edge and
mobile computing. Efficient NLP research aims to comprehensively consider
computation, time and carbon emission for the entire life-cycle of NLP,
including data preparation, model training and inference. In this survey, we
focus on the inference stage and review the current state of model compression
and acceleration for pretrained language models, including benchmarks, metrics
and methodology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08712">
<div class="article-summary-box-inner">
<span><p>To date, there are no effective treatments for most neurodegenerative
diseases. Knowledge graphs can provide comprehensive and semantic
representation for heterogeneous data, and have been successfully leveraged in
many biomedical applications including drug repurposing. Our objective is to
construct a knowledge graph from literature to study relations between
Alzheimer's disease (AD) and chemicals, drugs and dietary supplements in order
to identify opportunities to prevent or delay neurodegenerative progression. We
collected biomedical annotations and extracted their relations using SemRep via
SemMedDB. We used both a BERT-based classifier and rule-based methods during
data preprocessing to exclude noise while preserving most AD-related semantic
triples. The 1,672,110 filtered triples were used to train with knowledge graph
completion algorithms (i.e., TransE, DistMult, and ComplEx) to predict
candidates that might be helpful for AD treatment or prevention. Among three
knowledge graph completion models, TransE outperformed the other two (MR =
13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further
evaluate the prediction results. We found supporting evidence for most highly
ranked candidates predicted by our model which indicates that our approach can
inform reliable new knowledge. This paper shows that our graph mining model can
predict reliable new relationships between AD and other entities (i.e., dietary
supplements, chemicals, and drugs). The knowledge graph constructed can
facilitate data-driven knowledge discoveries and the generation of novel
hypotheses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models. (arXiv:2205.12247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12247">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that Pre-trained Language Models (PLMs) store the
relational knowledge learned from data and utilize it for performing downstream
tasks. However, commonsense knowledge across different regions may vary. For
instance, the color of bridal dress is white in American weddings whereas it is
red in Chinese weddings. In this paper, we introduce a benchmark dataset,
Geo-Diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for
probing the diversity of the relational knowledge in multilingual PLMs.
GeoMLAMA contains 3,125 prompts in English, Chinese, Hindi, Persian, and
Swahili, with a wide coverage of concepts shared by people from American,
Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard
multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger
multilingual PLMs variants do not necessarily store geo-diverse concepts better
than its smaller variant; 2) multilingual PLMs are not intrinsically biased
towards knowledge from the Western countries (the United States); 3) the native
language of a country may not be the best language to probe its knowledge and
4) a language may better probe knowledge about a non-native country than its
native country. Code and data are released at
https://github.com/WadeYin9712/GeoMLAMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Language Models with Memory Augmentation. (arXiv:2205.12674v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12674">
<div class="article-summary-box-inner">
<span><p>Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. (arXiv:2207.00220v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00220">
<div class="article-summary-box-inner">
<span><p>One concern with the rise of large language models lies with their potential
for significant harm, particularly from pretraining on biased, obscene,
copyrighted, and private information. Emerging ethical approaches have
attempted to filter pretraining material, but such approaches have been ad hoc
and failed to take context into account. We offer an approach to filtering
grounded in law, which has directly addressed the tradeoffs in filtering
material. First, we gather and make available the Pile of Law, a 256GB (and
growing) dataset of open-source English-language legal and administrative data,
covering court opinions, contracts, administrative rules, and legislative
records. Pretraining on the Pile of Law may help with legal tasks that have the
promise to improve access to justice. Second, we distill the legal norms that
governments have developed to constrain the inclusion of toxic or private
content into actionable lessons for researchers and discuss how our dataset
reflects these norms. Third, we show how the Pile of Law offers researchers the
opportunity to learn such filtering rules directly from the data, providing an
exciting new research direction in model-based processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural Machine Translation. (arXiv:2209.15285v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15285">
<div class="article-summary-box-inner">
<span><p>With the recent advance in neural machine translation demonstrating its
importance, research on quality estimation (QE) has been steadily progressing.
QE aims to automatically predict the quality of machine translation (MT) output
without reference sentences. Despite its high utility in the real world, there
remain several limitations concerning manual QE data creation: inevitably
incurred non-trivial costs due to the need for translation experts, and issues
with data scaling and language expansion. To tackle these limitations, we
present QUAK, a Korean-English synthetic QE dataset generated in a fully
automatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and
QUAK-H, produced through three strategies that are relatively free from
language constraints. Since each strategy requires no human effort, which
facilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M
for QUAK-M. As an experiment, we quantitatively analyze word-level QE results
in various ways while performing statistical analysis. Moreover, we show that
datasets scaled in an efficient way also contribute to performance improvements
by observing meaningful performance gains in QUAK-M, P when adding data up to
1.58M.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Analogical Reasoning over Knowledge Graphs. (arXiv:2210.00312v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00312">
<div class="article-summary-box-inner">
<span><p>Analogical reasoning is fundamental to human cognition and holds an important
place in various fields. However, previous studies mainly focus on single-modal
analogical reasoning and ignore taking advantage of structure knowledge.
Notably, the research in cognitive psychology has demonstrated that information
from multimodal sources always brings more powerful cognitive transfer than
single modality sources. To this end, we introduce the new task of multimodal
analogical reasoning over knowledge graphs, which requires multimodal reasoning
ability with the help of background knowledge. Specifically, we construct a
Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph
MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained
Transformer baselines, illustrating the potential challenges of the proposed
task. We further propose a novel model-agnostic Multimodal analogical reasoning
framework with Transformer (MarT) motivated by the structure mapping theory,
which can obtain better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Voice Detection and Audio Splicing Detection using SE-Res2Net-Conformer Architecture. (arXiv:2210.03581v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03581">
<div class="article-summary-box-inner">
<span><p>Synthetic voice and splicing audio clips have been generated to spoof
Internet users and artificial intelligence (AI) technologies such as voice
authentication. Existing research work treats spoofing countermeasures as a
binary classification problem: bonafide vs. spoof. This paper extends the
existing Res2Net by involving the recent Conformer block to further exploit the
local patterns on acoustic features. Experimental results on ASVspoof 2019
database show that the proposed SE-Res2Net-Conformer architecture is able to
improve the spoofing countermeasures performance for the logical access
scenario.
</p>
<p>In addition, this paper also proposes to re-formulate the existing audio
splicing detection problem. Instead of identifying the complete splicing
segments, it is more useful to detect the boundaries of the spliced segments.
Moreover, a deep learning approach can be used to solve the problem, which is
different from the previous signal processing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Close Look into the Calibration of Pre-trained Language Models. (arXiv:2211.00151v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00151">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) achieve remarkable performance on many
downstream tasks, but may fail in giving reliable estimates of their predictive
uncertainty. Given the lack of a comprehensive understanding of PLMs
calibration, we take a close look into this new research problem, aiming to
answer two questions: (1) Do PLMs learn to become calibrated in the training
process? (2) How effective are existing calibration methods? For the first
question, we conduct fine-grained control experiments to study the dynamic
change in PLMs' calibration performance in training. We consider six factors as
control variables, including dataset difficulty, available training samples,
training steps, the number of tunable parameters, model scale, and pretraining.
In experiments, we observe a consistent change in calibration performance
across six factors. We find that PLMs don't learn to become calibrated in
training, evidenced by the continual increase in confidence, no matter the
predictions are correct or not. We highlight that our finding presents some
contradiction with two established conclusions: (a) Larger PLMs are more
calibrated; (b) Pretraining improves model calibration. Next, we study the
effectiveness of existing calibration methods in mitigating the overconfidence
issue, in both in-distribution and various out-of-distribution settings.
Besides unlearnable calibration methods, we adapt two recently proposed
learnable methods that directly collect data to train models to have reasonable
confidence estimations. Also, we propose extended learnable methods based on
existing ones to further improve or maintain PLMs calibration without
sacrificing the original task performance. Experimental results show that
learnable methods significantly reduce PLMs' confidence in wrong predictions,
and our methods exhibit superior performance compared with previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01562">
<div class="article-summary-box-inner">
<span><p>Neural language models (LMs) have achieved impressive results on various
language-based reasoning tasks by utilizing latent knowledge encoded in their
own pretrained parameters. To make this reasoning process more explicit, recent
works retrieve a rationalizing LM's internal knowledge by training or prompting
it to generate free-text rationales, which can be used to guide task
predictions made by either the same LM or a separate reasoning LM. However,
rationalizing LMs require expensive rationale annotation and/or computation,
without any assurance that their generated rationales improve LM task
performance or faithfully reflect LM decision-making. In this paper, we propose
PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns
to faithfully reason over rationales via counterfactual regularization. First,
PINTO maps out a suitable reasoning process for the task input by prompting a
frozen rationalizing LM to generate a free-text rationale. Second, PINTO's
reasoning LM is fine-tuned to solve the task using the generated rationale as
context, while regularized to output less confident predictions when the
rationale is perturbed. Across four datasets, we show that PINTO significantly
improves the generalization ability of the reasoning LM, yielding higher
performance on both in-distribution and out-of-distribution test sets. Also, we
find that PINTO's rationales are more faithful to its task predictions than
those generated by competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12588">
<div class="article-summary-box-inner">
<span><p>Recently, there has been significant progress in teaching language models to
perform step-by-step reasoning to solve complex numerical reasoning tasks.
Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these
tasks. CoT uses language models to perform both reasoning and computation in
the multi-step `thought' process. To disentangle computation from reasoning, we
propose `Program of Thoughts' (PoT), which uses language models (mainly Codex)
to express the reasoning process as a program. The computation is relegated to
an external computer, which executes the generated programs to derive the
answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,
TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)
for both few-shot and zero-shot setups. Under both few-shot and zero-shot
settings, PoT can show an average performance gain over CoT by around 12\%
across all the evaluated datasets. By combining PoT with self-consistency
decoding, we can achieve SoTA performance on all math problem datasets and
near-SoTA performance on financial datasets. All of our data and code are
released in
Github\footnote{\url{https://github.com/wenhuchen/Program-of-Thoughts}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label Few-shot ICD Coding as Autoregressive Generation with Prompt. (arXiv:2211.13813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13813">
<div class="article-summary-box-inner">
<span><p>Automatic International Classification of Diseases (ICD) coding aims to
assign multiple ICD codes to a medical note with an average of 3,000+ tokens.
This task is challenging due to the high-dimensional space of multi-label
assignment (155,000+ ICD code candidates) and the long-tail challenge - Many
ICD codes are infrequently assigned yet infrequent ICD codes are important
clinically. This study addresses the long-tail challenge by transforming this
multi-label classification task into an autoregressive generation task.
Specifically, we first introduce a novel pretraining objective to generate free
text diagnoses and procedure using the SOAP structure, the medical logic
physicians use for note documentation. Second, instead of directly predicting
the high dimensional space of ICD codes, our model generates the lower
dimension of text descriptions, which then infer ICD codes. Third, we designed
a novel prompt template for multi-label classification. We evaluate our
Generation with Prompt model with the benchmark of all code assignment
(MIMIC-III-full) and few shot ICD code assignment evaluation benchmark
(MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with
a marco F1 30.2, which substantially outperforms the previous MIMIC-III-full
SOTA model (marco F1 4.3) and the model specifically designed for few/zero shot
setting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross
attention reranker with prompts, to integrate previous SOTA and our best
few-shot coding predictions. Experiments on MIMIC-III-full show that our
ensemble learner substantially improves both macro and micro F1, from 10.4 to
14.6 and from 58.2 to 59.1, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14228">
<div class="article-summary-box-inner">
<span><p>Students' ability to ask curious questions is a crucial skill that improves
their learning processes. To train this skill, previous research has used a
conversational agent that propose specific cues to prompt children's curiosity
during learning. Despite showing pedagogical efficiency, this method is still
limited since it relies on generating the said prompts by hand for each
educational resource, which can be a very long and costly process. In this
context, we leverage the advances in the natural language processing field and
explore using a large language model (GPT-3) to automate the generation of this
agent's curiosity-prompting cues to help children ask more and deeper
questions. We then used this study to investigate a different
curiosity-prompting behavior for the agent. The study was conducted with 75
students aged between 9 and 10. They either interacted with a hand-crafted
conversational agent that proposes "closed" manually-extracted cues leading to
predefined questions, a GPT-3-driven one that proposes the same type of cues,
or a GPT-3-driven one that proposes "open" cues that can lead to several
possible questions. Results showed a similar question-asking performance
between children who had the two "closed" agents, but a significantly better
one for participants with the "open" agent. Our first results suggest the
validity of using GPT-3 to facilitate the implementation of
curiosity-stimulating learning technologies. In a second step, we also show
that GPT-3 can be efficient in proposing the relevant open cues that leave
children with more autonomy to express their curiosity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SKDBERT: Compressing BERT via Stochastic Knowledge Distillation. (arXiv:2211.14466v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14466">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain
compact BERT-style language model dubbed SKDBERT. In each iteration, SKD
samples a teacher model from a pre-defined teacher ensemble, which consists of
multiple teacher models with multi-level capacities, to transfer knowledge into
student model in an one-to-one manner. Sampling distribution plays an important
role in SKD. We heuristically present three types of sampling distributions to
assign appropriate probabilities for multi-level teacher models. SKD has two
advantages: 1) it can preserve the diversities of multi-level teacher models
via stochastically sampling single teacher model in each iteration, and 2) it
can also improve the efficacy of knowledge distillation via multi-level teacher
models when large capacity gap exists between the teacher model and the student
model. Experimental results on GLUE benchmark show that SKDBERT reduces the
size of a BERT$_{\rm BASE}$ model by 40% while retaining 99.5% performances of
language understanding and being 100% faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AWTE-BERT:Attending to Wordpiece Tokenization Explicitly on BERT for Joint Intent Classification and SlotFilling. (arXiv:2211.14829v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14829">
<div class="article-summary-box-inner">
<span><p>Intent classification and slot filling are two core tasks in natural language
understanding (NLU). The interaction nature of the two tasks makes the joint
models often outperform the single designs. One of the promising solutions,
called BERT (Bidirectional Encoder Representations from Transformers), achieves
the joint optimization of the two tasks. BERT adopts the wordpiece to tokenize
each input token into multiple sub-tokens, which causes a mismatch between the
tokens and the labels lengths. Previous methods utilize the hidden states
corresponding to the first sub-token as input to the classifier, which limits
performance improvement since some hidden semantic informations is discarded in
the fine-tune process. To address this issue, we propose a novel joint model
based on BERT, which explicitly models the multiple sub-tokens features after
wordpiece tokenization, thereby generating the context features that contribute
to slot filling. Specifically, we encode the hidden states corresponding to
multiple sub-tokens into a context vector via the attention mechanism. Then, we
feed each context vector into the slot filling encoder, which preserves the
integrity of the sentence. Experimental results demonstrate that our proposed
model achieves significant improvement on intent classification accuracy, slot
filling F1, and sentence-level semantic frame accuracy on two public benchmark
datasets. The F1 score of the slot filling in particular has been improved from
96.1 to 98.2 (2.1% absolute) on the ATIS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction. (arXiv:2211.15003v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15003">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in
sentiment analysis research, aiming to extract triplets of the aspect term, its
corresponding opinion term, and its associated sentiment polarity from a given
sentence. Recently, many neural networks based models with different tagging
schemes have been proposed, but almost all of them have their limitations:
heavily relying on 1) prior assumption that each word is only associated with a
single role (e.g., aspect term, or opinion term, etc. ) and 2) word-level
interactions and treating each opinion/aspect as a set of independent words.
Hence, they perform poorly on the complex ASTE task, such as a word associated
with multiple roles or an aspect/opinion term with multiple words. Hence, we
propose a novel approach, Span TAgging and Greedy infErence (STAGE), to extract
sentiment triplets in span-level, where each span may consist of multiple words
and play different roles simultaneously. To this end, this paper formulates the
ASTE task as a multi-class span classification problem. Specifically, STAGE
generates more accurate aspect sentiment triplet extractions via exploring
span-level information and constraints, which consists of two components,
namely, span tagging scheme and greedy inference strategy. The former tag all
possible candidate spans based on a newly-defined tagging set. The latter
retrieves the aspect/opinion term with the maximum length from the candidate
sentiment snippet to output sentiment triplets. Furthermore, we propose a
simple but effective model based on the STAGE, which outperforms the
state-of-the-arts by a large margin on four widely-used datasets. Moreover, our
STAGE can be easily generalized to other pair/triplet extraction tasks, which
also demonstrates the superiority of the proposed scheme STAGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15661">
<div class="article-summary-box-inner">
<span><p>Neural sequence models, especially transformers, exhibit a remarkable
capacity for in-context learning. They can construct new predictors from
sequences of labeled examples $(x, f(x))$ presented in the input without
further parameter updates. We investigate the hypothesis that transformer-based
in-context learners implement standard learning algorithms implicitly, by
encoding smaller models in their activations, and updating these implicit
models as new examples appear in the context. Using linear regression as a
prototypical problem, we offer three sources of evidence for this hypothesis.
First, we prove by construction that transformers can implement learning
algorithms for linear models based on gradient descent and closed-form ridge
regression. Second, we show that trained in-context learners closely match the
predictors computed by gradient descent, ridge regression, and exact
least-squares regression, transitioning between different predictors as
transformer depth and dataset noise vary, and converging to Bayesian estimators
for large widths and depths. Third, we present preliminary evidence that
in-context learners share algorithmic features with these predictors: learners'
late layers non-linearly encode weight vectors and moment matrices. These
results suggest that in-context learning is understandable in algorithmic
terms, and that (at least in the linear case) learners may rediscover standard
estimation algorithms. Code and reference implementations are released at
https://github.com/ekinakyurek/google-research/blob/master/incontext.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-30 23:14:03.041929328 UTC">2022-11-30 23:14:03 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>