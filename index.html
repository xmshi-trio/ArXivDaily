<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-25T01:30:00Z">05-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14380">
<div class="article-summary-box-inner">
<span><p>Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g.,
redundancy and over-parameterization. Specifically, the heads of MHA were
originally designed to attend to information from different representation
subspaces, whereas prior studies found that some attention heads likely learn
similar features and can be pruned without harming performance. Inspired by the
minimum-redundancy feature selection, we assume that focusing on the most
representative and distinctive features with minimum resources can mitigate the
above issues and lead to more effective and efficient MHAs. In particular, we
propose Grouped Head Attention, trained with a self-supervised group constraint
that group attention heads, where each group focuses on an essential but
distinctive feature subset. We additionally propose a Voting-to-Stay procedure
to remove redundant heads, thus achieving a transformer with lighter weights.
Moreover, our method achieves significant performance gains on three
well-established tasks while considerably compressing parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. (arXiv:2305.14386v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14386">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel approach for distilling math word problem
solving capabilities from large language models (LLMs) into smaller, more
efficient student models. Our approach is designed to consider the student
model's weaknesses and foster a tailored learning experience by generating
targeted exercises aligned with educational science principles, such as
knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math
tutor and run two steps iteratively: 1) assessing the student model's current
learning status on a GPT-generated exercise book, and 2) improving the student
model by training it with tailored exercise samples generated by GPT-3.
Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and
PaLM) in accuracy across three distinct benchmarks while employing
significantly fewer parameters. Furthermore, we provide a comprehensive
analysis of the various components within our methodology to substantiate their
efficacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14387">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their ability to follow user instructions well. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following process
faces three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 45x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach. (arXiv:2305.14410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14410">
<div class="article-summary-box-inner">
<span><p>We are interested in image manipulation via natural language text -- a task
that is useful for multiple AI applications but requires complex reasoning over
multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning
(NSCL), which has been quite effective for the task of Visual Question
Answering (VQA), for the task of image manipulation. Our system referred to as
NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and
only requires weak supervision in the form of annotated data for VQA. NeuroSIM
parses an instruction into a symbolic program, based on a Domain Specific
Language (DSL) comprising of object attributes and manipulation operations,
that guides its execution. We create a new dataset for the task, and extensive
experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA
baselines that make use of supervised data for manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction. (arXiv:2305.14434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14434">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-Based
Sentiment Analysis (ABSA) that considers each opinion term, their expressed
sentiment, and the corresponding aspect targets. However, existing methods are
limited to the in-domain setting with two domains. Hence, we propose a
domain-expanded benchmark to address the in-domain, out-of-domain and
cross-domain settings. We support the new benchmark by annotating more than
4000 data samples for two new domains based on hotel and cosmetics reviews. Our
analysis of five existing methods shows that while there is a significant gap
between in-domain and out-of-domain performance, generative methods have a
strong potential for domain generalization. Our datasets, code implementation
and models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions. (arXiv:2305.14441v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14441">
<div class="article-summary-box-inner">
<span><p>Contrast consistency, the ability of a model to make consistently correct
predictions in the presence of perturbations, is an essential aspect in NLP.
While studied in tasks such as sentiment analysis and reading comprehension, it
remains unexplored in open-domain question answering (OpenQA) due to the
difficulty of collecting perturbed questions that satisfy factuality
requirements. In this work, we collect minimally edited questions as
challenging contrast sets to evaluate OpenQA models. Our collection approach
combines both human annotation and large language model generation. We find
that the widely used dense passage retriever (DPR) performs poorly on our
contrast sets, despite fitting the training set well and performing
competitively on standard test sets. To address this issue, we introduce a
simple and effective query-side contrastive loss with the aid of data
augmentation to improve DPR training. Our experiments on the contrast sets
demonstrate that DPR's contrast consistency is improved without sacrificing its
accuracy on the standard test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. (arXiv:2305.14450v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14450">
<div class="article-summary-box-inner">
<span><p>ChatGPT has stimulated the research boom in the field of large language
models. In this paper, we assess the capabilities of ChatGPT from four
perspectives including Performance, Evaluation Criteria, Robustness and Error
Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets
with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought
scenarios, and find a huge performance gap between ChatGPT and SOTA results.
Next, we rethink this gap and propose a soft-matching strategy for evaluation
to more accurately reflect ChatGPT's performance. Then, we analyze the
robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely
outputs invalid responses; 2) Irrelevant context and long-tail target types
greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the
subject-object relationships in RE task. Finally, we analyze the errors of
ChatGPT, and find that "unannotated spans" is the most dominant error type.
This raises concerns about the quality of annotated data, and indicates the
possibility of annotating data with ChatGPT. The data and code are released at
Github site.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Robustness of Finetuned Transformer-based NLP Models. (arXiv:2305.14453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14453">
<div class="article-summary-box-inner">
<span><p>Transformer-based pretrained models like BERT, GPT-2 and T5 have been
finetuned for a large number of natural language processing (NLP) tasks, and
have been shown to be very effective. However, while finetuning, what changes
across layers in these models with respect to pretrained checkpoints is
under-studied. Further, how robust are these models to perturbations in input
text? Does the robustness vary depending on the NLP task for which the models
have been finetuned? While there exists some work on studying robustness of
BERT finetuned for a few NLP tasks, there is no rigorous study which compares
this robustness across encoder only, decoder only and encoder-decoder models.
</p>
<p>In this paper, we study the robustness of three language models (BERT, GPT-2
and T5) with eight different text perturbations on the General Language
Understanding Evaluation (GLUE) benchmark. Also, we use two metrics (CKA and
STIR) to quantify changes between pretrained and finetuned language model
representations across layers. GPT-2 representations are more robust than BERT
and T5 across multiple types of input perturbation. Although models exhibit
good robustness broadly, dropping nouns, verbs or changing characters are the
most impactful. Overall, this study provides valuable insights into
perturbation-specific weaknesses of popular Transformer-based models which
should be kept in mind when passing inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14456">
<div class="article-summary-box-inner">
<span><p>Are language models culturally biased? It is important that language models
conform to the cultural aspects of the communities they serve. However, we show
in this paper that language models suffer from a significant bias towards
Western culture when handling and generating text in Arabic, often preferring,
and producing Western-fitting content as opposed to the relevant Arab content.
We quantify this bias through a likelihood scoring-based metric using naturally
occurring contexts that we collect from online social media. Our experiments
reveal that both Arabic monolingual and multilingual models exhibit bias
towards Western culture in eight different cultural aspects: person names,
food, clothing, location, literature, beverage, religion, and sports. Models
also tend to exhibit more bias when prompted with Arabic sentences that are
more linguistically aligned with English. These findings raise concerns about
the cultural relevance of current language models. Our analyses show that
providing culture-indicating tokens or culturally-relevant demonstrations to
the model can help in debiasing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14457">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel framework to pre-train language models for
enhancing their abilities of comparative reasoning over texts. While recent
research has developed models for NLP tasks that require comparative reasoning,
they suffer from costly manual data labeling and limited generalizability to
different tasks. Our approach involves a scalable method for collecting data
for text-based entity comparison, which leverages both structured and
unstructured data, and the design of three novel pre-training tasks. Evaluation
on a range of downstream tasks including comparative question answering,
question generation, and summarization shows that our pre-training framework
significantly improves the comparative reasoning abilities of language models,
especially under low-resource conditions. This work also releases the first
integrated benchmark for comparative reasoning over texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14458">
<div class="article-summary-box-inner">
<span><p>Large language models (e.g., GPT-3.5) are uniquely capable of producing
highly rated text simplification, yet current human evaluation methods fail to
provide a clear understanding of systems' specific strengths and weaknesses. To
address this limitation, we introduce SALSA, an edit-based human annotation
framework that enables holistic and fine-grained text simplification
evaluation. We develop twenty one linguistically grounded edit types, covering
the full spectrum of success and failure across dimensions of conceptual,
syntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations
on 700 simplifications, revealing discrepancies in the distribution of
transformation approaches performed by fine-tuned models, few-shot LLMs and
humans, and finding GPT-3.5 performs more quality edits than humans, but still
exhibits frequent errors. Using our fine-grained annotations, we develop
LENS-SALSA, a reference-free automatic simplification metric, trained to
predict sentence- and word-level quality simultaneously. Additionally, we
introduce word-level quality estimation for simplification and report promising
baseline results. Our training material, annotation toolkit, and data are
released at <a href="http://salsa-eval.com.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14459">
<div class="article-summary-box-inner">
<span><p>Automatically open-ended long text generation poses significant challenges
due to semantic incoherence and plot implausibility. Previous works usually
alleviate this problem through outlines in the form of short phrases or
abstractive signals by designing unsupervised tasks, which tend to be unstable
and weakly interpretable.
</p>
<p>Assuming that a summary serves as a mature outline, we introduce a two-stage,
summary-enhanced outline supervised generation framework. This framework
leverages the dual characteristics of the summarization task to improve outline
prediction, resulting in more explicit and plausible outlines. Furthermore, we
identify an underutilization issue in outline-based generation with both
standard pretrained language models (e.g., GPT-2, BART) and large language
models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit
outline control method for more effective utilization of generated outlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Massively Multi-domain Multilingual Readability Assessment. (arXiv:2305.14463v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14463">
<div class="article-summary-box-inner">
<span><p>We present ReadMe++, a massively multi-domain multilingual dataset for
automatic readability assessment. Prior work on readability assessment has been
mostly restricted to the English language and one or two text domains.
Additionally, the readability levels of sentences used in many previous
datasets are assumed on the document-level other than sentence-level, which
raises doubt about the quality of previous evaluations. We address those gaps
in the literature by providing an annotated dataset of 6,330 sentences in
Arabic, English, and Hindi collected from 64 different domains of text. Unlike
previous datasets, ReadMe++ offers more domain and language diversity and is
manually annotated at a sentence level using the Common European Framework of
Reference for Languages (CEFR) and through a Rank-and-Rate annotation framework
that reduces subjectivity in annotation. Our experiments demonstrate that
models fine-tuned using ReadMe++ achieve strong cross-lingual transfer
capabilities and generalization to unseen domains. ReadMe++ will be made
publicly available to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Run Like a Girl! Sports-Related Gender Bias in Language and Vision. (arXiv:2305.14468v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14468">
<div class="article-summary-box-inner">
<span><p>Gender bias in Language and Vision datasets and models has the potential to
perpetuate harmful stereotypes and discrimination. We analyze gender bias in
two Language and Vision datasets. Consistent with prior work, we find that both
datasets underrepresent women, which promotes their invisibilization. Moreover,
we hypothesize and find that a bias affects human naming choices for people
playing sports: speakers produce names indicating the sport (e.g. 'tennis
player' or 'surfer') more often when it is a man or a boy participating in the
sport than when it is a woman or a girl, with an average of 46% vs. 35% of
sports-related names for each gender. A computational model trained on these
naming data reproduces the bias. We argue that both the data and the model
result in representational harm against women.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. (arXiv:2305.14471v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14471">
<div class="article-summary-box-inner">
<span><p>Generative chat models, such as ChatGPT and GPT-4, have revolutionized
natural language generation (NLG) by incorporating instructions and human
feedback to achieve significant performance improvements. However, the lack of
standardized evaluation benchmarks for chat models, particularly for Chinese
and domain-specific models, hinders their assessment and progress. To address
this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark,
focusing on general and financial domains. The CGCE benchmark encompasses
diverse tasks, including 200 questions in the general domain and 150 specific
professional questions in the financial domain. Manual scoring evaluates
factors such as accuracy, coherence, expression clarity, and completeness. The
CGCE benchmark provides researchers with a standardized framework to assess and
compare Chinese generative chat models, fostering advancements in NLG research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14480">
<div class="article-summary-box-inner">
<span><p>Infectious disease outbreaks continue to pose a significant threat to human
health and well-being. To improve disease surveillance and understanding of
disease spread, several surveillance systems have been developed to monitor
daily news alerts and social media. However, existing systems lack thorough
epidemiological analysis in relation to corresponding alerts or news, largely
due to the scarcity of well-annotated reports data. To address this gap, we
introduce the Biomedical Alert News Dataset (BAND), which includes 1,508
samples from existing reported news articles, open emails, and alerts, as well
as 30 epidemiology-related questions. These questions necessitate the model's
expert reasoning abilities, thereby offering valuable insights into the
outbreak of the disease. The BAND dataset brings new challenges to the NLP
world, requiring better disguise capability of the content and the ability to
infer important information. We provide several benchmark tasks, including
Named Entity Recognition (NER), Question Answering (QA), and Event Extraction
(EE), to show how existing models are capable of handling these tasks in the
epidemiology domain. To the best of our knowledge, the BAND corpus is the
largest corpus of well-annotated biomedical outbreak alert news with
elaborately designed questions, making it a valuable resource for
epidemiologists and NLP researchers alike.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language. (arXiv:2305.14481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14481">
<div class="article-summary-box-inner">
<span><p>Using model weights pretrained on a high-resource language as a warm start
can reduce the need for data and compute to obtain high-quality language models
in low-resource languages. To accommodate the new language, the pretrained
vocabulary and embeddings need to be adapted. Previous work on embedding
initialization for such adapted vocabularies has mostly focused on monolingual
source models. In this paper, we investigate the multilingual source model
setting and propose FOCUS - Fast Overlapping Token Combinations Using
Sparsemax, a novel embedding initialization method that outperforms previous
work when adapting XLM-R. FOCUS represents newly added tokens as combinations
of tokens in the overlap of the pretrained and new vocabularies. The
overlapping tokens are selected based on semantic similarity in an auxiliary
token embedding space. Our implementation of FOCUS is publicly available on
GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries. (arXiv:2305.14482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14482">
<div class="article-summary-box-inner">
<span><p>We study how multilingual sentence representations capture European countries
and how this differs across European languages. We prompt the models with
templated sentences that we machine-translate into 12 European languages and
analyze the most prominent dimensions in the embeddings. Our analysis reveals
that the most prominent country feature in the embedding is its economic
strength in terms of GPD. When prompted specifically for job prestige, the
embedding space clearly distinguishes high and low-prestige jobs. The
occupational dimension is uncorrelated with the most dominant country
dimensions for three out of four studied models. One model: Distilled
Multilingual Universal Sentence Encoder, however, exhibited a connection
between occupational prestige and country of origin, which is a potential
source of nationality-based discrimination. Our findings are consistent across
languages and, to some extent, with the exception mentioned above, across
studied representation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14483">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have exhibited remarkable performance across
various natural language processing (NLP) tasks. However, fine-tuning these
models often necessitates substantial supervision, which can be expensive and
time-consuming to obtain. This paper introduces a novel unsupervised method
called LanguageModel Self-Improvement by Reinforcement Learning Contemplation
(SIRLC) that improves LLMs without reliance on external labels. Our approach is
grounded in the observation that it is simpler for language models to assess
text quality than to generate text. Building on this insight, SIRLC assigns
LLMs dual roles as both student and teacher. As a student, the LLM generates
answers to unlabeled questions, while as a teacher, it evaluates the generated
text and assigns scores accordingly. The model parameters are updated using
reinforcement learning to maximize the evaluation score. We demonstrate that
SIRLC can be applied to various NLP tasks, such as reasoning problems, text
generation, and machine translation. Our experiments show that SIRLC
effectively improves LLM performance without external supervision, resulting in
a 5.6% increase in answering accuracy for reasoning tasks and a rise in
BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be
applied to models of different sizes, showcasing its broad applicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14489">
<div class="article-summary-box-inner">
<span><p>Recent progress in domain adaptation for coreference resolution relies on
continued training using annotated data from target domains. At the same time,
pre-trained large language models (LMs) have exhibited strong zero- and
few-shot learning abilities across a wide range of NLP tasks including pronoun
resolution. While this demonstrates evidence of coreference ability, previous
work has mostly studied this ability using simple sentence-level datasets such
as the Winograd Schema Challenge. In this work, we assess the feasibility of
zero-shot learning for coreference resolution by evaluating instruction-tuned
language models on more difficult, linguistically-complex coreference
benchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting
outperforms current unsupervised coreference systems. Further investigations
reveal the robust zero-shot generalization ability of instruction-tuned LMs
across a wide range of domains, languages, and time periods, as well as a
strong reliance on high-quality mention detection systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment. (arXiv:2305.14492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14492">
<div class="article-summary-box-inner">
<span><p>Designing systems that can reason across cultures requires that they are
grounded in the norms of the contexts in which they operate. However, current
research on developing computational models of social norms has primarily
focused on American society. Here, we propose a novel approach to discover and
compare descriptive social norms across Chinese and American cultures. We
demonstrate our approach by leveraging discussions on a Chinese Q&amp;A
platform-Zhihu-and the existing SocialChemistry dataset as proxies for
contrasting cultural axes, align social situations cross-culturally, and
extract social norms from texts using in-context learning. Embedding
Chain-of-Thought prompting in a human-AI collaborative framework, we build a
high-quality dataset of 3,069 social norms aligned with social situations
across Chinese and American cultures alongside corresponding free-text
explanations. To test the ability of models to reason about social norms across
cultures, we introduce the task of explainable social norm entailment, showing
that existing models under 3B parameters have significant room for improvement
in both automatic and human evaluation. Further analysis of cross-cultural norm
differences based on our dataset shows empirical alignment with the social
orientations framework, revealing several situational and descriptive nuances
in norms across these cultures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14493">
<div class="article-summary-box-inner">
<span><p>Prompt-based models have made remarkable advancements in the fields of
zero-shot and few-shot learning, attracting a lot of attention from
researchers. Developing an effective prompt template plays a critical role.
However, prior studies have mainly focused on prompt vocabulary selection or
embedding initialization with the reserved prompt position fixed. In this
empirical study, we conduct the most comprehensive analysis to date of prompt
position option for natural language understanding tasks. Our findings quantify
the substantial impact prompt position has on model performance. We observe
that the prompt position used in prior studies is often sub-optimal for both
zero-shot and few-shot settings. These findings suggest prompt position
optimisation as an interesting research direction alongside the existing focus
on prompt engineering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14497">
<div class="article-summary-box-inner">
<span><p>Prompting methods such as Chain-of-Thought (CoT) have shed new light on
enhancing the reasoning capabilities of large language models, and researchers
have extensively explored the generation process of rationales and answers.
However, they have overlooked the potential challenges posed by the poor
quality of reasoning problems, which may influence the reasoning performance
significantly. In this work, we propose Self-Polish (SP), a novel method that
facilitates the model's problem-solving process by prompting them to
progressively refine the given problems to be more comprehensible and solvable.
Specifically, the method teaches models to eliminate irrelevant information,
rearrange the logic structure and organize local conditions into new ones
parallelly. SP is orthogonal to all other prompting methods, making it
convenient to integrate with state-of-the-art techniques for further
improvement. We conduct thorough experiments on five benchmarks to illustrate
the effectiveness of the proposed method. For example, with Text-davinci-003,
our method boosts the performance of standard few-shot prompting by $8.0\%$ on
GSM8K and $17.8\%$ on MultiArith; it also improves the performance of CoT by
$6.0\%$ on GSM8K and $6.0\%$ on MathQA, respectively. Furthermore, our method
also showcases impressive performance on robustness evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14499">
<div class="article-summary-box-inner">
<span><p>Neural document rerankers are extremely effective in terms of accuracy.
However, the best models require dedicated hardware for serving, which is
costly and often not feasible. To avoid this serving-time requirement, we
present a method of capturing up to 86% of the gains of a Transformer
cross-attention model with a lexicalized scoring function that only requires
10-6% of the Transformer's FLOPs per document and can be served using commodity
CPUs. When combined with a BM25 retriever, this approach matches the quality of
a state-of-the art dual encoder retriever, that still requires an accelerator
for query encoding. We introduce NAIL (Non-Autoregressive Indexing with
Language models) as a model architecture that is compatible with recent
encoder-decoder and decoder-only large language models, such as T5, GPT-3 and
PaLM. This model architecture can leverage existing pre-trained checkpoints and
can be fine-tuned for efficiently constructing document representations that do
not require neural processing of queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14502">
<div class="article-summary-box-inner">
<span><p>Many recent developments in large language models focus on prompting them to
perform specific tasks. One effective prompting method is in-context learning,
where the model performs a (possibly new) generation/prediction task given one
(or more) examples. Past work has shown that the choice of examples can make a
large impact on task performance. However, finding good examples is not
straightforward since the definition of a representative group of examples can
vary greatly depending on the task. While there are many existing methods for
selecting in-context examples, they generally score examples independently,
ignoring the dependency between them and the order in which they are provided
to the large language model. In this work, we propose Retrieval for In-Context
Learning (RetICL), a learnable method for modeling and optimally selecting
examples sequentially for in-context learning. We frame the problem of
sequential example selection as a Markov decision process, design an example
retriever model using an LSTM, and train it using proximal policy optimization
(PPO). We validate RetICL on math problem solving datasets and show that it
outperforms both heuristic and learnable baselines, and achieves
state-of-the-art accuracy on the TabMWP dataset. We also use case studies to
show that RetICL implicitly learns representations of math problem solving
strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models. (arXiv:2305.14507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14507">
<div class="article-summary-box-inner">
<span><p>We explore whether Large Language Models (LLMs) are capable of logical
reasoning with distorted facts, which we call Deduction under Perturbed
Evidence (DUPE). DUPE presents a unique challenge to LLMs since they typically
rely on their parameters, which encode mostly accurate information, to reason
and make inferences. However, in DUPE, LLMs must reason over manipulated or
falsified evidence present in their prompts, which can result in false
conclusions that are valid only under the manipulated evidence. Our goal with
DUPE is to determine whether LLMs can arrive at these false conclusions and
identify whether the dominant factor influencing the deduction process is the
encoded data in the parameters or the manipulated evidence in the prompts. To
evaluate the DUPE capabilities of LLMs, we create a DUPEd version of the
StrategyQA dataset, where facts are manipulated to reverse the answer to the
question. Our findings show that even the most advanced GPT models struggle to
reason on manipulated facts - showcasing poor DUPE skills - with accuracy
dropping by 45% compared to the original dataset. We also investigate prompt
settings inspired from student simulation models, which mitigate the accuracy
drop to some extent. Our findings have practical implications for understanding
the performance of LLMs in real-world applications such as student simulation
models that involve reasoning over inaccurate information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14521">
<div class="article-summary-box-inner">
<span><p>Machine learning models pre-trained on large datasets have achieved
remarkable convergence and robustness properties. However, these models often
exploit spurious correlations between certain attributes and labels, which are
prevalent in the majority of examples within specific categories but are not
predictive of these categories in general. The learned spurious correlations
may persist even after fine-tuning on new data, which degrades models'
performance on examples that do not exhibit the spurious correlation. In this
work, we propose a simple and highly effective method to eliminate spurious
correlations from pre-trained models. The key idea of our method is to leverage
a small set of examples with spurious attributes, and balance the spurious
attributes across all classes via data mixing. We theoretically confirm the
effectiveness of our method, and empirically demonstrate its state-of-the-art
performance on various vision and NLP tasks, including eliminating spurious
correlations from pre-trained ResNet50 on Waterbirds and CelebA, adversarially
pre-trained ResNet50 on ImageNet, and BERT pre-trained on CivilComments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation. (arXiv:2305.14533v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14533">
<div class="article-summary-box-inner">
<span><p>We release MMSMR, a Massively Multi-System MultiReference dataset to enable
future work on metrics and evaluation for dialog. Automatic metrics for
dialogue evaluation should be robust proxies for human judgments; however, the
verification of robustness is currently far from satisfactory. To quantify the
robustness correlation and understand what is necessary in a test set, we
create and release an 8-reference dialog dataset by extending single-reference
evaluation sets and introduce this new language learning conversation dataset.
We then train 1750 systems and evaluate them on our novel test set and the
DailyDialog dataset. We release the novel test set, and model hyper parameters,
inference outputs, and metric scores for each system on a variety of datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14534">
<div class="article-summary-box-inner">
<span><p>Propaganda is a form of communication intended to influence the opinions and
the mindset of the public to promote a particular agenda. With the rise of
social media, propaganda has spread rapidly, leading to the need for automatic
propaganda detection systems. Most work on propaganda detection has focused on
high-resource languages, such as English, and little effort has been made to
detect propaganda for low-resource languages. Yet, it is common to find a mix
of multiple languages in social media communication, a phenomenon known as
code-switching. Code-switching combines different languages within the same
text, which poses a challenge for automatic systems. With this in mind, here we
propose the novel task of detecting propaganda techniques in code-switched
text. To support this task, we create a corpus of 1,030 texts code-switching
between English and Roman Urdu, annotated with 20 propaganda techniques, which
we make publicly available. We perform a number of experiments contrasting
different experimental setups, and we find that it is important to model the
multilinguality directly (rather than using translation) as well as to use the
right fine-tuning strategy. The code and the dataset are publicly available at
https://github.com/mbzuai-nlp/propaganda-codeswitched-text
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14536">
<div class="article-summary-box-inner">
<span><p>Although automatic dialogue tutors hold great potential in making education
personalized and more accessible, research on such systems has been hampered by
a lack of sufficiently large and high-quality datasets. However, collecting
such datasets remains challenging, as recording tutoring sessions raises
privacy concerns and crowdsourcing leads to insufficient data quality. To
address this problem, we propose a framework to semi-synthetically generate
such dialogues by pairing real teachers with a large language model (LLM)
scaffolded to represent common student errors. In this paper, we describe our
ongoing efforts to use this framework to collect MathDial, a dataset of
currently ca. 1.5k tutoring dialogues grounded in multi-step math word
problems. We show that our dataset exhibits rich pedagogical properties,
focusing on guiding students using sense-making questions to let them explore
problems. Moreover, we outline that MathDial and its grounding annotations can
be used to finetune language models to be more effective tutors (and not just
solvers) and highlight remaining challenges that need to be addressed by the
research community. We will release our dataset publicly to foster research in
this socially important area of NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation. (arXiv:2305.14538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14538">
<div class="article-summary-box-inner">
<span><p>This paper presents a plug-and-play approach for translation with terminology
constraints. Terminology constraints are an important aspect of many modern
translation pipelines. In both specialized domains and newly emerging domains
(such as the COVID-19 pandemic), accurate translation of technical terms is
crucial. Recent approaches often train models to copy terminologies from the
input into the output sentence by feeding the target terminology along with the
input. But this requires expensive training whenever the underlying language
model is changed or the system should specialize to a new domain. We propose
Cascade Beam Search, a plug-and-play terminology-forcing approach that requires
no training. Cascade Beam Search has two parts: 1) logit manipulation to
increase the probability of target terminologies and 2) a cascading beam setup
based on grid beam search, where beams are grouped by the number of
terminologies they contain. We evaluate the performance of our approach by
competing against the top submissions of the WMT21 terminology translation
task. Our plug-and-play approach performs on par with the winning submissions
without using a domain-specific language model and with no additional training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. (arXiv:2305.14540v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14540">
<div class="article-summary-box-inner">
<span><p>With the recent appearance of LLMs in practical settings, having methods that
can effectively detect factual inconsistencies is crucial to reduce the
propagation of misinformation and improve trust in model outputs. When testing
on existing factual consistency benchmarks, we find that a few large language
models (LLMs) perform competitively on classification benchmarks for factual
inconsistency detection compared to traditional non-LLM methods. However, a
closer analysis reveals that most LLMs fail on more complex formulations of the
task and exposes issues with existing evaluation benchmarks, affecting
evaluation precision. To address this, we propose a new protocol for
inconsistency detection benchmark creation and implement it in a 10-domain
benchmark called SummEdits. This new benchmark is 20 times more cost-effective
per sample than previous benchmarks and highly reproducible, as we estimate
inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with
performance close to random chance. The best-performing model, GPT-4, is still
8\% below estimated human performance, highlighting the gaps in LLMs' ability
to reason about facts and detect inconsistencies when they occur.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Transferability of Whisper-based Representations for "In-the-Wild" Cross-Task Downstream Speech Applications. (arXiv:2305.14546v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14546">
<div class="article-summary-box-inner">
<span><p>Large self-supervised pre-trained speech models have achieved remarkable
success across various speech-processing tasks. The self-supervised training of
these models leads to universal speech representations that can be used for
different downstream tasks, ranging from automatic speech recognition (ASR) to
speaker identification. Recently, Whisper, a transformer-based model was
proposed and trained on large amount of weakly supervised data for ASR; it
outperformed several state-of-the-art self-supervised models. Given the
superiority of Whisper for ASR, in this paper we explore the transferability of
the representation for four other speech tasks in SUPERB benchmark. Moreover,
we explore the robustness of Whisper representation for ``in the wild'' tasks
where speech is corrupted by environment noise and room reverberation.
Experimental results show Whisper achieves promising results across tasks and
environmental conditions, thus showing potential for cross-task real-world
deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization. (arXiv:2305.14548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14548">
<div class="article-summary-box-inner">
<span><p>Existing factual consistency evaluation approaches for text summarization
provide binary predictions and limited insights into the weakness of
summarization systems. Therefore, we propose the task of fine-grained
inconsistency detection, the goal of which is to predict the fine-grained types
of factual errors in a summary. Motivated by how humans inspect factual
inconsistency in summaries, we propose an interpretable fine-grained
inconsistency detection model, FineGrainFact, which explicitly represents the
facts in the documents and summaries with semantic frames extracted by semantic
role labeling, and highlights the related semantic frames to predict
inconsistency. The highlighted semantic frames help verify predicted error
types and correct inconsistent summaries. Experiment results demonstrate that
our model outperforms strong baselines and provides evidence to support or
refute the summary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sources of Hallucination by Large Language Models on Inference Tasks. (arXiv:2305.14552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14552">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are claimed to be capable of Natural Language
Inference (NLI), necessary for applied tasks like question answering and
summarization, yet this capability is under-explored. We present a series of
behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which
probe their behavior using controlled experiments. We establish two factors
which predict much of their performance, and propose that these are major
sources of hallucination in generative LLM. First, the most influential factor
is memorization of the training data. We show that models falsely label NLI
test samples as entailing when the hypothesis is attested in the training text,
regardless of the premise. We further show that named entity IDs are used as
"indices" to access the memorized data. Second, we show that LLMs exploit a
further corpus-based heuristic using the relative frequencies of words. We show
that LLMs score significantly worse on NLI test samples which do not conform to
these factors than those which do; we also discuss a tension between the two
factors, and a performance trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations. (arXiv:2305.14555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14555">
<div class="article-summary-box-inner">
<span><p>Transformer models bring propelling advances in various NLP tasks, thus
inducing lots of interpretability research on the learned representations of
the models. However, we raise a fundamental question regarding the reliability
of the representations. Specifically, we investigate whether transformers learn
essentially isomorphic representation spaces, or those that are sensitive to
the random seeds in their pretraining process. In this work, we formulate the
Bijection Hypothesis, which suggests the use of bijective methods to align
different models' representation spaces. We propose a model based on invertible
neural networks, BERT-INN, to learn the bijection more effectively than other
existing bijective methods such as the canonical correlation analysis (CCA). We
show the advantage of BERT-INN both theoretically and through extensive
experiments, and apply it to align the reproduced BERT embeddings to draw
insights that are meaningful to the interpretability research. Our code is at
https://github.com/twinkle0331/BERT-similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations. (arXiv:2305.14556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14556">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models have exhibited unprecedented capabilities
in producing high-quality text via prompting techniques. This fact introduces
new possibilities for data collection and annotation, particularly in
situations where such data is scarce, complex to gather, expensive, or even
sensitive. In this paper, we explore the potential of these models to generate
and annotate goal-oriented dialogues, and conduct an in-depth analysis to
evaluate their quality. Our experiments employ ChatGPT, and encompass three
categories of goal-oriented dialogues (task-oriented, collaborative, and
explanatory), two generation modes (interactive and one-shot), and two
languages (English and Italian). Based on extensive human-based evaluations, we
demonstrate that the quality of generated dialogues and annotations is on par
with those generated by humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. (arXiv:2305.14564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14564">
<div class="article-summary-box-inner">
<span><p>Strategies such as chain-of-thought prompting improve the performance of
large language models (LLMs) on complex reasoning tasks by decomposing input
examples into intermediate steps. However, it remains unclear how to apply such
methods to reason over long input documents, in which both the decomposition
and the output of each intermediate step are non-trivial to obtain. In this
work, we propose PEARL, a prompting framework to improve reasoning over long
documents, which consists of three stages: action mining, plan formulation, and
plan execution. More specifically, given a question about a long document,
PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,
FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain
the answer. Each stage of PEARL is implemented via zero-shot or few-shot
prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate
PEARL on a challenging subset of the QuALITY dataset, which contains questions
that require complex reasoning over long narrative texts. PEARL outperforms
zero-shot and chain-of-thought prompting on this dataset, and ablation
experiments show that each stage of PEARL is critical to its performance.
Overall, PEARL is a first step towards leveraging LLMs to reason over long
documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Unified Question Answering: Tuning Models or Prompts?. (arXiv:2305.14569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14569">
<div class="article-summary-box-inner">
<span><p>Question-answering (QA) tasks often investigate specific question types,
knowledge domains, or reasoning skills, leading to specialized models catering
to specific categories of QA tasks. While recent research has explored the idea
of unified QA models, such models are usually explored for high-resource
scenarios and require re-training to extend their capabilities. To overcome
these drawbacks, the paper explores the potential of two paradigms of tuning,
model, and prompts, for unified QA under a low-resource setting. The paper
provides an exhaustive analysis of their applicability using 16 QA datasets,
revealing that prompt tuning can perform as well as model tuning in a few-shot
setting with a good initialization. The study also shows that parameter-sharing
results in superior few-shot performance, simple knowledge transfer techniques
for prompt initialization can be effective, and prompt tuning achieves a
significant performance boost from pre-training in a low-resource regime. The
research offers insights into the advantages and limitations of prompt tuning
for unified QA in a few-shot setting, contributing to the development of
effective and efficient systems in low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14571">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art models for natural language understanding require a
preprocessing step to convert raw text into discrete tokens. This process known
as tokenization relies on a pre-built vocabulary of words or sub-word
morphemes. This fixed vocabulary limits the model's robustness to spelling
errors and its capacity to adapt to new domains. In this work, we introduce a
novel open-vocabulary language model that adopts a hierarchical two-level
approach: one at the word level and another at the sequence level. Concretely,
we design an intra-word module that uses a shallow Transformer architecture to
learn word representations from their characters, and a deep inter-word
Transformer module that contextualizes each word representation by attending to
the entire word sequence. Our model thus directly operates on character
sequences with explicit awareness of word boundaries, but without biased
sub-word or word-level vocabulary. Experiments on various downstream tasks show
that our method outperforms strong baselines. We also demonstrate that our
hierarchical model is robust to textual corruption and domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Mitigating Indirect Stereotypes in Word Embeddings. (arXiv:2305.14574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14574">
<div class="article-summary-box-inner">
<span><p>Societal biases in the usage of words, including harmful stereotypes, are
frequently learned by common word embedding methods. These biases manifest not
only between a word and an explicit marker of its stereotype, but also between
words that share related stereotypes. This latter phenomenon, sometimes called
"indirect bias,'' has resisted prior attempts at debiasing. In this paper, we
propose a novel method called Biased Indirect Relationship Modification (BIRM)
to mitigate indirect bias in distributional word embeddings by modifying biased
relationships between words before embeddings are learned. This is done by
considering how the co-occurrence probability of a given pair of words changes
in the presence of words marking an attribute of bias, and using this to
average out the effect of a bias attribute. To evaluate this method, we perform
a series of common tests and demonstrate that measures of bias in the word
embeddings are reduced in exchange for minor reduction in the semantic quality
of the embeddings. In addition, we conduct novel tests for measuring indirect
stereotypes by extending the Word Embedding Association Test (WEAT) with new
test sets for indirect binary gender stereotypes. With these tests, we
demonstrate the presence of more subtle stereotypes not addressed by previous
work. The proposed method is able to reduce the presence of some of these new
stereotypes, serving as a crucial next step towards non-stereotyped word
embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings. (arXiv:2305.14576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14576">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have ignited a surge in demand for
effective fine-tuning techniques, particularly in low-resource domains and
languages. Active learning (AL), a set of algorithms designed to decrease
labeling costs by minimizing label complexity, has shown promise in confronting
the labeling bottleneck. Concurrently, adapter modules, designed for
parameter-efficient fine-tuning (PEFT), have showcased notable potential in
low-resource settings. However, the interplay between AL and adapter-based PEFT
remains unexplored. In our study, we empirically investigate PEFT behavior with
AL in low-resource settings for text classification tasks. Our findings affirm
the superiority of PEFT over full-fine tuning (FFT) in low-resource settings
and demonstrate that this advantage persists in AL setups. Finally, we delve
into the properties of PEFT and FFT through the lens of forgetting dynamics and
instance-level representations, linking them to AL instance selection behavior
and the stability of PEFT. Our research underscores the synergistic potential
of AL, PEFT, and TAPT in low-resource settings, paving the way for advancements
in efficient and effective fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14577">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) and the objective of masking-and-predicting in
particular have led to promising SSL performance on a variety of downstream
tasks. However, while most approaches randomly mask tokens, there is strong
intuition from the field of education that deciding what to mask can
substantially improve learning outcomes. We introduce Difference-Masking, an
approach that automatically chooses what to mask during continued pretraining
by considering what makes an unlabelled target domain different from the
pretraining domain. Empirically, we find that Difference-Masking outperforms
baselines on continued pretraining settings across four diverse language and
multimodal video tasks. The cross-task applicability of Difference-Masking
supports the effectiveness of our framework for SSL pretraining in language,
vision, and other domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?. (arXiv:2305.14578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14578">
<div class="article-summary-box-inner">
<span><p>Given the success of Graph Neural Networks (GNNs) for structure-aware machine
learning, numerous studies have explored their application to text
classification, as an alternative to traditional feature representation models.
However, most studies considered just a specific domain and validated on data
with particular characteristics. This work presents an extensive empirical
investigation of graph-based text representation methods proposed for text
classification, identifying practical implications and open challenges in the
field. We compare several GNN architectures as well as BERT across five
datasets, encompassing short and also long documents. The results show that: i)
graph performance is highly related to the textual input features and domain,
ii) despite its outstanding performance, BERT has difficulties converging when
dealing with short texts, iii) graph methods are particularly beneficial for
longer documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14580">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) systems play a key role in applications
involving human-machine interactions. Despite their importance, ASR models for
the Portuguese language proposed in the last decade have limitations in
relation to the correct identification of punctuation marks in automatic
transcriptions, which hinder the use of transcriptions by other systems,
models, and even by humans. However, recently Whisper ASR was proposed by
OpenAI, a general-purpose speech recognition model that has generated great
expectations in dealing with such limitations. This chapter presents the first
study on the performance of Whisper for punctuation prediction in the
Portuguese language. We present an experimental evaluation considering both
theoretical aspects involving pausing points (comma) and complete ideas
(exclamation, question, and fullstop), as well as practical aspects involving
transcript-based topic modeling - an application dependent on punctuation marks
for promising performance. We analyzed experimental results from videos of
Museum of the Person, a virtual museum that aims to tell and preserve people's
life histories, thus discussing the pros and cons of Whisper in a real-world
scenario. Although our experiments indicate that Whisper achieves
state-of-the-art results, we conclude that some punctuation marks require
improvements, such as exclamation, semicolon and colon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP. (arXiv:2305.14583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14583">
<div class="article-summary-box-inner">
<span><p>Language is multifaceted. A given utterance can be re-expressed in equivalent
forms, and its implicit and explicit content support various logical and
pragmatic inferences. When processing an utterance, we consider these different
aspects, as mediated by our interpretive goals -- understanding that "it's dark
in here" may be a veiled direction to turn on a light. Nonetheless, NLP methods
typically operate over the surface form alone, eliding this nuance.
</p>
<p>In this work, we represent language with language, and direct an LLM to
decompose utterances into logical and plausible inferences. The reduced
complexity of the decompositions makes them easier to embed, opening up novel
applications. Variations on our technique lead to state-of-the-art improvements
on sentence embedding benchmarks, a substantive application in computational
political science, and to a novel construct-discovery process, which we
validate with human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Topic Coherence Metrics. (arXiv:2305.14587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14587">
<div class="article-summary-box-inner">
<span><p>The recent explosion in work on neural topic modeling has been criticized for
optimizing automated topic evaluation metrics at the expense of actual
meaningful topic identification. But human annotation remains expensive and
time-consuming. We propose LLM-based methods inspired by standard human topic
evaluations, in a family of metrics called Contextualized Topic Coherence
(CTC). We evaluate both a fully automated version as well as a semi-automated
CTC that allows human-centered evaluation of coherence while maintaining the
efficiency of automated methods. We evaluate CTC relative to five other metrics
on six topic models and find that it outperforms automated topic coherence
methods, works well on short documents, and is not susceptible to meaningless
but high-scoring topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating end-to-end entity linking on domain-specific knowledge bases: Learning about ancient technologies from museum collections. (arXiv:2305.14588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14588">
<div class="article-summary-box-inner">
<span><p>To study social, economic, and historical questions, researchers in the
social sciences and humanities have started to use increasingly large
unstructured textual datasets. While recent advances in NLP provide many tools
to efficiently process such data, most existing approaches rely on generic
solutions whose performance and suitability for domain-specific tasks is not
well understood. This work presents an attempt to bridge this domain gap by
exploring the use of modern Entity Linking approaches for the enrichment of
museum collection data. We collect a dataset comprising of more than 1700 texts
annotated with 7,510 mention-entity pairs, evaluate some off-the-shelf
solutions in detail using this dataset and finally fine-tune a recent
end-to-end EL model on this data. We show that our fine-tuned model
significantly outperforms other approaches currently available in this domain
and present a proof-of-concept use case of this model. We release our dataset
and our best model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14590">
<div class="article-summary-box-inner">
<span><p>Current research in form understanding predominantly relies on large
pre-trained language models, necessitating extensive data for pre-training.
However, the importance of layout structure (i.e., the spatial relationship
between the entity blocks in the visually rich document) to relation extraction
has been overlooked. In this paper, we propose REgion-Aware Relation Extraction
(RE$^2$) that leverages region-level spatial structure among the entity blocks
to improve their relation prediction. We design an edge-aware graph attention
network to learn the interaction between entities while considering their
spatial relationship defined by their region-level representations. We also
introduce a constraint objective to regularize the model towards consistency
with the inherent constraints of the relation extraction task. Extensive
experiments across various datasets, languages and domains demonstrate the
superiority of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14591">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) excel at implementing code from functionality
descriptions, but struggle with algorithmic problems that require not only
implementation but also identification of the suitable algorithm. Moreover,
LLM-generated programs lack guaranteed correctness and require human
verification. To address these challenges, we propose ALGO, a framework that
synthesizes Algorithmic programs with LLM-Generated Oracles to guide the
creation and verify their correctness. ALGO first generates a probably correct
but possibly slow reference oracle by prompting an LLM to exhaustively
enumerate all the combinations of relevant variables. This oracle is then
utilized to guide an arbitrary search strategy in exploring the algorithm space
and to verify the algorithms synthesized. Our study shows that the
LLM-generated oracles are correct for 88% of the cases. With the oracles as
verifiers, ALGO can be integrated with any existing code generation model in a
model-agnostic manner to enhance its performance. Experiments show that when
equipped with ALGO, we achieve an 8x better one-submission pass rate over the
Codex model and a 2.6x better one-submission pass rate over CodeT, the current
state-of-the-art model on CodeContests. We can also get 1.3x better pass rate
over the ChatGPT Code Interpreter on unseen problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14592">
<div class="article-summary-box-inner">
<span><p>Style is used to convey authors' intentions and attitudes. Despite the
success of large pre-trained language models on style classification, prior
work relies on fine-tuning with labeled examples. Prompting large language
models to classify style without fine-tuning is challenging because language
styles can be difficult to define. In this study, we investigate the
effectiveness of style lexicons as a means for instructing language models how
to identify new styles that are unseen during training. Our experiments show
that lexicon-based instructions improve transfer zero-shot performance
significantly. We will release our code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14596">
<div class="article-summary-box-inner">
<span><p>When large language models (LMs) are applied in zero- or few-shot settings to
discriminative tasks such as multiple-choice questions, their attentiveness
(i.e., probability mass) is spread across many vocabulary tokens that are not
valid choices. Such a spread across multiple surface forms with identical
meaning is thought to cause an underestimation of a model's true performance,
referred to as the "surface form competition" (SFC) hypothesis. This has
motivated the introduction of various probability normalization methods.
However, many core questions remain unanswered. How do we measure SFC or
attentiveness? Are there direct ways of increasing attentiveness on valid
choices? Does increasing attentiveness always improve task accuracy? We propose
a mathematical formalism for studying this phenomenon, provide a metric for
quantifying attentiveness, and identify a simple method for increasing it --
namely, in-context learning with even just one example containing answer
choices. The formalism allows us to quantify SFC and bound its impact. Our
experiments on three diverse datasets and six LMs reveal several surprising
findings. For example, encouraging models to generate a valid answer choice
can, in fact, be detrimental to task performance for some LMs, and prior
probability normalization methods are less effective (sometimes even
detrimental) to instruction-tuned LMs. We conclude with practical insights for
effectively using prompted LMs for multiple-choice tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voices of Her: Analyzing Gender Differences in the AI Publication World. (arXiv:2305.14597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14597">
<div class="article-summary-box-inner">
<span><p>While several previous studies have analyzed gender bias in research, we are
still missing a comprehensive analysis of gender differences in the AI
community, covering diverse topics and different development trends. Using the
AI Scholar dataset of 78K researchers in the field of AI, we identify several
gender differences: (1) Although female researchers tend to have fewer overall
citations than males, this citation difference does not hold for all
academic-age groups; (2) There exist large gender homophily in co-authorship on
AI papers; (3) Female first-authored papers show distinct linguistic styles,
such as longer text, more positive emotion words, and more catchy titles than
male first-authored papers. Our analysis provides a window into the current
demographic trends in our AI community, and encourages more gender equality and
diversity in the future. Our code and data are at
https://github.com/causalNLP/ai-scholar-gender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. (arXiv:2305.14599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14599">
<div class="article-summary-box-inner">
<span><p>Traditional sentence embedding models encode sentences into vector
representations to capture useful properties such as the semantic similarity
between sentences. However, in addition to similarity, sentence semantics can
also be interpreted via compositional operations such as sentence fusion or
difference. It is unclear whether the compositional semantics of sentences can
be directly reflected as compositional operations in the embedding space. To
more effectively bridge the continuous embedding and discrete text spaces, we
explore the plausibility of incorporating various compositional properties into
the sentence embedding space that allows us to interpret embedding
transformations as compositional sentence operations. We propose InterSent, an
end-to-end framework for learning interpretable sentence embeddings that
supports compositional sentence operations in the embedding space. Our method
optimizes operator networks and a bottleneck encoder-decoder model to produce
meaningful and interpretable sentence embeddings. Experimental results
demonstrate that our method significantly improves the interpretability of
sentence embeddings on four textual generation tasks over existing approaches
while maintaining strong performance on traditional semantic similarity tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14600">
<div class="article-summary-box-inner">
<span><p>This paper addresses the question of how to efficiently learn from disjoint,
compatible label sequences. We argue that the compatible structures between
disjoint label sets help model learning and inference. We verify this
hypothesis on the task of semantic role labeling (SRL), specifically, tagging a
sentence with two role sequences: VerbNet arguments and PropBank arguments.
Prior work has shown that cross-task interaction improves performance. However,
the two tasks are still separately decoded, running the risk of generating
structurally inconsistent label sequences (as per lexicons like SEMLINK). To
eliminate this issue, we first propose a simple and effective setup that
jointly handles VerbNet and PropBank labels as one sequence. With this setup,
we show that enforcing SEMLINK constraints during decoding constantly improves
the overall F1. With special input constructions, our joint model infers
VerbNet arguments from PropBank arguments with over 99% accuracy. We also
propose a constrained marginal model that uses SEMLINK information during
training to further benefit from the large amounts of PropBank-only data. Our
models achieve state-of-the-art F1's on VerbNet and PropBank argument labeling
on the CoNLL05 dataset with strong out-of-domain generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenPI2.0: An Improved Dataset for Entity Tracking in Texts. (arXiv:2305.14603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14603">
<div class="article-summary-box-inner">
<span><p>Representing texts as information about entities has long been deemed
effective in event reasoning. We propose OpenPI2.0, an improved dataset for
tracking entity states in procedural texts. OpenPI2.0 features not only
canonicalized entities that facilitate evaluation, but also salience
annotations including both manual labels and automatic predictions. Regarding
entity salience, we provide a survey on annotation subjectivity, modeling
feasibility, and downstream applications in tasks such as question answering
and classical planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14610">
<div class="article-summary-box-inner">
<span><p>We introduce the notion of geopolitical bias -- a tendency to report
different geopolitical knowledge depending on the linguistic context. As a case
study, we consider territorial disputes between countries. For example, for the
widely contested Spratly Islands, would an LM be more likely to say they belong
to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To
evaluate if such biases exist, we first collect a dataset of territorial
disputes from Wikipedia, then associate each territory with a set of
multilingual, multiple-choice questions. This dataset, termed BorderLines,
consists of 250 territories with questions in 45 languages. We pose these
question sets to language models, and analyze geopolitical bias in their
responses through several proposed quantitative metrics. The metrics compare
between responses in different question languages as well as to the actual
geopolitical situation. The phenomenon of geopolitical bias is a uniquely
cross-lingual evaluation, contrasting with prior work's monolingual (mostly
English) focus on bias evaluation. Its existence shows that the knowledge of
LMs, unlike multilingual humans, is inconsistent across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14613">
<div class="article-summary-box-inner">
<span><p>Trustworthy language models should abstain from answering questions when they
do not know the answer. However, the answer to a question can be unknown for a
variety of reasons. Prior research has focused on the case in which the
question is clear and the answer is unambiguous but possibly unknown. However,
the answer to a question can also be unclear due to uncertainty of the
questioner's intent or context. We investigate question answering from this
perspective, focusing on answering a subset of questions with a high degree of
accuracy, from a set of questions in which many are inherently ambiguous. In
this setting, we find that the most reliable approach to calibration involves
quantifying repetition within a set of sampled model outputs, rather than the
model's likelihood or self-verification as used in prior work. % We find this
to be the case across different types of uncertainty, varying model scales and
both with or without instruction tuning. Our results suggest that
sampling-based confidence scores help calibrate answers to relatively
unambiguous questions, with more dramatic improvements on ambiguous questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Grounding Issues in Image Caption. (arXiv:2305.14616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14616">
<div class="article-summary-box-inner">
<span><p>This paper explores the grounding issue concerning multimodal semantic
representation from a computational cognitive-linguistic view. Five perceptual
properties of groundedness are annotated and analyzed: Affordance, Perceptual
salience, Object number, Gaze cueing, and Ecological Niche Association (ENA).
We annotated selected images from the Flickr30k dataset with exploratory
analyses and statistical modeling of their captions. Our findings suggest that
a comprehensive understanding of an object or event requires cognitive
attention, semantic distinctions in linguistic expression, and multimodal
construction. During this construction process, viewers integrate situated
meaning and affordance into multimodal semantics, which is consolidated into
image captions used in the image-text dataset incorporating visual and textual
elements. Our findings suggest that situated meaning and affordance grounding
are critical for grounded natural language understanding systems to generate
appropriate responses and show the potential to advance the understanding of
human construal in diverse situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14617">
<div class="article-summary-box-inner">
<span><p>Understanding the speaker's intended meaning often involves drawing
commonsense inferences to reason about what is not stated explicitly. In
multi-event sentences, it requires understanding the relationships between
events based on contextual knowledge. We propose COMET-M (Multi-Event), an
event-centric commonsense model capable of generating commonsense inferences
for a target event within a complex sentence. COMET-M builds upon COMET
(Bosselut et al., 2019), which excels at generating event-centric inferences
for simple sentences, but struggles with the complexity of multi-event
sentences prevalent in natural text. To overcome this limitation, we curate a
multi-event inference dataset of 35K human-written inferences. We trained
COMET-M on the human-written inferences and also created baselines using
automatically labeled examples. Experimental results demonstrate the
significant performance improvement of COMET-M over COMET in generating
multi-event inferences. Moreover, COMET-M successfully produces distinct
inferences for each target event, taking the complete context into
consideration. COMET-M holds promise for downstream tasks involving natural
text such as coreference resolution, dialogue, and story understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations. (arXiv:2305.14618v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14618">
<div class="article-summary-box-inner">
<span><p>Abductive reasoning aims to find plausible explanations for an event. This
style of reasoning is critical for commonsense tasks where there are often
multiple plausible explanations. Existing approaches for abductive reasoning in
natural language processing (NLP) often rely on manually generated annotations
for supervision; however, such annotations can be subjective and biased.
Instead of using direct supervision, this work proposes an approach for
abductive commonsense reasoning that exploits the fact that only a subset of
explanations is correct for a given context. The method uses posterior
regularization to enforce a mutual exclusion constraint, encouraging the model
to learn the distinction between fluent explanations and plausible ones. We
evaluate our approach on a diverse set of abductive reasoning datasets;
experimental results show that our approach outperforms or is comparable to
directly applying pretrained language models in a zero-shot manner and other
knowledge-augmented zero-shot methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EXnet: Efficient In-context Learning for Data-less Text classification. (arXiv:2305.14622v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14622">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (PLMs) have made significant progress in
encoding world knowledge and spawned a new set of learning paradigms including
zero-shot, few-shot, and in-context learning. Many language tasks can be
modeled as a set of prompts (for example, is this text about geography?) and
language models can provide binary answers, i.e., Yes or No. There is evidence
to suggest that the next-word prediction used by many PLMs does not align well
with zero-shot paradigms. Therefore, PLMs are fine-tuned as a
question-answering system. In-context learning extends zero-shot learning by
incorporating prompts and examples, resulting in increased task accuracy. Our
paper presents EXnet, a model specifically designed to perform in-context
learning without any limitations on the number of examples. We argue that
in-context learning is an effective method to increase task accuracy, and
providing examples facilitates cross-task generalization, especially when it
comes to text classification tasks. With extensive experiments, we show that
even our smallest model (15M parameters) generalizes to several unseen
classification tasks and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models. (arXiv:2305.14623v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14623">
<div class="article-summary-box-inner">
<span><p>Fact-checking is an essential task in NLP that is commonly utilized for
validating the factual accuracy of claims. Prior work has mainly focused on
fine-tuning pre-trained languages models on specific datasets, which can be
computationally intensive and time-consuming. With the rapid development of
large language models (LLMs), such as ChatGPT and GPT-3, researchers are now
exploring their in-context learning capabilities for a wide range of tasks. In
this paper, we aim to assess the capacity of LLMs for fact-checking by
introducing Self-Checker, a framework comprising a set of plug-and-play modules
that facilitate fact-checking by purely prompting LLMs in an almost zero-shot
setting. This framework provides a fast and efficient way to construct
fact-checking systems in low-resource environments. Empirical results
demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.
However, there is still significant room for improvement compared to SOTA
fine-tuned models, which suggests that LLM adoption could be a promising
approach for future fact-checking research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KNN-LM Does Not Improve Open-ended Text Generation. (arXiv:2305.14625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14625">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the generation quality of interpolation-based
retrieval-augmented language models (LMs). These methods, best exemplified by
the KNN-LM, interpolate the LM's predicted distribution of the next word with a
distribution formed from the most relevant retrievals for a given prefix. While
the KNN-LM and related methods yield impressive decreases in perplexity, we
discover that they do not exhibit corresponding improvements in open-ended
generation quality, as measured by both automatic evaluation metrics (e.g.,
MAUVE) and human evaluations. Digging deeper, we find that interpolating with a
retrieval distribution actually increases perplexity compared to a baseline
Transformer LM for the majority of tokens in the WikiText-103 test set, even
though the overall perplexity is lower due to a smaller number of tokens for
which perplexity dramatically decreases after interpolation. However, when
decoding a long sequence at inference time, significant improvements on this
smaller subset of tokens are washed out by slightly worse predictions on most
tokens. Furthermore, we discover that the entropy of the retrieval distribution
increases faster than that of the base LM as the generated sequence becomes
longer, which indicates that retrieval is less reliable when using
model-generated text as queries (i.e., is subject to exposure bias). We hope
that our analysis spurs future work on improved decoding algorithms and
interpolation strategies for retrieval-augmented language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14627">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have emerged as a widely-used tool for
information seeking, but their generated outputs are prone to hallucination. In
this work, we aim to enable LLMs to generate text with citations, improving
their factual correctness and verifiability. Existing work mainly relies on
commercial search engines and human evaluation, making it challenging to
reproduce and compare with different modeling approaches. We propose ALCE, the
first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a
diverse set of questions and retrieval corpora and requires building end-to-end
systems to retrieve supporting evidence and generate answers with citations. We
build automatic metrics along three dimensions -- fluency, correctness, and
citation quality -- and demonstrate their strong correlation with human
judgements. Our experiments with state-of-the-art LLMs and novel prompting
strategies show that current systems have considerable room for improvements --
for example, on the ELI5 dataset, even the best model has 49% of its
generations lacking complete citation support. Our extensive analyses further
highlight promising future directions, including developing better retrievers,
advancing long-context LLMs, and improving the ability to synthesize
information from multiple sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture of Prompt Experts for Generalizable and Interpretable Question Answering. (arXiv:2305.14628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14628">
<div class="article-summary-box-inner">
<span><p>One of the ultimate quests of question answering (QA) is to deploy a system
that can answer any type of question from the users, and refrain from answering
when it does not know the answer. While recent advancements in scaling large
language models (LLMs) brought significant improvements on various QA datasets,
it remains difficult for a single model to generalize across question types
that require distinct reasoning abilities. In this paper, we first provide
empirical evidence that state-of-the-art LLMs such as Codex suffer from poor
generalizability on question types beyond those seen in the prompt. To address
this, we propose a Mixture-of-Prompt-Experts (MOPE) system that ensembles
multiple specialized LLMs. We first implement each specialized model based on
the same backbone model (Codex) but with prompts optimized for different
reasoning categories including factual, multihop, mathematical, and commonsense
reasoning. By strategically selecting the best specialized model for each given
question, our MOPE system significantly outperforms any single specialized
model on a collection of 12 QA datasets from four reasoning types. Moreover,
the attribution and agreement among specialized expert models offer greater
interpretability, allowing for better selective question answering. Our human
study further confirms that presenting the expert predictions and answer
selection process helps annotators more accurately decide when to trust the
system's output. We release all code and data to facilitate future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing Causal Models of Word Meaning in GPT-3 and -4. (arXiv:2305.14630v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14630">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have driven extraordinary improvements in NLP.
However, it is unclear how such models represent lexical concepts-i.e., the
meanings of the words they use. This paper evaluates the lexical
representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of
concept representations which focuses on representations of words describing
artifacts (such as "mop", "pencil", and "whistle"). The theory posits a causal
graph that relates the meanings of such words to the form, use, and history of
the objects to which they refer. We test LLMs using the same stimuli originally
used by Chaigneau et al. (2004) to evaluate the theory in humans, and consider
a variety of prompt designs. Our experiments concern judgements about causal
outcomes, object function, and object naming. We find no evidence that GPT-3
encodes the causal structure hypothesized by HIPE, but do find evidence that
GPT-4 encodes such structure. The results contribute to a growing body of
research characterizing the representational capacity of large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14635">
<div class="article-summary-box-inner">
<span><p>End-to-end speech translation (ST) is the task of translating speech signals
in the source language into text in the target language. As a cross-modal task,
end-to-end ST is difficult to train with limited data. Existing methods often
try to transfer knowledge from machine translation (MT), but their performances
are restricted by the modality gap between speech and text. In this paper, we
propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality
gap. We find the alignment between speech and text sequences via optimal
transport and then mix up the sequences from different modalities at a token
level using the alignment. Experiments on the MuST-C ST benchmark demonstrate
that CMOT achieves an average BLEU of 30.0 in 8 translation directions,
outperforming previous methods. Further analysis shows CMOT can adaptively find
the alignment between modalities, which helps alleviate the modality gap
between speech and text. Code is publicly available at
https://github.com/ictnlp/CMOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization. (arXiv:2305.14645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14645">
<div class="article-summary-box-inner">
<span><p>Biomedical entity linking and event extraction are two crucial tasks to
support text understanding and retrieval in the biomedical domain. These two
tasks intrinsically benefit each other: entity linking disambiguates the
biomedical concepts by referring to external knowledge bases and the domain
knowledge further provides additional clues to understand and extract the
biological processes, while event extraction identifies a key trigger and
entities involved to describe each biological process which also captures the
structural context to better disambiguate the biomedical entities. However,
previous research typically solves these two tasks separately or in a pipeline,
leading to error propagation. What's more, it's even more challenging to solve
these two tasks together as there is no existing dataset that contains
annotations for both tasks. To solve these challenges, we propose joint
biomedical entity linking and event extraction by regarding the event
structures and entity references in knowledge bases as latent variables and
updating the two task-specific models in a hard Expectation-Maximization (EM)
fashion: (1) predicting the missing variables for each partially annotated
dataset based on the current two task-specific models, and (2) updating the
parameters of each model on the corresponding pseudo completed dataset.
Experimental results on two benchmark datasets: Genia 2011 for event extraction
and BC4GO for entity linking, show that our joint framework significantly
improves the model for each individual task and outperforms the strong
baselines for both tasks. We will make the code and model checkpoints publicly
available once the paper is accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14647">
<div class="article-summary-box-inner">
<span><p>Opinions in the scientific domain can be divergent, leading to controversy or
consensus among reviewers. However, current opinion summarization datasets
mostly focus on product review domains, which do not account for this
variability under the assumption that the input opinions are non-controversial.
To address this gap, we propose the task of scientific opinion summarization,
where research paper reviews are synthesized into meta-reviews. To facilitate
this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews
and 40,903 paper reviews from 39 conferences. Furthermore, we propose the
Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down
the task into several stages and iteratively refines the summary under the
guidance of questions from a checklist. We conclude that (1) human-written
summaries are not always reliable since many do not follow the guideline, and
(2) the combination of task decomposition and iterative self-refinement shows
promising discussion involvement ability and can be applied to other complex
text generation using black-box LLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14651">
<div class="article-summary-box-inner">
<span><p>Recent embedding-based methods have achieved great successes on exploiting
entity alignment from knowledge graph (KG) embeddings of multiple modals. In
this paper, we study embedding-based entity alignment (EEA) from a perspective
of generative models. We show that EEA is a special problem where the main
objective is analogous to that in a typical generative model, based on which we
theoretically prove the effectiveness of the recently developed generative
adversarial network (GAN)-based EEA methods. We then reveal that their
incomplete objective limits the capacity on both entity alignment and entity
synthesis (i.e., generating new entities). We mitigate this problem by
introducing a generative EEA (abbr., GEEA) framework with the proposed mutual
variational autoencoder (M-VAE) as the generative model. M-VAE can convert an
entity from one KG to another and generate new entities from random noise
vectors. We demonstrate the power of GEEA with theoretical analysis and
empirical experiments on both entity alignment and entity synthesis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14652">
<div class="article-summary-box-inner">
<span><p>Video multimodal fusion aims to integrate multimodal signals in videos, such
as visual, audio and text, to make a complementary prediction with multiple
modalities contents. However, unlike other image-text multimodal tasks, video
has longer multimodal sequences with more redundancy and noise in both visual
and audio modalities. Prior denoising methods like forget gate are coarse in
the granularity of noise filtering. They often suppress the redundant and noisy
information at the risk of losing critical information. Therefore, we propose a
denoising bottleneck fusion (DBF) model for fine-grained video multimodal
fusion. On the one hand, we employ a bottleneck mechanism to filter out noise
and redundancy with a restrained receptive field. On the other hand, we use a
mutual information maximization module to regulate the filter-out module to
preserve key information within different modalities. Our DBF model achieves
significant improvement over current state-of-the-art baselines on multiple
benchmarks covering multimodal sentiment analysis and multimodal summarization
tasks. It proves that our model can effectively capture salient features from
noisy and redundant video, audio, and text inputs. The code for this paper is
publicly available at https://github.com/WSXRHFG/DBF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14658">
<div class="article-summary-box-inner">
<span><p>LLMs (large language models) such as ChatGPT have shown remarkable language
understanding and generation capabilities. Although reference-free evaluators
based on LLMs show better human alignment than traditional reference-based
evaluators, there are many challenges in using reference-free evaluators based
on LLMs. Reference-free evaluators are more suitable for open-ended examples
with different semantics responses. But not all examples are open-ended. For
closed-ended examples with unique correct semantic response, reference-free
evaluators will still consider it high quality when giving a response that is
inconsistent with the facts and the semantic of reference. In order to
comprehensively evaluate the reliability of evaluators based on LLMs, we
construct two adversarial meta-evaluation dialogue generation datasets
KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared
to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more
challenging since they requires evaluators to be able to reasonably evaluate
closed-ended examples with the help of external knowledge or even its own
knowledge. Empirical results show that the ability of LLMs to identify
unreasonable responses is insufficient. There are risks in using eference-free
evaluators based on LLMs to evaluate the quality of dialogue responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction. (arXiv:2305.14659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14659">
<div class="article-summary-box-inner">
<span><p>Learning template based information extraction from documents is a crucial
yet difficult task. Prior template-based IE approaches assume foreknowledge of
the domain templates; however, real-world IE do not have pre-defined schemas
and it is a figure-out-as you go phenomena. To quickly bootstrap templates in a
real-world setting, we need to induce template slots from documents with zero
or minimal supervision. Since the purpose of question answering intersect with
the goal of information extraction, we use automatic question generation to
induce template slots from the documents and investigate how a tiny amount of a
proxy human-supervision on-the-fly (termed as InteractiveIE) can further boost
the performance. Extensive experiments on biomedical and legal documents, where
obtaining training data is expensive, reveal encouraging trends of performance
improvement using InteractiveIE over AI-only baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction. (arXiv:2305.14660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14660">
<div class="article-summary-box-inner">
<span><p>Mathematical symbol definition extraction is important for improving
scholarly reading interfaces and scholarly information extraction (IE).
However, the task poses several challenges: math symbols are difficult to
process as they are not composed of natural language morphemes; and scholarly
papers often contain sentences that require resolving complex coordinate
structures. We present SymDef, an English language dataset of 5,927 sentences
from full-text scientific papers where each sentence is annotated with all
mathematical symbols linked with their corresponding definitions. This dataset
focuses specifically on complex coordination structures such as "respectively"
constructions, which often contain overlapping definition spans. We also
introduce a new definition extraction method that masks mathematical symbols,
creates a copy of each sentence for each symbol, specifies a target symbol, and
predicts its corresponding definition spans using slot filling. Our experiments
show that our definition extraction model significantly outperforms RoBERTa and
other strong IE baseline systems by 10.9 points with a macro F1 score of 84.82.
With our dataset and model, we can detect complex definitions in scholarly
documents to make scientific writing more readable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Are What You Annotate: Towards Better Models through Annotator Representations. (arXiv:2305.14663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14663">
<div class="article-summary-box-inner">
<span><p>Annotator disagreement is ubiquitous in natural language processing (NLP)
tasks. There are multiple reasons for such disagreements, including the
subjectivity of the task, difficult cases, unclear guidelines, and so on.
Rather than simply aggregating labels to obtain data annotations, we instead
propose to explicitly account for the annotator idiosyncrasies and leverage
them in the modeling process. We create representations for the annotators
(annotator embeddings) and their annotations (annotation embeddings) with
learnable matrices associated with each. Our approach significantly improves
model performance on various NLP benchmarks by adding fewer than 1% model
parameters. By capturing the unique tendencies and subjectivity of individual
annotators, our embeddings help democratize AI and ensure that AI models are
inclusive of diverse viewpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Models in NLP: A Survey. (arXiv:2305.14671v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14671">
<div class="article-summary-box-inner">
<span><p>This survey paper provides a comprehensive review of the use of diffusion
models in natural language processing (NLP). Diffusion models are a class of
mathematical models that aim to capture the diffusion of information or signals
across a network or manifold. In NLP, diffusion models have been used in a
variety of applications, such as natural language generation, sentiment
analysis, topic modeling, and machine translation. This paper discusses the
different formulations of diffusion models used in NLP, their strengths and
limitations, and their applications. We also perform a thorough comparison
between diffusion models and alternative generative models, specifically
highlighting the autoregressive (AR) models, while also examining how diverse
architectures incorporate the Transformer in conjunction with diffusion models.
Compared to AR models, diffusion models have significant advantages for
parallel generation, text interpolation, token-level controls such as syntactic
structures and semantic contents, and robustness. Exploring further
permutations of integrating Transformers into diffusion models would be a
valuable pursuit. Also, the development of multimodal diffusion models and
large-scale diffusion language models with notable capabilities for few-shot
learning would be important directions for the future advance of diffusion
models in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Character Similarity with Vision Transformers. (arXiv:2305.14672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14672">
<div class="article-summary-box-inner">
<span><p>Record linkage is a bedrock of quantitative social science, as analyses often
require linking data from multiple, noisy sources. Off-the-shelf string
matching methods are widely used, as they are straightforward and cheap to
implement and scale. Not all character substitutions are equally probable, and
for some settings there are widely used handcrafted lists denoting which string
substitutions are more likely, that improve the accuracy of string matching.
However, such lists do not exist for many settings, skewing research with
linked datasets towards a few high-resource contexts that are not
representative of the diversity of human societies. This study develops an
extensible way to measure character substitution costs for OCR'ed documents, by
employing large-scale self-supervised training of vision transformers (ViT)
with augmented digital fonts. For each language written with the CJK script, we
contrastively learn a metric space where different augmentations of the same
character are represented nearby. In this space, homoglyphic characters - those
with similar appearance such as ``O'' and ``0'' - have similar vector
representations. Using the cosine distance between characters' representations
as the substitution cost in an edit distance matching algorithm significantly
improves record linkage compared to other widely used string matching methods,
as OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly
capture character visual similarity across any script, including low-resource
settings. We illustrate this by creating homoglyph sets for 3,000 year old
ancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is
able to capture relationships in how different abstract concepts were
conceptualized by ancient societies, that have been noted in the archaeological
literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions. (arXiv:2305.14676v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14676">
<div class="article-summary-box-inner">
<span><p>Generalization to unseen tasks is an important ability for few-shot learners
to achieve better zero-/few-shot performance on diverse tasks. However, such
generalization to vision-language tasks including grounding and generation
tasks has been under-explored; existing few-shot VL models struggle to handle
tasks that involve object grounding and multiple images such as visual
commonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded
vIsion Language aLigning, a novel VL model that can be generalized to diverse
tasks including visual question answering, captioning, and grounding tasks with
no or very few training instances. Specifically, GRILL learns object grounding
and localization by exploiting object-text alignments, which enables it to
transfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model
on various zero-/few-shot VL tasks and show that it consistently surpasses the
state-of-the-art few-shot methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14681">
<div class="article-summary-box-inner">
<span><p>Does inverse scaling only occur as a function of model parameter size, or can
it also occur over the course of training? We carry out an exploratory study
investigating whether, over the course of training on the language modeling
task, the performance of language models at specific tasks can decrease while
general performance remains high. We find that for two tasks from the Inverse
Scaling Challenge - quote-repetition and redefine-math - this is indeed the
case. Specifically, we find that for Pythia (Biderman et al., 2023) models with
a higher number of parameters, performance decreases over the course of
training at these two tasks, despite these models showing standard (positive)
scaling overall. This highlights the importance of testing model performance at
all relevant benchmarks any time they are trained on additional data, even if
their overall performance improves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering. (arXiv:2305.14682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14682">
<div class="article-summary-box-inner">
<span><p>Hybrid Question-Answering (HQA), which targets reasoning over tables and
passages linked from table cells, has witnessed significant research in recent
years. A common challenge in HQA and other passage-table QA datasets is that it
is generally unrealistic to iterate over all table rows, columns, and linked
passages to retrieve evidence. Such a challenge made it difficult for previous
studies to show their reasoning ability in retrieving answers. To bridge this
gap, we propose a novel Table-alignment-based Cell-selection and Reasoning
model (TACR) for hybrid text and table QA, evaluated on the HybridQA and
WikiTableQuestions datasets. In evidence retrieval, we design a
table-question-alignment enhanced cell-selection method to retrieve
fine-grained evidence. In answer reasoning, we incorporate a QA module that
treats the row containing selected cells as context. Experimental results over
the HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves
state-of-the-art results on cell selection and outperforms fine-grained
evidence retrieval baselines on HybridQA, while achieving competitive
performance on WTQ. We also conducted a detailed analysis to demonstrate that
being able to align questions to tables in the cell-selection stage can result
in important gains from experiments of over 90\% table row and column selection
accuracy, meanwhile also improving output explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14688">
<div class="article-summary-box-inner">
<span><p>The answering quality of an aligned large language model (LLM) can be
drastically improved if treated with proper crafting of prompts. In this paper,
we propose ExpertPrompting to elicit the potential of LLMs to answer as
distinguished experts. We first utilize In-Context Learning to automatically
synthesize detailed and customized descriptions of the expert identity for each
specific instruction, and then ask LLMs to provide answer conditioned on such
agent background. Based on this augmented prompting strategy, we produce a new
set of instruction-following data using GPT-3.5, and train a competitive
open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation
to show that 1) the expert data is of significantly higher quality than vanilla
answers, and 2) ExpertLLaMA outperforms existing open-source opponents and
achieves 96\% of the original ChatGPT's capability. All data and the
ExpertLLaMA model will be made publicly available at
\url{https://github.com/OFA-Sys/ExpertLLaMA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs. (arXiv:2305.14693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14693">
<div class="article-summary-box-inner">
<span><p>Have Large Language Models (LLMs) developed a personality? The short answer
is a resounding "We Don't Know!". In this paper, we show that we do not yet
have the right tools to measure personality in language models. Personality is
an important characteristic that influences behavior. As LLMs emulate
human-like intelligence and performance in various tasks, a natural question to
ask is whether these models have developed a personality. Previous works have
evaluated machine personality through self-assessment personality tests, which
are a set of multiple-choice questions created to evaluate personality in
humans. A fundamental assumption here is that human personality tests can
accurately measure personality in machines. In this paper, we investigate the
emergence of personality in five LLMs of different sizes ranging from 1.5B to
30B. We propose the Option-Order Symmetry property as a necessary condition for
the reliability of these self-assessment tests. Under this condition, the
answer to self-assessment questions is invariant to the order in which the
options are presented. We find that many LLMs personality test responses do not
preserve option-order symmetry. We take a deeper look at LLMs test responses
where option-order symmetry is preserved to find that in these cases, LLMs do
not take into account the situational statement being tested and produce the
exact same answer irrespective of the situation being tested. We also identify
the existence of inherent biases in these LLMs which is the root cause of the
aforementioned phenomenon and makes self-assessment tests unreliable. These
observations indicate that self-assessment tests are not the correct tools to
measure personality in LLMs. Through this paper, we hope to draw attention to
the shortcomings of current literature in measuring personality in LLMs and
call for developing tools for machine personality measurement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14695">
<div class="article-summary-box-inner">
<span><p>Entity bias widely affects pretrained (large) language models, causing them
to excessively rely on (biased) parametric knowledge to make unfaithful
predictions. Although causality-inspired methods have shown great potential to
mitigate entity bias, it is hard to precisely estimate the parameters of
underlying causal models in practice. The rise of black-box LLMs also makes the
situation even worse, because of their inaccessible parameters and uncalibrated
logits. To address these problems, we propose a specific structured causal
model (SCM) whose parameters are comparatively easier to estimate. Building
upon this SCM, we propose causal intervention techniques to mitigate entity
bias for both white-box and black-box settings. The proposed causal
intervention perturbs the original entity with neighboring entities. This
intervention reduces specific biasing information pertaining to the original
entity while still preserving sufficient common predictive information from
similar entities. When evaluated on the relation extraction task, our
training-time intervention significantly improves the F1 score of RoBERTa by
5.7 points on EntRED, in which spurious shortcuts between entities and labels
are removed. Meanwhile, our in-context intervention effectively reduces the
knowledge conflicts between parametric knowledge and contextual knowledge in
GPT-3.5 and improves the F1 score by 9.14 points on a challenging test set
derived from Re-TACRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank. (arXiv:2305.14696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14696">
<div class="article-summary-box-inner">
<span><p>Deep neural classifiers trained with cross-entropy loss (CE loss) often
suffer from poor calibration, necessitating the task of out-of-distribution
(OOD) detection. Traditional supervised OOD detection methods require expensive
manual annotation of in-distribution and OOD samples. To address the annotation
bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that
requires only in-distribution samples as supervision. We cast OOD detection as
an inter-document intra-label (IDIL) ranking problem and train the classifier
with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a
set of in-distribution documents and their labels, for each label, we train the
classifier to rank the softmax scores of documents belonging to that label to
be higher than the scores of documents that belong to other labels. Unlike CE
loss, our IDIL loss function reaches zero when the desired confidence ranking
is achieved and gradients are backpropagated to decrease probabilities
associated with incorrect labels rather than continuously increasing the
probability of the correct label. Extensive experiments with several
classifiers on multiple classification datasets demonstrate the effectiveness
of our method in both coarse- and fine-grained settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. (arXiv:2305.14701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14701">
<div class="article-summary-box-inner">
<span><p>Humans can learn languages from remarkably little experience. Developing
computational models that explain this ability has been a major challenge in
cognitive science. Bayesian models that build in strong inductive biases -
factors that guide generalization - have been successful at explaining how
humans might generalize from few examples in controlled settings but are
usually too restrictive to be tractably applied to more naturalistic data. By
contrast, neural networks have flexible representations that allow them to
learn well from naturalistic data but require many more examples than humans
receive. We show that learning from limited naturalistic data is possible with
an approach that combines the strong inductive biases of a Bayesian model with
the flexible representations of a neural network. This approach works by
distilling a Bayesian model's biases into a neural network. Like a Bayesian
model, the resulting system can learn formal linguistic patterns from a small
number of examples. Like a neural network, it can also learn aspects of English
syntax from a corpus of natural language - and it outperforms a standard neural
network at acquiring the linguistic phenomena of recursion and priming.
Bridging the divide between Bayesian models and neural networks makes it
possible to handle a broader range of learning scenarios than either approach
can handle on its own.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14702">
<div class="article-summary-box-inner">
<span><p>Pairwise human judgments are pivotal in guiding large language models (LLMs)
to generate outputs that align with human preferences. They are also often used
in summarization evaluation, complementing existing automatic metrics. Despite
their significance, however, there has been limited research probing these
pairwise human judgments. The collective impact and respective weights of
factors such as informativeness, coherence, fluency, and factual consistency
remain elusive. The impact of hidden factors on the final judgment is also
unclear. In this paper, we conduct an in-depth examination of a dataset of
pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce
model, we identify key factors that could potentially influence human
judgments. Our research uncovers the inherent preferences embedded in human
judgments and suggests strategies to boost sample efficiency. Finally, we
provide insights on the construction of balanced datasets for human judgment
evaluations, a crucial step in shaping the behaviors of future LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14705">
<div class="article-summary-box-inner">
<span><p>The explosive growth of language models and their applications have led to an
increased demand for efficient and scalable methods. In this paper, we
introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert
(MoE) models. We show that naively finetuning MoE models on a task-specific
dataset (in other words, no instruction-finetuning) often yield worse
performance compared to dense models of the same computational complexity.
However, our Flan-MoE outperforms dense models under multiple experiment
settings: instruction-finetuning only and instruction-finetuning followed by
task-specific finetuning. This shows that instruction-finetuning is an
essential stage for MoE models. Specifically, our largest model, Flan-MoE-32B,
surpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing
only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the
design of large-scale, high-performance language models, under the setting of
task-agnostic learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14707">
<div class="article-summary-box-inner">
<span><p>Due to the prohibitively high cost of creating error correction datasets,
most Factual Claim Correction methods rely on a powerful verification model to
guide the correction process. This leads to a significant drop in performance
in domains like Scientific Claim Correction, where good verification models do
not always exist. In this work, we introduce a claim correction system that
makes no domain assumptions and does not require a verifier but is able to
outperform existing methods by an order of magnitude -- achieving 94%
correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open
dataset, compared to the next best methods 0.5% and 1.50% respectively. Our
method leverages the power of prompting with LLMs during training to create a
richly annotated dataset that can be used for fully supervised training and
regularization. We additionally use a claim-aware decoding procedure to improve
the quality of corrected claims. Our method is competitive with the very LLM
that was used to generate the annotated dataset -- with GPT3.5 achieving 89.5%
and 60% correction accuracy on SciFact and SciFact-Open, despite using 1250
times as many parameters as our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14710">
<div class="article-summary-box-inner">
<span><p>Instruction-tuned models are trained on crowdsourcing datasets with task
instructions to achieve superior performance. However, in this work we raise
security concerns about this training paradigm. Our studies demonstrate that an
attacker can inject backdoors by issuing very few malicious instructions among
thousands of gathered data and control model behavior through data poisoning,
without even the need of modifying data instances or labels themselves. Through
such instruction attacks, the attacker can achieve over 90% attack success rate
across four commonly used NLP datasets, and cause persistent backdoors that are
easily transferred to 15 diverse datasets zero-shot. In this way, the attacker
can directly apply poisoned instructions designed for one dataset on many other
datasets. Moreover, the poisoned model cannot be cured by continual learning.
Lastly, instruction attacks show resistance to existing inference-time defense.
These findings highlight the need for more robust defenses against data
poisoning attacks in instructiontuning models and underscore the importance of
ensuring data quality in instruction crowdsourcing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14711">
<div class="article-summary-box-inner">
<span><p>Pretrained model-based evaluation metrics have demonstrated strong
performance with high correlations with human judgments in various natural
language generation tasks such as image captioning. Despite the impressive
results, their impact on fairness is under-explored -- it is widely
acknowledged that pretrained models can encode societal biases, and utilizing
them for evaluation purposes may inadvertently manifest and potentially amplify
biases. In this paper, we conduct a systematic study in gender biases of
model-based evaluation metrics with a focus on image captioning tasks.
Specifically, we first identify and quantify gender biases in different
evaluation metrics regarding profession, activity, and object concepts. Then,
we demonstrate the negative consequences of using these biased metrics, such as
favoring biased generation models in deployment and propagating the biases to
generation models through reinforcement learning. We also present a simple but
effective alternative to reduce gender biases by combining n-gram
matching-based and pretrained model-based evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GlobalBench: A Benchmark for Global Progress in Natural Language Processing. (arXiv:2305.14716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14716">
<div class="article-summary-box-inner">
<span><p>Despite the major advances in NLP, significant disparities in NLP system
performance across languages still exist. Arguably, these are due to uneven
resource allocation and sub-optimal incentives to work on less resourced
languages. To track and further incentivize the global development of equitable
language technology, we introduce GlobalBench. Prior multilingual benchmarks
are static and have focused on a limited number of tasks and languages. In
contrast, GlobalBench is an ever-expanding collection that aims to dynamically
track progress on all NLP datasets in all languages. Rather than solely
measuring accuracy, GlobalBench also tracks the estimated per-speaker utility
and equity of technology across all languages, providing a multi-faceted view
of how language technology is serving people of the world. Furthermore,
GlobalBench is designed to identify the most under-served languages, and
rewards research efforts directed towards those languages. At present, the most
under-served languages are the ones with a relatively high population, but
nonetheless overlooked by composite multilingual benchmarks (like Punjabi,
Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190
languages, and has 1,128 system submissions spanning 62 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Correlations Between Contexts and Definitions with Multiple Definition Modeling. (arXiv:2305.14717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14717">
<div class="article-summary-box-inner">
<span><p>Definition modeling is an important task in advanced natural language
applications such as understanding and conversation. Since its introduction, it
focus on generating one definition for a target word or phrase in a given
context, which we refer to as Single Definition Modeling (SDM). However, this
approach does not adequately model the correlations and patterns among
different contexts and definitions of words. In addition, the creation of a
training dataset for SDM requires significant human expertise and effort. In
this paper, we carefully design a new task called Multiple Definition Modeling
(MDM) that pool together all contexts and definition of target words. We
demonstrate the ease of creating a model as well as multiple training sets
automatically. % In the experiments, we demonstrate and analyze the benefits of
MDM, including improving SDM's performance by using MDM as the pretraining task
and its comparable performance in the zero-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14718">
<div class="article-summary-box-inner">
<span><p>Improving language model generations according to some user-defined quality
or style constraints is challenging. Typical approaches include learning on
additional human-written data, filtering ``low-quality'' data using heuristics
and/or using reinforcement learning with human feedback (RLHF). However,
filtering can remove valuable training signals, whereas data collection and
RLHF constantly require additional human-written or LM exploration data which
can be costly to obtain. A natural question to ask is ``Can we leverage RL to
optimize LM utility on existing crowd-sourced and internet data?''
</p>
<p>To this end, we present Left-over Lunch RL (LoL-RL), a simple training
algorithm that uses offline policy gradients for learning language generation
tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary
classifier-based or human-defined utility functions on any sequence-to-sequence
data. Experiments with five different language generation tasks using models of
varying sizes and multiple rewards show that models trained with LoL-RL can
consistently outperform the best supervised learning models. We also release
our experimental code. https://github.com/abaheti95/LoL-RL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CuRIAM: Corpus re Interpretation and Metalanguage in U.S. Supreme Court Opinions. (arXiv:2305.14719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14719">
<div class="article-summary-box-inner">
<span><p>Most judicial decisions involve the interpretation of legal texts; as such,
judicial opinion requires the use of language as a medium to comment on or draw
attention to other language. Language used this way is called metalanguage. We
develop an annotation schema for categorizing types of legal metalanguage and
apply our schema to a set of U.S. Supreme Court opinions, yielding a corpus
totaling 59k tokens. We remark on several patterns observed in the kinds of
metalanguage used by the justices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14724">
<div class="article-summary-box-inner">
<span><p>Visual metaphors are powerful rhetorical devices used to persuade or
communicate creative ideas through images. Similar to linguistic metaphors,
they convey meaning implicitly through symbolism and juxtaposition of the
symbols. We propose a new task of generating visual metaphors from linguistic
metaphors. This is a challenging task for diffusion-based text-to-image models,
such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning
and compositionality. We propose to solve the task through the collaboration
between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3
(davinci-002) with Chain-of-Thought prompting generates text that represents a
visual elaboration of the linguistic metaphor containing the implicit meaning
and relevant objects, which is then used as input to the diffusion-based
text-to-image models.Using a human-AI collaboration framework, where humans
interact both with the LLM and the top-performing diffusion model, we create a
high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic
metaphors and their associated visual elaborations. Evaluation by professional
illustrators shows the promise of LLM-Diffusion Model collaboration for this
task.To evaluate the utility of our Human-AI collaboration framework and the
quality of our dataset, we perform both an intrinsic human-based evaluation and
an extrinsic evaluation using visual entailment as a downstream task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes. (arXiv:2305.14725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14725">
<div class="article-summary-box-inner">
<span><p>We propose attribute-aware multimodal entity linking, where the input is a
mention described with a text and image, and the goal is to predict the
corresponding target entity from a multimodal knowledge base (KB) where each
entity is also described with a text description, a visual image and a set of
attributes and values. To support this research, we construct AMELI, a
large-scale dataset consisting of 18,472 reviews and 35,598 products. To
establish baseline performance on AMELI, we experiment with the current
state-of-the-art multimodal entity linking approaches and our enhanced
attribute-aware model and demonstrate the importance of incorporating the
attribute information into the entity linking process. To be best of our
knowledge, we are the first to build benchmark dataset and solutions for the
attribute-aware multimodal entity linking task. Datasets and codes will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14726">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can use in-context demonstrations to improve
performance on zero-shot tasks. However, selecting the best in-context examples
is challenging because model performance can vary widely depending on the
selected examples. We present a cross-entropy difference (CED) method for
selecting in-context demonstrations. Our method is based on the observation
that the effectiveness of in-context demonstrations negatively correlates with
the perplexity of the test example by a language model that was finetuned on
that demonstration. We utilize parameter efficient finetuning to train small
models on training data that are used for computing the cross-entropy
difference between a test example and every candidate in-context demonstration.
This metric is used to rank and select in-context demonstrations independently
for each test input. We evaluate our method on a mix-domain dataset that
combines 8 benchmarks, representing 4 text generation tasks, showing that CED
for in-context demonstration selection can improve performance for a variety of
LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations. (arXiv:2305.14728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14728">
<div class="article-summary-box-inner">
<span><p>Although deep language representations have become the dominant form of
language featurization in recent years, in many settings it is important to
understand a model's decision-making process. This necessitates not only an
interpretable model but also interpretable features. In particular, language
must be featurized in a way that is interpretable while still characterizing
the original text well. We present SenteCon, a method for introducing human
interpretability in deep language representations. Given a passage of text,
SenteCon encodes the text as a layer of interpretable categories in which each
dimension corresponds to the relevance of a specific category. Our empirical
evaluations indicate that encoding language with SenteCon provides high-level
interpretability at little to no cost to predictive performance on downstream
tasks. Moreover, we find that SenteCon outperforms existing interpretable
language representations with respect to both its downstream performance and
its agreement with human characterizations of the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation. (arXiv:2305.14734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14734">
<div class="article-summary-box-inner">
<span><p>Grammatical error correction (GEC) is a well-explored problem in English with
many existing models and datasets. However, research on GEC in morphologically
rich languages has been limited due to challenges such as data scarcity and
language complexity. In this paper, we present the first results on Arabic GEC
by using two newly developed Transformer-based pretrained sequence-to-sequence
models. We address the task of multi-class Arabic grammatical error detection
(GED) and present the first results on multi-class Arabic GED. We show that
using GED information as auxiliary input in GEC models improves GEC performance
across three datasets spanning different genres. Moreover, we also investigate
the use of contextual morphological preprocessing in aiding GEC systems. Our
models achieve state-of-the-art results on two Arabic GEC shared tasks datasets
and establish a strong benchmark on a newly created dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14735">
<div class="article-summary-box-inner">
<span><p>A standard method for measuring the impacts of AI on marginalized communities
is to determine performance discrepancies between specified demographic groups.
These approaches aim to address harms toward vulnerable groups, but they
obscure harm patterns faced by intersectional subgroups or shared across
demographic groups. We instead operationalize "the margins" as data points that
are statistical outliers due to having demographic attributes distant from the
"norm" and measure harms toward these outliers. We propose a Group-Based
Performance Disparity Index (GPDI) that measures the extent to which a
subdivision of a dataset into subgroups identifies those facing increased
harms. We apply our approach to detecting disparities in toxicity detection and
find that text targeting outliers is 28% to 86% more toxic for all types of
toxicity examined. We also discover that model performance is consistently
worse for demographic outliers, with disparities in error between outliers and
non-outliers ranging from 28% to 71% across toxicity types. Our outlier-based
analysis has comparable or higher GPDI than traditional subgroup-based
analyses, suggesting that outlier analysis enhances identification of subgroups
facing greater harms. Finally, we find that minoritized racial and religious
groups are most associated with outliers, which suggests that outlier analysis
is particularly beneficial for identifying harms against those groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. (arXiv:2305.14739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14739">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) often struggle to pay enough attention to the input
context, and generate texts that are unfaithful or contain hallucinations. To
mitigate this issue, we present context-aware decoding (CAD), which follows a
contrastive output distribution that amplifies the difference between the
output probabilities when a model is used with and without context. Our
experiments show that CAD, without additional training, significantly improves
the faithfulness of different LM families, including OPT, GPT, LLaMA and
FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality
metrics). Furthermore, CAD is particularly effective in overriding a model's
prior knowledge when it contradicts the provided context, leading to
substantial improvements in tasks where resolving the knowledge conflict is
essential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14740">
<div class="article-summary-box-inner">
<span><p>We introduce ECHo, a diagnostic dataset of event causality inference grounded
in visual-and-linguistic social scenarios. ECHo employs real-world
human-centric deductive information collected from crime drama, bridging the
gap in multimodal reasoning towards higher social intelligence through the
elicitation of intermediate Theory-of-Mind (ToM). We propose a unified
framework aligned with the Chain-of-Thought (CoT) paradigm to assess the
reasoning capability of current AI systems. This ToM-enhanced CoT pipeline can
accommodate and integrate various large foundation models in zero-shot
visual-and-linguistic understanding. With this framework, we scrutinize the
advanced large language and multimodal models via three complementary
human-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging
dataset to expose imperfections and inconsistencies in reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation. (arXiv:2305.14750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14750">
<div class="article-summary-box-inner">
<span><p>When answering complex questions, large language models (LLMs) may produce
answers that do not satisfy all criteria of the question. While existing
self-evaluation techniques aim to detect if such answers are correct, these
techniques are unable to determine which criteria of the question are satisfied
by the generated answers. To address this issue, we propose answer-based claim
decomposition (ABCD), a prompting strategy that decomposes questions into a
series of true/false claims that can be used to verify which criteria of the
input question an answer satisfies. Using the decomposed ABCD claims, we
perform fine-grained self-evaluation. Through preliminary experiments on three
datasets, including a newly-collected challenge dataset ObscureQA, we find that
GPT-3.5 has some ability to determine to what extent its answer satisfies the
criteria of the input question, and can give insights into the errors and
knowledge gaps of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade. (arXiv:2305.14751v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14751">
<div class="article-summary-box-inner">
<span><p>In the constant updates of the product dialogue systems, we need to retrain
the natural language understanding (NLU) model as new data from the real users
would be merged into the existent data accumulated in the last updates. Within
the newly added data, new intents would emerge and might have semantic
entanglement with the existing intents, e.g. new intents that are semantically
too specific or generic are actually subset or superset of some existing
intents in the semantic space, thus impairing the robustness of the NLU model.
As the first attempt to solve this problem, we setup a new benchmark consisting
of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent
detection with imperfect data in the system update as a multi-label
classification task with positive but unlabeled intents, which asks the models
to recognize all the proper intents, including the ones with semantic
entanglement, in the inference. We also propose comprehensive baseline models
and conduct in-depth analyses for the benchmark, showing that the semantically
entangled intents can be effectively recognized with an automatic workflow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14755">
<div class="article-summary-box-inner">
<span><p>Most existing stylistic text rewriting methods operate on a sentence level,
but ignoring the broader context of the text can lead to generic, ambiguous,
and incoherent rewrites. In this paper, we propose the integration of preceding
textual context into both the rewriting and evaluation stages of stylistic text
rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We
conduct a comparative evaluation of rewriting through few-shot prompting of
GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites.
Our experiments show that humans often prefer contextual rewrites over
non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To
bridge this gap, we propose context-infused versions of common automatic
metrics, and show that these better reflect human preferences. Overall, our
paper highlights the importance of integrating preceding textual context into
both the rewriting and evaluation stages of stylistic text rewriting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14757">
<div class="article-summary-box-inner">
<span><p>We present metrics for evaluating dialog systems through a
psychologically-grounded "human" lens: conversational agents express a
diversity of both states (short-term factors like emotions) and traits
(longer-term factors like personality) just as people do. These interpretable
metrics consist of five measures from established psychology constructs that
can be applied both across dialogs and on turns within dialogs: emotional
entropy, linguistic style and emotion matching, as well as agreeableness and
empathy. We compare these human metrics against 6 state-of-the-art automatic
metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We
also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which
consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We
demonstrate the proposed human metrics offer novel information, are
uncorrelated with automatic metrics, and lead to increased accuracy beyond
existing automatic metrics for predicting crowd-sourced dialog judgements. The
interpretability and unique signal of our proposed human-centered framework
make it a valuable tool for evaluating and improving dialog systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization. (arXiv:2305.14760v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14760">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have achieved remarkable success in a variety of
natural language understanding tasks. Nevertheless, finetuning large pretrained
models on downstream tasks is susceptible to overfitting if the training set is
limited, which will lead to diminished performance. In this work, we propose a
dynamic fine-tuning strategy for pretrained language models called Bi-Drop. It
utilizes the gradient information of various sub-models generated by dropout to
update the model parameters selectively. Experiments on the GLUE benchmark show
that Bi-Drop outperforms previous fine-tuning methods by a considerable margin,
and exhibits consistent superiority over vanilla fine-tuning across various
pretrained models. Furthermore, empirical results indicate that Bi-Drop yields
substantial improvements in the multiple task or domain transfer, data
imbalance, and low-resource scenarios, demonstrating superb generalization
ability and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14761">
<div class="article-summary-box-inner">
<span><p>Charts are very popular for analyzing data, visualizing key insights and
answering complex reasoning questions about data. To facilitate chart-based
data analysis using natural language, several downstream tasks have been
introduced recently such as chart question answering and chart summarization.
However, most of the methods that solve these tasks use pretraining on language
or vision-language tasks that do not attempt to explicitly model the structure
of the charts (e.g., how data is visually encoded and how chart elements are
related to each other). To address this, we first build a large corpus of
charts covering a wide variety of topics and visual styles. We then present
UniChart, a pretrained model for chart comprehension and reasoning. UniChart
encodes the relevant text, data, and visual elements of charts and then uses a
chart-grounded text decoder to generate the expected output in natural
language. We propose several chart-specific pretraining tasks that include: (i)
low-level tasks to extract the visual elements (e.g., bars, lines) and data
from charts, and (ii) high-level tasks to acquire chart understanding and
reasoning skills. We find that pretraining the model on a large corpus with
chart-specific low- and high-level tasks followed by finetuning on three
down-streaming tasks results in state-of-the-art performance on three
downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. (arXiv:2305.14763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14763">
<div class="article-summary-box-inner">
<span><p>The escalating debate on AI's capabilities warrants developing reliable
metrics to assess machine "intelligence". Recently, many anecdotal examples
were used to suggest that newer large language models (LLMs) like ChatGPT and
GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached
conflicting conclusions regarding those abilities. We investigate the extent of
LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs
exhibit certain N-ToM abilities, this behavior is far from being robust. We
further examine the factors impacting performance on N-ToM tasks and discover
that LLMs struggle with adversarial examples, indicating reliance on shallow
heuristics rather than robust ToM abilities. We caution against drawing
conclusions from anecdotal examples, limited benchmark testing, and using
human-designed psychological tests to evaluate models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14766">
<div class="article-summary-box-inner">
<span><p>Open-domain question answering is a crucial task that often requires
accessing external information. Existing methods typically adopt a single-turn
retrieve-then-read approach, where relevant documents are first retrieved, and
questions are then answered based on the retrieved information. However, there
are cases where answering a question requires implicit knowledge that is not
directly retrievable from the question itself. In this work, we propose a novel
question-answering pipeline called eamSearchQA. Our approach leverages large
language models(LLMs) to iteratively generate new questions about the original
question, enabling an iterative reasoning process. By iteratively refining and
expanding the scope of the question, our method aims to capture and utilize
hidden knowledge that may not be directly obtainable through retrieval. We
evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The
experimental results demonstrate that BeamSearchQA significantly outperforms
other zero-shot baselines, indicating its effectiveness in tackling the
challenges of open-domain question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14770">
<div class="article-summary-box-inner">
<span><p>The rise of large language models (LLMs) has brought a critical need for
high-quality human-labeled data, particularly for processes like human feedback
and evaluation. A common practice is to label data via consensus annotation
over the judgments of multiple crowdworkers. However, different annotators may
have different interpretations of labeling schemes unless given extensive
training, and for subjective NLP tasks, even trained expert annotators can
diverge heavily. We show that these nuances can be captured by high quality
natural language explanations, and propose a method to rescale ordinal
annotation in the presence of disagreement using LLMs. Specifically, we feed
Likert ratings and corresponding natural language explanations into an LLM and
prompt it to produce a numeric score. This score should reflect the underlying
assessment of the example by the annotator. The presence of explanations allows
the LLM to homogenize ratings across annotators in spite of scale usage
differences. We explore our technique in the context of a document-grounded
question answering task on which large language models achieve near-human
performance. Among questions where annotators identify incompleteness in the
answers, our rescaling improves correlation between nearly all annotator pairs,
improving pairwise correlation on these examples by an average of 0.2 Kendall's
tau.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14771">
<div class="article-summary-box-inner">
<span><p>Diffusion-based language models (LMs) have been shown to be competent
generative models that are easy to control at inference and are a promising
alternative to autoregressive LMs. While autoregressive LMs have benefited
immensely from scaling and instruction-based learning, existing studies on
diffusion LMs have been conducted on a relatively smaller scale. Starting with
a recently proposed diffusion model SSD-LM, in this work we explore methods to
scale it from 0.4B to 13B parameters, proposing several techniques to improve
its training and inference efficiency. We call the new model SSD-2. We further
show that this model can be easily finetuned to follow instructions. Finally,
leveraging diffusion models' capability at inference-time control, we show that
SSD-2 facilitates novel ensembles with 100x smaller models that can be
customized and deployed by individual users. We find that compared to
autoregressive models, the collaboration between diffusion models is more
effective, leading to higher-quality and more relevant model responses due to
their ability to incorporate bi-directional contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Controllable QA-based Framework for Decontextualization. (arXiv:2305.14772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14772">
<div class="article-summary-box-inner">
<span><p>Many real-world applications require surfacing extracted snippets to users,
whether motivated by assistive tools for literature surveys or document
cross-referencing, or needs to mitigate and recover from model generated
inaccuracies., Yet, these passages can be difficult to consume when divorced
from their original document context. In this work, we explore the limits of
LLMs to perform decontextualization of document snippets in user-facing
scenarios, focusing on two real-world settings - question answering and
citation context previews for scientific documents. We propose a
question-answering framework for decontextualization that allows for better
handling of user information needs and preferences when determining the scope
of rewriting. We present results showing state-of-the-art LLMs under our
framework remain competitive with end-to-end approaches. We also explore
incorporating user preferences into the system, finding our framework allows
for controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. (arXiv:2305.14775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14775">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (PLMs) have shown evidence of acquiring
vast amounts of knowledge, it remains unclear how much of this parametric
knowledge is actually usable in performing downstream tasks. We propose a
systematic framework to measure parametric knowledge utilization in PLMs. Our
framework first extracts knowledge from a PLM's parameters and subsequently
constructs a downstream task around this extracted knowledge. Performance on
this task thus depends exclusively on utilizing the model's possessed
knowledge, avoiding confounding factors like insufficient signal. As an
instantiation, we study factual knowledge of PLMs and measure utilization
across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps -
in acquired vs. utilized knowledge, (2) they show limited robustness in
utilizing knowledge under distribution shifts, and (3) larger models close the
acquired knowledge gap but the utilized knowledge gap remains. Overall, our
study provides insights into PLMs' capabilities beyond their acquired
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14779">
<div class="article-summary-box-inner">
<span><p>In this work we present an approach for generating alternative text (or
alt-text) descriptions for images shared on social media, specifically Twitter.
This task is more than just a special case of image captioning, as alt-text is
both more literally descriptive and context-specific. Also critically, images
posted to Twitter are often accompanied by user-written text that despite not
necessarily describing the image may provide useful context that if properly
leveraged can be informative -- e.g. the tweet may name an uncommon object in
the image that the model has not previously seen. We address this with a CLIP
prefix model that extracts an embedding of the image and passes it to a mapping
network that outputs a short sequence in word embedding space, or a ``prefix'',
to which we also concatenate the text from the tweet itself. This lets the
model condition on both visual and textual information from the post. The
combined multimodal prefix is then fed as a prompt to a pretrained language
model which autoregressively completes the sequence to generate the alt-text.
While prior work has used similar methods for captioning, ours is the first to
our knowledge that incorporates textual information from the associated social
media post into the prefix as well, and we further demonstrate through
ablations that utility of these two information sources stacks. We put forward
a new dataset scraped from Twitter and evaluate on it across a variety of
automated metrics as well as human evaluation, and show that our approach of
conditioning on both tweet text and visual information significantly
outperforms prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Phonetic Representation for Chinese Spelling Correction. (arXiv:2305.14783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14783">
<div class="article-summary-box-inner">
<span><p>Chinese Spelling Correction (CSC) aims to detect and correct erroneous
characters in Chinese texts. Although efforts have been made to introduce
phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic
representations with character representations, which tends to weaken the
representation effect of normal texts. In this work, we propose to disentangle
the two types of features to allow for direct interaction between textual and
phonetic information. To learn useful phonetic representations, we introduce a
pinyin-to-character objective to ask the model to predict the correct
characters based solely on phonetic information, where a separation mask is
imposed to disable attention from phonetic input to text. To avoid overfitting
the phonetics, we further design a self-distillation module to ensure that
semantic information plays a major role in the prediction. Extensive
experiments on three CSC benchmarks demonstrate the superiority of our method
in using phonetic information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anthropomorphization of AI: Opportunities and Risks. (arXiv:2305.14784v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14784">
<div class="article-summary-box-inner">
<span><p>Anthropomorphization is the tendency to attribute human-like traits to
non-human entities. It is prevalent in many social contexts -- children
anthropomorphize toys, adults do so with brands, and it is a literary device.
It is also a versatile tool in science, with behavioral psychology and
evolutionary biology meticulously documenting its consequences. With widespread
adoption of AI systems, and the push from stakeholders to make it human-like
through alignment techniques, human voice, and pictorial avatars, the tendency
for users to anthropomorphize it increases significantly. We take a dyadic
approach to understanding this phenomenon with large language models (LLMs) by
studying (1) the objective legal implications, as analyzed through the lens of
the recent blueprint of AI bill of rights and the (2) subtle psychological
aspects customization and anthropomorphization. We find that anthropomorphized
LLMs customized for different user bases violate multiple provisions in the
legislative blueprint. In addition, we point out that anthropomorphization of
LLMs affects the influence they can have on their users, thus having the
potential to fundamentally change the nature of human-AI interaction, with
potential for manipulation and negative influence. With LLMs being
hyper-personalized for vulnerable groups like children and patients among
others, our work is a timely and important contribution. We propose a
conservative strategy for the cautious use of anthropomorphization to improve
trustworthiness of AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14785">
<div class="article-summary-box-inner">
<span><p>This paper sheds light on the limitations of ChatGPT's understanding
capabilities, focusing on simple inference tasks that are typically easy for
humans but appear to be challenging for the model. Specifically, we target (i)
grammatically-specified entailments, (ii) premises with evidential adverbs of
uncertainty, and (iii) monotonicity entailments. We present expert-designed
evaluation sets for these inference types and conduct experiments in a
zero-shot setup. Our results show that the model struggles with these types of
inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT
demonstrates knowledge of the underlying linguistic concepts when prompted
directly, it often fails to incorporate this knowledge to make correct
inferences. Even more strikingly, further experiments show that embedding the
premise under presupposition triggers or non-factive verbs causes the model to
predict entailment more frequently {regardless} of the correct semantic label.
Overall these results suggest that, despite GPT's celebrated language
understanding capacity, ChatGPT has blindspots with respect to certain types of
entailment, and that certain entailment-cancelling features act as ``blinds''
overshadowing the semantics of the embedded premise. Our analyses emphasize the
need for further research into the linguistic comprehension and reasoning
capabilities of LLMs, in order to improve their reliability, and establish
their trustworthiness for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14788">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) are powerful and widely-applicable
tools, but their usefulness is constrained by a finite context window and the
expensive computational cost of processing long text documents. We propose to
adapt pre-trained LMs into AutoCompressors. These models are capable of
compressing long contexts into compact summary vectors, which are then
accessible to the model as soft prompts. Summary vectors are trained with an
unsupervised objective, whereby long documents are processed in segments and
summary vectors from all previous segments are used in language modeling. We
fine-tune OPT models on sequences of up to 30,720 tokens and show that
AutoCompressors can utilize long contexts to improve perplexity. We evaluate
AutoCompressors on in-context learning by compressing task demonstrations. We
find that summary vectors are good substitutes for plain-text demonstrations,
increasing accuracy while reducing inference cost. Finally, we explore the
benefits of pre-computing summary vectors for large corpora by applying summary
vectors to retrieval-augmented language modeling. Overall, AutoCompressors
emerge as a simple and inexpensive solution for extending the context window of
LMs while speeding up inference over long contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14790">
<div class="article-summary-box-inner">
<span><p>Topic segmentation and outline generation strive to divide a document into
coherent topic sections and generate corresponding subheadings. Such a process
unveils the discourse topic structure of a document that benefits quickly
grasping and understanding the overall context of the document from a higher
level. However, research and applications in this field have been restrained
due to the lack of proper paragraph-level topic representations and
large-scale, high-quality corpora in Chinese compared to the success achieved
in English. Addressing these issues, we introduce a hierarchical
paragraph-level topic structure representation with title, subheading, and
paragraph that comprehensively models the document discourse topic structure.
In addition, we ensure a more holistic representation of topic distribution
within the document by using sentences instead of keywords to represent
sub-topics. Following this representation, we construct the largest Chinese
Paragraph-level Topic Structure corpus (CPTS), four times larger than the
previously largest one. We also employ a two-stage man-machine collaborative
annotation method to ensure the high quality of the corpus both in form and
semantics. Finally, we validate the computability of CPTS on two fundamental
tasks (topic segmentation and outline generation) by several strong baselines,
and its efficacy has been preliminarily confirmed on the downstream task:
discourse parsing. The representation, corpus, and benchmark we established
will provide a solid foundation for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Counterfactual Generator: Strengths and Weaknesses. (arXiv:2305.14791v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14791">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable performance in a
range of natural language understanding and generation tasks. Yet, their
ability to generate counterfactuals, which can be used for areas like data
augmentation, remains under-explored. This study aims to investigate the
counterfactual generation capabilities of LLMs and analysis factors that
influence this ability. First, we evaluate how effective are LLMs in
counterfactual generation through data augmentation experiments for small
language models (SLMs) across four tasks: sentiment analysis, natural language
inference, named entity recognition, and relation extraction. While LLMs show
promising enhancements in various settings, they struggle in complex tasks due
to their self-limitations and the lack of logical guidance to produce
counterfactuals that align with commonsense. Second, our analysis reveals the
pivotal role of providing accurate task definitions and detailed step-by-step
instructions to LLMs in generating counterfactuals. Interestingly, we also find
that LLMs can generate reasonable counterfactuals even with unreasonable
demonstrations, which illustrates that demonstrations are primarily to regulate
the output format.This study provides the first comprehensive insight into
counterfactual generation abilities of LLMs, and offers a novel perspective on
utilizing LLMs for data augmentation to enhance SLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14793">
<div class="article-summary-box-inner">
<span><p>Methods to generate text from structured data have advanced significantly in
recent years, primarily due to fine-tuning of pre-trained language models on
large datasets. However, such models can fail to produce output faithful to the
input data, particularly on out-of-domain data. Sufficient annotated data is
often not available for specific domains, leading us to seek an unsupervised
approach to improve the faithfulness of output text. Since the problem is
fundamentally one of consistency between the representations of the structured
data and text, we evaluate the effectiveness of cycle training in this work.
Cycle training uses two models which are inverses of each other: one that
generates text from structured data, and one which generates the structured
data from natural language text. We show that cycle training, when initialized
with a small amount of supervised data (100 samples in our case), achieves
nearly the same performance as fully supervised approaches for the data-to-text
generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform
extensive empirical analysis with automated evaluation metrics and a newly
designed human evaluation schema to reveal different cycle training strategies'
effectiveness of reducing various types of generation errors. Our code is
publicly available at https://github.com/Edillower/CycleNLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14794">
<div class="article-summary-box-inner">
<span><p>Recent advances in weakly supervised text classification mostly focus on
designing sophisticated methods to turn high-level human heuristics into
quality pseudo-labels. In this paper, we revisit the seed matching-based
method, which is arguably the simplest way to generate pseudo-labels, and show
that its power was greatly underestimated. We show that the limited performance
of seed matching is largely due to the label bias injected by the simple
seed-match rule, which prevents the classifier from learning reliable
confidence for selecting high-quality pseudo-labels. Interestingly, simply
deleting the seed words present in the matched input texts can mitigate the
label bias and help learn better confidence. Subsequently, the performance
achieved by seed matching can be improved significantly, making it on par with
or even better than the state-of-the-art. Furthermore, to handle the case when
the seed words are not made known, we propose to simply delete the word tokens
in the input text randomly with a high deletion ratio. Remarkably, seed
matching equipped with this random deletion method can often achieve even
better performance than that with seed deletion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14795">
<div class="article-summary-box-inner">
<span><p>The information stored in large language models (LLMs) falls out of date
quickly, and retraining from scratch is often not an option. This has recently
given rise to a range of techniques for injecting new facts through updating
model weights. Current evaluation paradigms are extremely limited, mainly
validating the recall of edited facts, but changing one fact should cause
rippling changes to the model's related beliefs. If we edit the UK Prime
Minister to now be Rishi Sunak, then we should get a different answer to Who is
married to the British Prime Minister? In this work, we present a benchmark
MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising
multi-hop questions that assess whether edited models correctly answer
questions where the answer should change as an entailed consequence of edited
facts. While we find that current knowledge-editing approaches can recall
edited facts accurately, they fail catastrophically on the constructed
multi-hop questions. We thus propose a simple memory-based approach, MeLLo,
which stores all edited facts externally while prompting the language model
iteratively to generate answers that are consistent with the edited facts.
While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up
to 175B) and outperforms previous model editors by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14802">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have exhibited an impressive ability to perform
in-context learning (ICL) from only a few examples, but the success of ICL
varies widely from task to task. Thus, it is important to quickly determine
whether ICL is applicable to a new task, but directly evaluating ICL accuracy
can be expensive in situations where test data is expensive to annotate -- the
exact situations where ICL is most appealing. In this paper, we propose the
task of ICL accuracy estimation, in which we predict the accuracy of an LLM
when doing in-context learning on a new task given only unlabeled data for that
task. To perform ICL accuracy estimation, we propose a method that trains a
meta-model using LLM confidence scores as features. We compare our method to
several strong accuracy estimation baselines on a new benchmark that covers 4
LLMs and 3 task collections. On average, the meta-model improves over all
baselines and achieves the same estimation performance as directly evaluating
on 40 labeled test examples per task, across the total 12 settings. We
encourage future work to improve on our methods and evaluate on our ICL
accuracy estimation benchmark to deepen our understanding of when ICL works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models. (arXiv:2111.00160v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00160">
<div class="article-summary-box-inner">
<span><p>Gigantic pre-trained models have become central to natural language
processing (NLP), serving as the starting point for fine-tuning towards a range
of downstream tasks. However, two pain points persist for this paradigm: (a) as
the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the
fine-tuning process can be time-consuming and computationally expensive; (b)
the fine-tuned model has the same size as its starting point by default, which
is neither sensible due to its more specialized functionality, nor practical
since many fine-tuned models will be deployed in resource-constrained
environments. To address these pain points, we propose a framework for
resource- and parameter-efficient fine-tuning by leveraging the sparsity prior
in both weight updates and the final model weights. Our proposed framework,
dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two
key objectives: (i) parameter efficient fine-tuning - by enforcing
sparsity-aware low-rank updates on top of the pre-trained weights; and (ii)
resource-efficient inference - by encouraging a sparse weight structure towards
the final fine-tuned model. We leverage sparsity in these two directions by
exploiting both unstructured and structured sparse patterns in pre-trained
language models via a unified approach. Extensive experiments and in-depth
investigations, with diverse network backbones (i.e., BERT, RoBERTa, and GPT-2)
on dozens of datasets, consistently demonstrate impressive
parameter-/inference-efficiency, while maintaining competitive downstream
performance. For instance, DSEE saves about 25% inference FLOPs while achieving
comparable performance, with 0.5% trainable parameters on BERT. Codes are
available in https://github.com/VITA-Group/DSEE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05337">
<div class="article-summary-box-inner">
<span><p>Controllable Text Generation (CTG) is emerging area in the field of natural
language generation (NLG). It is regarded as crucial for the development of
advanced text generation technologies that are more natural and better meet the
specific constraints in practical applications. In recent years, methods using
large-scale pre-trained language models (PLMs), in particular the widely used
transformer-based PLMs, have become a new paradigm of NLG, allowing generation
of more diverse and fluent text. However, due to the lower level of
interpretability of deep neural networks, the controllability of these methods
need to be guaranteed. To this end, controllable text generation using
transformer-based PLMs has become a rapidly growing yet challenging new
research hotspot. A diverse range of approaches have emerged in the recent 3-4
years, targeting different CTG tasks which may require different types of
controlled constraints. In this paper, we present a systematic critical review
on the common tasks, main approaches and evaluation methods in this area.
Finally, we discuss the challenges that the field is facing, and put forward
various promising future directions. To the best of our knowledge, this is the
first survey paper to summarize CTG techniques from the perspective of PLMs. We
hope it can help researchers in related fields to quickly track the academic
frontier, providing them with a landscape of the area and a roadmap for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models. (arXiv:2202.01802v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01802">
<div class="article-summary-box-inner">
<span><p>Adaptive mobile device-based health interventions often use machine learning
models trained on non-mobile device data, such as social media text, due to the
difficulty and high expense of collecting large text message (SMS) data.
Therefore, understanding the differences and generalization of models between
these platforms is crucial for proper deployment. We examined the
psycho-linguistic differences between Facebook and text messages, and their
impact on out-of-domain model performance, using a sample of 120 users who
shared both. We found that users use Facebook for sharing experiences (e.g.,
leisure) and SMS for task-oriented and conversational purposes (e.g., plan
confirmations), reflecting the differences in the affordances. To examine the
downstream effects of these differences, we used pre-trained Facebook-based
language models to estimate age, gender, depression, life satisfaction, and
stress on both Facebook and SMS. We found no significant differences in
correlations between the estimates and self-reports across 6 of 8 models. These
results suggest using pre-trained Facebook language models to achieve better
accuracy with just-in-time interventions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Cross-lingual Prompting with Dual Prompt Augmentation. (arXiv:2202.07255v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07255">
<div class="article-summary-box-inner">
<span><p>Prompting shows promising results in few-shot scenarios. However, its
strength for multilingual/cross-lingual problems has not been fully exploited.
Zhao and Sch\"utze (2021) made initial explorations in this direction by
presenting that cross-lingual prompting outperforms cross-lingual finetuning.
In this paper, we conduct an empirical exploration on the effect of each
component in cross-lingual prompting and derive language-agnostic Universal
Prompting, which helps alleviate the discrepancies between source-language
training and target-language inference. Based on this, we propose DPA, a dual
prompt augmentation framework, aiming at relieving the data scarcity issue in
few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54%
with only 16 English training examples per class, significantly better than
34.99% of finetuning. Our code is available at
https://github.com/DAMO-NLP-SG/DPA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12505">
<div class="article-summary-box-inner">
<span><p>Recent works in Event Argument Extraction (EAE) have focused on improving
model generalizability to cater to new events and domains. However, standard
benchmarking datasets like ACE and ERE cover less than 40 event types and 25
entity-centric argument roles. Limited diversity and coverage hinder these
datasets from adequately evaluating the generalizability of EAE models. In this
paper, we first contribute by creating a large and diverse EAE ontology. This
ontology is created by transforming FrameNet, a comprehensive semantic role
labeling (SRL) dataset for EAE, by exploiting the similarity between these two
tasks. Then, exhaustive human expert annotations are collected to build the
ontology, concluding with 115 events and 220 argument roles, with a significant
portion of roles not being entities. We utilize this ontology to further
introduce GENEVA, a diverse generalizability benchmarking dataset comprising
four test suites, aimed at evaluating models' ability to handle limited data
and unseen event type generalization. We benchmark six EAE models from various
families. The results show that owing to non-entity argument roles, even the
best-performing model can only achieve 39% F1 score, indicating how GENEVA
provides new challenges for generalization in EAE. Overall, our large and
diverse EAE ontology can aid in creating more comprehensive future resources,
while GENEVA is a challenging benchmarking dataset encouraging further research
for improving generalizability in EAE. The code and data can be found at
https://github.com/PlusLabNLP/GENEVA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Large Language Models Better Reasoners with Step-Aware Verifier. (arXiv:2206.02336v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02336">
<div class="article-summary-box-inner">
<span><p>Few-shot learning is a challenging task that requires language models to
generalize from limited examples. Large language models like GPT-3 and PaLM
have made impressive progress in this area, but they still face difficulties in
reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve
their reasoning skills, previous work has proposed to guide the language model
with prompts that elicit a series of reasoning steps before giving the final
answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in
problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on
Reasoning Step), a novel approach that further enhances the reasoning
capability of language models. DIVERSE has three main components: first, it
generates diverse prompts to explore different reasoning paths for the same
question; second, it uses a verifier to filter out incorrect answers based on a
weighted voting scheme; and third, it verifies each reasoning step individually
instead of the whole chain. We evaluate DIVERSE on the latest language model
code-davinci-002 and show that it achieves new state-of-the-art results on six
of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07550">
<div class="article-summary-box-inner">
<span><p>Originating as a philosophical quest, the study of personality concerns how
individuals differ in thinking, feeling, and behaving. Towards building social
machines that work with humans on a daily basis, we are motivated to ask: Do
existing Large Language Models (LLMs) possess personalities akin to their human
counterparts? If so, how can we evaluate them? Further, given this evaluation
framework, how can we induce a particular personality in a controllable
fashion? To answer these three questions, we propose the Machine Personality
Inventory (MPI) dataset for evaluating the machine personality; MPI follows
standardized personality tests, built upon the Big Five Personality Factors
(Big Five) theory and personality assessment inventories. By systematically
evaluating LLMs with MPI, we provide the first piece of evidence showing the
existence of personality in LLMs. We further devise a Personality Prompting
(P^2) method to induce LLMs with a specific personality in a controllable
manner, capable of producing diverse behaviors. We hope this work sheds light
on future studies by adopting personality as the essential guide for various
downstream tasks, building human-like and in situ dialogue agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">News Summarization and Evaluation in the Era of GPT-3. (arXiv:2209.12356v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12356">
<div class="article-summary-box-inner">
<span><p>The recent success of prompting large language models like GPT-3 has led to a
paradigm shift in NLP research. In this paper, we study its impact on text
summarization, focusing on the classic benchmark domain of news summarization.
First, we investigate how GPT-3 compares against fine-tuned models trained on
large summarization datasets. We show that not only do humans overwhelmingly
prefer GPT-3 summaries, prompted using only a task description, but these also
do not suffer from common dataset-specific issues such as poor factuality.
Next, we study what this means for evaluation, particularly the role of gold
standard test sets. Our experiments show that both reference-based and
reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.
Finally, we evaluate models on a setting beyond generic summarization,
specifically keyword-based summarization, and show how dominant fine-tuning
approaches compare to prompting.
</p>
<p>To support further research, we release: (a) a corpus of 10K generated
summaries from fine-tuned and prompt-based models across 4 standard
summarization benchmarks, (b) 1K human preference judgments comparing different
systems for generic- and keyword-based summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07586">
<div class="article-summary-box-inner">
<span><p>Most weakly supervised named entity recognition (NER) models rely on
domain-specific dictionaries provided by experts. This approach is infeasible
in many domains where dictionaries do not exist. While a phrase retrieval model
was used to construct pseudo-dictionaries with entities retrieved from
Wikipedia automatically in a recent study, these dictionaries often have
limited coverage because the retriever is likely to retrieve popular entities
rather than rare ones. In this study, we present a novel framework, HighGEN,
that generates NER datasets with high-coverage pseudo-dictionaries.
Specifically, we create entity-rich dictionaries with a novel search method,
called phrase embedding search, which encourages the retriever to search a
space densely populated with various entities. In addition, we use a new
verification process based on the embedding distance between candidate entity
mentions and entity types to reduce the false-positive noise in weak labels
generated by high-coverage dictionaries. We demonstrate that HighGEN
outperforms the previous best model by an average F1 score of 4.7 across five
NER benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14389">
<div class="article-summary-box-inner">
<span><p>Research on Korean grammatical error correction (GEC) is limited, compared to
other major languages such as English. We attribute this problematic
circumstance to the lack of a carefully designed evaluation benchmark for
Korean GEC. In this work, we collect three datasets from different sources
(Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean
grammatical errors. Considering the nature of Korean grammar, We then define 14
error types for Korean and provide KAGAS (Korean Automatic Grammatical error
Annotation System), which can automatically annotate error types from parallel
corpora. We use KAGAS on our datasets to make an evaluation benchmark for
Korean, and present baseline models trained from our datasets. We show that the
model trained with our datasets significantly outperforms the currently used
statistical Korean GEC system (Hanspell) on a wider range of error types,
demonstrating the diversity and usefulness of the datasets. The implementations
and datasets are open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00375">
<div class="article-summary-box-inner">
<span><p>The gender of any voice user interface is a key element of its perceived
identity. Recently, there has been increasing interest in interfaces where the
gender is ambiguous rather than clearly identifying as female or male. This
work addresses the task of generating novel gender-ambiguous TTS voices in a
multi-speaker, multilingual setting. This is accomplished by efficiently
sampling from a latent speaker embedding space using a proposed gender-aware
method. Extensive objective and subjective evaluations clearly indicate that
this method is able to efficiently generate a range of novel, diverse voices
that are consistent and perceived as more gender-ambiguous than a baseline
voice across all the languages examined. Interestingly, the gender perception
is found to be robust across two demographic factors of the listeners: native
language and gender. To our knowledge, this is the first systematic and
validated approach that can reliably generate a variety of gender-ambiguous
voices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse scaling can become U-shaped. (arXiv:2211.02011v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02011">
<div class="article-summary-box-inner">
<span><p>Scaling up language models has been empirically shown to improve performance
on a wide range of downstream tasks. However, if we were to observe worse
performance as a function of scale ("inverse scaling") on certain tasks, this
would indicate that scaling can also encourage behaviors that are misaligned
with human preferences. The Inverse Scaling Prize (McKenzie et al. 2022)
identified eleven such inverse scaling tasks, evaluated on models of up to 280B
parameters and up to 500 zettaFLOPs of training compute. This paper takes a
closer look at these inverse scaling tasks. We evaluate models of up to 540B
parameters, trained on five times more compute than those evaluated in the
Inverse Scaling Prize. With this increased range of model sizes and training
compute, only four out of the eleven tasks remain inverse scaling. Six out of
the eleven tasks exhibit "U-shaped scaling", where performance decreases up to
a certain size, and then increases again up to the largest model evaluated (the
one remaining task displays positive scaling). In addition, we find that 1-shot
examples and chain-of-thought can help mitigate undesirable scaling patterns
even further. U-shaped scaling suggests that the inverse scaling trend observed
in McKenzie et al. (2022) may not continue to hold for larger models, which we
attribute to the presence of distractor tasks that only sufficiently large
models can avoid.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09710">
<div class="article-summary-box-inner">
<span><p>Midrash collections are complex rabbinic works that consist of text in
multiple languages, which evolved through long processes of unstable oral and
written transmission. Determining the origin of a given passage in such a
compilation is not always straightforward and is often a matter of dispute
among scholars, yet it is essential for scholars' understanding of the passage
and its relationship to other texts in the rabbinic corpus.
</p>
<p>To help solve this problem, we propose a system for classification of
rabbinic literature based on its style, leveraging recently released pretrained
Transformer models for Hebrew. Additionally, we demonstrate how our method can
be applied to uncover lost material from Midrash Tanhuma.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09761">
<div class="article-summary-box-inner">
<span><p>Transformers achieve unrivalled performance in modelling language, but remain
inefficient in terms of memory and time complexity. A possible remedy is to
reduce the sequence length in the intermediate layers by pooling fixed-length
segments of tokens. Nevertheless, natural units of meaning, such as words or
phrases, display varying sizes. To address this mismatch, we equip language
models with a dynamic-pooling mechanism, which predicts segment boundaries in
an autoregressive fashion. We compare several methods to infer boundaries,
including end-to-end learning through stochastic re-parameterisation,
supervised learning (based on segmentations from subword tokenizers or spikes
in conditional entropy), as well as linguistically motivated boundaries. We
perform character-level evaluation on texts from multiple datasets and
morphologically diverse languages. The results demonstrate that dynamic
pooling, which jointly segments and models language, is both faster and more
accurate than vanilla Transformers and fixed-length pooling within the same
computational budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11087">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models (LLMs) reflect the inherent social biases
of their training corpus. Many methods have been proposed to mitigate this
issue, but they often fail to debias or they sacrifice model accuracy. We use
conceptors--a soft projection method--to identify and remove the bias subspace
in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1)
bias subspace projection by post-processing; and (2) a new architecture,
conceptor-intervened BERT (CI-BERT), which explicitly incorporates the
conceptor projection into all layers during training. We find that conceptor
post-processing achieves state-of-the-art (SoTA) debiasing results while
maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is
robust in various scenarios and can mitigate intersectional bias efficiently by
its logical operation on the existing bias subspaces. Although CI-BERT's
training takes all layers' bias into account and can beat its post-processing
counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We
also show the importance of carefully constructing the bias subspace. The best
results are obtained by removing outliers from the list of biased words,
combining them (via the conceptor AND operation), and computing their
embeddings using the sentences from a cleaner corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11300">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning has proved to be a valuable component
for out-of-distribution (OoD) detection with only the texts of in-distribution
(ID) examples. These approaches either train a language model from scratch or
fine-tune a pre-trained language model using ID examples, and then take the
perplexity output by the language model as OoD scores. In this paper, we
analyze the complementary characteristics of both OoD detection methods and
propose a multi-level knowledge distillation approach that integrates their
strengths while mitigating their limitations. Specifically, we use a fine-tuned
model as the teacher to teach a randomly initialized student model on the ID
examples. Besides the prediction layer distillation, we present a
similarity-based intermediate layer distillation method to thoroughly explore
the representation space of the teacher model. In this way, the learned student
can better represent the ID data manifold while gaining a stronger ability to
map OoD examples outside the ID data manifold with the regularization inherited
from pre-training. Besides, the student model sees only ID examples during
parameter learning, further promoting more distinguishable features for OoD
detection. We conduct extensive experiments over multiple benchmark datasets,
i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the
proposed method yields new state-of-the-art performance. We also explore its
application as an AIGC detector to distinguish between answers generated by
ChatGPT and human experts. It is observed that our model exceeds human
evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13308">
<div class="article-summary-box-inner">
<span><p>Learned representations of scientific documents can serve as valuable input
features for downstream tasks, without the need for further fine-tuning.
However, existing benchmarks for evaluating these representations fail to
capture the diversity of relevant tasks. In response, we introduce SciRepEval,
the first comprehensive benchmark for training and evaluating scientific
document representations. It includes 25 challenging and realistic tasks, 11 of
which are new, across four formats: classification, regression, ranking and
search. We then use the benchmark to study and improve the generalization
ability of scientific document representation models. We show how
state-of-the-art models struggle to generalize across task formats, and that
simple multi-task training fails to improve them. However, a new approach that
learns multiple embeddings per document, each tailored to a different format,
can improve performance. We experiment with task-format-specific control codes
and adapters in a multi-task setting and find that they outperform the existing
single-embedding state-of-the-art by up to 1.5 points absolute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v6 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02908">
<div class="article-summary-box-inner">
<span><p>Autonomous cars are indispensable when humans go further down the hands-free
route. Although existing literature highlights that the acceptance of the
autonomous car will increase if it drives in a human-like manner, sparse
research offers the naturalistic experience from a passenger's seat perspective
to examine the humanness of current autonomous cars. The present study tested
whether the AI driver could create a human-like ride experience for passengers
based on 69 participants' feedback in a real-road scenario. We designed a ride
experience-based version of the non-verbal Turing test for automated driving.
Participants rode in autonomous cars (driven by either human or AI drivers) as
a passenger and judged whether the driver was human or AI. The AI driver failed
to pass our test because passengers detected the AI driver above chance. In
contrast, when the human driver drove the car, the passengers' judgement was
around chance. We further investigated how human passengers ascribe humanness
in our test. Based on Lewin's field theory, we advanced a computational model
combining signal detection theory with pre-trained language models to predict
passengers' humanness rating behaviour. We employed affective transition
between pre-study baseline emotions and corresponding post-stage emotions as
the signal strength of our model. Results showed that the passengers'
ascription of humanness would increase with the greater affective transition.
Our study suggested an important role of affective transition in passengers'
ascription of humanness, which might become a future direction for autonomous
driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages. (arXiv:2212.05409v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05409">
<div class="article-summary-box-inner">
<span><p>Building Natural Language Understanding (NLU) capabilities for Indic
languages, which have a collective speaker base of more than one billion
speakers is absolutely crucial. In this work, we aim to improve the NLU
capabilities of Indic languages by making contributions along 3 important axes
(i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on
Indic languages. Specifically, we curate the largest monolingual corpora,
IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a
2.3x increase over prior work, while supporting 12 additional languages. Next,
we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse
NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME
contains a total of 105 evaluation sets, of which 52 are new contributions to
the literature. To the best of our knowledge, this is the first effort towards
creating a standard benchmark for Indic languages that aims to test the
multilingual zero-shot capabilities of pretrained language models. Finally, we
train IndicBERT v2, a state-of-the-art model supporting all the languages.
Averaged across languages and tasks, the model achieves an absolute improvement
of 2 points over a strong baseline. The data and models are available at
https://github.com/AI4Bharat/IndicBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A fine-grained comparison of pragmatic language understanding in humans and language models. (arXiv:2212.06801v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06801">
<div class="article-summary-box-inner">
<span><p>Pragmatics and non-literal language understanding are essential to human
communication, and present a long-standing challenge for artificial language
models. We perform a fine-grained comparison of language models and humans on
seven pragmatic phenomena, using zero-shot prompting on an expert-curated set
of English materials. We ask whether models (1) select pragmatic
interpretations of speaker utterances, (2) make similar error patterns as
humans, and (3) use similar linguistic cues as humans to solve the tasks. We
find that the largest models achieve high accuracy and match human error
patterns: within incorrect responses, models favor literal interpretations over
heuristic-based distractors. We also find preliminary evidence that models and
humans are sensitive to similar linguistic cues. Our results suggest that
pragmatic behaviors can emerge in models without explicitly constructed
representations of mental states. However, models tend to struggle with
phenomena relying on social expectation violations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Text Generation via Probability Density Estimation in the Latent Space. (arXiv:2212.08307v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08307">
<div class="article-summary-box-inner">
<span><p>Previous work on controllable text generation has explored the idea of
control from the latent space, such as optimizing a representation with
attribute-related classifiers or sampling a representation from relevant
discrete samples. However, they are not effective enough in modeling both the
latent space and the control, leaving controlled text with low quality and
diversity. In this work, we propose a novel control framework using probability
density estimation in the latent space. Our method utilizes an invertible
transformation function, the Normalizing Flow, that maps the complex
distributions in the latent space to simple Gaussian distributions in the prior
space. Thus, we can perform sophisticated and flexible control in the prior
space and feed the control effects back into the latent space owing to the
one-one-mapping property of invertible transformations. Experiments on
single-attribute controls and multi-attribute control reveal that our method
outperforms several strong baselines on attribute relevance and text quality
and achieves the SOTA. Further analysis of control strength adjustment
demonstrates the flexibility of our control strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoder Tuning: Efficient Language Understanding as Decoding. (arXiv:2212.08408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08408">
<div class="article-summary-box-inner">
<span><p>With the evergrowing sizes of pre-trained models (PTMs), it has been an
emerging practice to only provide the inference APIs for users, namely
model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen,
most current approaches focus on the input side, seeking for powerful prompts
to stimulate models for correct answers. However, we argue that input-side
adaptation could be arduous due to the lack of gradient signals and they
usually require thousands of API queries, resulting in high computation and
time costs. In light of this, we present Decoder Tuning (DecT), which in
contrast optimizes task-specific decoder networks on the output side.
Specifically, DecT first extracts prompt-stimulated output scores for initial
predictions. On top of that, we train an additional decoder network on the
output representations to incorporate posterior data knowledge. By
gradient-based optimization, DecT can be trained within several seconds and
requires only one PTM query per sample. Empirically, we conduct extensive
natural language understanding experiments and show that DecT significantly
outperforms state-of-the-art algorithms with a $200\times$ speed-up.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Better Reasoners with Self-Verification. (arXiv:2212.09561v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09561">
<div class="article-summary-box-inner">
<span><p>Recently, with the chain of thought (CoT) prompting, large language models
(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural
language processing tasks such as arithmetic, commonsense, and logical
reasoning. However, LLMs with CoT require multi-step prompting and multi-token
prediction, which is highly sensitive to individual mistakes and vulnerable to
error accumulation. The above issues make the LLMs need the ability to verify
the answers. In fact, after inferring conclusions in some thinking decision
tasks, people often check them by re-verifying steps to avoid some mistakes. In
this paper, we propose and prove that LLMs also have similar self-verification
abilities. We take the conclusion obtained by CoT as one of the conditions for
solving the original problem. By taking turns masking the original conditions
and predicting their results, we calculate an explainable answer verification
score based on whether the re-predicted conditions are correct. Experimental
results demonstrate that the proposed method can improve the reasoning
performance on various arithmetic, commonsense, and logical reasoning datasets.
Our code is publicly available at:
https://github.com/WENGSYX/Self-Verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering. (arXiv:2212.09662v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09662">
<div class="article-summary-box-inner">
<span><p>Visual language data such as plots, charts, and infographics are ubiquitous
in the human world. However, state-of-the-art vision-language models do not
perform well on these data. We propose MatCha (Math reasoning and Chart
derendering pretraining) to enhance visual language models' capabilities in
jointly modeling charts/plots and language data. Specifically, we propose
several pretraining tasks that cover plot deconstruction and numerical
reasoning which are the key capabilities in visual language modeling.
</p>
<p>We perform the MatCha pretraining starting from Pix2Struct, a recently
proposed image-to-text visual language model. On standard benchmarks such as
PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as
much as nearly 20%. We also examine how well MatCha pretraining transfers to
domains such as screenshots, textbook diagrams, and document figures and
observe overall improvement, verifying the usefulness of MatCha pretraining on
broader visual language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context. (arXiv:2212.10007v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10007">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (LM) for code have achieved great success
in code completion, they generate code conditioned only on the contents within
the file, i.e., in-file context, but ignore the rich semantics in other files
within the same project, i.e., cross-file context, a critical source of
information that is especially useful in modern modular software development.
Such overlooking constrains code language models' capacity in code completion,
leading to unexpected behaviors such as generating hallucinated class member
functions or function calls with unexpected arguments. In this work, we develop
a cross-file context finder tool, CCFINDER, that effectively locates and
retrieves the most relevant cross-file context. We propose CoCoMIC, a framework
that incorporates cross-file context to learn the in-file and cross-file
context jointly on top of pretrained code LMs. CoCoMIC successfully improves
the existing code LM with a 33.94% relative increase in exact match and a
28.69% relative increase in identifier matching for code completion when the
cross-file context is provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis. (arXiv:2212.10356v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10356">
<div class="article-summary-box-inner">
<span><p>Length extrapolation permits training a transformer language model on short
sequences that preserves perplexities when tested on substantially longer
sequences. A relative positional embedding design, ALiBi, has had the widest
usage to date. We dissect ALiBi via the lens of receptive field analysis
empowered by a novel cumulative normalized gradient tool. The concept of
receptive field further allows us to modify the vanilla Sinusoidal positional
embedding to create ~\textbf{Sandwich}, the first parameter-free relative
positional embedding design that truly length information uses longer than the
training sequence. Sandwich shares with KERPLE and T5 the same logarithmic
decaying temporal bias pattern with learnable relative positional embeddings;
these elucidate future extrapolatable positional embedding design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. (arXiv:2212.10380v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10380">
<div class="article-summary-box-inner">
<span><p>Dual encoders are now the dominant architecture for dense retrieval. Yet, we
have little understanding of how they represent text, and why this leads to
good performance. In this work, we shed light on this question via
distributions over the vocabulary. We propose to interpret the vector
representations produced by dual encoders by projecting them into the model's
vocabulary space. We show that the resulting projections contain rich semantic
information, and draw connection between them and sparse retrieval. We find
that this view can offer an explanation for some of the failure cases of dense
retrievers. For example, we observe that the inability of models to handle tail
entities is correlated with a tendency of the token distributions to forget
some of the tokens of those entities. We leverage this insight and propose a
simple way to enrich query and passage representations with lexical information
at inference time, and show that this significantly improves performance
compared to the original model in zero-shot settings, and specifically on the
BEIR benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10465">
<div class="article-summary-box-inner">
<span><p>We present SODA: the first publicly available, million-scale high-quality
social dialogue dataset. In contrast to most existing crowdsourced, small-scale
dialogue corpora, we distill 1.5M socially-grounded dialogues from a large
language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by
contextualizing social commonsense knowledge from a knowledge graph (Atomic10x;
West et al., 2022). Human evaluation shows that dialogues in SODA are more
consistent, specific, and (surprisingly) natural than those in prior
human-authored datasets.
</p>
<p>Using SODA, we train COSMO: a generalizable conversation model that is
significantly more natural and consistent on unseen datasets than
best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).
Experiments reveal COSMO is sometimes even preferred to the original
human-written gold responses. Additionally, our results shed light on the
distinction between knowledge-enriched conversations and natural social
chitchats. We make our data, models, and code public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?. (arXiv:2212.10504v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10504">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) systems are mainly based on the
slot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down
into smaller, controllable units (i.e., slots) to fulfill a specific task. A
series of approaches based on this framework achieved remarkable success on
various TOD benchmarks. However, we argue that the current TOD benchmarks are
limited to surrogate real-world scenarios and that the current TOD models are
still a long way to cover the scenarios. In this position paper, we first
identify current status and limitations of SF-TOD systems. After that, we
explore the WebTOD framework, the alternative direction for building a scalable
TOD system when a web/mobile interface is available. In WebTOD, the dialogue
system learns how to understand the web/mobile interface that the human agent
interacts with, powered by a large-scale language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DePlot: One-shot visual language reasoning by plot-to-table translation. (arXiv:2212.10505v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10505">
<div class="article-summary-box-inner">
<span><p>Visual language such as charts and plots is ubiquitous in the human world.
Comprehending plots and charts requires strong reasoning skills. Prior
state-of-the-art (SOTA) models require at least tens of thousands of training
examples and their reasoning capabilities are still much limited, especially on
complex human-written queries. This paper presents the first one-shot solution
to visual language reasoning. We decompose the challenge of visual language
reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over
the translated text. The key in this method is a modality conversion module,
named as DePlot, which translates the image of a plot or chart to a linearized
table. The output of DePlot can then be directly used to prompt a pretrained
large language model (LLM), exploiting the few-shot reasoning capabilities of
LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing
unified task formats and metrics, and train DePlot end-to-end on this task.
DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
fashion. Compared with a SOTA model finetuned on more than &gt;28k data points,
DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over
finetuned SOTA on human-written queries from the task of chart QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards multi-document summarization in the open-domain. (arXiv:2212.10526v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10526">
<div class="article-summary-box-inner">
<span><p>Multi-document summarization (MDS) traditionally assumes a set of
topic-related documents are provided. However, this document set is often an
artifact of the dataset curation process; in practice, it is not necessarily
available and would need to be retrieved given an information need, i.e. a
question or topic statement. We study this more challenging "open-domain"
setting by formalizing the task and bootstrapping it using existing datasets,
retrievers and summarizers. Via extensive experimentation, we determine that:
(1) state-of-the-art summarizers suffer large reductions in performance when
applied to the open-domain, even when retrieval performance is high, (2)
additional training in the open-domain setting can reduce this sensitivity to
imperfect retrieval, and (3) summarizers are insensitive to the retrieval of
duplicate documents and the order of retrieved documents, but highly sensitive
to other errors, like the retrieval of irrelevant documents. Based on our
results, we provide practical guidelines to enable future work on open-domain
MDS, e.g. how to choose the number of retrieved documents to summarize. Our
results suggest that new methods for retrieval and summarization, as well as
annotated resources for training and evaluation, will be necessary for further
progress in the open-domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGeo: Multi-Modal Geographic Pre-Training Method. (arXiv:2301.04283v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04283">
<div class="article-summary-box-inner">
<span><p>As a core task in location-based services (LBS) (e.g., navigation maps),
query and point of interest (POI) matching connects users' intent with
real-world geographic information. Recently, pre-trained models (PTMs) have
made advancements in many natural language processing (NLP) tasks. Generic
text-based PTMs do not have enough geographic knowledge for query-POI matching.
To overcome this limitation, related literature attempts to employ
domain-adaptive pre-training based on geo-related corpus. However, a query
generally contains mentions of multiple geographic objects, such as nearby
roads and regions of interest (ROIs). The geographic context (GC), i.e., these
diverse geographic objects and their relationships, is therefore pivotal to
retrieving the most relevant POI. Single-modal PTMs can barely make use of the
important GC and therefore have limited performance. In this work, we propose a
novel query-POI matching method Multi-modal Geographic language model (MGeo),
which comprises a geographic encoder and a multi-modal interaction module. MGeo
represents GC as a new modality and is able to fully extract multi-modal
correlations for accurate query-POI matching. Besides, there is no publicly
available benchmark for this topic. In order to facilitate further research, we
build a new open-source large-scale benchmark Geographic TExtual Similarity
(GeoTES). The POIs come from an open-source geographic information system
(GIS). The queries are manually generated by annotators to prevent privacy
issues. Compared with several strong baselines, the extensive experiment
results and detailed ablation analyses on GeoTES demonstrate that our proposed
multi-modal pre-training method can significantly improve the query-POI
matching capability of generic PTMs, even when the queries' GC is not provided.
Our code and dataset are publicly available at
https://github.com/PhantomGrapes/MGeo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REPLUG: Retrieval-Augmented Black-Box Language Models. (arXiv:2301.12652v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12652">
<div class="article-summary-box-inner">
<span><p>We introduce REPLUG, a retrieval-augmented language modeling framework that
treats the language model (LM) as a black box and augments it with a tuneable
retrieval model. Unlike prior retrieval-augmented LMs that train language
models with special cross attention mechanisms to encode the retrieved text,
REPLUG simply prepends retrieved documents to the input for the frozen
black-box LM. This simple design can be easily applied to any existing
retrieval and language models. Furthermore, we show that the LM can be used to
supervise the retrieval model, which can then find documents that help the LM
make better predictions. Our experiments demonstrate that REPLUG with the tuned
retriever significantly improves the performance of GPT-3 (175B) on language
modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by
5.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.01973">
<div class="article-summary-box-inner">
<span><p>Current benchmarks for evaluating neural code models focus on only a small
subset of programming languages, excluding many popular languages such as Go or
Rust. To ameliorate this issue, we present the BabelCode framework for
execution-based evaluation of any benchmark in any language. BabelCode enables
new investigations into the qualitative performance of models' memory, runtime,
and individual test case results. Additionally, we present a new code
translation dataset called Translating Python Programming Puzzles (TP3) from
the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves
translating expert-level python functions to any language. With both BabelCode
and the TP3 benchmark, we investigate if balancing the distributions of 14
languages in a training dataset improves a large language model's performance
on low-resource languages. Training a model on a balanced corpus results in, on
average, 12.34% higher $pass@k$ across all tasks and languages compared to the
baseline. We find that this strategy achieves 66.48% better $pass@k$ on
low-resource languages at the cost of only a 12.94% decrease to high-resource
languages. In our three translation tasks, this strategy yields, on average,
30.77% better low-resource $pass@k$ while having 19.58% worse high-resource
$pass@k$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting. (arXiv:2302.04813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04813">
<div class="article-summary-box-inner">
<span><p>Recent work has addressed textual reasoning tasks by prompting large language
models with explanations via the chain-of-thought paradigm. However, subtly
different explanations can yield widely varying downstream task accuracy, so
explanations that have not been "tuned" for a task, such as off-the-shelf
explanations written by non-experts, may lead to mediocre performance. This
paper tackles the problem of how to optimize explanation-infused prompts in a
black-box fashion. We first generate sets of candidate explanations for each
example in the prompt using a leave-one-out scheme. We then use a two-stage
framework where we first evaluate explanations for each in-context example in
isolation according to two proxy metrics, log likelihood and accuracy on new
examples. Finally, we search over sets of explanations to find a set that
yields high performance against a silver-labeled development set. Across four
textual reasoning tasks spanning question answering, mathematical reasoning,
and natural language inference, results show that our proxy metrics correlate
with ground truth accuracy and our overall method can effectively improve
prompts over crowdworker annotations and naive search strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training. (arXiv:2302.09736v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09736">
<div class="article-summary-box-inner">
<span><p>Although large-scale video-language pre-training models, which usually build
a global alignment between the video and the text, have achieved remarkable
progress on various downstream tasks, the idea of adopting fine-grained
information during the pre-training stage is not well explored. In this work,
we propose STOA-VLP, a pre-training framework that jointly models object and
action information across spatial and temporal dimensions. More specifically,
the model regards object trajectories across frames and multiple action
features from the video as fine-grained features. Besides, We design two
auxiliary tasks to better incorporate both kinds of information into the
pre-training process of the video-language model. The first is the dynamic
object-text alignment task, which builds a better connection between object
trajectories and the relevant noun tokens. The second is the spatial-temporal
action set prediction, which guides the model to generate consistent action
features by predicting actions found in the text. Extensive experiments on
three downstream tasks (video captioning, text-video retrieval, and video
question answering) demonstrate the effectiveness of our proposed STOA-VLP
(e.g. 3.7 Rouge-L improvements on MSR-VTT video captioning benchmark, 2.9%
accuracy improvements on MSVD video question answering benchmark, compared to
previous approaches).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14225">
<div class="article-summary-box-inner">
<span><p>Masked Language Modeling (MLM) is widely used to pretrain language models.
The standard random masking strategy in MLM causes the pre-trained language
models (PLMs) to be biased toward high-frequency tokens. Representation
learning of rare tokens is poor and PLMs have limited performance on downstream
tasks. To alleviate this frequency bias issue, we propose two simple and
effective Weighted Sampling strategies for masking tokens based on the token
frequency and training loss. We apply these two strategies to BERT and obtain
Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity
benchmark (STS) show that WSBERT significantly improves sentence embeddings
over BERT. Combining WSBERT with calibration methods and prompt learning
further improves sentence embeddings. We also investigate fine-tuning WSBERT on
the GLUE benchmark and show that Weighted Sampling also improves the transfer
learning capability of the backbone PLM. We further analyze and provide
insights into how WSBERT improves token embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.01590">
<div class="article-summary-box-inner">
<span><p>This paper proposes a framework to formally link a fragment of an algebraic
language to a Graph Neural Network (GNN). It relies on Context Free Grammars
(CFG) to organise algebraic operations into generative rules that can be
translated into a GNN layer model. Since the rules and variables of a CFG
directly derived from a language contain redundancies, a grammar reduction
scheme is presented making tractable the translation into a GNN layer. Applying
this strategy, a grammar compliant with the third-order Weisfeiler-Lehman
(3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably
3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us
to provide algebraic formulas to count the cycles of length up to six and
chordal cycles at the edge level, which enlightens the counting power of 3-WL.
Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other
3-WL GNNs on many downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translate your gibberish: black-box adversarial attack on machine translation systems. (arXiv:2303.10974v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10974">
<div class="article-summary-box-inner">
<span><p>Neural networks are deployed widely in natural language processing tasks on
the industrial scale, and perhaps the most often they are used as compounds of
automatic machine translation systems. In this work, we present a simple
approach to fool state-of-the-art machine translation tools in the task of
translation from Russian to English and vice versa. Using a novel black-box
gradient-free tensor-based optimizer, we show that many online translation
tools, such as Google, DeepL, and Yandex, may both produce wrong or offensive
translations for nonsensical adversarial input queries and refuse to translate
seemingly benign input phrases. This vulnerability may interfere with
understanding a new language and simply worsen the user's experience while
using machine translation systems, and, hence, additional improvements of these
tools are required to establish better translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission. (arXiv:2303.15049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15049">
<div class="article-summary-box-inner">
<span><p>We present the InterviewBot that dynamically integrates conversation history
and customized topics into a coherent embedding space to conduct 10 mins
hybrid-domain (open and closed) conversations with foreign students applying to
U.S. colleges for assessing their academic and cultural readiness. To build a
neural-based end-to-end dialogue model, 7,361 audio recordings of
human-to-human interviews are automatically transcribed, where 440 are manually
corrected for finetuning and evaluation. To overcome the input/output size
limit of a transformer-based encoder-decoder model, two new methods are
proposed, context attention and topic storing, allowing the model to make
relevant and consistent interactions. Our final model is tested both
statistically by comparing its responses to the interview data and dynamically
by inviting professional interviewers and various students to interact with it
in real-time, finding it highly satisfactory in fluency and context awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16634">
<div class="article-summary-box-inner">
<span><p>The quality of texts generated by natural language generation (NLG) systems
is hard to measure automatically. Conventional reference-based metrics, such as
BLEU and ROUGE, have been shown to have relatively low correlation with human
judgments, especially for tasks that require creativity and diversity. Recent
studies suggest using large language models (LLMs) as reference-free metrics
for NLG evaluation, which have the benefit of being applicable to new tasks
that lack human references. However, these LLM-based evaluators still have
lower human correspondence than medium-size neural evaluators. In this work, we
present G-Eval, a framework of using large language models with
chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of
NLG outputs. We experiment with two generation tasks, text summarization and
dialogue generation. We show that G-Eval with GPT-4 as the backbone model
achieves a Spearman correlation of 0.514 with human on summarization task,
outperforming all previous methods by a large margin. We also propose
preliminary analysis on the behavior of LLM-based evaluators, and highlight the
potential issue of LLM-based evaluators having a bias towards the LLM-generated
texts. The code is at https://github.com/nlpyang/geval
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01196">
<div class="article-summary-box-inner">
<span><p>Chat models, such as ChatGPT, have shown impressive capabilities and have
been rapidly adopted across numerous domains. However, these models are only
accessible through a restricted API, creating barriers for new research and
progress in the field. We propose a pipeline that can automatically generate a
high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a
conversation with itself. Subsequently, we employ parameter-efficient tuning to
enhance LLaMA, an open-source large language model. The resulting model, named
Baize, demonstrates good performance in multi-turn dialogues with guardrails
that minimize potential risks. Furthermore, we propose a new technique called
Self-Distill with Feedback, to further improve the performance of the Baize
models with feedback from ChatGPT. The Baize models and data are released for
research purposes only at https://github.com/project-baize/baize-chatbot. An
online demo is also available at
https://huggingface.co/spaces/project-baize/chat-with-baize.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. (arXiv:2304.09842v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09842">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved remarkable progress in solving
various natural language processing tasks due to emergent reasoning abilities.
However, LLMs have inherent limitations as they are incapable of accessing
up-to-date information (stored on the Web or in task-specific knowledge bases),
using external tools, and performing precise mathematical and logical
reasoning. In this paper, we present Chameleon, an AI system that mitigates
these limitations by augmenting LLMs with plug-and-play modules for
compositional reasoning. Chameleon synthesizes programs by composing various
tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python
functions, and heuristic-based modules) for accomplishing complex reasoning
tasks. At the heart of Chameleon is an LLM-based planner that assembles a
sequence of tools to execute to generate the final response. We showcase the
effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning
tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%
overall accuracy on ScienceQA, improving the best published few-shot result by
11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,
lifting the state of the art to 98.78%. Our analysis also shows that the
GPT-4-powered planner exhibits more consistent and rational tool selection via
inferring potential constraints from instructions, compared to a
ChatGPT-powered planner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Does ChatGPT Fall Short in Providing Truthful Answers?. (arXiv:2304.10513v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10513">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in aspects like truthfulness, e.g.
providing accurate and reliable outputs. Therefore, in this paper, we seek to
understand why ChatGPT falls short in providing truthful answers. For this
purpose, we first analyze the failures of ChatGPT in complex open-domain
question answering and identifies the abilities under the failures.
Specifically, we categorize ChatGPT's failures into four types: comprehension,
factualness, specificity, and inference. We further pinpoint three critical
abilities associated with QA failures: knowledge memorization, knowledge
recall, and knowledge reasoning. Additionally, we conduct experiments centered
on these abilities and propose potential approaches to enhance truthfulness.
The results indicate that furnishing the model with fine-grained external
knowledge, hints for knowledge recall, and guidance for reasoning can empower
the model to answer questions more truthfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14402">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with instruction fine-tuning demonstrate
superior generative capabilities. However, these models are resource-intensive.
To alleviate this issue, we explore distilling knowledge from instruction-tuned
LLMs into much smaller ones. To this end, we carefully develop a large set of
2.58M instructions based on both existing and newly-generated instructions. In
addition to being sizable, we design our instructions to cover a broad set of
topics to ensure diversity. Extensive analysis of our instruction dataset
confirms its diversity, and we generate responses for these instructions using
gpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of
models, collectively referred to as LaMini-LM, which includes models from both
the encoder-decoder and decoder-only families, with varying sizes. We evaluate
the performance of our models using automatic metrics on 15 different natural
language processing (NLP) benchmarks, as well as through human assessment. The
results demonstrate that our proposed LaMini-LM models are comparable to
competitive baselines, while being nearly 10 times smaller in size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04476">
<div class="article-summary-box-inner">
<span><p>The speech-to-singing (STS) voice conversion task aims to generate singing
samples corresponding to speech recordings while facing a major challenge: the
alignment between the target (singing) pitch contour and the source (speech)
content is difficult to learn in a text-free situation. This paper proposes
AlignSTS, an STS model based on explicit cross-modal alignment, which views
speech variance such as pitch and content as different modalities. Inspired by
the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1)
adopts a novel rhythm adaptor to predict the target rhythm representation to
bridge the modality gap between content and pitch, where the rhythm
representation is computed in a simple yet effective way and is quantized into
a discrete space; and 2) uses the predicted rhythm representation to re-align
the content based on cross-attention and conducts a cross-modal fusion for
re-synthesize. Extensive experiments show that AlignSTS achieves superior
performance in terms of both objective and subjective metrics. Audio samples
are available at https://alignsts.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06575">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown surprisingly good performance in
multilingual neural machine translation (MNMT) even when trained without
parallel data. Yet, despite the fact that the amount of training data is
gigantic, they still struggle with translating rare words, particularly for
low-resource languages. Even worse, it is usually unrealistic to retrieve
relevant demonstrations for in-context learning with low-resource languages on
LLMs, which restricts the practical use of LLMs for translation -- how should
we mitigate this problem? To this end, we present a novel method, CoD, which
augments LLMs with prior knowledge with the chains of multilingual dictionaries
for a subset of input words to elicit translation abilities for LLMs. Extensive
experiments indicate that augmenting ChatGPT with CoD elicits large gains by up
to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in
Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the
importance of chaining the multilingual dictionaries, as well as the
superiority of CoD to few-shot demonstration for low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06626">
<div class="article-summary-box-inner">
<span><p>Though majority vote among annotators is typically used for ground truth
labels in natural language processing, annotator disagreement in tasks such as
hate speech detection may reflect differences in opinion across groups, not
noise. Thus, a crucial problem in hate speech detection is determining whether
a statement is offensive to the demographic group that it targets, when that
group may constitute a small fraction of the annotator pool. We construct a
model that predicts individual annotator ratings on potentially offensive text
and combines this information with the predicted target group of the text to
model the opinions of target group members. We show gains across a range of
metrics, including raising performance over the baseline by 22% at predicting
individual annotators' ratings and by 33% at predicting variance among
annotators, which provides a metric for model uncertainty downstream. We find
that annotator ratings can be predicted using their demographic information and
opinions on online content, without the need to track identifying annotator IDs
that link each annotator to their ratings. We also find that use of
non-invasive survey questions on annotators' online experiences helps to
maximize privacy and minimize unnecessary collection of demographic information
when predicting annotators' opinions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better speech synthesis through scaling. (arXiv:2305.07243v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07243">
<div class="article-summary-box-inner">
<span><p>In recent years, the field of image generation has been revolutionized by the
application of autoregressive transformers and DDPMs. These approaches model
the process of image generation as a step-wise probabilistic processes and
leverage large amounts of compute and data to learn the image distribution.
This methodology of improving performance need not be confined to images. This
paper describes a way to apply advances in the image generative domain to
speech synthesis. The result is TorToise -- an expressive, multi-voice
text-to-speech system.
</p>
<p>All model code and trained weights have been open-sourced at
https://github.com/neonbjb/tortoise-tts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10429">
<div class="article-summary-box-inner">
<span><p>The mixture proportions of pretraining data domains (e.g., Wikipedia, books,
web text) greatly affect language model (LM) performance. In this paper, we
propose Domain Reweighting with Minimax Optimization (DoReMi), which first
trains a small proxy model using group distributionally robust optimization
(Group DRO) over domains to produce domain weights (mixture proportions)
without knowledge of downstream tasks. We then resample a dataset with these
domain weights and train a larger, full-sized model. In our experiments, we use
DoReMi on a 280M-parameter proxy model to find domain weights for training an
8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves
perplexity across all domains, even when it downweights a domain. DoReMi
improves average few-shot downstream accuracy by 6.5% points over a baseline
model trained using The Pile's default domain weights and reaches the baseline
accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has
no knowledge of downstream tasks, even matches the performance of using domain
weights tuned on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10998">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) encode a large amount of world knowledge.
However, as such knowledge is frozen at the time of model training, the models
become static and limited by the training data at that time. In order to
further improve the capacity of LLMs for knowledge-intensive tasks, we consider
augmenting LLMs with the large-scale web using search engine. Unlike previous
augmentation sources (e.g., Wikipedia data dump), the web provides broader,
more comprehensive and constantly updated information. In this paper, we
present a web-augmented LLM UNIWEB, which is trained over 16
knowledge-intensive tasks in a unified text-to-text format. Instead of simply
using the retrieved contents from web, our approach has made two major
improvements. Firstly, we propose an adaptive search engine assisted learning
method that can self-evaluate the confidence level of LLM's predictions, and
adaptively determine when to refer to the web for more data, which can avoid
useless or noisy augmentation from web. Secondly, we design a pretraining task,
i.e., continual knowledge learning, based on salient spans prediction, to
reduce the discrepancy between the encoded and retrieved knowledge. Experiments
on a wide range of knowledge-intensive tasks show that our model significantly
outperforms previous retrieval-augmented methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11169">
<div class="article-summary-box-inner">
<span><p>We present evidence that language models can learn meaning despite being
trained only to perform next token prediction on text, specifically a corpus of
programs. Each program is preceded by a specification in the form of (textual)
input-output examples. Working with programs enables us to precisely define
concepts relevant to meaning in language (e.g., correctness and semantics),
making program synthesis well-suited as an intermediate testbed for
characterizing the presence (or absence) of meaning in language models.
</p>
<p>We first train a Transformer model on the corpus of programs, then probe the
trained model's hidden states as it completes a program given a specification.
Despite providing no inductive bias toward learning the semantics of the
language, we find that a linear probe is able to extract abstractions of both
current and future program states from the model states. Moreover, there is a
strong, statistically significant correlation between the accuracy of the probe
and the model's ability to generate a program that implements the
specification. To evaluate whether the semantics are represented in the model
states rather than learned by the probe, we design a novel experimental
procedure that intervenes on the semantics of the language while preserving the
lexicon and syntax. We also demonstrate that the model learns to generate
correct programs that are, on average, shorter than those in the training set,
which is evidence that language model outputs may differ from the training
distribution in semantically meaningful ways. In summary, this paper does not
propose any new techniques for training language models, but develops an
experimental framework for and provides insights into the acquisition and
representation of (formal) meaning in language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11326">
<div class="article-summary-box-inner">
<span><p>Tabular data is the most common format to publish and exchange structured
data online. A clear example is the growing number of open data portals
published by all types of public administrations. However, exploitation of
these data sources is currently limited to technical people able to
programmatically manipulate and digest such data. As an alternative, we propose
the use of chatbots to offer a conversational interface to facilitate the
exploration of tabular data sources. With our approach, any regular citizen can
benefit and leverage them. Moreover, our chatbots are not manually created:
instead, they are automatically generated from the data source itself thanks to
the instantiation of a configurable collection of conversation patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11490">
<div class="article-summary-box-inner">
<span><p>Building on the recent remarkable development of large language models
(LLMs), active attempts are being made to extend the utility of LLMs to
multimodal tasks. There have been previous efforts to link language and visual
information, and attempts to add visual capabilities to LLMs are ongoing as
well. However, existing attempts use LLMs only as image decoders and no attempt
has been made to generate images in the same line as the natural language. By
adopting a VQ-GAN framework in which latent representations of images are
treated as a kind of text tokens, we present a novel method to fine-tune a
pre-trained LLM to read and generate images like text without any structural
changes, extra training objectives, or the need for training an ad-hoc network
while still preserving the of the instruction-following capability of the LLM.
We apply this framework to chest X-ray (CXR) image and report generation tasks
as it is a domain in which translation of complex information between visual
and language domains is important. The code is available at
https://github.com/hyn2028/llm-cxr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages. (arXiv:2305.11938v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11938">
<div class="article-summary-box-inner">
<span><p>Data scarcity is a crucial issue for the development of highly multilingual
NLP systems. Yet for many under-represented languages (ULs) -- languages for
which NLP re-search is particularly far behind in meeting user needs -- it is
feasible to annotate small amounts of data. Motivated by this, we propose
XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather
than zero-shot; its focus on user-centric tasks -- tasks with broad adoption by
speakers of high-resource languages; and its focus on under-represented
languages where this scarce-data scenario tends to be most realistic. XTREME-UP
evaluates the capabilities of language models across 88 under-represented
languages over 9 key user-centric technologies including ASR, OCR, MT, and
information access tasks that are of general utility. We create new datasets
for OCR, autocomplete, semantic parsing, and transliteration, and build on and
refine existing datasets for other tasks. XTREME-UP provides methodology for
evaluating many modeling scenarios including text-only, multi-modal (vision,
audio, and text),supervised parameter tuning, and in-context learning. We
evaluate commonly used models on the benchmark. We release all code and scripts
to train and evaluate models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12524">
<div class="article-summary-box-inner">
<span><p>The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in
solving fundamental math problems like GSM8K by achieving over 90% accuracy.
However, their capabilities to solve more challenging math problems which
require domain-specific knowledge (i.e. theorem) have yet to be investigated.
In this paper, we introduce TheoremQA, the first theorem-driven
question-answering dataset designed to evaluate AI models' capabilities to
apply theorems to solve challenging science problems. TheoremQA is curated by
domain experts containing 800 high-quality questions covering 350 theorems
(e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem,
Elasticity Theorem, etc) from Math, Physics, EE&amp;CS, and Finance. We evaluate a
wide spectrum of 16 large language and code models with different prompting
strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that
GPT-4's capabilities to solve these problems are unparalleled, achieving an
accuracy of 51% with Program-of-Thoughts Prompting. All the existing
open-sourced models are below 15%, barely surpassing the random-guess baseline.
Given the diversity and broad coverage of TheoremQA, we believe it can be used
as a better benchmark to evaluate LLMs' capabilities to solve challenging
science problems. The data and code are released in
https://github.com/wenhuchen/TheoremQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering. (arXiv:2305.12820v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12820">
<div class="article-summary-box-inner">
<span><p>Recent advances in tabular question answering (QA) with large language models
are constrained in their coverage and only answer questions over a single
table. However, real-world queries are complex in nature, often over multiple
tables in a relational database or web page. Single table questions do not
involve common table operations such as set operations, Cartesian products
(joins), or nested queries. Furthermore, multi-table operations often result in
a tabular output, which necessitates table generation capabilities of tabular
QA models. To fill this gap, we propose a new task of answering questions over
multiple tables. Our model, MultiTabQA, not only answers questions over
multiple tables, but also generalizes to generate tabular answers. To enable
effective training, we build a pre-training dataset comprising of 132,645 SQL
queries and tabular answers. Further, we evaluate the generated tables by
introducing table-specific metrics of varying strictness assessing various
levels of granularity of the table structure. MultiTabQA outperforms
state-of-the-art single table QA models adapted to a multi-table QA setting by
finetuning on three datasets: Spider, Atis and GeoQuery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model. (arXiv:2305.13014v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13014">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as powerful generative Artificial
Intelligence solutions which can be applied to several fields and areas of
work. This paper presents results and reflection of an experiment done to use
the model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic
Analysis. Previous research on this subject has largely worked on conducting
deductive analysis. Thematic Analysis is a qualitative method for analysis
commonly used in social sciences and it is based on interpretations made by the
human analyst(s) and the identification of explicit and latent meanings in
qualitative data. Attempting an analysis based on human interpretation with an
LLM clearly is a provocation but also a way to learn something about how these
systems can or cannot be used in qualitative research. The paper presents the
motivations for attempting this emulation, it reflects on how the six steps to
a Thematic Analysis proposed by Braun and Clarke can at least partially be
reproduced with the LLM and it also reflects on what are the outputs produced
by the model. The paper used two existing datasets of open access
semi-structured interviews, previously analysed with Thematic Analysis by other
researchers. It used the previously produced analysis (and the related themes)
to compare with the results produced by the LLM. The results show that the
model can infer at least partially some of the main Themes. The objective of
the paper is not to replace human analysts in qualitative analysis but to learn
if some elements of LLM data manipulation can to an extent be of support for
qualitative research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web. (arXiv:2305.13117v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13117">
<div class="article-summary-box-inner">
<span><p>Existing datasets for automated fact-checking have substantial limitations,
such as relying on artificial claims, lacking annotations for evidence and
intermediate reasoning, or including evidence published after the claim. In
this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims
covering fact-checks by 50 different organizations. Each claim is annotated
with question-answer pairs supported by evidence available online, as well as
textual justifications explaining how the evidence combines to produce a
verdict. Through a multi-round annotation process, we avoid common pitfalls
including context dependence, evidence insufficiency, and temporal leakage, and
reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We
develop a baseline as well as an evaluation scheme for verifying claims through
several question-answering steps against the open web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13300">
<div class="article-summary-box-inner">
<span><p>By providing external information to large language models (LLMs), tool
augmentation (including retrieval augmentation) has emerged as a promising
solution for addressing the limitations of LLMs' static parametric memory.
However, how receptive are LLMs to such external evidence, especially when the
evidence conflicts with their parametric memory? We present the first
comprehensive and controlled investigation into the behavior of LLMs when
encountering knowledge conflicts. We propose a systematic framework to elicit
high-quality parametric memory from LLMs and construct the corresponding
counter-memory, which enables us to conduct a series of controlled experiments.
Our investigation reveals seemingly contradicting behaviors of LLMs. On the one
hand, different from prior wisdom, we find that LLMs can be highly receptive to
external evidence even when that conflicts with their parametric memory, given
that the external evidence is coherent and convincing. On the other hand, LLMs
also demonstrate a strong confirmation bias when the external evidence contains
some information that is consistent with their parametric memory, despite being
presented with conflicting evidence at the same time. These results pose
important implications that are worth careful consideration for the further
development and deployment of tool- and retrieval-augmented LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v2 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13484">
<div class="article-summary-box-inner">
<span><p>In the rapidly evolving field of deep learning, the performance of model
inference has become a pivotal aspect as models become more complex and are
deployed in diverse applications. Among these, autoregressive models stand out
due to their state-of-the-art performance in numerous generative tasks. These
models, by design, harness a temporal dependency structure, where the current
token's probability distribution is conditioned on preceding tokens. This
inherently sequential characteristic, however, adheres to the Markov Chain
assumption and lacks temporal parallelism, which poses unique challenges.
Particularly in industrial contexts where inference requests, following a
Poisson time distribution, necessitate diverse response lengths, this absence
of parallelism is more profound. Existing solutions, such as dynamic batching
and concurrent model instances, nevertheless, come with severe overheads and a
lack of flexibility, these coarse-grained methods fall short of achieving
optimal latency and throughput. To address these shortcomings, we propose
Flavor -- a temporal fusion framework for efficient inference in autoregressive
models, eliminating the need for heuristic settings and applies to a wide range
of inference scenarios. By providing more fine-grained parallelism on the
temporality of requests and employing an efficient memory shuffle algorithm,
Flover achieves up to 11x faster inference on GPT models compared to the
cutting-edge solutions provided by NVIDIA Triton FasterTransformer. Crucially,
by leveraging the advanced tensor parallel technique, Flover proves efficacious
across diverse computational landscapes, from single-GPU setups to multi-node
scenarios, thereby offering robust performance optimization that transcends
hardware boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13507">
<div class="article-summary-box-inner">
<span><p>Misinformation, i.e. factually incorrect information, is often conveyed in
multiple modalities, e.g. an image accompanied by a caption. It is perceived as
more credible by humans, and spreads faster and wider than its text-only
counterparts. While an increasing body of research investigates automated
fact-checking (AFC), previous surveys mostly focus on textual misinformation.
In this survey, we conceptualise a framework for AFC including subtasks unique
to multimodal misinformation. Furthermore, we discuss related terminological
developed in different communities in the context of our framework. We focus on
four modalities prevalent in real-world fact-checking: text, image, audio, and
video. We survey benchmarks and models, and discuss limitations and promising
directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13582">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models have enabled significant
advancements in cross-lingual transfer. However, these models often exhibit a
performance disparity when transferring from high-resource languages to
low-resource languages, especially for languages that are underrepresented or
not in the pre-training data. Motivated by the superior performance of these
models on high-resource languages compared to low-resource languages, we
introduce a Translation-and-fusion framework, which translates low-resource
language text into a high-resource language for annotation using fully
supervised models before fusing the annotations back into the low-resource
language. Based on this framework, we present TransFusion, a model trained to
fuse predictions from a high-resource language to make robust predictions on
low-resource languages. We evaluate our methods on two low-resource named
entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25
languages, and show consistent improvement up to +16 F$_1$ over English
fine-tuning systems, achieving state-of-the-art performance compared to
Translate-train systems. Our analysis depicts the unique advantages of the
TransFusion method which is robust to translation errors and source language
prediction errors, and complimentary to adapted multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IdEALS: Idiomatic Expressions for Advancement of Language Skills. (arXiv:2305.13637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13637">
<div class="article-summary-box-inner">
<span><p>Although significant progress has been made in developing methods for
Grammatical Error Correction (GEC), addressing word choice improvements has
been notably lacking and enhancing sentence expressivity by replacing phrases
with advanced expressions is an understudied aspect. In this paper, we focus on
this area and present our investigation into the task of incorporating the
usage of idiomatic expressions in student writing. To facilitate our study, we
curate extensive training sets and expert-annotated testing sets using
real-world data and evaluate various approaches and compare their performance
against human experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13718">
<div class="article-summary-box-inner">
<span><p>Existing efforts to improve logical reasoning ability of language models have
predominantly relied on supervised fine-tuning, hindering generalization to new
domains and/or tasks. The development of Large Langauge Models (LLMs) has
demonstrated the capacity of compressing abundant knowledge into a single
proxy, enabling them to tackle multiple tasks effectively. Our preliminary
experiments, nevertheless, show that LLMs do not show capability on logical
reasoning. The performance of LLMs on logical reasoning benchmarks is far
behind the existing state-of-the-art baselines. In this paper, we make the
first attempt to investigate the feasibility of incorporating logical knowledge
through self-supervised post-training, and activating it via in-context
learning, which we termed as LogicLLM. Specifically, we devise an
auto-regressive objective variant of MERIt and integrate it with two LLM
series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to
13 billion. The results on two challenging logical reasoning benchmarks
demonstrate the effectiveness of LogicLLM. Besides, we conduct extensive
ablation studies to analyze the key factors in designing logic-oriented proxy
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13971">
<div class="article-summary-box-inner">
<span><p>LLMs have shown impressive few-shot performance across many tasks. However,
they still struggle when it comes to reliably generating complex output
structures, such as those required for information extraction. This limitation
stems from the fact that LLMs, without fine-tuning, tend to generate free text
rather than structures precisely following a specific grammar. In this work, we
propose to enrich the decoding with formal grammar constraints. More
concretely, given Context-Free Grammar(CFG), our framework ensures that the
token generated in each decoding step would lead to a valid continuation
compliant with the grammar production rules. This process guarantees the
generation of valid sequences. Importantly, our framework can be readily
combined with any CFG or decoding algorithm. We demonstrate that the outputs of
many NLP tasks can be represented as formal languages, making them suitable for
direct use in our framework. We conducted experiments with two challenging
tasks involving large alphabets in their grammar (Wikidata entities and
relations): information extraction and entity disambiguation. Our results with
LLaMA models indicate that grammar-constrained decoding substantially
outperforms unconstrained decoding and even competes with task-specific
fine-tuned models. These findings suggest that integrating grammar-based
constraints during decoding holds great promise in making LLMs reliably produce
structured outputs, especially in setting where training data is scarce and
fine-tuning is expensive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-stop Training of Multiple Capacity Models. (arXiv:2305.14066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14066">
<div class="article-summary-box-inner">
<span><p>Training models with varying capacities can be advantageous for deploying
them in different scenarios. While high-capacity models offer better
performance, low-capacity models require fewer computing resources for training
and inference. In this work, we propose a novel one-stop training framework to
jointly train high-capacity and low-capactiy models. This framework consists of
two composite model architectures and a joint training algorithm called
Two-Stage Joint-Training (TSJT). Unlike knowledge distillation, where multiple
capacity models are trained from scratch separately, our approach integrates
supervisions from different capacity models simultaneously, leading to faster
and more efficient convergence. Extensive experiments on the multilingual
machine translation benchmark WMT10 show that our method outperforms
low-capacity baseline models and achieves comparable or better performance on
high-capacity models. Notably, the analysis demonstrates that our method
significantly influences the initial training process, leading to more
efficient convergence and superior solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Acceptability Judgements. (arXiv:2305.14091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14091">
<div class="article-summary-box-inner">
<span><p>Years have passed since the NLP community has last focused on linguistic
acceptability. In this work, we revisit this topic in the context of large
language models. We introduce CoLAC - Corpus of Linguistic Acceptability in
Chinese, the first large-scale non-English acceptability dataset that is
verified by native speakers and comes with two sets of labels. Our experiments
show that even the largest InstructGPT model performs only at chance level on
CoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised
models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer
experiments and fine-grained linguistic analysis, we demonstrate for the first
time that knowledge of linguistic acceptability can be transferred across
typologically distinct languages, as well as be traced back to pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14195">
<div class="article-summary-box-inner">
<span><p>While large pre-trained language models (LMs) find greater use across NLP,
existing evaluation protocols do not consider how LM language use aligns with
particular human demographic groups, which can be an important consideration in
conversational AI applications. To remedy this gap, we consider how LM language
skills can be measured and compared to human sub-populations. We suggest
clinical techniques from Speech Language Pathology, which has well-established
norms for acquisition of language skills, organized by (human) age. We conduct
evaluation with a domain expert (i.e., a clinically licensed speech language
pathologist), and also propose automated techniques to substitute clinical
evaluation at scale. We find LM capability varies widely depending on task with
GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring
inference about word meanings and simultaneously outperforming a typical 21
year old at memorization. GPT-3.5 (InstructGPT) also has trouble with social
language use, exhibiting less than 50\% of the tested pragmatic skills. It
shows errors in understanding particular word parts-of-speech and associative
word relations, among other lexical features. Ultimately, findings reiterate
the importance of considering demographic alignment and conversational goals
when using these models as public-facing tools. Our framework will be publicly
available via code, data, and a python package.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models. (arXiv:2305.14323v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14323">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) have achieved excellent performance in
a variety of evaluation benchmarks, they still struggle in complex reasoning
tasks which require specific knowledge and multi-hop reasoning. To improve the
reasoning abilities, we propose \textbf{ChatCoT}, a tool-augmented
chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model
the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize
tools in a more natural way through chatting. At each turn, LLMs can either
interact with tools or perform the reasoning. Our approach can effectively
leverage the multi-turn conversation ability of chat-based LLMs, and integrate
the thought chain following and tools manipulation in a unified way. Specially,
we initialize the early turns of the conversation by the tools, tasks and
reasoning format, and propose an iterative \emph{tool-augmented reasoning} step
to perform step-by-step tool-augmented reasoning. The experiment results on two
complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of
ChatCoT on complex reasoning tasks, achieving a 6.8\% relative improvement over
the state-of-the-art baseline. Our code and data are available at:
\url{https://github.com/RUCAIBOX/ChatCoT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor Prediction: Automatic Refinement of Internet Links. (arXiv:2305.14337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14337">
<div class="article-summary-box-inner">
<span><p>Internet links enable users to deepen their understanding of a topic by
providing convenient access to related information. However, the majority of
links are unanchored -- they link to a target webpage as a whole, and readers
may expend considerable effort localizing the specific parts of the target
webpage that enrich their understanding of the link's source context. To help
readers effectively find information in linked webpages, we introduce the task
of anchor prediction, where the goal is to identify the specific part of the
linked target webpage that is most related to the source linking context. We
release the AuthorAnchors dataset, a collection of 34K naturally-occurring
anchored links, which reflect relevance judgments by the authors of the source
article. To model reader relevance judgments, we annotate and release
ReaderAnchors, an evaluation set of anchors that readers find useful. Our
analysis shows that effective anchor prediction often requires jointly
reasoning over lengthy source and target webpages to determine their implicit
relations and identify parts of the target webpage that are related but not
redundant. We benchmark a performant T5-based ranking approach to establish
baseline performance on the task, finding ample room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10626">
<div class="article-summary-box-inner">
<span><p>While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical elastic
weight consolidation (EWC) for selective weight updates, combined with low-rank
adapters (LoRA) for training efficiency. Extensive experiments show our
approach substantially improves base LMs on 18 downstream tasks by 64.28% on
average. In particular, the small LMs (1.3B and 6B) enhanced by our approach
match or even outperform much larger LMs (e.g., ChatGPT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning of Medical Concepts Embedding using BEHRT. (arXiv:2305.13052v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13052">
<div class="article-summary-box-inner">
<span><p>Electronic Health Records (EHR) data contains medical records such as
diagnoses, medications, procedures, and treatments of patients. This data is
often considered sensitive medical information. Therefore, the EHR data from
the medical centers often cannot be shared, making it difficult to create
prediction models using multi-center EHR data, which is essential for such
models' robustness and generalizability. Federated Learning (FL) is an
algorithmic approach that allows learning a shared model using data in multiple
locations without the need to store all data in a central place. An example of
a prediction model's task is to predict future diseases. More specifically, the
model needs to predict patient's next visit diagnoses, based on current and
previous clinical data. Such a prediction model can support care providers in
making clinical decisions and even provide preventive treatment. We propose a
federated learning approach for learning medical concepts embedding. This
pre-trained model can be used for fine-tuning for specific downstream tasks.
Our approach is based on an embedding model like BEHRT, a deep neural sequence
transduction model for EHR. We train using federated learning, both the Masked
Language Modeling (MLM) and the next visit downstream model. We demonstrate our
approach on the MIMIC-IV dataset. We compare the performance of a model trained
with FL against a model trained on centralized data. We find that our federated
learning approach reaches very close to the performance of a centralized model,
and it outperforms local models in terms of average precision. We also show
that pre-trained MLM improves the model's average precision performance in the
next visit prediction task, compared to an MLM model without pre-training. Our
code is available at https://github.com/nadavlab/FederatedBEHRT.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-25 23:11:20.717695735 UTC">2023-05-25 23:11:20 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>