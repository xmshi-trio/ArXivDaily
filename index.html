<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-23T01:30:00Z">05-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Judgments of research co-created by generative AI: experimental evidence. (arXiv:2305.11873v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11873">
<div class="article-summary-box-inner">
<span><p>The introduction of ChatGPT has fuelled a public debate on the use of
generative AI (large language models; LLMs), including its use by researchers.
In the current work, we test whether delegating parts of the research process
to LLMs leads people to distrust and devalue researchers and scientific output.
Participants (N=402) considered a researcher who delegates elements of the
research process to a PhD student or LLM, and rated (1) moral acceptability,
(2) trust in the scientist to oversee future projects, and (3) the accuracy and
quality of the output. People judged delegating to an LLM as less acceptable
than delegating to a human (d = -0.78). Delegation to an LLM also decreased
trust to oversee future research projects (d = -0.80), and people thought the
results would be less accurate and of lower quality (d = -0.85). We discuss how
this devaluation might transfer into the underreporting of generative AI use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks. (arXiv:2305.11916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11916">
<div class="article-summary-box-inner">
<span><p>Computational complexity and overthinking problems have become the
bottlenecks for pre-training language models (PLMs) with millions or even
trillions of parameters. A Flexible-Patience-Based Early Exiting method
(F-PABEE) has been proposed to alleviate the problems mentioned above for
single-label classification (SLC) and multi-label classification (MLC) tasks.
F-PABEE makes predictions at the classifier and will exit early if predicted
distributions of cross-layer are consecutively similar. It is more flexible
than the previous state-of-the-art (SOTA) early exiting method PABEE because it
can simultaneously adjust the similarity score thresholds and the patience
parameters. Extensive experiments show that: (1) F-PABEE makes a better
speedup-accuracy balance than existing early exiting strategies on both SLC and
MLC tasks. (2) F-PABEE achieves faster inference and better performances on
different PLMs such as BERT and ALBERT. (3) F-PABEE-JSKD performs best for
F-PABEE with different similarity measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting. (arXiv:2305.11926v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11926">
<div class="article-summary-box-inner">
<span><p>We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech
(TTS) synthesis model that can produce high-quality speech. Benefiting from a
modularized training paradigm exploiting self-supervised speech
representations, MParrotTTS adapts to a new language with minimal supervised
data and generalizes to languages not seen while training the self-supervised
backbone. Moreover, without training on any bilingual or parallel examples,
MParrotTTS can transfer voices across languages while preserving the
speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using
a French speaker's voice and accent. We present extensive results on six
languages in terms of speech naturalness and speaker similarity in parallel and
cross-lingual synthesis. The proposed model outperforms the state-of-the-art
multilingual TTS models and baselines, using only a small fraction of
supervised training data. Speech samples from our model can be found at
https://paper2438.github.io/tts/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages. (arXiv:2305.11938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11938">
<div class="article-summary-box-inner">
<span><p>Data scarcity is a crucial issue for the development of highly multilingual
NLP systems. Yet for many under-represented languages (ULs) -- languages for
which NLP re-search is particularly far behind in meeting user needs -- it is
feasible to annotate small amounts of data. Motivated by this, we propose
XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather
than zero-shot; its focus on user-centric tasks -- tasks with broad adoption by
speakers of high-resource languages; and its focus on under-represented
languages where this scarce-data scenario tends to be most realistic. XTREME-UP
evaluates the capabilities of language models across 88 under-represented
languages over 9 key user-centric technologies including ASR, OCR, MT, and
information access tasks that are of general utility. We create new datasets
for OCR, autocomplete, semantic parsing, and transliteration, and build on and
refine existing datasets for other tasks. XTREME-UP provides methodology for
evaluating many modeling scenarios including text-only, multi-modal (vision,
audio, and text),supervised parameter tuning, and in-context learning. We
evaluate commonly used models on the benchmark. We release all code and scripts
to train and evaluate models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Viability of Synthetic Query Generation for Relevance Prediction. (arXiv:2305.11944v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11944">
<div class="article-summary-box-inner">
<span><p>Query-document relevance prediction is a critical problem in Information
Retrieval systems. This problem has increasingly been tackled using
(pretrained) transformer-based models which are finetuned using large
collections of labeled data. However, in specialized domains such as e-commerce
and healthcare, the viability of this approach is limited by the dearth of
large in-domain data. To address this paucity, recent methods leverage these
powerful models to generate high-quality task and domain-specific synthetic
data. Prior work has largely explored synthetic data generation or query
generation (QGen) for Question-Answering (QA) and binary (yes/no) relevance
prediction, where for instance, the QGen models are given a document, and
trained to generate a query relevant to that document. However in many
problems, we have a more fine-grained notion of relevance than a simple yes/no
label. Thus, in this work, we conduct a detailed study into how QGen approaches
can be leveraged for nuanced relevance prediction. We demonstrate that --
contrary to claims from prior works -- current QGen approaches fall short of
the more conventional cross-domain transfer-learning approaches. Via empirical
studies spanning 3 public e-commerce benchmarks, we identify new shortcomings
of existing QGen approaches -- including their inability to distinguish between
different grades of relevance. To address this, we introduce label-conditioned
QGen models which incorporates knowledge about the different relevance. While
our experiments demonstrate that these modifications help improve performance
of QGen techniques, we also find that QGen approaches struggle to capture the
full nuance of the relevance label space and as a result the generated queries
are not faithful to the desired relevance label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye-SpatialNet: Spatial Information Extraction from Ophthalmology Notes. (arXiv:2305.11948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11948">
<div class="article-summary-box-inner">
<span><p>We introduce an annotated corpus of 600 ophthalmology notes labeled with
detailed spatial and contextual information of ophthalmic entities. We extend
our previously proposed frame semantics-based spatial representation schema,
Rad-SpatialNet, to represent spatial language in ophthalmology text, resulting
in the Eye-SpatialNet schema. The spatially-grounded entities are findings,
procedures, and drugs. To accurately capture all spatial details, we add some
domain-specific elements in Eye-SpatialNet. The annotated corpus contains 1715
spatial triggers, 7308 findings, 2424 anatomies, and 9914 descriptors. To
automatically extract the spatial information, we employ a two-turn question
answering approach based on the transformer language model BERT. The results
are promising, with F1 scores of 89.31, 74.86, and 88.47 for spatial triggers,
Figure, and Ground frame elements, respectively. This is the first work to
represent and extract a wide variety of clinical information in ophthalmology.
Extracting detailed information can benefit ophthalmology applications and
research targeted toward disease progression and screening.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-QA: Unsupervised Knowledge Guided Language Model Alignment. (arXiv:2305.11952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11952">
<div class="article-summary-box-inner">
<span><p>Large-scale language models like ChatGPT and GPT-4 have gained attention for
their impressive conversational and generative capabilities. However, the
creation of supervised paired question-answering data for instruction tuning
presents formidable challenges. This endeavor necessitates substantial human
effort for data annotation and wrestles with issues concerning data quality,
diversity, accuracy, and other related factors. To overcome these obstacles, we
introduce an innovative framework named Self-QA, which replaces the traditional
practice of human-written instruction seeds with a vast amount of unsupervised
knowledge, enabling the model to generate a larger quantity of correct and
domain-specific instruction data. The effectiveness of our proposed method is
demonstrated through experiments conducted on unsupervised corpora from various
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weak Supervision Approach for Few-Shot Aspect Based Sentiment. (arXiv:2305.11979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11979">
<div class="article-summary-box-inner">
<span><p>We explore how weak supervision on abundant unlabeled data can be leveraged
to improve few-shot performance in aspect-based sentiment analysis (ABSA)
tasks. We propose a pipeline approach to construct a noisy ABSA dataset, and we
use it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. We
test the resulting model on three widely used ABSA datasets, before and after
fine-tuning. Our proposed method preserves the full fine-tuning performance
while showing significant improvements (15.84% absolute F1) in the few-shot
learning scenario for the harder tasks. In zero-shot (i.e., without
fine-tuning), our method outperforms the previous state of the art on the
aspect extraction sentiment classification (AESC) task and is, additionally,
capable of performing the harder aspect sentiment triplet extraction (ASTE)
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of medium-large Language Models at zero-shot closed book generative question answering. (arXiv:2305.11991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11991">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have garnered significant attention, but the
definition of "large" lacks clarity. This paper focuses on medium-sized
lan-guage models (MLMs), defined as having at least six billion parameters but
less than 100 billion. The study evaluates MLMs regarding zero-shot genera-tive
question answering, which requires models to provide elaborate answers without
external document retrieval. The paper introduces an own test da-taset and
presents results from human evaluation. Results show that combin-ing the best
answers from different MLMs yielded an overall correct answer rate of 82.7%
which is better than the 60.9% of ChatGPT. The best MLM achieved 46.4% and has
7B parameters, which highlights the importance of using appropriate training
data for fine-tuning rather than solely relying on the number of parameters.
More fine-grained feedback should be used to further improve the quality of
answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis. (arXiv:2305.11993v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11993">
<div class="article-summary-box-inner">
<span><p>We propose using automatically generated natural language definitions of
contextualised word usages as interpretable word and word sense
representations. Given a collection of usage examples for a target word, and
the corresponding data-driven usage clusters (i.e., word senses), a definition
is generated for each usage with a specialised Flan-T5 language model, and the
most prototypical definition in a usage cluster is chosen as the sense label.
</p>
<p>We demonstrate how the resulting sense labels can make existing approaches to
semantic change analysis more interpretable, and how they can allow users --
historical linguists, lexicographers, or social scientists -- to explore and
intuitively explain diachronic trajectories of word meaning. Semantic change
analysis is only one of many possible applications of the `definitions as
representations' paradigm. Beyond being human-readable, contextualised
definitions also outperform token or usage sentence embeddings in
word-in-context semantic similarity judgements, making them a new promising
type of lexical representation for NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Approaches to Lexical Simplification: A Survey. (arXiv:2305.12000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12000">
<div class="article-summary-box-inner">
<span><p>Lexical Simplification (LS) is the task of replacing complex for simpler
words in a sentence whilst preserving the sentence's original meaning. LS is
the lexical component of Text Simplification (TS) with the aim of making texts
more accessible to various target populations. A past survey (Paetzold and
Specia, 2017) has provided a detailed overview of LS. Since this survey,
however, the AI/NLP community has been taken by storm by recent advances in
deep learning, particularly with the introduction of large language models
(LLM) and prompt learning. The high performance of these models sparked renewed
interest in LS. To reflect these recent advances, we present a comprehensive
survey of papers published between 2017 and 2023 on LS and its sub-tasks with a
special focus on deep learning. We also present benchmark datasets for the
future development of LS systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models. (arXiv:2305.12001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12001">
<div class="article-summary-box-inner">
<span><p>In this paper, we conduct a thorough investigation into the reasoning
capabilities of Large Language Models (LLMs), focusing specifically on the Open
Pretrained Transformers (OPT) models as a representative of such models. Our
study entails finetuning three different sizes of OPT on a carefully curated
reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned
without explanations, and OPT-RE, finetuned with explanations. We then evaluate
all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS
benchmark, covering 26 distinct reasoning skills, utilizing three prompting
techniques. Through a comprehensive grid of 27 configurations and 6,156 test
evaluations, we investigate the dimensions of finetuning, prompting, and scale
to understand the role of explanations on different reasoning skills. Our
findings reveal that having explanations in the fewshot exemplar has no
significant impact on the model's performance when the model is finetuned,
while positively affecting the non-finetuned counterpart. Moreover, we observe
a slight yet consistent increase in classification accuracy as we incorporate
explanations during prompting and finetuning, respectively. Finally, we offer
insights on which skills benefit the most from incorporating explanations
during finetuning and prompting, such as Numerical (+20.4%) and Analogical
(+13.9%) reasoning, as well as skills that exhibit negligible or negative
effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters. (arXiv:2305.12002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12002">
<div class="article-summary-box-inner">
<span><p>In recent years, pre-trained language models have undergone rapid development
with the emergence of large-scale models. However, there is a lack of
open-sourced chat models specifically designed for the Chinese language,
especially in the field of Chinese finance, at the scale of hundreds of
billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese
chat model to date, built upon the BLOOM-176B architecture. Additionally, we
propose a novel training method called hybrid-tuning to mitigate catastrophic
forgetting. By combining general-domain with domain-specific knowledge and
integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable
of providing accurate and contextually appropriate responses in the Chinese
financial domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases. (arXiv:2305.12018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12018">
<div class="article-summary-box-inner">
<span><p>Energy-based models (EBMs) have gained popularity for controlled text
generation due to their high applicability to a wide range of constraints.
However, sampling from EBMs is non-trivial, as it often requires a large number
of iterations to converge to plausible text, which slows down the decoding
process and makes it less practical for real-world applications. In this work,
we propose BOLT, which relies on tunable biases to directly adjust the language
model's output logits. Unlike prior work, BOLT maintains the generator's
autoregressive nature to assert a strong control on token-wise conditional
dependencies and overall fluency, and thus converges faster. When compared with
state-of-the-arts on controlled generation tasks using both soft constraints
(e.g., sentiment control) and hard constraints (e.g., keyword-guided topic
control), BOLT demonstrates significantly improved efficiency and fluency. On
sentiment control, BOLT is 7x faster than competitive baselines, and more
fluent in 74.4% of the evaluation samples according to human judges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12027">
<div class="article-summary-box-inner">
<span><p>Entity linking methods based on dense retrieval are an efficient and widely
used solution in large-scale applications, but they fall short of the
performance of generative models, as they are sensitive to the structure of the
embedding space. In order to address this issue, this paper introduces DUCK, an
approach to infusing structural information in the space of entity
representations, using prior knowledge of entity types. Inspired by duck typing
in programming languages, we propose to define the type of an entity based on
the relations that it has with other entities in a knowledge graph. Then,
porting the concept of box embeddings to spherical polar coordinates, we
propose to represent relations as boxes on the hypersphere. We optimize the
model to cluster entities of similar type by placing them inside the boxes
corresponding to their relations. Our experiments show that our method sets new
state-of-the-art results on standard entity-disambiguation benchmarks, it
improves the performance of the model by up to 7.9 F1 points, outperforms other
type-aware approaches, and matches the results of generative models with 18
times more parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12029">
<div class="article-summary-box-inner">
<span><p>Current disfluency detection models focus on individual utterances each from
a single speaker. However, numerous discontinuity phenomena in spoken
conversational transcripts occur across multiple turns, hampering human
readability and the performance of downstream NLP tasks. This study addresses
these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken
conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We
design a data labeling schema to collect the high-quality dataset and provide
extensive data analysis. Furthermore, we leverage two modeling approaches for
experimental evaluation as benchmarks for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12031">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) present immense potential in the medical field,
yet concerns over data privacy, regulatory compliance, and model stability
restrict their widespread adoption. Although the distillation of
high-performing closed-source LLMs has proven effective for general tasks,
their application in healthcare is limited due to reduced domain knowledge and
remnants of alignment behavior hindering clinical tasks. To address these
challenges, we propose Dialogue-Based Knowledge Encoding (DBKE). DBKE enhances
models' implicit knowledge base and primes them for conversational recall,
augmenting their conversational capabilities and enabling a soft alignment for
subsequent use cases. By transforming dense academic source text into synthetic
dialogue, DBKE broadens the model's knowledge base and enables a soft alignment
that guides downstream behaviours. We present Clinical Camel, an open-source,
healthcare-focused conversational model, to showcase the effectiveness of DBKE.
Clinical Camel outperforms GPT-3.5 on the United States Medical Licensing
Examination (USMLE) Step 1 and Step 3 with scores of 53.2 % and 58.2 %,
respectively, compared to GPT-3.5's scores of 36.1 % and 55.7 %. Clinical Camel
adeptly handles multi-stage clinical case problems, provides adaptive
counseling, and generates clinical notes. However, it is prone to
hallucinations, which pose a significant obstacle in safety-critical settings.
The performance of Clinical Camel underscores the importance of continued
research and development of open-source models for the safe and effective
integration of LLMs in healthcare settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12057">
<div class="article-summary-box-inner">
<span><p>We propose extending the Sequence-level Knowledge Distillation (Kim and Rush,
2016) with n-best reranking to consider not only the top-1 hypotheses but also
the top n-best hypotheses of teacher models. Our approach leverages a diverse
set of models, including publicly-available large pretrained models, to provide
more accurate pseudo-labels for training student models. We validate our
proposal on the WMT21 German-English translation task and demonstrate that our
student model achieves comparable accuracy to a large translation model with
4.7 billion parameters from (Tran et al., 2021) while having two orders of
magnitude fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining. (arXiv:2305.12074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12074">
<div class="article-summary-box-inner">
<span><p>Many text mining models are constructed by fine-tuning a large deep
pre-trained language model (PLM) in downstream tasks. However, a significant
challenge is maintaining performance when we use a lightweight model with
limited labeled samples. We present DisCo, a semi-supervised learning (SSL)
framework for fine-tuning a cohort of small student models generated from a
large PLM using knowledge distillation. Our key insight is to share
complementary knowledge among distilled student cohorts to promote their SSL
effectiveness. DisCo employs a novel co-training technique to optimize multiple
small student models by promoting knowledge sharing among students under
diversified views: model views produced by different distillation strategies
and data views produced by various input augmentations. We evaluate DisCo on
both semi-supervised text classification and extractive summarization tasks.
Experimental results show that DisCo can produce student models that are 7.6
times smaller and 4.8 times faster in inference than the baseline PLMs while
maintaining comparable performance. We also show that DisCo-generated student
models outperform the similar-sized models elaborately tuned in distinct tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer. (arXiv:2305.12077v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12077">
<div class="article-summary-box-inner">
<span><p>In real-world scenarios, labeled samples for dialogue summarization are
usually limited (i.e., few-shot) due to high annotation costs for high-quality
dialogue summaries. To efficiently learn from few-shot samples, previous works
have utilized massive annotated data from other downstream tasks and then
performed prompt transfer in prompt tuning so as to enable cross-task knowledge
transfer. However, existing general-purpose prompt transfer techniques lack
consideration for dialogue-specific information. In this paper, we focus on
improving the prompt transfer from dialogue state tracking to dialogue
summarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which
leverages skeleton generation as extra supervision that functions as a medium
connecting the distinct source and target task and resulting in the model's
better consumption of dialogue state information. To automatically extract
dialogue skeletons as supervised training data for skeleton generation, we
design a novel approach with perturbation-based probes requiring neither
annotation effort nor domain knowledge. Training the model on such skeletons
can also help preserve model capability during prompt transfer. Our method
significantly outperforms existing baselines. In-depth analyses demonstrate the
effectiveness of our method in facilitating cross-task knowledge transfer in
few-shot dialogue summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Entropy Rate Constancy in Text. (arXiv:2305.12084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12084">
<div class="article-summary-box-inner">
<span><p>The uniform information density (UID) hypothesis states that humans tend to
distribute information roughly evenly across an utterance or discourse. Early
evidence in support of the UID hypothesis came from Genzel &amp; Charniak (2002),
which proposed an entropy rate constancy principle based on the probability of
English text under n-gram language models. We re-evaluate the claims of Genzel
&amp; Charniak (2002) with neural language models, failing to find clear evidence
in support of entropy rate constancy. We conduct a range of experiments across
datasets, model sizes, and languages and discuss implications for the uniform
information density hypothesis and linguistic theories of efficient
communication more broadly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix Propagation: Parameter-Efficient Tuning for Long Sequences. (arXiv:2305.12086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12086">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient tuning aims to mitigate the large memory requirements of
adapting pretrained language models for downstream tasks. For example, one
popular method, prefix-tuning, prepends trainable tokens to sequences while
freezing the rest of the model's parameters. Although such models attain
comparable performance with fine-tuning when applied to sequences with short to
moderate lengths, we show their inferior performance when modelling long
sequences. To bridge this gap, we propose prefix-propagation, a simple but
effective approach that conditions prefixes on previous hidden states. We
empirically demonstrate that prefix-propagation outperforms prefix-tuning
across long-document tasks, while using 50% fewer parameters. To further
investigate the proposed architecture, we also show its advantage in
calibration, and perform additional study on its relationship with kernel
attention. To the best of our knowledge, this work is the first to focus on
parameter-efficient learning for long-sequence language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UP5: Unbiased Foundation Model for Fairness-aware Recommendation. (arXiv:2305.12090v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12090">
<div class="article-summary-box-inner">
<span><p>Recent advancements in foundation models such as large language models (LLM)
have propelled them to the forefront of recommender systems (RS). Moreover,
fairness in RS is critical since many users apply it for decision-making and
demand fulfillment. However, at present, there is a lack of understanding
regarding the level of fairness exhibited by recommendation foundation models
and the appropriate methods for equitably treating different groups of users in
foundation models. In this paper, we focus on user-side unfairness problem and
show through a thorough examination that there is unfairness involved in LLMs
that lead to unfair recommendation results. To eliminate bias from LLM for
fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5)
foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP
includes two sub-modules: a personalized prefix prompt that enhances fairness
with respect to individual sensitive attributes, and a Prompt Mixture that
integrates multiple counterfactually-fair prompts for a set of sensitive
attributes. Experiments are conducted on two real-world datasets, MovieLens-1M
and Insurance, and results are compared with both matching-based and
sequential-based fairness-aware recommendation models. The results show that
UP5 achieves better recommendation performance and meanwhile exhibits a high
level of fairness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"What do others think?": Task-Oriented Conversational Modeling with Subjective Knowledge. (arXiv:2305.12091v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12091">
<div class="article-summary-box-inner">
<span><p>Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that
assist users in accomplishing specific goals, such as booking a hotel or a
restaurant. Traditional TODs rely on domain-specific APIs/DBs or external
factual knowledge to generate responses, which cannot accommodate subjective
user requests (e.g., "Is the WIFI reliable?" or "Does the restaurant have a
good atmosphere?"). To address this issue, we propose a novel task of
subjective-knowledge-based TOD (SK-TOD). We also propose the first
corresponding dataset, which contains subjective knowledge-seeking dialogue
contexts and manually annotated responses grounded in subjective knowledge
sources. When evaluated with existing TOD approaches, we find that this task
poses new challenges such as aggregating diverse opinions from multiple
knowledge snippets. We hope this task and dataset can promote further research
on TOD and subjective content understanding. The code and the dataset are
available at https://github.com/alexa/dstc11-track5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain. (arXiv:2305.12092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12092">
<div class="article-summary-box-inner">
<span><p>The increasing number of benchmarks for Natural Language Processing (NLP)
tasks in the computational job market domain highlights the demand for methods
that can handle job-related tasks such as skill extraction, skill
classification, job title classification, and de-identification. While some
approaches have been developed that are specific to the job market domain,
there is a lack of generalized, multilingual models and benchmarks for these
tasks. In this study, we introduce a language model called ESCOXLM-R, based on
XLM-R, which uses domain-adaptive pre-training on the European Skills,
Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27
languages. The pre-training objectives for ESCOXLM-R include dynamic masked
language modeling and a novel additional objective for inducing multilingual
taxonomical ESCO relations. We comprehensively evaluate the performance of
ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and
find that it achieves state-of-the-art results on 6 out of 9 datasets. Our
analysis reveals that ESCOXLM-R performs better on short spans and outperforms
XLM-R on entity-level and surface-level span-F1, likely due to ESCO containing
short skill and occupation titles, and encoding information on the
entity-level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?. (arXiv:2305.12096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12096">
<div class="article-summary-box-inner">
<span><p>Pre-training on large corpora of text enables the language models to acquire
a vast amount of factual and commonsense knowledge which allows them to achieve
remarkable performance on a variety of language understanding tasks. They
typically acquire this knowledge by learning from the pre-training text and
capturing certain patterns from it. However, real-world settings often present
scenarios that do not abide by these patterns i.e. scenarios that break the
common assumptions. Can state-of-the-art NLP models correctly reason over the
contexts of such scenarios?
</p>
<p>Addressing the above question, in this paper, we investigate the ability of
models to correctly reason over contexts that break the common assumptions. To
this end, we first systematically create evaluation data in which each data
instance consists of (a) a common assumption, (b) a context that follows the
assumption, (c) a context that breaks the assumption, and (d) questions based
on the contexts. Then, through evaluations on multiple models including GPT-3
and Flan T5, we show that while doing fairly well on contexts that follow the
common assumptions, the models struggle to correctly reason over contexts that
break those assumptions. Specifically, the performance gap is as high as 20%
absolute points. Furthermore, we thoroughly analyze these results revealing
several interesting findings. We believe our work and findings will encourage
and facilitate further research in developing more robust models that can also
reliably reason over contexts that break the common assumptions. Data is
available at \url{https://github.com/nrjvarshney/break_the_common_assumptions}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EE-TTS: Emphatic Expressive TTS with Linguistic Information. (arXiv:2305.12107v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12107">
<div class="article-summary-box-inner">
<span><p>While Current TTS systems perform well in synthesizing high-quality speech,
producing highly expressive speech remains a challenge. Emphasis, as a critical
factor in determining the expressiveness of speech, has attracted more
attention nowadays. Previous works usually enhance the emphasis by adding
intermediate features, but they can not guarantee the overall expressiveness of
the speech. To resolve this matter, we propose Emphatic Expressive TTS
(EE-TTS), which leverages multi-level linguistic information from syntax and
semantics. EE-TTS contains an emphasis predictor that can identify appropriate
emphasis positions from text and a conditioned acoustic model to synthesize
expressive speech with emphasis and linguistic information. Experimental
results indicate that EE-TTS outperforms baseline with MOS improvements of 0.49
and 0.67 in expressiveness and naturalness. EE-TTS also shows strong
generalization across different datasets according to AB test results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization. (arXiv:2305.12123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12123">
<div class="article-summary-box-inner">
<span><p>Models trained with empirical risk minimization (ERM) are revealed to easily
rely on spurious correlations, resulting in poor generalization. Group
distributionally robust optimization (group DRO) can alleviate this problem by
minimizing the worst-case loss over pre-defined groups. While promising, in
practice factors like expensive annotations and privacy preclude the
availability of group labels. More crucially, when taking a closer look at the
failure modes of out-of-distribution generalization, the typical procedure of
reweighting in group DRO loses efficiency. Hinged on the limitations, in this
work, we reformulate the group DRO framework by proposing Q-Diversity.
Characterized by an interactive training mode, Q-Diversity relaxes the group
identification from annotation into direct parameterization. Furthermore, a
novel mixing strategy across groups is presented to diversify the
under-represented groups. In a series of experiments on both synthetic and
real-world text classification tasks, results demonstrate that Q-Diversity can
consistently improve worst-case accuracy under different distributional shifts,
outperforming state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifting the Curse of Capacity Gap in Distilling Language Models. (arXiv:2305.12129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12129">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (LMs) have shown compelling performance on various
downstream tasks, but unfortunately they require a tremendous amount of
inference compute. Knowledge distillation finds a path to compress LMs to small
ones with a teacher-student paradigm. However, when the capacity gap between
the teacher and the student is large, a curse of capacity gap appears, invoking
a deficiency in distilling LMs. While a few studies have been carried out to
fill the gap, the curse is not yet well tackled. In this paper, we aim at
lifting the curse of capacity gap via enlarging the capacity of the student
without notably increasing the inference compute. Largely motivated by sparse
activation regime of mixture of experts (MoE), we propose a mixture of minimal
experts (MiniMoE), which imposes extra parameters to the student but introduces
almost no additional inference compute. Experimental results on GLUE and CoNLL
demonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a
large extent. MiniMoE also achieves the state-of-the-art performance at small
FLOPs compared with a range of competitive baselines. With a compression rate
as much as $\sim$50$\times$, MiniMoE preserves $\sim$95\% GLUE score of the
teacher.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hedges in Bidirectional Translations of Publicity-Oriented Documents. (arXiv:2305.12146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12146">
<div class="article-summary-box-inner">
<span><p>Hedges are widely studied across registers and disciplines, yet research on
the translation of hedges in political texts is extremely limited. This
contrastive study is dedicated to investigating whether there is a diachronic
change in the frequencies of hedging devices in the target texts, to what
extent the changing frequencies of translated hedges through years are
attributed to the source texts, and what translation strategies are adopted to
deal with them. For the purposes of this research, two types of official
political texts and their translations from China and the United Nations were
collected to form three sub-corpora. Results show that hedges tend to appear
more frequently in English political texts, be it original English or
translated English. In addition, directionality seems to play an important role
in influencing both the frequencies and translation strategies regarding the
use of hedges. A noticeable diachronic increase of hedging devices is also
observed in our corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12147">
<div class="article-summary-box-inner">
<span><p>Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive
chain-of-thought reasoning ability. Recent work on self-instruction tuning,
such as Alpaca, has focused on enhancing the general proficiency of models.
These instructions enable the model to achieve performance comparable to
GPT-3.5 on general tasks like open-domain text generation and paraphrasing.
However, they fall short of helping the model handle complex reasoning tasks.
To bridge the gap, this paper presents LogiCoT, a new instruction-tuning
dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the
process of harvesting instructions for prompting GPT-4 to generate
chain-of-thought rationales. LogiCoT serves as an instruction set for teaching
models of logical reasoning and elicits general reasoning skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-visiting Automated Topic Model Evaluation with Large Language Models. (arXiv:2305.12152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12152">
<div class="article-summary-box-inner">
<span><p>Topic models are used to make sense of large text collections. However,
automatically evaluating topic model output and determining the optimal number
of topics both have been longstanding challenges, with no effective automated
solutions to date. This paper proposes using large language models to evaluate
such output. We find that large language models appropriately assess the
resulting topics, correlating more strongly with human judgments than existing
automated metrics. We then investigate whether we can use large language models
to automatically determine the optimal number of topics. We automatically
assign labels to documents and choosing configurations with the most pure
labels returns reasonable values for the optimal number of topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Compose Syntactic and Semantic Representations Appropriately for Compositional Generalization. (arXiv:2305.12169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12169">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that sequence-to-sequence (Seq2Seq) models are
limited in solving the compositional generalization (CG) tasks, failing to
systematically generalize to unseen compositions of seen components. There is
mounting evidence that one of the reasons hindering CG is the representation of
the encoder uppermost layer is entangled. In other words, the syntactic and
semantic representations of sequences are twisted inappropriately. However,
most previous studies mainly concentrate on enhancing semantic information at
token-level, rather than composing the syntactic and semantic representations
of sequences appropriately as humans do. In addition, we consider the
representation entanglement problem they found is not comprehensive, and
further hypothesize that source keys and values representations passing into
different decoder layers are also entangled. Staring from this intuition and
inspired by humans' strategies for CG, we propose COMPSITION (Compose Syntactic
and Semantic Representations), an extension to Seq2Seq models to learn to
compose representations of different encoder layers appropriately for
generating different keys and values passing into different decoder layers
through introducing a composed layer between the encoder and decoder.
COMPSITION achieves competitive and even state-of-the-art results on two
realistic benchmarks, which empirically demonstrates the effectiveness of our
proposal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages. (arXiv:2305.12182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12182">
<div class="article-summary-box-inner">
<span><p>The NLP community has mainly focused on scaling Large Language Models (LLMs)
vertically, i.e., making them better for about 100 languages. We instead scale
LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM
that covers 511 languages, almost all of them low-resource. An important part
of this effort is to collect and clean Glot500-c, a corpus that covers these
511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five
diverse tasks across these languages. We observe large improvements for both
high-resource and lowresource languages compared to an XLM-R baseline. Our
analysis shows that no single factor explains the quality of multilingual LLM
representations. Rather, a combination of factors determines quality including
corpus size, script, "help" from related languages and the total capacity of
the model. Our work addresses an important goal of NLP research: we should not
limit NLP to a small fraction of the world's languages and instead strive to
support as many languages as possible to bring the benefits of NLP technology
to all languages and cultures. Code, data and models are available at
https://github.com/cisnlp/Glot500.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paragraph-level Citation Recommendation based on Topic Sentences as Queries. (arXiv:2305.12190v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12190">
<div class="article-summary-box-inner">
<span><p>Citation recommendation (CR) models may help authors find relevant articles
at various stages of the paper writing process. Most research has dealt with
either global CR, which produces general recommendations suitable for the
initial writing stage, or local CR, which produces specific recommendations
more fitting for the final writing stages. We propose the task of
paragraph-level CR as a middle ground between the two approaches, where the
paragraph's topic sentence is taken as input and recommendations for citing
within the paragraph are produced at the output. We propose a model for this
task, fine-tune it using the quadruplet loss on the dataset of ACL papers, and
show improvements over the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs. (arXiv:2305.12191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12191">
<div class="article-summary-box-inner">
<span><p>A major concern in using deep learning based generative models for
document-grounded dialogs is the potential generation of responses that are not
\textit{faithful} to the underlying document. Existing automated metrics used
for evaluating the faithfulness of response with respect to the grounding
document measure the degree of similarity between the generated response and
the document's content. However, these automated metrics are far from being
well aligned with human judgments. Therefore, to improve the measurement of
faithfulness, we propose a new metric that utilizes (Conditional) Point-wise
Mutual Information (PMI) between the generated response and the source
document, conditioned on the dialogue. PMI quantifies the extent to which the
document influences the generated response -- with a higher PMI indicating a
more faithful response. We build upon this idea to create a new decoding
technique that incorporates PMI into the response generation process to predict
more faithful responses. Our experiments on the BEGIN benchmark demonstrate an
improved correlation of our metric with human evaluation. We also show that our
decoding technique is effective in generating more faithful responses when
compared to standard decoding techniques on a set of publicly available
document-grounded dialog datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experimental results from applying GPT-4 to an unpublished formal language. (arXiv:2305.12196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12196">
<div class="article-summary-box-inner">
<span><p>Can large language models be used to complete mathematical tasks that are
traditionally performed either manually or with the aid of theorem provers? To
answer this question, a state-of-the-art system, GPT-4, was provided with a
concise natural language specification for a previously unpublished formal
system and asked to complete a number of tasks, from stating function and type
definitions to proving simple theorems and verifying user-supplied proofs. The
system completed all tasks successfully, showed extensive domain knowledge,
invented helpful new syntax and semantics, and exhibited generalization and
inference abilities. So the answer seems to be: yes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models. (arXiv:2305.12199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12199">
<div class="article-summary-box-inner">
<span><p>The VNHSGE (VietNamese High School Graduation Examination) dataset, developed
exclusively for evaluating large language models (LLMs), is introduced in this
article. The dataset, which covers nine subjects, was generated from the
Vietnamese National High School Graduation Examination and comparable tests.
300 literary essays have been included, and there are over 19,000
multiple-choice questions on a range of topics. The dataset assesses LLMs in
multitasking situations such as question answering, text generation, reading
comprehension, visual question answering, and more by including both textual
data and accompanying images. Using ChatGPT and BingChat, we evaluated LLMs on
the VNHSGE dataset and contrasted their performance with that of Vietnamese
students to see how well they performed. The results show that ChatGPT and
BingChat both perform at a human level in a number of areas, including
literature, English, history, geography, and civics education. They still have
space to grow, though, especially in the areas of mathematics, physics,
chemistry, and biology. The VNHSGE dataset seeks to provide an adequate
benchmark for assessing the abilities of LLMs with its wide-ranging coverage
and variety of activities. We intend to promote future developments in the
creation of LLMs by making this dataset available to the scientific community,
especially in resolving LLMs' limits in disciplines involving mathematics and
the natural sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Distillation with Meta Learning for Knowledge Graph Completion. (arXiv:2305.12209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12209">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a selfdistillation framework with meta
learning(MetaSD) for knowledge graph completion with dynamic pruning, which
aims to learn compressed graph embeddings and tackle the longtail samples.
Specifically, we first propose a dynamic pruning technique to obtain a small
pruned model from a large source model, where the pruning mask of the pruned
model could be updated adaptively per epoch after the model weights are
updated. The pruned model is supposed to be more sensitive to difficult to
memorize samples(e.g., longtail samples) than the source model. Then, we
propose a onestep meta selfdistillation method for distilling comprehensive
knowledge from the source model to the pruned model, where the two models
coevolve in a dynamic manner during training. In particular, we exploit the
performance of the pruned model, which is trained alongside the source model in
one iteration, to improve the source models knowledge transfer ability for the
next iteration via meta learning. Extensive experiments show that MetaSD
achieves competitive performance compared to strong baselines, while being 10x
smaller than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT. (arXiv:2305.12212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12212">
<div class="article-summary-box-inner">
<span><p>Multimodal Named Entity Recognition (MNER) on social media aims to enhance
textual entity prediction by incorporating image-based clues. Existing research
in this domain has primarily focused on maximizing the utilization of
potentially relevant information in images or incorporating external knowledge
from explicit knowledge bases (KBs). However, these methods either neglect the
necessity of providing the model with relevant external knowledge, or the
retrieved external knowledge suffers from high redundancy. To address these
problems, we propose a conceptually simple two-stage framework called Prompt
ChatGPT In MNER (PGIM) in this paper. We leverage ChatGPT as an implicit
knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the
model's performance in MNER tasks. Specifically, we first utilize a Multimodal
Similar Example Awareness module to select suitable examples from a small
number of manually annotated samples. These examples are then integrated into a
formatted prompt template tailored to the MNER task, guiding ChatGPT to
generate auxiliary refined knowledge. Finally, the acquired knowledge is
integrated with the raw text and inputted into the downstream model for further
processing. Extensive experiments show that our PGIM significantly outperforms
all existing state-of-the-art methods on two classic MNER datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Specification of Reinforcement Learning Policies through Differentiable Decision Trees. (arXiv:2101.07140v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07140">
<div class="article-summary-box-inner">
<span><p>Human-AI policy specification is a novel procedure we define in which humans
can collaboratively warm-start a robot's reinforcement learning policy. This
procedure is comprised of two steps; (1) Policy Specification, i.e. humans
specifying the behavior they would like their companion robot to accomplish,
and (2) Policy Optimization, i.e. the robot applying reinforcement learning to
improve the initial policy. Existing approaches to enabling collaborative
policy specification are often unintelligible black-box methods, and are not
catered towards making the autonomous system accessible to a novice end-user.
In this paper, we develop a novel collaborative framework to allow humans to
initialize and interpret an autonomous agent's behavior. Through our framework,
we enable humans to specify an initial behavior model via unstructured, natural
language (NL), which we convert to lexical decision trees. Next, we leverage
these translated specifications, to warm-start reinforcement learning and allow
the agent to further optimize these potentially suboptimal policies. Our
approach warm-starts an RL agent by utilizing non-expert natural language
specifications without incurring the additional domain exploration costs. We
validate our approach by showing that our model is able to produce &gt;80%
translation accuracy, and that policies initialized by a human can match the
performance of relevant RL baselines in two domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02442">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have achieved great success in various NLP, vision,
and speech tasks. However, the core of Transformer, the self-attention
mechanism, has a quadratic time and memory complexity with respect to the
sequence length, which hinders applications of Transformer-based models to long
sequences. Many approaches have been proposed to mitigate this problem, such as
sparse attention mechanisms, low-rank matrix approximations and scalable
kernels, and token mixing alternatives to self-attention. We propose a novel
Pooling Network (PoNet) for token mixing in long sequences with linear
complexity. We design multi-granularity pooling and pooling fusion to capture
different levels of contextual information and combine their interactions with
tokens. On the Long Range Arena benchmark, PoNet significantly outperforms
Transformer and achieves competitive accuracy, while being only slightly slower
than the fastest model, FNet, across all sequence lengths measured on GPUs. We
also conduct systematic studies on the transfer learning capability of PoNet
and observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE
benchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis
demonstrates effectiveness of the designed multi-granularity pooling and
pooling fusion for token mixing in long sequences and efficacy of the designed
pre-training tasks for PoNet to learn transferable contextualized language
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion. (arXiv:2204.00618v5 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00618">
<div class="article-summary-box-inner">
<span><p>We explore cross-lingual multi-speaker speech synthesis and cross-lingual
voice conversion applied to data augmentation for automatic speech recognition
(ASR) systems in low/medium-resource scenarios. Through extensive experiments,
we show that our approach permits the application of speech synthesis and voice
conversion to improve ASR systems using only one target-language speaker during
model training. We also managed to close the gap between ASR models trained
with synthesized versus human speech compared to other works that use many
speakers. Finally, we show that it is possible to obtain promising ASR training
results with our data augmentation method using only a single real speaker in a
target language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CORAL: Contextual Response Retrievability Loss Function for Training Dialog Generation Models. (arXiv:2205.10558v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10558">
<div class="article-summary-box-inner">
<span><p>In the field of Natural Language Processing, there are many tasks that can be
tackled effectively using the cross-entropy (CE) loss function. However, the
task of dialog generation poses unique challenges for CE loss. This is because
CE loss assumes that, for any given input, the only possible output is the one
available as the ground truth in the training dataset. But, in dialog
generation, there can be multiple valid responses (for a given context) that
not only have different surface forms but can also be semantically different.
Furthermore, CE loss computation for the dialog generation task does not take
the input context into consideration and, hence, it grades the response
irrespective of the context. To grade the generated response for qualities like
relevance, engagingness, etc., the loss function should depend on both the
context and the generated response. To address these limitations, this paper
proposes CORAL, a novel loss function based on a reinforcement learning (RL)
view of the dialog generation task with a reward function that estimates human
preference for generated responses while considering both the context and the
response. Furthermore, to overcome challenges such as high sample complexity of
RL training and a large action space, we propose a mix-policy training
algorithm. Notably, using CORAL we can train dialog generation models without
assuming the ground-truth as the only correct response. Extensive comparisons
on benchmark datasets demonstrate that CORAL based models outperform strong
state-of-the-art baseline models of different sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models. (arXiv:2205.12213v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12213">
<div class="article-summary-box-inner">
<span><p>Methods for adapting language models (LMs) to new tasks and domains have
traditionally assumed white-box access to the model, and work by modifying its
parameters. However, this is incompatible with a recent trend in the field,
where the highest quality models are only available as black-boxes through
inference APIs. Even when the model weights are available, the computational
cost of fine-tuning large LMs can be prohibitive for most practitioners. In
this work, we present a lightweight method for adapting large LMs to new
domains and tasks, assuming no access to their weights or intermediate
activations. Our approach fine-tunes a small white-box LM and combines it with
the large black-box LM at the probability level through a small network,
learned on a small validation set. We validate our approach by adapting a large
LM (OPT-30B) to several domains and a downstream task (machine translation),
observing improved performance in all cases, of up to 9\%, while using a domain
expert 23x smaller.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection. (arXiv:2205.12495v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12495">
<div class="article-summary-box-inner">
<span><p>Hate speech detection is complex; it relies on commonsense reasoning,
knowledge of stereotypes, and an understanding of social nuance that differs
from one culture to the next. It is also difficult to collect a large-scale
hate speech annotated dataset. In this work, we frame this problem as a
few-shot learning task, and show significant gains with decomposing the task
into its "constituent" parts. In addition, we see that infusing knowledge from
reasoning datasets (e.g. Atomic2020) improves the performance even further.
Moreover, we observe that the trained models generalize to out-of-distribution
datasets, showing the superiority of task decomposition and knowledge infusion
compared to previously used methods. Concretely, our method outperforms the
baseline by 17.83% absolute gain in the 16-shot case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Multimodal Transformer with Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis. (arXiv:2208.07589v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.07589">
<div class="article-summary-box-inner">
<span><p>With the proliferation of user-generated online videos, Multimodal Sentiment
Analysis (MSA) has attracted increasing attention recently. Despite significant
progress, there are still two major challenges on the way towards robust MSA:
1) inefficiency when modeling cross-modal interactions in unaligned multimodal
data; and 2) vulnerability to random modality feature missing which typically
occurs in realistic settings. In this paper, we propose a generic and unified
framework to address them, named Efficient Multimodal Transformer with
Dual-Level Feature Restoration (EMT-DLFR). Concretely, EMT employs
utterance-level representations from each modality as the global multimodal
context to interact with local unimodal features and mutually promote each
other. It not only avoids the quadratic scaling cost of previous local-local
cross-modal interaction methods but also leads to better performance. To
improve model robustness in the incomplete modality setting, on the one hand,
DLFR performs low-level feature reconstruction to implicitly encourage the
model to learn semantic information from incomplete data. On the other hand, it
innovatively regards complete and incomplete data as two different views of one
sample and utilizes siamese representation learning to explicitly attract their
high-level representations. Comprehensive experiments on three popular datasets
demonstrate that our method achieves superior performance in both complete and
incomplete modality settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. (arXiv:2209.08141v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.08141">
<div class="article-summary-box-inner">
<span><p>Probabilistic models of language understanding are valuable tools for
investigating human language use. However, they need to be hand-designed for a
particular domain. In contrast, large language models (LLMs) are trained on
text that spans a wide array of domains, but they lack the structure and
interpretability of probabilistic models. In this paper, we use
chain-of-thought prompts to introduce structures from probabilistic models into
LLMs. We explore this approach in the case of metaphor understanding. Our
chain-of-thought prompts lead language models to infer latent variables and
reason about their relationships in order to choose appropriate paraphrases for
metaphors. The latent variables and relationships chosen are informed by
theories of metaphor understanding from cognitive psychology. We apply these
prompts to the two largest versions of GPT-3 and show that they can improve
performance in a paraphrase selection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Few-shot Approach to Resume Information Extraction via Prompts. (arXiv:2209.09450v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.09450">
<div class="article-summary-box-inner">
<span><p>Prompt learning's fine-tune performance on text classification tasks has
attracted the NLP community. This paper applies it to resume information
extraction, improving existing methods for this task. We created manual
templates and verbalizers tailored to resume texts and compared the performance
of Masked Language Model (MLM) and Seq2Seq PLMs. Also, we enhanced the
verbalizer design for Knowledgeable Prompt-tuning, contributing to prompt
template design across NLP tasks. We present the Manual Knowledgeable
Verbalizer (MKV), a rule for constructing verbalizers for specific
applications. Our tests show that MKV rules yield more effective, robust
templates and verbalizers than existing methods. Our MKV approach resolved
sample imbalance, surpassing current automatic prompt methods. This study
underscores the value of tailored prompt learning for resume extraction,
stressing the importance of custom-designed templates and verbalizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling. (arXiv:2210.05043v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05043">
<div class="article-summary-box-inner">
<span><p>Ensembling BERT models often significantly improves accuracy, but at the cost
of significantly more computation and memory footprint. In this work, we
propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction
tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses
multiple CLS tokens with a parameterization and objective that encourages their
diversity. Thus instead of fine-tuning each BERT model in an ensemble (and
running them all at test time), we need only fine-tune our single Multi-CLS
BERT model (and run the one model at test time, ensembling just the multiple
final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on
top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and
Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS
BERT reliably improves both overall accuracy and confidence estimation. When
only 100 training samples are available in GLUE, the Multi-CLS BERT_Base model
can even outperform the corresponding BERT_Large model. We analyze the behavior
of our Multi-CLS BERT, showing that it has many of the same characteristics and
behavior as a typical BERT 5-way ensemble, but with nearly 4-times less
computation and memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06996">
<div class="article-summary-box-inner">
<span><p>Domain-specific neural machine translation (NMT) systems (\eg, in educational
applications) are socially significant with the potential to help make
information accessible to a diverse set of users in multilingual societies. It
is desirable that such NMT systems be lexically constrained and draw from
domain-specific dictionaries. Dictionaries could present multiple candidate
translations for a source word/phrase due to the polysemous nature of words.
The onus is then on the NMT model to choose the contextually most appropriate
candidate. Prior work has largely ignored this problem and focused on the
single candidate constraint setting wherein the target word or phrase is
replaced by a single constraint. In this work we present \dictdis, a lexically
constrained NMT system that disambiguates between multiple candidate
translations derived from dictionaries. We achieve this by augmenting training
data with multiple dictionary candidates to actively encourage disambiguation
during training by implicitly aligning multiple candidate constraints. We
demonstrate the utility of \dictdis\ via extensive experiments on English-Hindi
and English-German sentences in a variety of domains including regulatory,
finance, engineering. We also present comparisons on standard benchmark test
datasets. In comparison with existing approaches for lexically constrained and
unconstrained NMT, we demonstrate superior performance with respect to
constraint copy and disambiguation related measures on all domains while also
obtaining improved fluency of up to 2-3 BLEU points on some domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10260">
<div class="article-summary-box-inner">
<span><p>Named entity recognition is a traditional task in natural language
processing. In particular, nested entity recognition receives extensive
attention for the widespread existence of the nesting scenario. The latest
research migrates the well-established paradigm of set prediction in object
detection to cope with entity nesting. However, the manual creation of query
vectors, which fail to adapt to the rich semantic information in the context,
limits these approaches. An end-to-end entity detection approach with proposer
and regressor is presented in this paper to tackle the issues. First, the
proposer utilizes the feature pyramid network to generate high-quality entity
proposals. Then, the regressor refines the proposals for generating the final
prediction. The model adopts encoder-only architecture and thus obtains the
advantages of the richness of query semantics, high precision of entity
localization, and easiness of model training. Moreover, we introduce the novel
spatially modulated attention and progressive refinement for further
improvement. Extensive experiments demonstrate that our model achieves advanced
performance in flat and nested NER, achieving a new state-of-the-art F1 score
of 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Speech Translation and Named Entity Recognition. (arXiv:2210.11987v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11987">
<div class="article-summary-box-inner">
<span><p>Modern automatic translation systems aim at place the human at the center by
providing contextual support and knowledge. In this context, a critical task is
enriching the output with information regarding the mentioned entities, which
is currently achieved processing the generated translation with named entity
recognition (NER) and entity linking systems. In light of the recent promising
results shown by direct speech translation (ST) models and the known weaknesses
of cascades (error propagation and additional latency), in this paper we
propose multitask models that jointly perform ST and NER, and compare them with
a cascade baseline. The experimental results show that our models significantly
outperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in
terms of translation quality, and with the same computational efficiency of a
plain direct ST model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech. (arXiv:2210.15368v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15368">
<div class="article-summary-box-inner">
<span><p>The lack of clean speech is a practical challenge to the development of
speech enhancement systems, which means that there is an inevitable mismatch
between their training criterion and evaluation metric. In response to this
unfavorable situation, we propose a training and inference strategy that
additionally uses enhanced speech as a target by improving the previously
proposed noisy-target training (NyTT). Because homogeneity between in-domain
noise and extraneous noise is the key to the effectiveness of NyTT, we train
various student models by remixing 1) the teacher model's estimated speech and
noise for enhanced-target training or 2) raw noisy speech and the teacher
model's estimated noise for noisy-target training. Experimental results show
that our proposed method outperforms several baselines, especially with the
teacher/student inference, where predicted clean speech is derived successively
through the teacher and final student models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. (arXiv:2211.05705v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05705">
<div class="article-summary-box-inner">
<span><p>The rapid development of aspect-based sentiment analysis (ABSA) within recent
decades shows great potential for real-world society. The current ABSA works,
however, are mostly limited to the scenario of a single text piece, leaving the
study in dialogue contexts unexplored. To bridge the gap between fine-grained
sentiment analysis and conversational opinion mining, in this work, we
introduce a novel task of conversational aspect-based sentiment quadruple
analysis, namely DiaASQ, aiming to detect the quadruple of
target-aspect-opinion-sentiment in a dialogue. We manually construct a
large-scale high-quality DiaASQ dataset in both Chinese and English languages.
We deliberately develop a neural model to benchmark the task, which advances in
effectively performing end-to-end quadruple prediction, and manages to
incorporate rich dialogue-specific and discourse feature representations for
better cross-utterance quadruple extraction. We hope the new benchmark will
spur more advancements in the sentiment analysis community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Language Models for Linguistic Structure. (arXiv:2211.07830v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07830">
<div class="article-summary-box-inner">
<span><p>Although pretrained language models (PLMs) can be prompted to perform a wide
range of language tasks, it remains an open question how much this ability
comes from generalizable linguistic understanding versus surface-level lexical
patterns. To test this, we present a structured prompting approach for
linguistic structured prediction tasks, allowing us to perform zero- and
few-shot sequence tagging with autoregressive PLMs. We evaluate this approach
on part-of-speech tagging, named entity recognition, and sentence chunking,
demonstrating strong few-shot performance in all cases. We also find that while
PLMs contain significant prior knowledge of task labels due to task leakage
into the pretraining corpus, structured prompting can also retrieve linguistic
structure with arbitrary labels. These findings indicate that the in-context
learning ability and linguistic knowledge of PLMs generalizes beyond
memorization of their training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08073">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) are known to improve the generalization
performance of natural language understanding models by leveraging large
amounts of data during the pre-training phase. However, the out-of-distribution
(OOD) generalization problem remains a challenge in many NLP tasks, limiting
the real-world deployment of these methods. This paper presents the first
attempt at creating a unified benchmark named GLUE-X for evaluating OOD
robustness in NLP models, highlighting the importance of OOD robustness and
providing insights on how to measure the robustness of a model and how to
improve it. The benchmark includes 13 publicly available datasets for OOD
testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly
used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for
improved OOD accuracy in NLP tasks, as significant performance degradation was
observed in all settings compared to in-distribution (ID) accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEAL: Stable and Active Learning for Few-Shot Prompting. (arXiv:2211.08358v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08358">
<div class="article-summary-box-inner">
<span><p>Few-shot classification has made great strides due to foundation models that,
through priming and prompting, are highly effective few-shot learners. However,
this approach has high variance both across different sets of few shots (data
selection) and across different finetuning runs (run variability). This is
problematic not only because it impedes the fair comparison of different
approaches, but especially because it makes few-shot learning too unreliable
for many real-world applications. To alleviate these issues, we make two
contributions for more stable and effective few-shot learning: First, we
propose novel ensembling methods and show that they substantially reduce run
variability. Second, we introduce a new active learning (AL) criterion for data
selection and present the first AL-based approach specifically tailored towards
prompt-based learning. In our experiments, we show that our combined method,
MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),
improves overall performance of prompt-based finetuning by 2.3 points on five
diverse tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches. (arXiv:2211.08371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08371">
<div class="article-summary-box-inner">
<span><p>People rely heavily on context to enrich meaning beyond what is literally
said, enabling concise but effective communication. To interact successfully
and naturally with people, user-facing artificial intelligence systems will
require similar skills in pragmatics: relying on various types of context --
from shared linguistic goals and conventions, to the visual and embodied world
-- to use language effectively. We survey existing grounded settings and
pragmatic modeling approaches and analyze how the task goals, environmental
contexts, and communicative affordances in each work enrich linguistic meaning.
We present recommendations for future grounded task design to naturally elicit
pragmatic phenomena, and suggest directions that focus on a broader range of
communicative contexts and affordances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations. (arXiv:2211.11093v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11093">
<div class="article-summary-box-inner">
<span><p>Entities and relationships between entities are vital in the real world.
Essentially, we understand the world by understanding entities and relations.
For instance, to understand a field, e.g., computer science, we need to
understand the relevant concepts, e.g., machine learning, and the relationships
between concepts, e.g., machine learning and artificial intelligence. To
understand a person, we should first know who he/she is and how he/she is
related to others. To understand entities and relations, humans may refer to
natural language descriptions. For instance, when learning a new scientific
term, people usually start by reading its definition in dictionaries or
encyclopedias. To know the relationship between two entities, humans tend to
create a sentence to connect them. In this paper, we propose [RE]VER: A Unified
Model for Verbalizing Entities and Relations. Specifically, we attempt to build
a system that takes any entity or entity set as input and generates a sentence
to represent entities and relations, named "natural language representation".
Extensive experiments demonstrate that our model can generate high-quality
sentences describing entities and entity relationships and facilitate various
tasks on entities and relations, including definition modeling, relation
modeling, and generative commonsense reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16031">
<div class="article-summary-box-inner">
<span><p>Syntax is a latent hierarchical structure which underpins the robust and
compositional nature of human language. In this work, we further explore the
hypothesis that syntactic dependencies can be represented in the attention
distributions of language models trained on text and propose a new method to
induce these structures theory-agnostically. Instead of modeling syntactic
relations as defined by annotation schemata, we model a more general property
implicit in the definition of dependency relations, syntactic substitutability.
This property captures the fact that the words at either end of a syntactic
dependency can be substituted with words from the same syntactic category,
defining a set of syntactically-invariant sentences whose representations are
then used as the basis for parsing. We demonstrate that our method results in
both qualitative and quantitative gains, for example achieving 78.3% recall on
a long-distance subject-verb agreement task vs. 8.5% with a previous
unsupervised parsing method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EURO: ESPnet Unsupervised ASR Open-source Toolkit. (arXiv:2211.17196v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.17196">
<div class="article-summary-box-inner">
<span><p>This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO),
an end-to-end open-source toolkit for unsupervised automatic speech recognition
(UASR). EURO adopts the state-of-the-art UASR learning method introduced by the
Wav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised
speech representations and adversarial training. In addition to wav2vec2, EURO
extends the functionality and promotes reproducibility for UASR tasks by
integrating S3PRL and k2, resulting in flexible frontends from 27
self-supervised models and various graph-based decoding strategies. EURO is
implemented in ESPnet and follows its unified pipeline to provide UASR recipes
with a complete setup. This improves the pipeline's efficiency and allows EURO
to be easily applied to existing datasets in ESPnet. Extensive experiments on
three mainstream self-supervised models demonstrate the toolkit's effectiveness
and achieve state-of-the-art UASR performance on TIMIT and LibriSpeech
datasets. EURO will be publicly available at https://github.com/espnet/espnet,
aiming to promote this exciting and emerging research area based on UASR
through open-source activity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Identification of Eviction Status from Electronic Health Record Notes. (arXiv:2212.02762v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02762">
<div class="article-summary-box-inner">
<span><p>Objective: Evictions are important social and behavioral determinants of
health. Evictions are associated with a cascade of negative events that can
lead to unemployment, housing insecurity/homelessness, long-term poverty, and
mental health problems. In this study, we developed a natural language
processing system to automatically detect eviction status from electronic
health record (EHR) notes.
</p>
<p>Materials and Methods: We first defined eviction status (eviction presence
and eviction period) and then annotated eviction status in 5000 EHR notes from
the Veterans Health Administration (VHA). We developed a novel model, KIRESH,
that has shown to substantially outperform other state-of-the-art models such
as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.
Moreover, we designed a novel prompt to further improve the model performance
by using the intrinsic connection between the two sub-tasks of eviction
presence and period prediction. Finally, we used the Temperature Scaling-based
Calibration on our KIRESH-Prompt method to avoid over-confidence issues arising
from the imbalance dataset.
</p>
<p>Results: KIRESH-Prompt substantially outperformed strong baseline models
including fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153
Macro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,
0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also
conducted additional experiments on a benchmark social determinants of health
(SBDH) dataset to demonstrate the generalizability of our methods.
</p>
<p>Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction
status classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an
eviction surveillance system to help address the US Veterans' housing
insecurity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next. (arXiv:2212.04068v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04068">
<div class="article-summary-box-inner">
<span><p>While pre-trained Chinese language models have demonstrated impressive
performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task
remains a challenge. Previous research has explored using information such as
glyphs and phonetics to improve the ability to distinguish misspelled
characters, with good results. However, the generalization ability of these
models is not well understood: it is unclear whether they incorporate
glyph-phonetic information and, if so, whether this information is fully
utilized. In this paper, we aim to better understand the role of glyph-phonetic
information in the CSC task and suggest directions for improvement.
Additionally, we propose a new, more challenging, and practical setting for
testing the generalizability of CSC models. All code is made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06711">
<div class="article-summary-box-inner">
<span><p>Text-based personality computing (TPC) has gained many research interests in
NLP. In this paper, we describe 15 challenges that we consider deserving the
attention of the research community. These challenges are organized by the
following topics: personality taxonomies, measurement quality, datasets,
performance evaluation, modelling choices, as well as ethics and fairness. When
addressing each challenge, not only do we combine perspectives from both NLP
and social sciences, but also offer concrete suggestions. We hope to inspire
more valid and reliable TPC research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RISE: Leveraging Retrieval Techniques for Summarization Evaluation. (arXiv:2212.08775v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08775">
<div class="article-summary-box-inner">
<span><p>Evaluating automatically-generated text summaries is a challenging task.
While there have been many interesting approaches, they still fall short of
human evaluations. We present RISE, a new approach for evaluating summaries by
leveraging techniques from information retrieval. RISE is first trained as a
retrieval task using a dual-encoder retrieval setup, and can then be
subsequently utilized for evaluating a generated summary given an input
document, without gold reference summaries. RISE is especially well suited when
working on new datasets where one may not have reference summaries available
for evaluation. We conduct comprehensive experiments on the SummEval benchmark
(Fabbri et al., 2021) and the results show that RISE has higher correlation
with human evaluations compared to many past approaches to summarization
evaluation. Furthermore, RISE also demonstrates data-efficiency and
generalizability across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Norm of Word Embedding Encodes Information Gain. (arXiv:2212.09663v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09663">
<div class="article-summary-box-inner">
<span><p>Distributed representations of words encode lexical semantic information, but
what type of information is encoded, and how? Focusing on the skip-gram with
negative-sampling method, we found that the squared norm of static word
embedding encodes the information gain conveyed by the word; the information
gain is defined by the Kullback-Leibler divergence of the co-occurrence
distribution of the word to the unigram distribution of the corpus. Our
findings are explained by the theoretical framework of the exponential family
of probability distributions and confirmed through precise experiments that
remove spurious correlations arising from word frequency. We demonstrate that
both the KL divergence and the squared norm of embedding provide a useful
metric of a word's informativeness in tasks such as keyword extraction,
part-of-speech discrimination, and hypernym classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations. (arXiv:2212.09699v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09699">
<div class="article-summary-box-inner">
<span><p>End-to-end Speech Translation is hindered by a lack of available data
resources. While most of them are based on documents, a sentence-level version
is available, which is however single and static, potentially impeding the
usefulness of the data. We propose a new data augmentation strategy,
SegAugment, to address this issue by generating multiple alternative
sentence-level versions of a dataset. Our method utilizes an Audio Segmentation
system, which re-segments the speech of each document with different length
constraints, after which we obtain the target text via alignment methods.
Experiments demonstrate consistent gains across eight language pairs in MuST-C,
with an average increase of 2.5 BLEU points, and up to 5 BLEU for low-resource
scenarios in mTEDx. Furthermore, when combined with a strong system, SegAugment
establishes new state-of-the-art results in MuST-C. Finally, we show that the
proposed method can also successfully augment sentence-level datasets, and that
it enables Speech Translation models to close the gap between the manual and
automatic segmentation at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales. (arXiv:2212.09721v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09721">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have yielded impressive results on many language
reasoning tasks, but their unexpected errors raise doubts about their reasoning
abilities. In light of this, there is growing interest in finetuning/prompting
LMs with both task instances and their associated free-text rationales (FTRs),
which explain the correct reasoning process for predicting the correct task
output (i.e., how to be "right for the right reasons"). However, existing
finetuning methods fail to improve LM performance, while prompting needs
prohibitively large (i.e., &gt;50B) LMs to work well. We propose KNIFE, which
shows that reasoning knowledge can be effectively distilled from FTRs into a
small (i.e., &lt;1B) LM and improve the LM's performance. First, KNIFE finetunes a
teacher LM (given task input and FTR) to predict the task output, transferring
reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE
finetunes a student LM (given task input only) such that its hidden states are
aligned with the teacher's. Thus, the student is endowed with reasoning
knowledge but can be used for inference without direct FTR input. On two
question-answering datasets, KNIFE outperforms various finetuning and prompting
baselines in fully-supervised and low-resource settings. Also, we observe that
FTR quality is crucial to KNIFE's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers. (arXiv:2212.10325v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10325">
<div class="article-summary-box-inner">
<span><p>Diffusion model, a new generative modelling paradigm, has achieved great
success in image, audio, and video generation. However, considering the
discrete categorical nature of text, it is not trivial to extend continuous
diffusion models to natural language, and text diffusion models are less
studied. Sequence-to-sequence text generation is one of the essential natural
language processing topics. In this work, we apply diffusion models to approach
sequence-to-sequence text generation, and explore whether the superiority
generation performance of diffusion model can transfer to natural language
domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence
generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to
model denoising function. In order to improve generation quality, SeqDiffuSeq
combines the self-conditioning technique and a newly proposed adaptive noise
schedule technique. The adaptive noise schedule has the difficulty of denoising
evenly distributed across time steps, and considers exclusive noise schedules
for tokens at different positional order. Experiment results illustrate the
good performance on sequence-to-sequence generation in terms of text quality
and inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models. (arXiv:2212.10471v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10471">
<div class="article-summary-box-inner">
<span><p>Previous work has demonstrated the effectiveness of planning for story
generation exclusively in a monolingual setting focusing primarily on English.
We consider whether planning brings advantages to automatic story generation
across languages. We propose a new task of cross-lingual story generation with
planning and present a new dataset for this task. We conduct a comprehensive
study of different plans and generate stories in several languages, by
leveraging the creative and reasoning capabilities of large pre-trained
language models. Our results demonstrate that plans which structure stories
into three acts lead to more coherent and interesting narratives, while
allowing to explicitly control their content and structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines. (arXiv:2212.10557v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10557">
<div class="article-summary-box-inner">
<span><p>Dialogue models are able to generate coherent and fluent responses, but they
can still be challenging to control and may produce non-engaging, unsafe
results. This unpredictability diminishes user trust and can hinder the use of
the models in the real world. To address this, we introduce DialGuide, a novel
framework for controlling dialogue model behavior using natural language rules,
or guidelines. These guidelines provide information about the context they are
applicable to and what should be included in the response, allowing the models
to generate responses that are more closely aligned with the developer's
expectations and intent. We evaluate DialGuide on three tasks in open-domain
dialogue response generation: guideline selection, response generation, and
response entailment verification. Our dataset contains 10,737 positive and
15,467 negative dialogue context-response-guideline triplets across two domains
- chit-chat and safety. We provide baseline models for the tasks and benchmark
their performance. We also demonstrate that DialGuide is effective in the
dialogue safety domain, producing safe and engaging responses that follow
developer guidelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black-box language model explanation by context length probing. (arXiv:2212.14815v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14815">
<div class="article-summary-box-inner">
<span><p>The increasingly widespread adoption of large language models has highlighted
the need for improving their explainability. We present context length probing,
a novel explanation technique for causal language models, based on tracking the
predictions of a model as a function of the length of available context, and
allowing to assign differential importance scores to different contexts. The
technique is model-agnostic and does not rely on access to model internals
beyond computing token-level probabilities. We apply context length probing to
large pre-trained language models and offer some initial analyses and insights,
including the potential for studying long-range dependencies. The source code
and an interactive demo of the method are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer. (arXiv:2301.06735v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.06735">
<div class="article-summary-box-inner">
<span><p>It is difficult for an E2E ASR system to recognize words such as entities
appearing infrequently in the training data. A widely used method to mitigate
this issue is feeding contextual information into the acoustic model. Previous
works have proven that a compact and accurate contextual list can boost the
performance significantly. In this paper, we propose an efficient approach to
obtain a high quality contextual list for a unified streaming/non-streaming
based E2E model. Specifically, we make use of the phone-level streaming output
to first filter the predefined contextual word list then fuse it into
non-casual encoder and decoder to generate the final recognition results. Our
approach improve the accuracy of the contextual ASR system and speed up the
inference process. Experiments on two datasets demonstrates over 20% CERR
comparing to the baseline system. Meanwile, the RTF of our system can be
stabilized within 0.15 when the size of the contextual word list grows over
6000.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Peanuts Fall in Love with Distributional Semantics?. (arXiv:2301.08731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08731">
<div class="article-summary-box-inner">
<span><p>Context changes expectations about upcoming words - following a story
involving an anthropomorphic peanut, comprehenders expect the sentence the
peanut was in love more than the peanut was salted, as indexed by N400
amplitude (Nieuwland &amp; van Berkum, 2006). This updating of expectations has
been explained using Situation Models - mental representations of a described
event. However, recent work showing that N400 amplitude is predictable from
distributional information alone raises the question whether situation models
are necessary for these contextual effects. We model the results of Nieuwland
and van Berkum (2006) using six computational language models and three sets of
word vectors, none of which have explicit situation models or semantic
grounding. We find that a subset of these can fully model the effect found by
Nieuwland and van Berkum (2006). Thus, at least some processing effects
normally explained through situation models may not in fact require explicit
situation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling. (arXiv:2302.06605v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06605">
<div class="article-summary-box-inner">
<span><p>Large-scale vision-language pre-trained models have shown promising
transferability to various downstream tasks. As the size of these foundation
models and the number of downstream tasks grow, the standard full fine-tuning
paradigm becomes unsustainable due to heavy computational and storage costs.
This paper proposes UniAdapter, which unifies unimodal and multimodal adapters
for parameter-efficient cross-modal adaptation on pre-trained vision-language
models. Specifically, adapters are distributed to different modalities and
their interactions, with the total number of tunable parameters reduced by
partial weight sharing. The unified and knowledge-sharing design enables
powerful cross-modal representations that can benefit various downstream tasks,
requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive
experiments on 6 cross-modal downstream benchmarks (including video-text
retrieval, image-text retrieval, VideoQA, and VQA) show that in most cases,
UniAdapter not only outperforms the state-of-the-arts, but even beats the full
fine-tuning strategy. Particularly, on the MSRVTT retrieval task, UniAdapter
achieves 49.7% recall@1 with 2.2% model parameters, outperforming the latest
competitors by 2.0%. The code and models are available at
https://github.com/RERV/UniAdapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08143">
<div class="article-summary-box-inner">
<span><p>Prompt tuning (PT) which only tunes the embeddings of an additional sequence
of tokens per task, keeping the pre-trained language model (PLM) frozen, has
shown remarkable performance in few-shot learning. Despite this, PT has been
shown to rely heavily on good initialization of the prompt embeddings. In this
work, we study meta prompt tuning (MPT) to systematically explore how
meta-learning can help improve (if it can) cross-task generalization in PT
through learning to initialize the prompt embeddings from other relevant tasks.
We empirically analyze a representative set of meta learning algorithms in a
wide range of adaptation settings with different source/target task
configurations on a large set of few-shot tasks. With extensive experiments and
analysis, we demonstrate the effectiveness of MPT. We find the improvement to
be significant particularly on classification tasks. For other kinds of tasks
such as question answering, we observe that while MPT can outperform PT in most
cases, it does not always outperform multi-task learning. We further provide an
in-depth analysis from the perspective of task similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches. (arXiv:2302.08950v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08950">
<div class="article-summary-box-inner">
<span><p>Wake word detection exists in most intelligent homes and portable devices. It
offers these devices the ability to "wake up" when summoned at a low cost of
power and computing. This paper focuses on understanding alignment's role in
developing a wake-word system that answers a generic phrase. We discuss three
approaches. The first is alignment-based, where the model is trained with
frame-wise cross-entropy. The second is alignment-free, where the model is
trained with CTC. The third, proposed by us, is a hybrid solution in which the
model is trained with a small set of aligned data and then tuned with a
sizeable unaligned dataset. We compare the three approaches and evaluate the
impact of the different aligned-to-unaligned ratios for hybrid training. Our
results show that the alignment-free system performs better than the
alignment-based for the target operating point, and with a small fraction of
the data (20%), we can train a model that complies with our initial
constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. (arXiv:2303.00807v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00807">
<div class="article-summary-box-inner">
<span><p>Many information retrieval tasks require large labeled datasets for
fine-tuning. However, such datasets are often unavailable, and their utility
for real-world applications can diminish quickly due to domain shifts. To
address this challenge, we develop and motivate a method for using large
language models (LLMs) to generate large numbers of synthetic queries cheaply.
The method begins by generating a small number of synthetic queries using an
expensive LLM. After that, a much less expensive one is used to create large
numbers of synthetic queries, which are used to fine-tune a family of reranker
models. These rerankers are then distilled into a single efficient retriever
for use in the target domain. We show that this technique boosts zero-shot
accuracy in long-tail domains, even where only 2K synthetic queries are used
for fine-tuning, and that it achieves substantially lower latency than standard
reranking methods. We make our end-to-end approach, including our synthetic
datasets and replication code, publicly available on Github:
https://github.com/primeqa/primeqa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflexion: Language Agents with Verbal Reinforcement Learning. (arXiv:2303.11366v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11366">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been increasingly used to interact with
external environments (e.g., games, compilers, APIs) as goal-driven agents.
However, it remains challenging for these language agents to quickly and
efficiently learn from trial-and-error as traditional reinforcement learning
methods require extensive training samples and expensive model fine-tuning. We
propose Reflexion, a novel framework to reinforce language agents not by
updating weights, but instead through linguistic feedback. Concretely,
Reflexion agents verbally reflect on task feedback signals, then maintain their
own reflective text in an episodic memory buffer to induce better
decision-making in subsequent trials. Reflexion is flexible enough to
incorporate various types (scalar values or free-form language) and sources
(external or internally simulated) of feedback signals, and obtains significant
improvements over a baseline agent across diverse tasks (sequential
decision-making, coding, language reasoning). For example, Reflexion achieves a
91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous
state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis
studies using different feedback signals, feedback incorporation methods, and
agent types, and provide insights into how they affect performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12314">
<div class="article-summary-box-inner">
<span><p>Prompt tuning is a parameter-efficient method, which learns soft prompts and
conditions frozen language models to perform specific downstream tasks. Though
effective, prompt tuning under few-shot settings on the one hand heavily relies
on a good initialization of soft prompts. On the other hand, it can easily
overfit to few-shot training samples, thereby undermining generalizability.
Existing works leverage pre-training or supervised meta-learning to initialize
soft prompts but they fail to data-efficiently generalize to unseen downstream
tasks. To address the above problems, this paper proposes a novel
Self-supervised meta-prompt learning framework with meta-gradient
regularization for few-shot generalization (SUPMER). SUPMER leverages
self-supervised meta-learning with a diverse set of well-designed meta-tasks to
learn a universal prompt initialization for efficient adaptation using only
unlabeled data. Additionally, it jointly meta-learns a gradient regularization
function to transform raw gradients into a domain-generalizable direction, thus
alleviating the problem of overfitting. Extensive experiments show that SUPMER
achieves better performance for different few-shot downstream tasks, and also
exhibits a stronger domain generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12528">
<div class="article-summary-box-inner">
<span><p>Generative AI models have impressive performance on many Natural Language
Processing tasks such as language understanding, reasoning and language
generation. One of the most important questions that is being asked by the AI
community today is about the capabilities and limits of these models, and it is
clear that evaluating generative AI is very challenging. Most studies on
generative Large Language Models (LLMs) are restricted to English and it is
unclear how capable these models are at understanding and generating other
languages. We present the first comprehensive benchmarking of generative LLMs -
MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse
tasks and 33 typologically diverse languages. We also compare the performance
of generative LLMs to State of the Art (SOTA) non-autoregressive models on
these tasks to determine how well generative models perform compared to the
previous generation of LLMs. We present a thorough analysis of the performance
of models across languages and discuss some of the reasons why generative LLMs
are currently not optimal for all languages. We create a framework for
evaluating generative LLMs in the multilingual setting and provide directions
for future progress in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inspecting and Editing Knowledge Representations in Language Models. (arXiv:2304.00740v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00740">
<div class="article-summary-box-inner">
<span><p>Neural language models (LMs) represent facts about the world described by
text. Sometimes these facts derive from training data (in most LMs, a
representation of the word "banana" encodes the fact that bananas are fruits).
Sometimes facts derive from input text itself (a representation of the sentence
"I poured out the bottle" encodes the fact that the bottle became empty). We
describe REMEDI, a method for learning to map statements in natural language to
fact encodings in an LM's internal representation system. REMEDI encodings can
be used as knowledge editors: when added to LM hidden representations, they
modify downstream generation to be consistent with new facts. REMEDI encodings
may also be used as probes: when compared to LM representations, they reveal
which properties LMs already attribute to mentioned entities, in some cases
making it possible to predict when LMs will generate outputs that conflict with
background knowledge or input text. REMEDI thus links work on probing,
prompting, and LM editing, and offers steps toward general tools for
fine-grained inspection and control of knowledge in LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why think step by step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03843">
<div class="article-summary-box-inner">
<span><p>Humans have a powerful and mysterious capacity to reason. By working through
a series of purely mental steps, we can make inferences we would not be capable
of making directly -- despite the fact that we get no additional data from the
world. Similarly, when large language models generate a series of intermediate
steps (a chain of thought) before answering a question, they often produce
better answers than they otherwise would. We investigate why and how
chain-of-thought reasoning is useful in language models, testing the hypothesis
that reasoning is effective when training data consists of local clusters of
variables that influence each other strongly. These training conditions enable
the chaining of accurate local inferences in order to estimate relationships
between variables that were not seen together in training. We prove that there
will exist a "reasoning gap", where reasoning through intermediate variables
improves inference, for the simple case of an autoregressive density estimator
trained on local samples from a chain-structured probabilistic model. We then
test our hypothesis empirically in more complex models, training an
autoregressive language model on samples from Bayes nets but only including a
subset of variables in each sample. We test language models' ability to match
conditional probabilities with and without intermediate reasoning steps,
finding that intermediate steps are only helpful when the training data is
locally structured with respect to dependencies between variables and that the
combination of locally-structured observations and reasoning is much more
data-efficient than training on all variables. Our results illustrate how the
effectiveness of reasoning step by step is rooted in the local statistical
structure of the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05302">
<div class="article-summary-box-inner">
<span><p>Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment
of large language models with human preferences, significantly enhancing the
quality of interactions between humans and these models. InstructGPT implements
RLHF through several stages, including Supervised Fine-Tuning (SFT), reward
model training, and Proximal Policy Optimization (PPO). PPO, however, is
sensitive to hyperparameters and requires a minimum of four models in its
standard implementation, which makes it hard to train. In contrast, we propose
a novel learning paradigm called RRHF, which scores responses generated by
different sampling policies and learns to align them with human preferences
through ranking loss. RRHF can efficiently align language model output
probabilities with human preferences as robust as fine-tuning and it only needs
1 to 2 models during tuning. In addition, RRHF can be considered an extension
of SFT and reward models while being simpler than PPO in terms of coding, model
counts, and hyperparameters. The entire alignment process can be accomplished
within a single RRHF training session. We evaluate RRHF using LLaMA and Alpaca
on Helpful and Harmless data, demonstrating performance comparable to PPO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11406">
<div class="article-summary-box-inner">
<span><p>This paper highlights the importance of personalization in the current state
of natural language understanding and generation and introduces the LaMP
benchmark -- a novel benchmark for training and evaluating language models for
producing personalized outputs. LaMP offers a comprehensive evaluation
framework with diverse language tasks and multiple entries for each user
profile. It consists of seven personalized tasks, spanning three classification
and four text generation tasks. We also propose a retrieval augmentation
approach that retrieves personalized items from user profiles to construct
personalized prompts for large language models. Our baseline zero-shot and
fine-tuned model results indicate that LMs utilizing profile augmentation
outperform their counterparts that do not factor in profile information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMC-LLaMA: Further Finetuning LLaMA on Medical Papers. (arXiv:2304.14454v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14454">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have showcased remarkable capabilities in
natural language understanding in various domains. These models can usually
behave well on daily dialog, or question answering scenarios, however, in areas
that value precision, for example, in medical applications, they often exhibit
unsatisfactory performance due to a lack of domain-specific knowledge. In this
report, we introduce PMC-LLaMA, an open-source language model that is acquired
by fine-tuning an open-source language model on a total of 4.8 million
biomedical academic papers for further injecting medical knowledge, enhancing
its capability in medical domain. Our preliminary evaluations are conducted on
three biomedical QA datasets, including PubMedQA, MedMCQA, and USMLE, showing
that the our model after finetuning, i.e., PMC-LLaMA, demonstrates better
understanding of biomedical domain-specific concepts, thus achieving high
performance on QA benchmarks. The model and codes, along with an online demo,
are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14732">
<div class="article-summary-box-inner">
<span><p>Making the contents generated by Large Language Model (LLM) such as ChatGPT,
accurate, credible and traceable is crucial, especially in complex
knowledge-intensive tasks that require multi-step reasoning and each of which
needs knowledge to solve. Introducing Information Retrieval (IR) to provide LLM
with external knowledge is good potential to solve this problem. However, where
and how to introduce IR into LLM is a big challenge. Previous work has the
disadvantage that the wrong knowledge retrieved by IR misleads the LLM or
breaks the reasoning chain of LLM. In this paper, we propose a novel framework
called Search-in-the-Chain (SearChain) for the interaction between LLM and IR
to solve the challenges. First, LLM generates the global reasoning chain called
Chain-of-Query (CoQ) where each node consists of an IR-oriented query and the
answer to the query. Second, IR verifies the answer of each node of CoQ, it
corrects the answer that is not consistent with the retrieved information when
IR gives high confidence, which improves the credibility. Third, LLM can mark
its missing knowledge in CoQ and IR can provide this knowledge to LLM. These
three operations improve the accuracy of LLM for complex knowledge-intensive
tasks in terms of reasoning ability and knowledge. Finally, SearChain generates
the reasoning process and marks references to supporting documents for each
reasoning step, which improves traceability. SearChain transforms the topology
of reasoning from chain to tree, which can modify the reasoning direction.
Experiment shows that SearChain outperforms baselines on complex
knowledge-intensive tasks including multi-hop question-answering, slot filling,
fact checking, and long-form question-answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01879">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) beyond a certain scale, demonstrate the emergent
capability of generating free-text rationales for their predictions via
chain-of-thought (CoT) prompting. While CoT can yield dramatically improved
performance, such gains are only observed for sufficiently large LMs. Even more
concerning, there is little guarantee that the generated rationales are
consistent with LM's predictions or faithfully justify the decisions. In this
work, we propose a faithful knowledge distillation method to learn a small,
self-consistent CoT model from a teacher model that is orders of magnitude
larger. To form better supervision, we elicit rationales supporting the gold
answers from a large LM (teacher) by contrastive decoding, which encourages the
teacher to generate tokens that become more plausible only when the answer is
considered. To ensure faithful distillation, we use the teacher-generated
rationales to learn a student LM with a counterfactual reasoning objective,
which prevents the student from ignoring the rationales to make inconsistent
predictions. Experiments show that, while yielding comparable end-task
performance, our method can generate CoT rationales that are more faithful than
baselines do. Further analysis suggests that such a model respects the
rationales more when making decisions; thus, we can improve its performance
more by refining its rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01918">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a popular approach in natural language
processing, particularly for the learning of sentence embeddings. However, the
discrete nature of natural language makes it difficult to ensure the quality of
positive and negative sample pairs generated through data augmentation methods.
Although supervised contrastive learning can produce more accurate sample pairs
with human feedback labels, it still lacks fine-grained training signals. In
this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of
sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our
method utilizes AI feedback from large pre-trained language models (LLMs) to
construct sample pairs with fine-grained sample similarity scores to improve
contrastive learning. Besides, we combine human feedback and AI feedback to
provide better supervision signals for supervised contrastive learning of
sentence embeddings. Experimental results show that our method achieves
state-of-the-art performance on several semantic textual similarity (STS) and
transfer learning tasks compared to other unsupervised and supervised
contrastive learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. (arXiv:2305.02468v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02468">
<div class="article-summary-box-inner">
<span><p>Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks
by tracking dialogue states and generating appropriate responses to help users
achieve defined goals. Recently, end-to-end dialogue models pre-trained based
on large datasets have shown promising performance in the conversational
system. However, they share the same parameters to train tasks of the dialogue
system (NLU, DST, NLG), so debugging each task is challenging. Also, they
require a lot of effort to fine-tune large parameters to create a task-oriented
chatbot, making it difficult for non-experts to handle. Therefore, we intend to
train relatively lightweight and fast models compared to PLM. In this paper, we
propose an End-to-end TOD system with Task-Optimized Adapters which learn
independently per task, adding only small number of parameters after fixed
layers of pre-trained network. We also enhance the performance of the DST and
NLG modules through reinforcement learning, overcoming the learning curve that
has lacked at the adapter learning and enabling the natural and consistent
response generation that is appropriate for the goal. Our method is a
model-agnostic approach and does not require prompt-tuning as only input data
without a prompt. As results of the experiment, our method shows competitive
performance on the MultiWOZ benchmark compared to the existing end-to-end
models. In particular, we attain state-of-the-art performance on the DST task
of 2.2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations. (arXiv:2305.03117v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03117">
<div class="article-summary-box-inner">
<span><p>Human-annotated labels and explanations are critical for training explainable
NLP models. However, unlike human-annotated labels whose quality is easier to
calibrate (e.g., with a majority vote), human-crafted free-form explanations
can be quite subjective. Before blindly using them as ground truth to train ML
models, a vital question needs to be asked: How do we evaluate a
human-annotated explanation's quality? In this paper, we build on the view that
the quality of a human-annotated explanation can be measured based on its
helpfulness (or impairment) to the ML models' performance for the desired NLP
tasks for which the annotations were collected. In comparison to the commonly
used Simulatability score, we define a new metric that can take into
consideration the helpfulness of an explanation for model performance at both
fine-tuning and inference. With the help of a unified dataset format, we
evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model
architectures (T5 and BART), and the results show that our proposed metric can
objectively evaluate the quality of human-annotated explanations, while
Simulatability falls short.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03169">
<div class="article-summary-box-inner">
<span><p>In the era of big data, there is an increasing need for healthcare providers,
communities, and researchers to share data and collaborate to improve health
outcomes, generate valuable insights, and advance research. The Health
Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law
designed to protect sensitive health information by defining regulations for
protected health information (PHI). However, it does not provide efficient
tools for detecting or removing PHI before data sharing. One of the challenges
in this area of research is the heterogeneous nature of PHI fields in data
across different parties. This variability makes rule-based sensitive variable
identification systems that work on one database fail on another. To address
this issue, our paper explores the use of machine learning algorithms to
identify sensitive variables in structured data, thus facilitating the
de-identification process. We made a key observation that the distributions of
metadata of PHI fields and non-PHI fields are very different. Based on this
novel finding, we engineered over 30 features from the metadata of the original
features and used machine learning to build classification models to
automatically identify PHI fields in structured Electronic Health Record (EHR)
data. We trained the model on a variety of large EHR databases from different
data sources and found that our algorithm achieves 99% accuracy when detecting
PHI-related fields for unseen datasets. The implications of our study are
significant and can benefit industries that handle sensitive data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. (arXiv:2305.04091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04091">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently been shown to deliver impressive
performance in various NLP tasks. To tackle multi-step reasoning tasks,
few-shot chain-of-thought (CoT) prompting includes a few manually crafted
step-by-step reasoning demonstrations which enable LLMs to explicitly generate
reasoning steps and improve their reasoning task accuracy. To eliminate the
manual effort, Zero-shot-CoT concatenates the target problem statement with
"Let's think step by step" as an input prompt to LLMs. Despite the success of
Zero-shot-CoT, it still suffers from three pitfalls: calculation errors,
missing-step errors, and semantic misunderstanding errors. To address the
missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of
two components: first, devising a plan to divide the entire task into smaller
subtasks, and then carrying out the subtasks according to the plan. To address
the calculation errors and improve the quality of generated reasoning steps, we
extend PS prompting with more detailed instructions and derive PS+ prompting.
We evaluate our proposed prompting strategy on ten datasets across three
reasoning problems. The experimental results over GPT-3 show that our proposed
zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets
by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought
Prompting, and has comparable performance with 8-shot CoT prompting on the math
reasoning problem. The code can be found at
https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04160">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable language abilities.
GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities
beyond previous visual language models. We attribute this to the use of more
advanced LLMs compared with previous multimodal models. Unfortunately, the
model architecture and training strategies of GPT-4 are unknown. To endow LLMs
with multimodal capabilities, we propose X-LLM, which converts Multi-modalities
(images, speech, videos) into foreign languages using X2L interfaces and inputs
them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple
frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X''
denotes multi-modalities such as image, speech, and videos, and ``L'' denotes
languages. X-LLM's training consists of three stages: (1) Converting Multimodal
Information: The first stage trains each X2L interface to align with its
respective single-modal encoder separately to convert multimodal information
into languages. (2) Aligning X2L representations with the LLM: single-modal
encoders are aligned with the LLM through X2L interfaces independently. (3)
Integrating multiple modalities: all single-modal encoders are aligned with the
LLM through X2L interfaces to integrate multimodal capabilities into the LLM.
Our experiments show that X-LLM demonstrates impressive multimodel chat
abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen
images/instructions, and yields a 84.5\% relative score compared with GPT-4 on
a synthetic multimodal instruction-following dataset. And we also conduct
quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote
the era of LLM-based speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGR: Multi-generator based Rationalization. (arXiv:2305.04492v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04492">
<div class="article-summary-box-inner">
<span><p>Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05252">
<div class="article-summary-box-inner">
<span><p>In everyday life, humans often plan their actions by following step-by-step
instructions in the form of goal-oriented scripts. Previous work has exploited
language models (LMs) to plan for abstract goals of stereotypical activities
(e.g., "make a cake"), but leaves more specific goals with multi-facet
constraints understudied (e.g., "make a cake for diabetics"). In this paper, we
define the task of constrained language planning for the first time. We propose
an overgenerate-then-filter approach to improve large language models (LLMs) on
this task, and use it to distill a novel constrained language planning dataset,
CoScript, which consists of 55,000 scripts. Empirical results demonstrate that
our method significantly improves the constrained language planning ability of
LLMs, especially on constraint faithfulness. Furthermore, CoScript is
demonstrated to be quite effective in endowing smaller LMs with constrained
language planning ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05352">
<div class="article-summary-box-inner">
<span><p>The recent release of large language model (LLM) based chatbots, such as
ChatGPT, has attracted significant attention on foundation models. It is widely
believed that foundation models will serve as the fundamental building blocks
for future AI systems. As foundation models are in their early stages, the
design of foundation model based systems has not yet been systematically
explored. There is little understanding about the impact of introducing
foundation models in software architecture. Therefore, in this paper, we
propose a taxonomy of foundation model based systems, which classifies and
compares the characteristics of foundation models and design options of
foundation model based systems. Our taxonomy comprises three categories:
foundation model pretraining and fine-tuning, architecture design of foundation
model based systems, and responsible-AI-by-design. This taxonomy provides
concrete guidance for making major design decisions when designing foundation
model based systems and highlights trade-offs arising from design decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Refinement via Interaction Between Search Engines and Large Language Models. (arXiv:2305.07402v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07402">
<div class="article-summary-box-inner">
<span><p>Information retrieval (IR) plays a crucial role in locating relevant
resources from vast amounts of data, and its applications have evolved from
traditional knowledge bases to modern search engines (SEs). The emergence of
large language models (LLMs) has further revolutionized the IR field by
enabling users to interact with search systems in natural language. In this
paper, we explore the advantages and disadvantages of LLMs and SEs,
highlighting their respective strengths in understanding user-issued queries
and retrieving up-to-date information. To leverage the benefits of both
paradigms while circumventing their limitations, we propose InteR, a novel
framework that facilitates knowledge refinement through interaction between SEs
and LLMs. InteR allows SEs to expand knowledge in queries using LLM-generated
knowledge collections and enables LLMs to enhance prompt formulation using
SE-retrieved documents. This iterative refinement process augments the inputs
of SEs and LLMs, leading to more accurate retrieval. Experiments on large-scale
retrieval benchmarks involving web search and low-resource retrieval tasks
demonstrate that InteR achieves overall superior zero-shot retrieval
performance compared to state-of-the-art methods, even those using relevance
judgment. Source code is available at https://github.com/Cyril-JZ/InteR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07922">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) pretrained on vast source code have achieved
prominent progress in code intelligence. However, existing code LLMs have two
main limitations in terms of architecture and pretraining tasks. First, they
often adopt a specific architecture (encoder-only or decoder-only) or rely on a
unified encoder-decoder network for different downstream tasks. The former
paradigm is limited by inflexibility in applications while in the latter, the
model is treated as a single system for all tasks, leading to suboptimal
performance on a subset of tasks. Secondly, they often employ a limited set of
pretraining objectives which might not be relevant to some downstream tasks and
hence result in substantial performance degrade. To address these limitations,
we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which
component modules can be flexibly combined to suit a wide range of downstream
code tasks. Such flexibility is enabled by our proposed mixture of pretraining
objectives to mitigate the pretrain-finetune discrepancy. These objectives
cover span denoising, contrastive learning, text-code matching, and causal LM
pretraining tasks, on both unimodal and bimodal multilingual code corpora.
Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs
without training from scratch to efficiently scale up our models, and explore
instruction-tuning to align with natural language instructions. We extensively
evaluate CodeT5+ on over 20 code-related benchmarks in different settings,
including zero-shot, finetuning, and instruction-tuning. We observe
state-of-the-art (SoTA) model performance on various code-related tasks, such
as code generation and completion, math programming, and text-to-code retrieval
tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA
results on HumanEval code generation task against other open code LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08135">
<div class="article-summary-box-inner">
<span><p>Existing knowledge-enhanced methods have achieved remarkable results in
certain QA tasks via obtaining diverse knowledge from different knowledge
bases. However, limited by the properties of retrieved knowledge, they still
have trouble benefiting from both the knowledge relevance and distinguishment
simultaneously. To address the challenge, we propose CPACE, a Concept-centric
Prompt-bAsed Contrastive Explanation Generation model, which aims to convert
obtained symbolic knowledge into a contrastive explanation for better
distinguishing the differences among given candidates. Firstly, following
previous works, we retrieve different types of symbolic knowledge with a
concept-centric knowledge extraction module. After that, we generate
corresponding contrastive explanations using acquired symbolic knowledge and
explanation prompts as guidance for better modeling the knowledge
distinguishment and interpretability. Finally, we regard the generated
contrastive explanation as external knowledge for downstream task enhancement.
We conduct a series of experiments on three widely-used question-answering
datasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the
help of generated contrastive explanation, our CPACE model achieves new SOTA on
CSQA (89.8% on the testing set, 0.9% higher than human performance), and gains
impressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coreference-aware Double-channel Attention Network for Multi-party Dialogue Reading Comprehension. (arXiv:2305.08348v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08348">
<div class="article-summary-box-inner">
<span><p>We tackle Multi-party Dialogue Reading Comprehension (abbr., MDRC). MDRC
stands for an extractive reading comprehension task grounded on a batch of
dialogues among multiple interlocutors. It is challenging due to the
requirement of understanding cross-utterance contexts and relationships in a
multi-turn multi-party conversation. Previous studies have made great efforts
on the utterance profiling of a single interlocutor and graph-based interaction
modeling. The corresponding solutions contribute to the answer-oriented
reasoning on a series of well-organized and thread-aware conversational
contexts. However, the current MDRC models still suffer from two bottlenecks.
On the one hand, a pronoun like "it" most probably produces multi-skip
reasoning throughout the utterances of different interlocutors. On the other
hand, an MDRC encoder is potentially puzzled by fuzzy features, i.e., the
mixture of inner linguistic features in utterances and external interactive
features among utterances. To overcome the bottlenecks, we propose a
coreference-aware attention modeling method to strengthen the reasoning
ability. In addition, we construct a two-channel encoding network. It
separately encodes utterance profiles and interactive relationships, so as to
relieve the confusion among heterogeneous features. We experiment on the
benchmark corpora Molweni and FriendsQA. Experimental results demonstrate that
our approach yields substantial improvements on both corpora, compared to the
fine-tuned BERT and ELECTRA baselines. The maximum performance gain is about
2.5\% F1-score. Besides, our MDRC models outperform the state-of-the-art in
most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Classification via Large Language Models. (arXiv:2305.08377v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08377">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable success of large-scale Language Models (LLMs) such as
GPT-3, their performances still significantly underperform fine-tuned models in
the task of text classification. This is due to (1) the lack of reasoning
ability in addressing complex linguistic phenomena (e.g., intensification,
contrast, irony etc); (2) limited number of tokens allowed in in-context
learning.
</p>
<p>In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts
a progressive reasoning strategy tailored to addressing the complex linguistic
phenomena involved in text classification: CARP first prompts LLMs to find
superficial clues (e.g., keywords, tones, semantic relations, references, etc),
based on which a diagnostic reasoning process is induced for final decisions.
To further address the limited-token issue, CARP uses a fine-tuned model on the
supervised dataset for $k$NN demonstration search in the in-context learning,
allowing the model to take the advantage of both LLM's generalization ability
and the task-specific evidence provided by the full labeled dataset.
Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used
text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on
AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance
comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP
delivers impressive abilities on low-resource and domain-adaptation setups.
Specifically, using 16 examples per class, CARP achieves comparable
performances to supervised models with 1,024 examples per class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Speech Dialogue Translation Mediating Speakers of Different Languages. (arXiv:2305.09210v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09210">
<div class="article-summary-box-inner">
<span><p>We present a new task, speech dialogue translation mediating speakers of
different languages. We construct the SpeechBSD dataset for the task and
conduct baseline experiments. Furthermore, we consider context to be an
important aspect that needs to be addressed in this task and propose two ways
of utilizing context, namely monolingual context and bilingual context. We
conduct cascaded speech translation experiments using Whisper and mBART, and
show that bilingual context performs better in our settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10250">
<div class="article-summary-box-inner">
<span><p>Revolutionary advancements in Large Language Models have drastically reshaped
our interactions with artificial intelligence systems. Despite this, a notable
hindrance remains-the deficiency of a long-term memory mechanism within these
models. This shortfall becomes increasingly evident in situations demanding
sustained interaction, such as personal companion systems and psychological
counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored
for LLMs. MemoryBank enables the models to summon relevant memories,
continually evolve through continuous memory updates, comprehend, and adapt to
a user personality by synthesizing information from past interactions. To mimic
anthropomorphic behaviors and selectively preserve memory, MemoryBank
incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting
Curve theory, which permits the AI to forget and reinforce memory based on time
elapsed and the relative significance of the memory, thereby offering a
human-like memory mechanism. MemoryBank is versatile in accommodating both
closed-source models like ChatGPT and open-source models like ChatGLM. We
exemplify application of MemoryBank through the creation of an LLM-based
chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned
with psychological dialogs, SiliconFriend displays heightened empathy in its
interactions. Experiment involves both qualitative analysis with real-world
user dialogs and quantitative analysis with simulated dialogs. In the latter,
ChatGPT acts as users with diverse characteristics and generates long-term
dialog contexts covering a wide array of topics. The results of our analysis
reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong
capability for long-term companionship as it can provide emphatic response,
recall relevant memories and understand user personality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. (arXiv:2305.10263v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10263">
<div class="article-summary-box-inner">
<span><p>Large language models have recently made tremendous progress in a variety of
aspects, e.g., cross-task generalization, instruction following.
Comprehensively evaluating the capability of large language models in multiple
tasks is of great importance. In this paper, we propose M3KE, a Massive
Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to
measure knowledge acquired by Chinese large language models by testing their
multitask accuracy in zero- and few-shot settings. We have collected 20,477
questions from 71 tasks. Our selection covers all major levels of Chinese
education system, ranging from the primary school to college, as well as a wide
variety of subjects, including humanities, history, politics, law, education,
psychology, science, technology, art and religion. All questions are
multiple-choice questions with four options, hence guaranteeing a standardized
and unified assessment process. We've assessed a number of state-of-the-art
open-source Chinese large language models on the proposed benchmark. The size
of these models varies from 335M to 130B parameters. Experiment results
demonstrate that they perform significantly worse than GPT-3.5 that reaches an
accuracy of ~ 48% on M3KE. The dataset is available at
https://github.com/tjunlp-lab/M3KE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10306">
<div class="article-summary-box-inner">
<span><p>We propose a new paradigm for universal information extraction (IE) that is
compatible with any schema format and applicable to a list of IE tasks, such as
named entity recognition, relation extraction, event extraction and sentiment
analysis. Our approach converts the text-based IE tasks as the token-pair
problem, which uniformly disassembles all extraction targets into joint span
detection, classification and association problems with a unified extractive
framework, namely UniEX. UniEX can synchronously encode schema-based prompt and
textual information, and collaboratively learn the generalized knowledge from
pre-defined information using the auto-encoder language models. We develop a
traffine attention mechanism to integrate heterogeneous factors including
tasks, labels and inside tokens, and obtain the extraction target via a scoring
matrix. Experiment results show that UniEX can outperform generative universal
IE models in terms of performance and inference-speed on $14$ benchmarks IE
datasets with the supervised setting. The state-of-the-art performance in
low-resource scenarios also verifies the transferability and effectiveness of
UniEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10400">
<div class="article-summary-box-inner">
<span><p>Automatically determining whether a text and a corresponding image are
semantically aligned is a significant challenge for vision-language models,
with applications in generative text-to-image and image-to-text tasks. In this
work, we study methods for automatic text-image alignment evaluation. We first
introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets
from both text-to-image and image-to-text generation tasks, with human
judgements for whether a given text-image pair is semantically aligned. We then
describe two automatic methods to determine alignment: the first involving a
pipeline based on question generation and visual question answering models, and
the second employing an end-to-end classification approach by finetuning
multimodal pretrained models. Both methods surpass prior approaches in various
text-image alignment tasks, with significant improvements in challenging cases
that involve complex composition or unnatural images. Finally, we demonstrate
how our approaches can localize specific misalignments between an image and a
given text, and how they can be used to automatically re-rank candidates in
text-to-image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10435">
<div class="article-summary-box-inner">
<span><p>The Generative Pre-trained Transformer (GPT) represents a notable
breakthrough in the domain of natural language processing, which is propelling
us toward the development of machines that can understand and communicate using
language in a manner that closely resembles that of humans. GPT is based on the
transformer architecture, a deep neural network designed for natural language
processing tasks. Due to their impressive performance on natural language
processing tasks and ability to effectively converse, GPT have gained
significant popularity among researchers and industrial communities, making
them one of the most widely used and effective models in natural language
processing and related fields, which motivated to conduct this review. This
review provides a detailed overview of the GPT, including its architecture,
working process, training procedures, enabling technologies, and its impact on
various applications. In this review, we also explored the potential challenges
and limitations of a GPT. Furthermore, we discuss potential solutions and
future directions. Overall, this paper aims to provide a comprehensive
understanding of GPT, enabling technologies, their impact on various
applications, emerging challenges, and potential solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paxion: Patching Action Knowledge in Video-Language Foundation Models. (arXiv:2305.10683v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10683">
<div class="article-summary-box-inner">
<span><p>Action knowledge involves the understanding of textual, visual, and temporal
aspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)
containing two carefully designed probing tasks: Action Antonym and Video
Reversal, which targets multimodal alignment capabilities and temporal
understanding skills of the model, respectively. Despite recent video-language
models' (VidLM) impressive performance on various benchmark tasks, our
diagnostic tasks reveal their surprising deficiency (near-random performance)
in action knowledge, suggesting that current models rely on object recognition
abilities as a shortcut for action understanding. To remedy this, we propose a
novel framework, Paxion, along with a new Discriminative Video Dynamics
Modeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher
network to encode new action knowledge and a Knowledge Fuser component to
integrate the Patcher into frozen VidLMs without compromising their existing
capabilities. Due to limitations of the widely-used Video-Text Contrastive
(VTC) loss for learning action knowledge, we introduce the DVDM objective to
train the Knowledge Patcher. DVDM forces the model to encode the correlation
between the action text and the correct ordering of video frames. Our extensive
analyses show that Paxion and DVDM together effectively fill the gap in action
knowledge understanding (~50% to 80%), while maintaining or improving
performance on a wide spectrum of both object- and action-centric downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10833">
<div class="article-summary-box-inner">
<span><p>The domain of Botany is rich with metaphorical terms. Those terms play an
important role in the description and identification of flowers and plants.
However, the identification of such terms in discourse is an arduous task. This
leads in some cases to committing errors during translation processes and
lexicographic tasks. The process is even more challenging when it comes to
machine translation, both in the cases of single-word terms and multi-word
terms. One of the recent concerns of Natural Language Processing (NLP)
applications and Machine Translation (MT) technologies is the automatic
identification of metaphor-based words in discourse through Deep Learning (DL).
In this study, we seek to fill this gap through the use of thirteen popular
transformer based models, as well as ChatGPT, and we show that discriminative
models perform better than GPT-3.5 model with our best performer reporting
92.2349% F1 score in metaphoric flower and plant names identification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11096">
<div class="article-summary-box-inner">
<span><p>End-to-end sign language translation (SLT) aims to convert sign language
videos into spoken language texts directly without intermediate
representations. It has been a challenging task due to the modality gap between
sign videos and texts and the data scarcity of labeled data. To tackle these
challenges, we propose a novel Cross-modality Data Augmentation (XmDA)
framework to transfer the powerful gloss-to-text translation capabilities to
end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo
gloss-text pairs from the sign gloss translation model. Specifically, XmDA
consists of two key components, namely, cross-modality mix-up and
cross-modality knowledge distillation. The former explicitly encourages the
alignment between sign video features and gloss embeddings to bridge the
modality gap. The latter utilizes the generation knowledge from gloss-to-text
teacher models to guide the spoken language text generation. Experimental
results on two widely used SLT datasets, i.e., PHOENIX-2014T and CSL-Daily,
demonstrate that the proposed XmDA framework significantly and consistently
outperforms the baseline models. Extensive analyses confirm our claim that XmDA
enhances spoken language text generation by reducing the representation
distance between videos and texts, as well as improving the processing of
low-frequency words and long sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate. (arXiv:2305.11595v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11595">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated human-like intelligence and
are widely used in various applications. However, LLMs still exhibit various
kinds of inconsistency problems. Existing works mainly focus on the
inconsistency issues within a single LLM, while we investigate the
inter-consistency among multiple LLMs, which is critical for collaborating to
solve a complex task. To examine whether LLMs can collaborate to ultimately
achieve a consensus for the shared goal and whether LLMs easily change their
viewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct
a three-stage debate aligned with real-world scenarios: fair debate, mismatched
debate, and roundtable debate. Through extensive experiments on the commonsense
reasoning task, LLMs not only become more inter-consistent but also achieve
higher performance. Moreover, we observe that stronger LLMs tend to dominate
the debates by adhering to their perspectives, while weaker ones are more
likely to change viewpoints. Additionally, we highlight the importance of a
competent judge, such as GPT-4, to draw more proper conclusions. Our work
contributes to understanding the inter-consistency among LLMs and lays the
foundation for the development of future collaboration methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11747">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as ChatGPT, are prone to generate
hallucinations, \ie content that conflicts with the source or cannot be
verified by the factual knowledge. To understand what types of content and to
which extent LLMs are apt to hallucinate, we introduce the Hallucination
Evaluation for Large Language Models (HaluEval) benchmark, a large collection
of generated and human-annotated hallucinated samples for evaluating the
performance of LLMs in recognizing hallucination. To generate these samples, we
propose a ChatGPT-based two-step framework, \ie sampling-then-filtering.
Besides, we also hire some human labelers to annotate the hallucinations in
ChatGPT responses. The empirical results suggest that ChatGPT is likely to
generate hallucinated content in specific topics by fabricating unverifiable
information (\ie about $11.4\%$ user queries). Moreover, existing LLMs face
great challenges in recognizing the hallucinations in texts. While, our
experiments also prove that the hallucination recognition can be improved by
providing external knowledge or adding reasoning steps. Our benchmark can be
accessed at https://github.com/RUCAIBox/HaluEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11790">
<div class="article-summary-box-inner">
<span><p>Prompting with natural language instructions has recently emerged as a
popular method of harnessing the capabilities of large language models. Given
the inherent ambiguity present in natural language, it is intuitive to consider
the possible advantages of prompting with less ambiguous prompt styles, such as
the use of pseudo-code.
</p>
<p>In this paper we explore if prompting via pseudo-code instructions helps
improve the performance of pre-trained language models. We manually create a
dataset of pseudo-code prompts for 132 different tasks spanning classification,
QA and generative language tasks, sourced from the Super-NaturalInstructions
dataset. Using these prompts along with their counterparts in natural language,
we study their performance on two LLM families - BLOOM and CodeGen. Our
experiments show that using pseudo-code instructions leads to better results,
with an average increase (absolute) of 7-16 points in F1 scores for
classification tasks and an improvement (relative) of 12-38% in aggregate
ROUGE-L scores across all tasks. We include detailed ablation studies which
indicate that code comments, docstrings, and the structural clues encoded in
pseudo-code all contribute towards the improvement in performance.
</p>
<p>To the best of our knowledge our work is the first to demonstrate how
pseudo-code prompts can be helpful in improving the performance of pre-trained
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11828">
<div class="article-summary-box-inner">
<span><p>Medical systematic reviews are crucial for informing clinical decision making
and healthcare policy. But producing such reviews is onerous and
time-consuming. Thus, high-quality evidence synopses are not available for many
questions and may be outdated even when they are available. Large language
models (LLMs) are now capable of generating long-form texts, suggesting the
tantalizing possibility of automatically generating literature reviews on
demand. However, LLMs sometimes generate inaccurate (and potentially
misleading) texts by hallucinating or omitting important information. In the
healthcare context, this may render LLMs unusable at best and dangerous at
worst. Most discussion surrounding the benefits and risks of LLMs have been
divorced from specific applications. In this work, we seek to qualitatively
characterize the potential utility and risks of LLMs for assisting in
production of medical evidence reviews. We conducted 16 semi-structured
interviews with international experts in systematic reviews, grounding
discussion in the context of generating evidence reviews. Domain experts
indicated that LLMs could aid writing reviews, as a tool for drafting or
creating plain language summaries, generating templates or suggestions,
distilling information, crosschecking, and synthesizing or interpreting text
inputs. But they also identified issues with model outputs and expressed
concerns about potential downstream harms of confidently composed but
inaccurate LLM outputs which might mislead. Other anticipated potential
downstream harms included lessened accountability and proliferation of
automatically generated reviews that might be of low quality. Informed by this
qualitative analysis, we identify criteria for rigorous evaluation of
biomedical LLMs aligned with domain expert views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11863">
<div class="article-summary-box-inner">
<span><p>Representations from transformer-based unidirectional language models are
known to be effective at predicting brain responses to natural language.
However, most studies comparing language models to brains have used GPT-2 or
similarly sized language models. Here we tested whether larger open-source
models such as those from the OPT and LLaMA families are better at predicting
brain responses recorded using fMRI. Mirroring scaling results from other
contexts, we found that brain prediction performance scales log-linearly with
model size from 125M to 30B parameter models, with ~15% increased encoding
performance as measured by correlation with a held-out test set across 3
subjects. Similar log-linear behavior was observed when scaling the size of the
fMRI training set. We also characterized scaling for acoustic encoding models
that use HuBERT, WavLM, and Whisper, and we found comparable improvements with
model size. A noise ceiling analysis of these large, high-performance encoding
models showed that performance is nearing the theoretical maximum for brain
areas such as the precuneus and higher auditory cortex. These results suggest
that increasing scale in both models and data will yield incredibly effective
models of language processing in the brain, enabling better scientific
understanding as well as applications such as decoding.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-23 23:11:03.850106765 UTC">2023-05-23 23:11:03 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>