<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-27T01:30:00Z">04-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13060">
<div class="article-summary-box-inner">
<span><p>Both humans and transformer language models are able to learn language
without explicit structural supervision. What inductive learning biases make
this learning possible? In this study, we examine the effect of different
inductive learning biases by predisposing language models with structural
biases through pretraining on artificial structured data, and then evaluating
by fine-tuning on English. Our experimental setup gives us the ability to
actively control the inductive bias of language models. With our experiments,
we investigate the comparative success of three types of inductive bias: 1) an
inductive bias for recursive, hierarchical processing 2) an inductive bias for
unrestricted token-token dependencies that can't be modeled by context-free
grammars, and 3) an inductive bias for a Zipfian power-law vocabulary
distribution. We show that complex token-token interactions form the best
inductive biases, and that this is strongest in the non-context-free case. We
also show that a Zipfian vocabulary distribution forms a good inductive bias
independently from grammatical structure. Our study leverages the capabilities
of transformer models to run controlled language learning experiments that are
not possible to run in humans, and surfaces hypotheses about the structures
that facilitate language learning in both humans and machines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAST: Scalable Lattice-Based Speech Modelling in JAX. (arXiv:2304.13134v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13134">
<div class="article-summary-box-inner">
<span><p>We introduce LAST, a LAttice-based Speech Transducer library in JAX. With an
emphasis on flexibility, ease-of-use, and scalability, LAST implements
differentiable weighted finite state automaton (WFSA) algorithms needed for
training \&amp; inference that scale to a large WFSA such as a recognition lattice
over the entire utterance. Despite these WFSA algorithms being well-known in
the literature, new challenges arise from performance characteristics of modern
architectures, and from nuances in automatic differentiation. We describe a
suite of generally applicable techniques employed in LAST to address these
challenges, and demonstrate their effectiveness with benchmarks on TPUv3 and
V100 GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13140">
<div class="article-summary-box-inner">
<span><p>The challenges faced by text classification with large tag systems in natural
language processing tasks include multiple tag systems, uneven data
distribution, and high noise. To address these problems, the ESimCSE
unsupervised comparative learning and UDA semi-supervised comparative learning
models are combined through the use of joint training techniques in the
models.The ESimCSE model efficiently learns text vector representations using
unlabeled data to achieve better classification results, while UDA is trained
using unlabeled data through semi-supervised learning methods to improve the
prediction performance of the models and stability, and further improve the
generalization ability of the model. In addition, adversarial training
techniques FGM and PGD are used in the model training process to improve the
robustness and reliability of the model. The experimental results show that
there is an 8% and 10% accuracy improvement relative to Baseline on the public
dataset Ruesters as well as on the operational dataset, respectively, and a 15%
improvement in manual validation accuracy can be achieved on the operational
dataset, indicating that the method is effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities. (arXiv:2304.13149v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13149">
<div class="article-summary-box-inner">
<span><p>Virtual assistants are becoming increasingly important speech-driven
Information Retrieval platforms that assist users with various tasks.
</p>
<p>We discuss open problems and challenges with respect to modeling spoken
information queries for virtual assistants, and list opportunities where
Information Retrieval methods and research can be applied to improve the
quality of virtual assistant speech recognition.
</p>
<p>We discuss how query domain classification, knowledge graphs and user
interaction data, and query personalization can be helpful to improve the
accurate recognition of spoken information domain queries. Finally, we also
provide a brief overview of current problems and challenges in speech
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports. (arXiv:2304.13180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13180">
<div class="article-summary-box-inner">
<span><p>With the increasing number of clinical trial reports generated every day, it
is becoming hard to keep up with novel discoveries that inform evidence-based
healthcare recommendations. To help automate this process and assist medical
experts, NLP solutions are being developed. This motivated the SemEval-2023
Task 7, where the goal was to develop an NLP system for two tasks: evidence
retrieval and natural language inference from clinical trial data. In this
paper, we describe our two developed systems. The first one is a pipeline
system that models the two tasks separately, while the second one is a joint
system that learns the two tasks simultaneously with a shared representation
and a multi-task learning approach. The final system combines their outputs in
an ensemble system. We formalize the models, present their characteristics and
challenges, and provide an analysis of achieved results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TABLET: Learning From Instructions For Tabular Data. (arXiv:2304.13188v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13188">
<div class="article-summary-box-inner">
<span><p>Acquiring high-quality data is often a significant challenge in training
machine learning (ML) models for tabular prediction, particularly in
privacy-sensitive and costly domains like medicine and finance. Providing
natural language instructions to large language models (LLMs) offers an
alternative solution. However, it is unclear how effectively instructions
leverage the knowledge in LLMs for solving tabular prediction problems. To
address this gap, we introduce TABLET, a benchmark of 20 diverse tabular
datasets annotated with instructions that vary in their phrasing, granularity,
and technicality. Additionally, TABLET includes the instructions' logic and
structured modifications to the instructions. We find in-context instructions
increase zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for
ChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular
prediction in our benchmark by evaluating instruction faithfulness. We find
LLMs often ignore instructions and fail to predict specific instances
correctly, even with examples. Our analysis on TABLET shows that, while
instructions help LLM performance, learning from instructions for tabular data
requires new capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Explainable and Safe Conversational Agents for Mental Health: A Survey. (arXiv:2304.13191v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13191">
<div class="article-summary-box-inner">
<span><p>Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to
support the overburdened global healthcare system that gets 60 million primary
care visits, and 6 million Emergency Room (ER) visits annually. These systems
are built by clinical psychologists, psychiatrists, and Artificial Intelligence
(AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role
of VMHAs is to provide emotional support through information, focusing less on
developing a reflective conversation with the patient. A more comprehensive,
safe and explainable approach is required to build responsible VMHAs to ask
follow-up questions or provide a well-informed response. This survey offers a
systematic critical review of the existing conversational agents in mental
health, followed by new insights into the improvements of VMHAs with contextual
knowledge, datasets, and their emerging role in clinical decision support. We
also provide new directions toward enriching the user experience of VMHAs with
explainability, safety, and wholesome trustworthiness. Finally, we provide
evaluation metrics and practical considerations for VMHAs beyond the current
literature to build trust between VMHAs and patients in active communications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Curious Case of Code Prompts. (arXiv:2304.13250v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13250">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that prompting language models with code-like
representations of natural language leads to performance improvements on
structured reasoning tasks. However, such tasks comprise only a small subset of
all natural language tasks. In our work, we seek to answer whether or not
code-prompting is the preferred way of interacting with language models in
general. We compare code and text prompts across three popular GPT models
(davinci, code-davinci-002, and text-davinci-002) on a broader selection of
tasks (e.g., QA, sentiment, summarization) and find that with few exceptions,
code prompts do not consistently outperform text prompts. Furthermore, we show
that the style of code prompt has a large effect on performance for some but
not all tasks and that fine-tuning on text instructions leads to better
relative performance of code prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13273">
<div class="article-summary-box-inner">
<span><p>With the development of Vision-Language Pre-training Models (VLPMs)
represented by CLIP and ALIGN, significant breakthroughs have been achieved for
association-based visual tasks such as image classification and image-text
retrieval by the zero-shot capability of CLIP without fine-tuning. However,
CLIP is hard to apply to generation-based tasks. This is due to the lack of
decoder architecture and pre-training tasks for generation. Although previous
works have created generation capacity for CLIP through additional language
models, a modality gap between the CLIP representations of different modalities
and the inability of CLIP to model the offset of this gap, which fails the
concept to transfer across modalities. To solve the problem, we try to map
images/videos to the language modality and generate captions from the language
modality. In this paper, we propose the K-nearest-neighbor Cross-modality
Mapping (Knight), a zero-shot method from association to generation. With
text-only unsupervised training, Knight achieves state-of-the-art performance
in zero-shot methods for image captioning and video captioning. Our code is
available at https://github.com/junyangwang0410/Knight.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. (arXiv:2304.13276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13276">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are known for their exceptional performance in
natural language processing, making them highly effective in many human
life-related or even job-related tasks. The attention mechanism in the
Transformer architecture is a critical component of LLMs, as it allows the
model to selectively focus on specific input parts. The softmax unit, which is
a key part of the attention mechanism, normalizes the attention scores. Hence,
the performance of LLMs in various NLP tasks depends significantly on the
crucial role played by the attention mechanism with the softmax unit.
</p>
<p>In-context learning, as one of the celebrated abilities of recent LLMs, is an
important concept in querying LLMs such as ChatGPT. Without further parameter
updates, Transformers can learn to predict based on few in-context examples.
However, the reason why Transformers becomes in-context learners is not well
understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the
in-context learning from a mathematical perspective based on a linear
regression formulation $\min_x\| Ax - b \|_2$, which show Transformers'
capability of learning linear functions in context.
</p>
<p>In this work, we study the in-context learning based on a softmax regression
formulation $\min_{x} \| \langle \exp(Ax), {\bf 1}_n \rangle^{-1} \exp(Ax) - b
\|_2$ of Transformer's attention mechanism. We show the upper bounds of the
data transformations induced by a single self-attention layer and by
gradient-descent on a $\ell_2$ regression loss for softmax prediction function,
which imply that when training self-attention-only Transformers for fundamental
regression tasks, the models learned by gradient-descent and Transformers show
great similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Slot and Intent Detection in Low-Resource Languages. (arXiv:2304.13292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13292">
<div class="article-summary-box-inner">
<span><p>Intent detection and slot filling are critical tasks in spoken and natural
language understanding for task-oriented dialog systems. In this work we
describe our participation in the slot and intent detection for low-resource
language varieties (SID4LR; Aepli et al. (2023)). We investigate the slot and
intent detection (SID) tasks using a wide range of models and settings. Given
the recent success of multitask-prompted finetuning of large language models,
we also test the generalization capability of the recent encoder-decoder model
mT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have
never intentionally seen. We show that our best model outperforms the baseline
by a large margin (up to +30 F1 points) in both SID tasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13301">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT
and GPT-4 have significantly impacted the AI community, including Text-to-SQL
tasks. Some evaluations and analyses on LLMs show their potential to generate
SQL queries but they point out poorly designed prompts (e.g. simplistic
construction or random sampling) limit LLMs' performance and may cause
unnecessary or irrelevant outputs. To address these issues, we propose
CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5
for precise control over case-relevant and case-irrelevant knowledge in
Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for
GPT-3.5, which involves (1) adaptively retrieving cases according to the
question intention by de-semantizing the input question, and (2) an adaptive
fallback mechanism to ensure the informativeness of the prompt, as well as the
relevance between cases and the prompt. In the de-semanticization phase, we
designed Semantic Domain Relevance Evaluator(SDRE), combined with Poincar\'e
detector(mining implicit semantics in hyperbolic space), TextAlign(discovering
explicit matches), and Positector (part-of-speech detector). SDRE semantically
and syntactically generates in-context exemplar annotations for the new case.
On the three cross-domain datasets, our framework outperforms the
state-of-the-art(SOTA) model in execution accuracy by 3.7\%, 2.5\%, and 8.2\%,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nominal Topology for Data Languages. (arXiv:2304.13337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13337">
<div class="article-summary-box-inner">
<span><p>We propose a novel topological perspective on data languages recognizable by
orbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite
nominal topological spaces. Assuming globally bounded support sizes, they
coincide with nominal Stone spaces and are shown to be dually equivalent to a
subcategory of nominal boolean algebras. Recognizable data languages are
characterized as topologically clopen sets of pro-orbit-finite words. In
addition, we explore the expressive power of pro-orbit-finite equations by
establishing a nominal version of Reiterman's pseudovariety theorem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System. (arXiv:2304.13343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13343">
<div class="article-summary-box-inner">
<span><p>Large-scale Language Models (LLMs) are constrained by their inability to
process lengthy inputs. To address this limitation, we propose the
Self-Controlled Memory (SCM) system to unleash infinite-length input capacity
for large-scale language models. Our SCM system is composed of three key
modules: the language model agent, the memory stream, and the memory
controller. The language model agent iteratively processes ultra-long inputs
and stores all historical information in the memory stream. The memory
controller provides the agent with both long-term memory (archived memory) and
short-term memory (flash memory) to generate precise and coherent responses.
The controller determines which memories from archived memory should be
activated and how to incorporate them into the model input. Our SCM system can
be integrated with any LLMs to enable them to process ultra-long texts without
any modification or fine-tuning. Experimental results show that our SCM system
enables LLMs, which are not optimized for multi-turn dialogue, to achieve
multi-turn dialogue capabilities that are comparable to ChatGPT, and to
outperform ChatGPT in scenarios involving ultra-long document summarization or
long-term conversations. Additionally, we will supply a test set, which covers
common long-text input scenarios, for evaluating the abilities of LLMs in
processing long documents.~\footnote{Working in
progress.}\footnote{\url{https://github.com/wbbeyourself/SCM4LLMs}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multidimensional Evaluation for Text Style Transfer Using ChatGPT. (arXiv:2304.13462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13462">
<div class="article-summary-box-inner">
<span><p>We investigate the potential of ChatGPT as a multidimensional evaluator for
the task of \emph{Text Style Transfer}, alongside, and in comparison to,
existing automatic metrics as well as human judgements. We focus on a zero-shot
setting, i.e. prompting ChatGPT with specific task instructions, and test its
performance on three commonly-used dimensions of text style transfer
evaluation: style strength, content preservation, and fluency. We perform a
comprehensive correlation analysis for two transfer directions (and overall) at
different levels. Compared to existing automatic metrics, ChatGPT achieves
competitive correlations with human judgments. These preliminary results are
expected to provide a first glimpse into the role of large language models in
the multidimensional evaluation of stylized text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets. (arXiv:2304.13557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13557">
<div class="article-summary-box-inner">
<span><p>As virtual assistants continue to be taken up globally, there is an
ever-greater need for these speech-based systems to communicate naturally in a
variety of languages. Crowdsourcing initiatives have focused on multilingual
translation of big, open data sets for use in natural language processing
(NLP). Yet, language translation is often not one-to-one, and biases can
trickle in. In this late-breaking work, we focus on the case of pronouns
translated between English and Japanese in the crowdsourced Tatoeba database.
We found that masculine pronoun biases were present overall, even though
plurality in language was accounted for in other ways. Importantly, we detected
biases in the translation process that reflect nuanced reactions to the
presence of feminine, neutral, and/or non-binary pronouns. We raise the issue
of translation bias for pronouns and offer a practical solution to embed
plurality in NLP data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. (arXiv:2304.13559v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13559">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class
of database systems that can seamlessly query text and tables using SQL. To
enable seamless querying of textual data using SQL in an MMDB, we propose to
extend relational databases with so-called multi-modal operators (MMOps) which
are based on the advances of recent large language models such as GPT-3. The
main idea of MMOps is that they allow text collections to be treated as tables
without the need to manually transform the data. As we show in our evaluation,
our MMDB prototype can not only outperform state-of-the-art approaches such as
text-to-table in terms of accuracy and performance but it also requires
significantly less training data to fine-tune the model for an unseen text
collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13567">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have shown state-of-the-art performance in Natural
Language Processing (NLP) tasks. Downstream tasks such as Named Entity
Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data
imbalance issues, specifically in terms of the ratio of positive to negative
examples, and class imbalance. In this paper, we investigate an additional
specific issue for language models, namely the position bias of positive
examples in token classification tasks. Therefore, we conduct an in-depth
evaluation of the impact of position bias on the performance of LMs when
fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and
OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We
propose an evaluation approach to investigate position bias in Transformer
models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as
GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in
their performance. To mitigate this effect, we propose two methods: Random
Position Shifting and Context Perturbation, that we apply on batches during the
training process. The results show an improvement of $\approx$ 2\% in the
performance of the model on CoNLL03, UD_en, and TweeBank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toxic comments reduce the activity of volunteer editors on Wikipedia. (arXiv:2304.13568v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13568">
<div class="article-summary-box-inner">
<span><p>Wikipedia is one of the most successful collaborative projects in history. It
is the largest encyclopedia ever created, with millions of users worldwide
relying on it as the first source of information as well as for fact-checking
and in-depth research. As Wikipedia relies solely on the efforts of its
volunteer-editors, its success might be particularly affected by toxic speech.
In this paper, we analyze all 57 million comments made on user talk pages of
8.5 million editors across the six most active language editions of Wikipedia
to study the potential impact of toxicity on editors' behaviour. We find that
toxic comments consistently reduce the activity of editors, leading to an
estimated loss of 0.5-2 active days per user in the short term. This amounts to
multiple human-years of lost productivity when considering the number of active
contributors to Wikipedia. The effects of toxic comments are even greater in
the long term, as they significantly increase the risk of editors leaving the
project altogether. Using an agent-based model, we demonstrate that toxicity
attacks on Wikipedia have the potential to impede the progress of the entire
project. Our results underscore the importance of mitigating toxic speech on
collaborative platforms such as Wikipedia to ensure their continued success.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shades of meaning: Uncovering the geometry of ambiguous word representations through contextualised language models. (arXiv:2304.13597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13597">
<div class="article-summary-box-inner">
<span><p>Lexical ambiguity presents a profound and enduring challenge to the language
sciences. Researchers for decades have grappled with the problem of how
language users learn, represent and process words with more than one meaning.
Our work offers new insight into psychological understanding of lexical
ambiguity through a series of simulations that capitalise on recent advances in
contextual language models. These models have no grounded understanding of the
meanings of words at all; they simply learn to predict words based on the
surrounding context provided by other words. Yet, our analyses show that their
representations capture fine-grained meaningful distinctions between
unambiguous, homonymous, and polysemous words that align with lexicographic
classifications and psychological theorising. These findings provide
quantitative support for modern psychological conceptualisations of lexical
ambiguity and raise new challenges for understanding of the way that contextual
information shapes the meanings of words across different timescales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13620">
<div class="article-summary-box-inner">
<span><p>Automatic chart to text summarization is an effective tool for the visually
impaired people along with providing precise insights of tabular data in
natural language to the user. A large and well-structured dataset is always a
key part for data driven models. In this paper, we propose ChartSumm: a
large-scale benchmark dataset consisting of a total of 84,363 charts along with
their metadata and descriptions covering a wide range of topics and chart types
to generate short and long summaries. Extensive experiments with strong
baseline models show that even though these models generate fluent and
informative summaries by achieving decent scores in various automatic
evaluation metrics, they often face issues like suffering from hallucination,
missing out important data points, in addition to incorrect explanation of
complex trends in the charts. We also investigated the potential of expanding
ChartSumm to other languages using automated translation tools. These make our
dataset a challenging benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HausaNLP at SemEval-2023 Task 12: Leveraging African Low Resource TweetData for Sentiment Analysis. (arXiv:2304.13634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13634">
<div class="article-summary-box-inner">
<span><p>We present the findings of SemEval-2023 Task 12, a shared task on sentiment
analysis for low-resource African languages using Twitter dataset. The task
featured three subtasks; subtask A is monolingual sentiment classification with
12 tracks which are all monolingual languages, subtask B is multilingual
sentiment classification using the tracks in subtask A and subtask C is a
zero-shot sentiment classification. We present the results and findings of
subtask A, subtask B and subtask C. We also release the code on github. Our
goal is to leverage low-resource tweet data using pre-trained Afro-xlmr-large,
AfriBERTa-Large, Bert-base-arabic-camelbert-da-sentiment (Arabic-camelbert),
Multilingual-BERT (mBERT) and BERT models for sentiment analysis of 14 African
languages. The datasets for these subtasks consists of a gold standard
multi-class labeled Twitter datasets from these languages. Our results
demonstrate that Afro-xlmr-large model performed better compared to the other
models in most of the languages datasets. Similarly, Nigerian languages: Hausa,
Igbo, and Yoruba achieved better performance compared to other languages and
this can be attributed to the higher volume of data present in the languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering. (arXiv:2304.13649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13649">
<div class="article-summary-box-inner">
<span><p>Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a
question about an image whose answer does not lie in the image. This paper
presents a new pipeline for KI-VQA tasks, consisting of a retriever and a
reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval
framework in which documents and queries are encoded into a shared embedding
space using uni-modal (textual) and multi-modal encoders. We introduce an
iterative knowledge distillation approach that bridges the gap between the
representation spaces in these two encoders. Extensive evaluation on two
well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR
outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA,
respectively. Utilizing the passages retrieved by DEDR, we further introduce
MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating
a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and
each retrieved passage separately and uses all passages jointly in its decoder.
Compared to competitive baselines in the literature, this approach leads to
5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA
and FVQA, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Implicit Feedback to Improve Question Generation. (arXiv:2304.13664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13664">
<div class="article-summary-box-inner">
<span><p>Question Generation (QG) is a task of Natural Language Processing (NLP) that
aims at automatically generating questions from text. Many applications can
benefit from automatically generated questions, but often it is necessary to
curate those questions, either by selecting or editing them. This task is
informative on its own, but it is typically done post-generation, and, thus,
the effort is wasted. In addition, most existing systems cannot incorporate
this feedback back into them easily. In this work, we present a system, GEN,
that learns from such (implicit) feedback. Following a pattern-based approach,
it takes as input a small set of sentence/question pairs and creates patterns
which are then applied to new unseen sentences. Each generated question, after
being corrected by the user, is used as a new seed in the next iteration, so
more patterns are created each time. We also take advantage of the corrections
made by the user to score the patterns and therefore rank the generated
questions. Results show that GEN is able to improve by learning from both
levels of implicit feedback when compared to the version with no learning,
considering the top 5, 10, and 20 questions. Improvements go up from 10%,
depending on the metric and strategy used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13689">
<div class="article-summary-box-inner">
<span><p>Human-spoken questions are critical to evaluating the performance of spoken
question answering (SQA) systems that serve several real-world use cases
including digital assistants. We present a new large-scale community-shared SQA
dataset, HeySQuAD that consists of 76k human-spoken questions and 97k
machine-generated questions and corresponding textual answers derived from the
SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to
understand noisy spoken questions and answer the questions accurately. To this
end, we run extensive benchmarks on the human-spoken and machine-generated
questions to quantify the differences in noise from both sources and its
subsequent impact on the model and answering accuracy. Importantly, for the
task of SQA, where we want to answer human-spoken questions, we observe that
training using the transcribed human-spoken and original SQuAD questions leads
to significant improvements (12.51%) over training using only the original
SQuAD textual questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13712">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive and practical guide for practitioners and
end-users working with Large Language Models (LLMs) in their downstream natural
language processing (NLP) tasks. We provide discussions and insights into the
usage of LLMs from the perspectives of models, data, and downstream tasks.
Firstly, we offer an introduction and brief summary of current GPT- and
BERT-style LLMs. Then, we discuss the influence of pre-training data, training
data, and test data. Most importantly, we provide a detailed discussion about
the use and non-use cases of large language models for various natural language
processing tasks, such as knowledge-intensive tasks, traditional natural
language understanding tasks, natural language generation tasks, emergent
abilities, and considerations for specific tasks.We present various use cases
and non-use cases to illustrate the practical applications and limitations of
LLMs in real-world scenarios. We also try to understand the importance of data
and the specific challenges associated with each NLP task. Furthermore, we
explore the impact of spurious biases on LLMs and delve into other essential
considerations, such as efficiency, cost, and latency, to ensure a
comprehensive understanding of deploying LLMs in practice. This comprehensive
guide aims to provide researchers and practitioners with valuable insights and
best practices for working with LLMs, thereby enabling the successful
implementation of these models in a wide range of NLP tasks. A curated list of
practical guide resources of LLMs, regularly updated, can be found at
\url{https://github.com/Mooler0410/LLMsPracticalGuide}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13714">
<div class="article-summary-box-inner">
<span><p>Despite growing interest in using large language models (LLMs) in healthcare,
current explorations do not assess the real-world utility and safety of LLMs in
clinical settings. Our objective was to determine whether two LLMs can serve
information needs submitted by physicians as questions to an informatics
consultation service in a safe and concordant manner. Sixty six questions from
an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple
prompts. 12 physicians assessed the LLM responses' possibility of patient harm
and concordance with existing reports from an informatics consultation service.
Physician assessments were summarized based on majority vote. For no questions
did a majority of physicians deem either LLM response as harmful. For GPT-3.5,
responses to 8 questions were concordant with the informatics consult report,
20 discordant, and 9 were unable to be assessed. There were 29 responses with
no majority on "Agree", "Disagree", and "Unable to assess". For GPT-4,
responses to 13 questions were concordant, 15 discordant, and 3 were unable to
be assessed. There were 35 responses with no majority. Responses from both LLMs
were largely devoid of overt harm, but less than 20% of the responses agreed
with an answer from an informatics consultation service, responses contained
hallucinated references, and physicians were divided on what constitutes harm.
These results suggest that while general purpose LLMs are able to provide safe
and credible responses, they often do not meet the specific information need of
a given question. A definitive evaluation of the usefulness of LLMs in
healthcare settings will likely require additional research on prompt
engineering, calibration, and custom-tailoring of general purpose models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses. (arXiv:2109.03490v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03490">
<div class="article-summary-box-inner">
<span><p>It has been proposed that the order in which words are prepared for
production depends on the speaker's language. When producing the translation
equivalent of the small cat, speakers of German or Dutch select the
gender-marked determiner at a relatively early stage of production. Speakers of
French or Italian postpone the encoding of a determiner or adjective until the
phonological form of the noun is available. Hence, even though the words are
produced in the same order (e.g., die kleine Katze in German, le petit chat in
French), they are not planned in the same order and might require different
amounts of advanced planning prior to production onset. This distinction
between early and late selection languages was proposed to account for the
observation that speakers of Germanic and Slavic languages, but not of Romance
languages, are slower to name pictures in the context of a distractor word of a
different gender. Meta-analyses are conducted to provide the first direct test
of this cross-linguistic difference and to test a prediction of the late
selection hypothesis. They confirm the existence of the gender congruency
effect in German/Slavic languages and its absence in Romance languages when
target and distractor words are presented simultaneously. They do not allow
confirming the hypothesis that in the latter languages, a similar effect
emerges when the presentation of the distractor is delayed. Overall, these
analyses confirm the cross-linguistic difference but show that the evidence
available to date is not sufficient to confirm or reject the late selection
hypothesis as an explanation of this difference. We highlight specific
directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07657">
<div class="article-summary-box-inner">
<span><p>Sepsis is a life-threatening condition with organ dysfunction and is a
leading cause of death and critical illness worldwide. Even a few hours of
delay in the treatment of sepsis results in increased mortality. Early
detection of sepsis during emergency department triage would allow early
initiation of lab analysis, antibiotic administration, and other sepsis
treatment protocols. The purpose of this study was to compare sepsis detection
performance at ED triage (prior to the use of laboratory diagnostics) of the
standard sepsis screening algorithm (SIRS with source of infection) and a
machine learning algorithm trained on EHR triage data. A machine learning model
(KATE Sepsis) was developed using patient encounters with triage data from
16participating hospitals. KATE Sepsis and standard screening were
retrospectively evaluated on the adult population of 512,949 medical records.
KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of
71.09% (70.12% - 71.98%) and specificity of 94.81% (94.75% - 94.87%). Standard
screening demonstrates an AUC of 0.6826 (0.6774 - 0.6878) with sensitivity of
40.8% (39.71% - 41.86%) and specificity of95.72% (95.68% - 95.78%). The KATE
Sepsis model trained to detect sepsis demonstrates 77.67% (75.78% -79.42%)
sensitivity in detecting severe sepsis and 86.95% (84.2% - 88.81%) sensitivity
in detecting septic shock. The standard screening protocol demonstrates 43.06%
(41% - 45.87%) sensitivity in detecting severe sepsis and40% (36.55% - 43.26%)
sensitivity in detecting septic shock. Future research should focus on the
prospective impact of KATE Sepsis on administration of antibiotics, readmission
rate, morbidity and mortality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modelling with Pixels. (arXiv:2207.06991v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06991">
<div class="article-summary-box-inner">
<span><p>Language models are defined over a finite set of inputs, which creates a
vocabulary bottleneck when we attempt to scale the number of supported
languages. Tackling this bottleneck results in a trade-off between what can be
represented in the embedding matrix and computational issues in the output
layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which
suffers from neither of these issues. PIXEL is a pretrained language model that
renders text as images, making it possible to transfer representations across
languages based on orthographic similarity or the co-activation of pixels.
PIXEL is trained to reconstruct the pixels of masked patches instead of
predicting a distribution over tokens. We pretrain the 86M parameter PIXEL
model on the same English data as BERT and evaluate on syntactic and semantic
tasks in typologically diverse languages, including various non-Latin scripts.
We find that PIXEL substantially outperforms BERT on syntactic and semantic
processing tasks on scripts that are not found in the pretraining data, but
PIXEL is slightly weaker than BERT when working with Latin scripts.
Furthermore, we find that PIXEL is more robust than BERT to orthographic
attacks and linguistic code-switching, further confirming the benefits of
modelling language with pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification. (arXiv:2211.11158v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11158">
<div class="article-summary-box-inner">
<span><p>Concept Bottleneck Models (CBM) are inherently interpretable models that
factor model decisions into human-readable concepts. They allow people to
easily understand why a model is failing, a critical feature for high-stakes
applications. CBMs require manually specified concepts and often under-perform
their black box counterparts, preventing their broad adoption. We address these
shortcomings and are first to show how to construct high-performance CBMs
without manual specification of similar accuracy to black box models. Our
approach, Language Guided Bottlenecks (LaBo), leverages a language model,
GPT-3, to define a large space of possible bottlenecks. Given a problem domain,
LaBo uses GPT-3 to produce factual sentences about categories to form candidate
concepts. LaBo efficiently searches possible bottlenecks through a novel
submodular utility that promotes the selection of discriminative and diverse
information. Ultimately, GPT-3's sentential concepts can be aligned to images
using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a
highly effective prior for concepts important to visual recognition. In the
evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot
classification: they are 11.7% more accurate than black box linear probes at 1
shot and comparable with more data. Overall, LaBo demonstrates that inherently
interpretable models can be widely applied at similar, or better, performance
than black box approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06094">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated outstanding performance on a wide
range of tasks such as question answering and code generation. On a high level,
given an input, a language model can be used to automatically complete the
sequence in a statistically-likely way. Based on this, users prompt these
models with language instructions or examples, to implement a variety of
downstream tasks. Advanced prompting methods can even imply interaction between
the language model, a user, and external tools such as calculators. However, to
obtain state-of-the-art performance or adapt language models for specific
tasks, complex task- and model-specific programs have to be implemented, which
may still require ad-hoc interaction.
</p>
<p>Based on this, we present the novel idea of Language Model Programming (LMP).
LMP generalizes language model prompting from pure text prompts to an intuitive
combination of text prompting and scripting. Additionally, LMP allows
constraints to be specified over the language model output. This enables easy
adaption to many tasks while abstracting language model internals and providing
high-level semantics.
</p>
<p>To enable LMP, we implement LMQL(short for Language Model Query Language),
which leverages the constraints and control flow from an LMP prompt to generate
an efficient inference procedure that minimizes the number of expensive calls
to the underlying language model.
</p>
<p>We show that LMQL can capture a wide range of state-of-the-art prompting
methods in an intuitive way, especially facilitating interactive flows that are
challenging to implement with existing high-level APIs. Our evaluation shows
that we retain or increase the accuracy on several downstream tasks, while also
significantly reducing the required amount of computation or cost in the case
of pay-to-use APIs (26-85% cost savings).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11719">
<div class="article-summary-box-inner">
<span><p>Despite the great development of document summarisation techniques nowadays,
factual inconsistencies between the generated summaries and the original texts
still occur from time to time. This study explores the possibility of adopting
prompts to incorporate factual knowledge into generated summaries. We
specifically study prefix-tuning that uses a set of trainable continuous prefix
prompts together with discrete natural language prompts to aid summary
generation. Experimental results demonstrate that the trainable prefixes can
help the summarisation model extract information from discrete prompts
precisely, thus generating knowledge-preserving summaries that are factually
consistent with the discrete prompts. The ROUGE improvements of the generated
summaries indicate that explicitly adding factual knowledge into the
summarisation process could boost the overall performance, showing great
potential for applying it to other natural language processing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Sentence-Level Factuality of News and Bias of Media Outlets. (arXiv:2301.11850v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11850">
<div class="article-summary-box-inner">
<span><p>Predicting the factuality of news reporting and bias of media outlets is
surely relevant for automated news credibility and fact-checking. While prior
work has focused on the veracity of news, we propose a fine-grained reliability
analysis of the entire media. Specifically, we study the prediction of
sentence-level factuality of news reporting and bias of media outlets, which
may explain more accurately the overall reliability of the entire source. We
first manually produced a large sentence-level dataset, titled "FactNews",
composed of 6,191 sentences expertly annotated according to factuality and
media bias definitions from AllSides. As a result, baseline models for
sentence-level factuality prediction were presented by fine-tuning BERT.
Finally, due to the severity of fake news and political polarization in Brazil,
both dataset and baseline were proposed for Portuguese. However, our approach
may be applied to any other language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04729">
<div class="article-summary-box-inner">
<span><p>A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the
feasibility of stealing such information with only a few dollars, e.g.,
$\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games. (arXiv:2304.07258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07258">
<div class="article-summary-box-inner">
<span><p>Language models pre-trained on large self-supervised corpora, followed by
task-specific fine-tuning has become the dominant paradigm in NLP. These
pre-training datasets often have a one-to-many structure--e.g. in dialogue
there are many valid responses for a given context. However, only some of these
responses will be desirable in our downstream task. This raises the question of
how we should train the model such that it can emulate the desirable
behaviours, but not the undesirable ones. Current approaches train in a
one-to-one setup--only a single target response is given for a single dialogue
context--leading to models only learning to predict the average response, while
ignoring the full range of possible responses. Using text-based games as a
testbed, our approach, PASA, uses discrete latent variables to capture the
range of different behaviours represented in our larger pre-training dataset.
We then use knowledge distillation to distil the posterior probability
distribution into a student model. This probability distribution is far richer
than learning from only the hard targets of the dataset, and thus allows the
student model to benefit from the richer range of actions the teacher model has
learned. Results show up to 49% empirical improvement over the previous
state-of-the-art model on the Jericho Walkthroughs dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The language of sounds unheard: Exploring sensory semantic knowledge in large language models. (arXiv:2304.07830v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07830">
<div class="article-summary-box-inner">
<span><p>Semantic dimensions of sound have been playing a central role in
understanding the nature of auditory sensory experience as well as the broader
relation between perception, language, and meaning. Accordingly, and given the
recent proliferation of large language models (LLMs), here we asked whether
such models exhibit an organisation of perceptual semantics similar to those
observed in humans. Specifically, we prompted ChatGPT, a chatbot based on a
state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic
scales. We elicited multiple responses in separate chats, analogous to having
multiple human raters. ChatGPT generated semantic profiles that only partially
correlated with human ratings, yet showed robust agreement along well-known
psychophysical dimensions of musical sounds such as brightness (bright-dark)
and pitch height (deep-high). Exploratory factor analysis suggested the same
dimensionality but different spatial configuration of a latent factor space
between the chatbot and human ratings. Unexpectedly, the chatbot showed degrees
of internal variability that were comparable in magnitude to that of human
ratings. Our work highlights the potential of LLMs to capture salient
dimensions of human sensory experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09349">
<div class="article-summary-box-inner">
<span><p>Embodied AI focuses on the study and development of intelligent systems that
possess a physical or virtual embodiment (i.e. robots) and are able to
dynamically interact with their environment. Memory and control are the two
essential parts of an embodied system and usually require separate frameworks
to model each of them. In this paper, we propose a novel and generalizable
framework called LLM-Brain: using Large-scale Language Model as a robotic brain
to unify egocentric memory and control. The LLM-Brain framework integrates
multiple multimodal language models for robotic tasks, utilizing a zero-shot
learning approach. All components within LLM-Brain communicate using natural
language in closed-loop multi-round dialogues that encompass perception,
planning, control, and memory. The core of the system is an embodied LLM to
maintain egocentric memory and control the robot. We demonstrate LLM-Brain by
examining two downstream tasks: active exploration and embodied question
answering. The active exploration tasks require the robot to extensively
explore an unknown environment within a limited number of actions. Meanwhile,
the embodied question answering tasks necessitate that the robot answers
questions based on observations acquired during prior explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10428">
<div class="article-summary-box-inner">
<span><p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA
performances on a variety of NLP tasks, its performance on NER is still
significantly below supervised baselines. This is due to the gap between the
two tasks the NER and LLMs: the former is a sequence labeling task in nature
while the latter is a text-generation model.
</p>
<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the
gap by transforming the sequence labeling task to a generation task that can be
easily adapted by LLMs e.g., the task of finding location entities in the input
text "Columbus is a city" is transformed to generate the text sequence
"@@Columbus## is a city", where special tokens @@## marks the entity to
extract. To efficiently address the "hallucination" issue of LLMs, where LLMs
have a strong inclination to over-confidently label NULL inputs as entities, we
propose a self-verification strategy by prompting LLMs to ask itself whether
the extracted entities belong to a labeled entity tag.
</p>
<p>We conduct experiments on five widely adopted NER datasets, and GPT-NER
achieves comparable performances to fully supervised baselines, which is the
first time as far as we are concerned. More importantly, we find that GPT-NER
exhibits a greater ability in the low-resource and few-shot setups, when the
amount of training data is extremely scarce, GPT-NER performs significantly
better than supervised models. This demonstrates the capabilities of GPT-NER in
real-world NER applications where the number of labeled examples is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11490">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) excel in many tasks in 2023, but they still face
challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require
understanding agents' beliefs, goals, and mental states, are essential for
common-sense reasoning involving humans, making it crucial to enhance LLM
performance in this area. This study measures the ToM performance of GPT-4 and
three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates
the effectiveness of in-context learning in improving their ToM comprehension.
We evaluated prompts featuring two-shot chain of thought reasoning and
step-by-step thinking instructions. We found that LLMs trained with
Reinforcement Learning from Human Feedback (RLHF) (all models excluding
Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed
best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell
short of the 87% human accuracy on the test set. However, when supplied with
prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM
accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate
prompting enhances LLM ToM reasoning, and they underscore the context-dependent
nature of LLM cognitive capacities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. (arXiv:2304.11960v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11960">
<div class="article-summary-box-inner">
<span><p>Publicly available information contains valuable information for Cyber Threat
Intelligence (CTI). This can be used to prevent attacks that have already taken
place on other systems. Ideally, only the initial attack succeeds and all
subsequent ones are detected and stopped. But while there are different
standards to exchange this information, a lot of it is shared in articles or
blog posts in non-standardized ways. Manually scanning through multiple online
portals and news pages to discover new threats and extracting them is a
time-consuming task. To automize parts of this scanning process, multiple
papers propose extractors that use Natural Language Processing (NLP) to extract
Indicators of Compromise (IOCs) from documents. However, while this already
solves the problem of extracting the information out of documents, the search
for these documents is rarely considered. In this paper, a new focused crawler
is proposed called ThreatCrawl, which uses Bidirectional Encoder
Representations from Transformers (BERT)-based models to classify documents and
adapt its crawling path dynamically. While ThreatCrawl has difficulties to
classify the specific type of Open Source Intelligence (OSINT) named in texts,
e.g., IOC content, it can successfully find relevant documents and modify its
path accordingly. It yields harvest rates of up to 52%, which are, to the best
of our knowledge, better than the current state of the art.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-27 23:12:26.427615357 UTC">2023-04-27 23:12:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>