<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-31T01:30:00Z">08-31</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks For Document Layout Analysis. (arXiv:2308.15517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15517">
<div class="article-summary-box-inner">
<span><p>Document AI aims to automatically analyze documents by leveraging natural
language processing and computer vision techniques. One of the major tasks of
Document AI is document layout analysis, which structures document pages by
interpreting the content and spatial relationships of layout, image, and text.
This task can be image-centric, wherein the aim is to identify and label
various regions such as authors and paragraphs, or text-centric, where the
focus is on classifying individual words in a document. Although there are
increasingly sophisticated methods for improving layout analysis, doubts remain
about the extent to which their findings can be generalized to a broader
context. Specifically, prior work developed systems based on very different
architectures, such as transformer-based, graph-based, and CNNs. However, no
work has mentioned the effectiveness of these models in a comparative analysis.
Moreover, while language-independent Document AI models capable of knowledge
transfer have been developed, it remains to be investigated to what degree they
can effectively transfer knowledge. In this study, we aim to fill these gaps by
conducting a comparative evaluation of state-of-the-art models in document
layout analysis and investigating the potential of cross-lingual layout
analysis by utilizing machine translation techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection. (arXiv:2308.15711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15711">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have revolutionized the way we interact with
information, but they often generate nonfactual text, raising concerns about
their reliability. Previous methods use external knowledge as references for
text generation to enhance factuality but often struggle with the knowledge
mix-up(e.g., entity mismatch) of irrelevant references. Besides,as the length
of the output text grows, the randomness of sampling can escalate,
detrimentally impacting the factual accuracy of the generated text. In this
paper, we present DKGen, which divide the text generation process into an
iterative process. In each iteration, DKGen takes the input query, the
previously generated text and a subset of the reference passages as input to
generate short text. During the process, the subset is dynamically selected
from the full passage set based on their relevance to the previously generated
text and the query, largely eliminating the irrelevant references from input.
To further enhance DKGen's ability to correctly use these external knowledge,
DKGen distills the relevance order of reference passages to the cross-attention
distribution of decoder. We train and evaluate DKGen on a large-scale benchmark
dataset. Experiment results show that DKGen outperforms all baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying and Analyzing Entity-level Memorization in Large Language Models. (arXiv:2308.15727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15727">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been proven capable of memorizing their
training data, which can be extracted through specifically designed prompts. As
the scale of datasets continues to grow, privacy risks arising from
memorization have attracted increasing attention. Quantifying language model
memorization helps evaluate potential privacy risks. However, prior works on
quantifying memorization require access to the precise original data or incur
substantial computational overhead, making it difficult for applications in
real-world language models. To this end, we propose a fine-grained,
entity-level definition to quantify memorization with conditions and metrics
closer to real-world scenarios. In addition, we also present an approach for
efficiently extracting sensitive entities from autoregressive language models.
We conduct extensive experiments based on the proposed, probing language
models' ability to reconstruct sensitive entities under different settings. We
find that language models have strong memorization at the entity level and are
able to reproduce the training data even with partial leakages. The results
demonstrate that LLMs not only memorize their training data but also understand
associations between entities. These findings necessitate that trainers of LLMs
exercise greater prudence regarding model memorization, adopting memorization
mitigation techniques to preclude privacy violations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyberbullying Detection for Low-resource Languages and Dialects: Review of the State of the Art. (arXiv:2308.15745v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15745">
<div class="article-summary-box-inner">
<span><p>The struggle of social media platforms to moderate content in a timely
manner, encourages users to abuse such platforms to spread vulgar or abusive
language, which, when performed repeatedly becomes cyberbullying a social
problem taking place in virtual environments, yet with real-world consequences,
such as depression, withdrawal, or even suicide attempts of its victims.
Systems for the automatic detection and mitigation of cyberbullying have been
developed but, unfortunately, the vast majority of them are for the English
language, with only a handful available for low-resource languages. To estimate
the present state of research and recognize the needs for further development,
in this paper we present a comprehensive systematic survey of studies done so
far for automatic cyberbullying detection in low-resource languages. We
analyzed all studies on this topic that were available. We investigated more
than seventy published studies on automatic detection of cyberbullying or
related language in low-resource languages and dialects that were published
between around 2017 and January 2023. There are 23 low-resource languages and
dialects covered by this paper, including Bangla, Hindi, Dravidian languages
and others. In the survey, we identify some of the research gaps of previous
studies, which include the lack of reliable definitions of cyberbullying and
its relevant subcategories, biases in the acquisition, and annotation of data.
Based on recognizing those research gaps, we provide some suggestions for
improving the general research conduct in cyberbullying detection, with a
primary focus on low-resource languages. Based on those proposed suggestions,
we collect and release a cyberbullying dataset in the Chittagonian dialect of
Bangla and propose a number of initial ML solutions trained on that dataset. In
addition, pre-trained transformer-based the BanglaBERT model was also
attempted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Based MoE for Multitask Multilingual Machine Translation. (arXiv:2308.15772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15772">
<div class="article-summary-box-inner">
<span><p>Mixture-of-experts (MoE) architecture has been proven a powerful method for
diverse tasks in training deep models in many applications. However, current
MoE implementations are task agnostic, treating all tokens from different tasks
in the same manner. In this work, we instead design a novel method that
incorporates task information into MoE models at different granular levels with
shared dynamic task-based adapters. Our experiments and analysis show the
advantages of our approaches over the dense and canonical MoE models on
multi-task multilingual machine translations. With task-specific adapters, our
models can additionally generalize to new tasks efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HAlf-MAsked Model for Named Entity Sentiment analysis. (arXiv:2308.15793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15793">
<div class="article-summary-box-inner">
<span><p>Named Entity Sentiment analysis (NESA) is one of the most actively developing
application domains in Natural Language Processing (NLP). Social media NESA is
a significant field of opinion analysis since detecting and tracking sentiment
trends in the news flow is crucial for building various analytical systems and
monitoring the media image of specific people or companies. In this paper, we
study different transformers-based solutions NESA in RuSentNE-23 evaluation.
Despite the effectiveness of the BERT-like models, they can still struggle with
certain challenges, such as overfitting, which appeared to be the main obstacle
in achieving high accuracy on the RuSentNE-23 data. We present several
approaches to overcome this problem, among which there is a novel technique of
additional pass over given data with masked entity before making the final
prediction so that we can combine logits from the model when it knows the exact
entity it predicts sentiment for and when it does not. Utilizing this
technique, we ensemble multiple BERT- like models trained on different subsets
of data to improve overall performance. Our proposed model achieves the best
result on RuSentNE-23 evaluation data and demonstrates improved consistency in
entity-level sentiment analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15812">
<div class="article-summary-box-inner">
<span><p>Aligning large language models (LLMs) with human values and intents
critically involves the use of human or AI feedback. While dense feedback
annotations are expensive to acquire and integrate, sparse feedback presents a
structural design choice between ratings (e.g., score Response A on a scale of
1-7) and rankings (e.g., is Response A better than Response B?). In this work,
we analyze the effect of this design choice for the alignment and evaluation of
LLMs. We uncover an inconsistency problem wherein the preferences inferred from
ratings and rankings significantly disagree 60% for both human and AI
annotators. Our subsequent analysis identifies various facets of annotator
biases that explain this phenomena, such as human annotators would rate denser
responses higher while preferring accuracy during pairwise judgments. To our
surprise, we also observe that the choice of feedback protocol also has a
significant effect on the evaluation of aligned LLMs. In particular, we find
that LLMs that leverage rankings data for alignment (say model X) are preferred
over those that leverage ratings data (say model Y), with a rank-based
evaluation protocol (is X/Y's response better than reference response?) but not
with a rating-based evaluation protocol (score Rank X/Y's response on a scale
of 1-7). Our findings thus shed light on critical gaps in methods for
evaluating the real-world utility of language models and their strong
dependence on the feedback protocol used for alignment. Our code and data are
available at https://github.com/Hritikbansal/sparse_feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-grounded Natural Language Recommendation Explanation. (arXiv:2308.15813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15813">
<div class="article-summary-box-inner">
<span><p>Explanations accompanied by a recommendation can assist users in
understanding the decision made by recommendation systems, which in turn
increases a user's confidence and trust in the system. Recently, research has
focused on generating natural language explanations in a human-readable format.
Thus far, the proposed approaches leverage item reviews written by users, which
are often subjective, sparse in language, and unable to account for new items
that have not been purchased or reviewed before. Instead, we aim to generate
fact-grounded recommendation explanations that are objectively described with
item features while implicitly considering a user's preferences, based on the
user's purchase history. To achieve this, we propose a knowledge graph (KG)
approach to natural language explainable recommendation. Our approach draws on
user-item features through a novel collaborative filtering-based KG
representation to produce fact-grounded, personalized explanations, while
jointly learning user-item representations for recommendation scoring.
Experimental results show that our approach consistently outperforms previous
state-of-the-art models on natural language explainable recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards One-Shot Learning for Text Classification using Inductive Logic Programming. (arXiv:2308.15885v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15885">
<div class="article-summary-box-inner">
<span><p>With the ever-increasing potential of AI to perform personalised tasks, it is
becoming essential to develop new machine learning techniques which are
data-efficient and do not require hundreds or thousands of training data. In
this paper, we explore an Inductive Logic Programming approach for one-shot
text classification. In particular, we explore the framework of
Meta-Interpretive Learning (MIL), along with using common-sense background
knowledge extracted from ConceptNet. Results indicate that MIL can learn text
classification rules from a small number of training examples. Moreover, the
higher complexity of chosen examples, the higher accuracy of the outcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15906">
<div class="article-summary-box-inner">
<span><p>Our interdisciplinary study investigates how effectively U.S. laws confront
the challenges posed by Generative AI to human values. Through an analysis of
diverse hypothetical scenarios crafted during an expert workshop, we have
identified notable gaps and uncertainties within the existing legal framework
regarding the protection of fundamental values, such as autonomy, privacy,
dignity, diversity, equality, and physical/mental well-being. Constitutional
and civil rights, it appears, may not provide sufficient protection against
AI-generated discriminatory outputs. Furthermore, even if we exclude the
liability shield provided by Section 230, proving causation for defamation and
product liability claims is a challenging endeavor due to the intricate and
opaque nature of AI systems. To address the unique and unforeseeable threats
posed by Generative AI, we advocate for legal frameworks that evolve to
recognize new threat and provide proactive, auditable guidelines to industry
stakeholders. Addressing these issues requires deep interdisciplinary
collaborations to identify harms, values, and mitigation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15930">
<div class="article-summary-box-inner">
<span><p>Multi-modal large language models have garnered significant interest
recently. Though, most of the works focus on vision-language multi-modal models
providing strong capabilities in following vision-and-language instructions.
However, we claim that speech is also an important modality through which
humans interact with the world. Hence, it is crucial for a general-purpose
assistant to be able to follow multi-modal speech-and-language instructions. In
this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an
end-to-end trained large multi-modal speech-language model with cross-modal
conversational abilities, capable of following speech-and-language
instructions. Our early experiments show that LLaSM demonstrates a more
convenient and natural way for humans to interact with artificial intelligence.
Specifically, we also release a large Speech Instruction Following dataset
LLaSM-Audio-Instructions. Code and demo are available at
https://github.com/LinkSoul-AI/LLaSM and
https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions
dataset is available at
https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Multilabel Topic Classification in the Kyrgyz Language. (arXiv:2308.15952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15952">
<div class="article-summary-box-inner">
<span><p>Kyrgyz is a very underrepresented language in terms of modern natural
language processing resources. In this work, we present a new public benchmark
for topic classification in Kyrgyz, introducing a dataset based on collected
and annotated data from the news site 24.KG and presenting several baseline
models for news classification in the multilabel setting. We train and evaluate
both classical statistical and neural models, reporting the scores, discussing
the results, and proposing directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting. (arXiv:2308.15961v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15961">
<div class="article-summary-box-inner">
<span><p>The task of radiology reporting comprises describing and interpreting the
medical findings in radiographic images, including description of their
location and appearance. Automated approaches to radiology reporting require
the image to be encoded into a suitable token representation for input to the
language model. Previous methods commonly use convolutional neural networks to
encode an image into a series of image-level feature map representations.
However, the generated reports often exhibit realistic style but imperfect
accuracy. Inspired by recent works for image captioning in the general domain
in which each visual token corresponds to an object detected in an image, we
investigate whether using local tokens corresponding to anatomical structures
can improve the quality of the generated reports. We introduce a novel
adaptation of Faster R-CNN in which finding detection is performed for the
candidate bounding boxes extracted during anatomical structure localisation. We
use the resulting bounding box feature representations as our set of
finding-aware anatomical tokens. This encourages the extracted anatomical
tokens to be informative about the findings they contain (required for the
final task of radiology reporting). Evaluating on the MIMIC-CXR dataset of
chest X-Ray images, we show that task-aware anatomical tokens give
state-of-the-art performance when integrated into an automated reporting
pipeline, yielding generated reports with improved clinical accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MerA: Merging Pretrained Adapters For Few-Shot Learning. (arXiv:2308.15982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15982">
<div class="article-summary-box-inner">
<span><p>Adapter tuning, which updates only a few parameters, has become a mainstream
method for fine-tuning pretrained language models to downstream tasks. However,
it often yields subpar results in few-shot learning. AdapterFusion, which
assembles pretrained adapters using composition layers tailored to specific
tasks, is a possible solution but significantly increases trainable parameters
and deployment costs. Despite this, our preliminary study reveals that even
single adapters can outperform Adapterfusion in few-shot learning, urging us to
propose \textbf{\texttt{Merging Pretrained Adapters}} (MerA) that efficiently
incorporates pretrained adapters to a single model through model fusion.
Extensive experiments on two PLMs demonstrate that MerA achieves substantial
improvements compared to both single adapters and AdapterFusion. To further
enhance the capacity of MerA, we also introduce a simple yet effective
technique, referred to as the "\textit{same-track}" setting, that merges
adapters from the same track of pretraining tasks. With the implementation of
the "\textit{same-track}" setting, we observe even more impressive gains,
surpassing the performance of both full fine-tuning and adapter tuning by a
substantial margin, e.g., 3.5\% in MRPC and 5.0\% in MNLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15987">
<div class="article-summary-box-inner">
<span><p>In the era of large-scale language models, the substantial parameter size
poses significant challenges for deployment. Being a prevalent compression
technique, quantization has emerged as the mainstream practice to tackle this
issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and
activations in such bit widths). In this study, we propose a novel W4A8
post-training quantization method for the available open-sourced LLMs, which
combines the advantages of both two recipes. Therefore, we can leverage the
benefit in the I/O utilization of 4-bit weight quantization and the
acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces
notorious performance degradation. As a remedy, we involve layerwise activation
quantization strategies which feature a novel logarithmic equalization for most
intractable layers, and we combine them with fine-grained weight quantization.
Without whistles and bells, we eliminate the necessity for further fine-tuning
and obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and
LLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is
achievable for the deployment of large language models, fostering their
wide-spreading real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with Auxiliary Relations. (arXiv:2308.16055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16055">
<div class="article-summary-box-inner">
<span><p>Knowledge graph entity typing (KGET) is a task to predict the missing entity
types in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried to
solve the KGET task by introducing an auxiliary relation, 'hasType', to model
the relationship between entities and their types. However, a single auxiliary
relation has limited expressiveness for diverse entity-type patterns. We
improve the expressiveness of KGE methods by introducing multiple auxiliary
relations in this work. Similar entity types are grouped to reduce the number
of auxiliary relations and improve their capability to model entity-type
patterns with different granularities. With the presence of multiple auxiliary
relations, we propose a method adopting an Asynchronous learning scheme for
Entity Typing, named AsyncET, which updates the entity and type embeddings
alternatively to keep the learned entity embedding up-to-date and informative
for entity type prediction. Experiments are conducted on two commonly used KGET
datasets to show that the performance of KGE methods on the KGET task can be
substantially improved by the proposed multiple auxiliary relations and
asynchronous embedding learning. Furthermore, our method has a significant
advantage over state-of-the-art methods in model sizes and time complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap. (arXiv:2308.16060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16060">
<div class="article-summary-box-inner">
<span><p>We present Text-to-OverpassQL, a task designed to facilitate a natural
language interface for querying geodata from OpenStreetMap (OSM). The Overpass
Query Language (OverpassQL) allows users to formulate complex database queries
and is widely adopted in the OSM ecosystem. Generating Overpass queries from
natural language input serves multiple use-cases. It enables novice users to
utilize OverpassQL without prior knowledge, assists experienced users with
crafting advanced queries, and enables tool-augmented large language models to
access information stored in the OSM database. In order to assess the
performance of current sequence generation models on this task, we propose
OverpassNL, a dataset of 8,352 queries with corresponding natural language
inputs. We further introduce task specific evaluation metrics and ground the
evaluation of the Text-to-OverpassQL task by executing the queries against the
OSM database. We establish strong baselines by finetuning sequence-to-sequence
models and adapting large language models with in-context examples. The
detailed evaluation reveals strengths and weaknesses of the considered learning
strategies, laying the foundations for further research into the
Text-to-OverpassQL task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning. (arXiv:2308.16061v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16061">
<div class="article-summary-box-inner">
<span><p>Ransomware-as-a-service (RaaS) is increasing the scale and complexity of
ransomware attacks. Understanding the internal operations behind RaaS has been
a challenge due to the illegality of such activities. The recent chat leak of
the Conti RaaS operator, one of the most infamous ransomware operators on the
international scene, offers a key opportunity to better understand the inner
workings of such organizations. This paper analyzes the main topic discussions
in the Conti chat leak using machine learning techniques such as Natural
Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as
visualization strategies. Five discussion topics are found: 1) Business, 2)
Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer
Service/Problem Solving. Moreover, the distribution of topics among Conti
members shows that only 4% of individuals have specialized discussions while
almost all individuals (96%) are all-rounders, meaning that their discussions
revolve around the five topics. The results also indicate that a significant
proportion of Conti discussions are non-tech related. This study thus
highlights that running such large RaaS operations requires a workforce skilled
beyond technical abilities, with individuals involved in various tasks, from
management to customer service or problem solving. The discussion topics also
show that the organization behind the Conti RaaS oper5086933ator shares
similarities with a large firm. We conclude that, although RaaS represents an
example of specialization in the cybercrime industry, only a few members are
specialized in one topic, while the rest runs and coordinates the RaaS
operation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages. (arXiv:2308.16075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16075">
<div class="article-summary-box-inner">
<span><p>The study investigates the effectiveness of utilizing multimodal information
in Neural Machine Translation (NMT). While prior research focused on using
multimodal data in low-resource scenarios, this study examines how image
features impact translation when added to a large-scale, pre-trained unimodal
NMT system. Surprisingly, the study finds that images might be redundant in
this context. Additionally, the research introduces synthetic noise to assess
whether images help the model deal with textual noise. Multimodal models
slightly outperform text-only models in noisy settings, even with random
images. The study's experiments translate from English to Hindi, Bengali, and
Malayalam, outperforming state-of-the-art benchmarks significantly.
Interestingly, the effect of visual context varies with source text noise: no
visual context works best for non-noisy translations, cropped image features
are optimal for low noise, and full image features work better in high-noise
scenarios. This sheds light on the role of visual context, especially in noisy
settings, opening up a new research direction for Noisy Neural Machine
Translation in multimodal setups. The research emphasizes the importance of
combining visual and textual information for improved translation in various
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grandma Karl is 27 years old -- research agenda for pseudonymization of research data. (arXiv:2308.16109v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16109">
<div class="article-summary-box-inner">
<span><p>Accessibility of research data is critical for advances in many research
fields, but textual data often cannot be shared due to the personal and
sensitive information which it contains, e.g names or political opinions.
General Data Protection Regulation (GDPR) suggests pseudonymization as a
solution to secure open access to research data, but we need to learn more
about pseudonymization as an approach before adopting it for manipulation of
research data. This paper outlines a research agenda within pseudonymization,
namely need of studies into the effects of pseudonymization on unstructured
data in relation to e.g. readability and language assessment, as well as the
effectiveness of pseudonymization as a way of protecting writer identity, while
also exploring different ways of developing context-sensitive algorithms for
detection, labelling and replacement of personal information in unstructured
data. The recently granted project on pseudonymization Grandma Karl is 27 years
old addresses exactly those challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Response: Emergent analogical reasoning in large language models. (arXiv:2308.16118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16118">
<div class="article-summary-box-inner">
<span><p>In their recent Nature Human Behaviour paper, "Emergent analogical reasoning
in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that
"large language models such as GPT-3 have acquired an emergent ability to find
zero-shot solutions to a broad range of analogy problems." In this response, we
provide counterexamples of the letter string analogies. In our tests, GPT-3
fails to solve even the easiest variants of the problems presented in the
original paper. Zero-shot reasoning is an extraordinary claim that requires
extraordinary evidence. We do not see that evidence in our experiments. To
strengthen claims of humanlike reasoning such as zero-shot reasoning, it is
important that the field develop approaches that rule out data memorization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16137">
<div class="article-summary-box-inner">
<span><p>In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex tasks, they often face the
needs to conduct longer reasoning processes or understanding larger contexts.
In these situations, the length generalization failure of LLMs on long
sequences become more prominent. Most pre-training schemes truncate training
sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to
generate fluent texts, let alone carry out downstream tasks, after longer
contexts, even with relative positional encoding which is designed to cope with
this problem. Common solutions such as finetuning on longer corpora often
involves daunting hardware and time costs and requires careful training process
design. To more efficiently leverage the generation capacity of existing LLMs,
we theoretically and empirically investigate the main out-of-distribution (OOD)
factors contributing to this problem. Inspired by this diagnosis, we propose a
simple yet effective solution for on-the-fly length generalization,
LM-Infinite, which involves only a $\Lambda$-shaped attention mask and a
distance limit while requiring no parameter updates or learning. We find it
applicable to a variety of LLMs using relative-position encoding methods.
LM-Infinite is computational efficient with $O(n)$ time and space, and
demonstrates consistent fluency and generation quality to as long as 32k tokens
on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. On downstream
task such as passkey retrieval, it continues to work on inputs much longer than
training lengths where vanilla models fail immediately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16149">
<div class="article-summary-box-inner">
<span><p>We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric
foundation and instruction-tuned open generative large language models (LLMs).
The models are based on the GPT-3 decoder-only architecture and are pretrained
on a mixture of Arabic and English texts, including source code in various
programming languages. With 13 billion parameters, they demonstrate better
knowledge and reasoning capabilities in Arabic than any existing open Arabic
and multilingual models by a sizable margin, based on extensive evaluation.
Moreover, the models are competitive in English compared to English-centric
open models of similar size, despite being trained on much less English data.
We provide a detailed description of the training, the tuning, the safety
alignment, and the evaluation of the models. We release two open versions of
the model -- the foundation Jais model, and an instruction-tuned Jais-chat
variant -- with the aim of promoting research on Arabic LLMs. Available at
https://huggingface.co/inception-mbzuai/jais-13b-chat
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment. (arXiv:2308.16175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16175">
<div class="article-summary-box-inner">
<span><p>We introduce BSDetector, a method for detecting bad and speculative answers
from a pretrained Large Language Model by estimating a numeric confidence score
for any output it generated. Our uncertainty quantification technique works for
any LLM accessible only via a black-box API, and combines intrinsic and
extrinsic assessments of confidence into a single trustworthiness estimate for
any LLM response to a given prompt. Our method is extremely general and can
applied to all of the best LLMs available today (whose training data remains
unknown). By expending a bit of extra computation, users of any LLM API can now
get the same response as they would ordinarily, as well as a confidence
estimate that caution when not to trust this response. Experiments on both
closed and open-form Question-Answer benchmarks reveal that BSDetector more
accurately identifies incorrect LLM responses than alternative uncertainty
estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple
responses from the LLM and considering the one with the highest confidence
score, we can additionally obtain more accurate responses from the same LLM,
without any extra training steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marshall-Olkin Power-Law Distributions in Length-Frequency of Entities. (arXiv:1811.03325v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.03325">
<div class="article-summary-box-inner">
<span><p>Entities involve important concepts with concrete meanings and play important
roles in numerous linguistic tasks. Entities have different forms in different
linguistic tasks and researchers treat those different forms as different
concepts. In this paper, we are curious to know whether there are some common
characteristics that connect those different forms of entities. Specifically,
we investigate the underlying distributions of entities from different types
and different languages, trying to figure out some common characteristics
behind those diverse entities. After analyzing twelve datasets about different
types of entities and eighteen datasets about entities in different languages,
we find that while these entities are dramatically diverse from each other in
many aspects, their length-frequencies can be well characterized by a family of
Marshall-Olkin power-law (MOPL) distributions. We conduct experiments on those
thirty datasets about entities in different types and different languages, and
experimental results demonstrate that MOPL models characterize the
length-frequencies of entities much better than two state-of-the-art power-law
models and an alternative log-normal model. Experimental results also
demonstrate that MOPL models are scalable to the length-frequency of entities
in large-scale real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLSE: Corpus of Linguistically Significant Entities. (arXiv:2211.02423v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02423">
<div class="article-summary-box-inner">
<span><p>One of the biggest challenges of natural language generation (NLG) is the
proper handling of named entities. Named entities are a common source of
grammar mistakes such as wrong prepositions, wrong article handling, or
incorrect entity inflection. Without factoring linguistic representation, such
errors are often underrepresented when evaluating on a small set of arbitrarily
picked argument values, or when translating a dataset from a linguistically
simpler language, like English, to a linguistically complex language, like
Russian. However, for some applications, broadly precise grammatical
correctness is critical -- native speakers may find entity-related grammar
errors silly, jarring, or even offensive.
</p>
<p>To enable the creation of more linguistically diverse NLG datasets, we
release a Corpus of Linguistically Significant Entities (CLSE) annotated by
linguist experts. The corpus includes 34 languages and covers 74 different
semantic types to support various applications from airline ticketing to video
games. To demonstrate one possible use of CLSE, we produce an augmented version
of the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE's entities and a
small number of human translations, we create a linguistically representative
NLG evaluation benchmark in three languages: French (high-resource), Marathi
(low-resource), and Russian (highly inflected language). We establish quality
baselines for neural, template-based, and hybrid NLG systems and discuss the
strengths and weaknesses of each approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge Enhanced Pre-trained Language Models. (arXiv:2211.05994v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05994">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PLMs) which are trained on large text corpus via
self-supervised learning method, have yielded promising performance on various
tasks in Natural Language Processing (NLP). However, though PLMs with huge
parameters can effectively possess rich knowledge learned from massive training
text and benefit downstream tasks at the fine-tuning stage, they still have
some limitations such as poor reasoning ability due to the lack of external
knowledge. Research has been dedicated to incorporating knowledge into PLMs to
tackle these issues. In this paper, we present a comprehensive review of
Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear
insight into this thriving field. We introduce appropriate taxonomies
respectively for Natural Language Understanding (NLU) and Natural Language
Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide
the types of knowledge into four categories: linguistic knowledge, text
knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are
categorized into KG-based and retrieval-based methods. Finally, we point out
some promising future directions of KE-PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(QA)$^2$: Question Answering with Questionable Assumptions. (arXiv:2212.10003v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10003">
<div class="article-summary-box-inner">
<span><p>Naturally occurring information-seeking questions often contain questionable
assumptions -- assumptions that are false or unverifiable. Questions containing
questionable assumptions are challenging because they require a distinct answer
strategy that deviates from typical answers for information-seeking questions.
For instance, the question "When did Marie Curie discover Uranium?" cannot be
answered as a typical "when" question without addressing the false assumption
"Marie Curie discovered Uranium". In this work, we propose (QA)$^2$ (Question
Answering with Questionable Assumptions), an open-domain evaluation dataset
consisting of naturally occurring search engine queries that may or may not
contain questionable assumptions. To be successful on (QA)$^2$, systems must be
able to detect questionable assumptions and also be able to produce adequate
responses for both typical information-seeking questions and ones with
questionable assumptions. Through human rater acceptability on end-to-end QA
with (QA)$^2$, we find that current models do struggle with handling
questionable assumptions, leaving substantial headroom for progress.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Natural Language Understanding with Large Language Models and Answer Set Programming. (arXiv:2302.03780v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03780">
<div class="article-summary-box-inner">
<span><p>Humans understand language by extracting information (meaning) from
sentences, combining it with existing commonsense knowledge, and then
performing reasoning to draw conclusions. While large language models (LLMs)
such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a
variety of NLP tasks, they fall short in problems that require reasoning. They
also cannot reliably explain the answers generated for a given question. In
order to emulate humans better, we propose STAR, a framework that combines LLMs
with Answer Set Programming (ASP). We show how LLMs can be used to effectively
extract knowledge -- represented as predicates -- from language. Goal-directed
ASP is then employed to reliably reason over this knowledge. We apply the STAR
framework to three different NLU tasks requiring reasoning: qualitative
reasoning, mathematical reasoning, and goal-directed conversation. Our
experiments reveal that STAR is able to bridge the gap of reasoning in NLU
tasks, leading to significant performance improvements, especially for smaller
LLMs, i.e., LLMs with a smaller number of parameters. NLU applications
developed using the STAR framework are also explainable: along with the
predicates generated, a justification in the form of a proof tree can be
produced for a given output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going Beyond Nouns With Vision & Language Models Using Synthetic Data. (arXiv:2303.17590v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17590">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained Vision &amp; Language (VL) models have shown remarkable
performance in many applications, enabling replacing a fixed set of supported
classes with zero-shot open vocabulary reasoning over (almost arbitrary)
natural language prompts. However, recent works have uncovered a fundamental
weakness of these models. For example, their difficulty to understand Visual
Language Concepts (VLC) that go 'beyond nouns' such as the meaning of
non-object words (e.g., attributes, actions, relations, states, etc.), or
difficulty in performing compositional reasoning such as understanding the
significance of the order of the words in a sentence. In this work, we
investigate to which extent purely synthetic data could be leveraged to teach
these models to overcome such shortcomings without compromising their zero-shot
capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale
synthetic dataset and data generation codebase allowing to generate additional
suitable data to improve VLC understanding and compositional reasoning of VL
models. Additionally, we propose a general VL finetuning strategy for
effectively leveraging SyViC towards achieving these improvements. Our
extensive experiments and ablations on VL-Checklist, Winoground, and ARO
benchmarks demonstrate that it is possible to adapt strong pre-trained VL
models with synthetic data significantly enhancing their VLC understanding
(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their
zero-shot accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06767">
<div class="article-summary-box-inner">
<span><p>Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models. (arXiv:2305.06566v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06566">
<div class="article-summary-box-inner">
<span><p>Personalized content-based recommender systems have become indispensable
tools for users to navigate through the vast amount of content available on
platforms like daily news websites and book recommendation services. However,
existing recommenders face significant challenges in understanding the content
of items. Large language models (LLMs), which possess deep semantic
comprehension and extensive knowledge from pretraining, have proven to be
effective in various natural language processing tasks. In this study, we
explore the potential of leveraging both open- and closed-source LLMs to
enhance content-based recommendation. With open-source LLMs, we utilize their
deep layers as content encoders, enriching the representation of content at the
embedding level. For closed-source LLMs, we employ prompting techniques to
enrich the training data at the token level. Through comprehensive experiments,
we demonstrate the high effectiveness of both types of LLMs and show the
synergistic relationship between them. Notably, we observed a significant
relative improvement of up to 19.32% compared to existing state-of-the-art
recommendation models. These findings highlight the immense potential of both
open- and closed-source of LLMs in enhancing content-based recommendation
systems. We will make our code and LLM-generated data available for other
researchers to reproduce our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v3 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09438">
<div class="article-summary-box-inner">
<span><p>Message Passing Interface (MPI) plays a crucial role in distributed memory
parallelization across multiple nodes. However, parallelizing MPI code
manually, and specifically, performing domain decomposition, is a challenging,
error-prone task. In this paper, we address this problem by developing
MPI-RICAL, a novel data-driven, programming-assistance tool that assists
programmers in writing domain decomposition based distributed memory
parallelization code. Specifically, we train a supervised language model to
suggest MPI functions and their proper locations in the code on the fly. We
also introduce MPICodeCorpus, the first publicly available corpus of MPI-based
parallel programs that is created by mining more than 15,000 open-source
repositories on GitHub. Experimental results have been done on MPICodeCorpus
and more importantly, on a compiled benchmark of MPI-based parallel programs
for numerical computations that represent real-world scientific applications.
MPI-RICAL achieves F1 scores between 0.87-0.91 on these programs, demonstrating
its accuracy in suggesting correct MPI functions at appropriate code
locations.. The source code used in this work, as well as other relevant
sources, are available at:
https://github.com/Scientific-Computing-Lab-NRCN/MPI-rical
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17680">
<div class="article-summary-box-inner">
<span><p>Recent research has focused on using large language models (LLMs) to generate
explanations for hate speech through fine-tuning or prompting. Despite the
growing interest in this area, these generated explanations' effectiveness and
potential limitations remain poorly understood. A key concern is that these
explanations, generated by LLMs, may lead to erroneous judgments about the
nature of flagged content by both users and content moderators. For instance,
an LLM-generated explanation might inaccurately convince a content moderator
that a benign piece of content is hateful. In light of this, we propose an
analytical framework for examining hate speech explanations and conducted an
extensive survey on evaluating such explanations. Specifically, we prompted
GPT-3 to generate explanations for both hateful and non-hateful content, and a
survey was conducted with 2,400 unique respondents to evaluate the generated
explanations. Our findings reveal that (1) human evaluators rated the
GPT-generated explanations as high quality in terms of linguistic fluency,
informativeness, persuasiveness, and logical soundness, (2) the persuasive
nature of these explanations, however, varied depending on the prompting
strategy employed, and (3) this persuasiveness may result in incorrect
judgments about the hatefulness of the content. Our study underscores the need
for caution in applying LLM-generated explanations for content moderation. Code
and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are not Fair Evaluators. (arXiv:2305.17926v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17926">
<div class="article-summary-box-inner">
<span><p>In this paper, we uncover a systematic bias in the evaluation paradigm of
adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and
compare the quality of responses generated by candidate models. We find that
the quality ranking of candidate responses can be easily hacked by simply
altering their order of appearance in the context. This manipulation allows us
to skew the evaluation result, making one model appear considerably superior to
the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries
with ChatGPT as an evaluator. To address this issue, we propose a calibration
framework with three simple yet effective strategies: 1) Multiple Evidence
Calibration, which requires the evaluator model to generate multiple evaluation
evidence before assigning ratings; 2) Balanced Position Calibration, which
aggregates results across various orders to determine the final score; 3)
Human-in-the-Loop Calibration, which introduces a balanced position diversity
entropy to measure the difficulty of each example and seeks human assistance
when needed. We also manually annotate the "win/tie/lose" outcomes of responses
from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and
extensive experiments demonstrate that our approach successfully mitigates
evaluation bias, resulting in closer alignment with human judgments. We release
our code and human annotation at \url{https://github.com/i-Eval/FairEval} to
facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08303">
<div class="article-summary-box-inner">
<span><p>Dense retrieval (DR) converts queries and documents into dense embeddings and
measures the similarity between queries and documents in vector space. One of
the challenges in DR is the lack of domain-specific training data. While DR
models can learn from large-scale public datasets like MS MARCO through
transfer learning, evidence shows that not all DR models and domains can
benefit from transfer learning equally. Recently, some researchers have
resorted to large language models (LLMs) to improve the zero-shot and few-shot
DR models. However, the hard prompts or human-written prompts utilized in these
works cannot guarantee the good quality of generated weak queries. To tackle
this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task,
we leverage soft prompt-tuning to optimize a task-specific soft prompt on
limited ground truth data and then prompt the LLMs to tag unlabeled documents
with weak queries, yielding enough weak document-query pairs to train
task-specific dense retrievers. We design a filter to select high-quality
example document-query pairs in the prompt to further improve the quality of
weak tagged queries. To the best of our knowledge, there is no prior work
utilizing soft prompt tuning to augment DR models. The experiments demonstrate
that SPTAR outperforms the unsupervised baselines BM25 and the recently
proposed LLMs-based augmentation method for DR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering. (arXiv:2307.15745v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15745">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) has the potential to make the Internet more
accessible in an interactive way, allowing people who cannot see images to ask
questions about them. However, multiple studies have shown that people who are
blind or have low-vision prefer image explanations that incorporate the context
in which an image appears, yet current VQA datasets focus on images in
isolation. We argue that VQA models will not fully succeed at meeting people's
needs unless they take context into account. To further motivate and analyze
the distinction between different contexts, we introduce Context-VQA, a VQA
dataset that pairs images with contexts, specifically types of websites (e.g.,
a shopping website). We find that the types of questions vary systematically
across contexts. For example, images presented in a travel context garner 2
times more "Where?" questions, and images on social media and news garner 2.8
and 1.8 times more "Who?" questions than the average. We also find that context
effects are especially important when participants can't see the image. These
results demonstrate that context affects the types of questions asked and that
VQA models should be context-sensitive to better meet people's needs,
especially in accessibility settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. (arXiv:2308.03188v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03188">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable performance across
a wide array of NLP tasks. However, their efficacy is undermined by undesired
and inconsistent behaviors, including hallucination, unfaithful reasoning, and
toxic content. A promising approach to rectify these flaws is self-correction,
where the LLM itself is prompted or guided to fix problems in its own output.
Techniques leveraging automated feedback -- either produced by the LLM itself
or some external system -- are of particular interest as they are a promising
way to make LLM-based solutions more practical and deployable with minimal
human feedback. This paper presents a comprehensive review of this emerging
class of techniques. We analyze and taxonomize a wide array of recent work
utilizing these strategies, including training-time, generation-time, and
post-hoc correction. We also summarize the major applications of this strategy
and conclude by discussing future directions and challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.05361">
<div class="article-summary-box-inner">
<span><p>We present WeaverBird, an intelligent dialogue system designed specifically
for the finance domain. Our system harnesses a large language model of GPT
architecture that has been tuned using extensive corpora of finance-related
text. As a result, our system possesses the capability to understand complex
financial queries, such as "How should I manage my investments during
inflation?", and provide informed responses. Furthermore, our system
incorporates a local knowledge base and a search engine to retrieve relevant
information. The final responses are conditioned on the search results and
include proper citations to the sources, thus enjoying an enhanced credibility.
Through a range of finance-related questions, we have demonstrated the superior
performance of our system compared to other models. To experience our system
firsthand, users can interact with our live demo at
https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at
https://www.youtube.com/watch?v=fyV2qQkX6Tc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06032">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) could enhance access to the legal system.
However, empirical research on their effectiveness in conducting legal tasks is
scant. We study securities cases involving cryptocurrencies as one of numerous
contexts where AI could support the legal process, studying LLMs' legal
reasoning and drafting capabilities. We examine whether a) an LLM can
accurately determine which laws are potentially being violated from a fact
pattern, and b) whether there is a difference in juror decision-making based on
complaints written by a lawyer compared to an LLM. We feed fact patterns from
real-life cases to GPT-3.5 and evaluate its ability to determine correct
potential violations from the scenario and exclude spurious violations. Second,
we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's
legal reasoning skills proved weak, though we expect improvement in future
models, particularly given the violations it suggested tended to be correct (it
merely missed additional, correct violations). GPT-3.5 performed better at
legal drafting, and jurors' decisions were not statistically significantly
associated with the author of the document upon which they based their
decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks,
they would be unable to replace lawyers at this stage. However, their drafting
skills (though, perhaps, still inferior to lawyers), could provide access to
justice for more individuals by reducing the cost of legal services. Our
research is the first to systematically study LLMs' legal drafting and
reasoning capabilities in litigation, as well as in securities law and
cryptocurrency-related misconduct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09662">
<div class="article-summary-box-inner">
<span><p>Larger language models (LLMs) have taken the world by storm with their
massive multi-tasking capabilities simply by optimizing over a next-word
prediction objective. With the emergence of their properties and encoded
knowledge, the risk of LLMs producing harmful outputs increases, making them
unfit for scalable deployment for the public. In this work, we propose a new
safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that
even widely deployed models are susceptible to the Chain of Utterances-based
(CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and
ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We
also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in
generating harmful responses in more than 86% of the red-teaming attempts.
Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It
constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting,
we collect a dataset that consists of 1.9K harmful questions covering a wide
range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2)
SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the
safety alignment of LLMs by minimizing the negative log-likelihood over helpful
responses and penalizing over harmful responses by gradient accent over sample
loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely
aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the
utility of the baseline models (TruthfulQA, MMLU, and BBH).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10390">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) have demonstrated commendable performance
across a myriad of domains and tasks, existing LLMs still exhibit a palpable
deficit in handling multimodal functionalities, especially for the Spoken
Question Answering (SQA) task which necessitates precise alignment and deep
interaction between speech and text features. To address the SQA challenge on
LLMs, we initially curated the free-form and open-ended LibriSQA dataset from
Librispeech, comprising Part I with natural conversational formats and Part II
encompassing multiple-choice questions followed by answers and analytical
segments. Both parts collectively include 107k SQA pairs that cover various
topics. Given the evident paucity of existing speech-text LLMs, we propose a
lightweight, end-to-end framework to execute the SQA task on the LibriSQA,
witnessing significant results. By reforming ASR into the SQA format, we
further substantiate our framework's capability in handling ASR tasks. Our
empirical findings bolster the LLMs' aptitude for aligning and comprehending
multimodal information, paving the way for the development of universal
multimodal LLMs. The dataset and demo can be found at
https://github.com/ZihanZhaoSJTU/LibriSQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations. (arXiv:2308.13081v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13081">
<div class="article-summary-box-inner">
<span><p>This document presents adequate formal terminology for the mathematical
specification of a subset of Agent Based Models (ABMs) in the field of
Demography. The simulation of the targeted ABMs follows a fixed-step
single-clocked pattern. The proposed terminology further improves the model
understanding and can act as a stand-alone methodology for the specification
and optionally the documentation of a significant set of (demographic) ABMs.
Nevertheless, it is imaginable the this terminology probably with further
extensions can be merged with the largely-informal widely-used model
documentation and communication O.D.D. protocol [Grimm and et al., 2020,
Amouroux et al., 2010] to reduce many sources of ambiguity, hindering model
replications by other modelers. A published demographic model documentation,
largely simplified version of the Lone Parent Model [Gostoli and Silverman,
2020] is separately published in [Elsheikh, 2023b] as illustration for the
formal terminology. The model was implemented in the Julia language [Elsheikh,
2023a] based on the Agents.jl julia package [Datseris et al., 2022].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13399">
<div class="article-summary-box-inner">
<span><p>We propose an unsupervised method to extract keywords and keyphrases from
texts based on a pre-trained language model (LM) and Shannon's information
maximization. Specifically, our method extracts phrases having the highest
conditional entropy under the LM. The resulting set of keyphrases turns out to
solve a relevant information-theoretic problem: if provided as side
information, it leads to the expected minimal binary code length in compressing
the text using the LM and an entropy encoder. Alternately, the resulting set is
an approximation via a causal LM to the set of phrases that minimize the
entropy of the text when conditioned upon it. Empirically, the method provides
results comparable to the most commonly used methods in various keyphrase
extraction benchmark challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.14306">
<div class="article-summary-box-inner">
<span><p>Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model's performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.14359">
<div class="article-summary-box-inner">
<span><p>Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Text-based Dialogue State Tracker for Spoken Dialogues. (arXiv:2308.15053v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15053">
<div class="article-summary-box-inner">
<span><p>Although there have been remarkable advances in dialogue systems through the
dialogue systems technology competition (DSTC), it remains one of the key
challenges to building a robust task-oriented dialogue system with a speech
interface. Most of the progress has been made for text-based dialogue systems
since there are abundant datasets with written corpora while those with spoken
dialogues are very scarce. However, as can be seen from voice assistant systems
such as Siri and Alexa, it is of practical importance to transfer the success
to spoken dialogues. In this paper, we describe our engineering effort in
building a highly successful model that participated in the speech-aware
dialogue systems technology challenge track in DSTC11. Our model consists of
three major modules: (1) automatic speech recognition error correction to
bridge the gap between the spoken and the text utterances, (2) text-based
dialogue system (D3ST) for estimating the slots and values using slot
descriptions, and (3) post-processing for recovering the error of the estimated
slot value. Our experiments show that it is important to use an explicit
automatic speech recognition error correction module, post-processing, and data
augmentation to adapt a text-based dialogue state tracker for spoken dialogue
corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT. (arXiv:2308.15122v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15122">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks (SNNs) offer a promising avenue to implement deep
neural networks in a more energy-efficient way. However, the network
architectures of existing SNNs for language tasks are too simplistic, and deep
architectures have not been fully explored, resulting in a significant
performance gap compared to mainstream transformer-based networks such as BERT.
To this end, we improve a recently-proposed spiking transformer (i.e.,
Spikformer) to make it possible to process language tasks and propose a
two-stage knowledge distillation method for training it, which combines
pre-training by distilling knowledge from BERT with a large collection of
unlabelled texts and fine-tuning with task-specific instances via knowledge
distillation again from the BERT fine-tuned on the same training examples.
Through extensive experimentation, we show that the models trained with our
method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve
comparable results to BERTs on text classification tasks for both English and
Chinese with much less energy consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions. (arXiv:2308.15214v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15214">
<div class="article-summary-box-inner">
<span><p>We demonstrate an embodied conversational agent that can function as a
receptionist and generate a mixture of open and closed-domain dialogue along
with facial expressions, by using a large language model (LLM) to develop an
engaging conversation. We deployed the system onto a Furhat robot, which is
highly expressive and capable of using both verbal and nonverbal cues during
interaction. The system was designed specifically for the National Robotarium
to interact with visitors through natural conversations, providing them with
information about the facilities, research, news, upcoming events, etc. The
system utilises the state-of-the-art GPT-3.5 model to generate such information
along with domain-general conversations and facial expressions based on prompt
engineering.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-31 23:11:02.842351006 UTC">2023-08-31 23:11:02 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>