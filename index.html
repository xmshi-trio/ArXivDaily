<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-18T01:30:00Z">10-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Quantum-Classical Machine Learning for Sentiment Analysis. (arXiv:2310.10672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10672">
<div class="article-summary-box-inner">
<span><p>The collaboration between quantum computing and classical machine learning
offers potential advantages in natural language processing, particularly in the
sentiment analysis of human emotions and opinions expressed in large-scale
datasets. In this work, we propose a methodology for sentiment analysis using
hybrid quantum-classical machine learning algorithms. We investigate quantum
kernel approaches and variational quantum circuit-based classifiers and
integrate them with classical dimension reduction techniques such as PCA and
Haar wavelet transform. The proposed methodology is evaluated using two
distinct datasets, based on English and Bengali languages. Experimental results
show that after dimensionality reduction of the data, performance of the
quantum-based hybrid algorithms were consistent and better than classical
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors. (arXiv:2310.10673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10673">
<div class="article-summary-box-inner">
<span><p>This paper shows how LLMs (Large Language Models) may be used to estimate a
summary of the emotional state associated with piece of text. The summary of
emotional state is a dictionary of words used to describe emotion together with
the probability of the word appearing after a prompt comprising the original
text and an emotion eliciting tail. Through emotion analysis of Amazon product
reviews we demonstrate emotion descriptors can be mapped into a PCA type space.
It was hoped that text descriptions of actions to improve a current text
described state could also be elicited through a tail prompt. Experiment seemed
to indicate that this is not straightforward to make work. This failure put our
hoped for selection of action via choosing the best predict ed outcome via
comparing emotional responses out of reach for the moment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp. (arXiv:2310.10675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10675">
<div class="article-summary-box-inner">
<span><p>In the era of digital transformation, customer service is of paramount
importance to the success of organizations, and to meet the growing demand for
immediate responses and personalized assistance 24 hours a day, chatbots have
become a promising tool to solve these problems. Currently, there are many
companies that need to provide these solutions to their customers, which
motivates us to study this problem and offer a suitable solution. The objective
of this study is to develop a chatbot based on natural language processing to
improve customer satisfaction and improve the quality of service provided by
the company through WhatsApp. The solution focuses on creating a chatbot that
efficiently and effectively handles user queries. A literature review related
to existing chatbots has been conducted, analyzing methodological approaches,
artificial intelligence techniques and quality attributes used in the
implementation of chatbots. The results found highlight that chatbots based on
natural language processing enable fast and accurate responses, which improves
the efficiency of customer service, as chatbots contribute to customer
satisfaction by providing accurate answers and quick solutions to their queries
at any time. Some authors point out that artificial intelligence techniques,
such as machine learning, improve the learning and adaptability of chatbots as
user interactions occur, so a good choice of appropriate natural language
understanding technologies is essential for optimal chatbot performance. The
results of this study will provide a solid foundation for the design and
development of effective chatbots for customer service, ensuring a satisfactory
user experience and thus meeting the needs of the organization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMs as Potential Brainstorming Partners for Math and Science Problems. (arXiv:2310.10677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10677">
<div class="article-summary-box-inner">
<span><p>With the recent rise of widely successful deep learning models, there is
emerging interest among professionals in various math and science communities
to see and evaluate the state-of-the-art models' abilities to collaborate on
finding or solving problems that often require creativity and thus
brainstorming. While a significant chasm still exists between current
human-machine intellectual collaborations and the resolution of complex math
and science problems, such as the six unsolved Millennium Prize Problems, our
initial investigation into this matter reveals a promising step towards
bridging the divide. This is due to the recent advancements in Large Language
Models (LLMs). More specifically, we conduct comprehensive case studies to
explore both the capabilities and limitations of the current state-of-the-art
LLM, notably GPT-4, in collective brainstorming with humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10679">
<div class="article-summary-box-inner">
<span><p>We use a large-scale experiment (N=8000) to determine whether GPT-4 can
replicate cross-cultural differences in the Big Five, measured using the
Ten-Item Personality Inventory. We used the US and South Korea as the cultural
pair, given that prior research suggests substantial personality differences
between people from these two countries. We manipulated the target of the
simulation (US vs. Korean), the language of the inventory (English vs. Korean),
and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4
replicated the cross-cultural differences for each factor. However, mean
ratings had an upward bias and exhibited lower variation than in the human
samples, as well as lower structural validity. Overall, we provide preliminary
evidence that LLMs can aid cross-cultural psychological research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10683">
<div class="article-summary-box-inner">
<span><p>We study how to perform unlearning, i.e. forgetting undesirable
(mis)behaviors, on large language models (LLMs). We show at least three
scenarios of aligning LLMs with human preferences can benefit from unlearning:
(1) removing harmful responses, (2) erasing copyright-protected content as
requested, and (3) eliminating hallucinations. Unlearning, as an alignment
technique, has three advantages. (1) It only requires negative (e.g. harmful)
examples, which are much easier and cheaper to collect (e.g. via red teaming or
user reporting) than positive (e.g. helpful and often human-written) examples
required in RLHF (RL from human feedback). (2) It is computationally efficient.
(3) It is especially effective when we know which training samples cause the
misbehavior. To the best of our knowledge, our work is among the first to
explore LLM unlearning. We are also among the first to formulate the settings,
goals, and evaluations in LLM unlearning. We show that if practitioners only
have limited resources, and therefore the priority is to stop generating
undesirable outputs rather than to try to generate desirable outputs,
unlearning is particularly appealing. Despite only having negative samples, our
ablation study shows that unlearning can still achieve better alignment
performance than RLHF with just 2% of its computational time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Tree-search Ability of Large Language Models. (arXiv:2310.10686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10686">
<div class="article-summary-box-inner">
<span><p>Large Language Models have excelled in remarkable reasoning capabilities with
advanced prompting techniques, but they fall short on tasks that require
exploration, strategic foresight, and sequential decision-making. Recent works
propose to utilize external programs to define search logic, such that LLMs can
perform passive tree search to solve more challenging reasoning tasks. Though
impressive results have been achieved, there are several fundamental
limitations of these approaches. First, passive tree searches are not efficient
as they usually require multiple rounds of LLM API calls to solve one single
problem. Moreover, passive search methods are not flexible since they need
task-specific program designs. Then a natural question arises: can we maintain
the tree-search capability of LLMs without the aid of external programs, and
can still generate responses that clearly demonstrate the process of a
tree-structure search? To this end, we propose a new concept called autonomous
tree-search ability of LLM, which can automatically generate a response
containing search trajectories for the correct answer. Concretely, we perform
search trajectories using capable LLM API via a fixed system prompt, allowing
them to perform autonomous tree-search (ATS) right out of the box. Experiments
on 4 puzzle games demonstrate our method can achieve huge improvements. The
ATS-BFS method outperforms the Chain of Thought approach by achieving an
average accuracy improvement of 33%. Compared to Tree of Thoughts, it requires
65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.
Moreover, we have collected data using the ATS prompt method and fine-tuned
LLaMA. This approach yield a greater improvement compared to the ones
fine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an
average of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10688">
<div class="article-summary-box-inner">
<span><p>Motivated by recent advances in large language models for Natural Language
Processing (NLP), we design a time-series foundation model for forecasting
whose out-of-the-box zero-shot performance on a variety of public datasets
comes close to the accuracy of state-of-the-art supervised forecasting models
for each individual dataset. Our model is based on pretraining a
patched-decoder style attention model on a large time-series corpus, and can
work well across different forecasting history lengths, prediction lengths and
temporal granularities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10690">
<div class="article-summary-box-inner">
<span><p>Student modeling is central to many educational technologies as it enables
the prediction of future learning outcomes and targeted instructional
strategies. However, open-ended learning environments pose challenges for
accurately modeling students due to the diverse behaviors exhibited by students
and the absence of a well-defined set of learning skills. To approach these
challenges, we explore the application of Large Language Models (LLMs) for
in-context student modeling in open-ended learning environments. We introduce a
novel framework, LLM-SS, that leverages LLMs for synthesizing student's
behavior. More concretely, given a particular student's solving attempt on a
reference task as observation, the goal is to synthesize the student's attempt
on a target task. Our framework can be combined with different LLMs; moreover,
we fine-tune LLMs using domain-specific expertise to boost their understanding
of domain background and student behaviors. We evaluate several concrete
methods based on LLM-SS using the StudentSyn benchmark, an existing student's
attempt synthesis benchmark in visual programming. Experimental results show a
significant improvement compared to baseline methods included in the StudentSyn
benchmark. Furthermore, our method using the fine-tuned Llama2-70B model
improves noticeably compared to using the base model and becomes on par with
using the state-of-the-art GPT-4 model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation. (arXiv:2310.10698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10698">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have showcased remarkable prowess in code
generation. However, automated code generation is still challenging since it
requires a high-level semantic mapping between natural language requirements
and codes. Most existing LLMs-based approaches for code generation rely on
decoder-only causal language models often treate codes merely as plain text
tokens, i.e., feeding the requirements as a prompt input, and outputing code as
flat sequence of tokens, potentially missing the rich semantic features
inherent in source code. To bridge this gap, this paper proposes the "Semantic
Chain-of-Thought" approach to intruduce semantic information of code, named
SeCoT. Our motivation is that the semantic information of the source code (\eg
data flow and control flow) describes more precise program execution behavior,
intention and function. By guiding LLM consider and integrate semantic
information, we can achieve a more granular understanding and representation of
code, enhancing code generation accuracy. Meanwhile, while traditional
techniques leveraging such semantic information require complex static or
dynamic code analysis to obtain features such as data flow and control flow,
SeCoT demonstrates that this process can be fully automated via the intrinsic
capabilities of LLMs (i.e., in-context learning), while being generalizable and
applicable to challenging domains. While SeCoT can be applied with different
LLMs, this paper focuses on the powerful GPT-style models: ChatGPT(close-source
model) and WizardCoder(open-source model). The experimental study on three
popular DL benchmarks (i.e., HumanEval, HumanEval-ET and MBPP) shows that SeCoT
can achieves state-of-the-art performance, greatly improving the potential for
large models and code generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10701">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimized Tokenization for Transcribed Error Correction. (arXiv:2310.10704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10704">
<div class="article-summary-box-inner">
<span><p>The challenges facing speech recognition systems, such as variations in
pronunciations, adverse audio conditions, and the scarcity of labeled data,
emphasize the necessity for a post-processing step that corrects recurring
errors. Previous research has shown the advantages of employing dedicated error
correction models, yet training such models requires large amounts of labeled
data which is not easily obtained. To overcome this limitation, synthetic
transcribed-like data is often utilized, however, bridging the distribution gap
between transcribed errors and synthetic noise is not trivial. In this paper,
we demonstrate that the performance of correction models can be significantly
increased by training solely using synthetic data. Specifically, we empirically
show that: (1) synthetic data generated using the error distribution derived
from a set of transcribed data outperforms the common approach of applying
random perturbations; (2) applying language-specific adjustments to the
vocabulary of a BPE tokenizer strike a balance between adapting to unseen
distributions and retaining knowledge of transcribed errors. We showcase the
benefits of these key observations, and evaluate our approach using multiple
languages, speech recognition systems and prominent speech recognition
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10706">
<div class="article-summary-box-inner">
<span><p>To explore how humans can best leverage LLMs for writing and how interacting
with these models affects feelings of ownership and trust in the writing
process, we compared common human-AI interaction types (e.g., guiding system,
selecting from system outputs, post-editing outputs) in the context of
LLM-assisted news headline generation. While LLMs alone can generate
satisfactory news headlines, on average, human control is needed to fix
undesirable model outputs. Of the interaction methods, guiding and selecting
model output added the most benefit with the lowest cost (in time and effort).
Further, AI assistance did not harm participants' perception of control
compared to freeform editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning. (arXiv:2310.10707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10707">
<div class="article-summary-box-inner">
<span><p>Paraphrasing of offensive content is a better alternative to content removal
and helps improve civility in a communication environment. Supervised
paraphrasers; however, rely heavily on large quantities of labelled data to
help preserve meaning and intent. They also retain a large portion of the
offensiveness of the original content, which raises questions on their overall
usability. In this paper we aim to assist practitioners in developing usable
paraphrasers by exploring In-Context Learning (ICL) with large language models
(LLMs), i.e., using a limited number of input-label demonstration pairs to
guide the model in generating desired outputs for specific queries. Our study
focuses on key factors such as -- number and order of demonstrations, exclusion
of prompt instruction, and reduction in measured toxicity. We perform
principled evaluation on three datasets, including our proposed Context-Aware
Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite
paraphrases, and additional dialogue context. We evaluate our approach using
two closed source and one open source LLM. Our results reveal that ICL is
comparable to supervised methods in generation quality, while being
qualitatively better by 25% on human evaluation and attaining lower toxicity by
76%. Also, ICL-based paraphrasers only show a slight reduction in performance
even with just 10% training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning. (arXiv:2310.10735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10735">
<div class="article-summary-box-inner">
<span><p>Maintaining a consistent persona is a key quality for any open domain
dialogue system. Current state-of-the-art systems do this by training agents
with supervised learning or online reinforcement learning (RL). However,
systems trained with supervised learning often lack consistency as they are
never punished for uttering contradictions. Additional training with RL can
alleviate some of these issues, however the training process is expensive.
Instead, we propose an offline RL framework to improve the persona consistency
of dialogue systems. Our framework allows us to combine the advantages of
previous methods as we can inexpensively train our model on existing data as in
supervised learning, while punishing and rewarding specific utterances as in
RL. We also introduce a simple importance sampling method to reduce the
variance of importance weights in offline RL training which we call
Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic
and human evaluations show that our framework improves both the persona
consistency and dialogue quality of a state-of-the-art social chatbot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards reducing hallucination in extracting information from financial reports using Large Language Models. (arXiv:2310.10760v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10760">
<div class="article-summary-box-inner">
<span><p>For a financial analyst, the question and answer (Q\&amp;A) segment of the
company financial report is a crucial piece of information for various analysis
and investment decisions. However, extracting valuable insights from the Q\&amp;A
section has posed considerable challenges as the conventional methods such as
detailed reading and note-taking lack scalability and are susceptible to human
errors, and Optical Character Recognition (OCR) and similar techniques
encounter difficulties in accurately processing unstructured transcript text,
often missing subtle linguistic nuances that drive investor decisions. Here, we
demonstrate the utilization of Large Language Models (LLMs) to efficiently and
rapidly extract information from earnings report transcripts while ensuring
high accuracy transforming the extraction process as well as reducing
hallucination by combining retrieval-augmented generation technique as well as
metadata. We evaluate the outcomes of various LLMs with and without using our
proposed approach based on various objective metrics for evaluating Q\&amp;A
systems, and empirically demonstrate superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10765">
<div class="article-summary-box-inner">
<span><p>Rapid progress has been made in instruction-learning for image editing with
natural-language instruction, as exemplified by InstructPix2Pix. In
biomedicine, such methods can be applied to counterfactual image generation,
which helps differentiate causal structure from spurious correlation and
facilitate robust image interpretation for disease progression modeling.
However, generic image-editing models are ill-suited for the biomedical domain,
and counterfactual biomedical image generation is largely underexplored. In
this paper, we present BiomedJourney, a novel method for counterfactual
biomedical image generation by instruction-learning from multimodal patient
journeys. Given a patient with two biomedical images taken at different time
points, we use GPT-4 to process the corresponding imaging reports and generate
a natural language description of disease progression. The resulting triples
(prior image, progression description, new image) are then used to train a
latent diffusion model for counterfactual biomedical image generation. Given
the relative scarcity of image time series data, we introduce a two-stage
curriculum that first pretrains the denoising network using the much more
abundant single image-report pairs (with dummy prior image), and then continues
training using the counterfactual triples. Experiments using the standard
MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive
battery of tests on counterfactual medical image generation, BiomedJourney
substantially outperforms prior state-of-the-art methods in instruction image
editing and medical image generation such as InstructPix2Pix and RoentGen. To
facilitate future study in counterfactual medical generation, we plan to
release our instruction-learning code and pretrained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali. (arXiv:2310.10781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10781">
<div class="article-summary-box-inner">
<span><p>This paper presents the system that we have developed while solving this
shared task on violence inciting text detection in Bangla. We explain both the
traditional and the recent approaches that we have used to make our models
learn. Our proposed system helps to classify if the given text contains any
threat. We studied the impact of data augmentation when there is a limited
dataset available. Our quantitative results show that finetuning a
multilingual-e5-base model performed the best in our task compared to other
transformer-based architectures. We obtained a macro F1 of 68.11\% in the test
set and our performance in this shared task is ranked at 23 in the leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Models of Speech Infer Universal Articulatory Kinematics. (arXiv:2310.10788v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10788">
<div class="article-summary-box-inner">
<span><p>Self-Supervised Learning (SSL) based models of speech have shown remarkable
performance on a range of downstream tasks. These state-of-the-art models have
remained blackboxes, but many recent studies have begun "probing" models like
HuBERT, to correlate their internal representations to different aspects of
speech. In this paper, we show "inference of articulatory kinematics" as
fundamental property of SSL models, i.e., the ability of these models to
transform acoustics into the causal articulatory dynamics underlying the speech
signal. We also show that this abstraction is largely overlapping across the
language of the data used to train the model, with preference to the language
with similar phonological system. Furthermore, we show that with simple affine
transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable
across speakers, even across genders, languages, and dialects, showing the
generalizability of this property. Together, these results shed new light on
the internals of SSL models that are critical to their superior performance,
and open up new avenues into language-agnostic universal models for speech
engineering, that are interpretable and grounded in speech science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT. (arXiv:2310.10803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10803">
<div class="article-summary-box-inner">
<span><p>Data-driven unit discovery in self-supervised learning (SSL) of speech has
embarked on a new era of spoken language processing. Yet, the discovered units
often remain in phonetic space, limiting the utility of SSL representations.
Here, we demonstrate that a syllabic organization emerges in learning
sentence-level representation of speech. In particular, we adopt
"self-distillation" objective to fine-tune the pretrained HuBERT with an
aggregator token that summarizes the entire sentence. Without any supervision,
the resulting model draws definite boundaries in speech, and the
representations across frames show salient syllabic structures. We demonstrate
that this emergent structure largely corresponds to the ground truth syllables.
Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating
sentence-level representation of speech. When compared to previous models, our
model outperforms in both unsupervised syllable discovery and learning
sentence-level representation. Together, we demonstrate that the
self-distillation of HuBERT gives rise to syllabic organization without relying
on external labels or modalities, and potentially provides novel data-driven
units for spoken language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks. (arXiv:2310.10830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10830">
<div class="article-summary-box-inner">
<span><p>It is commonly perceived that online fake news and reliable news exhibit
stark differences in writing styles, such as the use of sensationalist versus
objective language. However, we emphasize that style-related features can also
be exploited for style-based attacks. Notably, the rise of powerful Large
Language Models (LLMs) has enabled malicious users to mimic the style of
trustworthy news outlets at minimal cost. Our analysis reveals that
LLM-camouflaged fake news content leads to substantial performance degradation
of state-of-the-art text-based detectors (up to 38% decrease in F1 Score),
posing a significant challenge for automated detection in online ecosystems. To
address this, we introduce SheepDog, a style-agnostic fake news detector robust
to news writing styles. SheepDog achieves this adaptability through
LLM-empowered news reframing, which customizes each article to match different
writing styles using style-oriented reframing prompts. By employing
style-agnostic training, SheepDog enhances its resilience to stylistic
variations by maximizing prediction consistency across these diverse
reframings. Furthermore, SheepDog extracts content-focused veracity
attributions from LLMs, where the news content is evaluated against a set of
fact-checking rationales. These attributions provide supplementary information
and potential interpretability that assist veracity prediction. On three
benchmark datasets, empirical results show that SheepDog consistently yields
significant improvements over competitive baselines and enhances robustness
against LLM-empowered style attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. (arXiv:2310.10844v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10844">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are swiftly advancing in architecture and
capability, and as they integrate more deeply into complex systems, the urgency
to scrutinize their security properties grows. This paper surveys research in
the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield
of trustworthy ML, combining the perspectives of Natural Language Processing
and Security. Prior work has shown that even safety-aligned LLMs (via
instruction tuning and reinforcement learning through human feedback) can be
susceptible to adversarial attacks, which exploit weaknesses and mislead AI
systems, as evidenced by the prevalence of `jailbreak' attacks on models like
ChatGPT and Bard. In this survey, we first provide an overview of large
language models, describe their safety alignment, and categorize existing
research based on various learning structures: textual-only attacks,
multi-modal attacks, and additional attack methods specifically targeting
complex systems, such as federated learning or multi-agent systems. We also
offer comprehensive remarks on works that focus on the fundamental sources of
vulnerabilities and potential defenses. To make this field more accessible to
newcomers, we present a systematic review of existing works, a structured
typology of adversarial attack concepts, and additional resources, including
slides for presentations on related topics at the 62nd Annual Meeting of the
Association for Computational Linguistics (ACL'24).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoTFormer: More Tokens With Attention Make Up For Less Depth. (arXiv:2310.10845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10845">
<div class="article-summary-box-inner">
<span><p>The race to continually develop ever larger and deeper foundational models is
underway. However, techniques like the Chain-of-Thought (CoT) method continue
to play a pivotal role in achieving optimal downstream performance. In this
work, we establish an approximate parallel between using chain-of-thought and
employing a deeper transformer. Building on this insight, we introduce
CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to
achieve capacity comparable to a deeper model. Our empirical findings
demonstrate the effectiveness of CoTFormers, as they significantly outperform
larger standard transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts. (arXiv:2310.10865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10865">
<div class="article-summary-box-inner">
<span><p>Recent studies show that traditional fairytales are rife with harmful gender
biases. To help mitigate these gender biases in fairytales, this work aims to
assess learned biases of language models by evaluating their robustness against
gender perturbations. Specifically, we focus on Question Answering (QA) tasks
in fairytales. Using counterfactual data augmentation to the FairytaleQA
dataset, we evaluate model robustness against swapped gender character
information, and then mitigate learned biases by introducing counterfactual
gender stereotypes during training time. We additionally introduce a novel
approach that utilizes the massive vocabulary of language models to support
text genres beyond fairytales. Our experimental results suggest that models are
sensitive to gender perturbations, with significant performance drops compared
to the original testing set. However, when first fine-tuned on a counterfactual
training dataset, models are less sensitive to the later introduced anti-gender
stereotyped text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models. (arXiv:2310.10873v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10873">
<div class="article-summary-box-inner">
<span><p>In-context learning is a promising paradigm that utilizes in-context examples
as prompts for the predictions of large language models. These prompts are
crucial for achieving strong performance. However, since the prompts need to be
sampled from a large volume of annotated examples, finding the right prompt may
result in high annotation costs. To address this challenge, this paper
introduces an influence-driven selective annotation method that aims to
minimize annotation costs while improving the quality of in-context examples.
The essence of our method is to select a pivotal subset from a large-scale
unlabeled data pool to annotate for the subsequent sampling of prompts.
Specifically, a directed graph is first constructed to represent unlabeled
data. Afterward, the influence of candidate unlabeled subsets is quantified
with a diffusion process. A simple yet effective greedy algorithm for unlabeled
data selection is lastly introduced. It iteratively selects the data if it
provides a maximum marginal gain with respect to quantified influence. Compared
with previous efforts on selective annotations, our influence-driven method
works in an end-to-end manner, avoids an intractable explicit balance between
data diversity and representativeness, and enjoys theoretical support.
Experiments confirm the superiority of the proposed method on various
benchmarks, achieving better performance under lower time consumption during
subset selection. The project page is available at
https://skzhang1.github.io/IDEAL/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT. (arXiv:2310.10903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10903">
<div class="article-summary-box-inner">
<span><p>The rapid proliferation of ChatGPT has incited debates regarding its impact
on human writing. Amid concerns about declining writing standards, this study
investigates the role of ChatGPT in facilitating academic writing, especially
among language learners. Using a case study approach, this study examines the
experiences of Kailing, a doctoral student, who integrates ChatGPT throughout
their academic writing process. The study employs activity theory as a lens for
understanding writing with generative AI tools and data analyzed includes
semi-structured interviews, writing samples, and GPT logs. Results indicate
that Kailing effectively collaborates with ChatGPT across various writing
stages while preserving her distinct authorial voice and agency. This
underscores the potential of AI tools such as ChatGPT to enhance academic
writing for language learners without overshadowing individual authenticity.
This case study offers a critical exploration of how ChatGPT is utilized in the
academic writing process and the preservation of a student's authentic voice
when engaging with the tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain. (arXiv:2310.10920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10920">
<div class="article-summary-box-inner">
<span><p>As LLMs have become increasingly popular, they have been used in almost every
field. But as the application for LLMs expands from generic fields to narrow,
focused science domains, there exists an ever-increasing gap in ways to
evaluate their efficacy in those fields. For the benchmarks that do exist, a
lot of them focus on questions that don't require proper understanding of the
subject in question. In this paper, we present NuclearQA, a human-made
benchmark of 100 questions to evaluate language models in the nuclear domain,
consisting of a varying collection of questions that have been specifically
designed by experts to test the abilities of language models. We detail our
approach and show how the mix of several types of questions makes our benchmark
uniquely capable of evaluating models in the nuclear domain. We also present
our own evaluation metric for assessing LLM's performances due to the
limitations of existing ones. Our experiments on state-of-the-art models
suggest that even the best LLMs perform less than satisfactorily on our
benchmark, demonstrating the scientific knowledge gap of existing LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio. (arXiv:2310.10922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10922">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has been used to leverage unlabelled data, improving
accuracy and generalisation of speech systems through the training of
representation models. While many recent works have sought to produce effective
representations across a variety of acoustic domains, languages, modalities and
even simultaneous speakers, these studies have all been limited to
single-channel audio recordings. This paper presents Spatial HuBERT, a
self-supervised speech representation model that learns both acoustic and
spatial information pertaining to a single speaker in a potentially noisy
environment by using multi-channel audio inputs. Spatial HuBERT learns
representations that outperform state-of-the-art single-channel speech
representations on a variety of spatial downstream tasks, particularly in
reverberant and noisy environments. We also demonstrate the utility of the
representations learned by Spatial HuBERT on a speech localisation downstream
task. Along with this paper, we publicly release a new dataset of 100 000
simulated first-order ambisonics room impulse responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Transformer Architecture for Natural Language Processing. (arXiv:2310.10930v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10930">
<div class="article-summary-box-inner">
<span><p>Transformer is a state-of-the-art model in the field of natural language
processing (NLP). Current NLP models primarily increase the number of
transformers to improve processing performance. However, this technique
requires a lot of training resources such as computing capacity. In this paper,
a novel structure of Transformer is proposed. It is featured by full layer
normalization, weighted residual connection, positional encoding exploiting
reinforcement learning, and zero masked self-attention. The proposed
Transformer model, which is called Enhanced Transformer, is validated by the
bilingual evaluation understudy (BLEU) score obtained with the Multi30k
translation dataset. As a result, the Enhanced Transformer achieves 202.96%
higher BLEU score as compared to the original transformer with the translation
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti. (arXiv:2310.10935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10935">
<div class="article-summary-box-inner">
<span><p>As voice assistants cement their place in our technologically advanced
society, there remains a need to cater to the diverse linguistic landscape,
including colloquial forms of low-resource languages. Our study introduces the
first-ever comprehensive dataset for intent detection and slot filling in
formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples
across 10 unique intents. Our analysis reveals the robustness of large language
models for tackling downstream tasks with inadequate data. The GPT-3.5 model
achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot
filling for colloquial Bangla.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts. (arXiv:2310.10941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10941">
<div class="article-summary-box-inner">
<span><p>Depression is a mental health disorder that has a profound impact on people's
lives. Recent research suggests that signs of depression can be detected in the
way individuals communicate, both through spoken words and written texts. In
particular, social media posts are a rich and convenient text source that we
may examine for depressive symptoms. The Beck Depression Inventory (BDI)
Questionnaire, which is frequently used to gauge the severity of depression, is
one instrument that can aid in this study. We can narrow our study to only
those symptoms since each BDI question is linked to a particular depressive
symptom. It's important to remember that not everyone with depression exhibits
all symptoms at once, but rather a combination of them. Therefore, it is
extremely useful to be able to determine if a sentence or a piece of
user-generated content is pertinent to a certain condition. With this in mind,
the eRisk 2023 Task 1 was designed to do exactly that: assess the relevance of
different sentences to the symptoms of depression as outlined in the BDI
questionnaire. This report is all about how our team, Mason-NLP, participated
in this subtask, which involved identifying sentences related to different
depression symptoms. We used a deep learning approach that incorporated
MentalBERT, RoBERTa, and LSTM. Despite our efforts, the evaluation results were
lower than expected, underscoring the challenges inherent in ranking sentences
from an extensive dataset about depression, which necessitates both appropriate
methodological choices and significant computational resources. We anticipate
that future iterations of this shared task will yield improved results as our
understanding and techniques evolve.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEQ: Trainable Equivalent Transformation for Quantization of LLMs. (arXiv:2310.10944v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10944">
<div class="article-summary-box-inner">
<span><p>As large language models (LLMs) become more prevalent, there is a growing
need for new and improved quantization methods that can meet the
computationalast layer demands of these modern architectures while maintaining
the accuracy. In this paper, we present TEQ, a trainable equivalent
transformation that preserves the FP32 precision of the model output while
taking advantage of low-precision quantization, especially 3 and 4 bits
weight-only quantization. The training process is lightweight, requiring only
1K steps and fewer than 0.1 percent of the original model's trainable
parameters. Furthermore, the transformation does not add any computational
overhead during inference. Our results are on-par with the state-of-the-art
(SOTA) methods on typical LLMs. Our approach can be combined with other methods
to achieve even better performance. The code is available at
https://github.com/intel/neural-compressor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A State-Vector Framework for Dataset Effects. (arXiv:2310.10955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10955">
<div class="article-summary-box-inner">
<span><p>The impressive success of recent deep neural network (DNN)-based systems is
significantly influenced by the high-quality datasets used in training.
However, the effects of the datasets, especially how they interact with each
other, remain underexplored. We propose a state-vector framework to enable
rigorous studies in this direction. This framework uses idealized probing test
results as the bases of a vector space. This framework allows us to quantify
the effects of both standalone and interacting datasets. We show that the
significant effects of some commonly-used language understanding datasets are
characteristic and are concentrated on a few linguistic dimensions.
Additionally, we observe some ``spill-over'' effects: the datasets could impact
the models along dimensions that may seem unrelated to the intended tasks. Our
state-vector framework paves the way for a systematic understanding of the
dataset effects, a crucial component in responsible and robust model
development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computing the optimal keyboard through a geometric analysis of the English language. (arXiv:2310.10956v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10956">
<div class="article-summary-box-inner">
<span><p>In the context of a group project for the course COMSW4995 002 - Geometric
Data Analysis, we bring our attention to the design of fast-typing keyboards.
Leveraging some geometric tools in an optimization framework allowed us to
propose novel keyboard layouts that offer a faster typing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models. (arXiv:2310.10962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10962">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been proven to be effective in learning better
sentence representations. However, to train a contrastive learning model, large
numbers of labeled sentences are required to construct positive and negative
pairs explicitly, such as those in natural language inference (NLI) datasets.
Unfortunately, acquiring sufficient high-quality labeled data can be both
time-consuming and resource-intensive, leading researchers to focus on
developing methods for learning unsupervised sentence representations. As there
is no clear relationship between these unstructured randomly-sampled sentences,
building positive and negative pairs over them is tricky and problematic. To
tackle these challenges, in this paper, we propose SemCSR, a semantic-aware
contrastive sentence representation framework. By leveraging the generation and
evaluation capabilities of large language models (LLMs), we can automatically
construct a high-quality NLI-style corpus without any human annotation, and
further incorporate the generated sentence pairs into learning a contrastive
sentence representation model. Extensive experiments and comprehensive analyses
demonstrate the effectiveness of our proposed framework for learning a better
sentence representation with LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset. (arXiv:2310.10967v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10967">
<div class="article-summary-box-inner">
<span><p>The need for high-quality data has been a key issue hindering the research of
dialogue tasks. Recent studies try to build datasets through manual, web
crawling, and large pre-trained models. However, man-made data is expensive and
data collected from the internet often includes generic responses, meaningless
statements, and toxic dialogues. Automatic data generation through large models
is a cost-effective method, but for open-domain multimodal dialogue tasks,
there are still three drawbacks: 1) There is currently no open-source large
model that can accept multimodal input; 2) The content generated by the model
lacks interpretability; 3) The generated data is usually difficult to quality
control and require extensive resource to collect. To alleviate the significant
human and resource expenditure in data collection, we propose a Multimodal Data
Construction Framework (MDCF). MDCF designs proper prompts to spur the
large-scale pre-trained language model to generate well-formed and satisfactory
content. Additionally, MDCF also automatically provides explanation for a given
image and its corresponding dialogue, which can provide a certain degree of
interpretability and facilitate manual follow-up quality inspection. Based on
this, we release an Explanatory Multimodal Open-Domain dialogue dataset
(EXMODD). Experiments indicate a positive correlation between the model's
ability to generate accurate understandings and high-quality responses. Our
code and data can be found at https://github.com/poplpr/EXMODD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instructive Dialogue Summarization with Query Aggregations. (arXiv:2310.10981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10981">
<div class="article-summary-box-inner">
<span><p>Conventional dialogue summarization methods directly generate summaries and
do not consider user's specific interests. This poses challenges in cases where
the users are more focused on particular topics or aspects. With the
advancement of instruction-finetuned language models, we introduce
instruction-tuning to dialogues to expand the capability set of dialogue
summarization models. To overcome the scarcity of instructive dialogue
summarization data, we propose a three-step approach to synthesize high-quality
query-based summarization triples. This process involves summary-anchored query
generation, query filtering, and query-based summary generation. By training a
unified model called InstructDS (Instructive Dialogue Summarization) on three
summarization datasets with multi-purpose instructive triples, we expand the
capability of dialogue summarization models. We evaluate our method on four
datasets, including dialogue summarization and dialogue reading comprehension.
Experimental results show that our approach outperforms the state-of-the-art
models and even models with larger sizes. Additionally, our model exhibits
higher generalizability and faithfulness, as confirmed by human subjective
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correction Focused Language Model Training for Speech Recognition. (arXiv:2310.11003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11003">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have been commonly adopted to boost the performance of
automatic speech recognition (ASR) particularly in domain adaptation tasks.
Conventional way of LM training treats all the words in corpora equally,
resulting in suboptimal improvements in ASR performance. In this work, we
introduce a novel correction focused LM training approach which aims to
prioritize ASR fallible words. The word-level ASR fallibility score,
representing the likelihood of ASR mis-recognition, is defined and shaped as a
prior word distribution to guide the LM training. To enable correction focused
training with text-only corpora, large language models (LLMs) are employed as
fallibility score predictors and text generators through multi-task
fine-tuning. Experimental results for domain adaptation tasks demonstrate the
effectiveness of our proposed method. Compared with conventional LMs,
correction focused training achieves up to relatively 5.5% word error rate
(WER) reduction in sufficient text scenarios. In insufficient text scenarios,
LM training with LLM-generated text achieves up to relatively 13% WER
reduction, while correction focused training further obtains up to relatively
6% WER reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction. (arXiv:2310.11016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11016">
<div class="article-summary-box-inner">
<span><p>Recent advances in multimodal pre-trained models have significantly improved
information extraction from visually-rich documents (VrDs), in which named
entity recognition (NER) is treated as a sequence-labeling task of predicting
the BIO entity tags for tokens, following the typical setting of NLP. However,
BIO-tagging scheme relies on the correct order of model inputs, which is not
guaranteed in real-world NER on scanned VrDs where text are recognized and
arranged by OCR systems. Such reading order issue hinders the accurate marking
of entities by BIO-tagging scheme, making it impossible for sequence-labeling
methods to predict correct named entities. To address the reading order issue,
we introduce Token Path Prediction (TPP), a simple prediction head to predict
entity mentions as token sequences within documents. Alternative to token
classification, TPP models the document layout as a complete directed graph of
tokens, and predicts token paths within the graph as entities. For better
evaluation of VrD-NER systems, we also propose two revised benchmark datasets
of NER on scanned documents which can reflect real-world scenarios. Experiment
results demonstrate the effectiveness of our method, and suggest its potential
to be a universal solution to various information extraction tasks on
documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation. (arXiv:2310.11026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11026">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation of text generation is essential for improving the
accuracy of generation tasks. In light of the current trend towards
increasingly larger decoder-based language models, we investigate automatic
evaluation methods based on such models for text generation. This paper
compares various methods, including tuning with encoder-based models and large
language models under equal conditions, on two different tasks, machine
translation evaluation and semantic textual similarity, in two languages,
Japanese and English. Experimental results show that compared to the tuned
encoder-based models, the tuned decoder-based models perform poorly. The
analysis of the causes for this suggests that the decoder-based models focus on
surface word sequences and do not capture meaning. It is also revealed that
in-context learning of very large decoder-based models such as ChatGPT makes it
difficult to identify fine-grained semantic differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance. (arXiv:2310.11035v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11035">
<div class="article-summary-box-inner">
<span><p>Although lyrics represent an essential component of music, few music
information processing studies have been conducted on the characteristics of
lyricists. Because these characteristics may be valuable for musical
applications, such as recommendations, they warrant further study. We
considered a potential method that extracts features representing the
characteristics of lyricists from lyrics. Because these features must be
identified prior to extraction, we focused on lyricists with easily
identifiable features. We believe that it is desirable for singers to perform
unique songs that share certain characteristics specific to the singer.
Accordingly, we hypothesized that lyricists account for the unique
characteristics of the singers they write lyrics for. In other words,
lyric-lyricist classification performance or the ease of capturing the features
of a lyricist from the lyrics may depend on the variety of singers. In this
study, we observed a relationship between lyricist-singer entropy or the
variety of singers associated with a single lyricist and lyric-lyricist
classification performance. As an example, the lyricist-singer entropy is
minimal when the lyricist writes lyrics for only one singer. In our
experiments, we grouped lyricists among five groups in terms of lyricist-singer
entropy and assessed the lyric-lyricist classification performance within each
group. Consequently, the best F1 score was obtained for the group with the
lowest lyricist-singer entropy. Our results suggest that further analyses of
the features contributing to lyric-lyricist classification performance on the
lowest lyricist-singer entropy group may improve the feature extraction task
for lyricists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation. (arXiv:2310.11049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11049">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the SemEval-2023 for Task 6 on
LegalEval: Understanding Legal Texts. Our submission concentrated on three
subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment
Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation
(CJPE) for Task-C2. We conducted various experiments on these subtasks and
presented the results in detail, including data statistics and methodology. It
is worth noting that legal tasks, such as those tackled in this research, have
been gaining importance due to the increasing need to automate legal analysis
and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$,
and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the
leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning. (arXiv:2310.11053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11053">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made unprecedented breakthroughs, yet their
increasing integration into everyday life might raise societal risks due to
generated unethical content. Despite extensive study on specific issues like
bias, the intrinsic values of LLMs remain largely unexplored from a moral
philosophy perspective. This work delves into ethical values utilizing Moral
Foundation Theory. Moving beyond conventional discriminative evaluations with
poor reliability, we propose DeNEVIL, a novel prompt generation algorithm
tailored to dynamically exploit LLMs' value vulnerabilities and elicit the
violation of ethics in a generative manner, revealing their underlying value
inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset
comprising 2,397 prompts covering 500+ value principles, and then benchmark the
intrinsic values across a spectrum of LLMs. We discovered that most models are
essentially misaligned, necessitating further ethical value alignment. In
response, we develop VILMO, an in-context alignment method that substantially
enhances the value compliance of LLM outputs by learning to generate
appropriate value instructions, outperforming existing competitors. Our methods
are suitable for black-box and open-source models, offering a promising initial
step in studying the ethical values of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11069">
<div class="article-summary-box-inner">
<span><p>Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models. (arXiv:2310.11079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11079">
<div class="article-summary-box-inner">
<span><p>Recently, researchers have made considerable improvements in dialogue systems
with the progress of large language models (LLMs) such as ChatGPT and GPT-4.
These LLM-based chatbots encode the potential biases while retaining
disparities that can harm humans during interactions. The traditional biases
investigation methods often rely on human-written test cases. However, these
test cases are usually expensive and limited. In this work, we propose a
first-of-its-kind method that automatically generates test cases to detect
LLMs' potential gender bias. We apply our method to three well-known LLMs and
find that the generated test cases effectively identify the presence of biases.
To address the biases identified, we propose a mitigation strategy that uses
the generated test cases as demonstrations for in-context learning to
circumvent the need for parameter fine-tuning. The experimental results show
that LLMs generate fairer responses with the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding writing style in social media with a supervised contrastively pre-trained transformer. (arXiv:2310.11081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11081">
<div class="article-summary-box-inner">
<span><p>Online Social Networks serve as fertile ground for harmful behavior, ranging
from hate speech to the dissemination of disinformation. Malicious actors now
have unprecedented freedom to misbehave, leading to severe societal unrest and
dire consequences, as exemplified by events such as the Capitol assault during
the US presidential election and the Antivaxx movement during the COVID-19
pandemic. Understanding online language has become more pressing than ever.
While existing works predominantly focus on content analysis, we aim to shift
the focus towards understanding harmful behaviors by relating content to their
respective authors. Numerous novel approaches attempt to learn the stylistic
features of authors in texts, but many of these approaches are constrained by
small datasets or sub-optimal training losses. To overcome these limitations,
we introduce the Style Transformer for Authorship Representations (STAR),
trained on a large corpus derived from public sources of 4.5 x 10^6 authored
texts involving 70k heterogeneous authors. Our model leverages Supervised
Contrastive Loss to teach the model to minimize the distance between texts
authored by the same individual. This author pretext pre-training task yields
competitive performance at zero-shot with PAN challenges on attribution and
clustering. Additionally, we attain promising results on PAN verification
challenges using a single dense layer, with our model serving as an embedding
encoder. Finally, we present results from our test partition on Reddit. Using a
support base of 8 documents of 512 tokens, we can discern authors from sets of
up to 1616 authors with at least 80\% accuracy. We share our pre-trained model
at huggingface (https://huggingface.co/AIDA-UPM/star) and our code is available
at (https://github.com/jahuerta92/star)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11085">
<div class="article-summary-box-inner">
<span><p>Relation extraction aims at inferring structured human knowledge from textual
documents. State-of-the-art methods based on language models commonly have two
limitations: (1) they require named entities to be either given as input or
infer them, which introduces additional noise, and (2) they require human
annotations of documents. As a remedy, we present a novel framework for
in-context few-shot relation extraction via pre-trained language models. To the
best of our knowledge, we are the first to reformulate the relation extraction
task as a tailored in-context few-shot learning paradigm. Thereby, we achieve
crucial benefits in that we eliminate the need for both named entity
recognition and human annotation of documents. Unlike existing methods based on
fine-tuning, our framework is flexible in that it can be easily updated for a
new set of relations without re-training. We evaluate our framework using
DocRED, the largest publicly available dataset for document-level relation
extraction, and demonstrate that our framework achieves state-of-the-art
performance. Finally, our framework allows us to identify missing annotations,
and we thus show that our framework actually performs much better than the
original labels from the development set of DocRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11097">
<div class="article-summary-box-inner">
<span><p>The Italian Digital Media Observatory (IDMO) project, part of a European
initiative, focuses on countering disinformation and fake news. This report
outlines contributions from Rai-CRITS to the project, including: (i) the
creation of novel datasets for testing technologies (ii) development of an
automatic model for categorizing Pagella Politica verdicts to facilitate
broader analysis (iii) creation of an automatic model for recognizing textual
entailment with exceptional accuracy on the FEVER dataset (iv) assessment using
GPT-4 to identify textual entailmen (v) a game to raise awareness about fake
news at national events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-form Simultaneous Speech Translation: Thesis Proposal. (arXiv:2310.11141v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11141">
<div class="article-summary-box-inner">
<span><p>Simultaneous speech translation (SST) aims to provide real-time translation
of spoken language, even before the speaker finishes their sentence.
Traditionally, SST has been addressed primarily by cascaded systems that
decompose the task into subtasks, including speech recognition, segmentation,
and machine translation. However, the advent of deep learning has sparked
significant interest in end-to-end (E2E) systems. Nevertheless, a major
limitation of most approaches to E2E SST reported in the current literature is
that they assume that the source speech is pre-segmented into sentences, which
is a significant obstacle for practical, real-world applications. This thesis
proposal addresses end-to-end simultaneous speech translation, particularly in
the long-form setting, i.e., without pre-segmentation. We present a survey of
the latest advancements in E2E SST, assess the primary obstacles in SST and its
relevance to long-form scenarios, and suggest approaches to tackle these
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Quo Vadis of the Relationship between Language and Large Language Models. (arXiv:2310.11146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11146">
<div class="article-summary-box-inner">
<span><p>In the field of Artificial (General) Intelligence (AI), the several recent
advancements in Natural language processing (NLP) activities relying on Large
Language Models (LLMs) have come to encourage the adoption of LLMs as
scientific models of language. While the terminology employed for the
characterization of LLMs favors their embracing as such, it is not clear that
they are in a place to offer insights into the target system they seek to
represent. After identifying the most important theoretical and empirical risks
brought about by the adoption of scientific models that lack transparency, we
discuss LLMs relating them to every scientific model's fundamental components:
the object, the medium, the meaning and the user. We conclude that, at their
current stage of development, LLMs hardly offer any explanations for language,
and then we provide an outlook for more informative future research directions
on this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing the Creativity of Large Language Models: Can models produce divergent semantic association?. (arXiv:2310.11158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11158">
<div class="article-summary-box-inner">
<span><p>Large language models possess remarkable capacity for processing language,
but it remains unclear whether these models can further generate creative
content. The present study aims to investigate the creative thinking of large
language models through a cognitive perspective. We utilize the divergent
association task (DAT), an objective measurement of creativity that asks models
to generate unrelated words and calculates the semantic distance between them.
We compare the results across different models and decoding strategies. Our
findings indicate that: (1) When using the greedy search strategy, GPT-4
outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level.
(2) Stochastic sampling and temperature scaling are effective to obtain higher
DAT scores for models except GPT-4, but face a trade-off between creativity and
stability. These results imply that advanced large language models have
divergent semantic associations, which is a fundamental process underlying
creativity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems. (arXiv:2310.11163v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11163">
<div class="article-summary-box-inner">
<span><p>We present IMTLab, an open-source end-to-end interactive machine translation
(IMT) system platform that enables researchers to quickly build IMT systems
with state-of-the-art models, perform an end-to-end evaluation, and diagnose
the weakness of systems. IMTLab treats the whole interactive translation
process as a task-oriented dialogue with a human-in-the-loop setting, in which
human interventions can be explicitly incorporated to produce high-quality,
error-free translations. To this end, a general communication interface is
designed to support the flexible IMT architectures and user policies. Based on
the proposed design, we construct a simulated and real interactive environment
to achieve end-to-end evaluation and leverage the framework to systematically
evaluate previous IMT systems. Our simulated and manual experiments show that
the prefix-constrained decoding approach still gains the lowest editing cost in
the end-to-end evaluation, while BiTIIMT achieves comparable editing cost with
a better interactive experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing. (arXiv:2310.11166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11166">
<div class="article-summary-box-inner">
<span><p>English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is
available\footnote{\url{https://huggingface.co/uitnlp/visobert}} only for
research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11191">
<div class="article-summary-box-inner">
<span><p>Text simplification has emerged as an increasingly useful application of AI
for bridging the communication gap in specialized fields such as medicine,
where the lexicon is often dominated by technical jargon and complex
constructs. Despite notable progress, methods in medical simplification
sometimes result in the generated text having lower quality and diversity. In
this work, we explore ways to further improve the readability of text
simplification in the medical domain. We propose (1) a new unlikelihood loss
that encourages generation of simpler terms and (2) a reranked beam search
decoding method that optimizes for simplicity, which achieve better performance
on readability metrics on three datasets. This study's findings offer promising
avenues for improving text simplification in the medical field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. (arXiv:2310.11207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11207">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as ChatGPT have demonstrated superior
performance on a variety of natural language processing (NLP) tasks including
sentiment analysis, mathematical reasoning and summarization. Furthermore,
since these models are instruction-tuned on human conversations to produce
"helpful" responses, they can and often will produce explanations along with
the response, which we call self-explanations. For example, when analyzing the
sentiment of a movie review, the model may output not only the positivity of
the sentiment, but also an explanation (e.g., by listing the sentiment-laden
words such as "fantastic" and "memorable" in the review). How good are these
automatically generated self-explanations? In this paper, we investigate this
question on the task of sentiment analysis and for feature attribution
explanation, one of the most commonly studied settings in the interpretability
literature (for pre-ChatGPT models). Specifically, we study different ways to
elicit the self-explanations, evaluate their faithfulness on a set of
evaluation metrics, and compare them to traditional explanation methods such as
occlusion or LIME saliency maps. Through an extensive set of experiments, we
find that ChatGPT's self-explanations perform on par with traditional ones, but
are quite different from them according to various agreement metrics, meanwhile
being much cheaper to produce (as they are generated along with the
prediction). In addition, we identified several interesting characteristics of
them, which prompt us to rethink many current model interpretability practices
in the era of ChatGPT(-like) LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models. (arXiv:2310.11220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11220">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) have made considerable advancements in
understanding and generating unstructured text, their application in structured
data remains underexplored. Particularly, using LLMs for complex reasoning
tasks on knowledge graphs (KGs) remains largely untouched. To address this, we
propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing
KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and
Inference, each aimed at partitioning sentences, retrieving relevant graph
components, and deriving logical conclusions, respectively. We evaluate KG-GPT
using KG-based fact verification and KGQA benchmarks, with the model showing
competitive and robust performance, even outperforming several fully-supervised
models. Our work, therefore, marks a significant step in unifying structured
and unstructured data processing within the realm of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms. (arXiv:2310.11227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11227">
<div class="article-summary-box-inner">
<span><p>Reports of human-like behaviors in foundation models are growing, with
psychological theories providing enduring tools to investigate these behaviors.
However, current research tends to directly apply these human-oriented tools
without verifying the faithfulness of their outcomes. In this paper, we
introduce a framework, RealBehavior, which is designed to characterize the
humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our
framework assesses the faithfulness of results based on reproducibility,
internal and external consistency, and generalizability. Our findings suggest
that a simple application of psychological tools cannot faithfully characterize
all human-like behaviors. Moreover, we discuss the impacts of aligning models
with human and social values, arguing for the necessity of diversifying
alignment objectives to prevent the creation of models with restricted
characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watermarking LLMs with Weight Quantization. (arXiv:2310.11237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11237">
<div class="article-summary-box-inner">
<span><p>Abuse of large language models reveals high risks as large language models
are being deployed at an astonishing speed. It is important to protect the
model weights to avoid malicious usage that violates licenses of open-source
large language models. This paper proposes a novel watermarking strategy that
plants watermarks in the quantization process of large language models without
pre-defined triggers during inference. The watermark works when the model is
used in the fp32 mode and remains hidden when the model is quantized to int8,
in this way, the users can only inference the model without further supervised
fine-tuning of the model. We successfully plant the watermark into open-source
large language model weights including GPT-Neo and LLaMA. We hope our proposed
method can provide a potential direction for protecting model weights in the
era of large language model applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11244">
<div class="article-summary-box-inner">
<span><p>Entity Matching is the task of deciding whether two entity descriptions refer
to the same real-world entity. Entity Matching is a central step in most data
integration pipelines and an enabler for many e-commerce applications which
require to match products offers from different vendors. State-of-the-art
entity matching methods often rely on pre-trained language models (PLMs) such
as BERT or RoBERTa. Two major drawbacks of these models for entity matching are
that (i) the models require significant amounts of task-specific training data
and (ii) the fine-tuned models are not robust concerning out-of-distribution
entities. In this paper, we investigate using large language models (LLMs) for
entity matching as a less domain-specific training data reliant and more robust
alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5
and GPT4, as well as open source LLMs based on Llama2 which can be run locally.
We evaluate these models in a zero-shot scenario as well as a scenario where
task-specific training data is available. We compare different prompt designs
as well as the prompt sensitivity of the models in the zero-shot scenario. We
investigate (i) the selection of in-context demonstrations, (ii) the generation
of matching rules, as well as (iii) fine-tuning GPT3.5 in the second scenario
using the same pool of training data across the different approaches. Our
experiments show that GPT4 without any task-specific training data outperforms
fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets
reaching F1 scores around 90%. The experiments with in-context learning and
rule generation show that all models beside of GPT4 benefit from these
techniques (on average 5.9% and 2.2% F1), while GPT4 does not need such
additional guidance in most cases...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion. (arXiv:2310.11248v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11248">
<div class="article-summary-box-inner">
<span><p>Code completion models have made significant progress in recent years, yet
current popular evaluation datasets, such as HumanEval and MBPP, predominantly
focus on code completion tasks within a single file. This over-simplified
setting falls short of representing the real-world software development
scenario where repositories span multiple files with numerous cross-file
dependencies, and accessing and understanding cross-file context is often
required to complete the code correctly.
</p>
<p>To fill in this gap, we propose CrossCodeEval, a diverse and multilingual
code completion benchmark that necessitates an in-depth cross-file contextual
understanding to complete the code accurately. CrossCodeEval is built on a
diverse set of real-world, open-sourced, permissively-licensed repositories in
four popular programming languages: Python, Java, TypeScript, and C#. To create
examples that strictly require cross-file context for accurate completion, we
propose a straightforward yet efficient static-analysis-based approach to
pinpoint the use of cross-file context within the current file.
</p>
<p>Extensive experiments on state-of-the-art code language models like CodeGen
and StarCoder demonstrate that CrossCodeEval is extremely challenging when the
relevant cross-file context is absent, and we see clear improvements when
adding these context into the prompt. However, despite such improvements, the
pinnacle of performance remains notably unattained even with the
highest-performing model, indicating that CrossCodeEval is also capable of
assessing model's capability in leveraging extensive context to make better
code completion. Finally, we benchmarked various methods in retrieving
cross-file context, and show that CrossCodeEval can also be used to measure the
capability of code retrievers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges. (arXiv:2310.11252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11252">
<div class="article-summary-box-inner">
<span><p>The growing popularity of generative language models has amplified interest
in interactive methods to guide model outputs. Prompt refinement is considered
one of the most effective means to influence output among these methods. We
identify several challenges associated with prompting large language models,
categorized into data- and model-specific, linguistic, and socio-linguistic
challenges. A comprehensive examination of model outputs, including runner-up
candidates and their corresponding probabilities, is needed to address these
issues. The beam search tree, the prevalent algorithm to sample model outputs,
can inherently supply this information. Consequently, we introduce an
interactive visual method for investigating the beam search tree, facilitating
analysis of the decisions made by the model during generation. We
quantitatively show the value of exposing the beam search tree and present five
detailed analysis scenarios addressing the identified challenges. Our
methodology validates existing results and offers additional insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01331">
<div class="article-summary-box-inner">
<span><p>Protection of human rights is one of the most important problems of our
world. In this paper, our aim is to provide a dataset which covers one of the
most significant human rights contradiction in recent months affected the whole
world, George Floyd incident. We propose a labeled dataset for topic detection
that contains 17 million tweets. These Tweets are collected from 25 May 2020 to
21 August 2020 that covers 89 days from start of this incident. We labeled the
dataset by monitoring most trending news topics from global and local
newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We
evaluated the results of these two methods with three different k values for
metrics of precision, recall and f1-score. The collected dataset is available
at https://github.com/MeysamAsgariC/BLMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake news detection using parallel BERT deep neural networks. (arXiv:2204.04793v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04793">
<div class="article-summary-box-inner">
<span><p>Fake news is a growing challenge for social networks and media. Detection of
fake news always has been a problem for many years, but after the evolution of
social networks and increasing speed of news dissemination in recent years has
been considered again. There are several approaches to solving this problem,
one of which is to detect fake news based on its text style using deep neural
networks. In recent years, one of the most used forms of deep neural networks
for natural language processing is transfer learning with transformers. BERT is
one of the most promising transformers who outperforms other models in many NLP
benchmarks. This article, we introduce MWPBert, which uses two parallel BERT
networks to perform veracity detection on full-text news articles. One of the
BERT networks encodes news headline, and another encodes news body. Since the
input length of the BERT network is limited and constant and the news body is
usually a long text, we cannot fed the whole news text into the BERT.
Therefore, using the MaxWorth algorithm, we selected the part of the news text
that is more valuable for fact-checking, and fed it into the BERT network.
Finally, we encode the output of the two BERT networks to an output network to
classify the news. The experiment results showed that the proposed model
outperformed previous models in terms of accuracy and other performance
measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format. (arXiv:2211.17148v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.17148">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) systems function as digital assistants, guiding
users through various tasks such as booking flights or finding restaurants.
Existing toolkits for building TOD systems often fall short of in delivering
comprehensive arrays of data, models, and experimental environments with a
user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue
system toolkit crafted to bridge this gap. Our unified data format simplifies
the integration of diverse datasets and models, significantly reducing
complexity and cost for studying generalization and transfer. Enhanced with
robust reinforcement learning (RL) tools, featuring a streamlined training
process, in-depth evaluation tools, and a selection of user simulators,
ConvLab-3 supports the rapid development and evaluation of robust dialogue
policies. Through an extensive study, we demonstrate the efficacy of transfer
learning and RL and showcase that ConvLab-3 is not only a powerful tool for
seasoned researchers but also an accessible platform for newcomers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11916">
<div class="article-summary-box-inner">
<span><p>In recent years, pre-trained large language models (LLMs) have demonstrated
remarkable efficiency in achieving an inference-time few-shot learning
capability known as in-context learning. However, existing literature has
highlighted the sensitivity of this capability to the selection of few-shot
demonstrations. Current understandings of the underlying mechanisms by which
this capability arises from regular language model pretraining objectives
remain disconnected from the real-world LLMs. This study aims to examine the
in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs
as latent variable models. On this premise, we propose an algorithm to select
optimal demonstrations from a set of annotated data with a small LM, and then
directly generalize the selected demonstrations to larger LMs. We demonstrate
significant improvement over baselines, averaged over eight GPT models on eight
real-world text classification datasets. We also demonstrate the real-world
usefulness of our algorithm on GSM8K, a math word problem dataset. Our
empirical findings support our hypothesis that LLMs implicitly infer a latent
variable containing task information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11713">
<div class="article-summary-box-inner">
<span><p>Pre-trained vision and language models have demonstrated state-of-the-art
capabilities over existing tasks involving images and texts, including visual
question answering. However, it remains unclear whether these models possess
the capability to answer questions that are not only querying visual content
but knowledge-intensive and information-seeking. In this study, we introduce
InfoSeek, a visual question answering dataset tailored for information-seeking
questions that cannot be answered with only common sense knowledge. Using
InfoSeek, we analyze various pre-trained visual question answering models and
gain insights into their characteristics. Our findings reveal that
state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)
face challenges in answering visual information-seeking questions, but
fine-tuning on the InfoSeek dataset elicits models to use fine-grained
knowledge that was learned during their pre-training. Furthermore, we show that
accurate visual entity recognition can be used to improve performance on
InfoSeek by retrieving relevant documents, showing a significant space for
improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets. (arXiv:2302.13959v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13959">
<div class="article-summary-box-inner">
<span><p>Increasingly larger datasets have become a standard ingredient to advancing
the state-of-the-art in NLP. However, data quality might have already become
the bottleneck to unlock further gains. Given the diversity and the sizes of
modern datasets, standard data filtering is not straight-forward to apply,
because of the multifacetedness of the harmful data and elusiveness of
filtering rules that would generalize across multiple tasks. We study the
fitness of task-agnostic self-influence scores of training examples for data
cleaning, analyze their efficacy in capturing naturally occurring outliers, and
investigate to what extent self-influence based data cleaning can improve
downstream performance in machine translation, question answering and text
classification, building up on recent approaches to self-influence calculation
and automated curriculum learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13780">
<div class="article-summary-box-inner">
<span><p>ChatGPT shows remarkable capabilities for machine translation (MT). Several
prior studies have shown that it achieves comparable results to commercial
systems for high-resource languages, but lags behind in complex tasks, e.g.,
low-resource and distant-language-pairs translation. However, they usually
adopt simple prompts which can not fully elicit the capability of ChatGPT. In
this paper, we aim to further mine ChatGPT's translation ability by revisiting
several aspects: temperature, task information, and domain information, and
correspondingly propose an optimal temperature setting and two (simple but
effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts
(DSP). We show that: 1) The performance of ChatGPT depends largely on
temperature, and a lower temperature usually can achieve better performance; 2)
Emphasizing the task information can further improve ChatGPT's performance,
particularly in complex MT tasks; 3) Introducing domain information can elicit
ChatGPT's generalization ability and improve its performance in the specific
domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT
tasks, which can be partially addressed by our proposed prompts but still need
to be highlighted for the MT/NLP community. We also explore the effects of
advanced in-context learning strategies and find a (negative but interesting)
observation: the powerful chain-of-thought prompt leads to word-by-word
translation behavior, thus bringing significant translation degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01492">
<div class="article-summary-box-inner">
<span><p>The truth is significantly hampered by massive rumors that spread along with
breaking news or popular topics. Since there is sufficient corpus gathered from
the same domain for model training, existing rumor detection algorithms show
promising performance on yesterday's news. However, due to a lack of
substantial training data and prior expert knowledge, they are poor at spotting
rumors concerning unforeseen events, especially those propagated in different
languages (i.e., low-resource regimes). In this paper, we propose a unified
contrastive transfer framework to detect rumors by adapting the features
learned from well-resourced rumor data to that of the low-resourced with only
few-shot annotations. More specifically, we first represent rumor circulated on
social media as an undirected topology for enhancing the interaction of user
opinions, and then train a Multi-scale Graph Convolutional Network via a
unified contrastive paradigm to mine effective clues simultaneously from post
semantics and propagation structure. Our model explicitly breaks the barriers
of the domain and/or language issues, via language alignment and a novel
domain-adaptive contrastive learning mechanism. To well-generalize the
representation learning using a small set of annotated target events, we reveal
that rumor-indicative signal is closely correlated with the uniformity of the
distribution of these events. We design a target-wise contrastive training
mechanism with three event-level data augmentation strategies, capable of
unifying the representations by distinguishing target events. Extensive
experiments conducted on four low-resource datasets collected from real-world
microblog platforms demonstrate that our framework achieves much better
performance than state-of-the-art methods and exhibits a superior capacity for
detecting rumors at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04250">
<div class="article-summary-box-inner">
<span><p>Methods for making high-quality recommendations often rely on learning latent
representations from interaction data. These methods, while performant, do not
provide ready mechanisms for users to control the recommendation they receive.
Our work tackles this problem by proposing LACE, a novel concept value
bottleneck model for controllable text recommendations. LACE represents each
user with a succinct set of human-readable concepts through retrieval given
user-interacted documents and learns personalized representations of the
concepts based on user documents. This concept based user profile is then
leveraged to make recommendations. The design of our model affords control over
the recommendations through a number of intuitive interactions with a
transparent user profile. We first establish the quality of recommendations
obtained from LACE in an offline evaluation on three recommendation tasks
spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we
validate the controllability of LACE under simulated user interactions.
Finally, we implement LACE in an interactive controllable recommender system
and conduct a user study to demonstrate that users are able to improve the
quality of recommendations they receive through interactions with an editable
user profile.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale. (arXiv:2304.12206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.12206">
<div class="article-summary-box-inner">
<span><p>Existing question answering (QA) systems owe much of their success to large,
high-quality training data. Such annotation efforts are costly, and the
difficulty compounds in the cross-lingual setting. Therefore, prior
cross-lingual QA work has focused on releasing evaluation datasets, and then
applying zero-shot methods as baselines. This work proposes a synthetic data
generation method for cross-lingual QA which leverages indirect supervision
from existing parallel corpora. Our method termed PAXQA (Projecting annotations
for cross-lingual (x) QA) decomposes cross-lingual QA into two stages. First,
we apply a question generation (QG) model to the English side. Second, we apply
annotation projection to translate both the questions and answers. To better
translate questions, we propose a novel use of lexically-constrained machine
translation, in which constrained entities are extracted from the parallel
bitexts.
</p>
<p>We apply PAXQA to generate cross-lingual QA examples in 4 languages (662K
examples total), and perform human evaluation on a subset to create validation
and test splits. We then show that models fine-tuned on these datasets
outperform prior synthetic data generation models over several extractive QA
datasets. The largest performance gains are for directions with non-English
questions and English contexts. Ablation studies show that our dataset
generation method is relatively robust to noise from automatic word alignments,
showing the sufficient quality of our generations. To facilitate follow-up
work, we release our code and datasets at https://github.com/manestay/paxqa .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Internal State of an LLM Knows When It's Lying. (arXiv:2304.13734v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13734">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) have shown exceptional performance in
various tasks, one of their most prominent drawbacks is generating inaccurate
or false information with a confident tone. In this paper, we provide evidence
that the LLM's internal state can be used to reveal the truthfulness of
statements. This includes both statements provided to the LLM, and statements
that the LLM itself generates. Our approach is to train a classifier that
outputs the probability that a statement is truthful, based on the hidden layer
activations of the LLM as it reads or generates the statement. Experiments
demonstrate that given a set of test sentences, of which half are true and half
false, our trained classifier achieves an average of 71\% to 83\% accuracy
labeling which sentences are true versus false, depending on the LLM base
model. Furthermore, we explore the relationship between our classifier's
performance and approaches based on the probability assigned to the sentence by
the LLM. We show that while LLM-assigned sentence probability is related to
sentence truthfulness, this probability is also dependent on sentence length
and the frequencies of words in the sentence, resulting in our trained
classifier providing a more reliable approach to detecting truthfulness,
highlighting its potential to enhance the reliability of LLM-generated content
and its practical applicability in real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07609">
<div class="article-summary-box-inner">
<span><p>The remarkable achievements of Large Language Models (LLMs) have led to the
emergence of a novel recommendation paradigm -- Recommendation via LLM
(RecLLM). Nevertheless, it is important to note that LLMs may contain social
prejudices, and therefore, the fairness of recommendations made by RecLLM
requires further investigation. To avoid the potential risks of RecLLM, it is
imperative to evaluate the fairness of RecLLM with respect to various sensitive
attributes on the user side. Due to the differences between the RecLLM paradigm
and the traditional recommendation paradigm, it is problematic to directly use
the fairness benchmark of traditional recommendation. To address the dilemma,
we propose a novel benchmark called Fairness of Recommendation via LLM
(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset
that accounts for eight sensitive attributes1 in two recommendation scenarios:
music and movies. By utilizing our FaiRLLM benchmark, we conducted an
evaluation of ChatGPT and discovered that it still exhibits unfairness to some
sensitive attributes when generating recommendations. Our code and dataset can
be found at https://github.com/jizhi-zhang/FaiRLLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09652">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) remains elusive even with
current large pretrained language models on text and speech, especially in
multilingual cases. Machine translation has been established as a powerful
pretraining objective on text as it enables the model to capture high-level
semantics of the input utterance and associations between different languages,
which is desired for speech models that work on lower-level acoustic frames.
Motivated particularly by the task of cross-lingual SLU, we demonstrate that
the task of speech translation (ST) is a good means of pretraining speech
models for end-to-end SLU on both intra- and cross-lingual scenarios.
</p>
<p>By introducing ST, our models reach higher performance over baselines on
monolingual and multilingual intent classification as well as spoken question
answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the
effectiveness of our methods, we also create new benchmark datasets from both
synthetic and real sources, for speech summarization and low-resource/zero-shot
transfer from English to French or Spanish. We further show the value of
preserving knowledge for the ST pretraining task for better downstream
performance, possibly using Bayesian transfer regularizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10819">
<div class="article-summary-box-inner">
<span><p>Evaluating the performance of Grammatical Error Correction (GEC) systems is a
challenging task due to its subjectivity. Designing an evaluation metric that
is as objective as possible is crucial to the development of GEC task. However,
mainstream evaluation metrics, i.e., reference-based metrics, introduce bias
into the multi-reference evaluation by extracting edits without considering the
presence of multiple references. To overcome this issue, we propose Chunk-LEvel
Multi-reference Evaluation (CLEME), designed to evaluate GEC systems in the
multi-reference evaluation setting. CLEME builds chunk sequences with
consistent boundaries for the source, the hypothesis and references, thus
eliminating the bias caused by inconsistent edit boundaries. Furthermore, we
observe the consistent boundary could also act as the boundary of grammatical
errors, based on which the F$_{0.5}$ score is then computed following the
correction independence assumption. We conduct experiments on six English
reference sets based on the CoNLL-2014 shared task. Extensive experiments and
detailed analyses demonstrate the correctness of our discovery and the
effectiveness of CLEME. Further analysis reveals that CLEME is robust to
evaluate GEC systems across reference sets with varying numbers of references
and annotation style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models. (arXiv:2305.11171v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11171">
<div class="article-summary-box-inner">
<span><p>Factual consistency evaluation is often conducted using Natural Language
Inference (NLI) models, yet these models exhibit limited success in evaluating
summaries. Previous work improved such models with synthetic training data.
However, the data is typically based on perturbed human-written summaries,
which often differ in their characteristics from real model-generated summaries
and have limited coverage of possible factual errors. Alternatively, large
language models (LLMs) have recently shown promising results in directly
evaluating generative tasks, but are too computationally expensive for
practical use. Motivated by these limitations, we introduce TrueTeacher, a
method for generating synthetic data by annotating diverse model-generated
summaries using a LLM. Unlike prior work, TrueTeacher does not rely on
human-written summaries, and is multilingual by nature. Experiments on the TRUE
benchmark show that a student model trained using our data, substantially
outperforms both the state-of-the-art model with similar capacity, and the LLM
teacher. In a systematic study, we compare TrueTeacher to existing synthetic
data generation methods and demonstrate its superiority and robustness to
domain-shift. We also show that our method generalizes to multilingual
scenarios. Lastly, we release our large scale synthetic dataset (1.4M
examples), generated using TrueTeacher, and a checkpoint trained on this data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation. (arXiv:2305.11490v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11490">
<div class="article-summary-box-inner">
<span><p>Following the impressive development of LLMs, vision-language alignment in
LLMs is actively being researched to enable multimodal reasoning and visual IO.
This direction of research is particularly relevant to medical imaging because
medical image analysis and generation consist of reasoning based on a
combination of visual features and prior knowledge. Many recent works have
focused on training adapter networks that serve as an information bridge
between image processing networks and LLMs; but presumably, in order to achieve
maximum reasoning potential of LLMs on visual information as well, visual and
language features should be allowed to interact more freely. This is especially
important in the medical domain because understanding and generating medical
images such as chest X-rays (CXR) require not only accurate visual and
language-based reasoning but also a more intimate mapping between the two
modalities. Thus, taking inspiration from previous work on the transformer and
VQ-GAN combination for bidirectional image and text generation, we build upon
this approach and develop a method for instruction-tuning an LLM pre-trained
only on text to gain vision-language capabilities for medical images.
Specifically, we leverage a pretrained LLM's existing question-answering and
instruction-following abilities to teach it to understand visual inputs by
instructing it to answer questions about image inputs and, symmetrically,
output both text and image responses appropriate to a given query by tuning the
LLM with diverse tasks that encompass image-based text-generation and
text-based image-generation. We show that our model, LLM-CXR, trained in this
approach shows better image-text alignment in both CXR understanding and
generation tasks while being smaller in size compared to previously developed
models that perform a narrower range of tasks. The code is at
https://github.com/hyn2028/llm-cxr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Simplification of Medical Texts. (arXiv:2305.12532v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12532">
<div class="article-summary-box-inner">
<span><p>Automated text simplification aims to produce simple versions of complex
texts. This task is especially useful in the medical domain, where the latest
medical findings are typically communicated via complex and technical articles.
This creates barriers for laypeople seeking access to up-to-date medical
findings, consequently impeding progress on health literacy. Most existing work
on medical text simplification has focused on monolingual settings, with the
result that such evidence would be available only in just one language (most
often, English). This work addresses this limitation via multilingual
simplification, i.e., directly simplifying complex texts into simplified texts
in multiple languages. We introduce MultiCochrane, the first sentence-aligned
multilingual text simplification dataset for the medical domain in four
languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and
zero-shot models across these languages, with extensive human assessments and
analyses. Although models can now generate viable simplified texts, we identify
outstanding challenges that this dataset might be used to address.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space. (arXiv:2305.12785v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12785">
<div class="article-summary-box-inner">
<span><p>Multi-aspect controllable text generation aims to generate fluent sentences
that possess multiple desired attributes simultaneously. Traditional methods
either combine many operators in the decoding stage, often with costly
iteration or search in the discrete text space, or train separate controllers
for each aspect, resulting in a degeneration of text quality due to the
discrepancy between different aspects. To address these limitations, we
introduce a novel approach for multi-aspect control, namely MacLaSa, that
estimates compact latent space for multiple aspects and performs efficient
sampling with a robust sampler based on ordinary differential equations (ODEs).
To eliminate the domain gaps between different aspects, we utilize a
Variational Autoencoder (VAE) network to map text sequences from varying data
sources into close latent representations. The estimated latent space enables
the formulation of joint energy-based models (EBMs) and the plugging in of
arbitrary attribute discriminators to achieve multi-aspect control. Afterwards,
we draw latent vector samples with an ODE-based sampler and feed sampled
examples to the VAE decoder to produce target text sequences. Experimental
results demonstrate that MacLaSa outperforms several strong baselines on
attribute relevance and textual quality while maintaining a high inference
speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unsupervised Recognition of Semantic Differences in Related Documents. (arXiv:2305.13303v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13303">
<div class="article-summary-box-inner">
<span><p>Automatically highlighting words that cause semantic differences between two
documents could be useful for a wide range of applications. We formulate
recognizing semantic differences (RSD) as a token-level regression task and
study three unsupervised approaches that rely on a masked language model. To
assess the approaches, we begin with basic English sentences and gradually move
to more complex, cross-lingual document pairs. Our results show that an
approach based on word alignment and sentence-level contrastive learning has a
robust correlation to gold labels. However, all unsupervised approaches still
leave a large margin of improvement. Code to reproduce our experiments is
available at https://github.com/ZurichNLP/recognizing-semantic-differences
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality. (arXiv:2305.13812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13812">
<div class="article-summary-box-inner">
<span><p>Contrastively trained vision-language models have achieved remarkable
progress in vision and language representation learning, leading to
state-of-the-art models for various downstream multimodal tasks. However,
recent research has highlighted severe limitations of these models in their
ability to perform compositional reasoning over objects, attributes, and
relations. Scene graphs have emerged as an effective way to understand images
compositionally. These are graph-structured semantic representations of images
that contain objects, their attributes, and relations with other objects in a
scene. In this work, we consider the scene graph parsed from text as a proxy
for the image scene graph and propose a graph decomposition and augmentation
framework along with a coarse-to-fine contrastive learning objective between
images and text that aligns sentences of various complexities to the same
image. Along with this, we propose novel negative mining techniques in the
scene graph space for improving attribute binding and relation understanding.
Through extensive experiments, we demonstrate the effectiveness of our approach
that significantly improves attribute binding, relation understanding,
systematic generalization, and productivity on multiple recently proposed
benchmarks (For example, improvements upto $18\%$ for systematic
generalization, $16.5\%$ for relation understanding over a strong baseline),
while achieving similar or better performance than CLIP on various general
multimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2H: Building Multimodal Navigation Helpers that Respond to Help Requests. (arXiv:2305.14260v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14260">
<div class="article-summary-box-inner">
<span><p>Intelligent navigation-helper agents are critical as they can navigate users
in unknown areas through environmental awareness and conversational ability,
serving as potential accessibility tools for individuals with disabilities. In
this work, we first introduce a novel benchmark, Respond to Help Requests
(R2H), to promote the development of multi-modal navigation helpers capable of
responding to requests for help, utilizing existing dialog-based embodied
datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH),
which assesses the helper agent's ability to generate informative responses
based on a given dialog history, and (2) Respond during Interaction (RdI),
which evaluates the effectiveness and efficiency of the response during
consistent cooperation with a task performer. Furthermore, we explore two
approaches to construct the navigation-helper agent, including fine-tuning a
novel task-oriented multi-modal response generation model that can see and
respond, named SeeRee, and employing a multi-modal large language model in a
zero-shot manner. Analysis of the task and method was conducted based on both
automatic benchmarking and human evaluations. Project website:
https://sites.google.com/view/response2helprequests/home.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration. (arXiv:2305.14324v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14324">
<div class="article-summary-box-inner">
<span><p>Kendall's tau is frequently used to meta-evaluate how well machine
translation (MT) evaluation metrics score individual translations. Its focus on
pairwise score comparisons is intuitive but raises the question of how ties
should be handled, a gray area that has motivated different variants in the
literature. We demonstrate that, in settings like modern MT meta-evaluation,
existing variants have weaknesses arising from their handling of ties, and in
some situations can even be gamed. We propose instead to meta-evaluate metrics
with a version of pairwise accuracy that gives metrics credit for correctly
predicting ties, in combination with a tie calibration procedure that
automatically introduces ties into metric scores, enabling fair comparison
between metrics that do and do not predict ties. We argue and provide
experimental evidence that these modifications lead to fairer ranking-based
assessments of metric performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14342">
<div class="article-summary-box-inner">
<span><p>Given the massive cost of language model pre-training, a non-trivial
improvement of the optimization algorithm would lead to a material reduction on
the time and cost of training. Adam and its variants have been state-of-the-art
for years, and more sophisticated second-order (Hessian-based) optimizers often
incur too much per-step overhead. In this paper, we propose Sophia,
Second-order Clipped Stochastic Optimization, a simple scalable second-order
optimizer that uses a light-weight estimate of the diagonal Hessian as the
pre-conditioner. The update is the moving average of the gradients divided by
the moving average of the estimated Hessian, followed by element-wise clipping.
The clipping controls the worst-case update size and tames the negative impact
of non-convexity and rapid change of Hessian along the trajectory. Sophia only
estimates the diagonal Hessian every handful of iterations, which has
negligible average per-step time and memory overhead. On language modeling with
GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up
compared to Adam in the number of steps, total compute, and wall-clock time,
achieving the same perplexity with 50% fewer steps, less total compute, and
reduced wall-clock time. Theoretically, we show that Sophia, in a much
simplified setting, adapts to the heterogeneous curvatures in different
parameter dimensions, and thus has a run-time bound that does not depend on the
condition number of the loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Natural Language Generation. (arXiv:2305.15040v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15040">
<div class="article-summary-box-inner">
<span><p>The field of Natural Language Generation (NLG) suffers from a severe shortage
of labeled data due to the extremely expensive and time-consuming process
involved in manual annotation. A natural approach for coping with this problem
is active learning (AL), a well-known machine learning technique for improving
annotation efficiency by selectively choosing the most informative examples to
label. However, while AL has been well-researched in the context of text
classification, its application to NLG remains largely unexplored. In this
paper, we present a first systematic study of active learning for NLG,
considering a diverse set of tasks and multiple leading selection strategies,
and harnessing a strong instruction-tuned model. Our results indicate that the
performance of existing AL strategies is inconsistent, surpassing the baseline
of random example selection in some cases but not in others. We highlight some
notable differences between the classification and generation scenarios, and
analyze the selection behaviors of existing AL strategies. Our findings
motivate exploring novel approaches for applying AL to generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Not Strong Abstract Reasoners. (arXiv:2305.19555v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19555">
<div class="article-summary-box-inner">
<span><p>Large Language Models have shown tremendous performance on a large variety of
natural language processing tasks, ranging from text comprehension to common
sense reasoning. However, the mechanisms responsible for this success remain
opaque, and it is unclear whether LLMs can achieve human-like cognitive
capabilities or whether these models are still fundamentally circumscribed.
Abstract reasoning is a fundamental task for cognition, consisting of finding
and applying a general pattern from few data. Evaluating deep neural
architectures on this task could give insight into their potential limitations
regarding reasoning and their broad generalisation abilities, yet this is
currently an under-explored area. In this paper, we introduce a new benchmark
for evaluating language models beyond memorization on abstract reasoning tasks.
We perform extensive evaluations of state-of-the-art LLMs, showing that they
currently achieve very limited performance in contrast with other natural
language tasks, and we examine the reasons for this difference. We apply
techniques that have been shown to improve performance on other NLP tasks and
show that their impact on abstract reasoning is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Self-Repair a Silver Bullet for Code Generation?. (arXiv:2306.09896v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09896">
<div class="article-summary-box-inner">
<span><p>Large language models have shown remarkable aptitude in code generation, but
still struggle on challenging tasks. Self-repair -- in which the model debugs
and fixes mistakes in its own code -- has recently become a popular way to
boost performance in these settings. However, only very limited studies on how
and when self-repair works effectively exist in the literature, and one might
wonder to what extent a model is really capable of repairing mistakes in code
which was originally generated by that very same model. In this paper, we
analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on
problems taken from HumanEval or APPS, finding that when the cost of carrying
out repair is taken into account, gains are often modest, vary significantly
between subsets of the data, and are sometimes not present at all. We
hypothesize that this is because self-repair is bottlenecked by the model's
ability to provide feedback on its own code; boosting the feedback with
stronger models, we observe performance gains even in settings where the model
does not benefit from self-repair. Finally, we find that providing the model
with feedback from human participants greatly benefits repair even for GPT-4,
and carry out a brief qualitative analysis of the differences observed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.10070">
<div class="article-summary-box-inner">
<span><p>ChatGPT has drawn considerable attention from both the general public and
domain experts with its remarkable text generation capabilities. This has
subsequently led to the emergence of diverse applications in the field of
biomedicine and health. In this work, we examine the diverse applications of
large language models (LLMs), such as ChatGPT, in biomedicine and health.
Specifically we explore the areas of biomedical information retrieval, question
answering, medical text summarization, information extraction, and medical
education, and investigate whether LLMs possess the transformative power to
revolutionize these tasks or whether the distinct complexities of biomedical
domain presents unique challenges. Following an extensive literature survey, we
find that significant advances have been made in the field of text generation
tasks, surpassing the previous state-of-the-art methods. For other
applications, the advances have been modest. Overall, LLMs have not yet
revolutionized biomedicine, but recent rapid progress indicates that such
methods hold great potential to provide valuable means for accelerating
discovery and improving health. We also find that the use of LLMs, like
ChatGPT, in the fields of biomedicine and health entails various risks and
challenges, including fabricated information in its generated responses, as
well as legal and privacy concerns associated with sensitive patient data. We
believe this survey can provide a comprehensive and timely overview to
biomedical researchers and healthcare practitioners on the opportunities and
challenges associated with using ChatGPT and other LLMs for transforming
biomedicine and health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.15724">
<div class="article-summary-box-inner">
<span><p>The ability to detect and analyze failed executions automatically is crucial
for an explainable and robust robotic system. Recently, Large Language Models
(LLMs) have demonstrated strong reasoning abilities on textual inputs. To
leverage the power of LLMs for robot failure explanation, we introduce REFLECT,
a framework which queries LLM for failure reasoning based on a hierarchical
summary of robot past experiences generated from multisensory observations. The
failure explanation can further guide a language-based planner to correct the
failure and complete the task. To systematically evaluate the framework, we
create the RoboFail dataset with a variety of tasks and failure scenarios. We
demonstrate that the LLM-based framework is able to generate informative
failure explanations that assist successful correction planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17181">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GAN) is a model for data synthesis, which
creates plausible data through the competition of generator and discriminator.
Although GAN application to image synthesis is extensively studied, it has
inherent limitations to natural language generation. Because natural language
is composed of discrete tokens, a generator has difficulty updating its
gradient through backpropagation; therefore, most text-GAN studies generate
sentences starting with a random token based on a reward system. Thus, the
generators of previous studies are pre-trained in an autoregressive way before
adversarial training, causing data memorization that synthesized sentences
reproduce the training data. In this paper, we synthesize sentences using a
framework similar to the original GAN. More specifically, we propose Text
Embedding Space Generative Adversarial Networks (TESGAN) which generate
continuous text embedding spaces instead of discrete tokens to solve the
gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised
learning which does not directly refer to the text of the training data to
overcome the data memorization issue. By adopting this novel method, TESGAN can
synthesize new sentences, showing the potential of unsupervised learning for
text synthesis. We expect to see extended research combining Large Language
Models with a new perspective of viewing text as an continuous space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03109">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10236">
<div class="article-summary-box-inner">
<span><p>The recent performance leap of Large Language Models (LLMs) opens up new
opportunities across numerous industrial applications and domains. However,
erroneous generations, such as false predictions, misinformation, and
hallucination made by LLMs, have also raised severe concerns for the
trustworthiness of LLMs', especially in safety-, security- and
reliability-sensitive scenarios, potentially hindering real-world adoptions.
While uncertainty estimation has shown its potential for interpreting the
prediction risks made by general machine learning (ML) models, little is known
about whether and to what extent it can help explore an LLM's capabilities and
counteract its undesired behavior. To bridge the gap, in this paper, we
initiate an exploratory study on the risk assessment of LLMs from the lens of
uncertainty. In particular, we experiment with twelve uncertainty estimation
methods and four LLMs on four prominent natural language processing (NLP) tasks
to investigate to what extent uncertainty estimation techniques could help
characterize the prediction risks of LLMs. Our findings validate the
effectiveness of uncertainty estimation for revealing LLMs'
uncertain/non-factual predictions. In addition to general NLP tasks, we
extensively conduct experiments with four LLMs for code generation on two
datasets. We find that uncertainty estimation can potentially uncover buggy
programs generated by LLMs. Insights from our study shed light on future design
and development for reliable LLMs, facilitating further research toward
enhancing the trustworthiness of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01263">
<div class="article-summary-box-inner">
<span><p>Without proper safeguards, large language models will readily follow
malicious instructions and generate toxic content. This risk motivates safety
efforts such as red-teaming and large-scale feedback learning, which aim to
make models both helpful and harmless. However, there is a tension between
these two objectives, since harmlessness requires models to refuse to comply
with unsafe prompts, and thus not be helpful. Recent anecdotal evidence
suggests that some models may have struck a poor balance, so that even clearly
safe prompts are refused if they use similar language to unsafe prompts or
mention sensitive topics. In this paper, we introduce a new test suite called
XSTest to identify such eXaggerated Safety behaviours in a systematic way.
XSTest comprises 250 safe prompts across ten prompt types that well-calibrated
models should not refuse to comply with, and 200 unsafe prompts as contrasts
that models, for most applications, should refuse. We describe XSTest's
creation and composition, and then use the test suite to highlight systematic
failure modes in state-of-the-art language models as well as more general
challenges in building safer language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10335">
<div class="article-summary-box-inner">
<span><p>Recently, the large language models (LLMs) have shown extraordinary ability
in understanding natural language and generating programming code. It has been
a common practice of software engineers to consult LLMs when encountering
coding questions. Although efforts have been made to avoid syntax errors and
align the code with the intended semantics, the reliability and robustness of
the code generationfrom LLMs have not yet been thoroughly studied. The
executable code is not equivalent to the reliable and robust code, especially
in the context of real-world software development. The misuse of APIs in the
generated code could lead to severe problem, such as resource leaks, program
crashes. To make things worse, the users of LLM code generation services are
actually the developers that are most vulnerable to these code that seems right
-- They are always novice developers that are not familiar with the APIs that
LLMs generate code for them. Therefore, they could hardly tell the misuse in
the code generated by LLMs, which further facilitates the incorrect code
applied in real-world software. Existing code evaluation benchmark and datasets
focus on crafting small tasks such as programming questions in coding
interviews, which however deviates from the problem that developers would ask
LLM for real-world coding help. To fill the missing piece, in this work, we
propose a dataset RobustAPI for evaluating the reliability and robustness of
code generated by LLMs. We collect 1208 coding questions from StackOverflow on
24 representative Java APIs. We summarize thecommon misuse patterns of these
APIs and evaluate them oncurrent popular LLMs. The evaluation results show that
evenfor GPT-4, 62% of the generated code contains API misuses,which would cause
unexpected consequences if the code isintroduced into real-world software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15452">
<div class="article-summary-box-inner">
<span><p>The reasoning capabilities of Large Language Models (LLMs) play a pivotal
role in the realm of embodied artificial intelligence. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04965">
<div class="article-summary-box-inner">
<span><p>While impressive performance has been achieved in image captioning, the
limited diversity of the generated captions and the large parameter scale
remain major barriers to the real-word application of these systems. In this
work, we propose a lightweight image captioning network in combination with
continuous diffusion, called Prefix-diffusion. To achieve diversity, we design
an efficient method that injects prefix image embeddings into the denoising
process of the diffusion model. In order to reduce trainable parameters, we
employ a pre-trained model to extract image features and further design an
extra mapping network. Prefix-diffusion is able to generate diverse captions
with relatively less parameters, while maintaining the fluency and relevance of
the captions benefiting from the generative capabilities of the diffusion
model. Our work paves the way for scaling up diffusion models for image
captioning, and achieves promising performance compared with recent approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10066">
<div class="article-summary-box-inner">
<span><p>In this study, we aimed to determine if fine-tuned large language models
(LLMs) can generate accurate, personalized impressions for whole-body PET
reports. Twelve language models were trained on a corpus of PET reports using
the teacher-forcing algorithm, with the report findings as input and the
clinical impressions as reference. An extra input token encodes the reading
physician's identity, allowing models to learn physician-specific reporting
styles. Our corpus comprised 37,370 retrospective PET reports collected from
our institution between 2010 and 2022. To identify the best LLM, 30 evaluation
metrics were benchmarked against quality scores from two nuclear medicine (NM)
physicians, with the most aligned metrics selecting the model for expert
evaluation. In a subset of data, model-generated impressions and original
clinical impressions were assessed by three NM physicians according to 6
quality dimensions (3-point scale) and an overall utility score (5-point
scale). Each physician reviewed 12 of their own reports and 12 reports from
other physicians. Bootstrap resampling was used for statistical analysis. Of
all evaluation metrics, domain-adapted BARTScore and PEGASUSScore showed the
highest Spearman's rank correlations (0.568 and 0.563) with physician
preferences. Based on these metrics, the fine-tuned PEGASUS model was selected
as the top LLM. When physicians reviewed PEGASUS-generated impressions in their
own style, 89% were considered clinically acceptable, with a mean utility score
of 4.08 out of 5. Physicians rated these personalized impressions as comparable
in overall utility to the impressions dictated by other physicians (4.03,
P=0.41). In conclusion, personalized impressions generated by PEGASUS were
clinically useful, highlighting its potential to expedite PET reporting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12053">
<div class="article-summary-box-inner">
<span><p>This paper is devoted to the development of a localized Large Language Model
(LLM) specifically for Arabic, a language imbued with unique cultural
characteristics inadequately addressed by current mainstream models.
Significant concerns emerge when addressing cultural sensitivity and local
values. To address this, the paper proposes a comprehensive solution that
includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT)
utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside
Reinforcement Learning with AI Feedback (RLAIF) employing a reward model
attuned to local culture and values. The goal is to cultivate culturally
cognizant and value-aligned Arabic LLMs capable of accommodating the diverse,
application-specific needs of Arabic-speaking communities. Comprehensive
evaluations reveal that the resulting model, dubbed 'AceGPT', sets the
state-of-the-art standard for open Arabic LLMs across various benchmarks,
including the instruction-following benchmark (i.e., Arabic Vicuna-80 and
Arabic AlpacaEval), knowledge benchmark (i.e., Arabic MMLU and EXAMs), and the
newly introduced Arabic Cultural and Value Alignment benchmark. Notably, AceGPT
outperforms Turbo in the popular Vicuna-80 benchmark when evaluated with GPT-4,
despite the benchmark's limited scale. Codes, data, and models are in
https://github.com/FreedomIntelligence/AceGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnglE-optimized Text Embeddings. (arXiv:2309.12871v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12871">
<div class="article-summary-box-inner">
<span><p>High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis and Detection of Pathological Voice using Glottal Source Features. (arXiv:2309.14080v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14080">
<div class="article-summary-box-inner">
<span><p>Automatic detection of voice pathology enables objective assessment and
earlier intervention for the diagnosis. This study provides a systematic
analysis of glottal source features and investigates their effectiveness in
voice pathology detection. Glottal source features are extracted using glottal
flows estimated with the quasi-closed phase (QCP) glottal inverse filtering
method, using approximate glottal source signals computed with the zero
frequency filtering (ZFF) method, and using acoustic voice signals directly. In
addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from
the glottal source waveforms computed by QCP and ZFF to effectively capture the
variations in glottal source spectra of pathological voice. Experiments were
carried out using two databases, the Hospital Universitario Principe de
Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database.
Analysis of features revealed that the glottal source contains information that
discriminates normal and pathological voice. Pathology detection experiments
were carried out using support vector machine (SVM). From the detection
experiments it was observed that the performance achieved with the studied
glottal source features is comparable or better than that of conventional MFCCs
and perceptual linear prediction (PLP) features. The best detection performance
was achieved when the glottal source features were combined with the
conventional MFCCs and PLP features, which indicates the complementary nature
of the features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech. (arXiv:2309.14107v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14107">
<div class="article-summary-box-inner">
<span><p>Automatic detection and severity level classification of dysarthria directly
from acoustic speech signals can be used as a tool in medical diagnosis. In
this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor
to build detection and severity level classification systems for dysarthric
speech. The experiments were carried out with the popularly used UA-speech
database. In the detection experiments, the results revealed that the best
performance was obtained using the embeddings from the first layer of the
wav2vec model that yielded an absolute improvement of 1.23% in accuracy
compared to the best performing baseline feature (spectrogram). In the studied
severity level classification task, the results revealed that the embeddings
from the final layer gave an absolute improvement of 10.62% in accuracy
compared to the best baseline features (mel-frequency cepstral coefficients).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural State Classification in Semantic Inference Reading Comprehension. (arXiv:2309.15714v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.15714">
<div class="article-summary-box-inner">
<span><p>With the recent proliferation of large language models (LLMs), such as
Generative Pre-trained Transformers (GPT), there has been a significant shift
in exploring human and machine comprehension of semantic language meaning. This
shift calls for interdisciplinary research that bridges cognitive science and
natural language processing (NLP). This pilot study aims to provide insights
into individuals' neural states during a semantic relation
reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and
electroencephalographic (EEG) data to study how the brain processes words with
varying degrees of relevance to a keyword during reading. We also use a feature
engineering approach to improve the fixation-related EEG data classification
while participants read words with high versus low relevance to the keyword.
The best validation accuracy in this word-level classification is over 60\%
across 12 subjects. Words of high relevance to the inference keyword had
significantly more eye fixations per word: 1.0584 compared to 0.6576 when
excluding no-fixation words, and 1.5126 compared to 1.4026 when including them.
This study represents the first attempt to classify brain states at a word
level using LLM knowledge. It provides valuable insights into human cognitive
abilities and the realm of Artificial General Intelligence (AGI), and offers
guidance for developing potential reading-assisted technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.16039">
<div class="article-summary-box-inner">
<span><p>We present a series of long-context LLMs that support effective context
windows of up to 32,768 tokens. Our model series are built through continual
pretraining from Llama 2 with longer training sequences and on a dataset where
long texts are upsampled. We perform extensive evaluation on language modeling,
synthetic context probing tasks, and a wide range of research benchmarks. On
research benchmarks, our models achieve consistent improvements on most regular
tasks and significant improvements on long-context tasks over Llama 2. Notably,
with a cost-effective instruction tuning procedure that does not require
human-annotated long instruction data, the 70B variant can already surpass
gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.
Alongside these results, we provide an in-depth analysis on the individual
components of our method. We delve into Llama's position encodings and discuss
its limitation in modeling long dependencies. We also examine the impact of
various design choices in the pretraining process, including the data mix and
the training curriculum of sequence lengths -- our ablation experiments suggest
that having abundant long texts in the pretrain dataset is not the key to
achieving strong performance, and we empirically verify that long context
continual pretraining is more efficient and similarly effective compared to
pretraining from scratch with long sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning. (arXiv:2309.16705v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.16705">
<div class="article-summary-box-inner">
<span><p>Addressing the gap in understanding visual comprehension in Large Language
Models (LLMs), we designed a challenge-response study, subjecting Google Bard
and GPT-Vision to 64 visual tasks, spanning categories like "Visual Situational
Reasoning" and "Next Scene Prediction." Previous models, such as GPT4, leaned
heavily on optical character recognition tools like Tesseract, whereas Bard and
GPT-Vision, akin to Google Lens and Visual API, employ deep learning techniques
for visual text recognition. However, our findings spotlight both
vision-language model's limitations: while proficient in solving visual
CAPTCHAs that stump ChatGPT alone, it falters in recreating visual elements
like ASCII art or analyzing Tic Tac Toe grids, suggesting an over-reliance on
educated visual guesses. The prediction problem based on visual inputs appears
particularly challenging with no common-sense guesses for next-scene
forecasting based on current "next-token" multimodal models. This study
provides experimental insights into the current capacities and areas for
improvement in multimodal LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-Driver: Learning to Drive with GPT. (arXiv:2310.01415v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01415">
<div class="article-summary-box-inner">
<span><p>We present a simple yet effective approach that can transform the OpenAI
GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion
planning is a core challenge in autonomous driving, aiming to plan a driving
trajectory that is safe and comfortable. Existing motion planners predominantly
leverage heuristic methods to forecast driving trajectories, yet these
approaches demonstrate insufficient generalization capabilities in the face of
novel and unseen driving scenarios. In this paper, we propose a novel approach
to motion planning that capitalizes on the strong reasoning capabilities and
generalization potential inherent to Large Language Models (LLMs). The
fundamental insight of our approach is the reformulation of motion planning as
a language modeling problem, a perspective not previously explored.
Specifically, we represent the planner inputs and outputs as language tokens,
and leverage the LLM to generate driving trajectories through a language
description of coordinate positions. Furthermore, we propose a novel
prompting-reasoning-finetuning strategy to stimulate the numerical reasoning
potential of the LLM. With this strategy, the LLM can describe highly precise
trajectory coordinates and also its internal decision-making process in natural
language. We evaluate our approach on the large-scale nuScenes dataset, and
extensive experiments substantiate the effectiveness, generalization ability,
and interpretability of our GPT-based motion planner. Code is now available at
https://github.com/PointsCoder/GPT-Driver.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.06434">
<div class="article-summary-box-inner">
<span><p>We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.07276">
<div class="article-summary-box-inner">
<span><p>Recent advancements in biological research leverage the integration of
molecules, proteins, and natural language to enhance drug discovery. However,
current models exhibit several limitations, such as the generation of invalid
molecular SMILES, underutilization of contextual information, and equal
treatment of structured and unstructured knowledge. To address these issues, we
propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches
cross-modal integration in biology with chemical knowledge and natural language
associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular
representations and extracts knowledge from the surrounding context of
bio-entities in unstructured biological literature. Furthermore,
$\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,
leading to more effective utilization of information. After fine-tuning, BioT5
shows superior performance across a wide range of tasks, demonstrating its
strong capability of capturing underlying relations and properties of
bio-entities. Our code is available at
$\href{https://github.com/QizhiPei/BioT5}{Github}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08659">
<div class="article-summary-box-inner">
<span><p>Quantization is an indispensable technique for serving Large Language Models
(LLMs) and has recently found its way into LoRA fine-tuning. In this work we
focus on the scenario where quantization and LoRA fine-tuning are applied
together on a pre-trained model. In such cases it is common to observe a
consistent gap in the performance on downstream tasks between full fine-tuning
and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ
(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that
simultaneously quantizes an LLM and finds a proper low-rank initialization for
LoRA fine-tuning. Such an initialization alleviates the discrepancy between the
quantized and full-precision model and significantly improves the
generalization in downstream tasks. We evaluate our method on natural language
understanding, question answering, summarization, and natural language
generation tasks. Experiments show that our method is highly effective and
outperforms existing quantization methods, especially in the challenging 2-bit
and 2/4-bit mixed precision regimes. We will release our code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation. (arXiv:2310.08943v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08943">
<div class="article-summary-box-inner">
<span><p>Knowledge-grounded dialogue generation aims to mitigate the issue of text
degeneration by incorporating external knowledge to supplement the context.
However, the model often fails to internalize this information into responses
in a human-like manner. Instead, it simply inserts segments of the provided
knowledge into generic responses. As a result, the generated responses tend to
be tedious, incoherent, and in lack of interactivity which means the
degeneration problem is still unsolved. In this work, we first find that such
copying-style degeneration is primarily due to the weak likelihood objective,
which allows the model to "cheat" the objective by merely duplicating knowledge
segments in a superficial pattern matching based on overlap. To overcome this
challenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL)
framework that dynamically samples negative examples and subsequently penalizes
degeneration behaviors at both the token-level and sequence-level. Extensive
experiments on the WoW dataset demonstrate the effectiveness of our approach
across various pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09430">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly
advanced the performance of artificial systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness to perform logical reasoning remain under-evaluated. To probe this
ability, we propose three new logical reasoning datasets named "ReClor-plus",
"LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with
randomly shuffled options, the second with the correct choices replaced by
"none of the other options are correct", and a combination of the previous two
subsets. We carry out experiments on these datasets with both discriminative
and generative LLMs and show that these simple tricks greatly hinder the
performance of the language models. Despite their superior performance on the
original publicly available datasets, we find that all models struggle to
answer our newly constructed datasets. We show that introducing task variations
by perturbing a sizable training set can markedly improve the model's
generalisation and robustness in logical reasoning tasks. Moreover, applying
logic-driven data augmentation for fine-tuning, combined with prompting can
enhance the generalisation performance of both discriminative large language
models and generative large language models. These results offer insights into
assessing and improving the generalisation and robustness of large language
models for logical reasoning tasks. We make our source code and data publicly
available
\url{https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09520">
<div class="article-summary-box-inner">
<span><p>While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Expression Tree Decoding Strategy for Mathematical Equation Generation. (arXiv:2310.09619v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09619">
<div class="article-summary-box-inner">
<span><p>Generating mathematical equations from natural language requires an accurate
understanding of the relations among math expressions. Existing approaches can
be broadly categorized into token-level and expression-level generation. The
former treats equations as a mathematical language, sequentially generating
math tokens. Expression-level methods generate each expression one by one.
However, each expression represents a solving step, and there naturally exist
parallel or dependent relations between these steps, which are ignored by
current sequential methods. Therefore, we integrate tree structure into the
expression-level generation and advocate an expression tree decoding strategy.
To generate a tree with expression as its node, we employ a layer-wise parallel
decoding strategy: we decode multiple independent expressions (leaf nodes) in
parallel at each layer and repeat parallel decoding layer by layer to
sequentially generate these parent node expressions that depend on others.
Besides, a bipartite matching algorithm is adopted to align multiple
predictions with annotations for each layer. Experiments show our method
outperforms other baselines, especially for these equations with complex
structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring. (arXiv:2310.09680v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09680">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) has witnessed a profound research
interest. Recent breakthroughs have given ASR systems different prospects such
as faithfully transcribing spoken language, which is a pivotal advancement in
building conversational agents. However, there is still an imminent challenge
of accurately discerning context-dependent words and phrases. In this work, we
propose a novel approach for enhancing contextual recognition within ASR
systems via semantic lattice processing leveraging the power of deep learning
models in accurately delivering spot-on transcriptions across a wide variety of
vocabularies and speaking styles. Our solution consists of using Hidden Markov
Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks
(DNN) models integrating both language and acoustic modeling for better
accuracy. We infused our network with the use of a transformer-based model to
properly rescore the word lattice achieving remarkable capabilities with a
palpable reduction in Word Error Rate (WER). We demonstrate the effectiveness
of our proposed framework on the LibriSpeech dataset with empirical analyses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. (arXiv:2310.09909v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.09909">
<div class="article-summary-box-inner">
<span><p>Driven by the large foundation models, the development of artificial
intelligence has witnessed tremendous progress lately, leading to a surge of
general interest from the public. In this study, we aim to assess the
performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm
of multimodal medical diagnosis. Our evaluation encompasses 17 human body
systems, including Central Nervous System, Head and Neck, Cardiac, Chest,
Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,
Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,
Pediatrics, with images taken from 8 modalities used in daily clinic routine,
e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),
Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA),
Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on
multiple clinical tasks with or without patent history provided, including
imaging modality and anatomy recognition, disease diagnosis, report generation,
disease localisation.
</p>
<p>Our observation shows that, while GPT-4V demonstrates proficiency in
distinguishing between medical image modalities and anatomy, it faces
significant challenges in disease diagnosis and generating comprehensive
reports. These findings underscore that while large multimodal models have made
significant advancements in computer vision and natural language processing, it
remains far from being used to effectively support real-world medical
applications and clinical decision-making.
</p>
<p>All images used in this report can be found in
https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP for Crypto-Asset Regulation: A Roadmap. (arXiv:2310.10333v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.10333">
<div class="article-summary-box-inner">
<span><p>In the rapidly evolving field of crypto-assets, white papers are essential
documents for investor guidance, and are now subject to unprecedented content
requirements under the EU's Markets in Crypto-Assets Regulation (MiCAR).
Natural Language Processing can serve as a powerful tool for both analyzing
these documents and assisting in regulatory compliance. This paper delivers two
contributions to the topic. First, we survey existing applications of textual
analysis to unregulated crypto-asset white papers, uncovering a research gap
that could be bridged with interdisciplinary collaboration. We then conduct an
analysis of the changes introduced by MiCAR, highlighting the opportunities and
challenges of integrating NLP within the new regulatory framework. The findings
set the stage for further research, with the potential to benefit regulators,
crypto-asset issuers, and investors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defining implication relation for classical logic. (arXiv:1312.7832v10 [math.LO] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1312.7832">
<div class="article-summary-box-inner">
<span><p>In classical logic, "P implies Q" is equivalent to "not-P or Q". It is well
known that the equivalence is problematic. Actually, from "P implies Q", "not-P
or Q" can be inferred ("Implication-to-disjunction" is valid), while from
"not-P or Q", "P implies Q" cannot be inferred in general
("Disjunction-to-implication" is not generally valid), so the equivalence
between them is invalid in general. This work aims to remove exactly the
incorrect Disjunction-to-implication from classical logic (CL). The paper
proposes a logical system (IRL) with the expected properties: (1) CL is simply
obtained by adding Disjunction-to-implication to IRL, and (2)
Disjunction-to-implication is independent of IRL (either
Disjunction-to-implication or its negation cannot be derived in IRL) in the
general case. In other words, IRL is just the system obtained by exactly
removing Disjunction-to-implication from CL.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-18 23:11:15.278903579 UTC">2023-10-18 23:11:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>