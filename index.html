<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-03T01:30:00Z">11-03</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CascadeXML: Rethinking Transformers for End-to-end Multi-resolution Training in Extreme Multi-label Classification. (arXiv:2211.00640v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00640">
<div class="article-summary-box-inner">
<span><p>Extreme Multi-label Text Classification (XMC) involves learning a classifier
that can assign an input with a subset of most relevant labels from millions of
label choices. Recent approaches, such as XR-Transformer and LightXML, leverage
a transformer instance to achieve state-of-the-art performance. However, in
this process, these approaches need to make various trade-offs between
performance and computational requirements. A major shortcoming, as compared to
the Bi-LSTM based AttentionXML, is that they fail to keep separate feature
representations for each resolution in a label tree. We thus propose
CascadeXML, an end-to-end multi-resolution learning pipeline, which can harness
the multi-layered architecture of a transformer model for attending to
different label resolutions with separate feature representations. CascadeXML
significantly outperforms all existing approaches with non-trivial gains
obtained on benchmark datasets consisting of up to three million labels. Code
for CascadeXML will be made publicly available at
\url{https://github.com/xmc-aalto/cascadexml}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Inter-character Relationship-driven Story Generation. (arXiv:2211.00676v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00676">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the task of modeling interpersonal relationships
for story generation. For addressing this task, we propose Relationships as
Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories
sentence by sentence and has two major components - a relationship selector and
a story continuer. The relationship selector specifies a latent variable to
pick the relationship to exhibit in the next sentence and the story continuer
generates the next sentence while expressing the selected relationship in a
coherent way. Our automatic and human evaluations demonstrate that ReLiSt is
able to generate stories with relationships that are more faithful to desired
relationships while maintaining the content quality. The relationship
assignments to sentences during inference bring interpretability to ReLiSt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TOE: A Grid-Tagging Discontinuous NER Model Enhanced by Embedding Tag/Word Relations and More Fine-Grained Tags. (arXiv:2211.00684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00684">
<div class="article-summary-box-inner">
<span><p>So far, discontinuous named entity recognition (NER) has received increasing
research attention and many related methods have surged such as
hypergraph-based methods, span-based methods, and sequence-to-sequence
(Seq2Seq) methods, etc. However, these methods more or less suffer from some
problems such as decoding ambiguity and efficiency, which limit their
performance. Recently, grid-tagging methods, which benefit from the flexible
design of tagging systems and model architectures, have shown superiority to
adapt for various information extraction tasks. In this paper, we follow the
line of such methods and propose a competitive grid-tagging model for
discontinuous NER. We call our model TOE because we incorporate two kinds of
Tag-Oriented Enhancement mechanisms into a state-of-the-art (SOTA) grid-tagging
model that casts the NER problem into word-word relationship prediction. First,
we design a Tag Representation Embedding Module (TREM) to force our model to
consider not only word-word relationships but also word-tag and tag-tag
relationships. Concretely, we construct tag representations and embed them into
TREM, so that TREM can treat tag and word representations as
queries/keys/values and utilize self-attention to model their relationships. On
the other hand, motivated by the Next-Neighboring-Word (NNW) and Tail-Head-Word
(THW) tags in the SOTA model, we add two new symmetric tags, namely
Previous-Neighboring-Word (PNW) and Head-Tail-Word (HTW), to model more
fine-grained word-word relationships and alleviate error propagation from tag
prediction. In the experiments of three benchmark datasets, namely CADEC,
ShARe13 and ShARe14, our TOE model pushes the SOTA results by about 0.83%,
0.05% and 0.66% in F1, demonstrating its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural Language Instructions. (arXiv:2211.00688v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00688">
<div class="article-summary-box-inner">
<span><p>The adoption of pre-trained language models to generate action plans for
embodied agents is a promising research strategy. However, execution of
instructions in real or simulated environments requires verification of the
feasibility of actions as well as their relevance to the completion of a goal.
We propose a new method that combines a language model and reinforcement
learning for the task of building objects in a Minecraft-like environment
according to the natural language instructions. Our method first generates a
set of consistently achievable sub-goals from the instructions and then
completes associated sub-tasks with a pre-trained RL policy. The proposed
method formed the RL baseline at the IGLU 2022 competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Pivoting Model for Effective Event Detection. (arXiv:2211.00709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00709">
<div class="article-summary-box-inner">
<span><p>Event Detection, which aims to identify and classify mentions of event
instances from unstructured articles, is an important task in Natural Language
Processing (NLP). Existing techniques for event detection only use homogeneous
one-hot vectors to represent the event type classes, ignoring the fact that the
semantic meaning of the types is important to the task. Such an approach is
inefficient and prone to overfitting. In this paper, we propose a Semantic
Pivoting Model for Effective Event Detection (SPEED), which explicitly
incorporates prior information during training and captures semantically
meaningful correlations between input and events. Experimental results show
that our proposed model achieves state-of-the-art performance and outperforms
the baselines in multiple settings without using any external resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Natural Language Generation on Near-Term Devices. (arXiv:2211.00727v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00727">
<div class="article-summary-box-inner">
<span><p>The emergence of noisy medium-scale quantum devices has led to
proof-of-concept applications for quantum computing in various domains.
Examples include Natural Language Processing (NLP) where sentence
classification experiments have been carried out, as well as procedural
generation, where tasks such as geopolitical map creation, and image
manipulation have been performed. We explore applications at the intersection
of these two areas by designing a hybrid quantum-classical algorithm for
sentence generation.
</p>
<p>Our algorithm is based on the well-known simulated annealing technique for
combinatorial optimisation. An implementation is provided and used to
demonstrate successful sentence generation on both simulated and real quantum
hardware. A variant of our algorithm can also be used for music generation.
</p>
<p>This paper aims to be self-contained, introducing all the necessary
background on NLP and quantum computing along the way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00732">
<div class="article-summary-box-inner">
<span><p>Online encyclopedias, such as Wikipedia, have been well-developed and
researched in the last two decades. One can find any attributes or other
information of a wiki item on a wiki page edited by a community of volunteers.
However, the traditional text along with images can hardly express some other
aspects of an item. For example, when we talk about "Shiba Inu", one may care
more about "How to feed it" or "How to train it to not protect its food".
Currently, short-video platforms have become a hallmark in the online world.
Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts, short-video
apps have changed how we consume and create content today. Except for
entertainment short videos, we can find more and more authors sharing
insightful knowledge widely across all walks of life. These short videos, which
we call knowledge videos, can easily express any aspects (E.g. hair or
how-to-feed) consumers want to know about an item (E.g. Shiba Inu), and they
can be systematically analyzed and organized like an online encyclopedia. In
this paper, we propose Kuaipedia, a massive multi-modal encyclopedia consisting
of items, aspects, and short videos linking to them, which is extracted from
billions of videos of Kuaishou, a well-known short-video platform in China. We
first collected items from multiple sources and mined user-centered aspects
from millions of users' queries to build an item-aspect tree. Then we propose a
new task called "multi-modal item-aspect linking" as an expansion of "entity
linking" to link short videos into item-aspect pairs and build the whole short
video encyclopedia. Intrinsic evaluations show that our encyclopedia is of
large scale and highly accurate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality. (arXiv:2211.00768v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00768">
<div class="article-summary-box-inner">
<span><p>Recent visuolinguistic pre-trained models show promising progress on various
end tasks such as image retrieval and video captioning. Yet, they fail
miserably on the recently proposed Winoground dataset, which challenges models
to match paired images and English captions, with items constructed to overlap
lexically but differ in meaning (e.g., "there is a mug in some grass" vs.
"there is some grass in a mug"). By annotating the dataset using new
fine-grained tags, we show that solving the Winoground task requires not just
compositional language understanding, but a host of other abilities like
commonsense reasoning or locating small, out-of-focus objects in low-resolution
images. In this paper, we identify the dataset's main challenges through a
suite of experiments on related tasks (probing task, image retrieval task),
data augmentation, and manual inspection of the dataset. Our analysis suggests
that a main challenge in visuolinguistic models may lie in fusing visual and
textual representations, rather than in compositional language understanding.
We release our annotation and code at
https://github.com/ajd12342/why-winoground-hard .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified End-to-End Speech Recognition and Endpointing for Fast and Efficient Speech Systems. (arXiv:2211.00786v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00786">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) systems typically rely on an external
endpointer (EP) model to identify speech boundaries. In this work, we propose a
method to jointly train the ASR and EP tasks in a single end-to-end (E2E)
multitask model, improving EP quality by optionally leveraging information from
the ASR audio encoder. We introduce a "switch" connection, which trains the EP
to consume either the audio frames directly or low-level latent representations
from the ASR model. This results in a single E2E model that can be used during
inference to perform frame filtering at low cost, and also make high quality
end-of-query (EOQ) predictions based on ongoing ASR computation. We present
results on a voice search test set showing that, compared to separate
single-task models, this approach reduces median endpoint latency by 120 ms
(30.8% reduction), and 90th percentile latency by 170 ms (23.0% reduction),
without regressing word error rate. For continuous recognition, WER improves by
10.6% (relative).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder. (arXiv:2211.00792v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00792">
<div class="article-summary-box-inner">
<span><p>We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech
recognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced
encoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR
has been actively studied, aiming to utilize versatile linguistic knowledge for
generating accurate text. One crucial factor that makes this integration
challenging lies in the vocabulary mismatch; the vocabulary constructed for a
pre-trained LM is generally too large for E2E-ASR training and is likely to
have a mismatch against a target ASR domain. To overcome such an issue, we
propose BECTRA, an extended version of our previous BERT-CTC, that realizes
BERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based
model, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder
using a vocabulary suitable for a target task. With the combination of the
transducer and BERT-CTC, we also propose a novel inference algorithm for taking
advantage of both autoregressive and non-autoregressive decoding. Experimental
results on several ASR tasks, varying in amounts of data, speaking styles, and
languages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing
with the vocabulary mismatch while exploiting BERT knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss. (arXiv:2211.00795v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00795">
<div class="article-summary-box-inner">
<span><p>This paper presents InterMPL, a semi-supervised learning method of end-to-end
automatic speech recognition (ASR) that performs pseudo-labeling (PL) with
intermediate supervision. Momentum PL (MPL) trains a connectionist temporal
classification (CTC)-based model on unlabeled data by continuously generating
pseudo-labels on the fly and improving their quality. In contrast to
autoregressive formulations, such as the attention-based encoder-decoder and
transducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in
general, owing to its simple/fast inference algorithm and robustness against
generating collapsed labels. However, CTC generally yields inferior performance
than the autoregressive models due to the conditional independence assumption,
thereby limiting the performance of MPL. We propose to enhance MPL by
introducing intermediate loss, inspired by the recent advances in CTC-based
modeling. Specifically, we focus on self-conditional and hierarchical
conditional CTC, that apply auxiliary CTC losses to intermediate layers such
that the conditional independence assumption is explicitly relaxed. We also
explore how pseudo-labels should be generated and used as supervision for
intermediate losses. Experimental results in different semi-supervised settings
demonstrate that the proposed approach outperforms MPL and improves an ASR
model by up to a 12.1% absolute performance gain. In addition, our detailed
analysis validates the importance of the intermediate loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Title2Event: Benchmarking Open Event Extraction with a Large-scale Chinese Title Dataset. (arXiv:2211.00869v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00869">
<div class="article-summary-box-inner">
<span><p>Event extraction (EE) is crucial to downstream tasks such as new aggregation
and event knowledge graph construction. Most existing EE datasets manually
define fixed event types and design specific schema for each of them, failing
to cover diverse events emerging from the online text. Moreover, news titles,
an important source of event mentions, have not gained enough attention in
current EE research. In this paper, We present Title2Event, a large-scale
sentence-level dataset benchmarking Open Event Extraction without restricting
event types. Title2Event contains more than 42,000 news titles in 34 topics
collected from Chinese web pages. To the best of our knowledge, it is currently
the largest manually-annotated Chinese dataset for open event extraction. We
further conduct experiments on Title2Event with different models and show that
the characteristics of titles make it challenging for event extraction,
addressing the significance of advanced study on this problem. The dataset and
baseline codes are available at https://open-event-hub.github.io/title2event.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations. (arXiv:2211.00881v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00881">
<div class="article-summary-box-inner">
<span><p>Syntactically controlled paraphrase generation has become an emerging
research direction in recent years. Most existing approaches require annotated
paraphrase pairs for training and are thus costly to extend to new domains.
Unsupervised approaches, on the other hand, do not need paraphrase pairs but
suffer from relatively poor performance in terms of syntactic control and
quality of generated paraphrases. In this paper, we demonstrate that leveraging
Abstract Meaning Representations (AMR) can greatly improve the performance of
unsupervised syntactically controlled paraphrase generation. Our proposed
model, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR
graph and the constituency parse of the input sentence into two disentangled
semantic and syntactic embeddings. A decoder is then learned to reconstruct the
input sentence from the semantic and syntactic embeddings. Our experiments show
that AMRPG generates more accurate syntactically controlled paraphrases, both
quantitatively and qualitatively, compared to the existing unsupervised
approaches. We also demonstrate that the paraphrases generated by AMRPG can be
used for data augmentation to improve the robustness of NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIMD-size aware weight regularization for fast neural vocoding on CPU. (arXiv:2211.00898v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00898">
<div class="article-summary-box-inner">
<span><p>This paper proposes weight regularization for a faster neural vocoder.
Pruning time-consuming DNN modules is a promising way to realize a real-time
vocoder on a CPU (e.g. WaveRNN, LPCNet). Regularization that encourages
sparsity is also effective in avoiding the quality degradation created by
pruning. However, the orders of weight matrices must be contiguous in SIMD size
for fast vocoding. To ensure this order, we propose explicit SIMD size aware
regularization. Our proposed method reshapes a weight matrix into a tensor so
that the weights are aligned by group size in advance, and then computes the
group Lasso-like regularization loss. Experiments on 70% sparse subband WaveRNN
show that pruning in conventional Lasso and column-wise group Lasso degrades
the synthetic speech's naturalness. The vocoder with proposed regularization 1)
achieves comparable naturalness to that without pruning and 2) performs
meaningfully faster than other conventional vocoders using regularization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation. (arXiv:2211.00910v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00910">
<div class="article-summary-box-inner">
<span><p>Recently, the practical deployment of open-domain dialogue systems has been
plagued by the knowledge issue of information deficiency and factual
inaccuracy. To this end, we introduce PLATO-K based on two-stage dialogic
learning to strengthen internal knowledge memorization and external knowledge
exploitation. In the first stage, PLATO-K learns through massive dialogue
corpora and memorizes essential knowledge into model parameters. In the second
stage, PLATO-K mimics human beings to search for external information and to
leverage the knowledge in response generation. Extensive experiments reveal
that the knowledge issue is alleviated significantly in PLATO-K with such
comprehensive internal and external knowledge enhancement. Compared to the
existing state-of-the-art Chinese dialogue model, the overall engagingness of
PLATO-K is improved remarkably by 36.2% and 49.2% on chit-chat and
knowledge-intensive conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models. (arXiv:2211.00915v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00915">
<div class="article-summary-box-inner">
<span><p>Retriever-reader models achieve competitive performance across many different
NLP tasks such as open question answering and dialogue conversations. In this
work, we notice these models easily overfit the top-rank retrieval passages and
standard training fails to reason over the entire retrieval passages. We
introduce a learnable passage mask mechanism which desensitizes the impact from
the top-rank retrieval passages and prevents the model from overfitting.
Controlling the gradient variance with fewer mask candidates and selecting the
mask candidates with one-shot bi-level optimization, our learnable
regularization strategy enforces the answer generation to focus on the entire
retrieval passages. Experiments on different tasks across open question
answering, dialogue conversation, and fact verification show that our method
consistently outperforms its baselines. Extensive experiments and ablation
studies demonstrate that our method can be general, effective, and beneficial
for many NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialect-robust Evaluation of Generated Text. (arXiv:2211.00922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00922">
<div class="article-summary-box-inner">
<span><p>Evaluation metrics that are not robust to dialect variation make it
impossible to tell how well systems perform for many groups of users, and can
even penalize systems for producing text in lower-resource dialects. However,
currently, there exists no way to quantify how metrics respond to change in the
dialect of a generated utterance. We thus formalize dialect robustness and
dialect awareness as goals for NLG evaluation metrics. We introduce a suite of
methods and corresponding statistical tests one can use to assess metrics in
light of the two goals. Applying the suite to current state-of-the-art metrics,
we demonstrate that they are not dialect-robust and that semantic perturbations
frequently lead to smaller decreases in a metric than the introduction of
dialect features. As a first step to overcome this limitation, we propose a
training schema, NANO, which introduces regional and language information to
the pretraining process of a metric. We demonstrate that NANO provides a
size-efficient way for models to improve the dialect robustness while
simultaneously improving their performance on the standard metric benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00923">
<div class="article-summary-box-inner">
<span><p>One of the biggest challenges in designing mispronunciation detection models
is the unavailability of labeled L2 speech data. To overcome such data
scarcity, we introduce SpeechBlender -- a fine-grained data augmentation
pipeline for generating mispronunciation errors. The SpeechBlender utilizes
varieties of masks to target different regions of a phonetic unit, and use the
mixing factors to linearly interpolate raw speech signals while generating
erroneous pronunciation instances. The masks facilitate smooth blending of the
signals, thus generating more effective samples than the `Cut/Paste' method. We
show the effectiveness of our augmentation technique in a phoneme-level
pronunciation quality assessment task, leveraging only a good pronunciation
dataset. With SpeechBlender augmentation, we observed a 3% and 2% increase in
Pearson correlation coefficient (PCC) compared to no-augmentation and goodness
of pronunciation augmentation scenarios respectively for Speechocean762
testset. Moreover, a 2% rise in PCC is observed when comparing our single-task
phoneme-level mispronunciation detection model with a multi-task learning model
using multiple-granularity information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Internal Language Model Estimation based Adaptive Language Model Fusion for Domain Adaptation. (arXiv:2211.00968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00968">
<div class="article-summary-box-inner">
<span><p>ASR model deployment environment is ever-changing, and the incoming speech
can be switched across different domains during a session. This brings a
challenge for effective domain adaptation when only target domain text data is
available, and our objective is to obtain obviously improved performance on the
target domain while the performance on the general domain is less undermined.
In this paper, we propose an adaptive LM fusion approach called internal
language model estimation based adaptive domain adaptation (ILME-ADA). To
realize such an ILME-ADA, an interpolated log-likelihood score is calculated
based on the maximum of the scores from the internal LM and the external LM
(ELM) respectively. We demonstrate the efficacy of the proposed ILME-ADA method
with both RNN-T and LAS modeling frameworks employing neural network and n-gram
LMs as ELMs respectively on two domain specific (target) test sets. The
proposed method can achieve significantly better performance on the target test
sets while it gets minimal performance degradation on the general test set,
compared with both shallow and ILME-based LM fusion methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer. (arXiv:2211.00974v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00974">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformers currently dominate most NLP tasks. They impose,
however, limits on the maximum input length (512 sub-words in BERT), which are
too restrictive in the legal domain. Even sparse-attention models, such as
Longformer and BigBird, which increase the maximum input length to 4,096
sub-words, severely truncate texts in three of the six datasets of LexGLUE.
Simpler linear classifiers with TF-IDF features can handle texts of any length,
require far less resources to train and deploy, but are usually outperformed by
pre-trained Transformers. We explore two directions to cope with long legal
texts: (i) modifying a Longformer warm-started from LegalBERT to handle even
longer texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use
TF-IDF representations. The first approach is the best in terms of performance,
surpassing a hierarchical version of LegalBERT, which was the previous state of
the art in LexGLUE. The second approach leads to computationally more efficient
models at the expense of lower performance, but the resulting models still
outperform overall a linear SVM with TF-IDF features in long legal document
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual Recognizers Fusion for Code-switching Speech Recognition. (arXiv:2211.01046v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01046">
<div class="article-summary-box-inner">
<span><p>The bi-encoder structure has been intensively investigated in code-switching
(CS) automatic speech recognition (ASR). However, most existing methods require
the structures of two monolingual ASR models (MAMs) should be the same and only
use the encoder of MAMs. This leads to the problem that pre-trained MAMs cannot
be timely and fully used for CS ASR. In this paper, we propose a monolingual
recognizers fusion method for CS ASR. It has two stages: the speech awareness
(SA) stage and the language fusion (LF) stage. In the SA stage, acoustic
features are mapped to two language-specific predictions by two independent
MAMs. To keep the MAMs focused on their own language, we further extend the
language-aware training strategy for the MAMs. In the LF stage, the BELM fuses
two language-specific predictions to get the final prediction. Moreover, we
propose a text simulation strategy to simplify the training process of the BELM
and reduce reliance on CS data. Experiments on a Mandarin-English corpus show
the efficiency of the proposed method. The mix error rate is significantly
reduced on the test set after using open-source pre-trained MAMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Knowledge Distillation for Pre-trained Language Models. (arXiv:2211.01071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01071">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is an effective framework to transfer knowledge
from a large-scale teacher to a compact yet well-performing student. Previous
KD practices for pre-trained language models mainly transfer knowledge by
aligning instance-wise outputs between the teacher and student, while
neglecting an important knowledge source, i.e., the gradient of the teacher.
The gradient characterizes how the teacher responds to changes in inputs, which
we assume is beneficial for the student to better approximate the underlying
mapping function of the teacher. Therefore, we propose Gradient Knowledge
Distillation (GKD) to incorporate the gradient alignment objective into the
distillation process. Experimental results show that GKD outperforms previous
KD methods regarding student performance. Further analysis shows that
incorporating gradient knowledge makes the student behave more consistently
with the teacher, improving the interpretability greatly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based encoder-encoder architecture for Spoken Term Detection. (arXiv:2211.01089v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01089">
<div class="article-summary-box-inner">
<span><p>The paper presents a method for spoken term detection based on the
Transformer architecture. We propose the encoder-encoder architecture employing
two BERT-like encoders with additional modifications, including convolutional
and upsampling layers, attention masking, and shared parameters. The encoders
project a recognized hypothesis and a searched term into a shared embedding
space, where the score of the putative hit is computed using the calibrated dot
product. In the experiments, we used the Wav2Vec 2.0 speech recognizer, and the
proposed system outperformed a baseline method based on deep LSTMs on the
English and Czech STD datasets based on USC Shoah Foundation Visual History
Archive (MALACH).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User-Entity Differential Privacy in Learning Natural Language Models. (arXiv:2211.01141v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01141">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel concept of user-entity differential
privacy (UeDP) to provide formal privacy protection simultaneously to both
sensitive entities in textual data and data owners in learning natural language
models (NLMs). To preserve UeDP, we developed a novel algorithm, called
UeDP-Alg, optimizing the trade-off between privacy loss and model utility with
a tight sensitivity bound derived from seamlessly combining user and sensitive
entity sampling processes. An extensive theoretical analysis and evaluation
show that our UeDP-Alg outperforms baseline approaches in model utility under
the same privacy budget consumption on several NLM tasks, using benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01180">
<div class="article-summary-box-inner">
<span><p>This work investigates the use of large-scale, pre-trained models (CLIP and
HuBERT) for multilingual speech-image retrieval. For non-English speech-image
retrieval, we outperform the current state-of-the-art performance by a wide
margin when training separate models for each language, and show that a single
model which processes speech in all three languages still achieves retrieval
scores comparable with the prior state-of-the-art. We identify key differences
in model behavior and performance between English and non-English settings,
presumably attributable to the English-only pre-training of CLIP and HuBERT.
Finally, we show that our models can be used for mono- and cross-lingual
speech-text retrieval and cross-lingual speech-speech retrieval, despite never
having seen any parallel speech-text or speech-speech data during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model. (arXiv:2211.01200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01200">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models play an important role in
cross-lingual natural language understanding tasks. However, existing methods
did not focus on learning the semantic structure of representation, and thus
could not optimize their performance. In this paper, we propose Multi-level
Multilingual Knowledge Distillation (MMKD), a novel method for improving
multilingual language models. Specifically, we employ a teacher-student
framework to adopt rich semantic representation knowledge in English BERT. We
propose token-, word-, sentence-, and structure-level alignment objectives to
encourage multiple levels of consistency between source-target pairs and
correlation similarity between teacher and student models. We conduct
experiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and
XQuAD. Experimental results show that MMKD outperforms other baseline models of
similar size on XNLI and XQuAD and obtains comparable performance on PAWS-X.
Especially, MMKD obtains significant performance gains on low-resource
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01246">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new Self-Supervised Learning (SSL) algorithm
called data2vec-aqc, for speech representation learning from unlabeled speech
data. Our goal is to improve SSL for speech in domains where both unlabeled and
labeled data are limited. Building on the recently introduced data2vec, we
introduce additional modules to the data2vec framework that leverage the
benefit of data augmentations, quantized representations, and clustering. The
interaction between these modules helps solve the cross-contrastive loss as an
additional self-supervised objective. data2vec-aqc achieves up to 14.1% and
20.9% relative WER improvement over the existing state-of-the-art data2vec
system on the test-clean and test-other sets, respectively, of LibriSpeech,
without the use of any language model. Our proposed model also achieves up to
17.8% relative WER improvement over the baseline data2vec when fine-tuned on
Switchboard data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Vector Retrieval as Sparse Alignment. (arXiv:2211.01267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01267">
<div class="article-summary-box-inner">
<span><p>Multi-vector retrieval models improve over single-vector dual encoders on
many information retrieval tasks. In this paper, we cast the multi-vector
retrieval problem as sparse alignment between query and document tokens. We
propose AligneR, a novel multi-vector retrieval model that learns sparsified
pairwise alignments between query and document tokens (e.g. `dog' vs. `puppy')
and per-token unary saliences reflecting their relative importance for
retrieval. We show that controlling the sparsity of pairwise token alignments
often brings significant performance gains. While most factoid questions
focusing on a specific part of a document require a smaller number of
alignments, others requiring a broader understanding of a document favor a
larger number of alignments. Unary saliences, on the other hand, decide whether
a token ever needs to be aligned with others for retrieval (e.g. `kind' from
`kind of currency is used in new zealand}'). With sparsified unary saliences,
we are able to prune a large number of query and document token vectors and
improve the efficiency of multi-vector retrieval. We learn the sparse unary
saliences with entropy-regularized linear programming, which outperforms other
methods to achieve sparsity. In a zero-shot setting, AligneR scores 51.1 points
nDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the
BEIR benchmark. In addition, adapting pairwise alignments with a few examples
(&lt;= 8) further improves the performance up to 15.7 points nDCG@10 for argument
retrieval tasks. The unary saliences of AligneR helps us to keep only 20% of
the document token representations with minimal performance loss. We further
show that our model often produces interpretable alignments and significantly
improves its performance when initialized from larger language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Impact of Social Media Posts by Executives on Stock Prices. (arXiv:2211.01287v1 [q-fin.ST])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01287">
<div class="article-summary-box-inner">
<span><p>Predicting stock market movements has always been of great interest to
investors and an active area of research. Research has proven that popularity
of products is highly influenced by what people talk about. Social media like
Twitter, Reddit have become hotspots of such influences. This paper
investigates the impact of social media posts on close price prediction of
stocks using Twitter and Reddit posts. Our objective is to integrate sentiment
of social media data with historical stock data and study its effect on closing
prices using time series models. We carried out rigorous experiments and deep
analysis using multiple deep learning based models on different datasets to
study the influence of posts by executives and general people on the close
price. Experimental results on multiple stocks (Apple and Tesla) and
decentralised currencies (Bitcoin and Ethereum) consistently show improvements
in prediction on including social media data and greater improvements on
including executive posts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Intrinsic Compositionality In Transformers With Tree Projections. (arXiv:2211.01288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01288">
<div class="article-summary-box-inner">
<span><p>When trained on language data, do transformers learn some arbitrary
computation that utilizes the full capacity of the architecture or do they
learn a simpler, tree-like computation, hypothesized to underlie compositional
meaning systems like human languages? There is an apparent tension between
compositional accounts of human language understanding, which are based on a
restricted bottom-up computational process, and the enormous success of neural
models like transformers, which can route information arbitrarily between
different parts of their input. One possibility is that these models, while
extremely flexible in principle, in practice learn to interpret language
hierarchically, ultimately building sentence representations close to those
predictable by a bottom-up, tree-structured model. To evaluate this
possibility, we describe an unsupervised and parameter-free method to
\emph{functionally project} the behavior of any transformer into the space of
tree-structured networks. Given an input sentence, we produce a binary tree
that approximates the transformer's representation-building process and a score
that captures how "tree-like" the transformer's behavior is on the input. While
calculation of this score does not require training any additional models, it
provably upper-bounds the fit between a transformer and any tree-structured
approximation. Using this method, we show that transformers for three different
tasks become more tree-like over the course of training, in some cases
unsupervisedly recovering the same trees as supervised parsers. These trees, in
turn, are predictive of model behavior, with more tree-like models generalizing
better on tests of compositional generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting word frequencies in authorship attribution. (arXiv:2211.01289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01289">
<div class="article-summary-box-inner">
<span><p>In this paper, I introduce a simple method of computing relative word
frequencies for authorship attribution and similar stylometric tasks. Rather
than computing relative frequencies as the number of occurrences of a given
word divided by the total number of tokens in a text, I argue that a more
efficient normalization factor is the total number of relevant tokens only. The
notion of relevant words includes synonyms and, usually, a few dozen other
words in some ways semantically similar to a word in question. To determine
such a semantic background, one of word embedding models can be used. The
proposed method outperforms classical most-frequent-word approaches
substantially, usually by a few percentage points depending on the input
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Algebraic Framework for Stock & Flow Diagrams and Dynamical Systems Using Category Theory. (arXiv:2211.01290v1 [cs.LO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01290">
<div class="article-summary-box-inner">
<span><p>Mathematical modeling of infectious disease at scale is important, but
challenging. Some of these difficulties can be alleviated by an approach that
takes diagrams seriously as mathematical formalisms in their own right. Stock &amp;
flow diagrams are widely used as broadly accessible building blocks for
infectious disease modeling. In this chapter, rather than focusing on the
underlying mathematics, we informally use communicable disease examples created
by the implemented software of StockFlow.jl to explain the basics,
characteristics, and benefits of the categorical framework. We first
characterize categorical stock &amp; flow diagrams, and note the clear separation
between the syntax of stock &amp; flow diagrams and their semantics, demonstrating
three examples of semantics already implemented in the software: ODEs, causal
loop diagrams, and system structure diagrams. We then establish composition and
stratification frameworks and examples for stock &amp; flow diagrams. Applying
category theory, these frameworks can build large diagrams from smaller ones in
a modular fashion. Finally, we introduce the open-source ModelCollab software
for diagram-centric real-time collaborative modeling. Using the graphical user
interface, this web-based software allows the user to undertake the types of
categorically-rooted operations discussed above, but without any knowledge of
their categorical foundations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation. (arXiv:2211.01292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01292">
<div class="article-summary-box-inner">
<span><p>The cornerstone of multilingual neural translation is shared representations
across languages. Given the theoretically infinite representation power of
neural networks, semantically identical sentences are likely represented
differently. While representing sentences in the continuous latent space
ensures expressiveness, it introduces the risk of capturing of irrelevant
features which hinders the learning of a common representation. In this work,
we discretize the encoder output latent space of multilingual models by
assigning encoder states to entries in a codebook, which in effect represents
source sentences in a new artificial language. This discretization process not
only offers a new way to interpret the otherwise black-box model
representations, but, more importantly, gives potential for increasing
robustness in unseen testing conditions. We validate our approach on
large-scale experiments with realistic data volumes and domains. When tested in
zero-shot conditions, our approach is competitive with two strong alternatives
from the literature. We also use the learned artificial language to analyze
model behavior, and discover that using a similar bridge language increases
knowledge-sharing among the remaining languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-end Speaker Diarization in the Wild. (arXiv:2211.01299v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01299">
<div class="article-summary-box-inner">
<span><p>Speaker diarization algorithms address the "who spoke when" problem in audio
recordings. Algorithms trained end-to-end have proven superior to classical
modular-cascaded systems in constrained scenarios with a small number of
speakers. However, their performance for in-the-wild recordings containing more
speakers with shorter utterance lengths remains to be investigated. In this
paper, we address this gap, showing that an attractor-based end-to-end system
can also perform remarkably well in the latter scenario when first pre-trained
on a carefully-designed simulated dataset that matches the distribution of
in-the-wild recordings. We also propose to use an attention mechanism to
increase the network capacity in decoding more speaker attractors, and to
jointly train the attractors on a speaker recognition task to improve the
speaker attractor representation. Even though the model we propose is
audio-only, we find it significantly outperforms both audio-only and
audio-visual baselines on the AVA-AVD benchmark dataset, achieving
state-of-the-art results with an absolute reduction in diarization error of
23.3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting phoneme-level prosody latents using AR and flow-based Prior Networks for expressive speech synthesis. (arXiv:2211.01327v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01327">
<div class="article-summary-box-inner">
<span><p>A large part of the expressive speech synthesis literature focuses on
learning prosodic representations of the speech signal which are then modeled
by a prior distribution during inference. In this paper, we compare different
prior architectures at the task of predicting phoneme level prosodic
representations extracted with an unsupervised FVAE model. We use both
subjective and objective metrics to show that normalizing flow based prior
networks can result in more expressive speech at the cost of a slight drop in
quality. Furthermore, we show that the synthesized speech has higher
variability, for a given text, due to the nature of normalizing flows. We also
propose a Dynamical VAE model, that can generate higher quality speech although
with decreased expressiveness and variability compared to the flow based
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01335">
<div class="article-summary-box-inner">
<span><p>The tremendous success of CLIP (Radford et al., 2021) has promoted the
research and application of contrastive learning for vision-language
pretraining. However, while the publicly available CLIP models are mostly
pretrained on English data, it is hard to search for a CLIP pretrained on
Chinese data. We assume that pretraining a Chinese CLIP is essential to
research and industry for the following reasons. First, it can benefit the
vision-language retrieval in Chinese and thus promote the language-specific
multimodal representation learning. Second, the distribution of images in
Chinese websites should be different from that of images in English websites.
In this work, we construct a large-scale dataset of image-text pairs in
Chinese, where most data are retrieved from publicly available datasets, and we
pretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP
models of multiple sizes, spanning from 77 to 958 million parameters.
Furthermore, we propose a two-stage pretraining method, where the model is
first trained with the image encoder frozen and then trained with all
parameters being optimized, to achieve enhanced model performance. Our
comprehensive experiments demonstrate that Chinese CLIP can achieve the
state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups
of zero-shot learning and finetuning, and it is able to achieve competitive
performance in zero-shot image classification based on the evaluation on the
ELEVATER benchmark (Li et al., 2022). Furthermore, through the ablation study
we show that the two-stage pretraining method is the most effective compared
with the other options. We release our code in
https://github.com/OFA-Sys/Chinese-CLIP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages. (arXiv:2211.01338v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01338">
<div class="article-summary-box-inner">
<span><p>Cross-lingual dubbing of lecture videos requires the transcription of the
original audio, correction and removal of disfluencies, domain term discovery,
text-to-text translation into the target language, chunking of text using
target language rhythm, text-to-speech synthesis followed by isochronous
lipsyncing to the original video. This task becomes challenging when the source
and target languages belong to different language families, resulting in
differences in generated audio duration. This is further compounded by the
original speaker's rhythm, especially for extempore speech. This paper
describes the challenges in regenerating English lecture videos in Indian
languages semi-automatically. A prototype is developed for dubbing lectures
into 9 Indian languages. A mean-opinion-score (MOS) is obtained for two
languages, Hindi and Tamil, on two different courses. The output video is
compared with the original video in terms of MOS (1-5) and lip synchronisation
with scores of 4.09 and 3.74, respectively. The human effort also reduces by
75%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Emerging Technologies in Artificial Intelligence Scientific Ecosystem Using an Indicator-based Model. (arXiv:2211.01348v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01348">
<div class="article-summary-box-inner">
<span><p>Early identification of emergent topics is of eminent importance due to their
potential impacts on society. There are many methods for detecting emerging
terms and topics, all with advantages and drawbacks. However, there is no
consensus about the attributes and indicators of emergence. In this study, we
evaluate emerging topic detection in the field of artificial intelligence using
a new method to evaluate emergence. We also introduce two new attributes of
collaboration and technological impact which can help us use both paper and
patent information simultaneously. Our results confirm that the proposed new
method can successfully identify the emerging topics in the period of the
study. Moreover, this new method can provide us with the score of each
attribute and a final emergence score, which enable us to rank the emerging
topics with their emergence scores and each attribute score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Named Entity Recognition in Telephone Conversations via Effective Active Learning with Human in the Loop. (arXiv:2211.01354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01354">
<div class="article-summary-box-inner">
<span><p>Telephone transcription data can be very noisy due to speech recognition
errors, disfluencies, etc. Not only that annotating such data is very
challenging for the annotators, but also such data may have lots of annotation
errors even after the annotation job is completed, resulting in a very poor
model performance. In this paper, we present an active learning framework that
leverages human in the loop learning to identify data samples from the
annotated dataset for re-annotation that are more likely to contain annotation
errors. In this way, we largely reduce the need for data re-annotation for the
whole dataset. We conduct extensive experiments with our proposed approach for
Named Entity Recognition and observe that by re-annotating only about 6%
training instances out of the whole dataset, the F1 score for a certain entity
type can be significantly improved by about 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation. (arXiv:2211.01355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01355">
<div class="article-summary-box-inner">
<span><p>As generic machine translation (MT) quality has improved, the need for
targeted benchmarks that explore fine-grained aspects of quality has increased.
In particular, gender accuracy in translation can have implications in terms of
output fluency, translation accuracy, and ethics. In this paper, we introduce
MT-GenEval, a benchmark for evaluating gender accuracy in translation from
English into eight widely-spoken languages. MT-GenEval complements existing
benchmarks by providing realistic, gender-balanced, counterfactual data in
eight language pairs where the gender of individuals is unambiguous in the
input segment, including multi-sentence segments requiring inter-sentential
gender agreement. Our data and code is publicly available under a CC BY SA 3.0
license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Expand: Reinforced Pseudo-relevance Feedback Selection for Information-seeking Conversations. (arXiv:2011.12771v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12771">
<div class="article-summary-box-inner">
<span><p>Information-seeking conversation systems are increasingly popular in
real-world applications, especially for e-commerce companies. To retrieve
appropriate responses for users, it is necessary to compute the matching
degrees between candidate responses and users' queries with historical dialogue
utterances. As the contexts are usually much longer than responses, it is thus
necessary to expand the responses (usually short) with richer information.
Recent studies on pseudo-relevance feedback (PRF) have demonstrated its
effectiveness in query expansion for search engines, hence we consider
expanding response using PRF information. However, existing PRF approaches are
either based on heuristic rules or require heavy manual labeling, which are not
suitable for solving our task. To alleviate this problem, we treat the PRF
selection for response expansion as a learning task and propose a reinforced
learning method that can be trained in an end-to-end manner without any human
annotations. More specifically, we propose a reinforced selector to extract
useful PRF terms to enhance response candidates and a BERT-based response
ranker to rank the PRF-enhanced responses. The performance of the ranker serves
as a reward to guide the selector to extract useful PRF terms, which boosts the
overall task performance. Extensive experiments on both standard benchmarks and
commercial datasets prove the superiority of our reinforced PRF term selector
compared with other potential soft or hard selection methods. Both case studies
and quantitative analysis show that our model is capable of selecting
meaningful PRF terms to expand response candidates and also achieving the best
results compared with all baselines on a variety of evaluation metrics. We have
also deployed our method on online production in an e-commerce company, which
shows a significant improvement over the existing online ranking system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains. (arXiv:2012.01266v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01266">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been applied to various NLP tasks with
considerable performance gains. However, the large model sizes, together with
the long inference time, limit the deployment of such models in real-time
applications. One line of model compression approaches considers knowledge
distillation to distill large teacher models into small student models. Most of
these studies focus on single-domain only, which ignores the transferable
knowledge from other domains. We notice that training a teacher with
transferable knowledge digested across domains can achieve better
generalization capability to help knowledge distillation. Hence we propose a
Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model
that captures transferable knowledge across domains and passes such knowledge
to students. Specifically, we explicitly force the meta-teacher to capture
transferable knowledge at both instance-level and feature-level from multiple
domains, and then propose a meta-distillation algorithm to learn single-domain
student models with guidance from the meta-teacher. Experiments on public
multi-domain NLP tasks show the effectiveness and superiority of the proposed
Meta-KD framework. Further, we also demonstrate the capability of Meta-KD in
the settings where the training data is scarce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation. (arXiv:2109.06308v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06308">
<div class="article-summary-box-inner">
<span><p>Despite strong performance in many sequence-to-sequence tasks, autoregressive
models trained with maximum likelihood estimation suffer from exposure bias,
i.e. the discrepancy between the ground-truth prefixes used during training and
the model-generated prefixes used at inference time. Scheduled sampling is a
simple and empirically successful approach which addresses this issue by
incorporating model-generated prefixes into training. However, it has been
argued that it is an inconsistent training objective leading to models ignoring
the prefixes altogether. In this paper, we conduct systematic experiments and
find that scheduled sampling, while it ameliorates exposure bias by increasing
model reliance on the input sequence, worsens performance when the prefix at
inference time is correct, a form of catastrophic forgetting. We propose to use
Elastic Weight Consolidation to better balance mitigating exposure bias with
retaining performance. Experiments on four IWSLT'14 and WMT'14 translation
datasets demonstrate that our approach alleviates catastrophic forgetting and
significantly outperforms maximum likelihood estimation and scheduled sampling
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07983">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations for understanding the behavior of black box models
has gained a lot of attention recently as they provide potential for recourse.
In this paper, we propose a method Contrastive Attributed explanations for Text
(CAT) which provides contrastive explanations for natural language text data
with a novel twist as we build and exploit attribute classifiers leading to
more semantically meaningful explanations. To ensure that our contrastive
generated text has the fewest possible edits with respect to the original text,
while also being fluent and close to a human generated contrastive, we resort
to a minimal perturbation approach regularized using a BERT language model and
attribute classifiers trained on available attributes. We show through
qualitative examples and a user study that our method not only conveys more
insight because of these attributes, but also leads to better quality
(contrastive) text. Quantitatively, we show that our method outperforms other
state-of-the-art methods across four data sets on four benchmark metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation. (arXiv:2203.00281v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00281">
<div class="article-summary-box-inner">
<span><p>Recently CKY-based models show great potential in unsupervised grammar
induction thanks to their human-like encoding paradigm, which runs recursively
and hierarchically, but requires $O(n^3)$ time-complexity. Recursive
Transformer based on Differentiable Trees (R2D2) makes it possible to scale to
large language model pre-training even with complex tree encoder by introducing
a heuristic pruning method. However, the rule-based pruning approach suffers
from local optimum and slow inference issues. In this paper, we fix those
issues in a unified method. We propose to use a top-down parser as a
model-based pruning method, which also enables parallel encoding during
inference. Typically, our parser casts parsing as a split point scoring task,
which first scores all split points for a given sentence, and then recursively
splits a span into two by picking a split point with the highest score in the
current span. The reverse order of the splits is considered as the order of
pruning in R2D2 encoder. Beside the bi-directional language model loss, we also
optimize the parser by minimizing the KL distance between tree probabilities
from parser and R2D2. Our experiments show that our Fast-R2D2 improves
performance significantly in grammar induction and achieves competitive results
in downstream classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structurally Diverse Sampling for Sample-Efficient Training and Comprehensive Evaluation. (arXiv:2203.08445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08445">
<div class="article-summary-box-inner">
<span><p>A growing body of research has demonstrated the inability of NLP models to
generalize compositionally and has tried to alleviate it through specialized
architectures, training schemes, and data augmentation, among other approaches.
In this work, we study a different approach: training on instances with diverse
structures. We propose a model-agnostic algorithm for subsampling such sets of
instances from a labeled instance pool with structured outputs. Evaluating on
both compositional template splits and traditional IID splits of 5 semantic
parsing datasets of varying complexity, we show that structurally diverse
training using our algorithm leads to comparable or better generalization than
prior algorithms in 9 out of 10 dataset-split type pairs. In general, we find
structural diversity to consistently improve sample efficiency compared to
random train sets. Moreover, we show that structurally diverse sampling yields
comprehensive test sets that are a lot more challenging than IID test sets.
Finally, we provide two explanations for improved generalization from diverse
train sets: 1) improved coverage of output substructures, and 2) a reduction in
spurious correlations between these substructures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16965">
<div class="article-summary-box-inner">
<span><p>While self-supervised speech representation learning (SSL) models serve a
variety of downstream tasks, these models have been observed to overfit to the
domain from which the unlabelled data originates. To alleviate this issue, we
propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant
weights from models pre-trained on large amounts of out-of-domain (OOD) data.
Intuitively, this helps to make space for the target-domain ASR finetuning. The
redundant weights can be identified through various pruning strategies which
have been discussed in detail as a part of this work. Specifically, we
investigate the effect of the recently discovered Task-Agnostic and Task-Aware
pruning on PADA and propose a new pruning paradigm based on the latter, which
we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial
pruning mask from a well fine-tuned OOD model, which makes it starkly different
from the rest of the pruning strategies discussed in the paper. Our proposed
CD-TAW methodology achieves up to 20.6% relative WER improvement over our
baseline when fine-tuned on a 2-hour subset of Switchboard data without
language model (LM) decoding. Furthermore, we conduct a detailed analysis to
highlight the key design choices of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Descartes: Generating Short Descriptions of Wikipedia Articles. (arXiv:2205.10012v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10012">
<div class="article-summary-box-inner">
<span><p>Wikipedia is one of the richest knowledge sources on the Web today. In order
to facilitate navigating, searching, and maintaining its content, Wikipedia's
guidelines state that all articles should be annotated with a so-called short
description indicating the article's topic (e.g., the short description of beer
is "Alcoholic drink made from fermented cereal grains"). Nonetheless, a large
fraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no
short description yet, with detrimental effects for millions of Wikipedia
users. Motivated by this problem, we introduce the novel task of automatically
generating short descriptions for Wikipedia articles and propose Descartes, a
multilingual model for tackling it. Descartes integrates three sources of
information to generate an article description in a target language: the text
of the article in all its language versions, the already-existing descriptions
(if any) of the article in other languages, and semantic type information
obtained from a knowledge graph. We evaluate a Descartes model trained for
handling 25 languages simultaneously, showing that it beats baselines
(including a strong translation-based baseline) and performs on par with
monolingual models tailored for specific languages. A human evaluation on three
languages further shows that the quality of Descartes's descriptions is largely
indistinguishable from that of human-written descriptions; e.g., 91.3% of our
English descriptions (vs. 92.1% of human-written descriptions) pass the bar for
inclusion in Wikipedia, suggesting that Descartes is ready for production, with
the potential to support human editors in filling a major gap in today's
Wikipedia across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">kNN-Prompt: Nearest Neighbor Zero-Shot Inference. (arXiv:2205.13792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13792">
<div class="article-summary-box-inner">
<span><p>Retrieval-augmented language models (LMs) use non-parametric memory to
substantially outperform their non-retrieval counterparts on perplexity-based
evaluations, but it is an open question whether they achieve similar gains in
few- and zero-shot end-task accuracy. We extensively study one such model, the
k-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The
main challenge is to achieve coverage of the verbalizer tokens that define the
different end-task class labels. To address this challenge, we also introduce
kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy
verbalizers (e.g. to expand terrible to also include silly and other
task-specific synonyms for sentiment classification). Across nine diverse
end-tasks, using kNN-Prompt with GPT-2 large yields significant performance
boosts over strong zero-shot baselines (13.4% absolute improvement over the
base LM on average). We also show that other advantages of non-parametric
augmentation hold for end tasks; kNN-Prompt is effective for domain adaptation
with no further training, and gains increase with the size of the retrieval
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVLT: Textless Vision-Language Transformer. (arXiv:2209.14156v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14156">
<div class="article-summary-box-inner">
<span><p>In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02592">
<div class="article-summary-box-inner">
<span><p>While Self-Supervised Learning has helped reap the benefit of the scale from
the available unlabeled data, the learning paradigms are continuously being
bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which
uses clustering and an augmentation-based cross-contrastive loss as its
self-supervised objective. Through the clustering module, we scale down the
influence of those negative examples that are highly similar to the positive.
The Cross-Contrastive loss is computed between the encoder output of the
original sample and the quantizer output of its augmentation and vice-versa,
bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up
to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on
the test-clean and test-other sets, respectively, of LibriSpeech, without the
use of any language model. The proposed method also achieves up to 14.9%
relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on
Switchboard data. We make all our codes publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v2 [stat.ME] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08964">
<div class="article-summary-box-inner">
<span><p>This paper studies the time series forecasting problem from a whole new
perspective. In the existing SOTA time-series representation learning methods,
the forecasting models take a sequence of numerical values as input and yield
numerical values as output. The existing SOTA models are largely based on
Transformer architecture, modified with multiple encoding mechanisms to
incorporate the context and semantics around the historical data. In this
paper, we approach representation learning of time-series from the paradigm of
prompt-based natural language modeling. Inspired by the successes of
pre-trained language foundation models, we pose a question about whether these
models can also be adapted to solve time-series forecasting. Thus, we propose a
new forecasting paradigm: prompt-based time series forecasting (PromptCast). In
this novel task, the numerical input and output are transformed into prompts.
We frame the forecasting task in a sentence-to-sentence manner which makes it
possible to directly apply language models for forecasting purposes. To support
and facilitate the research of this task, we also present a large-scale dataset
(PISA) that includes three real-world forecasting scenarios. We evaluate
different SOTA numerical-based forecasting methods and language generation
models such as Bart. The benchmark results with single- and multi-step
forecasting settings demonstrate that the proposed prompt-based time series
forecasting with language generation models is a promising research direction.
In addition, in comparison to conventional numerical-based forecasting,
PromptCast shows a much better generalization ability under the zero-shot
setting. We believe that the proposed PromptCast task as well as our PISA
dataset could provide novel insights and further lead to new research
directions in the domain of time-series representation learning and
forecasting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding. (arXiv:2210.14169v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14169">
<div class="article-summary-box-inner">
<span><p>Dialogue understanding tasks often necessitate abundant annotated data to
achieve good performance and that presents challenges in low-resource settings.
To alleviate this barrier, we explore few-shot data augmentation for dialogue
understanding by prompting large pre-trained language models and present a
novel approach that iterates on augmentation quality by applying
weakly-supervised filters. We evaluate our methods on the emotion and act
classification tasks in DailyDialog and the intent classification task in
Facebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our
augmented data mixed with few-shot ground truth data are able to approach or
surpass existing state-of-the-art performance on both datasets. For DailyDialog
specifically, using 10% of the ground truth data we outperform the current
state-of-the-art model which uses 100% of the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$N$-gram is Back: Residual Learning of Neural Text Generation with $n$-gram Language Model. (arXiv:2210.14431v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14431">
<div class="article-summary-box-inner">
<span><p>$N$-gram language models (LM) have been largely superseded by neural LMs as
the latter exhibits better performance. However, we find that $n$-gram models
can achieve satisfactory performance on a large proportion of testing cases,
indicating they have already captured abundant knowledge of the language with
relatively low computational cost. With this observation, we propose to learn a
neural LM that fits the residual between an $n$-gram LM and the real-data
distribution. The combination of $n$-gram and neural LMs not only allows the
neural part to focus on the deeper understanding of language but also provides
a flexible way to customize an LM by switching the underlying $n$-gram model
without changing the neural model. Experimental results on three typical
language tasks (i.e., language modeling, machine translation, and
summarization) demonstrate that our approach attains additional performance
gains over popular standalone neural models consistently. We also show that our
approach allows for effective domain adaptation by simply switching to a
domain-specific $n$-gram model, without any extra training. Our code is
released at https://github.com/ghrua/NgramRes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16147">
<div class="article-summary-box-inner">
<span><p>To model behavioral and neural correlates of language comprehension in
naturalistic environments, researchers have turned to broad-coverage tools from
natural-language processing and machine learning. Where syntactic structure is
explicitly modeled, prior work has relied predominantly on context-free
grammars (CFG), yet such formalisms are not sufficiently expressive for human
languages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive
directly compositional models of grammar with flexible constituency that
affords incremental interpretation. In this work we evaluate whether a more
expressive CCG provides a better model than a CFG for human neural signals
collected with fMRI while participants listen to an audiobook story. We further
test between variants of CCG that differ in how they handle optional adjuncts.
These evaluations are carried out against a baseline that includes estimates of
next-word predictability from a Transformer neural network language model. Such
a comparison reveals unique contributions of CCG structure-building
predominantly in the left posterior temporal lobe: CCG-derived measures offer a
superior fit to neural signals compared to those derived from a CFG. These
effects are spatially distinct from bilateral superior temporal effects that
are unique to predictability. Neural effects for structure-building are thus
separable from predictability during naturalistic listening, and those effects
are best characterized by a grammar whose expressive power is motivated on
independent linguistic grounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning. (arXiv:2210.17451v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17451">
<div class="article-summary-box-inner">
<span><p>Standard fine-tuning of large pre-trained language models (PLMs) for
downstream tasks requires updating hundreds of millions to billions of
parameters, and storing a large copy of the PLM weights for every task
resulting in increased cost for storing, sharing and serving the models. To
address this, parameter-efficient fine-tuning (PEFT) techniques were introduced
where small trainable components are injected in the PLM and updated during
fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of
adaptation modules -- given the underlying PEFT method of choice -- introduced
in each Transformer layer while keeping most of the PLM weights frozen. For
instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture
of low rank decomposition matrices like LoRA to improve downstream task
performance over the corresponding PEFT methods for fully supervised and
few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the
same computational cost and the number of tunable parameters as the underlying
PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix
outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for
both NLU and NLG tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where to start? Analyzing the potential value of intermediate models. (arXiv:2211.00107v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00107">
<div class="article-summary-box-inner">
<span><p>Previous studies observed that finetuned models may be better base models
than the vanilla pretrained model. Such a model, finetuned on some source
dataset, may provide a better starting point for a new finetuning process on a
desired target dataset. Here, we perform a systematic analysis of this
\emph{intertraining} scheme, over a wide range of English classification tasks.
Surprisingly, our analysis suggests that the potential intertraining gain can
be analyzed \emph{independently} for the target dataset under consideration,
and for a base model being considered as a starting point. This is in contrast
to current perception that the alignment between the target dataset and the
source dataset used to generate the base model is a major factor in determining
intertraining success. We analyze different aspects that contribute to each.
Furthermore, we leverage our analysis to propose a practical and efficient
approach to determine if and how to select a base model in real-world settings.
Last, we release an updating ranking of best models in the HuggingFace hub per
architecture https://ibm.github.io/model-recycling/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia. (arXiv:2106.01601v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01601">
<div class="article-summary-box-inner">
<span><p>Human activities can be seen as sequences of events, which are crucial to
understanding societies. Disproportional event distribution for different
demographic groups can manifest and amplify social stereotypes, and potentially
jeopardize the ability of members in some groups to pursue certain goals. In
this paper, we present the first event-centric study of gender biases in a
Wikipedia corpus. To facilitate the study, we curate a corpus of career and
personal life descriptions with demographic information consisting of 7,854
fragments from 10,412 celebrities. Then we detect events with a
state-of-the-art event detection model, calibrate the results using
strategically generated templates, and extract events that have asymmetric
associations with genders. Our study discovers that the Wikipedia pages tend to
intermingle personal life events with professional events for females but not
for males, which calls for the awareness of the Wikipedia community to
formalize guidelines and train the editors to mind the implicit biases that
contributors carry. Our work also lays the foundation for future works on
quantifying and discovering event biases at the corpus level.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-03 23:16:56.810998363 UTC">2022-11-03 23:16:56 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>