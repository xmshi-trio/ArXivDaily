<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-11T01:30:00Z">01-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighborhood-Regularized Self-Training for Learning with Few Labels. (arXiv:2301.03726v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03726">
<div class="article-summary-box-inner">
<span><p>Training deep neural networks (DNNs) with limited supervision has been a
popular research topic as it can significantly alleviate the annotation burden.
Self-training has been successfully applied in semi-supervised learning tasks,
but one drawback of self-training is that it is vulnerable to the label noise
from incorrect pseudo labels. Inspired by the fact that samples with similar
labels tend to share similar representations, we develop a neighborhood-based
sample selection approach to tackle the issue of noisy pseudo labels. We
further stabilize self-training via aggregating the predictions from different
rounds during sample selection. Experiments on eight tasks show that our
proposed method outperforms the strongest self-training baseline with 1.83% and
2.51% performance gain for text and graph datasets on average. Our further
analysis demonstrates that our proposed data selection strategy reduces the
noise of pseudo labels by 36.8% and saves 57.3% of the time when compared with
the best baseline. Our code and appendices will be uploaded to
https://github.com/ritaranx/NeST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws for Generative Mixed-Modal Language Models. (arXiv:2301.03728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03728">
<div class="article-summary-box-inner">
<span><p>Generative language models define distributions over sequences of tokens that
can represent essentially any combination of data modalities (e.g., any
permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens
for language or code, and so on). To better understand the scaling properties
of such mixed-modal models, we conducted over 250 experiments using seven
different modalities and model sizes ranging from 8 million to 30 billion,
trained on 5-100 billion tokens. We report new mixed-modal scaling laws that
unify the contributions of individual modalities and the interactions between
them. Specifically, we explicitly model the optimal synergy and competition due
to data and model size as an additive term to previous uni-modal scaling laws.
We also find four empirical phenomena observed during the training, such as
emergent coordinate-ascent style training that naturally alternates between
modalities, guidelines for selecting critical hyper-parameters, and connections
between mixed-modal competition and training stability. Finally, we test our
scaling law by training a 30B speech-text model, which significantly
outperforms the corresponding unimodal models. Overall, our research provides
valuable insights into the design and training of mixed-modal generative
models, an important new class of unified models that have unique
distributional properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding. (arXiv:2301.03765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03765">
<div class="article-summary-box-inner">
<span><p>Current natural language understanding (NLU) models have been continuously
scaling up, both in terms of model size and input context, introducing more
hidden and input neurons. While this generally improves performance on average,
the extra neurons do not yield a consistent improvement for all instances. This
is because some hidden neurons are redundant, and the noise mixed in input
neurons tends to distract the model. Previous work mainly focuses on
extrinsically reducing low-utility neurons by additional post- or
pre-processing, such as network pruning and context selection, to avoid this
problem. Beyond that, can we make the model reduce redundant parameters and
suppress input noise by intrinsically enhancing the utility of each neuron? If
a model can efficiently utilize neurons, no matter which neurons are ablated
(disabled), the ablated submodel should perform no better than the original
full model. Based on such a comparison principle between models, we propose a
cross-model comparative loss for a broad range of tasks. Comparative loss is
essentially a ranking loss on top of the task-specific losses of the full and
ablated models, with the expectation that the task-specific loss of the full
model is minimal. We demonstrate the universal effectiveness of comparative
loss through extensive experiments on 14 datasets from 3 distinct NLU tasks
based on 4 widely used pretrained language models, and find it particularly
superior for models with few parameters or long input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnifySpeech: A Unified Framework for Zero-shot Text-to-Speech and Voice Conversion. (arXiv:2301.03801v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03801">
<div class="article-summary-box-inner">
<span><p>Text-to-speech (TTS) and voice conversion (VC) are two different tasks both
aiming at generating high quality speaking voice according to different input
modality. Due to their similarity, this paper proposes UnifySpeech, which
brings TTS and VC into a unified framework for the first time. The model is
based on the assumption that speech can be decoupled into three independent
components: content information, speaker information, prosody information. Both
TTS and VC can be regarded as mining these three parts of information from the
input and completing the reconstruction of speech. For TTS, the speech content
information is derived from the text, while in VC it's derived from the source
speech, so all the remaining units are shared except for the speech content
extraction module in the two tasks. We applied vector quantization and domain
constrain to bridge the gap between the content domains of TTS and VC.
Objective and subjective evaluation shows that by combining the two task, TTS
obtains better speaker modeling ability while VC gets hold of impressive speech
content decoupling capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Punctuation: A Novel Punctuation Technique Leveraging Bidirectional Context for Continuous Speech Recognition. (arXiv:2301.03819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03819">
<div class="article-summary-box-inner">
<span><p>While speech recognition Word Error Rate (WER) has reached human parity for
English, continuous speech recognition scenarios such as voice typing and
meeting transcriptions still suffer from segmentation and punctuation problems,
resulting from irregular pausing patterns or slow speakers. Transformer
sequence tagging models are effective at capturing long bi-directional context,
which is crucial for automatic punctuation. Automatic Speech Recognition (ASR)
production systems, however, are constrained by real-time requirements, making
it hard to incorporate the right context when making punctuation decisions.
Context within the segments produced by ASR decoders can be helpful but
limiting in overall punctuation performance for a continuous speech session. In
this paper, we propose a streaming approach for punctuation or re-punctuation
of ASR output using dynamic decoding windows and measure its impact on
punctuation and segmentation accuracy across scenarios. The new system tackles
over-segmentation issues, improving segmentation F0.5-score by 13.9%. Streaming
punctuation achieves an average BLEUscore improvement of 0.66 for the
downstream task of Machine Translation (MT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The use of new technologies to support Public Administration. Sentiment analysis and the case of the app IO. (arXiv:2301.03848v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03848">
<div class="article-summary-box-inner">
<span><p>App IO is an app developed for the Italian PA. It is definitely useful for
citizens to interact with the PA and to get services that were not digitized
yet. Nevertheless, it was not perceived in a good way by the citizens and it
has been criticized. As we wanted to find the root that caused all these bad
reviews we scraped feedback from mobile app stores using custom-coded automated
tools and - after that - we trained two machine learning models to perform both
sentiment analysis and emotion detection to understand what caused the bad
reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channel-aware Decoupling Network for Multi-turn Dialogue Comprehension. (arXiv:2301.03953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03953">
<div class="article-summary-box-inner">
<span><p>Training machines to understand natural language and interact with humans is
one of the major goals of artificial intelligence. Recent years have witnessed
an evolution from matching networks to pre-trained language models (PrLMs). In
contrast to the plain-text modeling as the focus of the PrLMs, dialogue texts
involve multiple speakers and reflect special characteristics such as topic
transitions and structure dependencies between distant utterances. However, the
related PrLM models commonly represent dialogues sequentially by processing the
pairwise dialogue history as a whole. Thus the hierarchical information on
either utterance interrelation or speaker roles coupled in such representations
is not well addressed. In this work, we propose compositional learning for
holistic interaction across the utterances beyond the sequential
contextualization from PrLMs, in order to capture the utterance-aware and
speaker-aware representations entailed in a dialogue history. We decouple the
contextualized word representations by masking mechanisms in Transformer-based
PrLM, making each word only focus on the words in current utterance, other
utterances, and two speaker roles (i.e., utterances of sender and utterances of
the receiver), respectively. In addition, we employ domain-adaptive training
strategies to help the model adapt to the dialogue domains. Experimental
results show that our method substantially boosts the strong PrLM baselines in
four public benchmark datasets, achieving new state-of-the-art performance over
previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI based approach to Trailer Generation for Online Educational Courses. (arXiv:2301.03957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03957">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an AI based approach to Trailer Generation in the
form of short videos for online educational courses. Trailers give an overview
of the course to the learners and help them make an informed choice about the
courses they want to learn. It also helps to generate curiosity and interest
among the learners and encourages them to pursue a course. While it is possible
to manually generate the trailers, it requires extensive human efforts and
skills over a broad spectrum of design, span selection, video editing, domain
knowledge, etc., thus making it time-consuming and expensive, especially in an
academic setting. The framework we propose in this work is a template based
method for video trailer generation, where most of the textual content of the
trailer is auto-generated and the trailer video is automatically generated, by
leveraging Machine Learning and Natural Language Processing techniques. The
proposed trailer is in the form of a timeline consisting of various fragments
created by selecting, para-phrasing or generating content using various
proposed techniques. The fragments are further enhanced by adding voice-over
text, subtitles, animations, etc., to create a holistic experience. Finally, we
perform user evaluation with 63 human evaluators for evaluating the trailers
generated by our system and the results obtained were encouraging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Mandarin-Cantonese Machine Translation. (arXiv:2301.03971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03971">
<div class="article-summary-box-inner">
<span><p>Advancements in unsupervised machine translation have enabled the development
of machine translation systems that can translate between languages for which
there is not an abundance of parallel data available. We explored unsupervised
machine translation between Mandarin Chinese and Cantonese. Despite the vast
number of native speakers of Cantonese, there is still no large-scale corpus
for the language, due to the fact that Cantonese is primarily used for oral
communication. The key contributions of our project include: 1. The creation of
a new corpus containing approximately 1 million Cantonese sentences, and 2. A
large-scale comparison across different model architectures, tokenization
schemes, and embedding structures. Our best model trained with character-based
tokenization and a Transformer architecture achieved a character-level BLEU of
25.1 when translating from Mandarin to Cantonese and of 24.4 when translating
from Cantonese to Mandarin. In this paper we discuss our research process,
experiments, and results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models sounds the Death Knell of Knowledge Graphs. (arXiv:2301.03980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03980">
<div class="article-summary-box-inner">
<span><p>Healthcare domain generates a lot of unstructured and semi-structured text.
Natural Language processing (NLP) has been used extensively to process this
data. Deep Learning based NLP especially Large Language Models (LLMs) such as
BERT have found broad acceptance and are used extensively for many
applications. A Language Model is a probability distribution over a word
sequence. Self-supervised Learning on a large corpus of data automatically
generates deep learning-based language models. BioBERT and Med-BERT are
language models pre-trained for the healthcare domain. Healthcare uses typical
NLP tasks such as question answering, information extraction, named entity
recognition, and search to simplify and improve processes. However, to ensure
robust application of the results, NLP practitioners need to normalize and
standardize them. One of the main ways of achieving normalization and
standardization is the use of Knowledge Graphs. A Knowledge Graph captures
concepts and their relationships for a specific domain, but their creation is
time-consuming and requires manual intervention from domain experts, which can
prove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical
Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are
popular ontologies from the healthcare domain. SNOMED CT and UMLS capture
concepts such as disease, symptoms and diagnosis and GO is the world's largest
source of information on the functions of genes. Healthcare has been dealing
with an explosion in information about different types of drugs, diseases, and
procedures. This paper argues that using Knowledge Graphs is not the best
solution for solving problems in this domain. We present experiments using LLMs
for the healthcare domain to demonstrate that language models provide the same
functionality as knowledge graphs, thereby making knowledge graphs redundant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">There is No Big Brother or Small Brother: Knowledge Infusion in Language Models for Link Prediction and Question Answering. (arXiv:2301.04013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04013">
<div class="article-summary-box-inner">
<span><p>The integration of knowledge graphs with deep learning is thriving in
improving the performance of various natural language processing (NLP) tasks.
In this paper, we focus on knowledge-infused link prediction and question
answering using language models, T5, and BLOOM across three domains: Aviation,
Movie, and Web. In this context, we infuse knowledge in large and small
language models and study their performance, and find the performance to be
similar. For the link prediction task on the Aviation Knowledge Graph, we
obtain a 0.2 hits@1 score using T5-small, T5-base, T5-large, and BLOOM. Using
template-based scripts, we create a set of 1 million synthetic factoid QA pairs
in the aviation domain from National Transportation Safety Board (NTSB)
reports. On our curated QA pairs, the three models of T5 achieve a 0.7 hits@1
score. We validate out findings with the paired student t-test and Cohen's
kappa scores. For link prediction on Aviation Knowledge Graph using T5-small
and T5-large, we obtain a Cohen's kappa score of 0.76, showing substantial
agreement between the models. Thus, we infer that small language models perform
similar to large language models with the infusion of knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Conversational Search Behavior For Domain Exploration. (arXiv:2301.04098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04098">
<div class="article-summary-box-inner">
<span><p>Conversational search has evolved as a new information retrieval paradigm,
marking a shift from traditional search systems towards interactive dialogues
with intelligent search agents. This change especially affects exploratory
information-seeking contexts, where conversational search systems can guide the
discovery of unfamiliar domains. In these scenarios, users find it often
difficult to express their information goals due to insufficient background
knowledge. Conversational interfaces can provide assistance by eliciting
information needs and narrowing down the search space. However, due to the
complexity of information-seeking behavior, the design of conversational
interfaces for retrieving information remains a great challenge. Although prior
work has employed user studies to empirically ground the system design, most
existing studies are limited to well-defined search tasks or known domains,
thus being less exploratory in nature. Therefore, we conducted a laboratory
study to investigate open-ended search behavior for navigation through unknown
information landscapes. The study comprised of 26 participants who were
restricted in their search to a text chat interface. Based on the collected
dialogue transcripts, we applied statistical analyses and process mining
techniques to uncover general information-seeking patterns across five
different domains. We not only identify core dialogue acts and their
interrelations that enable users to discover domain knowledge, but also derive
design suggestions for conversational search systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers. (arXiv:2301.04110v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04110">
<div class="article-summary-box-inner">
<span><p>Inference-time adaptation methods for semantic parsing are useful for
leveraging examples from newly-observed domains without repeated fine-tuning.
Existing approaches typically bias the decoder by simply concatenating
input-output example pairs (cases) from the new domain at the encoder's input
in a Seq-to-Seq model. Such methods cannot adequately leverage the structure of
logical forms in the case examples. We propose StructCBR, a structured
case-based reasoning approach, which leverages subtree-level similarity between
logical forms of cases and candidate outputs, resulting in better decoder
decisions. For the task of adapting Text-to-SQL models to unseen schemas, we
show that exploiting case examples in a structured manner via StructCBR offers
consistent performance improvements over prior inference-time adaptation
methods across five different databases. To the best of our knowledge, we are
the first to attempt inference-time adaptation of Text-to-SQL models, and
harness trainable structured similarity between subqueries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BASPRO: a balanced script producer for speech corpus collection based on the genetic algorithm. (arXiv:2301.04120v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04120">
<div class="article-summary-box-inner">
<span><p>The performance of speech-processing models is heavily influenced by the
speech corpus that is used for training and evaluation. In this study, we
propose BAlanced Script PROducer (BASPRO) system, which can automatically
construct a phonetically balanced and rich set of Chinese sentences for
collecting Mandarin Chinese speech data. First, we used pretrained natural
language processing systems to extract ten-character candidate sentences from a
large corpus of Chinese news texts. Then, we applied a genetic algorithm-based
method to select 20 phonetically balanced sentence sets, each containing 20
sentences, from the candidate sentences. Using BASPRO, we obtained a recording
script called TMNews, which contains 400 ten-character sentences. TMNews covers
84% of the syllables used in the real world. Moreover, the syllable
distribution has 0.96 cosine similarity to the real-world syllable
distribution. We converted the script into a speech corpus using two
text-to-speech systems. Using the designed speech corpus, we tested the
performances of speech enhancement (SE) and automatic speech recognition (ASR),
which are one of the most important regression- and classification-based speech
processing tasks, respectively. The experimental results show that the SE and
ASR models trained on the designed speech corpus outperform their counterparts
trained on a randomly composed speech corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation. (arXiv:2109.06308v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06308">
<div class="article-summary-box-inner">
<span><p>Despite strong performance in many sequence-to-sequence tasks, autoregressive
models trained with maximum likelihood estimation suffer from exposure bias,
i.e. the discrepancy between the ground-truth prefixes used during training and
the model-generated prefixes used at inference time. Scheduled sampling is a
simple and empirically successful approach which addresses this issue by
incorporating model-generated prefixes into training. However, it has been
argued that it is an inconsistent training objective leading to models ignoring
the prefixes altogether. In this paper, we conduct systematic experiments and
find that scheduled sampling, while it ameliorates exposure bias by increasing
model reliance on the input sequence, worsens performance when the prefix at
inference time is correct, a form of catastrophic forgetting. We propose to use
Elastic Weight Consolidation to better balance mitigating exposure bias with
retaining performance. Experiments on four IWSLT'14 and WMT'14 translation
datasets demonstrate that our approach alleviates catastrophic forgetting and
significantly outperforms maximum likelihood estimation and scheduled sampling
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01171">
<div class="article-summary-box-inner">
<span><p>Current language generation models suffer from issues such as repetition,
incoherence, and hallucinations. An often-repeated hypothesis is that this
brittleness of generation models is caused by the training and the generation
procedure mismatch, also referred to as exposure bias. In this paper, we verify
this hypothesis by analyzing exposure bias from an imitation learning
perspective. We show that exposure bias leads to an accumulation of errors,
analyze why perplexity fails to capture this accumulation, and empirically show
that this accumulation results in poor generation quality. Source code to
reproduce these experiments is available at
https://github.com/kushalarora/quantifying_exposure_bias
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Accurate and Faithful Discharge Instructions: Task, Dataset, and Model. (arXiv:2210.12777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12777">
<div class="article-summary-box-inner">
<span><p>The "Patient Instruction" (PI), known as "Discharge Instruction", which
contains critical instructional information provided both to carers and to the
patient at the time of discharge, is essential for the patient to manage their
condition outside hospital. An accurate and easy-to-follow PI can improve the
self-management of patients which can in turn reduce hospital readmission
rates. However, writing an appropriate PI can be extremely time-consuming for
physicians, and is subject to being incomplete or error-prone for (potentially
overworked) physicians. Therefore, we propose a new task that can provide an
objective means of avoiding incompleteness, while reducing clinical workload:
the automatic generation of the PI, which is imagined as being a document that
the clinician can review, modify, and approve as necessary (rather than taking
the human "out of the loop"). We build a benchmark clinical dataset and propose
the Re3Writer, which imitates the working patterns of physicians to first
retrieve related working experience from historical PIs written by physicians,
then reason related medical knowledge. Finally, it refines the retrieved
working experience and reasoned medical knowledge to extract useful
information, which is used to generate the PI for previously-unseen patient
according to their health records during hospitalization. Our experiments show
that, using our method, the performance of five different models can be
substantially boosted across all metrics, with up to 20%, 11%, and 19% relative
improvements in BLEU-4, ROUGE-L, and METEOR, respectively. Meanwhile, we show
results from human evaluations to measure the effectiveness in terms of its
usefulness for clinical practice. The code is available at
https://github.com/AI-in-Hospitals/Patient-Instructions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal. (arXiv:2212.05767v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05767">
<div class="article-summary-box-inner">
<span><p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing
facts based on mined logic rules underlying knowledge graphs (KGs), has become
a fast-growing research direction. It has been proven to significantly benefit
the usage of KGs in many AI applications, such as question answering and
recommendation systems, etc. According to the graph types, the existing KGR
models can be roughly divided into three categories, i.e., static models,
temporal models, and multi-modal models. The early works in this domain mainly
focus on static KGR and tend to directly apply general knowledge graph
embedding models to the reasoning task. However, these models are not suitable
for more complex but practical tasks, such as inductive static KGR, temporal
KGR, and multi-modal KGR. To this end, multiple works have been developed
recently, but no survey papers and open-source repositories comprehensively
summarize and discuss models in this important direction. To fill the gap, we
conduct a survey for knowledge graph reasoning tracing from static to temporal
and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR
models, and typical datasets are introduced and discussed consequently.
Moreover, we discuss the challenges and potential opportunities. The
corresponding open-source repository is shared on GitHub:
https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07796">
<div class="article-summary-box-inner">
<span><p>A fundamental characteristic common to both human vision and natural language
is their compositional nature. Yet, despite the performance gains contributed
by large vision and language pretraining, we find that - across 6 architectures
trained with 4 algorithms on massive datasets - they exhibit little
compositionality. To arrive at this conclusion, we introduce a new
compositionality evaluation benchmark CREPE which measures two important
aspects of compositionality identified by cognitive science literature:
systematicity and productivity. To measure systematicity, CREPE consists of
three test datasets. The three test sets are designed to test models trained on
three of the popular training datasets: CC-12M, YFCC-15M, and LAION-400M. They
contain 385K, 385K, and 373K image-text pairs and 237K, 210K, and 178K hard
negative captions. To test productivity, CREPE contains 17K image-text pairs
with nine different complexities plus 246K hard negative captions with atomic,
swapping, and negation foils. The datasets are generated by repurposing the
Visual Genome scene graphs and region descriptions and applying handcrafted
templates and GPT-3. For systematicity, we find that model performance
decreases consistently when novel compositions dominate the retrieval set, with
Recall@1 dropping by up to 8%. For productivity, models' retrieval success
decays as complexity increases, frequently nearing random chance at high
complexity. These results hold regardless of model and training dataset size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnnoBERT: Effectively Representing Multiple Annotators' Label Choices to Improve Hate Speech Detection. (arXiv:2212.10405v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10405">
<div class="article-summary-box-inner">
<span><p>Supervised approaches generally rely on majority-based labels. However, it is
hard to achieve high agreement among annotators in subjective tasks such as
hate speech detection. Existing neural network models principally regard labels
as categorical variables, while ignoring the semantic information in diverse
label texts. In this paper, we propose AnnoBERT, a first-of-its-kind
architecture integrating annotator characteristics and label text with a
transformer-based model to detect hate speech, with unique representations
based on each annotator's characteristics via Collaborative Topic Regression
(CTR) and integrate label text to enrich textual representations. During
training, the model associates annotators with their label choices given a
piece of text; during evaluation, when label information is not available, the
model predicts the aggregated label given by the participating annotators by
utilising the learnt association. The proposed approach displayed an advantage
in detecting hate speech, especially in the minority class and edge cases with
annotator disagreement. Improvement in the overall performance is the largest
when the dataset is more label-imbalanced, suggesting its practical value in
identifying real-world hate speech, as the volume of hate speech in-the-wild is
extremely small on social media, when compared with normal (non-hate) speech.
Through ablation studies, we show the relative contributions of annotator
embeddings and label text to the model performance, and tested a range of
alternative annotator embeddings and label text combinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01015">
<div class="article-summary-box-inner">
<span><p>In this paper we explore the task of modeling (semi) structured object
sequences; in particular we focus our attention on the problem of developing a
structure-aware input representation for such sequences. In such sequences, we
assume that each structured object is represented by a set of key-value pairs
which encode the attributes of the structured object. Given a universe of keys,
a sequence of structured objects can then be viewed as an evolution of the
values for each key, over time. We encode and construct a sequential
representation using the values for a particular key (Temporal Value Modeling -
TVM) and then self-attend over the set of key-conditioned value sequences to a
create a representation of the structured object sequence (Key Aggregation -
KA). We pre-train and fine-tune the two components independently and present an
innovative training schedule that interleaves the training of both modules with
shared attention heads. We find that this iterative two part-training results
in better performance than a unified network with hierarchical encoding as well
as over, other methods that use a {\em record-view} representation of the
sequence \cite{de2021transformers4rec} or a simple {\em flattened}
representation of the sequence. We conduct experiments using real-world data to
demonstrate the advantage of interleaving TVM-KA on multiple tasks and detailed
ablation studies motivating our modeling choices. We find that our approach
performs better than flattening sequence objects and also allows us to operate
on significantly larger sequences than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traditional Readability Formulas Compared for English. (arXiv:2301.02975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02975">
<div class="article-summary-box-inner">
<span><p>Traditional English readability formulas, or equations, were largely
developed in the 20th century. Nonetheless, many researchers still rely on them
for various NLP applications. This phenomenon is presumably due to the
convenience and straightforwardness of readability formulas. In this work, we
contribute to the NLP community by 1. introducing New English Readability
Formula (NERF), 2. recalibrating the coefficients of old readability formulas
(Flesch-Kincaid Grade Level, Fog Index, SMOG Index, Coleman-Liau Index, and
Automated Readability Index), 3. evaluating the readability formulas, for use
in text simplification studies and medical texts, and 4. developing a
Python-based program for the wide application to various NLP projects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of German Drama Texts Using Fine Tuned GPT-2 Models. (arXiv:2301.03119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03119">
<div class="article-summary-box-inner">
<span><p>This study is devoted to the automatic generation of German drama texts. We
suggest an approach consisting of two key steps: fine-tuning a GPT-2 model (the
outline model) to generate outlines of scenes based on keywords and fine-tuning
a second model (the generation model) to generate scenes from the scene
outline. The input for the neural model comprises two datasets: the German
Drama Corpus (GerDraCor) and German Text Archive (Deutsches Textarchiv or DTA).
In order to estimate the effectiveness of the proposed method, our models are
compared with baseline GPT-2 models. Our models perform well according to
automatic quantitative evaluation, but, conversely, manual qualitative analysis
reveals a poor quality of generated texts. This may be due to the quality of
the dataset or training inputs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-11 23:12:50.552895246 UTC">2023-01-11 23:12:50 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>