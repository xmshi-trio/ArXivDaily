<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2024-01-09T01:30:00Z">01-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Rule-Guided Joint Embedding Learning of Knowledge Graphs. (arXiv:2401.02968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02968">
<div class="article-summary-box-inner">
<span><p>In recent studies, the focus has been on enhancing knowledge graph embedding
learning, which encodes entities and relations in knowledge graphs into
low-dimensional vector spaces. While current models mainly consider the
structural aspects of these graphs, there's a wealth of contextual and literal
information in knowledge graphs that can be utilized for more effective
embeddings. This paper introduces a novel model that incorporates both
contextual and literal information into entity and relation embeddings,
utilizing graph convolutional networks. Specifically, for contextual
information, we assess its significance through confidence and relatedness
metrics. A unique rule-based method is developed to calculate the confidence
metric, and the relatedness metric is derived from the literal information's
representations. We validated our model's performance with thorough experiments
on two established benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Anomaly Detection in Text. (arXiv:2401.02971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02971">
<div class="article-summary-box-inner">
<span><p>Deep anomaly detection methods have become increasingly popular in recent
years, with methods like Stacked Autoencoders, Variational Autoencoders, and
Generative Adversarial Networks greatly improving the state-of-the-art. Other
methods rely on augmenting classical models (such as the One-Class Support
Vector Machine), by learning an appropriate kernel function using Neural
Networks. Recent developments in representation learning by self-supervision
are proving to be very beneficial in the context of anomaly detection. Inspired
by the advancements in anomaly detection using self-supervised learning in the
field of computer vision, this thesis aims to develop a method for detecting
anomalies by exploiting pretext tasks tailored for text corpora. This approach
greatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG
News, for both semi-supervised and unsupervised anomaly detection, thus proving
the potential for self-supervised anomaly detectors in the field of natural
language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REE-HDSC: Recognizing Extracted Entities for the Historical Database Suriname Curacao. (arXiv:2401.02972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02972">
<div class="article-summary-box-inner">
<span><p>We describe the project REE-HDSC and outline our efforts to improve the
quality of named entities extracted automatically from texts generated by
hand-written text recognition (HTR) software. We describe a six-step processing
pipeline and test it by processing 19th and 20th century death certificates
from the civil registry of Curacao. We find that the pipeline extracts dates
with high precision but that the precision of person name extraction is low.
Next we show how name precision extraction can be improved by retraining HTR
models with names, post-processing and by identifying and removing incorrect
names.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online. (arXiv:2401.02974v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02974">
<div class="article-summary-box-inner">
<span><p>This paper examines the efficacy of utilizing large language models (LLMs) to
detect public threats posted online. Amid rising concerns over the spread of
threatening rhetoric and advance notices of violence, automated content
analysis techniques may aid in early identification and moderation. Custom data
collection tools were developed to amass post titles from a popular Korean
online community, comprising 500 non-threat examples and 20 threats. Various
LLMs (GPT-3.5, GPT-4, PaLM) were prompted to classify individual posts as
either "threat" or "safe." Statistical analysis found all models demonstrated
strong accuracy, passing chi-square goodness of fit tests for both threat and
non-threat identification. GPT-4 performed best overall with 97.9% non-threat
and 100% threat accuracy. Affordability analysis also showed PaLM API pricing
as highly cost-efficient. The findings indicate LLMs can effectively augment
human content moderation at scale to help mitigate emerging online risks.
However, biases, transparency, and ethical oversight remain vital
considerations before real-world implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering Regulatory Affairs Complexity in Medical Products: A Qualitative Assessment Utilizing Open Coding and Natural Language Processing (NLP). (arXiv:2401.02975v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02975">
<div class="article-summary-box-inner">
<span><p>This study investigates the complexity of regulatory affairs in the medical
device industry, a critical factor influencing market access and patient care.
Through qualitative research, we sought expert insights to understand the
factors contributing to this complexity. The study involved semi-structured
interviews with 28 professionals from medical device companies, specializing in
various aspects of regulatory affairs. These interviews were analyzed using
open coding and Natural Language Processing (NLP) techniques. The findings
reveal key sources of complexity within the regulatory landscape, divided into
five domains: (A) Regulatory language complexity, (B) Intricacies within the
regulatory process, (C) Global-level complexities, (D) Database-related
considerations, and (E) Product-level issues. The participants highlighted the
need for strategies to streamline regulatory compliance, enhance interactions
between regulatory bodies and industry players, and develop adaptable
frameworks for rapid technological advancements. Emphasizing interdisciplinary
collaboration and increased transparency, the study concludes that these
elements are vital for establishing coherent and effective regulatory
procedures in the medical device sector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trace and Edit Relation Associations in GPT. (arXiv:2401.02976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02976">
<div class="article-summary-box-inner">
<span><p>This study introduces a novel approach for analyzing and modifying entity
relationships in GPT models, diverging from ROME's entity-focused methods. We
develop a relation tracing technique to understand the influence of language
model computations on relationship judgments. Using the FewRel dataset, we
identify key roles of MLP modules and attention mechanisms in processing
relationship information. Our method, tested against ROME on a new dataset,
shows improved balance in specificity and generalization, underscoring the
potential of manipulating early-layer modules for enhanced model understanding
and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from a Generative AI Predecessor -- The Many Motivations for Interacting with Conversational Agents. (arXiv:2401.02978v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02978">
<div class="article-summary-box-inner">
<span><p>For generative AI to succeed, how engaging a conversationalist must it be?
For almost sixty years, some conversational agents have responded to any
question or comment to keep a conversation going. In recent years, several
utilized machine learning or sophisticated language processing, such as Tay,
Xiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they
focused on engagement, not expertise. Millions of people were motivated to
engage with them. What were the attractions? Will generative AI do better if it
is equally engaging, or should it be less engaging? Prior to the emergence of
generative AI, we conducted a large-scale quantitative and qualitative analysis
to learn what motivated millions of people to engage with one such 'virtual
companion,' Microsoft's Zo. We examined the complete chat logs of 2000
anonymized people. We identified over a dozen motivations that people had for
interacting with this software. Designers learned different ways to increase
engagement. Generative conversational AI does not yet have a clear revenue
model to address its high cost. It might benefit from being more engaging, even
as it supports productivity and creativity. Our study and analysis point to
opportunities and challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are we describing the same sound? An analysis of word embedding spaces of expressive piano performance. (arXiv:2401.02979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02979">
<div class="article-summary-box-inner">
<span><p>Semantic embeddings play a crucial role in natural language-based information
retrieval. Embedding models represent words and contexts as vectors whose
spatial configuration is derived from the distribution of words in large text
corpora. While such representations are generally very powerful, they might
fail to account for fine-grained domain-specific nuances. In this article, we
investigate this uncertainty for the domain of characterizations of expressive
piano performance. Using a music research dataset of free text performance
characterizations and a follow-up study sorting the annotations into clusters,
we derive a ground truth for a domain-specific semantic similarity structure.
We test five embedding models and their similarity structure for correspondence
with the ground truth. We further assess the effects of contextualizing
prompts, hubness reduction, cross-modal similarity, and k-means clustering. The
quality of embedding models shows great variability with respect to this task;
more general models perform better than domain-adapted ones and the best model
configurations reach human-level agreement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02981">
<div class="article-summary-box-inner">
<span><p>Recent releases of pre-trained Large Language Models (LLMs) have gained
considerable traction, yet research on fine-tuning and employing
domain-specific LLMs remains scarce. This study investigates approaches for
fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,
foundational models, and methods for domain-specific pre-training. Focusing on
the financial sector, it details dataset selection, preprocessing, model
choice, and considerations crucial for LLM fine-tuning in finance. Addressing
the unique characteristics of financial data, the study explores the
construction of domain-specific vocabularies and considerations for security
and regulatory compliance. In the practical application of LLM fine-tuning, the
study outlines the procedure and implementation for generating domain-specific
LLMs in finance. Various financial cases, including stock price prediction,
sentiment analysis of financial news, automated document processing, research,
information extraction, and customer service enhancement, are exemplified. The
study explores the potential of LLMs in the financial domain, identifies
limitations, and proposes directions for improvement, contributing valuable
insights for future research. Ultimately, it advances natural language
processing technology in business, suggesting proactive LLM utilization in
financial services across industries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02982">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated impressive capabilities across
a wide range of tasks. However, their proficiency and reliability in the
specialized domain of Data Analysis, particularly with a focus on data-driven
thinking, remain uncertain. To bridge this gap, we introduce BIBench, a
comprehensive benchmark designed to evaluate the data analysis capabilities of
LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs
across three dimensions: 1) BI foundational knowledge, evaluating the models'
numerical reasoning and familiarity with financial concepts; 2) BI knowledge
application, determining the models' ability to quickly comprehend textual
information and generate analysis questions from multiple views; and 3) BI
technical skills, examining the models' use of technical knowledge to address
real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning
three categories of task types: classification, extraction, and generation.
Additionally, we've developed BIChat, a domain-specific dataset with over a
million data points, to fine-tune LLMs. We will release BIBenchmark, BIChat,
and the evaluation scripts at \url{https://github.com/cubenlp/BIBench}. This
benchmark aims to provide a measure for in-depth analysis of LLM abilities and
foster the advancement of LLMs in the field of data analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models in Mental Health Care: a Scoping Review. (arXiv:2401.02984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02984">
<div class="article-summary-box-inner">
<span><p>Objective: The growing use of large language models (LLMs) stimulates a need
for a comprehensive review of their applications and outcomes in mental health
care contexts. This scoping review aims to critically analyze the existing
development and applications of LLMs in mental health care, highlighting their
successes and identifying their challenges and limitations in these specialized
fields. Materials and Methods: A broad literature search was conducted in
November 2023 using six databases (PubMed, Web of Science, Google Scholar,
arXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred
Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A
total of 313 publications were initially identified, and after applying the
study inclusion criteria, 34 publications were selected for the final review.
Results: We identified diverse applications of LLMs in mental health care,
including diagnosis, therapy, patient engagement enhancement, etc. Key
challenges include data availability and reliability, nuanced handling of
mental states, and effective evaluation methods. Despite successes in accuracy
and accessibility improvement, gaps in clinical applicability and ethical
considerations were evident, pointing to the need for robust data, standardized
evaluations, and interdisciplinary collaboration. Conclusion: LLMs show
promising potential in advancing mental health care, with applications in
diagnostics, and patient support. Continued advancements depend on
collaborative, multidisciplinary efforts focused on framework enhancement,
rigorous dataset development, technological refinement, and ethical integration
to ensure the effective and safe application of LLMs in mental health care.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education. (arXiv:2401.02985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02985">
<div class="article-summary-box-inner">
<span><p>The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods. (arXiv:2401.02986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02986">
<div class="article-summary-box-inner">
<span><p>Organizations face the challenge of ensuring compliance with an increasing
amount of requirements from various regulatory documents. Which requirements
are relevant depends on aspects such as the geographic location of the
organization, its domain, size, and business processes. Considering these
contextual factors, as a first step, relevant documents (e.g., laws,
regulations, directives, policies) are identified, followed by a more detailed
analysis of which parts of the identified documents are relevant for which step
of a given business process. Nowadays the identification of regulatory
requirements relevant to business processes is mostly done manually by domain
and legal experts, posing a tremendous effort on them, especially for a large
number of regulatory documents which might frequently change. Hence, this work
examines how legal and domain experts can be assisted in the assessment of
relevant requirements. For this, we compare an embedding-based NLP ranking
method, a generative AI method using GPT-4, and a crowdsourced method with the
purely manual method of creating relevancy labels by experts. The proposed
methods are evaluated based on two case studies: an Australian insurance case
created with domain experts and a global banking use case, adapted from SAP
Signavio's workflow example of an international guideline. A gold standard is
created for both BPMN2.0 processes and matched to real-world textual
requirements from multiple regulatory documents. The evaluation and discussion
provide insights into strengths and weaknesses of each method regarding
applicability, automation, transparency, and reproducibility and provide
guidelines on which method combinations will maximize benefits for given
characteristics such as process usage, impact, and dynamics of an application
scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02987">
<div class="article-summary-box-inner">
<span><p>The emergence of pretrained models has significantly impacted from Natural
Language Processing (NLP) and Computer Vision to relational datasets.
Traditionally, these models are assessed through fine-tuned downstream tasks.
However, this raises the question of how to evaluate these models more
efficiently and more effectively. In this study, we explore a novel approach
where we leverage the meta features associated with each entity as a source of
worldly knowledge and employ entity representations from the models. We propose
using the consistency between these representations and the meta features as a
metric for evaluating pretrained models. Our method's effectiveness is
demonstrated across various domains, including models with relational datasets,
large language models and images models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Latent Dirichlet Allocation (LDA) Semantic Text Analytics Approach to Explore Topical Features in Charity Crowdfunding Campaigns. (arXiv:2401.02988v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02988">
<div class="article-summary-box-inner">
<span><p>Crowdfunding in the realm of the Social Web has received substantial
attention, with prior research examining various aspects of campaigns,
including project objectives, durations, and influential project categories for
successful fundraising. These factors are crucial for entrepreneurs seeking
donor support. However, the terrain of charity crowdfunding within the Social
Web remains relatively unexplored, lacking comprehension of the motivations
driving donations that often lack concrete reciprocation. Distinct from
conventional crowdfunding that offers tangible returns, charity crowdfunding
relies on intangible rewards like tax advantages, recognition posts, or
advisory roles. Such details are often embedded within campaign narratives,
yet, the analysis of textual content in charity crowdfunding is limited. This
study introduces an inventive text analytics framework, utilizing Latent
Dirichlet Allocation (LDA) to extract latent themes from textual descriptions
of charity campaigns. The study has explored four different themes, two each in
campaign and incentive descriptions. Campaign description themes are focused on
child and elderly health mainly the ones who are diagnosed with terminal
diseases. Incentive description themes are based on tax benefits, certificates,
and appreciation posts. These themes, combined with numerical parameters,
predict campaign success. The study was successful in using Random Forest
Classifier to predict success of the campaign using both thematic and numerical
parameters. The study distinguishes thematic categories, particularly medical
need-based charity and general causes, based on project and incentive
descriptions. In conclusion, this research bridges the gap by showcasing topic
modelling utility in uncharted charity crowdfunding domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLIDE-RL: Grounded Language Instruction through DEmonstration in RL. (arXiv:2401.02991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02991">
<div class="article-summary-box-inner">
<span><p>One of the final frontiers in the development of complex human - AI
collaborative systems is the ability of AI agents to comprehend the natural
language and perform tasks accordingly. However, training efficient
Reinforcement Learning (RL) agents grounded in natural language has been a
long-standing challenge due to the complexity and ambiguity of the language and
sparsity of the rewards, among other factors. Several advances in reinforcement
learning, curriculum learning, continual learning, language models have
independently contributed to effective training of grounded agents in various
environments. Leveraging these developments, we present a novel algorithm,
Grounded Language Instruction through DEmonstration in RL (GLIDE-RL) that
introduces a teacher-instructor-student curriculum learning framework for
training an RL agent capable of following natural language instructions that
can generalize to previously unseen language instructions. In this multi-agent
framework, the teacher and the student agents learn simultaneously based on the
student's current skill level. We further demonstrate the necessity for
training the student agent with not just one, but multiple teacher agents.
Experiments on a complex sparse reward environment validates the effectiveness
of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Unstructured Data Processing for ESG Reports: A Methodology for Structured Transformation and Enhanced Analysis. (arXiv:2401.02992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02992">
<div class="article-summary-box-inner">
<span><p>In the evolving field of corporate sustainability, analyzing unstructured
Environmental, Social, and Governance (ESG) reports is a complex challenge due
to their varied formats and intricate content. This study introduces an
innovative methodology utilizing the "Unstructured Core Library", specifically
tailored to address these challenges by transforming ESG reports into
structured, analyzable formats. Our approach significantly advances the
existing research by offering high-precision text cleaning, adept
identification and extraction of text from images, and standardization of
tables within these reports. Emphasizing its capability to handle diverse data
types, including text, images, and tables, the method adeptly manages the
nuances of differing page layouts and report styles across industries. This
research marks a substantial contribution to the fields of industrial ecology
and corporate sustainability assessment, paving the way for the application of
advanced NLP technologies and large language models in the analysis of
corporate governance and sustainability. Our code is available at
https://github.com/linancn/TianGong-AI-Unstructure.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02993">
<div class="article-summary-box-inner">
<span><p>Retrieval-based augmentations that aim to incorporate knowledge from an
external database into language models have achieved great success in various
knowledge-intensive (KI) tasks, such as question-answering and text generation.
However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as
text classification, is still challenging. Existing works focus on
concatenating retrievals to inputs as context to form the prompt-based inputs.
Unfortunately, such methods require language models to have the capability to
handle long texts. Besides, inferring such concatenated data would also consume
a significant amount of computational resources.
</p>
<p>To solve these challenges, we propose \textbf{ReFusion} in this paper, a
computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with
neural architecture search. The main idea is to directly fuse the retrieval
representations into the language models. Specifically, we first propose an
online retrieval module that retrieves representations of similar sentences.
Then, we present a retrieval fusion module including two effective ranking
schemes, i.e., reranker-based scheme and ordered-mask-based scheme, to fuse the
retrieval representations with hidden states. Furthermore, we use Neural
Architecture Search (NAS) to seek the optimal fusion structure across different
layers. Finally, we conduct comprehensive experiments, and the results
demonstrate our ReFusion can achieve superior and robust performance on various
NKI tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02994">
<div class="article-summary-box-inner">
<span><p>In conversational AI research, there's a noticeable trend towards developing
models with a larger number of parameters, exemplified by models like ChatGPT.
While these expansive models tend to generate increasingly better chat
responses, they demand significant computational resources and memory. This
study explores a pertinent question: Can a combination of smaller models
collaboratively achieve comparable or enhanced performance relative to a
singular large model? We introduce an approach termed "blending", a
straightforward yet effective method of integrating multiple chat AIs. Our
empirical evidence suggests that when specific smaller models are
synergistically blended, they can potentially outperform or match the
capabilities of much larger counterparts. For instance, integrating just three
models of moderate size (6B/13B paramaeters) can rival or even surpass the
performance metrics of a substantially larger model like ChatGPT (175B+
paramaters). This hypothesis is rigorously tested using A/B testing
methodologies with a large user base on the Chai research platform over a span
of thirty days. The findings underscore the potential of the "blending"
strategy as a viable approach for enhancing chat AI efficacy without a
corresponding surge in computational demands.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CANAMRF: An Attention-Based Model for Multimodal Depression Detection. (arXiv:2401.02995v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02995">
<div class="article-summary-box-inner">
<span><p>Multimodal depression detection is an important research topic that aims to
predict human mental states using multimodal data. Previous methods treat
different modalities equally and fuse each modality by na\"ive mathematical
operations without measuring the relative importance between them, which cannot
obtain well-performed multimodal representations for downstream depression
tasks. In order to tackle the aforementioned concern, we present a Cross-modal
Attention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for
multimodal depression detection. CANAMRF is constructed by a multimodal feature
extractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid
Attention Module. Through experimentation on two benchmark datasets, CANAMRF
demonstrates state-of-the-art performance, underscoring the effectiveness of
our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blar-SQL: Faster, Stronger, Smaller NL2SQL. (arXiv:2401.02997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02997">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have gained considerable notoriety in the field
of natural language to SQL tasks (NL2SQL). In this study, we show how task
decomposition can greatly benefit LLMs in database understanding and query
generation in order to answer human questions with an SQL query.
</p>
<p>We fined-tuned open source models, specifically Llama-2 and Code Llama, by
combining 2 different models each designated to focus on one of two tasks in
order to leverage each model's core competency to further increase the accuracy
of the final SQL query.
</p>
<p>We propose a new framework to divide the schema into chunks in order to fit
more information into a limited context. Our results are comparable with those
obtained by GPT-4 at the same time being 135 times smaller, 90 times faster and
more than 100 times cheaper than GPT-4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03003">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have made significant advancements in
code-related tasks, yet many LLMs treat code as simple sequences, neglecting
its structured nature. We introduce AST-T5, a novel pretraining paradigm that
leverages the Abstract Syntax Tree (AST) for enhanced code generation,
transpilation, and understanding. Using dynamic programming, our AST-Aware
Segmentation retains code structure, while our AST-Aware Span Corruption
objective equips the model to reconstruct various code structures. Unlike other
models, AST-T5 avoids intricate program analyses or architectural changes, so
it integrates seamlessly with any encoder-decoder Transformer. Evaluations show
that AST-T5 consistently outperforms similar-sized LMs across various
code-related tasks. Structure-awareness makes AST-T5 particularly powerful in
code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the
Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in
CodeXGLUE. Our code and model are publicly available at
https://github.com/gonglinyuan/ast_t5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Gender Biases in Language Patterns of Human-Conversational Agent Conversations. (arXiv:2401.03030v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03030">
<div class="article-summary-box-inner">
<span><p>With the rise of human-machine communication, machines are increasingly
designed with humanlike characteristics, such as gender, which can
inadvertently trigger cognitive biases. Many conversational agents (CAs), such
as voice assistants and chatbots, default to female personas, leading to
concerns about perpetuating gender stereotypes and inequality. Critiques have
emerged regarding the potential objectification of females and reinforcement of
gender stereotypes by these technologies. This research, situated in
conversational AI design, aims to delve deeper into the impacts of gender
biases in human-CA interactions. From a behavioral and communication research
standpoint, this program focuses not only on perceptions but also the
linguistic styles of users when interacting with CAs, as previous research has
rarely explored. It aims to understand how pre-existing gender biases might be
triggered by CAs' gender designs. It further investigates how CAs' gender
designs may reinforce gender biases and extend them to human-human
communication. The findings aim to inform ethical design of conversational
agents, addressing whether gender assignment in CAs is appropriate and how to
promote gender equality in design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Forgetting in Continual Pre-training of Aligned Large Language Models. (arXiv:2401.03129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03129">
<div class="article-summary-box-inner">
<span><p>Recent advances in Large Language Models (LLMs) have exhibited remarkable
proficiency across various tasks. Given the potent applications of LLMs in
numerous fields, there has been a surge in LLM development. In developing LLMs,
a common practice involves continual pre-training on previously fine-tuned
models. However, this can lead to catastrophic forgetting. In our work, we
investigate the phenomenon of forgetting that occurs during continual
pre-training on an existing fine-tuned LLM. We evaluate the impact of
continuous pre-training on the fine-tuned LLM across various dimensions,
including output format, knowledge, and reliability. Experiment results
highlight the non-trivial challenge of addressing catastrophic forgetting
during continual pre-training, especially the repetition issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification. (arXiv:2401.03158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03158">
<div class="article-summary-box-inner">
<span><p>Short Text Classification (STC) is crucial for processing and comprehending
the brief but substantial content prevalent on contemporary digital platforms.
The STC encounters difficulties in grasping semantic and syntactic intricacies,
an issue that is apparent in traditional pre-trained language models. Although
Graph Convolutional Networks enhance performance by integrating external
knowledge bases, these methods are limited by the quality and extent of the
knowledge applied. Recently, the emergence of Large Language Models (LLMs) and
Chain-of-Thought (CoT) has significantly improved the performance of complex
reasoning tasks. However, some studies have highlighted the limitations of
their application in fundamental NLP tasks. Consequently, this study sought to
employ CoT to investigate the capabilities of LLMs in STC tasks. This study
introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This
framework primarily incorporates Syntactic and Semantic Enrichment CoT,
effectively decomposing the STC task into four distinct steps: (i) essential
concept identification, (ii) common-sense knowledge retrieval, (iii) text
rewriting, and (iv) classification. This elicits the inherent knowledge and
abilities of LLMs to address the challenges in STC. Surprisingly, we found that
QLFR can also improve the performance of smaller models. Therefore, we
developed a CoT-Driven Multi-task learning (QLFR-CML) method to facilitate the
knowledge transfer from LLMs to smaller models. Extensive experimentation
across six short-text benchmarks validated the efficacy of the proposed
methods. Notably, QLFR achieved state-of-the-art performance on all datasets,
with significant improvements, particularly on the Ohsumed and TagMyNews
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03175">
<div class="article-summary-box-inner">
<span><p>Language Processing systems such as Part-of-speech tagging, Named entity
recognition, Machine translation, Speech recognition, and Language modeling
(LM) are well-studied in high-resource languages. Nevertheless, research on
these systems for several low-resource languages, including Bodo, Mizo,
Nagamese, and others, is either yet to commence or is in its nascent stages.
Language model plays a vital role in the downstream tasks of modern NLP.
Extensive studies are carried out on LMs for high-resource languages.
Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack
coverage. In this study, we first present BodoBERT, a language model for the
Bodo language. To the best of our knowledge, this work is the first such effort
to develop a language model for Bodo. Secondly, we present an ensemble DL-based
POS tagging model for Bodo. The POS tagging model is based on combinations of
BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We
cover several language models in the experiment to see how well they work in
POS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A
comparative experiment was also conducted on Assamese POS taggers, considering
that the language is spoken in the same region as Bodo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks. (arXiv:2401.03177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03177">
<div class="article-summary-box-inner">
<span><p>Text-video retrieval is a challenging task that aims to identify relevant
videos given textual queries. Compared to conventional textual retrieval, the
main obstacle for text-video retrieval is the semantic gap between the textual
nature of queries and the visual richness of video content. Previous works
primarily focus on aligning the query and the video by finely aggregating
word-frame matching signals. Inspired by the human cognitive process of
modularly judging the relevance between text and video, the judgment needs
high-order matching signal due to the consecutive and complex nature of video
contents. In this paper, we propose chunk-level text-video matching, where the
query chunks are extracted to describe a specific retrieval unit, and the video
chunks are segmented into distinct clips from videos. We formulate the
chunk-level matching as n-ary correlations modeling between words of the query
and frames of the video and introduce a multi-modal hypergraph for n-ary
correlation modeling. By representing textual units and video frames as nodes
and using hyperedges to depict their relationships, a multi-modal hypergraph is
constructed. In this way, the query and the video can be aligned in a
high-order semantic space. In addition, to enhance the model's generalization
ability, the extracted features are fed into a variational inference component
for computation, obtaining the variational representation under the Gaussian
distribution. The incorporation of hypergraphs and variational inference allows
our model to capture complex, n-ary interactions among textual and visual
contents. Experimental results demonstrate that our proposed method achieves
state-of-the-art performance on the text-video retrieval task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint-Reasoning based Disease Q&A System. (arXiv:2401.03181v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03181">
<div class="article-summary-box-inner">
<span><p>Medical question answer (QA) assistants respond to lay users' health-related
queries by synthesizing information from multiple sources using natural
language processing and related techniques. They can serve as vital tools to
alleviate issues of misinformation, information overload, and complexity of
medical language, thus addressing lay users' information needs while reducing
the burden on healthcare professionals. QA systems, the engines of such
assistants, have typically used either language models (LMs) or knowledge
graphs (KG), though the approaches could be complementary. LM-based QA systems
excel at understanding complex questions and providing well-formed answers, but
are prone to factual mistakes. KG-based QA systems, which represent facts well,
are mostly limited to answering short-answer questions with pre-created
templates. While a few studies have jointly used LM and KG approaches for
text-based QA, this was done to answer multiple-choice questions. Extant QA
systems also have limitations in terms of automation and performance. We
address these challenges by designing a novel, automated disease QA system
which effectively utilizes both LM and KG techniques through a joint-reasoning
approach to answer disease-related questions appropriate for lay users. Our
evaluation of the system using a range of quality metrics demonstrates its
efficacy over benchmark systems, including the popular ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">{\delta}-CAUSAL: Exploring Defeasibility in Causal Reasoning. (arXiv:2401.03183v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03183">
<div class="article-summary-box-inner">
<span><p>Defeasibility in causal reasoning implies that the causal relationship
between cause and effect can be strengthened or weakened. Namely, the causal
strength between cause and effect should increase or decrease with the
incorporation of strengthening arguments (supporters) or weakening arguments
(defeaters), respectively. However, existing works ignore defeasibility in
causal reasoning and fail to evaluate existing causal strength metrics in
defeasible settings. In this work, we present {\delta}-CAUSAL, the first
benchmark dataset for studying defeasibility in causal reasoning.
{\delta}-CAUSAL includes around 11K events spanning ten domains, featuring
defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters
and defeaters. We further show current causal strength metrics fail to reflect
the change of causal strength with the incorporation of supporters or defeaters
in {\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation
with Attention Rating), a metric that measures causal strength based on
token-level causal relationships. CESAR achieves a significant 69.7% relative
improvement over existing metrics, increasing from 47.2% to 80.1% in capturing
the causal strength change brought by supporters and defeaters. We further
demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and
10.7 points behind humans in generating supporters and defeaters, emphasizing
the challenge posed by {\delta}-CAUSAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03190">
<div class="article-summary-box-inner">
<span><p>Large language models are known for encoding a vast amount of factual
knowledge, but they often becomes outdated due to the ever-changing nature of
external information. A promising solution to this challenge is the utilization
of model editing methods to update the knowledge in an efficient manner.
However, the majority of existing model editing techniques are limited to
monolingual frameworks, thus failing to address the crucial issue of
cross-lingual knowledge synchronization for multilingual models. To tackle this
problem, we propose a simple yet effective method that trains multilingual
patch neuron to store cross-lingual knowledge. It can be easily adapted to
existing approaches to enhance their cross-lingual editing capabilities. To
evaluate our method, we conduct experiments using both the XNLI dataset and a
self-constructed XFEVER dataset. Experimental results demonstrate that our
proposed method achieves improved performance in cross-lingual editing tasks
without requiring excessive modifications to the original methodology, thereby
showcasing its user-friendly characteristics. Codes will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. (arXiv:2401.03205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03205">
<div class="article-summary-box-inner">
<span><p>In the era of large language models (LLMs), hallucination (i.e., the tendency
to generate factually incorrect content) poses great challenge to trustworthy
and reliable deployment of LLMs in real-world applications. To tackle the LLM
hallucination, three key questions should be well studied: how to detect
hallucinations (detection), why do LLMs hallucinate (source), and what can be
done to mitigate them (mitigation). To address these challenges, this work
presents a systematic empirical study on LLM hallucination, focused on the the
three aspects of hallucination detection, source and mitigation. Specially, we
construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet
effective detection method for LLM hallucination. Furthermore, we zoom into the
different training or utilization stages of LLMs and extensively analyze the
potential factors that lead to the LLM hallucination. Finally, we implement and
examine a series of widely used techniques to mitigate the hallucinations in
LLMs. Our work has led to several important findings to understand the
hallucination origin and mitigate the hallucinations in LLMs. Our code and data
can be accessed at https://github.com/RUCAIBox/HaluEval-2.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflections on Inductive Thematic Saturation as a potential metric for measuring the validity of an inductive Thematic Analysis with LLMs. (arXiv:2401.03239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03239">
<div class="article-summary-box-inner">
<span><p>This paper presents a set of reflections on saturation and the use of Large
Language Models (LLMs) for performing Thematic Analysis (TA). The paper
suggests that initial thematic saturation (ITS) could be used as a metric to
assess part of the transactional validity of TA with LLM, focusing on the
initial coding. The paper presents the initial coding of two datasets of
different sizes, and it reflects on how the LLM reaches some form of analytical
saturation during the coding. The procedure proposed in this work leads to the
creation of two codebooks, one comprising the total cumulative initial codes
and the other the total unique codes. The paper proposes a metric to
synthetically measure ITS using a simple mathematical calculation employing the
ratio between slopes of cumulative codes and unique codes. The paper
contributes to the initial body of work exploring how to perform qualitative
analysis with LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03253">
<div class="article-summary-box-inner">
<span><p>Recent advances achieved by deep learning models rely on the independent and
identically distributed assumption, hindering their applications in real-world
scenarios with domain shifts. To address the above issues, cross-domain
learning aims at extracting domain-invariant knowledge to reduce the domain
shift between training and testing data. However, in visual cross-domain
learning, traditional methods concentrate solely on the image modality,
neglecting the use of the text modality to alleviate the domain shift. In this
work, we propose Large Language models as Visual cross-dOmain learners (LLaVO).
LLaVO uses vision-language models to convert images into detailed textual
descriptions. A large language model is then finetuned on textual descriptions
of the source/target domain generated by a designed instruction template.
Extensive experimental results on various cross-domain tasks under the domain
generalization and unsupervised domain adaptation settings have demonstrated
the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03314">
<div class="article-summary-box-inner">
<span><p>Neural machine translation benefits from semantically rich representations.
Considerable progress in learning such representations has been achieved by
language modelling and mutual information maximization objectives using
contrastive learning. The language-dependent nature of language modelling
introduces a trade-off between the universality of the learned representations
and the model's performance on the language modelling tasks. Although
contrastive learning improves performance, its success cannot be attributed to
mutual information alone. We propose a novel Context Enhancement step to
improve performance on neural machine translation by maximizing mutual
information using the Barlow Twins loss. Unlike other approaches, we do not
explicitly augment the data but view languages as implicit augmentations,
eradicating the risk of disrupting semantic information. Further, our method
does not learn embeddings from scratch and can be generalised to any set of
pre-trained embeddings. Finally, we evaluate the language-agnosticism of our
embeddings through language classification and use them for neural machine
translation to compare with state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIXAR: Auto-Regressive Language Modeling in Pixel Space. (arXiv:2401.03321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03321">
<div class="article-summary-box-inner">
<span><p>Recent works showed the possibility of building open-vocabulary large
language models (LLMs) that directly operate on pixel representations and are
implemented as encoder-decoder models that reconstruct masked image patches of
rendered text. However, these pixel-based LLMs are limited to autoencoding
tasks and cannot generate new text as images. As such, they cannot be used for
open-answer or generative language tasks. In this work, we overcome this
limitation and introduce PIXAR, the first pixel-based autoregressive LLM that
does not rely on a pre-defined vocabulary for both input and output text.
Consisting of only a decoder, PIXAR can answer free-form generative tasks while
keeping the text representation learning performance on par with previous
encoder-decoder models. Furthermore, we highlight the challenges to
autoregressively generate non-blurred text as images and link this to the usual
maximum likelihood objective. We propose a simple adversarial pretraining that
significantly improves the readability and performance of PIXAR making it
comparable to GPT2 on short text generation tasks. This paves the way to
building open-vocabulary LLMs that are usable for free-form generative tasks
and questions the necessity of the usual symbolic input representation -- text
as tokens -- for these challenging tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.03346">
<div class="article-summary-box-inner">
<span><p>Hate speech has emerged as a major problem plaguing our social spaces today.
While there have been significant efforts to address this problem, existing
methods are still significantly limited in effectively detecting hate speech
online. A major limitation of existing methods is that hate speech detection is
a highly contextual problem, and these methods cannot fully capture the context
of hate speech to make accurate predictions. Recently, large language models
(LLMs) have demonstrated state-of-the-art performance in several natural
language tasks. LLMs have undergone extensive training using vast amounts of
natural language data, enabling them to grasp intricate contextual details.
Hence, they could be used as knowledge bases for context-aware hate speech
detection. However, a fundamental problem with using LLMs to detect hate speech
is that there are no studies on effectively prompting LLMs for context-aware
hate speech detection. In this study, we conduct a large-scale study of hate
speech detection, employing five established hate speech datasets. We discover
that LLMs not only match but often surpass the performance of current benchmark
machine learning models in identifying hate speech. By proposing four diverse
prompting strategies that optimize the use of LLMs in detecting hate speech.
Our study reveals that a meticulously crafted reasoning prompt can effectively
capture the context of hate speech by fully utilizing the knowledge base in
LLMs, significantly outperforming existing techniques. Furthermore, although
LLMs can provide a rich knowledge base for the contextual detection of hate
speech, suitable prompting strategies play a crucial role in effectively
leveraging this knowledge base for efficient detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00676">
<div class="article-summary-box-inner">
<span><p>Many adversarial attacks target natural language processing systems, most of
which succeed through modifying the individual tokens of a document. Despite
the apparent uniqueness of each of these attacks, fundamentally they are simply
a distinct configuration of four components: a goal function, allowable
transformations, a search method, and constraints. In this survey, we
systematically present the different components used throughout the literature,
using an attack-independent framework which allows for easy comparison and
categorisation of components. Our work aims to serve as a comprehensive guide
for newcomers to the field and to spark targeted research into refining the
individual attack components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15462">
<div class="article-summary-box-inner">
<span><p>We propose a margin-based loss for tuning joint vision-language models so
that their gradient-based explanations are consistent with region-level
annotations provided by humans for relatively smaller grounding datasets. We
refer to this objective as Attention Mask Consistency (AMC) and demonstrate
that it produces superior visual grounding results than previous methods that
rely on using vision-language models to score the outputs of object detectors.
Particularly, a model trained with AMC on top of standard vision-language
modeling objectives obtains a state-of-the-art accuracy of 86.49% in the
Flickr30k visual grounding benchmark, an absolute improvement of 5.38% when
compared to the best previous model trained under the same level of
supervision. Our approach also performs exceedingly well on established
benchmarks for referring expression comprehension where it obtains 80.34%
accuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC
is effective, easy to implement, and is general as it can be adopted by any
vision-language model, and can use any type of region annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Human-Language Model Interaction. (arXiv:2212.09746v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09746">
<div class="article-summary-box-inner">
<span><p>Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13126">
<div class="article-summary-box-inner">
<span><p>Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey on Instruction Following. (arXiv:2303.10475v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10475">
<div class="article-summary-box-inner">
<span><p>Task semantics can be expressed by a set of input-output examples or a piece
of textual instruction. Conventional machine learning approaches for natural
language processing (NLP) mainly rely on the availability of large-scale sets
of task-specific examples. Two issues arise: first, collecting task-specific
labeled examples does not apply to scenarios where tasks may be too complicated
or costly to annotate, or the system is required to handle a new task
immediately; second, this is not user-friendly since end-users are probably
more willing to provide task description rather than a set of examples before
using the system. Therefore, the community is paying increasing interest in a
new supervision-seeking paradigm for NLP: learning to follow task instructions,
i.e., instruction following. Despite its impressive progress, there are some
common issues that the community struggles with. This survey paper tries to
summarize and provide insights to the current research on instruction
following, particularly, by answering the following questions: (i) What is task
instruction, and what instruction types exist? (ii) How to model instructions?
(iii) What are popular instruction following datasets and evaluation metrics?
(iv) What factors influence and explain the instructions' performance? (v) What
challenges remain in instruction following? To our knowledge, this is the first
comprehensive survey about instruction following.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08242">
<div class="article-summary-box-inner">
<span><p>Numerical interactions leading to users sharing textual content published by
others are naturally represented by a network where the individuals are
associated with the nodes and the exchanged texts with the edges. To understand
those heterogeneous and complex data structures, clustering nodes into
homogeneous groups as well as rendering a comprehensible visualisation of the
data is mandatory. To address both issues, we introduce Deep-LPTM, a
model-based clustering strategy relying on a variational graph auto-encoder
approach as well as a probabilistic model to characterise the topics of
discussion. Deep-LPTM allows to build a joint representation of the nodes and
of the edges in two embeddings spaces. The parameters are inferred using a
variational inference algorithm. We also introduce IC2L, a model selection
criterion specifically designed to choose models with relevant clustering and
visualisation properties. An extensive benchmark study on synthetic data is
provided. In particular, we find that Deep-LPTM better recovers the partitions
of the nodes than the state-of-the art ETSBM and STBM. Eventually, the emails
of the Enron company are analysed and visualisations of the results are
presented, with meaningful highlights of the graph structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Non-native Speech Corpus Featuring Chinese-English Bilingual Children: Compilation and Rationale. (arXiv:2305.00446v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00446">
<div class="article-summary-box-inner">
<span><p>This paper introduces a non-native speech corpus consisting of narratives
from fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5
hours of children taking a narrative comprehension test in English (L2) are
presented, along with human-rated scores and annotations of grammatical and
pronunciation errors. The children also completed the parallel MAIN tests in
Chinese (L1) for reference purposes. For all tests we recorded audio and video
with our innovative self-developed remote collection methods. The video
recordings serve to mitigate the challenge of low intelligibility in L2
narratives produced by young children during the transcription process. This
corpus offers valuable resources for second language teaching and has the
potential to enhance the overall performance of automatic speech recognition
(ASR).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Recommender with Geometric Information Bottleneck. (arXiv:2305.05331v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05331">
<div class="article-summary-box-inner">
<span><p>Explainable recommender systems can explain their recommendation decisions,
enhancing user trust in the systems. Most explainable recommender systems
either rely on human-annotated rationales to train models for explanation
generation or leverage the attention mechanism to extract important text spans
from reviews as explanations. The extracted rationales are often confined to an
individual review and may fail to identify the implicit features beyond the
review text. To avoid the expensive human annotation process and to generate
explanations beyond individual reviews, we propose to incorporate a geometric
prior learnt from user-item interactions into a variational network which
infers latent factors from user-item reviews. The latent factors from an
individual user-item pair can be used for both recommendation and explanation
generation, which naturally inherit the global characteristics encoded in the
prior knowledge. Experimental results on three e-commerce datasets show that
our model significantly improves the interpretability of a variational
recommender using the Wasserstein distance while achieving performance
comparable to existing content-based recommender systems in terms of
recommendation behaviours.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPE:Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07789">
<div class="article-summary-box-inner">
<span><p>The dominant paradigm of textual question answering systems is based on
end-to-end neural networks, which excels at answering natural language
questions but falls short on complex ones. This stands in contrast to the broad
adaptation of semantic parsing approaches over structured data sources (e.g.,
relational database, knowledge graphs), that convert natural language questions
to logical forms and execute them with query engines. Towards combining the
strengths of neural and symbolic methods, we propose a framework of question
parsing and execution on textual QA. It comprises two central pillars: (1) We
parse the question of varying complexity into an intermediate representation,
named H-expression, which is composed of simple questions as the primitives and
symbolic operations representing the relationships among them; (2) To execute
the resulting H-expressions, we design a hybrid executor, which integrates the
deterministic rules to translate the symbolic operations with a drop-in neural
reader network to answer each decomposed simple question. Hence, the proposed
framework can be viewed as a top-down question parsing followed by a bottom-up
answer backtracking. The resulting H-expressions closely guide the execution
process, offering higher precision besides better interpretability while still
preserving the advantages of the neural readers for resolving its primitive
elements. Our extensive experiments on MuSiQue, 2WikiQA, HotpotQA, and NQ show
that the proposed parsing and hybrid execution framework outperforms existing
approaches in supervised, few-shot, and zero-shot settings, while also
effectively exposing its underlying reasoning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Decomposition and Interpretation of Complex Utterances. (arXiv:2305.08677v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08677">
<div class="article-summary-box-inner">
<span><p>Designing natural language interfaces has historically required collecting
supervised data to translate user requests into carefully designed intent
representations. This requires enumerating and labeling a long tail of user
requests, which is challenging. At the same time, large language models (LLMs)
encode knowledge about goals and plans that can help conversational assistants
interpret user requests requiring numerous steps to complete. We introduce an
approach to handle complex-intent-bearing utterances from a user via a process
of hierarchical natural language decomposition and interpretation. Our approach
uses a pre-trained language model to decompose a complex utterance into a
sequence of simpler natural language steps and interprets each step using the
language-to-program model designed for the interface. To test our approach, we
collect and release DeCU -- a new NL-to-program benchmark to evaluate
Decomposition of Complex Utterances. Experiments show that the proposed
approach enables the interpretation of complex utterances with almost no
complex training data, while outperforming standard few-shot prompting
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10037">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are increasingly adopted for a variety of tasks
with implicit graphical structures, such as planning in robotics, multi-hop
question answering or knowledge probing, structured commonsense reasoning, and
more. While LLMs have advanced the state-of-the-art on these tasks with
structure implications, whether LLMs could explicitly process textual
descriptions of graphs and structures, map them to grounded conceptual spaces,
and perform structured operations remains underexplored. To this end, we
propose NLGraph (Natural Language Graph), a comprehensive benchmark of
graph-based problem solving designed in natural language. NLGraph contains
29,370 problems, covering eight graph reasoning tasks with varying complexity
from simple tasks such as connectivity and shortest path up to complex problems
such as maximum flow and simulating graph neural networks. We evaluate LLMs
(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find
that 1) language models do demonstrate preliminary graph reasoning abilities,
2) the benefit of advanced prompting and in-context learning diminishes on more
complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the
face of spurious correlations in graph and problem settings. We then propose
Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based
approaches to enhance LLMs in solving natural language graph problems.
Build-a-Graph and Algorithmic prompting improve the performance of LLMs on
NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to
solve the most complicated graph reasoning tasks in our setup with language
models remains an open research question. The NLGraph benchmark and evaluation
code are available at https://github.com/Arthur-Heng/NLGraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14387">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their strong instruction-following abilities. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following requires
tackling three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 50x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00789">
<div class="article-summary-box-inner">
<span><p>The paper presents a novel three-step transfer learning framework for
enhancing cross-lingual transfer from high- to low-resource languages in the
downstream application of Automatic Speech Translation. The approach integrates
a semantic knowledge-distillation step into the existing two-step cross-lingual
transfer learning framework XLS-R. This extra step aims to encode semantic
knowledge in the multilingual speech encoder pre-trained via Self-Supervised
Learning using unlabeled speech. Our proposed three-step cross-lingual transfer
learning framework addresses the large cross-lingual transfer gap (TRFGap)
observed in the XLS-R framework between high-resource and low-resource
languages. We validate our proposal through extensive experiments and
comparisons on the CoVoST-2 benchmark, showing significant improvements in
translation performance, especially for low-resource languages, and a notable
reduction in the TRFGap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09312">
<div class="article-summary-box-inner">
<span><p>We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15504">
<div class="article-summary-box-inner">
<span><p>Instruction tuning has emerged as a promising approach to enhancing large
language models in following human instructions. It is shown that increasing
the diversity and number of instructions in the training data can consistently
enhance generalization performance, which facilitates a recent endeavor to
collect various instructions and integrate existing instruction tuning datasets
into larger collections. However, different users have their unique ways of
expressing instructions, and there often exist variations across different
datasets in the instruction styles and formats, i.e., format inconsistency. In
this work, we propose a framework named Unified Instruction Tuning (UIT), which
calls OpenAI APIs for automatic format transfer among different instruction
tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we
(1) demonstrate the necessity of maintaining format consistency in instruction
tuning; (2) improve the generalization performance on unseen instructions on
T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the
noise of automatic format transfer to make the UIT framework more practical and
a smaller offline model based on GPT-J that achieves comparable format transfer
capability to OpenAI APIs to reduce costs in practice. Further analysis
regarding variations of targeted formats and other effects is intended.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04306">
<div class="article-summary-box-inner">
<span><p>Metaphor as an advanced cognitive modality works by extracting familiar
concepts in the target domain in order to understand vague and abstract
concepts in the source domain. This helps humans to quickly understand and
master new domains and thus adapt to changing environments. With the continuous
development of metaphor research in the natural language community, many
studies using knowledge-assisted models to detect textual metaphors have
emerged in recent years. Compared to not using knowledge, systems that
introduce various kinds of knowledge achieve greater performance gains and
reach SOTA in a recent study. Based on this, the goal of this paper is to
provide a comprehensive review of research advances in the application of deep
learning for knowledge injection in metaphor detection tasks. We will first
systematically summarize and generalize the mainstream knowledge and knowledge
injection principles. Then, the datasets, evaluation metrics, and benchmark
models used in metaphor detection tasks are examined. Finally, we explore the
current issues facing knowledge injection methods and provide an outlook on
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11380">
<div class="article-summary-box-inner">
<span><p>This paper presents an end-to-end model designed to improve automatic speech
recognition (ASR) for a particular speaker in a crowded, noisy environment. The
model utilizes a single-channel speech enhancement module that isolates the
speaker's voice from background noise (ConVoiFilter) and an ASR module. The
model can decrease ASR's word error rate (WER) from 80% to 26.4% through this
approach. Typically, these two components are adjusted independently due to
variations in data requirements. However, speech enhancement can create
anomalies that decrease ASR efficiency. By implementing a joint fine-tuning
strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5%
in joint tuning. We openly share our pre-trained model to foster further
research hf.co/nguyenvulebinh/voice-filter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WavMark: Watermarking for Audio Generation. (arXiv:2308.12770v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12770">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in zero-shot voice synthesis have enabled imitating a
speaker's voice using just a few seconds of recording while maintaining a high
level of realism. Alongside its potential benefits, this powerful technology
introduces notable risks, including voice fraud and speaker impersonation.
Unlike the conventional approach of solely relying on passive methods for
detecting synthetic data, watermarking presents a proactive and robust defence
mechanism against these looming risks. This paper introduces an innovative
audio watermarking framework that encodes up to 32 bits of watermark within a
mere 1-second audio snippet. The watermark is imperceptible to human senses and
exhibits strong resilience against various attacks. It can serve as an
effective identifier for synthesized voices and holds potential for broader
applications in audio copyright protection. Moreover, this framework boasts
high flexibility, allowing for the combination of multiple watermark segments
to achieve heightened robustness and expanded capacity. Utilizing 10 to
20-second audio as the host, our approach demonstrates an average Bit Error
Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over
2800\% in BER compared to the state-of-the-art watermarking tool. See
https://aka.ms/wavmark for demos of our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Term Ad Memorability: Understanding and Generating Memorable Ads. (arXiv:2309.00378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00378">
<div class="article-summary-box-inner">
<span><p>Marketers spend billions of dollars on advertisements but to what end? At the
time of purchase, if customers cannot recognize the brand for which they saw an
ad, the money spent on the ad is essentially wasted. Despite its importance in
marketing, until now, there has been no study on the memorability of ads in the
ML literature. Most studies have been conducted on short-term recall (&lt;5 mins)
on specific content types like object and action videos. On the other hand, the
advertising industry only cares about long-term memorability, and ads are
almost always highly multimodal, depicting a story through its different
modalities. With this motivation, we release the first large-scale memorability
dataset, LAMDBA, consisting of 1749 participants and 2205 ads covering 276
brands. Running statistical tests over different participant subpopulations and
ad types, we find many interesting insights into what makes an ad memorable.
For e.g., we find that brands that use commercials with fast-moving scenes are
more memorable than those with slower scenes (p=8e-10) and that people who use
ad-blockers remember fewer ads than those who don't (p=5e-3). Next, to simulate
the memorability of marketing materials for a particular audience, we present a
novel model, Henry, trained to leverage real-world knowledge of LLMs and visual
knowledge to predict the memorability. We test Henry on all the prominent
memorability datasets in literature (both images and videos) and achieve
state-of-the-art performance across all of them. Henry shows strong
generalization showing better results in 0-shot on unseen datasets. Next, we
propose the task of memorable ad generation and release a large-scale ad
dataset, UltraLAMBDA, consisting of 4 million ads with their Henry-assigned
memorability scores. We show that aligning Henry to generate memorable content
improves memorability scores by more than 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02045">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made significant strides in both scientific
research and practical applications. Existing studies have demonstrated the
state-of-the-art (SOTA) performance of LLMs in various natural language
processing tasks. However, the question of how to further enhance LLMs'
performance in specific task using prompting strategies remains a pivotal
concern. This paper explores the enhancement of LLMs' performance in sentiment
analysis through the application of prompting strategies. We formulate the
process of prompting for sentiment analysis tasks and introduce two novel
strategies tailored for sentiment analysis: RolePlaying (RP) prompting and
Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT
prompting strategy which is a combination of RP prompting and CoT prompting. We
conduct comparative experiments on three distinct domain datasets to evaluate
the effectiveness of the proposed sentiment analysis strategies. The results
demonstrate that the adoption of the proposed prompting strategies leads to a
increasing enhancement in sentiment analysis accuracy. Further, the CoT
prompting strategy exhibits a notable impact on implicit sentiment analysis,
with the RP-CoT prompting strategy delivering the most superior performance
among all strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild. (arXiv:2309.08637v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08637">
<div class="article-summary-box-inner">
<span><p>Large language models with instruction-following abilities have
revolutionized the field of artificial intelligence. These models show
exceptional generalizability to tackle various real-world tasks through their
natural language interfaces. However, their performance heavily relies on
high-quality exemplar data, which is often difficult to obtain. This challenge
is further exacerbated when it comes to multimodal instruction following. We
introduce TextBind, an almost annotation-free framework for empowering larger
language models with the multi-turn interleaved multimodal
instruction-following capabilities. Our approach requires only image-caption
pairs and generates multi-turn multimodal instruction-response conversations
from a language model. To accommodate interleaved image-text inputs and
outputs, we devise MIM, a language model-centric architecture that seamlessly
integrates image encoder and decoder models. We release our dataset, model, and
demo to foster future research in the area of multimodal instruction following.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Understand Real-World Complex Instructions?. (arXiv:2309.09150v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09150">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can understand human instructions, showing their
potential for pragmatic applications beyond traditional NLP tasks. However,
they still struggle with complex instructions, which can be either complex task
descriptions that require multiple tasks and constraints, or complex input that
contains long context, noise, heterogeneous information and multi-turn format.
Due to these features, LLMs often ignore semantic constraints from task
descriptions, generate incorrect formats, violate length or sample count
constraints, and be unfaithful to the input text. Existing benchmarks are
insufficient to assess LLMs' ability to understand complex instructions, as
they are close-ended and simple. To bridge this gap, we propose CELLO, a
benchmark for evaluating LLMs' ability to follow complex instructions
systematically. We design eight features for complex instructions and construct
a comprehensive evaluation dataset from real-world scenarios. We also establish
four criteria and develop corresponding metrics, as current ones are
inadequate, biased or too strict and coarse-grained. We compare the performance
of representative Chinese-oriented and English-oriented models in following
complex instructions through extensive experiments. Resources of CELLO are
publicly available at https://github.com/Abbey4799/CELLO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models. (arXiv:2310.01119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01119">
<div class="article-summary-box-inner">
<span><p>The in-context learning ability of large language models (LLMs) enables them
to generalize to novel downstream tasks with relatively few labeled examples.
However, they require enormous computational resources to be deployed.
Alternatively, smaller models can solve specific tasks if fine-tuned with
enough labeled examples. These examples, however, are expensive to obtain. In
pursuit of the best of both worlds, we study synthetic data generation of
fine-tuning training data via fine-tuned teacher LLMs to improve the downstream
performance of much smaller models. In four text classification and two text
generation tasks, we find that both data generation and annotation dramatically
improve the respective downstream model's performance, occasionally
necessitating only a minor fraction of the original training dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01132">
<div class="article-summary-box-inner">
<span><p>With the aim to provide teachers with more specific, frequent, and actionable
feedback about their teaching, we explore how Large Language Models (LLMs) can
be used to estimate ``Instructional Support'' domain scores of the CLassroom
Assessment Scoring System (CLASS), a widely used observation protocol. We
design a machine learning architecture that uses either zero-shot prompting of
Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify
individual utterances of teachers' speech (transcribed automatically using
OpenAI's Whisper) for the presence of Instructional Support. Then, these
utterance-level judgments are aggregated over an entire 15-min observation
session to estimate a global CLASS score. Experiments on two CLASS-coded
datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic
CLASS Instructional Support estimation accuracy using the proposed method
(Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to
$R=0.55$); (2) LLMs yield slightly greater accuracy than BoW for this task,
though the best models often combined features extracted from both LLM and BoW;
and (3) for classifying individual utterances, there is still room for
improvement of automated methods compared to human-level judgments. Finally,
(4) we illustrate how the model's outputs can be visualized at the utterance
level to provide teachers with explainable feedback on which utterances were
most positively or negatively correlated with specific CLASS dimensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResidualTransformer: Residual Low-Rank Learning with Weight-Sharing for Transformer Layers. (arXiv:2310.02489v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02489">
<div class="article-summary-box-inner">
<span><p>Memory constraint of always-on devices is one of the major concerns when
deploying speech processing models on these devices. While larger models
trained with sufficiently large amount of data generally perform better, making
them fit in the device memory is a demanding challenge. In this paper, we aim
to reduce model size by reparameterizing model weights across Transformer
encoder layers and assuming a special weight composition and structure. More
specifically, inspired by ResNet and the more recent LoRA work, we propose an
approach named ResidualTransformer, where each weight matrix in a Transformer
layer comprises 1) a shared full-rank component with its adjacent layers, and
2) a unique low-rank component to itself. The low-rank matrices only account
for a small amount of model size increase. In addition, we add diagonal weight
matrices to improve modeling capacity of the low-rank matrices. Experiments of
our 10k-hour speech recognition and speech translation tasks show that the
Transformer encoder size can be reduced by ~3X with very slight performance
degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pragmatic Evaluation of Clarifying Questions with Fact-Level Masking. (arXiv:2310.11571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.11571">
<div class="article-summary-box-inner">
<span><p>The ability to derive useful information by asking clarifying questions (ACQ)
is an important element of real life collaboration on reasoning tasks, such as
question answering (QA). Existing natural language ACQ challenges, however,
evaluate generations based on word overlap rather than the value of the
information itself. Word overlap is often an inappropriate metric for question
generation since many different questions could be useful in a given situation,
and a single question can be phrased many different ways. Instead, we propose
evaluating questions pragmatically based on the value of the information they
retrieve. Here we present a definition and framework for natural language
pragmatic asking of clarifying questions (PACQ), the problem of generating
questions that result in answers useful for a reasoning task. We also present
fact-level masking (FLM), a procedure for converting natural language datasets
into self-supervised PACQ datasets by omitting particular critical facts.
Finally, we generate a PACQ dataset from the HotpotQA dataset using FLM and
evaluate several zero-shot language models on it. Our experiments show that
current zero-shot models struggle to ask questions that retrieve useful
information, as compared to human annotators. These results demonstrate an
opportunity to use FLM datasets and the PACQ framework to objectively evaluate
and improve question generation and other language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons. (arXiv:2310.16582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.16582">
<div class="article-summary-box-inner">
<span><p>Personality plays a pivotal role in shaping human expression patterns, thus
regulating the personality of large language models (LLMs) holds significant
potential in enhancing the user experience of LLMs. Previous methods either
relied on fine-tuning LLMs on specific corpora or necessitated manually crafted
prompts to elicit specific personalities from LLMs. However, the former
approach is inefficient and costly, while the latter cannot precisely
manipulate personality traits at a fine-grained level. To address the above
challenges, we have employed a novel Unsupervisedly-Built Personalized Lexicons
(UBPL) in a pluggable manner during the decoding phase of LLMs to manipulate
their personality traits. UBPL is a lexicon built through an unsupervised
approach from a situational judgment test dataset (SJTs4LLM). Users can utilize
UBPL to adjust the probability vectors of predicted words in the decoding phase
of LLMs, thus influencing the personality expression of LLMs. Extensive
experimentation demonstrates the remarkable effectiveness and pluggability of
our method for fine-grained manipulation of LLM's personality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.16781">
<div class="article-summary-box-inner">
<span><p>Although the mapping between sound and meaning in human language is assumed
to be largely arbitrary, research in cognitive science has shown that there are
non-trivial correlations between particular sounds and meanings across
languages and demographic groups, a phenomenon known as sound symbolism. Among
the many dimensions of meaning, sound symbolism is particularly salient and
well-demonstrated with regards to cross-modal associations between language and
the visual domain. In this work, we address the question of whether sound
symbolism is reflected in vision-and-language models such as CLIP and Stable
Diffusion. Using zero-shot knowledge probing to investigate the inherent
knowledge of these models, we find strong evidence that they do show this
pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our
work provides a novel method for demonstrating sound symbolism and
understanding its nature using computational tools. Our code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.08648">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have achieved notable success in numerous NLP tasks,
employing both fine-tuning and in-context learning (ICL) methods. While
language models demonstrate exceptional performance, they face robustness
challenges due to spurious correlations arising from imbalanced label
distributions in training data or ICL exemplars. Previous research has
primarily concentrated on word, phrase, and syntax features, neglecting the
concept level, often due to the absence of concept labels and difficulty in
identifying conceptual content in input texts. This paper introduces two main
contributions. First, we employ ChatGPT to assign concept labels to texts,
assessing concept bias in models during fine-tuning or ICL on test data. We
find that LMs, when encountering spurious correlations between a concept and a
label in training or prompts, resort to shortcuts for predictions. Second, we
introduce a data rebalancing technique that incorporates ChatGPT-generated
counterfactual data, thereby balancing label distribution and mitigating
spurious correlations. Our method's efficacy, surpassing traditional token
removal approaches, is validated through extensive testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Experimentation of Accuracy Metrics in Automated Medical Reporting: The Case of Otitis Consultations. (arXiv:2311.13273v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.13273">
<div class="article-summary-box-inner">
<span><p>Generative Artificial Intelligence (AI) can be used to automatically generate
medical reports based on transcripts of medical consultations. The aim is to
reduce the administrative burden that healthcare professionals face. The
accuracy of the generated reports needs to be established to ensure their
correctness and usefulness. There are several metrics for measuring the
accuracy of AI generated reports, but little work has been done towards the
application of these metrics in medical reporting. A comparative
experimentation of 10 accuracy metrics has been performed on AI generated
medical reports against their corresponding General Practitioner's (GP) medical
reports concerning Otitis consultations. The number of missing, incorrect, and
additional statements of the generated reports have been correlated with the
metric scores. In addition, we introduce and define a Composite Accuracy Score
which produces a single score for comparing the metrics within the field of
automated medical reporting. Findings show that based on the correlation study
and the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics
are the preferred metrics, which is not in line with previous work. These
findings help determine the accuracy of an AI generated medical report, which
aids the development of systems that generate medical reports for GPs to reduce
the administrative burden.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Beginner to Expert: Modeling Medical Knowledge into General LLMs. (arXiv:2312.01040v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.01040">
<div class="article-summary-box-inner">
<span><p>Recently, large language model (LLM) based artificial intelligence (AI)
systems have demonstrated remarkable capabilities in natural language
understanding and generation. However, these models face a significant
challenge when it comes to sensitive applications, such as reasoning over
medical knowledge and answering medical questions in a physician-like manner.
Prior studies attempted to overcome this challenge by increasing the model size
(&gt;100B) to learn more general medical knowledge, while there is still room for
improvement in LLMs with smaller-scale model sizes (&lt;100B). In this work, we
start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a
medical beginner towards a medical expert (called AntGLM-Med-10B), which
leverages a 3-stage optimization procedure, i.e., general medical knowledge
injection, medical domain instruction tuning, and specific medical task
adaptation. Our contributions are threefold: (1) We specifically investigate
how to adapt a pre-trained general LLM in medical domain, especially for a
specific medical task. (2) We collect and construct large-scale medical
datasets for each stage of the optimization process. These datasets encompass
various data types and tasks, such as question-answering, medical reasoning,
multi-choice questions, and medical conversations. (3) Specifically for
multi-choice questions in the medical domain, we propose a novel
Verification-of-Choice approach for prompting engineering, which significantly
enhances the reasoning ability of LLMs. Remarkably, by combining the above
approaches, our AntGLM-Med-10B model can outperform the most of LLMs on
PubMedQA, including both general and medical LLMs, even when these LLMs have
larger model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Impact of Fake News on the Anticipated Outcome of the 2024 Election Ahead of Time. (arXiv:2312.03750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.03750">
<div class="article-summary-box-inner">
<span><p>Despite increasing awareness and research around fake news, there is still a
significant need for datasets that specifically target racial slurs and biases
within North American political speeches. This is particulary important in the
context of upcoming North American elections. This study introduces a
comprehensive dataset that illuminates these critical aspects of
misinformation. To develop this fake news dataset, we scraped and built a
corpus of 40,000 news articles about political discourses in North America. A
portion of this dataset (4000) was then carefully annotated, using a blend of
advanced language models and human verification methods. We have made both
these datasets openly available to the research community and have conducted
benchmarking on the annotated data to demonstrate its utility. We release the
best-performing language model along with data. We encourage researchers and
developers to make use of this dataset and contribute to this ongoing
initiative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RJUA-QA: A Comprehensive QA Dataset for Urology. (arXiv:2312.09785v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.09785">
<div class="article-summary-box-inner">
<span><p>We introduce RJUA-QA, a novel medical dataset for question answering (QA) and
reasoning with clinical evidence, contributing to bridge the gap between
general large language models (LLMs) and medical-specific LLM applications.
RJUA-QA is derived from realistic clinical scenarios and aims to facilitate
LLMs in generating reliable diagnostic and advice. The dataset contains 2,132
curated Question-Context-Answer pairs, corresponding about 25,000 diagnostic
records and clinical cases. The dataset covers 67 common urological disease
categories, where the disease coverage exceeds 97.6\% of the population seeking
medical services in urology. Each data instance in RJUA-QA comprises: (1) a
question mirroring real patient to inquiry about clinical symptoms and medical
conditions, (2) a context including comprehensive expert knowledge, serving as
a reference for medical examination and diagnosis, (3) a doctor response
offering the diagnostic conclusion and suggested examination guidance, (4) a
diagnosed clinical disease as the recommended diagnostic outcome, and (5)
clinical advice providing recommendations for medical examination. RJUA-QA is
the first medical QA dataset for clinical reasoning over the patient inquiries,
where expert-level knowledge and experience are required for yielding
diagnostic conclusions and medical examination advice. A comprehensive
evaluation is conducted to evaluate the performance of both medical-specific
and general LLMs on the RJUA-QA dataset. Our data is are publicly available at
\url{https://github.com/alipay/RJU_Ant_QA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.11193">
<div class="article-summary-box-inner">
<span><p>Most open-source generative language models currently have a context window
of no more than 4k, limiting their ability when facing long text. Even models
with longer context windows cannot guarantee satisfactory accuracy on
long-context problems. To tackle this issue, we explore from the perspective of
training data and theoretically demonstrate that improving the capability to
handle long contexts requires "effective" rather than simply "long" data. Based
on this insight, we propose using the "original text paraphrasing" task and
successfully extend the context window of existing models to 32k through a
low-cost and effective method. Our fine-tuned model achieves state-of-the-art
accuracy in multi-document-QA among models of comparable scale. The model and
training data have been made available on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.11509">
<div class="article-summary-box-inner">
<span><p>We propose a Reinforcement-Learning-based system that would automatically
prescribe a hypothetical patient medication that may help the patient with
their mental-health-related speech disfluency, and adjust the medication and
the dosages in response to zero-cost frequent measurement of the fluency of the
patient. We demonstrate the components of the system: a module that detects and
evaluates speech disfluency on a large dataset we built, and a Reinforcement
Learning algorithm that automatically finds good combinations of medications.
To support the two modules, we collect data on the effect of psychiatric
medications for speech disfluency from the literature, and build a plausible
patient simulation system. We demonstrate that the Reinforcement Learning
system is, under some circumstances, able to converge to a good medication
regime. We collect and label a dataset of people with possible speech
disfluency and demonstrate our methods using that dataset. Our work is a proof
of concept: we show that there is promise in the idea of using automatic data
collection to address disfluency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting. (arXiv:2312.11945v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.11945">
<div class="article-summary-box-inner">
<span><p>Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the
source of important words, which is crucial to edit the incomplete utterance,
and introduce words from irrelevant utterances. We propose a novel and
effective multi-task information interaction framework including context
selection, edit matrix construction, and relevance merging to capture the
multi-granularity of semantic information. Benefiting from fetching the
relevant utterance and figuring out the important words, our approach
outperforms existing state-of-the-art models on two benchmark datasets
Restoration-200K and CANAND in this field. Code will be provided on
\url{https://github.com/yanmenxue/QR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.15548">
<div class="article-summary-box-inner">
<span><p>The difficulty of the information extraction task lies in dealing with the
task-specific label schemas and heterogeneous data structures. Recent work has
proposed methods based on large language models to uniformly model different
information extraction tasks. However, these existing methods are deficient in
their information extraction capabilities for Chinese languages other than
English. In this paper, we propose an end-to-end chat-enhanced instruction
tuning framework for universal information extraction (YAYI-UIE), which
supports both Chinese and English. Specifically, we utilize dialogue data and
information extraction data to enhance the information extraction performance
jointly. Experimental results show that our proposed framework achieves
state-of-the-art performance on Chinese datasets while also achieving
comparable performance on English datasets under both supervised settings and
zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action-Item-Driven Summarization of Long Meeting Transcripts. (arXiv:2312.17581v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.17581">
<div class="article-summary-box-inner">
<span><p>The increased prevalence of online meetings has significantly enhanced the
practicality of a model that can automatically generate the summary of a given
meeting. This paper introduces a novel and effective approach to automate the
generation of meeting summaries. Current approaches to this problem generate
general and basic summaries, considering the meeting simply as a long dialogue.
However, our novel algorithms can generate abstractive meeting summaries that
are driven by the action items contained in the meeting transcript. This is
done by recursively generating summaries and employing our action-item
extraction algorithm for each section of the meeting in parallel. All of these
sectional summaries are then combined and summarized together to create a
coherent and action-item-driven summary. In addition, this paper introduces
three novel methods for dividing up long transcripts into topic-based sections
to improve the time efficiency of our algorithm, as well as to resolve the
issue of large language models (LLMs) forgetting long-term dependencies. Our
pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an
approximately 4.98% increase from the current state-of-the-art result produced
by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.00793">
<div class="article-summary-box-inner">
<span><p>With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and difficult to circumvent or optimize effectively. To
address this concern, we introduce an advanced optimization framework called
SecFormer, to achieve fast and accurate PPI for Transformer models. By
implementing model design optimization, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials, Fourier series and Goldschmidt's
method to handle other complex nonlinear functions within PPI, such as GeLU,
LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer
outperforms MPCFormer in performance, showing improvements of $5.6\%$ and
$24.2\%$ for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In
terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, demonstrating its effectiveness
and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents. (arXiv:2401.00812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.00812">
<div class="article-summary-box-inner">
<span><p>The prominent large language models (LLMs) of today differ from past language
models not only in size, but also in the fact that they are trained on a
combination of natural language and formal language (code). As a medium between
humans and computers, code translates high-level goals into executable steps,
featuring standard syntax, logical consistency, abstraction, and modularity. In
this survey, we present an overview of the various benefits of integrating code
into LLMs' training data. Specifically, beyond enhancing LLMs in code
generation, we observe that these unique properties of code help (i) unlock the
reasoning ability of LLMs, enabling their applications to a range of more
complex natural language tasks; (ii) steer LLMs to produce structured and
precise intermediate steps, which can then be connected to external execution
ends through function calls; and (iii) take advantage of code compilation and
execution environment, which also provides diverse feedback for model
improvement. In addition, we trace how these profound capabilities of LLMs,
brought by code, have led to their emergence as intelligent agents (IAs) in
situations where the ability to understand instructions, decompose goals, plan
and execute actions, and refine from feedback are crucial to their success on
downstream tasks. Finally, we present several key challenges and future
directions of empowering LLMs with code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cheetah: Natural Language Generation for 517 African Languages. (arXiv:2401.01053v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01053">
<div class="article-summary-box-inner">
<span><p>Low-resource African languages pose unique challenges for natural language
processing (NLP) tasks, including natural language generation (NLG). In this
paper, we develop Cheetah, a massively multilingual NLG language model for
African languages. Cheetah supports 517 African languages and language
varieties, allowing us to address the scarcity of NLG resources and provide a
solution to foster linguistic diversity. We demonstrate the effectiveness of
Cheetah through comprehensive evaluations across seven generation downstream
tasks. In five of the seven tasks, Cheetah significantly outperforms other
models, showcasing its remarkable performance for generating coherent and
contextually appropriate text in a wide range of African languages. We
additionally conduct a detailed human evaluation to delve deeper into the
linguistic capabilities of Cheetah. The introduction of Cheetah has
far-reaching benefits for linguistic diversity. By leveraging pretrained models
and adapting them to specific languages, our approach facilitates the
development of practical NLG applications for African communities. The findings
of this study contribute to advancing NLP research in low-resource settings,
enabling greater accessibility and inclusion for African languages in a rapidly
expanding digital landscape. We publicly release our models for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01283">
<div class="article-summary-box-inner">
<span><p>Automatic machine translation metrics often use human translations to
determine the quality of system translations. Common wisdom in the field
dictates that the human references should be of very high quality. However,
there are no cost-benefit analyses that could be used to guide practitioners
who plan to collect references for machine translation evaluation. We find that
higher-quality references lead to better metric correlations with humans at the
segment-level. Having up to 7 references per segment and taking their average
helps all metrics. Interestingly, the references from vendors of different
qualities can be mixed together and improve metric success. Higher quality
references, however, cost more to create and we frame this as an optimization
problem: given a specific budget, what references should be collected to
maximize metric success. These findings can be used by evaluators of shared
tasks when references need to be created under a certain budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01286">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01313">
<div class="article-summary-box-inner">
<span><p>As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01523">
<div class="article-summary-box-inner">
<span><p>The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and imagery. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Semi-Supervised Learning Algorithms in Text Datasets. (arXiv:2401.01843v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01843">
<div class="article-summary-box-inner">
<span><p>Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01943">
<div class="article-summary-box-inner">
<span><p>The increasing use of tools and solutions based on Large Language Models
(LLMs) for various tasks in the medical domain has become a prominent trend.
Their use in this highly critical and sensitive domain has thus raised
important questions about their robustness, especially in response to
variations in input, and the reliability of the generated outputs. This study
addresses these questions by constructing a textual dataset based on the
ICD-10-CM code descriptions, widely used in US hospitals and containing many
clinical terms, and their easily reproducible rephrasing. We then benchmarked
existing embedding models, either generalist or specialized in the clinical
domain, in a semantic search task where the goal was to correctly match the
rephrased text to the original description. Our results showed that generalist
models performed better than clinical models, suggesting that existing clinical
specialized models are more sensitive to small changes in input that confuse
them. The highlighted problem of specialized models may be due to the fact that
they have not been trained on sufficient data, and in particular on datasets
that are not diverse enough to have a reliable global language understanding,
which is still necessary for accurate handling of medical documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding LLMs: A Comprehensive Overview from Training to Inference. (arXiv:2401.02038v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02038">
<div class="article-summary-box-inner">
<span><p>The introduction of ChatGPT has led to a significant increase in the
utilization of Large Language Models (LLMs) for addressing downstream tasks.
There's an increasing focus on cost-efficient training and deployment within
this context. Low-cost training and deployment of LLMs represent the future
development trend. This paper reviews the evolution of large language model
training techniques and inference deployment technologies aligned with this
emerging trend. The discussion on training includes various aspects, including
data preprocessing, training architecture, pre-training tasks, parallel
training, and relevant content related to model fine-tuning. On the inference
side, the paper covers topics such as model compression, parallel computation,
memory scheduling, and structural optimization. It also explores LLMs'
utilization and provides insights into their future development.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2024-01-09 23:11:50.988929010 UTC">2024-01-09 23:11:50 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>