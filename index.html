<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-02-15T01:30:00Z">02-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06674">
<div class="article-summary-box-inner">
<span><p>Identifying relevant Persona or Knowledge for conversational systems is a
critical component of grounded dialogue response generation. However, each
grounding has been studied in isolation with more practical multi-context tasks
only recently introduced. We define Persona and Knowledge Dual Context
Identification as the task to identify Persona and Knowledge jointly for a
given dialogue, which could be of elevated importance in complex multi-context
Dialogue settings. We develop a novel grounding retrieval method that utilizes
all contexts of dialogue simultaneously while also requiring limited training
via zero-shot inference due to compatibility with neural Q \&amp; A retrieval
models. We further analyze the hard-negative behavior of combining Persona and
Dialogue via our novel null-positive rank test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06675">
<div class="article-summary-box-inner">
<span><p>We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. The implementation of Lion is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag of Tricks for In-Distribution Calibration of Pretrained Transformers. (arXiv:2302.06690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06690">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (PLMs) have become a de-facto standard
promoting the accuracy of text classification tasks, recent studies find that
PLMs often predict over-confidently. Although various calibration methods have
been proposed, such as ensemble learning and data augmentation, most of the
methods have been verified in computer vision benchmarks rather than in
PLM-based text classification tasks. In this paper, we present an empirical
study on confidence calibration for PLMs, addressing three categories,
including confidence penalty losses, data augmentations, and ensemble methods.
We find that the ensemble model overfitted to the training set shows sub-par
calibration performance and also observe that PLMs trained with confidence
penalty loss have a trade-off between calibration and accuracy. Building on
these observations, we propose the Calibrated PLM (CALL), a combination of
calibration techniques. The CALL complements the drawbacks that may occur when
utilizing a calibration method individually and boosts both classification and
calibration accuracy. Design choices in CALL's training procedures are
extensively studied, and we provide a detailed analysis of how calibration
techniques affect the calibration performance of PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Pretraining in Reinforcement Learning with Large Language Models. (arXiv:2302.06692v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06692">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark). (arXiv:2302.06706v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06706">
<div class="article-summary-box-inner">
<span><p>Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) how good LLMs are by themselves in
generating and validating simple plans in commonsense planning tasks (of the
type that humans are generally quite good at) and (2) how good LLMs are in
being a source of heuristic guidance for other agents--either AI planners or
human planners--in their planning tasks. To investigate these questions in a
systematic rather than anecdotal manner, we start by developing a benchmark
suite based on the kinds of domains employed in the International Planning
Competition. On this benchmark, we evaluate LLMs in three modes: autonomous,
heuristic and human-in-the-loop. Our results show that LLM's ability to
autonomously generate executable plans is quite meager, averaging only about 3%
success rate. The heuristic and human-in-the-loop modes show slightly more
promise. In addition to these results, we also make our benchmark and
evaluation tools available to support investigations by research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Model Attribution Challenge. (arXiv:2302.06716v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06716">
<div class="article-summary-box-inner">
<span><p>We present the findings of the Machine Learning Model Attribution Challenge
(\href{https://mlmac.io}{https://mlmac.io}). Fine-tuned machine learning models
may derive from other trained models without obvious attribution
characteristics. In this challenge, participants identify the
publicly-available base models that underlie a set of anonymous, fine-tuned
large language models (LLMs) using only textual output of the models.
Contestants aim to correctly attribute the most fine-tuned models, with ties
broken in the favor of contestants whose solutions use fewer calls to the
fine-tuned models' API. The most successful approaches were manual, as
participants observed similarities between model outputs and developed
attribution heuristics based on public documentation of the base models, though
several teams also submitted automated, statistical solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STREET: A Multi-Task Structured Reasoning and Explanation Benchmark. (arXiv:2302.06729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06729">
<div class="article-summary-box-inner">
<span><p>We introduce STREET, a unified multi-task and multi-domain natural language
reasoning and explanation benchmark. Unlike most existing question-answering
(QA) datasets, we expect models to not only answer questions, but also produce
step-by-step structured explanations describing how premises in the question
are used to produce intermediate conclusions that can prove the correctness of
a certain answer. We perform extensive evaluation with popular language models
such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models
still lag behind human performance when producing such structured reasoning
steps. We believe this work will provide a way for the community to better
train and test systems on multi-step reasoning and explanations in natural
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06761">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) have made significant advances in various
Natural Language Processing (NLP) domains, but it is unclear to what extent
they can infer formal semantics in ontologies, which are often used to
represent conceptual knowledge and serve as the schema of data graphs. To
investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of
inference-based probing tasks and datasets from ontology subsumption axioms
involving both atomic and complex concepts. We conduct extensive experiments on
ontologies of different domains and scales, and our results demonstrate that
LMs encode relatively less background knowledge of Subsumption Inference (SI)
than traditional Natural Language Inference (NLI) but can improve on SI
significantly when a small number of samples are given. We will open-source our
code and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Stable Entropy Hypothesis and Entropy-Aware Decoding: An Analysis and Algorithm for Robust Natural Language Generation. (arXiv:2302.06784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06784">
<div class="article-summary-box-inner">
<span><p>State-of-the-art language generation models can degenerate when applied to
open-ended generation problems such as text completion, story generation, or
dialog modeling. This degeneration usually shows up in the form of incoherence,
lack of vocabulary diversity, and self-repetition or copying from the context.
In this paper, we postulate that ``human-like'' generations usually lie in a
narrow and nearly flat entropy band, and violation of these entropy bounds
correlates with degenerate behavior. Our experiments show that this stable
narrow entropy zone exists across models, tasks, and domains and confirm the
hypothesis that violations of this zone correlate with degeneration. We then
use this insight to propose an entropy-aware decoding algorithm that respects
these entropy bounds resulting in less degenerate, more contextual, and
"human-like" language generation in open-ended text generation settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Semantic Parsing in Understanding Procedural Text. (arXiv:2302.06829v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06829">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate whether symbolic semantic representations,
extracted from deep semantic parsers, can help reasoning over the states of
involved entities in a procedural text. We consider a deep semantic
parser~(TRIPS) and semantic role labeling as two sources of semantic parsing
knowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural
reasoning framework. Second, we integrate semantic parsing information into
state-of-the-art neural models to conduct procedural reasoning. Our experiments
indicate that explicitly incorporating such semantic knowledge improves
procedural understanding. This paper presents new metrics for evaluating
procedural reasoning tasks that clarify the challenges and identify differences
among neural, symbolic, and integrated models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLIAM: Literature-based Data Synthesis for Synergistic Drug Combination Prediction. (arXiv:2302.06860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06860">
<div class="article-summary-box-inner">
<span><p>Language models pre-trained on scientific literature corpora have
substantially advanced scientific discovery by offering high-quality feature
representations for downstream applications. However, these features are often
not interpretable, and thus can reveal limited insights to domain experts.
Instead of obtaining features from language models, we propose BLIAM, a
literature-based data synthesis approach to directly generate training data
points that are interpretable and model-agnostic to downstream applications.
The key idea of BLIAM is to create prompts using existing training data and
then use these prompts to synthesize new data points. BLIAM performs these two
steps iteratively as new data points will define more informative prompts and
new prompts will in turn synthesize more accurate data points. Notably,
literature-based data augmentation might introduce data leakage since labels of
test data points in downstream applications might have already been mentioned
in the language model corpus. To prevent such leakage, we introduce GDSC-combo,
a large-scale drug combination discovery dataset that was published after the
biomedical language model was trained. We found that BLIAM substantially
outperforms a non-augmented approach and manual prompting in this rigorous data
split setting. BLIAM can be further used to synthesize data points for novel
drugs and cell lines that were not even measured in biomedical experiments. In
addition to the promising prediction performance, the data points synthesized
by BLIAM are interpretable and model-agnostic, enabling in silico augmentation
for in vitro experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains. (arXiv:2302.06868v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06868">
<div class="article-summary-box-inner">
<span><p>Prompting pre-trained language models leads to promising results across
natural language processing tasks but is less effective when applied in
low-resource domains, due to the domain gap between the pre-training data and
the downstream task. In this work, we bridge this gap with a novel and
lightweight prompting methodology called SwitchPrompt for the adaptation of
language models trained on datasets from the general domain to diverse
low-resource domains. Using domain-specific keywords with a trainable gated
prompt, SwitchPrompt offers domain-oriented prompting, that is, effective
guidance on the target domains for general-domain language models. Our few-shot
experiments on three text classification benchmarks demonstrate the efficacy of
the general-domain pre-trained language models when used with SwitchPrompt.
They often even outperform their domain-specific counterparts trained with
baseline state-of-the-art prompting methods by up to 10.7% performance increase
in accuracy. This result indicates that SwitchPrompt effectively reduces the
need for domain-specific language model pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning gain differences between ChatGPT and human tutor generated algebra hints. (arXiv:2302.06871v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06871">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to
the frontiers of practical consumer use and leading industries to re-evaluate
how they allocate resources for content production. Authoring of open
educational resources and hint content within adaptive tutoring systems is
labor intensive. Should LLMs like ChatGPT produce educational content on par
with human-authored content, the implications would be significant for further
scaling of computer tutoring system approaches. In this paper, we conduct the
first learning gain evaluation of ChatGPT by comparing the efficacy of its
hints with hints authored by human tutors with 77 participants across two
algebra topic areas, Elementary Algebra and Intermediate Algebra. We find that
70% of hints produced by ChatGPT passed our manual quality checks and that both
human and ChatGPT conditions produced positive learning gains. However, gains
were only statistically significant for human tutor created hints. Learning
gains from human-created hints were substantially and statistically
significantly higher than ChatGPT hints in both topic areas, though ChatGPT
participants in the Intermediate Algebra experiment were near ceiling and not
even with the control at pre-test. We discuss the limitations of our study and
suggest several future directions for the field. Problem and hint content used
in the experiment is provided for replicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot learning approaches for classifying low resource domain specific software requirements. (arXiv:2302.06951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06951">
<div class="article-summary-box-inner">
<span><p>With the advent of strong pre-trained natural language processing models like
BERT, DeBERTa, MiniLM, T5, the data requirement for industries to fine-tune
these models to their niche use cases has drastically reduced (typically to a
few hundred annotated samples for achieving a reasonable performance). However,
the availability of even a few hundred annotated samples may not always be
guaranteed in low resource domains like automotive, which often limits the
usage of such deep learning models in an industrial setting. In this paper we
aim to address the challenge of fine-tuning such pre-trained models with only a
few annotated samples, also known as Few-shot learning. Our experiments focus
on evaluating the performance of a diverse set of algorithms and methodologies
to achieve the task of classifying BOSCH automotive domain textual software
requirements into 3 categories, while utilizing only 15 annotated samples per
category for fine-tuning. We find that while SciBERT and DeBERTa based models
tend to be the most accurate at 15 training samples, their performance
improvement scales minimally as the number of annotated samples is increased to
50 in comparison to Siamese and T5 based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Multi-source Active Learning for Natural Language Inference. (arXiv:2302.06976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06976">
<div class="article-summary-box-inner">
<span><p>In recent years, active learning has been successfully applied to an array of
NLP tasks. However, prior work often assumes that training and test data are
drawn from the same distribution. This is problematic, as in real-life settings
data may stem from several sources of varying relevance and quality. We show
that four popular active learning schemes fail to outperform random selection
when applied to unlabelled pools comprised of multiple data sources on the task
of natural language inference. We reveal that uncertainty-based strategies
perform poorly due to the acquisition of collective outliers, i.e.,
hard-to-learn instances that hamper learning and generalization. When outliers
are removed, strategies are found to recover and outperform random baselines.
In further analysis, we find that collective outliers vary in form between
sources, and show that hard-to-learn data is not always categorically harmful.
Lastly, we leverage dataset cartography to introduce difficulty-stratified
testing and find that different strategies are affected differently by example
learnability and difficulty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. (arXiv:2302.07027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07027">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) are trained on massive corpora, but often
need to specialize to specific domains. A parameter-efficient adaptation method
suggests training an adapter for each domain on the task of language modeling.
This leads to good in-domain scores but can be impractical for domain- or
resource-restricted settings. A solution is to use a related-domain adapter for
the novel domain at test time. In this paper, we introduce AdapterSoup, an
approach that performs weight-space averaging of adapters trained on different
domains. Our approach is embarrassingly parallel: first, we train a set of
domain-specific adapters; then, for each novel domain, we determine which
adapters should be averaged at test time. We present extensive experiments
showing that AdapterSoup consistently improves performance to new domains
without extra training. We also explore weight averaging of adapters trained on
the same domain with different hyper-parameters, and show that it preserves the
performance of a PLM on new domains while obtaining strong in-domain results.
We explore various approaches for choosing which adapters to combine, such as
text clustering and semantic similarity. We find that using clustering leads to
the most competitive results on novel domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Summarization Data to Help Text Simplification. (arXiv:2302.07124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07124">
<div class="article-summary-box-inner">
<span><p>One of the major problems with text simplification is the lack of
high-quality data. The sources of simplification datasets are limited to
Wikipedia and Newsela, restricting further development of this field. In this
paper, we analyzed the similarity between text summarization and text
simplification and exploited summarization data to help simplify. First, we
proposed an alignment algorithm to extract sentence pairs from summarization
datasets. Then, we designed four attributes to characterize the degree of
simplification and proposed a method to filter suitable pairs. We named these
pairs Sum4Simp (S4S). Next, we conducted human evaluations to show that S4S is
high-quality and compared it with a real simplification dataset. Finally, we
conducted experiments to illustrate that the S4S can improve the performance of
several mainstream simplification models, especially in low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Heritage Digital Twin: a bicycle made for two. The integration of digital methodologies into cultural heritage research. (arXiv:2302.07138v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07138">
<div class="article-summary-box-inner">
<span><p>The paper concerns the definition of a novel ontology for cultural heritage
based on the concept of digital twin. The ontology, called Heritage Digital
Twin ontology, is a compatible extension of the well-known CIDOC CRM ISO
standard for cultural heritage documentation and incorporates all the different
documentation systems presently in use for cultural heritage documentation. In
the authors' view, it supports documentation interoperability at a higher level
than the ones currently in use and enables effective cooperation among
different users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Complex Event Scenarios via Simple Entity-focused Questions. (arXiv:2302.07139v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07139">
<div class="article-summary-box-inner">
<span><p>Event scenarios are often complex and involve multiple event sequences
connected through different entity participants. Exploring such complex
scenarios requires an ability to branch through different sequences, something
that is difficult to achieve with standard event language modeling. To address
this, we propose a question-guided generation framework that models events in
complex scenarios as answers to questions about participants. At any step in
the generation process, the framework uses the previously generated events as
context, but generates the next event as an answer to one of three questions:
what else a participant did, what else happened to a participant, or what else
happened. The participants and the questions themselves can be sampled or be
provided as input from a user, allowing for controllable exploration. Our
empirical evaluation shows that this question-guided generation provides better
coverage of participants, diverse events within a domain, comparable
perplexities for modeling event sequences, and more effective control for
interactive schema generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified?. (arXiv:2302.07159v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07159">
<div class="article-summary-box-inner">
<span><p>As text-to-image systems continue to grow in popularity with the general
public, questions have arisen about bias and diversity in the generated images.
Here, we investigate properties of images generated in response to prompts
which are visually under-specified, but contain salient social attributes
(e.g., 'a portrait of a threatening person' versus 'a portrait of a friendly
person'). Grounding our work in social cognition theory, we find that in many
cases, images contain similar demographic biases to those reported in the
stereotype literature. However, trends are inconsistent across different models
and further investigation is warranted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07189">
<div class="article-summary-box-inner">
<span><p>Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification; the datasets for evaluation are relatively
rare. In this work, we propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have a corresponding KB entity
by matching them to a special NIL entity. To this end, we integrate novel
techniques including NIL representation, NIL classification, and synonym
enhancement. We also propose Ontology Pruning and Versioning strategies to
construct out-of-KB mentions from normal, in-KB EL datasets. Results on four
datasets of clinical notes and publications show that BLINKout outperforms
existing methods to detect out-of-KB mentions for medical ontologies UMLS and
SNOMED CT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Psycholinguistic Analysis of BERT's Representations of Compounds. (arXiv:2302.07232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07232">
<div class="article-summary-box-inner">
<span><p>This work studies the semantic representations learned by BERT for compounds,
that is, expressions such as sunlight or bodyguard. We build on recent studies
that explore semantic information in Transformers at the word level and test
whether BERT aligns with human semantic intuitions when dealing with
expressions (e.g., sunlight) whose overall meaning depends -- to a various
extent -- on the semantics of the constituent words (sun, light). We leverage a
dataset that includes human judgments on two psycholinguistic measures of
compound semantic analysis: lexeme meaning dominance (LMD; quantifying the
weight of each constituent toward the compound meaning) and semantic
transparency (ST; evaluating the extent to which the compound meaning is
recoverable from the constituents' semantics). We show that BERT-based measures
moderately align with human intuitions, especially when using contextualized
representations, and that LMD is overall more predictable than ST. Contrary to
the results reported for 'standard' words, higher, more contextualized layers
are the best at representing compound meaning. These findings shed new light on
the abilities of BERT in dealing with fine-grained semantic phenomena.
Moreover, they can provide insights into how speakers represent compounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Captioning with Guidance of Multimodal Latent Topics. (arXiv:1708.09667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1708.09667">
<div class="article-summary-box-inner">
<span><p>The topic diversity of open-domain videos leads to various vocabularies and
linguistic expressions in describing video contents, and therefore, makes the
video captioning task even more challenging. In this paper, we propose an
unified caption framework, M&amp;M TGM, which mines multimodal topics in
unsupervised fashion from data and guides the caption decoder with these
topics. Compared to pre-defined topics, the mined multimodal topics are more
semantically and visually coherent and can reflect the topic distribution of
videos better. We formulate the topic-aware caption generation as a multi-task
learning problem, in which we add a parallel task, topic prediction, in
addition to the caption task. For the topic prediction task, we use the mined
topics as the teacher to train a student topic prediction model, which learns
to predict the latent topics from multimodal contents of videos. The topic
prediction provides intermediate supervision to the learning process. As for
the caption task, we propose a novel topic-aware decoder to generate more
accurate and detailed video descriptions with the guidance from latent topics.
The entire learning procedure is end-to-end and it optimizes both tasks
simultaneously. The results from extensive experiments conducted on the MSR-VTT
and Youtube2Text datasets demonstrate the effectiveness of our proposed model.
M&amp;M TGM not only outperforms prior state-of-the-art methods on multiple
evaluation metrics and on both benchmark datasets, but also achieves better
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLiMP: The Benchmark of Linguistic Minimal Pairs for English. (arXiv:1912.00582v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.00582">
<div class="article-summary-box-inner">
<span><p>We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP),
a challenge set for evaluating what language models (LMs) know about major
grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each
containing 1000 minimal pairs isolating specific contrasts in syntax,
morphology, or semantics. The data is automatically generated according to
expert-crafted grammars, and aggregate human agreement with the labels is
96.4%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and
Transformer-XL) LMs. We find that state-of-the-art models identify
morphological contrasts reliably, but they struggle with semantic restrictions
on the distribution of quantifiers and negative polarity items and subtle
syntactic phenomena such as extraction islands.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar-aware sentence classification on quantum computers. (arXiv:2012.03756v2 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03756">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) is at the forefront of great advances in
contemporary AI, and it is arguably one of the most challenging areas of the
field. At the same time, in the area of Quantum Computing (QC), with the steady
growth of quantum hardware and notable improvements towards implementations of
quantum algorithms, we are approaching an era when quantum computers perform
tasks that cannot be done on classical computers with a reasonable amount of
resources. This provides a new range of opportunities for AI, and for NLP
specifically. In this work, we work with the Categorical Distributional
Compositional (DisCoCat) model of natural language meaning, whose underlying
mathematical underpinnings make it amenable to quantum instantiations. Earlier
work on fault-tolerant quantum algorithms has already demonstrated potential
quantum advantage for NLP, notably employing DisCoCat. In this work, we focus
on the capabilities of noisy intermediate-scale quantum (NISQ) hardware and
perform the first implementation of an NLP task on a NISQ processor, using the
DisCoCat framework. Sentences are instantiated as parameterised quantum
circuits; word-meanings are embedded in quantum states using parameterised
quantum-circuits and the sentence's grammatical structure faithfully manifests
as a pattern of entangling operations which compose the word-circuits into a
sentence-circuit. The circuits' parameters are trained using a classical
optimiser in a supervised NLP task of binary classification. Our novel QNLP
model shows concrete promise for scalability as the quality of the quantum
hardware improves in the near future and solidifies a novel branch of
experimental research at the intersection of QC and AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridge the Gap Between CV and NLP! An Optimization-based Textual Adversarial Attack Framework. (arXiv:2110.15317v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15317">
<div class="article-summary-box-inner">
<span><p>Despite recent success on various tasks, deep learning techniques still
perform poorly on adversarial examples with small perturbations. While
optimization-based methods for adversarial attacks are well-explored in the
field of computer vision, it is impractical to directly apply them in natural
language processing due to the discrete nature of the text. To address the
problem, we propose a unified framework to extend the existing
optimization-based adversarial attack methods in the vision domain to craft
textual adversarial samples. In this framework, continuously optimized
perturbations are added to the embedding layer and amplified in the forward
propagation process. Then the final perturbed latent representations are
decoded with a masked language model head to obtain potential adversarial
samples. In this paper, we instantiate our framework with an attack algorithm
named Textual Projected Gradient Descent (T-PGD). We find our algorithm
effective even using proxy gradient information. Therefore, we perform the more
challenging transfer black-box attack and conduct comprehensive experiments to
evaluate our attack algorithm with several models on three benchmark datasets.
Experimental results demonstrate that our method achieves an overall better
performance and produces more fluent and grammatical adversarial samples
compared to strong baseline methods. All the code and data will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Explanations and Human Understanding. (arXiv:2202.04092v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04092">
<div class="article-summary-box-inner">
<span><p>Explanations are hypothesized to improve human understanding of machine
learning models and achieve a variety of desirable outcomes, ranging from model
debugging to enhancing human decision making. However, empirical studies have
found mixed and even negative results. An open question, therefore, is under
what conditions explanations can improve human understanding and in what way.
Using adapted causal diagrams, we provide a formal characterization of the
interplay between machine explanations and human understanding, and show how
human intuitions play a central role in enabling human understanding.
Specifically, we identify three core concepts of interest that cover all
existing quantitative measures of understanding in the context of human-AI
decision making: task decision boundary, model decision boundary, and model
error. Our key result is that without assumptions about task-specific
intuitions, explanations may potentially improve human understanding of model
decision boundary, but they cannot improve human understanding of task decision
boundary or model error. To achieve complementary human-AI performance, we
articulate possible ways on how explanations need to work with human
intuitions. For instance, human intuitions about the relevance of features
(e.g., education is more important than age in predicting a person's income)
can be critical in detecting model error. We validate the importance of human
intuitions in shaping the outcome of machine explanations with empirical
human-subject studies. Overall, our work provides a general framework along
with actionable implications for future algorithmic development and empirical
experiments of machine explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Language Models Plagiarize?. (arXiv:2203.07618v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07618">
<div class="article-summary-box-inner">
<span><p>Past literature has illustrated that language models (LMs) often memorize
parts of training instances and reproduce them in natural language generation
(NLG) processes. However, it is unclear to what extent LMs "reuse" a training
corpus. For instance, models can generate paraphrased sentences that are
contextually similar to training samples. In this work, therefore, we study
three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2
generated texts, in comparison to its training data, and further analyze the
plagiarism patterns of fine-tuned LMs with domain-specific corpora which are
extensively used in practice. Our results suggest that (1) three types of
plagiarism widely exist in LMs beyond memorization, (2) both size and decoding
methods of LMs are strongly associated with the degrees of plagiarism they
exhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus
similarity and homogeneity. Given that a majority of LMs' training data is
scraped from the Web without informing content owners, their reiteration of
words, phrases, and even core ideas from training sets into generated texts has
ethical implications. Their patterns are likely to exacerbate as both the size
of LMs and their training data increase, raising concerns about
indiscriminately pursuing larger models with larger training corpora.
Plagiarized content can also contain individuals' personal and sensitive
information. These findings overall cast doubt on the practicality of current
LMs in mission-critical writing tasks and urge more discussions around the
observed phenomena. Data and source code are available at
https://github.com/Brit7777/LM-plagiarism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09161">
<div class="article-summary-box-inner">
<span><p>Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state-of-the-art
task-specific models. Conventional approaches to improve model performance via
creating datasets with large number of task instances or architectural changes
in the model may not be feasible for non-expert users. However, they can write
alternate instructions to represent an instruction task. Is
Instruction-augmentation helpful? We augment a subset of tasks in the expanded
version of NATURAL INSTRUCTIONS with additional instructions and find that it
significantly improves model performance (up to 35%), especially in the
low-data regime. Our results indicate that an additional instruction can be
equivalent to ~200 data samples on average across tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QASem Parsing: Text-to-text Modeling of QA-based Semantics. (arXiv:2205.11413v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11413">
<div class="article-summary-box-inner">
<span><p>Several recent works have suggested to represent semantic relations with
questions and answers, decomposing textual information into separate
interrogative natural language statements. In this paper, we consider three
QA-based semantic tasks - namely, QA-SRL, QANom and QADiscourse, each targeting
a certain type of predication - and propose to regard them as jointly providing
a comprehensive representation of textual information. To promote this goal, we
investigate how to best utilize the power of sequence-to-sequence (seq2seq)
pre-trained language models, within the unique setup of semi-structured
outputs, consisting of an unordered set of question-answer pairs. We examine
different input and output linearization strategies, and assess the effect of
multitask learning and of simple data augmentation techniques in the setting of
imbalanced training data. Consequently, we release the first unified QASem
parsing tool, practical for downstream applications who can benefit from an
explicit, QA-based account of information units in a text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RevUp: Revise and Update Information Bottleneck for Event Representation. (arXiv:2205.12248v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12248">
<div class="article-summary-box-inner">
<span><p>The existence of external (``side'') semantic knowledge has been shown to
result in more expressive computational event models. To enable the use of side
information that may be noisy or missing, we propose a semi-supervised
information bottleneck-based discrete latent variable model. We reparameterize
the model's discrete variables with auxiliary continuous latent variables and a
light-weight hierarchical structure. Our model is learned to minimize the
mutual information between the observed data and optional side knowledge that
is not already captured by the new, auxiliary variables. We theoretically show
that our approach generalizes past approaches, and perform an empirical case
study of our approach on event modeling. We corroborate our theoretical results
with strong empirical experiments, showing that the proposed method outperforms
previous proposed approaches on multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.10264">
<div class="article-summary-box-inner">
<span><p>We introduce a new type of test, called a Turing Experiment (TE), for
evaluating how well a language model, such as GPT-3, can simulate different
aspects of human behavior. Unlike the Turing Test, which involves simulating a
single arbitrary individual, a TE requires simulating a representative sample
of participants in human subject research. We give TEs that attempt to
replicate well-established findings in prior studies. We design a methodology
for simulating TEs and illustrate its use to compare how well different
language models are able to reproduce classic economic, psycholinguistic, and
social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram
Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing
findings were replicated using recent models, while the last TE reveals a
"hyper-accuracy distortion" present in some language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers with Learnable Activation Functions. (arXiv:2208.14111v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.14111">
<div class="article-summary-box-inner">
<span><p>Activation functions can have a significant impact on reducing the
topological complexity of input data and therefore improve the performance of
the model. Selecting a suitable activation function is an essential step in
neural model design. However, the choice of activation function is seldom
discussed or explored in Transformer-based language models. Their activation
functions are chosen beforehand and then remain fixed from pre-training to
fine-tuning. As a result, the inductive biases they imposed on models cannot be
adjusted during this long life cycle. Moreover, subsequently developed models
(e.g., RoBERTa, BART, and GPT-3) often follow up prior work (e.g., BERT) to use
the same activation function without justification. In this paper, we
investigate the effectiveness of using Rational Activation Function (RAF), a
learnable activation function, in the Transformer architecture. In contrast to
conventional, predefined activation functions, RAFs can adaptively learn
optimal activation functions during training according to input data. Our
experiments show the RAF-based Transformer (RAFT) achieves a lower validation
perplexity than a vanilla BERT with the GELU function. We further evaluate RAFT
on downstream tasks in low- and full-data settings. Our results show that RAFT
outperforms the counterpart model across the majority of tasks and settings.
For instance, RAFT outperforms vanilla BERT on the GLUE benchmark by 5.71
points on average in low-data scenario (where 100 training examples are
available) and by 2.05 points on SQuAD in full-data setting. Analysis of the
shapes of learned RAFs further unveils that they substantially vary between
different layers of the pre-trained model and mostly look very different from
conventional activation functions. RAFT opens a new research direction for
analyzing and interpreting pre-trained models according to the learned
activation functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus-Driven Contrastive Learniang for Medical Question Summarization. (arXiv:2209.00484v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.00484">
<div class="article-summary-box-inner">
<span><p>Automatic medical question summarization can significantly help the system to
understand consumer health questions and retrieve correct answers. The Seq2Seq
model based on maximum likelihood estimation (MLE) has been applied in this
task, which faces two general problems: the model can not capture well question
focus and and the traditional MLE strategy lacks the ability to understand
sentence-level semantics. To alleviate these problems, we propose a novel
question focus-driven contrastive learning framework (QFCL). Specially, we
propose an easy and effective approach to generate hard negative samples based
on the question focus, and exploit contrastive learning at both encoder and
decoder to obtain better sentence level representations. On three medical
benchmark datasets, our proposed model achieves new state-of-the-art results,
and obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline
BART model on three datasets respectively. Further human judgement and detailed
analysis prove that our QFCL model learns better sentence representations with
the ability to distinguish different sentence meanings, and generates
high-quality summaries by capturing question focus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.03755">
<div class="article-summary-box-inner">
<span><p>Mis- and disinformation are a substantial global threat to our security and
safety. To cope with the scale of online misinformation, researchers have been
working on automating fact-checking by retrieving and verifying against
relevant evidence. However, despite many advances, a comprehensive evaluation
of the possible attack vectors against such systems is still lacking.
Particularly, the automated fact-verification process might be vulnerable to
the exact disinformation campaigns it is trying to combat. In this work, we
assume an adversary that automatically tampers with the online evidence in
order to disrupt the fact-checking model via camouflaging the relevant evidence
or planting a misleading one. We first propose an exploratory taxonomy that
spans these two targets and the different threat model dimensions. Guided by
this, we design and propose several potential attack methods. We show that it
is possible to subtly modify claim-salient snippets in the evidence and
generate diverse and claim-aligned evidence. Thus, we highly degrade the
fact-checking performance under many different permutations of the taxonomy's
dimensions. The attacks are also robust against post-hoc modifications of the
claim. Our analysis further hints at potential limitations in models' inference
when faced with contradicting evidence. We emphasize that these attacks can
have harmful implications on the inspectable and human-in-the-loop usage
scenarios of such models, and conclude by discussing challenges and directions
for future defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How people talk about each other: Modeling Generalized Intergroup Bias and Emotion. (arXiv:2209.06687v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06687">
<div class="article-summary-box-inner">
<span><p>Current studies of bias in NLP rely mainly on identifying (unwanted or
negative) bias towards a specific demographic group. While this has led to
progress recognizing and mitigating negative bias, and having a clear notion of
the targeted group is necessary, it is not always practical. In this work we
extrapolate to a broader notion of bias, rooted in social science and
psychology literature. We move towards predicting interpersonal group
relationship (IGR) - modeling the relationship between the speaker and the
target in an utterance - using fine-grained interpersonal emotions as an
anchor. We build and release a dataset of English tweets by US Congress members
annotated for interpersonal emotion -- the first of its kind, and 'found
supervision' for IGR labels; our analyses show that subtle emotional signals
are indicative of different biases. While humans can perform better than chance
at identifying IGR given an utterance, we show that neural models perform much
better; furthermore, a shared encoding between IGR and interpersonal perceived
emotion enabled performance gains in both tasks. Data and code for this paper
are available at https://github.com/venkatasg/interpersonal-bias
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Tuning with Special Token Adaptation. (arXiv:2210.04382v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04382">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient tuning aims at updating only a small subset of parameters
when adapting a pretrained model to downstream tasks. In this work, we
introduce PASTA, in which we only modify the special token representations
(e.g., [SEP] and [CLS] in BERT) before the self-attention module at each layer
in Transformer-based models. PASTA achieves comparable performance to full
finetuning in natural language understanding tasks including text
classification and NER with up to only 0.029% of total parameters trained. Our
work not only provides a simple yet effective way of parameter-efficient
tuning, which has a wide range of practical applications when deploying
finetuned models for multiple tasks, but also demonstrates the pivotal role of
special tokens in pretrained language models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Like a bilingual baby: The advantage of visually grounding a bilingual language model. (arXiv:2210.05487v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05487">
<div class="article-summary-box-inner">
<span><p>Unlike most neural language models, humans learn language in a rich,
multi-sensory and, often, multi-lingual environment. Current language models
typically fail to fully capture the complexities of multilingual language use.
We train an LSTM language model on images and captions in English and Spanish
from MS-COCO-ES. We find that the visual grounding improves the model's
understanding of semantic similarity both within and across languages and
improves perplexity. However, we find no significant advantage of visual
grounding for abstract words. Our results provide additional evidence of the
advantages of visually grounded language models and point to the need for more
naturalistic language data from multilingual speakers and multilingual datasets
with perceptual grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Multimodal Learning for Emergence of Graphical Sensory-Motor Communication. (arXiv:2210.06468v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06468">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate whether artificial agents can develop a shared
language in an ecological setting where communication relies on a sensory-motor
channel. To this end, we introduce the Graphical Referential Game (GREG) where
a speaker must produce a graphical utterance to name a visual referent object
while a listener has to select the corresponding object among distractor
referents, given the delivered message. The utterances are drawing images
produced using dynamical motor primitives combined with a sketching library. To
tackle GREG we present CURVES: a multimodal contrastive deep learning mechanism
that represents the energy (alignment) between named referents and utterances
generated through gradient ascent on the learned energy landscape. We
demonstrate that CURVES not only succeeds at solving the GREG but also enables
agents to self-organize a language that generalizes to feature compositions
never seen during training. In addition to evaluating the communication
performance of our approach, we also explore the structure of the emerging
language. Specifically, we show that the resulting language forms a coherent
lexicon shared between agents and that basic compositional rules on the
graphical productions could not explain the compositional generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. (arXiv:2210.08933v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08933">
<div class="article-summary-box-inner">
<span><p>Recently, diffusion models have emerged as a new paradigm for generative
models. Despite the success in domains using continuous signals such as vision
and audio, adapting diffusion models to natural language is under-explored due
to the discrete nature of texts, especially for conditional generation. We
tackle this challenge by proposing DiffuSeq: a diffusion model designed for
sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation
over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or
even better performance than six established baselines, including a
state-of-the-art model that is based on pre-trained language models. Apart from
quality, an intriguing property of DiffuSeq is its high diversity during
generation, which is desired in many Seq2Seq tasks. We further include a
theoretical analysis revealing the connection between DiffuSeq and
autoregressive/non-autoregressive models. Bringing together theoretical
analysis and empirical evidence, we demonstrate the great potential of
diffusion models in complex conditional language generation tasks. Code is
available at \url{https://github.com/Shark-NLP/DiffuSeq}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Search Is What You Need For Neural Text Generation. (arXiv:2210.14140v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14140">
<div class="article-summary-box-inner">
<span><p>Generating text with autoregressive language models (LMs) is of great
importance to many natural language processing (NLP) applications. Previous
solutions for this task often produce text that contains degenerative
expressions or lacks semantic consistency. Recently, Su et al. introduced a new
decoding method, contrastive search, based on the isotropic representation
space of the language model and obtained new state of the art on various
benchmarks. Additionally, Su et al. argued that the representations of
autoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also
shared by previous studies. Therefore, to ensure the language model follows an
isotropic distribution, Su et al. proposed a contrastive learning scheme,
SimCTG, which calibrates the language model's representations through
additional training.
</p>
<p>In this study, we first answer the question: "Are autoregressive LMs really
anisotropic?". To this end, we extensively evaluate the isotropy of LMs across
16 major languages. Surprisingly, we find that the anisotropic problem only
exists in the two specific English GPT-2-small/medium models. On the other
hand, all other evaluated LMs are naturally isotropic which is in contrast to
the conclusion drawn by previous studies. Based on our findings, we further
assess the contrastive search decoding method using off-the-shelf LMs on four
generation tasks across 16 languages. Our experimental results demonstrate that
contrastive search significantly outperforms previous decoding methods without
any additional training. More notably, on 12 out of the 16 evaluated languages,
contrastive search performs comparably with human-level performances as judged
by human evaluations. Our code and other related resources are publicly
available at https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17406">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have been reported to have strong performance on
natural language processing tasks. However, performance metrics such as
accuracy do not measure the quality of the model in terms of its ability to
robustly represent complex linguistic structure. In this paper, focusing on the
ability of language models to represent syntax, we propose a framework to
assess the consistency and robustness of linguistic representations. To this
end, we introduce measures of robustness of neural network models that leverage
recent advances in extracting linguistic constructs from LLMs via probing
tasks, i.e., simple tasks used to extract meaningful information about a single
facet of a language model, such as syntax reconstruction and root
identification. Empirically, we study the performance of four LLMs across six
different corpora on the proposed robustness measures by analysing their
performance and robustness with respect to syntax-preserving perturbations. We
provide evidence that context-free representation (e.g., GloVe) are in some
cases competitive with context-dependent representations from modern LLMs
(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key
observation is that emergent syntactic representations in neural networks are
brittle. We make the code, trained models and logs available to the community
as a contribution to the debate about the capabilities of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2022 n2c2/UW Shared Task on Extracting Social Determinants of Health. (arXiv:2301.05571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05571">
<div class="article-summary-box-inner">
<span><p>Objective: The n2c2/UW SDOH Challenge explores the extraction of social
determinant of health (SDOH) information from clinical notes. The objectives
include the advancement of natural language processing (NLP) information
extraction techniques for SDOH and clinical information more broadly. This
paper presents the shared task, data, participating teams, performance results,
and considerations for future work.
</p>
<p>Materials and Methods: The task used the Social History Annotated Corpus
(SHAC), which consists of clinical text with detailed event-based annotations
for SDOH events such as alcohol, drug, tobacco, employment, and living
situation. Each SDOH event is characterized through attributes related to
status, extent, and temporality. The task includes three subtasks related to
information extraction (Subtask A), generalizability (Subtask B), and learning
transfer (Subtask C). In addressing this task, participants utilized a range of
techniques, including rules, knowledge bases, n-grams, word embeddings, and
pretrained language models (LM).
</p>
<p>Results: A total of 15 teams participated, and the top teams utilized
pretrained deep learning LM. The top team across all subtasks used a
sequence-to-sequence approach achieving 0.901 F1 for Subtask A, 0.774 F1
Subtask B, and 0.889 F1 for Subtask C.
</p>
<p>Conclusions: Similar to many NLP tasks and domains, pretrained LM yielded the
best performance, including generalizability and learning transfer. An error
analysis indicates extraction performance varies by SDOH, with lower
performance achieved for conditions, like substance use and homelessness, that
increase health risks (risk factors) and higher performance achieved for
conditions, like substance abstinence and living with family, that reduce
health risks (protective factors).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. (arXiv:2301.13688v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13688">
<div class="article-summary-box-inner">
<span><p>We study the design decisions of publicly available instruction tuning
methods, and break down the development of Flan 2022 (Chung et al., 2022).
Through careful ablation studies on the Flan Collection of tasks and methods,
we tease apart the effect of design decisions which enable Flan-T5 to
outperform prior work by 3-17%+ across evaluation settings. We find task
balancing and enrichment techniques are overlooked but critical to effective
instruction tuning, and in particular, training with mixed prompt settings
(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)
performance in all settings. In further experiments, we show Flan-T5 requires
less finetuning to converge higher and faster than T5 on single downstream
tasks, motivating instruction-tuned models as more computationally-efficient
starting checkpoints for new tasks. Finally, to accelerate research on
instruction tuning, we make the Flan 2022 collection of datasets, templates,
and methods publicly available at
https://github.com/google-research/FLAN/tree/main/flan/v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13816">
<div class="article-summary-box-inner">
<span><p>The utilization of programming language (PL) models, pretrained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting specific sequence-level
features of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that combines pretrained PL
models with Proximal Policy Optimization (PPO) deep reinforcement learning and
employs execution feedback as the external source of knowledge into the model
optimization. PPOCoder is transferable across different code generation tasks
and PLs. Extensive experiments on three code generation tasks demonstrate the
effectiveness of our proposed approach compared to SOTA methods, improving the
success rate of compilation and functional correctness over different PLs. Our
code can be found at https://github.com/reddy-lab-code-research/PPOCoder .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Easily Distracted by Irrelevant Context. (arXiv:2302.00093v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.00093">
<div class="article-summary-box-inner">
<span><p>Large language models have achieved impressive performance on various natural
language processing tasks. However, so far they have been evaluated primarily
on benchmarks where all information in the input context is relevant for
solving the task. In this work, we investigate the distractibility of large
language models, i.e., how the model problem-solving accuracy can be influenced
by irrelevant context. In particular, we introduce Grade-School Math with
Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant
information in the problem description. We use this benchmark to measure the
distractibility of cutting-edge prompting techniques for large language models,
and find that the model performance is dramatically decreased when irrelevant
information is included. We also identify several approaches for mitigating
this deficiency, such as decoding with self-consistency and adding to the
prompt an instruction that tells the language model to ignore the irrelevant
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Languages are Rewards: Chain of Hindsight Finetuning using Human Feedback. (arXiv:2302.02676v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02676">
<div class="article-summary-box-inner">
<span><p>Learning from human preferences is important for language models to be
helpful and useful for humans, and to align with human and social values.
Existing works focus on supervised finetuning of pretrained models, based on
curated model generations that are preferred by human labelers. Such works have
achieved remarkable successes in understanding and following instructions
(e.g., InstructGPT, ChatGPT, etc). However, to date, a key limitation of
supervised finetuning is that it cannot learn from negative ratings; models are
only trained on positive-rated data, which makes it data inefficient. Because
collecting human feedback data is both time consuming and expensive, it is
vital for the model to learn from all feedback, akin to the remarkable ability
of humans to learn from diverse feedback. In this work, we propose a novel
technique called Hindsight Finetuning for making language models learn from
diverse human feedback. In fact, our idea is motivated by how humans learn from
hindsight experience. We condition the model on a sequence of model generations
paired with hindsight feedback, and finetune the model to predict the most
preferred output. By doing so, models can learn to identify and correct
negative attributes or errors. Applying the method to GPT-J, we observe that it
significantly improves results on summarization and dialogue tasks using the
same amount of human feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03494">
<div class="article-summary-box-inner">
<span><p>Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Ten categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld. (arXiv:2302.05244v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05244">
<div class="article-summary-box-inner">
<span><p>Building open-ended agents that can autonomously discover a diversity of
behaviours is one of the long-standing goals of artificial intelligence. This
challenge can be studied in the framework of autotelic RL agents, i.e. agents
that learn by selecting and pursuing their own goals, self-organizing a
learning curriculum. Recent work identified language has a key dimension of
autotelic learning, in particular because it enables abstract goal sampling and
guidance from social peers for hindsight relabelling. Within this perspective,
we study the following open scientific questions: What is the impact of
hindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can
the agent learn from very rare language goal examples in its experience replay?
How can multiple forms of exploration be combined, and take advantage of easier
goals as stepping stones to reach harder ones? To address these questions, we
use ScienceWorld, a textual environment with rich abstract and combinatorial
physics. We show the importance of selectivity from the social peer's feedback;
that experience replay needs to over-sample examples of rare goals; and that
following self-generated goal sequences where the agent's competence is
intermediate leads to significant improvements in final performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3. (arXiv:2302.05729v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05729">
<div class="article-summary-box-inner">
<span><p>LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art
language model GPT-3, fine-tuned for the legal domain. The system is designed
to provide legal assistance to users in a conversational manner, helping them
with tasks such as answering legal questions, generating legal documents, and
providing legal advice. In this paper, we provide a brief overview of LawGPT
1.0, its architecture, and its performance on a set of legal benchmark tasks.
Please note that the detailed information about the model is protected by a
non-disclosure agreement (NDA) and cannot be disclosed in this report.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NYCU-TWO at Memotion 3: Good Foundation, Good Teacher, then you have Good Meme Analysis. (arXiv:2302.06078v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06078">
<div class="article-summary-box-inner">
<span><p>This paper presents a robust solution to the Memotion 3.0 Shared Task. The
goal of this task is to classify the emotion and the corresponding intensity
expressed by memes, which are usually in the form of images with short captions
on social media. Understanding the multi-modal features of the given memes will
be the key to solving the task. In this work, we use CLIP to extract aligned
image-text features and propose a novel meme sentiment analysis framework,
consisting of a Cooperative Teaching Model (CTM) for Task A and a Cascaded
Emotion Classifier (CEC) for Tasks B&amp;C. CTM is based on the idea of knowledge
distillation, and can better predict the sentiment of a given meme in Task A;
CEC can leverage the emotion intensity suggestion from the prediction of Task C
to classify the emotion more precisely in Task B. Experiments show that we
achieved the 2nd place ranking for both Task A and Task B and the 4th place
ranking for Task C, with weighted F1-scores of 0.342, 0.784, and 0.535
respectively. The results show the robustness and effectiveness of our
framework. Our code is released at github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Noisy Crowd Labels with Logics. (arXiv:2302.06337v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06337">
<div class="article-summary-box-inner">
<span><p>This paper explores the integration of symbolic logic knowledge into deep
neural networks for learning from noisy crowd labels. We introduce Logic-guided
Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic
knowledge distillation framework that learns from both noisy labeled data and
logic rules of interest. Unlike traditional EM methods, our framework contains
a ``pseudo-E-step'' that distills from the logic rules a new type of learning
target, which is then used in the ``pseudo-M-step'' for training the
classifier. Extensive evaluations on two real-world datasets for text sentiment
classification and named entity recognition demonstrate that the proposed
framework improves the state-of-the-art and provides a new solution to learning
from noisy crowd labels.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-02-15 23:13:15.788521846 UTC">2023-02-15 23:13:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>