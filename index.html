<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-11T01:30:00Z">10-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualize Before You Write: Imagination-Guided Open-Ended Text Generation. (arXiv:2210.03765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03765">
<div class="article-summary-box-inner">
<span><p>Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models (LM) in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in few-shot scenarios. Both automatic metrics and human evaluations
verify that the text snippets generated by our iNLG are coherent and
informative while displaying minor degeneration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings. (arXiv:2210.03766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03766">
<div class="article-summary-box-inner">
<span><p>Federated learning is a training paradigm that learns from multiple
distributed users without aggregating data on a centralized server. Such a
paradigm promises the ability to deploy machine-learning at-scale to a diverse
population of end-users without first collecting a large, labeled dataset for
all possible tasks. As federated learning typically averages learning updates
across a decentralized population, there is a growing need for personalization
of federated learning systems (i.e conversational agents must be able to
personalize to a specific user's preferences). In this work, we propose a new
direction for personalization research within federated learning, leveraging
both personal embeddings and shared context embeddings. We also present an
approach to predict these ``preference'' embeddings, enabling personalization
without backpropagation. Compared to state-of-the-art personalization
baselines, our approach achieves a 50\% improvement in test-time perplexity
using 0.001\% of the memory required by baseline approaches, and achieving
greater sample- and compute-efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph. (arXiv:2210.03768v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03768">
<div class="article-summary-box-inner">
<span><p>Translating natural language queries (NLQ) into structured query language
(SQL) in interfaces to relational databases is a challenging task that has been
widely studied by researchers from both the database and natural language
processing communities. Numerous works have been proposed to attack the natural
language interfaces to databases (NLIDB) problem either as a conventional
pipeline-based or an end-to-end deep-learning-based solution. Nevertheless,
regardless of the approach preferred, such solutions exhibit black-box nature,
which makes it difficult for potential users targeted by these systems to
comprehend the decisions made to produce the translated SQL. To this end, we
propose xDBTagger, an explainable hybrid translation pipeline that explains the
decisions made along the way to the user both textually and visually. We also
evaluate xDBTagger quantitatively in three real-world relational databases. The
evaluation results indicate that in addition to being fully interpretable,
xDBTagger is effective in terms of accuracy and translates the queries more
efficiently compared to other state-of-the-art pipeline-based systems up to
10000 times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts. (arXiv:2210.03797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03797">
<div class="article-summary-box-inner">
<span><p>Recent progress in language model pre-training has led to important
improvements in Named Entity Recognition (NER). Nonetheless, this progress has
been mainly tested in well-formatted documents such as news, Wikipedia, or
scientific articles. In social media the landscape is different, in which it
adds another layer of complexity due to its noisy and dynamic nature. In this
paper, we focus on NER in Twitter, one of the largest social media platforms,
and construct a new NER dataset, TweetNER7, which contains seven entity types
annotated over 11,382 tweets from September 2019 to August 2021. The dataset
was constructed by carefully distributing the tweets over time and taking
representative trends as a basis. Along with the dataset, we provide a set of
language model baselines and perform an analysis on the language model
performance on the task, especially analyzing the impact of different time
periods. In particular, we focus on three important temporal aspects in our
analysis: short-term degradation of NER models over time, strategies to
fine-tune a language model over different periods, and self-labeling as an
alternative to lack of recently-labeled data. TweetNER7 is released publicly
(https://huggingface.co/datasets/tner/tweetner7) along with the models
fine-tuned on it (NER models have been integrated into TweetNLP and can be
found athttps://github.com/asahi417/tner/tree/master/examples/tweetner7_paper).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmented Visual Question Answering with Outside Knowledge. (arXiv:2210.03809v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03809">
<div class="article-summary-box-inner">
<span><p>Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA
task that requires retrieval of external knowledge to answer questions about
images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve
documents from external knowledge bases, such as Wikipedia, but with DPR
trained separately from answer generation, introducing a potential limit on the
overall system performance. Instead, we propose a joint training scheme which
includes differentiable DPR integrated with answer generation so that the
system can be trained in an end-to-end fashion. Our experiments show that our
scheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also
introduce new diagnostic metrics to analyze how retrieval and generation
interact. The strong retrieval ability of our model significantly reduces the
number of retrieved documents needed in training, yielding significant benefits
in answer quality and computation required for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of the Effects of Decoding Algorithms on Fairness in Open-Ended Language Generation. (arXiv:2210.03826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03826">
<div class="article-summary-box-inner">
<span><p>Several prior works have shown that language models (LMs) can generate text
containing harmful social biases and stereotypes. While decoding algorithms
play a central role in determining properties of LM generated text, their
impact on the fairness of the generations has not been studied. We present a
systematic analysis of the impact of decoding algorithms on LM fairness, and
analyze the trade-off between fairness, diversity and quality. Our experiments
with top-$p$, top-$k$ and temperature decoding algorithms, in open-ended
language generation, show that fairness across demographic groups changes
significantly with change in decoding algorithm's hyper-parameters. Notably,
decoding algorithms that output more diverse text also output more texts with
negative sentiment and regard. We present several findings and provide
recommendations on standardized reporting of decoding details in fairness
evaluations and optimization of decoding algorithms for fairness alongside
quality and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking BERT: Evaluating and Optimizing Sparsified Attention. (arXiv:2210.03841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03841">
<div class="article-summary-box-inner">
<span><p>Transformers allow attention between all pairs of tokens, but there is reason
to believe that most of these connections - and their quadratic time and memory
- may not be necessary. But which ones? We evaluate the impact of
sparsification patterns with a series of ablation experiments. First, we
compare masks based on syntax, lexical similarity, and token position to random
connections, and measure which patterns reduce performance the least. We find
that on three common finetuning tasks even using attention that is at least 78%
sparse can have little effect on performance if applied at later transformer
layers, but that applying sparsity throughout the network reduces performance
significantly. Second, we vary the degree of sparsity for three patterns
supported by previous work, and find that connections to neighbouring tokens
are the most significant. Finally, we treat sparsity as an optimizable
parameter, and present an algorithm to learn degrees of neighboring connections
that gives a fine-grained control over the accuracy-sparsity trade-off while
approaching the performance of existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. (arXiv:2210.03849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03849">
<div class="article-summary-box-inner">
<span><p>With the recent advance in large pre-trained language models, researchers
have achieved record performances in NLP tasks that mostly focus on language
pattern matching. The community is experiencing the shift of the challenge from
how to model language to the imitation of complex reasoning abilities like
human beings. In this work, we investigate the application domain of finance
that involves real-world, complex numerical reasoning. We propose a new
large-scale dataset, ConvFinQA, aiming to study the chain of numerical
reasoning in conversational question answering. Our dataset poses great
challenge in modeling long-range, complex numerical reasoning paths in
real-world conversations. We conduct comprehensive experiments and analyses
with both the neural symbolic methods and the prompting-based methods, to
provide insights into the reasoning mechanisms of these two divisions. We
believe our new dataset should serve as a valuable resource to push forward the
exploration of real-world, complex reasoning tasks as the next research focus.
Our dataset and code is publicly available at
https://github.com/czyssrs/ConvFinQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models. (arXiv:2210.03858v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03858">
<div class="article-summary-box-inner">
<span><p>There are growing interests in adapting large-scale language models using
parameter-efficient fine-tuning methods. However, accelerating the model itself
and achieving better inference efficiency through model compression has not
been thoroughly explored yet. Model compression could provide the benefits of
reducing memory footprints, enabling low-precision computations, and ultimately
achieving cost-effective inference. To combine parameter-efficient adaptation
and model compression, we propose AlphaTuning consisting of post-training
quantization of the pre-trained language model and fine-tuning only some parts
of quantized parameters for a target task. Specifically, AlphaTuning works by
employing binary-coding quantization, which factorizes the full-precision
parameters into binary parameters and a separate set of scaling factors. During
the adaptation phase, the binary values are frozen for all tasks, while the
scaling factors are fine-tuned for the downstream task. We demonstrate that
AlphaTuning, when applied to GPT-2 and OPT, performs competitively with full
fine-tuning on a variety of downstream tasks while achieving &gt;10x compression
ratio under 4-bit quantization and &gt;1,000x reduction in the number of trainable
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models. (arXiv:2210.03871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03871">
<div class="article-summary-box-inner">
<span><p>Multi-task learning (MTL), instruction tuning, and prompting have recently
been shown to improve the generalizability of large language models to new
tasks. However, the benefits of such methods are less well-documented in
smaller language models, with some studies finding contradictory results. In
this work, we explore and isolate the effects of (i) model size, (ii) general
purpose MTL, (iii) in-domain MTL, (iv) instruction tuning, and (v) few-shot
fine-tuning for models with fewer than 500 million parameters. Our experiments
in the zero-shot setting demonstrate that models gain 31% relative improvement,
on average, from general purpose MTL, with an additional 37.6% relative gain
from in-domain MTL. Contradictory to prior works on large models, we find that
instruction tuning provides a modest 2% performance improvement for small
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness. (arXiv:2210.03884v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03884">
<div class="article-summary-box-inner">
<span><p>As a critical step to achieve human-like chatbots, empathetic response
generation has attained increasing interests. Previous attempts are incomplete
and not sufficient enough to elicit empathy because they only focus on the
initial aspect of empathy to automatically mimic the feelings and thoughts of
the user via other-awareness. However, they ignore to maintain and take the own
views of the system into account, which is a crucial process to achieve the
empathy called self-other awareness. To this end, we propose to generate
Empathetic response with explicit Self-Other Awareness (EmpSOA). Specifically,
three stages, self-other differentiation, self-other modulation and self-other
generation, are devised to clearly maintain, regulate and inject the self-other
aware information into the process of empathetic response generation. Both
automatic and human evaluations on the benchmark dataset demonstrate the
superiority of EmpSOA to generate more empathetic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving End-to-End Text Image Translation From the Auxiliary Text Translation Task. (arXiv:2210.03887v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03887">
<div class="article-summary-box-inner">
<span><p>End-to-end text image translation (TIT), which aims at translating the source
language embedded in images to the target language, has attracted intensive
attention in recent research. However, data sparsity limits the performance of
end-to-end text image translation. Multi-task learning is a non-trivial way to
alleviate this problem via exploring knowledge from complementary related
tasks. In this paper, we propose a novel text translation enhanced text image
translation, which trains the end-to-end model with text translation as an
auxiliary task. By sharing model parameters and multi-task training, our model
is able to take full advantage of easily-available large-scale text parallel
corpus. Extensive experimental results show our proposed method outperforms
existing end-to-end methods, and the joint multi-task learning with both text
translation and recognition tasks achieves better results, proving translation
and recognition auxiliary tasks are complementary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding. (arXiv:2210.03915v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03915">
<div class="article-summary-box-inner">
<span><p>E-commerce query understanding is the process of inferring the shopping
intent of customers by extracting semantic meaning from their search queries.
The recent progress of pre-trained masked language models (MLM) in natural
language processing is extremely attractive for developing effective query
understanding models. Specifically, MLM learns contextual text embedding via
recovering the masked tokens in the sentences. Such a pre-training process
relies on the sufficient contextual information. It is, however, less effective
for search queries, which are usually short text. When applying masking to
short search queries, most contextual information is lost and the intent of the
search queries may be changed. To mitigate the above issues for MLM
pre-training on search queries, we propose a novel pre-training task
specifically designed for short text, called Extended Token Classification
(ETC). Instead of masking the input text, our approach extends the input by
inserting tokens via a generator network, and trains a discriminator to
identify which tokens are inserted in the extended input. We conduct
experiments in an E-commerce store to demonstrate the effectiveness of ETC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Label Errors in Token Classification Data. (arXiv:2210.03920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03920">
<div class="article-summary-box-inner">
<span><p>Mislabeled examples are a common issue in real-world data, particularly for
tasks like token classification where many labels must be chosen on a
fine-grained basis. Here we consider the task of finding sentences that contain
label errors in token classification datasets. We study 11 different
straightforward methods that score tokens/sentences based on the predicted
class probabilities output by a (any) token classification model (trained via
any procedure). In precision-recall evaluations based on real-world label
errors in entity recognition data from CoNLL-2003, we identify a simple and
effective method that consistently detects those sentences containing label
errors when applied with different token classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Teachers Can Be Dense with Knowledge. (arXiv:2210.03923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03923">
<div class="article-summary-box-inner">
<span><p>Recent advances in distilling pretrained language models have discovered
that, besides the expressiveness of knowledge, the student-friendliness should
be taken into consideration to realize a truly knowledgable teacher. Based on a
pilot study, we find that over-parameterized teachers can produce expressive
yet student-unfriendly knowledge, and are thus limited in overall
knowledgableness. To remove the parameters that result in
student-unfriendliness, we propose a sparse teacher trick under the guidance of
an overall knowledgable score for each teacher parameter. The knowledgable
score is essentially an interpolation of the expressiveness and
student-friendliness scores. The aim is to ensure that the expressive
parameters are retained while the student-unfriendly ones are removed.
Extensive experiments on the GLUE benchmark show that the proposed sparse
teachers can be dense with knowledge and lead to students with compelling
performance in comparison with a series of competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EgoTaskQA: Understanding Human Tasks in Egocentric Videos. (arXiv:2210.03929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03929">
<div class="article-summary-box-inner">
<span><p>Understanding human tasks through video observations is an essential
capability of intelligent agents. The challenges of such capability lie in the
difficulty of generating a detailed understanding of situated actions, their
effects on object states (i.e., state changes), and their causal dependencies.
These challenges are further aggravated by the natural parallelism from
multi-tasking and partial observations in multi-agent collaboration. Most prior
works leverage action localization or future prediction as an indirect metric
for evaluating such task understanding from videos. To make a direct
evaluation, we introduce the EgoTaskQA benchmark that provides a single home
for the crucial dimensions of task understanding through question-answering on
real-world egocentric videos. We meticulously design questions that target the
understanding of (1) action dependencies and effects, (2) intents and goals,
and (3) agents' beliefs about others. These questions are divided into four
types, including descriptive (what status?), predictive (what will?),
explanatory (what caused?), and counterfactual (what if?) to provide diagnostic
analyses on spatial, temporal, and causal understandings of goal-oriented
tasks. We evaluate state-of-the-art video reasoning models on our benchmark and
show their significant gaps between humans in understanding complex
goal-oriented egocentric videos. We hope this effort will drive the vision
community to move onward with goal-oriented video understanding and reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling. (arXiv:2210.03941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03941">
<div class="article-summary-box-inner">
<span><p>While recent large-scale video-language pre-training made great progress in
video question answering, the design of spatial modeling of video-language
models is less fine-grained than that of image-language models; existing
practices of temporal modeling also suffer from weak and noisy alignment
between modalities. To learn fine-grained visual understanding, we decouple
spatial-temporal modeling and propose a hybrid pipeline, Decoupled
Spatial-Temporal Encoders, integrating an image- and a video-language encoder.
The former encodes spatial semantics from larger but sparsely sampled frames
independently of time, while the latter models temporal dynamics at lower
spatial but higher temporal resolution. To help the video-language model learn
temporal relations for video QA, we propose a novel pre-training objective,
Temporal Referring Modeling, which requires the model to identify temporal
positions of events in video sequences. Extensive experiments demonstrate that
our model outperforms previous work pre-trained on orders of magnitude larger
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConstGCN: Constrained Transmission-based Graph Convolutional Networks for Document-level Relation Extraction. (arXiv:2210.03949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03949">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction with graph neural networks faces a
fundamental graph construction gap between training and inference - the golden
graph structure only available during training, which causes that most methods
adopt heuristic or syntactic rules to construct a prior graph as a pseudo
proxy. In this paper, we propose $\textbf{ConstGCN}$, a novel graph
convolutional network which performs knowledge-based information propagation
between entities along with all specific relation spaces without any prior
graph construction. Specifically, it updates the entity representation by
aggregating information from all other entities along with each relation space,
thus modeling the relation-aware spatial information. To control the
information flow passing through the indeterminate relation spaces, we propose
to constrain the propagation using transmitting scores learned from the Noise
Contrastive Estimation between fact triples. Experimental results show that our
method outperforms the previous state-of-the-art (SOTA) approaches on the DocRE
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation. (arXiv:2210.03953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03953">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive translation (NAT) models are typically trained with the
cross-entropy loss, which forces the model outputs to be aligned verbatim with
the target sentence and will highly penalize small shifts in word positions.
Latent alignment models relax the explicit alignment by marginalizing out all
monotonic latent alignments with the CTC loss. However, they cannot handle
non-monotonic alignments, which is non-negligible as there is typically global
word reordering in machine translation. In this work, we explore non-monotonic
latent alignments for NAT. We extend the alignment space to non-monotonic
alignments to allow for the global word reordering and further consider all
alignments that overlap with the target sentence. We non-monotonically match
the alignments to the target sentence and train the latent alignment model to
maximize the F1 score of non-monotonic matching. Extensive experiments on major
WMT benchmarks show that our method substantially improves the translation
performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14
En-De with only one-iteration decoding, closing the gap between
non-autoregressive and autoregressive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning. (arXiv:2210.03963v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03963">
<div class="article-summary-box-inner">
<span><p>Contrastive learning methods achieve state-of-the-art results in unsupervised
sentence representation learning. Although playing essential roles in
contrastive learning, data augmentation methods applied on sentences have not
been fully explored. Current SOTA method SimCSE utilizes a simple dropout
mechanism as continuous augmentation which outperforms discrete augmentations
such as cropping, word deletion and synonym replacement. To understand the
underlying rationales, we revisit existing approaches and attempt to
hypothesize the desiderata of reasonable data augmentation methods: balance of
semantic consistency and expression diversity. Based on the hypothesis, we
propose three simple yet effective discrete sentence augmentation methods,
i.e., punctuation insertion, affirmative auxiliary and double negation. The
punctuation marks, auxiliaries and negative words act as minimal noises in
lexical level to produce diverse sentence expressions. Unlike traditional
augmentation methods which randomly modify the sentence, our augmentation rules
are well designed for generating semantically consistent and grammatically
correct sentences. We conduct extensive experiments on both English and Chinese
semantic textual similarity datasets. The results show the robustness and
effectiveness of the proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification. (arXiv:2210.03970v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03970">
<div class="article-summary-box-inner">
<span><p>Medical text learning has recently emerged as a promising area to improve
healthcare due to the wide adoption of electronic health record (EHR) systems.
The complexity of the medical text such as diverse length, mixed text types,
and full of medical jargon, poses a great challenge for developing effective
deep learning models. BERT has presented state-of-the-art results in many NLP
tasks, such as text classification and question answering. However, the
standalone BERT model cannot deal with the complexity of the medical text,
especially the lengthy clinical notes. Herein, we develop a new model called
KG-MTT-BERT (Knowledge Graph Enhanced Multi-Type Text BERT) by extending the
BERT model for long and multi-type text with the integration of the medical
knowledge graph. Our model can outperform all baselines and other
state-of-the-art models in diagnosis-related group (DRG) classification, which
requires comprehensive medical text for accurate classification. We also
demonstrated that our model can effectively handle multi-type text and the
integration of medical knowledge graph can significantly improve the
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition. (arXiv:2210.03980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03980">
<div class="article-summary-box-inner">
<span><p>Continual Learning for Named Entity Recognition (CL-NER) aims to learn a
growing number of entity types over time from a stream of data. However, simply
learning Other-Class in the same way as new entity types amplifies the
catastrophic forgetting and leads to a substantial performance drop. The main
cause behind this is that Other-Class samples usually contain old entity types,
and the old knowledge in these Other-Class samples is not preserved properly.
Thanks to the causal inference, we identify that the forgetting is caused by
the missing causal effect from the old data. To this end, we propose a unified
causal framework to retrieve the causality from both new entity types and
Other-Class. Furthermore, we apply curriculum learning to mitigate the impact
of label noise and introduce a self-adaptive weight for balancing the causal
effects between new entity types and Other-Class. Experimental results on three
benchmark datasets show that our method outperforms the state-of-the-art method
by a large margin. Moreover, our method can be combined with the existing
state-of-the-art methods to improve the performance in CL-NER
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bird-Eye Transformers for Text Generation Models. (arXiv:2210.03985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03985">
<div class="article-summary-box-inner">
<span><p>Transformers have become an indispensable module for text generation models
since their great success in machine translation. Previous works attribute
the~success of transformers to the query-key-value dot-product attention, which
provides a robust inductive bias by the fully connected token graphs. However,
we found that self-attention has a severe limitation. When predicting the
(i+1)-th token, self-attention only takes the i-th token as an information
collector, and it tends to give a high attention weight to those tokens similar
to itself. Therefore, most of the historical information that occurred before
the i-th token is not taken into consideration. Based on this observation, in
this paper, we propose a new architecture, called bird-eye transformer(BET),
which goes one step further to improve the performance of transformers by
reweighting self-attention to encourage it to focus more on important
historical information. We have conducted experiments on multiple text
generation tasks, including machine translation (2 datasets) and language
models (3 datasets). These experimental~results show that our proposed model
achieves a better performance than the baseline transformer architectures
on~all~datasets. The code is released at:
\url{https://sites.google.com/view/bet-transformer/home}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Language Models for Paragraph-Level Question Generation. (arXiv:2210.03992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03992">
<div class="article-summary-box-inner">
<span><p>Powerful generative models have led to recent progress in question generation
(QG). However, it is difficult to measure advances in QG research since there
are no standardized resources that allow a uniform comparison among approaches.
In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark
for QG that unifies existing question answering datasets by converting them to
a standard QG setting. It includes general-purpose datasets such as SQuAD for
English, datasets from ten domains and two styles, as well as datasets in eight
different languages. Using QG-Bench as a reference, we perform an extensive
analysis of the capabilities of language models for the task. First, we propose
robust QG baselines based on fine-tuning generative language models. Then, we
complement automatic evaluation based on standard metrics with an extensive
manual evaluation, which in turn sheds light on the difficulty of evaluating QG
models. Finally, we analyse both the domain adaptability of these models as
well as the effectiveness of multilingual models in languages other than
English. QG-Bench is released along with the fine-tuned models presented in the
paper https://github.com/asahi417/lm-question-generation, which are also
available as a demo https://autoqg.net/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation. (arXiv:2210.03999v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03999">
<div class="article-summary-box-inner">
<span><p>Recently, a new training oaxe loss has proven effective to ameliorate the
effect of multimodality for non-autoregressive translation (NAT), which removes
the penalty of word order errors in the standard cross-entropy loss. Starting
from the intuition that reordering generally occurs between phrases, we extend
oaxe by only allowing reordering between ngram phrases and still requiring a
strict match of word order within the phrases. Extensive experiments on NAT
benchmarks across language pairs and data scales demonstrate the effectiveness
and universality of our approach. %Further analyses show that the proposed
ngram-oaxe alleviates the multimodality problem with a better modeling of
phrase translation. Further analyses show that ngram-oaxe indeed improves the
translation of ngram phrases, and produces more fluent translation with a
better modeling of sentence structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDU-level Extractive Summarization with Varying Summary Lengths. (arXiv:2210.04029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04029">
<div class="article-summary-box-inner">
<span><p>Extractive models usually formulate text summarization as extracting top-k
important sentences from document as summary. Few work exploited extracting
finer-grained Elementary Discourse Unit (EDU) and there is little analysis and
justification for the extractive unit selection. To fill such a gap, this paper
firstly conducts oracle analysis to compare the upper bound of performance for
models based on EDUs and sentences. The analysis provides evidences from both
theoretical and experimental perspectives to justify that EDUs make more
concise and precise summary than sentences without losing salient information.
Then, considering this merit of EDUs, this paper further proposes EDU-level
extractive model with Varying summary Lengths (EDU-VL) and develops the
corresponding learning algorithm. EDU-VL learns to encode and predict
probabilities of EDUs in document, and encode EDU-level candidate summaries
with different lengths based on various $k$ values and select the best
candidate summary in an end-to-end training manner. Finally, the proposed and
developed approach is experimented on single and multi-document benchmark
datasets and shows the improved performances in comparison with the
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Task-Adaptive Pretraining for Dialogue Response Selection. (arXiv:2210.04073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04073">
<div class="article-summary-box-inner">
<span><p>Recent advancements in dialogue response selection (DRS) are based on the
\textit{task-adaptive pre-training (TAP)} approach, by first initializing their
model with BERT~\cite{devlin-etal-2019-bert}, and adapt to dialogue data with
dialogue-specific or fine-grained pre-training tasks. However, it is uncertain
whether BERT is the best initialization choice, or whether the proposed
dialogue-specific fine-grained learning tasks are actually better than MLM+NSP.
This paper aims to verify assumptions made in previous works and understand the
source of improvements for DRS. We show that initializing with RoBERTa achieve
similar performance as BERT, and MLM+NSP can outperform all previously proposed
TAP tasks, during which we also contribute a new state-of-the-art on the Ubuntu
corpus. Additional analyses shows that the main source of improvements comes
from the TAP step, and that the NSP task is crucial to DRS, different from
common NLU tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04074">
<div class="article-summary-box-inner">
<span><p>Natural language often describes events in different granularities, such that
more coarse-grained (goal) events can often be decomposed into fine-grained
sequences of (step) events. A critical but overlooked challenge in
understanding an event process lies in the fact that the step events are not
equally important to the central goal. In this paper, we seek to fill this gap
by studying how well current models can understand the essentiality of
different step events towards a goal event. As discussed by cognitive studies,
such an ability enables the machine to mimic human's commonsense reasoning
about preconditions and necessary efforts of daily-life tasks. Our work
contributes with a high-quality corpus of (goal, step) pairs from a community
guideline website WikiHow, where the steps are manually annotated with their
essentiality w.r.t. the goal. The high IAA indicates that humans have a
consistent understanding of the events. Despite evaluating various statistical
and massive pre-trained NLU models, we observe that existing SOTA models all
perform drastically behind humans, indicating the need for future investigation
of this crucial yet challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding. (arXiv:2210.04105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04105">
<div class="article-summary-box-inner">
<span><p>With the advent of pre-trained language models (LMs), increasing research
efforts have been focusing on infusing commonsense and domain-specific
knowledge to prepare LMs for downstream tasks. These works attempt to leverage
knowledge graphs, the de facto standard of symbolic knowledge representation,
along with pre-trained LMs. While existing approaches leverage external
knowledge, it remains an open question how to jointly incorporate knowledge
graphs representing varying contexts, from local (e.g., sentence), to
document-level, to global knowledge, to enable knowledge-rich and interpretable
exchange across these contexts. Such rich contextualization can be especially
beneficial for long document understanding tasks since standard pre-trained LMs
are typically bounded by the input sequence length. In light of these
challenges, we propose KALM, a Knowledge-Aware Language Model that jointly
leverages knowledge in local, document-level, and global contexts for long
document understanding. KALM first encodes long documents and knowledge graphs
into the three knowledge-aware context representations. It then processes each
context with context-specific layers, followed by a context fusion layer that
facilitates interpretable knowledge exchange to derive an overarching document
representation. Extensive experiments demonstrate that KALM achieves
state-of-the-art performance on three long document understanding tasks across
6 datasets/settings. Further analyses reveal that the three knowledge-aware
contexts are complementary and they all contribute to model performance, while
the importance and information exchange patterns of different contexts vary
with respect to different tasks and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05502">
<div class="article-summary-box-inner">
<span><p>While a large body of work has scrutinized the meaning of conditional
sentences, considerably less attention has been paid to formal models of their
pragmatic use and interpretation. Here, we take a probabilistic approach to
pragmatic reasoning about indicative conditionals which flexibly integrates
gradient beliefs about richly structured world states. We model listeners'
update of their prior beliefs about the causal structure of the world and the
joint probabilities of the consequent and antecedent based on assumptions about
the speaker's utterance production protocol. We show that, when supplied with
natural contextual assumptions, our model uniformly explains a number of
inferences attested in the literature, including epistemic inferences,
conditional perfection and the dependency between antecedent and consequent of
a conditional. We argue that this approach also helps explain three puzzles
introduced by Douven (2012) about updating with conditionals: depending on the
utterance context, the listener's belief in the antecedent may increase,
decrease or remain unchanged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digging Errors in NMT: Evaluating and Understanding Model Errors from Partial Hypothesis Space. (arXiv:2106.15217v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15217">
<div class="article-summary-box-inner">
<span><p>Solid evaluation of neural machine translation (NMT) is key to its
understanding and improvement. Current evaluation of an NMT system is usually
built upon a heuristic decoding algorithm (e.g., beam search) and an evaluation
metric assessing similarity between the translation and golden reference.
However, this system-level evaluation framework is limited by evaluating only
one best hypothesis and search errors brought by heuristic decoding algorithms.
To better understand NMT models, we propose a novel evaluation protocol, which
defines model errors with model's ranking capability over hypothesis space. To
tackle the problem of exponentially large space, we propose two approximation
methods, top region evaluation along with an exact top-$k$ decoding algorithm,
which finds top-ranked hypotheses in the whole hypothesis space, and Monte
Carlo sampling evaluation, which simulates hypothesis space from a broader
perspective. To quantify errors, we define our NMT model errors by measuring
distance between the hypothesis array ranked by the model and the ideally
ranked hypothesis array. After confirming the strong correlation with human
judgment, we apply our evaluation to various NMT benchmarks and model
architectures. We show that the state-of-the-art Transformer models face
serious ranking issues and only perform at the random chance level in the top
region. We further analyze model errors on architectures with different depths
and widths, as well as different data-augmentation techniques, showing how
these factors affect model errors. Finally, we connect model errors with the
search algorithms and provide interesting findings of beam search inductive
bias and correlation with Minimum Bayes Risk (MBR) decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling chronic pain experiences from online reports using the Reddit Reports of Chronic Pain dataset. (arXiv:2108.10218v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10218">
<div class="article-summary-box-inner">
<span><p>Purpose: Reveal and quantify qualities of reported experiences of chronic
pain on social media, from multiple pathological backgrounds, by means of the
novel Reddit Reports of Chronic Pain (RRCP) dataset, using Natural Language
Processing techniques. Methods: Define and validate the RRCP dataset for a set
of subreddits related to chronic pain. Identify the main concerns discussed in
each subreddit. Model each subreddit according to their main concerns. Compare
subreddit models. Results: The RRCP dataset comprises 86,537 Reddit submissions
from 12 subreddits related to chronic pain (each related to one pathological
background). Each RRCP subreddit has various main concerns. Some of these
concerns are shared between multiple subreddits (e.g., the subreddit Sciatica
semantically entails the subreddit backpain in their various concerns, but not
the other way around), whilst some concerns are exclusive to specific
subreddits (e.g., Interstitialcystitis and CrohnsDisease). These results
suggest that the reported experience of chronic pain, from multiple pathologies
(i.e., subreddits), has concerns relevant to all, and concerns exclusive to
certain pathologies. Our analysis details each of these concerns and their
relations. Conclusion: Although limited by intrinsic qualities of the Reddit
platform, to the best of our knowledge, this is the first research work
attempting to model the linguistic expression of various chronic pain-inducing
pathologies and comparing these models to identify and quantify the
similarities and differences between the corresponding emergent chronic pain
experiences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13658">
<div class="article-summary-box-inner">
<span><p>The understanding of time expressions includes two sub-tasks: recognition and
normalization. In recent years, significant progress has been made in the
recognition of time expressions while research on normalization has lagged
behind. Existing SOTA normalization methods highly rely on rules or grammars
designed by experts, which limits their performance on emerging corpora, such
as social media texts. In this paper, we model time expression normalization as
a sequence of operations to construct the normalized temporal value, and we
present a novel method called ARTime, which can automatically generate
normalization rules from training data without expert interventions.
Specifically, ARTime automatically captures possible operation sequences from
annotated data and generates normalization rules on time expressions with
common surface forms. The experimental results show that ARTime can
significantly surpass SOTA methods on the Tweets benchmark, and achieves
competitive results with existing expert-engineered rule methods on the
TempEval-3 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVStoryGen: A Dataset for Generating Stories with Character Descriptions. (arXiv:2109.08833v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08833">
<div class="article-summary-box-inner">
<span><p>We introduce TVStoryGen, a story generation dataset that requires generating
detailed TV show episode recaps from a brief summary and a set of documents
describing the characters involved. Unlike other story generation datasets,
TVStoryGen contains stories that are authored by professional screen-writers
and that feature complex interactions among multiple characters. Generating
stories in TVStoryGen requires drawing relevant information from the lengthy
provided documents about characters based on the brief summary. In addition, we
propose to train reverse models on our dataset for evaluating the faithfulness
of generated stories. We create TVStoryGen from fan-contributed websites, which
allows us to collect 26k episode recaps with 1868.7 tokens on average.
Empirically, we take a hierarchical story generation approach and find that the
neural model that uses oracle content selectors for character descriptions
demonstrates the best performance on automatic metrics, showing the potential
of our dataset to inspire future research on story generation with constraints.
Qualitative analysis shows that the best-performing model sometimes generates
content that is unfaithful to the short summaries, suggesting promising
directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Reasoning via Template Filling. (arXiv:2111.00539v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00539">
<div class="article-summary-box-inner">
<span><p>Large-scale sequence-to-sequence models have shown to be adept at both
multiple-choice and open-domain commonsense reasoning tasks. However, the
current systems do not provide the ability to control the various attributes of
the reasoning chain. To enable better controllability, we propose to study the
commonsense reasoning as a template filling task (TemplateCSR) -- where the
language models fills reasoning templates with the given constraints as control
factors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense
reasoning template-expansion pairs and (ii) introduce POTTER, a pretrained
sequence-to-sequence model using prompts to perform commonsense reasoning
across concepts. Our experiments show that our approach outperforms baselines
both in generation metrics and factuality metrics. We also present a detailed
error analysis on our approach's ability to reliably perform commonsense
reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections. (arXiv:2111.00701v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00701">
<div class="article-summary-box-inner">
<span><p>While there has been substantial progress in text comprehension through
simple factoid question answering, more holistic comprehension of a discourse
still presents a major challenge. Someone critically reflecting on a text as
they read it will pose curiosity-driven, often open-ended questions, which
reflect deep understanding of the content and require complex reasoning to
answer. A key challenge in building and evaluating models for this type of
discourse comprehension is the lack of annotated data, especially since finding
answers to such questions (which may not be answered at all) requires high
cognitive load for annotators over long documents. This paper presents a novel
paradigm that enables scalable data collection targeting the comprehension of
news documents, viewing these questions through the lens of discourse. The
resulting corpus, DCQA (Discourse Comprehension by Question Answering),
consists of 22,430 question-answer pairs across 607 English documents. DCQA
captures both discourse and semantic links between sentences in the form of
free-form, open-ended questions. On an evaluation set that we annotated on
questions from the INQUISITIVE dataset, we show that DCQA provides valuable
supervision for answering open-ended questions. We additionally design
pre-training methods utilizing existing question-answering resources, and use
synthetic data to accommodate unanswerable questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Predictive Uncertainty by Looking Back at Model Explanations. (arXiv:2201.03742v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03742">
<div class="article-summary-box-inner">
<span><p>Predictive uncertainty estimation of pre-trained language models is an
important measure of how likely people can trust their predictions. However,
little is known about what makes a model prediction uncertain. Explaining
predictive uncertainty is an important complement to explaining prediction
labels in helping users understand model decision making and gaining their
trust on model predictions, while has been largely ignored in prior works. In
this work, we propose to explain the predictive uncertainty of pre-trained
language models by extracting uncertain words from existing model explanations.
We find the uncertain words are those identified as making negative
contributions to prediction labels, while actually explaining the predictive
uncertainty. Experiments show that uncertainty explanations are indispensable
to explaining models and helping humans understand model prediction behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Contrastive Learning for Self-supervised Entity Alignment. (arXiv:2201.06225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06225">
<div class="article-summary-box-inner">
<span><p>Self-supervised entity alignment (EA) aims to link equivalent entities across
different knowledge graphs (KGs) without seed alignments. The current SOTA
self-supervised EA method draws inspiration from contrastive learning,
originally designed in computer vision based on instance discrimination and
contrastive loss, and suffers from two shortcomings. Firstly, it puts
unidirectional emphasis on pushing sampled negative entities far away rather
than pulling positively aligned pairs close, as is done in the well-established
supervised EA. Secondly, KGs contain rich side information (e.g., entity
description), and how to effectively leverage those information has not been
adequately investigated in self-supervised EA. In this paper, we propose an
interactive contrastive learning model for self-supervised EA. The model
encodes not only structures and semantics of entities (including entity name,
entity description, and entity neighborhood), but also conducts cross-KG
contrastive learning by building pseudo-aligned entity pairs. Experimental
results show that our approach outperforms previous best self-supervised
results by a large margin (over 9% average improvement) and performs on par
with previous SOTA supervised counterparts, demonstrating the effectiveness of
the interactive contrastive learning for self-supervised EA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02842">
<div class="article-summary-box-inner">
<span><p>The search for effective and robust metrics has been the focus of recent
theoretical and empirical work on generalization of deep neural networks (NNs).
In this paper, we discuss the performance of natural language processing (NLP)
models, and we evaluate various existing and novel generalization metrics.
Compared to prior studies, we (i) focus on NLP instead of computer vision (CV),
(ii) focus on generalization metrics that predict test error instead of the
generalization gap, (iii) focus on generalization metrics that do not need the
access to data, and (iv) focus on the heavy-tail (HT) phenomenon that has
received comparatively less attention in the study of NNs. We extend recent
HT-based work which focuses on power law (PL) distributions, and we study
exponential and exponentially truncated power law (E-TPL) fitting to the
empirical spectral densities (ESDs) of weight matrices. Our empirical studies
are carried on (i) hundreds of Transformers trained in different settings, in
which we systematically vary different hyperparameters, (ii) a total of 51
pretrained Transformers from eight families of Huggingface NLP models,
including BERT, GPT2, etc., and (iii) a total of 28 existing and novel
generalization metrics. From our empirical analyses, we show that shape
metrics, or the metrics obtained from fitting the shape of the ESDs, perform
uniformly better at predicting generalization performance than scale metrics
commonly studied in the literature, as measured by the rank correlations with
the generalization performance. We also show that among the three HT
distributions considered in our paper, the E-TPL fitting of ESDs performs the
most robustly when the models are trained in experimental settings, while the
PL fitting achieves the best performance on well-trained Huggingface models,
and that both E-TPL and PL metrics (which are both shape metrics) outperform
scale metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Reducing Model Update Regression in Structured Prediction for NLP. (arXiv:2202.02976v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02976">
<div class="article-summary-box-inner">
<span><p>Recent advance in deep learning has led to the rapid adoption of machine
learning-based NLP models in a wide range of applications. Despite the
continuous gain in accuracy, backward compatibility is also an important aspect
for industrial applications, yet it received little research attention.
Backward compatibility requires that the new model does not regress on cases
that were correctly handled by its predecessor. This work studies model update
regression in structured prediction tasks. We choose syntactic dependency
parsing and conversational semantic parsing as representative examples of
structured prediction tasks in NLP. First, we measure and analyze model update
regression in different model update settings. Next, we explore and benchmark
existing techniques for reducing model update regression including model
ensemble and knowledge distillation. We further propose a simple and effective
method, Backward-Congruent Re-ranking (BCR), by taking into account the
characteristics of structured prediction. Experiments show that BCR can better
mitigate model update regression than model ensemble and knowledge distillation
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evaluation Metrics for Paraphrase Generation. (arXiv:2202.08479v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08479">
<div class="article-summary-box-inner">
<span><p>In this paper we revisit automatic metrics for paraphrase evaluation and
obtain two findings that disobey conventional wisdom: (1) Reference-free
metrics achieve better performance than their reference-based counterparts. (2)
Most commonly used metrics do not align well with human annotation. Underlying
reasons behind the above findings are explored through additional experiments
and in-depth analyses. Based on the experiments and analyses, we propose
ParaScore, a new evaluation metric for paraphrase generation. It possesses the
merits of reference-based and reference-free metrics and explicitly models
lexical divergence. Experimental results demonstrate that ParaScore
significantly outperforms existing metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-LID: Leveraging BERT to Improve Spoken Language Identification. (arXiv:2203.00328v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00328">
<div class="article-summary-box-inner">
<span><p>Language identification is the task of automatically determining the identity
of a language conveyed by a spoken segment. It has a profound impact on the
multilingual interoperability of an intelligent speech system. Despite language
identification attaining high accuracy on medium or long utterances(&gt;3s), the
performance on short utterances (&lt;=1s) is still far from satisfactory. We
propose a BERT-based language identification system (BERT-LID) to improve
language identification performance, especially on short-duration speech
segments. We extend the original BERT model by taking the phonetic
posteriorgrams (PPG) derived from the front-end phone recognizer as input. Then
we deployed the optimal deep classifier followed by it for language
identification. Our BERT-LID model can improve the baseline accuracy by about
6.5% on long-segment identification and 19.9% on short-segment identification,
demonstrating our BERT-LID's effectiveness to language identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01104">
<div class="article-summary-box-inner">
<span><p>Recently, Mixture-of-Experts (short as MoE) architecture has achieved
remarkable success in increasing the model capacity of large-scale language
models. However, MoE requires incorporating significantly more parameters than
the base model being extended. In this paper, we propose building a
parameter-efficient MoE architecture by sharing information among experts. We
adopt the matrix product operator (MPO, a tensor decomposition from quantum
many-body physics) to reconstruct the parameter matrix in the expert layer and
increase model capacity for pre-trained language models by sharing parameters
of the central tensor (containing the core information) among different experts
while enabling the specificity through the auxiliary tensors (complementing the
central tensor) of different experts. To address the unbalanced optimization
issue, we further design the gradient mask strategy for the MPO-based MoE
architecture. Extensive experiments based on T5 and GPT-2 show improved
performance and efficiency of the pre-trained language model (27.2x reduction
in total parameters for the superior model performance, compared with the
Switch Transformers). Our code is publicly available at
\url{https://github.com/RUCAIBox/MPOE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01294">
<div class="article-summary-box-inner">
<span><p>Teachers often conduct surveys in order to collect data from a predefined
group of students to gain insights into topics of interest. When analyzing
surveys with open-ended textual responses, it is extremely time-consuming,
labor-intensive, and difficult to manually process all the responses into an
insightful and comprehensive report. In the analysis step, traditionally, the
teacher has to read each of the responses and decide on how to group them in
order to extract insightful information. Even though it is possible to group
the responses only using certain keywords, such an approach would be limited
since it not only fails to account for embedded contexts but also cannot detect
polysemous words or phrases and semantics that are not expressible in single
words. In this work, we present a novel end-to-end context-aware framework that
extracts, aggregates, and abbreviates embedded semantic patterns in
open-response survey data. Our framework relies on a pre-trained natural
language model in order to encode the textual data into semantic vectors. The
encoded vectors then get clustered either into an optimally tuned number of
groups or into a set of groups with pre-specified titles. In the former case,
the clusters are then further analyzed to extract a representative set of
keywords or summary sentences that serve as the labels of the clusters. In our
framework, for the designated clusters, we finally provide context-aware
wordclouds that demonstrate the semantically prominent keywords within each
group. Honoring user privacy, we have successfully built the on-device
implementation of our framework suitable for real-time analysis on mobile
devices and have tested it on a synthetic dataset. Our framework reduces the
costs at-scale by automating the process of extracting the most insightful
information pieces from survey data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarkBERT: Marking Word Boundaries Improves Chinese BERT. (arXiv:2203.06378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06378">
<div class="article-summary-box-inner">
<span><p>We present a Chinese BERT model dubbed MarkBERT that uses word information in
this work. Existing word-based BERT models regard words as basic units,
however, due to the vocabulary limit of BERT, they only cover high-frequency
words and fall back to character level when encountering out-of-vocabulary
(OOV) words. Different from existing works, MarkBERT keeps the vocabulary being
Chinese characters and inserts boundary markers between contiguous words. Such
design enables the model to handle any words in the same way, no matter they
are OOV words or not. Besides, our model has two additional benefits: first, it
is convenient to add word-level learning objectives over markers, which is
complementary to traditional character and sentence-level pretraining tasks;
second, it can easily incorporate richer semantics such as POS tags of words by
replacing generic markers with POS tag-specific markers. With the simple
markers insertion, MarkBERT can improve the performances of various downstream
tasks including language understanding and sequence labeling. \footnote{All the
codes and models will be made publicly available at
\url{https://github.com/daiyongya/markbert}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning. (arXiv:2203.06875v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06875">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been demonstrated to be effective in enhancing
pre-trained language models (PLMs) to derive superior universal sentence
embeddings. However, existing contrastive methods still have two limitations.
Firstly, previous works may acquire poor performance under domain shift
settings, thus hindering the application of sentence representations in
practice. We attribute this low performance to the over-parameterization of
PLMs with millions of parameters. To alleviate it, we propose PromCSE
(Prompt-based Contrastive Learning for Sentence Embeddings), which only trains
small-scale \emph{Soft Prompt} (i.e., a set of trainable vectors) while keeping
PLMs fixed. Secondly, the commonly used NT-Xent loss function of contrastive
learning does not fully exploit hard negatives in supervised learning settings.
To this end, we propose to integrate an Energy-based Hinge loss to enhance the
pairwise discriminative power, inspired by the connection between the NT-Xent
loss and the Energy-based Learning paradigm. Empirical results on seven
standard semantic textual similarity (STS) tasks and a domain-shifted STS task
both show the effectiveness of our method compared with the current
state-of-the-art sentence embedding models. Our code is publicly avaliable at
https://github.com/YJiangcm/PromCSE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General and Domain Adaptive Chinese Spelling Check with Error Consistent Pretraining. (arXiv:2203.10929v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10929">
<div class="article-summary-box-inner">
<span><p>The lack of label data is one of the significant bottlenecks for Chinese
Spelling Check (CSC). Existing researches use the method of automatic
generation by exploiting unlabeled data to expand the supervised corpus.
However, there is a big gap between the real input scenario and automatic
generated corpus. Thus, we develop a competitive general speller ECSpell which
adopts the Error Consistent masking strategy to create data for pretraining.
This error consistency masking strategy is used to specify the error types of
automatically generated sentences which is consistent with real scene. The
experimental result indicates our model outperforms previous state-of-the-art
models on the general benchmark. Moreover, spellers often work within a
particular domain in real life. Due to lots of uncommon domain terms,
experiments on our built domain specific datasets show that general models
perform terribly. Inspired by the common practice of input methods, we propose
to add an alterable user dictionary to handle the zero-shot domain adaption
problem. Specifically, we attach a User Dictionary guided inference module (UD)
to a general token classification based speller. Our experiments demonstrate
that ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other
baselines largely, even approaching the performance on the general benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Clinical Coding: What, Why, and Where We Are?. (arXiv:2203.11092v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11092">
<div class="article-summary-box-inner">
<span><p>Clinical coding is the task of transforming medical information in a
patient's health records into structured codes so that they can be used for
statistical analysis. This is a cognitive and time-consuming task that follows
a standard process in order to achieve a high level of consistency. Clinical
coding could potentially be supported by an automated system to improve the
efficiency and accuracy of the process. We introduce the idea of automated
clinical coding and summarise its challenges from the perspective of Artificial
Intelligence (AI) and Natural Language Processing (NLP), based on the
literature, our project experience over the past two and half years (late 2019
- early 2022), and discussions with clinical coding experts in Scotland and the
UK. Our research reveals the gaps between the current deep learning-based
approach applied to clinical coding and the need for explainability and
consistency in real-world practice. Knowledge-based methods that represent and
reason the standard, explainable process of a task may need to be incorporated
into deep learning-based methods for clinical coding. Automated clinical coding
is a promising task for AI, despite the technical and organisational
challenges. Coders are needed to be involved in the development process. There
is much to achieve to develop and deploy an AI-based automated system to
support coding in the next five years and beyond.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study. (arXiv:2203.16757v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16757">
<div class="article-summary-box-inner">
<span><p>Recently, the end-to-end training approach for multi-channel ASR has shown
its effectiveness, which usually consists of a beamforming front-end and a
recognition back-end. However, the end-to-end training becomes more difficult
due to the integration of multiple modules, particularly considering that
multi-channel speech data recorded in real environments are limited in size.
This raises the demand to exploit the single-channel data for multi-channel
end-to-end ASR. In this paper, we systematically compare the performance of
three schemes to exploit external single-channel data for multi-channel
end-to-end ASR, namely back-end pre-training, data scheduling, and data
simulation, under different settings such as the sizes of the single-channel
data and the choices of the front-end. Extensive experiments on CHiME-4 and
AISHELL-4 datasets demonstrate that while all three methods improve the
multi-channel end-to-end speech recognition performance, data simulation
outperforms the other two, at the cost of longer training time. Data scheduling
outperforms back-end pre-training marginally but nearly consistently,
presumably because that in the pre-training stage, the back-end tends to
overfit on the single-channel data, especially when the single-channel data
size is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can language models learn from explanations in context?. (arXiv:2204.02329v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02329">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) can perform new tasks by adapting to a few in-context
examples. For humans, explanations that connect examples to task principles can
improve learning. We therefore investigate whether explanations of few-shot
examples can help LMs. We annotate questions from 40 challenging tasks with
answer explanations, and various matched control explanations. We evaluate how
different types of explanations, instructions, and controls affect zero- and
few-shot performance. We analyze these results using statistical multilevel
modeling techniques that account for the nested dependencies among conditions,
tasks, prompts, and models. We find that explanations can improve performance
-- even without tuning. Furthermore, explanations hand-tuned for performance on
a small validation set offer substantially larger benefits, and building a
prompt by selecting examples and explanations together substantially improves
performance over selecting examples alone. Finally, even untuned explanations
outperform carefully matched controls, suggesting that the benefits are due to
the link between an example and its explanation, rather than lower-level
features. However, only large models benefit. In summary, explanations can
support the in-context learning of large LMs on challenging tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dialogue Policy for Continual Reinforcement Learning. (arXiv:2204.05928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05928">
<div class="article-summary-box-inner">
<span><p>Continual learning is one of the key components of human learning and a
necessary requirement of artificial intelligence. As dialogue can potentially
span infinitely many topics and tasks, a task-oriented dialogue system must
have the capability to continually learn, dynamically adapting to new
challenges while preserving the knowledge it already acquired. Despite the
importance, continual reinforcement learning of the dialogue policy has
remained largely unaddressed. The lack of a framework with training protocols,
baseline models and suitable metrics, has so far hindered research in this
direction. In this work we fill precisely this gap, enabling research in
dialogue policy optimisation to go from static to dynamic learning. We provide
a continual learning algorithm, baseline architectures and metrics for
assessing continual learning models. Moreover, we propose the dynamic dialogue
policy transformer (DDPT), a novel dynamic architecture that can integrate new
knowledge seamlessly, is capable of handling large state spaces and obtains
significant zero-shot performance when being exposed to unseen domains, without
any growth in network parameter size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08110">
<div class="article-summary-box-inner">
<span><p>English pretrained language models, which make up the backbone of many modern
NLP systems, require huge amounts of unlabeled training data. These models are
generally presented as being trained only on English text but have been found
to transfer surprisingly well to other languages. We investigate this
phenomenon and find that common English pretraining corpora actually contain
significant amounts of non-English text: even when less than 1% of data is not
English (well within the error rate of strong language classifiers), this leads
to hundreds of millions of foreign language tokens in large-scale datasets. We
then demonstrate that even these small percentages of non-English data
facilitate cross-lingual transfer for models trained on them, with target
language performance strongly correlated to the amount of in-language data seen
during pretraining. In light of these findings, we argue that no model is truly
monolingual when pretrained at scale, which should be considered when
evaluating cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification. (arXiv:2204.13413v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13413">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex label hierarchy. Recently, the
pretrained language models (PLM)have been widely adopted in HTC through a
fine-tuning paradigm. However, in this paradigm, there exists a huge gap
between the classification tasks with sophisticated label hierarchy and the
masked language model (MLM) pretraining tasks of PLMs and thus the potentials
of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose
HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label
MLM perspective. Specifically, we construct a dynamic virtual template and
label words that take the form of soft prompts to fuse the label hierarchy
knowledge and introduce a zero-bounded multi-label cross entropy loss to
harmonize the objectives of HTC and MLM. Extensive experiments show HPT
achieves state-of-the-art performances on 3 popular HTC datasets and is adept
at handling the imbalance and low resource situations. Our code is available at
https://github.com/wzh9969/HPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks. (arXiv:2205.00305v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00305">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models with millions of parameters require
large storage. Recent approaches tackle this shortcoming by training adapters,
but these approaches still require a relatively large number of parameters. In
this study, AdapterBias, a surprisingly simple yet effective adapter
architecture, is proposed. AdapterBias adds a token-dependent shift to the
hidden output of transformer layers to adapt to downstream tasks with only a
vector and a linear layer. Extensive experiments are conducted to demonstrate
the effectiveness of AdapterBias. The experiments show that our proposed method
can dramatically reduce the trainable parameters compared to the previous works
with a minimal decrease in task performances compared with fine-tuned
pre-trained models. We further find that AdapterBias automatically learns to
assign more significant representation shifts to the tokens related to the task
in consideration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UL2: Unifying Language Learning Paradigms. (arXiv:2205.05131v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05131">
<div class="article-summary-box-inner">
<span><p>Existing pre-trained models are generally geared towards a particular class
of problems. To date, there seems to be still no consensus on what the right
architecture and pre-training setup should be. This paper presents a unified
framework for pre-training models that are universally effective across
datasets and setups. We begin by disentangling architectural archetypes with
pre-training objectives -- two concepts that are commonly conflated. Next, we
present a generalized and unified perspective for self-supervision in NLP and
show how different pre-training objectives can be cast as one another and how
interpolating between different objectives can be effective. We then propose
Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse
pre-training paradigms together. We furthermore introduce a notion of mode
switching, wherein downstream fine-tuning is associated with specific
pre-training schemes. We conduct extensive ablative experiments to compare
multiple pre-training objectives and find that our method pushes the
Pareto-frontier by outperforming T5 and/or GPT-like models across multiple
diverse setups. Finally, by scaling our model up to 20B parameters, we achieve
SOTA performance on 50 well-established supervised NLP tasks ranging from
language generation (with automated and human evaluation), language
understanding, text classification, question answering, commonsense reasoning,
long text reasoning, structured knowledge grounding and information retrieval.
Our model also achieve strong results at in-context learning, outperforming
175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on
one-shot summarization. Finally, we show that UL2 20B works well with
chain-of-thought prompting and reasoning. We release Flax-based T5X model
checkpoints for the 20B model at
\url{https://github.com/google-research/google-research/tree/master/ul2}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Pretrained Language Models Good Long-tailed Learners. (arXiv:2205.05461v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05461">
<div class="article-summary-box-inner">
<span><p>Prompt-tuning has shown appealing performance in few-shot classification by
virtue of its capability in effectively exploiting pre-trained knowledge. This
motivates us to check the hypothesis that prompt-tuning is also a promising
choice for long-tailed classification, since the tail classes are intuitively
few-shot ones. To achieve this aim, we conduct empirical studies to examine the
hypothesis. The results demonstrate that prompt-tuning makes pretrained
language models at least good long-tailed learners. For intuitions on why
prompt-tuning can achieve good performance in long-tailed classification, we
carry out in-depth analyses by progressively bridging the gap between
prompt-tuning and commonly used finetuning. The summary is that the classifier
structure and parameterization form the key to making good long-tailed
learners, in comparison with the less important input structure. Finally, we
verify the applicability of our finding to few-shot classification. Good
long-tailed learners can be abbreviated as Glee.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. (arXiv:2205.06983v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06983">
<div class="article-summary-box-inner">
<span><p>Relational structures such as schema linking and schema encoding have been
validated as a key component to qualitatively translating natural language into
SQL queries. However, introducing these structural relations comes with prices:
they often result in a specialized model structure, which largely prohibits
using large pretrained models in text-to-SQL. To address this problem, we
propose RASAT: a Transformer seq2seq architecture augmented with relation-aware
self-attention that could leverage a variety of relational structures while
inheriting the pretrained parameters from the T5 model effectively. Our model
can incorporate almost all types of existing relations in the literature, and
in addition, we propose introducing co-reference relations for the multi-turn
scenario. Experimental results on three widely used text-to-SQL datasets,
covering both single-turn and multi-turn scenarios, have shown that RASAT could
achieve state-of-the-art results across all three benchmarks (75.5% EX on
Spider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Tokenization Learning. (arXiv:2205.11443v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11443">
<div class="article-summary-box-inner">
<span><p>In the presented study, we discover that so called "transition freedom"
metric appears superior for unsupervised tokenization purposes, compared to
statistical metrics such as mutual information and conditional probability,
providing F-measure scores in range from 0.71 to 1.0 across explored corpora.
We find that different languages require different derivatives of that metric
(such as variance and "peak values") for successful tokenization. Larger
training corpora does not necessarily effect in better tokenization quality,
while compacting the models eliminating statistically weak evidence tends to
improve performance. Proposed unsupervised tokenization technique provides
quality better or comparable to lexicon-based one, depending on the language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLUTE: Figurative Language Understanding through Textual Explanations. (arXiv:2205.12404v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12404">
<div class="article-summary-box-inner">
<span><p>Figurative language understanding has been recently framed as a recognizing
textual entailment (RTE) task (a.k.a. natural language inference, or NLI).
However, similar to classical RTE/NLI datasets, the current benchmarks suffer
from spurious correlations and annotation artifacts. To tackle this problem,
work on NLI has built explanation-based datasets such as e-SNLI, allowing us to
probe whether language models are right for the right reasons.Yet no such data
exists for figurative language, making it harder to assess genuine
understanding of such expressions. To address this issue, we release FLUTE, a
dataset of 9,000 figurative NLI instances with explanations, spanning four
categories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through
a model-in-the-loop framework based on GPT-3, crowd workers, and expert
annotators. We show how utilizing GPT-3 in conjunction with human annotators
(novices and experts) can aid in scaling up the creation of datasets even for
such complex linguistic phenomena as figurative language. The baseline
performance of the T5 model fine-tuned on FLUTE shows that our dataset can
bring us a step closer to develop-ing models that understand figurative
language through textual explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. (arXiv:2205.12522v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12522">
<div class="article-summary-box-inner">
<span><p>Research in massively multilingual image captioning has been severely
hampered by a lack of high-quality evaluation datasets. In this paper we
present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse
set of 3600 images annotated with human-generated reference captions in 36
languages. The images were selected from across the world, covering regions
where the 36 languages are spoken, and annotated with captions that achieve
consistency in terms of style across all languages, while avoiding annotation
artifacts due to direct translation. We apply this benchmark to model selection
for massively multilingual image captioning models, and show superior
correlation results with human evaluations when using XM3600 as golden
references for automatic metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFM: Dialogue Foundation Model for Universal Large-Scale Dialogue-Oriented Task Learning. (arXiv:2205.12662v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12662">
<div class="article-summary-box-inner">
<span><p>Building a universal conversational agent has been a long-standing goal of
the dialogue research community. Most previous works only focus on a small set
of dialogue tasks. In this work, we aim to build a unified dialogue foundation
model (DFM) which can be used to solve massive diverse dialogue tasks. To
achieve this goal, a large-scale well-annotated dialogue dataset with rich task
diversity (DialogZoo) is collected. We introduce a framework to unify all
dialogue tasks and propose novel auxiliary self-supervised tasks to achieve
stable training of DFM on the highly diverse large scale DialogZoo corpus.
Experiments show that, compared with models of the same size, DFM can achieve
state-of-the-art or competitive performance on very rich cross-domain
downstream dialogue tasks. This demonstrates that DFM largely extends the
ability of unified dialogue pre-trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01201">
<div class="article-summary-box-inner">
<span><p>This paper revisits visual representation in knowledge-based visual question
answering (VQA) and demonstrates that using regional information in a better
way can significantly improve the performance. While visual representation is
extensively studied in traditional VQA, it is under-explored in knowledge-based
VQA even though these two tasks share the common spirit, i.e., rely on visual
input to answer the question. Specifically, we observe that in most
state-of-the-art knowledge-based VQA methods: 1) visual features are extracted
either from the whole image or in a sliding window manner for retrieving
knowledge, and the important relationship within/among object regions is
neglected; 2) visual features are not well utilized in the final answering
model, which is counter-intuitive to some extent. Based on these observations,
we propose a new knowledge-based VQA method REVIVE, which tries to utilize the
explicit information of object regions not only in the knowledge retrieval
stage but also in the answering model. The key motivation is that object
regions and inherent relationship are important for knowledge-based VQA. We
perform extensive experiments on the standard OK-VQA dataset and achieve new
state-of-the-art performance, i.e., 58.0% accuracy, surpassing previous
state-of-the-art method by a large margin (+3.6%). We also conduct detailed
analysis and show the necessity of regional information in different framework
components for knowledge-based VQA. Code is publicly available at
https://github.com/yzleroy/REVIVE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech. (arXiv:2206.02147v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02147">
<div class="article-summary-box-inner">
<span><p>Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses demonstrate
that each design in Dict-TTS is effective. The code is available at
\url{https://github.com/Zain-Jiang/Dict-TTS}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation. (arXiv:2206.02369v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02369">
<div class="article-summary-box-inner">
<span><p>While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System. (arXiv:2206.02628v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02628">
<div class="article-summary-box-inner">
<span><p>Measuring the confidence of AI models is critical for safely deploying AI in
real-world industrial systems. One important application of confidence
measurement is information extraction from scanned documents. However, there
exists no solution to provide reliable confidence score for current
state-of-the-art deep-learning-based information extractors. In this paper, we
propose a complete and novel architecture to measure confidence of current deep
learning models in document information extraction task. Our architecture
consists of a Multi-modal Conformal Predictor and a Variational
Cluster-oriented Anomaly Detector, trained to faithfully estimate its
confidence on its outputs without the need of host models modification. We
evaluate our architecture on real-wold datasets, not only outperforming
competing confidence estimators by a huge margin but also demonstrating
generalization ability to out-of-distribution data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08155">
<div class="article-summary-box-inner">
<span><p>Video question answering (VideoQA) is a complex task that requires diverse
multi-modal data for training. Manual annotation of question and answers for
videos, however, is tedious and prohibits scalability. To tackle this problem,
recent methods consider zero-shot settings with no manual annotation of visual
question-answer. In particular, a promising approach adapts frozen
autoregressive language models pretrained on Web-scale text-only data to
multi-modal inputs. In contrast, we here build on frozen bidirectional language
models (BiLM) and show that such an approach provides a stronger and cheaper
alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs
with the frozen BiLM using light trainable modules, (ii) we train such modules
using Web-scraped multi-modal data, and finally (iii) we perform zero-shot
VideoQA inference through masked language modeling, where the masked text is
the answer to a given question. Our proposed approach, FrozenBiLM, outperforms
the state of the art in zero-shot VideoQA by a significant margin on a variety
of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,
TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in
the few-shot and fully-supervised setting. Our code and models are publicly
available at https://github.com/antoyang/FrozenBiLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GERNERMED++: Transfer Learning in German Medical NLP. (arXiv:2206.14504v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14504">
<div class="article-summary-box-inner">
<span><p>We present a statistical model for German medical natural language processing
trained for named entity recognition (NER) as an open, publicly available
model. The work serves as a refined successor to our first GERNERMED model
which is substantially outperformed by our work. We demonstrate the
effectiveness of combining multiple techniques in order to achieve strong
results in entity recognition performance by the means of transfer-learning on
pretrained deep language models (LM), word-alignment and neural machine
translation. Due to the sparse situation on open, public medical entity
recognition models for German texts, this work offers benefits to the German
research community on medical NLP as a baseline model. Since our model is based
on public English data, its weights are provided without legal restrictions on
usage and distribution. The sample code and the statistical model is available
at: https://github.com/frankkramer-lab/GERNERMED-pp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision. (arXiv:2206.14719v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14719">
<div class="article-summary-box-inner">
<span><p>Clinical trials are essential for drug development but are extremely
expensive and time-consuming to conduct. It is beneficial to study similar
historical trials when designing a clinical trial. However, lengthy trial
documents and lack of labeled data make trial similarity search difficult. We
propose a zero-shot clinical trial retrieval method, Trial2Vec, which learns
through self-supervision without annotating similar clinical trials.
Specifically, the meta-structure of trial documents (e.g., title, eligibility
criteria, target disease) along with clinical knowledge (e.g., UMLS knowledge
base https://www.nlm.nih.gov/research/umls/index.html) are leveraged to
automatically generate contrastive samples. Besides, Trial2Vec encodes trial
documents considering meta-structure thus producing compact embeddings
aggregating multi-aspect information from the whole document. We show that our
method yields medically interpretable embeddings by visualization and it gets a
15% average improvement over the best baselines on precision/recall for trial
retrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we
prove the pre-trained embeddings benefit the downstream trial outcome
prediction task over 240k trials. Software ias available at
https://github.com/RyanWangZf/Trial2Vec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forecasting Future World Events with Neural Networks. (arXiv:2206.15474v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15474">
<div class="article-summary-box-inner">
<span><p>Forecasting future world events is a challenging but valuable task. Forecasts
of climate, geopolitical conflict, pandemics and economic indicators help shape
policy and decision making. In these domains, the judgment of expert humans
contributes to the best forecasts. Given advances in language modeling, can
these forecasts be automated? To this end, we introduce Autocast, a dataset
containing thousands of forecasting questions and an accompanying news corpus.
Questions are taken from forecasting tournaments, ensuring high quality,
real-world importance, and diversity. The news corpus is organized by date,
allowing us to precisely simulate the conditions under which humans made past
forecasts (avoiding leakage from the future). Motivated by the difficulty of
forecasting numbers across orders of magnitude (e.g. global cases of COVID-19
in 2022), we also curate IntervalQA, a dataset of numerical questions and
metrics for calibration. We test language models on our forecasting task and
find that performance is far below a human expert baseline. However,
performance improves with increased model size and incorporation of relevant
information from the news corpus. In sum, Autocast poses a novel challenge for
large language models and improved performance could bring large practical
benefits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correspondences between word learning in children and captioning models. (arXiv:2207.09847v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09847">
<div class="article-summary-box-inner">
<span><p>For human children as well as machine learning systems, a key challenge in
learning a word is linking the word to the visual phenomena it describes. By
organizing model output into word categories used to analyze child language
learning data, we show a correspondence between word learning in children and
the performance of image captioning models. Although captioning models are
trained only on standard machine learning data, we find that their performance
in producing words from a variety of word categories correlates with the age at
which children acquire words from each of those categories. To explain why this
correspondence exists, we show that the performance of captioning models is
correlated with human judgments of the concreteness of words, suggesting that
these models are capturing the complex real-world association between words and
visual phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Perception as a Phenomenon of Quantization. (arXiv:2208.03726v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.03726">
<div class="article-summary-box-inner">
<span><p>For two decades, the formalism of quantum mechanics has been successfully
used to describe human decision processes, situations of heuristic reasoning,
and the contextuality of concepts and their combinations. The phenomenon of
'categorical perception' has put us on track to find a possible deeper cause of
the presence of this quantum structure in human cognition. Thus, we show that
in an archetype of human perception consisting of the reconciliation of a
bottom up stimulus with a top down cognitive expectation pattern, there arises
the typical warping of categorical perception, where groups of stimuli clump
together to form quanta, which move away from each other and lead to a
discretization of a dimension. The individual concepts, which are these quanta,
can be modeled by a quantum prototype theory with the square of the absolute
value of a corresponding Schr\"odinger wave function as the fuzzy prototype
structure, and the superposition of two such wave functions accounts for the
interference pattern that occurs when these concepts are combined. Using a
simple quantum measurement model, we analyze this archetype of human
perception, provide an overview of the experimental evidence base for
categorical perception with the phenomenon of warping leading to quantization,
and illustrate our analyses with two examples worked out in detail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training Transformers on Indian Legal Text. (arXiv:2209.06049v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06049">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing in the legal domain been benefited hugely by the
emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained
on legal text. There exist PLMs trained over European and US legal text, most
notably LegalBERT. However, with the rapidly increasing volume of NLP
applications on Indian legal documents, and the distinguishing characteristics
of Indian legal text, it has become necessary to pre-train LMs over Indian
legal text as well. In this work, we introduce transformer-based PLMs
pre-trained over a large corpus of Indian legal documents. We also apply these
PLMs over several benchmark legal NLP tasks over both Indian legal text, as
well as over legal text belonging to other domains (countries). The NLP tasks
with which we experiment include Legal Statute Identification from facts,
Semantic segmentation of court judgements, and Court Judgement Prediction. Our
experiments demonstrate the utility of the India-specific PLMs developed in
this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer. (arXiv:2209.08902v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.08902">
<div class="article-summary-box-inner">
<span><p>Both real and fake news in various domains, such as politics, health, and
entertainment are spread via online social media every day, necessitating fake
news detection for multiple domains. Among them, fake news in specific domains
like politics and health has more serious potential negative impacts on the
real world (e.g., the infodemic led by COVID-19 misinformation). Previous
studies focus on multi-domain fake news detection, by equally mining and
modeling the correlation between domains. However, these multi-domain methods
suffer from a seesaw problem: the performance of some domains is often improved
at the cost of hurting the performance of other domains, which could lead to an
unsatisfying performance in specific domains. To address this issue, we propose
a Domain- and Instance-level Transfer Framework for Fake News Detection
(DITFEND), which could improve the performance of specific target domains. To
transfer coarse-grained domain-level knowledge, we train a general model with
data of all domains from the meta-learning perspective. To transfer
fine-grained instance-level knowledge and adapt the general model to a target
domain, we train a language model on the target domain to evaluate the
transferability of each data instance in source domains and re-weigh each
instance's contribution. Offline experiments on two datasets demonstrate the
effectiveness of DITFEND. Online experiments show that DITFEND brings
additional improvements over the base models in a real-world scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whodunit? Learning to Contrast for Authorship Attribution. (arXiv:2209.11887v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.11887">
<div class="article-summary-box-inner">
<span><p>Authorship attribution is the task of identifying the author of a given text.
The key is finding representations that can differentiate between authors.
Existing approaches typically use manually designed features that capture a
dataset's content and style, but these approaches are dataset-dependent and
yield inconsistent performance across corpora. In this work, we propose
\textit{learning} author-specific representations by fine-tuning pre-trained
generic language representations with a contrastive objective (Contra-X). We
show that Contra-X learns representations that form highly separable clusters
for different authors. It advances the state-of-the-art on multiple human and
machine authorship attribution benchmarks, enabling improvements of up to 6.8%
over cross-entropy fine-tuning. However, we find that Contra-X improves overall
accuracy at the cost of sacrificing performance for some authors. Resolving
this tension will be an important direction for future work. To the best of our
knowledge, we are the first to integrate contrastive learning with pre-trained
language model fine-tuning for authorship attribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to tackle an emerging topic? Combining strong and weak labels for Covid news NER. (arXiv:2209.15108v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15108">
<div class="article-summary-box-inner">
<span><p>Being able to train Named Entity Recognition (NER) models for emerging topics
is crucial for many real-world applications especially in the medical domain
where new topics are continuously evolving out of the scope of existing models
and datasets. For a realistic evaluation setup, we introduce a novel COVID-19
news NER dataset (COVIDNEWS-NER) and release 3000 entries of hand annotated
strongly labelled sentences and 13000 auto-generated weakly labelled sentences.
Besides the dataset, we propose CONTROSTER, a recipe to strategically combine
weak and strong labels in improving NER in an emerging topic through transfer
learning. We show the effectiveness of CONTROSTER on COVIDNEWS-NER while
providing analysis on combining weak and strong labels for training. Our key
findings are: (1) Using weak data to formulate an initial backbone before
tuning on strong data outperforms methods trained on only strong or weak data.
(2) A combination of out-of-domain and in-domain weak label training is crucial
and can overcome saturation when being training on weak labels from a single
source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALT: A software for readability analysis of Portuguese-language texts. (arXiv:2210.00553v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00553">
<div class="article-summary-box-inner">
<span><p>In the initial stage of human life, communication, seen as a process of
social interaction, was always the best way to reach consensus between the
parties. Understanding and credibility in this process are essential for the
mutual agreement to be validated. But, how to do it so that this communication
reaches the great mass? This is the main challenge when what is sought is the
dissemination of information and its approval. In this context, this study
presents the ALT software, developed from original readability metrics adapted
to the Portuguese language, available on the web, to reduce communication
difficulties. The development of the software was motivated by the theory of
communicative action of Habermas, which uses a multidisciplinary style to
measure the credibility of the discourse in the communication channels used to
build and maintain a safe and healthy relationship with the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01911">
<div class="article-summary-box-inner">
<span><p>Recent works have shown that Large Language Models (LLMs) can be applied to
ground natural language to a wide variety of robot skills. However, in
practice, learning multi-task, language-conditioned robotic skills typically
requires large-scale data collection and frequent human intervention to reset
the environment or help correcting the current policies. In this work, we
propose a novel approach to efficiently learn general-purpose
language-conditioned robot skills from unstructured, offline and reset-free
data in the real world by exploiting a self-supervised visuo-lingual affordance
model, which requires annotating as little as 1% of the total data with
language. We evaluate our method in extensive experiments both in simulated and
real-world robotic tasks, achieving state-of-the-art performance on the
challenging CALVIN benchmark and learning over 25 distinct visuomotor
manipulation tasks with a single policy in the real world. We find that when
paired with LLMs to break down abstract natural language instructions into
subgoals via few-shot prompting, our method is capable of completing
long-horizon, multi-tier tasks in the real world, while requiring an order of
magnitude less data than previous approaches. Code and videos are available at
<a href="http://hulc2.cs.uni-freiburg.de">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering. (arXiv:2210.02933v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02933">
<div class="article-summary-box-inner">
<span><p>A common thread of open-domain question answering (QA) models employs a
retriever-reader pipeline that first retrieves a handful of relevant passages
from Wikipedia and then peruses the passages to produce an answer. However,
even state-of-the-art readers fail to capture the complex relationships between
entities appearing in questions and retrieved passages, leading to answers that
contradict the facts. In light of this, we propose a novel knowledge Graph
enhanced passage reader, namely Grape, to improve the reader performance for
open-domain QA. Specifically, for each pair of question and retrieved passage,
we first construct a localized bipartite graph, attributed to entity embeddings
extracted from the intermediate layer of the reader model. Then, a graph neural
network learns relational knowledge while fusing graph and contextual
representations into the hidden states of the reader model. Experiments on
three open-domain QA benchmarks show Grape can improve the state-of-the-art
performance by up to 2.2 exact match score with a negligible overhead increase,
with the same retriever and retrieved passages. Our code is publicly available
at https://github.com/jumxglhf/GRAPE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">State-of-the-art generalisation research in NLP: a taxonomy and review. (arXiv:2210.03050v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03050">
<div class="article-summary-box-inner">
<span><p>The ability to generalise well is one of the primary desiderata of natural
language processing (NLP). Yet, what `good generalisation' entails and how it
should be evaluated is not well understood, nor are there any common standards
to evaluate it. In this paper, we aim to lay the ground-work to improve both of
these issues. We present a taxonomy for characterising and understanding
generalisation research in NLP, we use that taxonomy to present a comprehensive
map of published generalisation studies, and we make recommendations for which
areas might deserve attention in the future. Our taxonomy is based on an
extensive literature review of generalisation research, and contains five axes
along which studies can differ: their main motivation, the type of
generalisation they aim to solve, the type of data shift they consider, the
source by which this data shift is obtained, and the locus of the shift within
the modelling pipeline. We use our taxonomy to classify over 400 previous
papers that test generalisation, for a total of more than 600 individual
experiments. Considering the results of this review, we present an in-depth
analysis of the current state of generalisation research in NLP, and make
recommendations for the future. Along with this paper, we release a webpage
where the results of our review can be dynamically explored, and which we
intend to up-date as new NLP generalisation studies are published. With this
work, we aim to make steps towards making state-of-the-art generalisation
testing the new status quo in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Extraction: A Survey. (arXiv:2210.03419v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03419">
<div class="article-summary-box-inner">
<span><p>Extracting the reported events from text is one of the key research themes in
natural language processing. This process includes several tasks such as event
detection, argument extraction, role labeling. As one of the most important
topics in natural language processing and natural language understanding, the
applications of event extraction spans across a wide range of domains such as
newswire, biomedical domain, history and humanity, and cyber security. This
report presents a comprehensive survey for event detection from textual
documents. In this report, we provide the task definition, the evaluation
method, as well as the benchmark datasets and a taxonomy of methodologies for
event extraction. We also present our vision of future research direction in
event detection.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-11 01:33:14.455520195 UTC">2022-10-11 01:33:14 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>