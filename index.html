<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-10-14T01:30:00Z">10-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergence of Shared Sensory-motor Graphical Language from Visual Input. (arXiv:2210.06468v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06468">
<div class="article-summary-box-inner">
<span><p>The framework of Language Games studies the emergence of languages in
populations of agents. Recent contributions relying on deep learning methods
focused on agents communicating via an idealized communication channel, where
utterances produced by a speaker are directly perceived by a listener. This
comes in contrast with human communication, which instead relies on a
sensory-motor channel, where motor commands produced by the speaker (e.g. vocal
or gestural articulators) result in sensory effects perceived by the listener
(e.g. audio or visual). Here, we investigate if agents can evolve a shared
language when they are equipped with a continuous sensory-motor system to
produce and perceive signs, e.g. drawings. To this end, we introduce the
Graphical Referential Game (GREG) where a speaker must produce a graphical
utterance to name a visual referent object consisting of combinations of MNIST
digits while a listener has to select the corresponding object among distractor
referents, given the produced message. The utterances are drawing images
produced using dynamical motor primitives combined with a sketching library. To
tackle GREG we present CURVES: a multimodal contrastive deep learning mechanism
that represents the energy (alignment) between named referents and utterances
generated through gradient ascent on the learned energy landscape. We, then,
present a set of experiments and metrics based on a systematic compositional
dataset to evaluate the resulting language. We show that our method allows the
emergence of a shared, graphical language with compositional properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models. (arXiv:2210.06475v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06475">
<div class="article-summary-box-inner">
<span><p>We introduce equi-tuning, a novel fine-tuning method that transforms
(potentially non-equivariant) pretrained models into group equivariant models
while incurring minimum $L_2$ loss between the feature representations of the
pretrained and the equivariant models. Large pretrained models can be
equi-tuned for different groups to satisfy the needs of various downstream
tasks. Equi-tuned models benefit from both group equivariance as an inductive
bias and semantic priors from pretrained models. We provide applications of
equi-tuning on three different tasks: image classification, compositional
generalization in language, and fairness in natural language generation (NLG).
We also provide a novel group-theoretic definition for fairness in NLG. The
effectiveness of this definition is shown by testing it against a standard
empirical method of fairness in NLG. We provide experimental results for
equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and
Densenet for image classification; RNNs, GRUs, and LSTMs for compositional
generalization; and GPT2 for fairness in NLG. We test these models on benchmark
datasets across all considered tasks to show the generality and effectiveness
of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUMBot: Summarizing Context in Open-Domain Dialogue Systems. (arXiv:2210.06496v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06496">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the problem of including relevant information
as context in open-domain dialogue systems. Most models struggle to identify
and incorporate important knowledge from dialogues and simply use the entire
turns as context, which increases the size of the input fed to the model with
unnecessary information. Additionally, due to the input size limitation of a
few hundred tokens of large pre-trained models, regions of the history are not
included and informative parts from the dialogue may be omitted. In order to
surpass this problem, we introduce a simple method that substitutes part of the
context with a summary instead of the whole history, which increases the
ability of models to keep track of all the previous relevant information. We
show that the inclusion of a summary may improve the answer generation task and
discuss some examples to further understand the system's weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subword Segmental Language Modelling for Nguni Languages. (arXiv:2210.06525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06525">
<div class="article-summary-box-inner">
<span><p>Subwords have become the standard units of text in NLP, enabling efficient
open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword
segmentation is viewed as a preprocessing step applied to the corpus before
training. This can lead to sub-optimal segmentations for low-resource languages
with complex morphologies. We propose a subword segmental language model (SSLM)
that learns how to segment words while being trained for autoregressive
language modelling. By unifying subword segmentation and language modelling,
our model learns subwords that optimise LM performance. We train our model on
the 4 Nguni languages of South Africa. These are low-resource agglutinative
languages, so subword information is critical. As an LM, SSLM outperforms
existing approaches such as BPE-based models on average across the 4 languages.
Furthermore, it outperforms standard subword segmenters on unsupervised
morphological segmentation. We also train our model as a word-level sequence
model, resulting in an unsupervised morphological segmenter that outperforms
existing methods by a large margin for all 4 languages. Our results show that
learning subword segmentation is an effective alternative to existing subword
segmenters, enabling the model to discover morpheme-like subwords that improve
its LM capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual textual data: an approach through multiple factor analysis. (arXiv:2210.06527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06527">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the analysis of open-ended questions answered in
different languages. Closed-ended questions, called contextual variables, are
asked to all respondents in order to understand the relationships between the
free and the closed responses among the different samples since the latter
assumably affect the word choices. We have developed "Multiple Factor Analysis
on Generalized Aggregated Lexical Tables" (MFA-GALT) to jointly study the
open-ended responses in different languages through the relationships between
the choice of words and the variables that drive this choice. MFA-GALT studies
if variability among words is structured in the same way by variability among
variables, and inversely, from one sample to another. An application on an
international satisfaction survey shows the easy-to-interpret results that are
proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing a general-purpose clinical language inference model from a large corpus of clinical notes. (arXiv:2210.06566v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06566">
<div class="article-summary-box-inner">
<span><p>Several biomedical language models have already been developed for clinical
language inference. However, these models typically utilize general
vocabularies and are trained on relatively small clinical corpora. We sought to
evaluate the impact of using a domain-specific vocabulary and a large clinical
training corpus on the performance of these language models in clinical
language inference. We trained a Bidirectional Encoder Decoder from
Transformers (BERT) model using a diverse, deidentified corpus of 75 million
deidentified clinical notes authored at the University of California, San
Francisco (UCSF). We evaluated this model on several clinical language
inference benchmark tasks: clinical and temporal concept recognition, relation
extraction and medical language inference. We also evaluated our model on two
tasks using discharge summaries from UCSF: diagnostic code assignment and
therapeutic class inference. Our model performs at par with the best publicly
available biomedical language models of comparable sizes on the public
benchmark tasks, and is significantly better than these models in a
within-system evaluation on the two tasks using UCSF data. The use of in-domain
vocabulary appears to improve the encoding of longer documents. The use of
large clinical corpora appears to enhance document encoding and inferential
accuracy. However, further research is needed to improve abbreviation
resolution, and numerical, temporal, and implicitly causal inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DATScore: Evaluating Translation with Data Augmented Translations. (arXiv:2210.06576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06576">
<div class="article-summary-box-inner">
<span><p>The rapid development of large pretrained language models has revolutionized
not only the field of Natural Language Generation (NLG) but also its
evaluation. Inspired by the recent work of BARTScore: a metric leveraging the
BART language model to evaluate the quality of generated text from various
aspects, we introduce DATScore. DATScore uses data augmentation techniques to
improve the evaluation of machine translation. Our main finding is that
introducing data augmented translations of the source and reference texts is
greatly helpful in evaluating the quality of the generated translation. We also
propose two novel score averaging and term weighting strategies to improve the
original score computing process of BARTScore. Experimental results on WMT show
that DATScore correlates better with human meta-evaluations than the other
recent state-of-the-art metrics, especially for low-resource languages.
Ablation studies demonstrate the value added by our new scoring strategies.
Moreover, we report in our extended experiments the performance of DATScore on
3 NLG tasks other than translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Question Answering with Generation of NQ-like Questions. (arXiv:2210.06599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06599">
<div class="article-summary-box-inner">
<span><p>Question Answering (QA) systems require a large amount of annotated data
which is costly and time-consuming to gather. Converting datasets of existing
QA benchmarks are challenging due to different formats and complexities. To
address these issues, we propose an algorithm to automatically generate shorter
questions resembling day-to-day human communication in the Natural Questions
(NQ) dataset from longer trivia questions in Quizbowl (QB) dataset by
leveraging conversion in style among the datasets. This provides an automated
way to generate more data for our QA systems. To ensure quality as well as
quantity of data, we detect and remove ill-formed questions using a neural
classifier. We demonstrate that in a low resource setting, using the generated
data improves the QA performance over the baseline system on both NQ and QB
data. Our algorithm improves the scalability of training data while maintaining
quality of data for QA systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Document-level Information Extraction via Imitation Learning. (arXiv:2210.06600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06600">
<div class="article-summary-box-inner">
<span><p>We present a novel iterative extraction (IterX) model for extracting complex
relations, or templates, i.e., N-tuples representing a mapping from named slots
to spans of text contained within a document. Documents may support zero or
more instances of a template of any particular type, leading to the tasks of
identifying the templates in a document, and extracting each template's slot
values. Our imitation learning approach relieves the need to use predefined
template orders to train an extractor and leads to state-of-the-art results on
two established benchmarks -- 4-ary relation extraction on SciREX and template
extraction on MUC-4 -- as well as a strong baseline on the new BETTER Granular
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenCQA: Open-ended Question Answering with Charts. (arXiv:2210.06628v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06628">
<div class="article-summary-box-inner">
<span><p>Charts are very popular to analyze data and convey important insights. People
often analyze visualizations to answer open-ended questions that require
explanatory answers. Answering such questions are often difficult and
time-consuming as it requires a lot of cognitive and perceptual efforts. To
address this challenge, we introduce a new task called OpenCQA, where the goal
is to answer an open-ended question about a chart with descriptive texts. We
present the annotation process and an in-depth analysis of our dataset. We
implement and evaluate a set of baselines under three practical settings. In
the first setting, a chart and the accompanying article is provided as input to
the model. The second setting provides only the relevant paragraph(s) to the
chart instead of the entire article, whereas the third setting requires the
model to generate an answer solely based on the chart. Our analysis of the
results show that the top performing models generally produce fluent and
coherent text while they struggle to perform complex logical and arithmetic
reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis. (arXiv:2210.06629v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06629">
<div class="article-summary-box-inner">
<span><p>Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis
task which involves four elements from user-generated texts: aspect term,
aspect category, opinion term, and sentiment polarity. Most computational
approaches focus on some of the ABSA sub-tasks such as tuple (aspect term,
sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)
extraction using either pipeline or joint modeling approaches. Recently,
generative approaches have been proposed to extract all four elements as (one
or more) quadruplets from text as a single task. In this work, we take a step
further and propose a unified framework for solving ABSA, and the associated
sub-tasks to improve the performance in few-shot scenarios. To this end, we
fine-tune a T5 model with instructional prompts in a multi-task learning
fashion covering all the sub-tasks, as well as the entire quadruple prediction
task. In experiments with multiple benchmark data sets, we show that the
proposed multi-task prompting approach brings performance boost (by absolute
$6.75$ F1) in the few-shot learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06633">
<div class="article-summary-box-inner">
<span><p>Multilingual information retrieval is challenging due to the lack of training
datasets for many low-resource languages. We present an effective method by
leveraging parallel and non-parallel corpora to improve the pretrained
multilingual language models' cross-lingual transfer ability for information
retrieval. We design the semantic contrastive loss as regular contrastive
learning to improve the cross-lingual alignment of parallel sentence pairs, and
we propose a new contrastive loss, the language contrastive loss, to leverage
both parallel corpora and non-parallel corpora to further improve multilingual
representation learning. We train our model on an English information retrieval
dataset, and test its zero-shot transfer ability to other languages. Our
experiment results show that our method brings significant improvement to prior
work on retrieval performance, while it requires much less computational
effort. Our model can work well even with a small number of parallel corpora.
And it can be used as an add-on module to any backbone and other tasks. Our
code is available at: https://github.com/xiyanghu/multilingualIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The COVID That Wasn't: Counterfactual Journalism Using GPT. (arXiv:2210.06644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06644">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the use of large language models to assess human
interpretations of real world events. To do so, we use a language model trained
prior to 2020 to artificially generate news articles concerning COVID-19 given
the headlines of actual articles written during the pandemic. We then compare
stylistic qualities of our artificially generated corpus with a news corpus, in
this case 5,082 articles produced by CBC News between January 23 and May 5,
2020. We find our artificially generated articles exhibits a considerably more
negative attitude towards COVID and a significantly lower reliance on
geopolitical framing. Our methods and results hold importance for researchers
seeking to simulate large scale cultural processes via recent breakthroughs in
text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-grounded Dialog State Tracking. (arXiv:2210.06656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06656">
<div class="article-summary-box-inner">
<span><p>Knowledge (including structured knowledge such as schema and ontology, and
unstructured knowledge such as web corpus) is a critical part of dialog
understanding, especially for unseen tasks and domains. Traditionally, such
domain-specific knowledge is encoded implicitly into model parameters for the
execution of downstream tasks, which makes training inefficient. In addition,
such models are not easily transferable to new tasks with different schemas. In
this work, we propose to perform dialog state tracking grounded on knowledge
encoded externally. We query relevant knowledge of various forms based on the
dialog context where such information can ground the prediction of dialog
states. We demonstrate superior performance of our proposed method over strong
baselines, especially in the few-shot learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller. (arXiv:2210.06694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06694">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new task of sub-event generation for an unseen
process to evaluate the understanding of the coherence of sub-event actions and
objects. To solve the problem, we design SubeventWriter, a sub-event sequence
generation framework with a coherence controller. Given an unseen process, the
framework can iteratively construct the sub-event sequence by generating one
sub-event at each iteration. We also design a very effective coherence
controller to decode more coherent sub-events. As our extensive experiments and
analysis indicate, SubeventWriter can generate more reliable and meaningful
sub-event sequences for unseen processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Reinforced User Simulator and Task-oriented Dialog System with Simplified Generative Architecture. (arXiv:2210.06706v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06706">
<div class="article-summary-box-inner">
<span><p>Recently, there has been progress in supervised funetuning pretrained GPT-2
to build end-to-end task-oriented dialog (TOD) systems. However, online
reinforcement learning of a GPT-2 based dialog system (DS), together with a
end-to-end user simulator (US), has not ever been explored. Moreover, a
drawback with existing GPT-2 based TOD systems is that they mostly employ the
whole dialog history as input, which brings inefficiencies in memory and
compute. In this paper, we first propose Simplified Generative Architectures
(SGA) for DS and US respectively, both based on GPT-2 but using shortened
history. Then, we successfully develop Jointly Reinforced US and DS, called
SGA-JRUD. Our DS with the proposed SGA, when only supervised trained, achieves
state-of-the-art performance on MultiWOZ2.1 and is more compute-efficient in
both training and generation. Extensive experiments on MultiWOZ2.1 further show
the superiority of SGA-JRUD in both offline and online evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Categorizing Semantic Representations for Neural Machine Translation. (arXiv:2210.06709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06709">
<div class="article-summary-box-inner">
<span><p>Modern neural machine translation (NMT) models have achieved competitive
performance in standard benchmarks. However, they have recently been shown to
suffer limitation in compositional generalization, failing to effectively learn
the translation of atoms (e.g., words) and their semantic composition (e.g.,
modification) from seen compounds (e.g., phrases), and thus suffering from
significantly weakened translation performance on unseen compounds during
inference. We address this issue by introducing categorization to the source
contextualized representations. The main idea is to enhance generalization by
reducing sparsity and overfitting, which is achieved by finding prototypes of
token representations over the training set and integrating their embeddings
into the source encoding. Experiments on a dedicated MT dataset (i.e.,
CoGnition) show that our method reduces compositional generalization error
rates by 24\% error reduction. In addition, our conceptually simple method
gives consistently better results than the Transformer baseline on a range of
general MT datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are few(1)-shot Table Reasoners. (arXiv:2210.06710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06710">
<div class="article-summary-box-inner">
<span><p>Recent literature has shown that large language models (LLMs) are generally
excellent few-shot reasoners to solve text reasoning tasks. However, the
capability of LLMs on table reasoning tasks is yet to be explored. In this
paper, we aim at understanding how well LLMs can perform on these table tasks
with few-shot in-context learning. Specifically, we evaluate LLMs on popular
table QA and fact verification datasets like WikiTableQuestion, FetaQA,
TabFact, and FEVEROUS and found that LLMs are really competent at complex
reasoning over table structures. When combined with `chain of thoughts'
prompting, GPT-3 is able to achieve very strong performance with only a 1-shot
demonstration. We further manually study the reasoning chains elicited from
LLMs and found that these reasoning chains are highly consistent with the
`ground truth' semantic form. We believe that our study opens new possibilities
to employ LLMs on different table-based reasoning tasks under few-shot
scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-resource Neural Machine Translation with Cross-modal Alignment. (arXiv:2210.06716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06716">
<div class="article-summary-box-inner">
<span><p>How to achieve neural machine translation with limited parallel data?
Existing techniques often rely on large-scale monolingual corpora, which is
impractical for some low-resource languages. In this paper, we turn to connect
several low-resource languages to a particular high-resource one by additional
visual modality. Specifically, we propose a cross-modal contrastive learning
method to learn a shared space for all languages, where both a coarse-grained
sentence-level objective and a fine-grained token-level one are introduced.
Experimental results and further analysis show that our method can effectively
learn the cross-modal and cross-lingual alignment with a small amount of
image-text pairs and achieves significant improvements over the text-only
baseline under both zero-shot and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIME: Weakly-Supervised Text Classification Without Seeds. (arXiv:2210.06720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06720">
<div class="article-summary-box-inner">
<span><p>In weakly-supervised text classification, only label names act as sources of
supervision. Predominant approaches to weakly-supervised text classification
utilize a two-phase framework, where test samples are first assigned
pseudo-labels and are then used to train a neural text classifier. In most
previous work, the pseudo-labeling step is dependent on obtaining seed words
that best capture the relevance of each class label. We present LIME, a
framework for weakly-supervised text classification that entirely replaces the
brittle seed-word generation process with entailment-based
pseudo-classification. We find that combining weakly-supervised classification
and textual entailment mitigates shortcomings of both, resulting in a more
streamlined and effective classification pipeline. With just an off-the-shelf
textual entailment model, LIME outperforms recent baselines in
weakly-supervised text classification and achieves state-of-the-art in 4
benchmarks. We open source our code at https://github.com/seongminp/LIME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Relational Reasoning via Connection Subgraph Pretraining. (arXiv:2210.06722v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06722">
<div class="article-summary-box-inner">
<span><p>Few-shot knowledge graph (KG) completion task aims to perform inductive
reasoning over the KG: given only a few support triplets of a new relation
$\bowtie$ (e.g., (chop,$\bowtie$,kitchen), (read,$\bowtie$,library), the goal
is to predict the query triplets of the same unseen relation $\bowtie$, e.g.,
(sleep,$\bowtie$,?). Current approaches cast the problem in a meta-learning
framework, where the model needs to be first jointly trained over many training
few-shot tasks, each being defined by its own relation, so that
learning/prediction on the target few-shot task can be effective. However, in
real-world KGs, curating many training tasks is a challenging ad hoc process.
Here we propose Connection Subgraph Reasoner (CSR), which can make predictions
for the target few-shot task directly without the need for pre-training on the
human curated set of training tasks. The key to CSR is that we explicitly model
a shared connection subgraph between support and query triplets, as inspired by
the principle of eliminative induction. To adapt to specific KG, we design a
corresponding self-supervised pretraining scheme with the objective of
reconstructing automatically sampled connection subgraphs. Our pretrained model
can then be directly applied to target few-shot tasks on without the need for
training few-shot tasks. Extensive experiments on real KGs, including NELL,
FB15K-237, and ConceptNet, demonstrate the effectiveness of our framework: we
show that even a learning-free implementation of CSR can already perform
competitively to existing methods on target few-shot tasks; with pretraining,
CSR can achieve significant gains of up to 52% on the more challenging
inductive few-shot tasks where the entities are also unseen during
(pre)training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Out-of-Domain Language Model Performance from Few Examples. (arXiv:2210.06725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06725">
<div class="article-summary-box-inner">
<span><p>While pretrained language models have exhibited impressive generalization
capabilities, they still behave unpredictably under certain domain shifts. In
particular, a model may learn a reasoning process on in-domain training data
that does not hold for out-of-domain test data. We address the task of
predicting out-of-domain (OOD) performance in a few-shot fashion: given a few
target-domain examples and a set of models with similar training performance,
can we understand how these models will perform on OOD test data? We benchmark
the performance on this task when looking at model accuracy on the few-shot
examples, then investigate how to incorporate analysis of the models' behavior
using feature attributions to better tackle this problem. Specifically, we
explore a set of "factors" designed to reveal model agreement with certain
pathological heuristics that may indicate worse generalization capabilities. On
textual entailment, paraphrase recognition, and a synthetic classification
task, we show that attribution-based factors can help rank relative model OOD
performance. However, accuracy on a few-shot test set is a surprisingly strong
baseline, particularly when the system designer does not have in-depth prior
knowledge about the domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanations from Large Language Models Make Small Reasoners Better. (arXiv:2210.06726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06726">
<div class="article-summary-box-inner">
<span><p>Integrating free-text explanations to in-context learning of large language
models (LLM) is shown to elicit strong reasoning capabilities along with
reasonable explanations. In this paper, we consider the problem of leveraging
the explanations generated by LLM to improve the training of small reasoners,
which are more favorable in real-production deployment due to their low cost.
We systematically explore three explanation generation approaches from LLM and
utilize a multi-task learning framework to facilitate small models to acquire
strong reasoning power together with explanation generation capabilities.
Experiments on multiple reasoning tasks show that our method can consistently
and significantly outperform finetuning baselines across different settings,
and even perform better than finetuning/prompting a 60x larger GPT-3 (175B)
model by up to 9.5% in accuracy. As a side benefit, human evaluation further
shows that our method can generate high-quality explanations to justify its
predictions, moving towards the goal of explainable AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries. (arXiv:2210.06741v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06741">
<div class="article-summary-box-inner">
<span><p>In this paper, we show that structures similar to self-attention are natural
to learn many sequence-to-sequence problems from the perspective of symmetry.
Inspired by language processing applications, we study the orthogonal
equivariance of seq2seq functions with knowledge, which are functions taking
two inputs -- an input sequence and a ``knowledge'' -- and outputting another
sequence. The knowledge consists of a set of vectors in the same embedding
space as the input sequence, containing the information of the language used to
process the input sequence. We show that orthogonal equivariance in the
embedding space is natural for seq2seq functions with knowledge, and under such
equivariance the function must take the form close to the self-attention. This
shows that network structures similar to self-attention are the right
structures to represent the target function of many seq2seq problems. The
representation can be further refined if a ``finite information principle'' is
considered, or a permutation equivariance holds for the elements of the input
sequence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shortcomings of Question Answering Based Factuality Frameworks for Error Localization. (arXiv:2210.06748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06748">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in abstractive summarization, models often generate
summaries with factual errors. Numerous approaches to detect these errors have
been proposed, the most popular of which are question answering (QA)-based
factuality metrics. These have been shown to work well at predicting
summary-level factuality and have potential to localize errors within
summaries, but this latter capability has not been systematically evaluated in
past research. In this paper, we conduct the first such analysis and find that,
contrary to our expectations, QA-based frameworks fail to correctly identify
error spans in generated summaries and are outperformed by trivial exact match
baselines. Our analysis reveals a major reason for such poor localization:
questions generated by the QG module often inherit errors from non-factual
summaries which are then propagated further into downstream modules. Moreover,
even human-in-the-loop question generation cannot easily offset these problems.
Our experiments conclusively show that there exist fundamental issues with
localization using the QA framework which cannot be fixed solely by stronger QA
and QG models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Unintended Memorization in Language Models via Alternating Teaching. (arXiv:2210.06772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06772">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that language models have a tendency to memorize
rare or unique sequences in the training corpora which can thus leak sensitive
attributes of user data. We employ a teacher-student framework and propose a
novel approach called alternating teaching to mitigate unintended memorization
in sequential modeling. In our method, multiple teachers are trained on
disjoint training sets whose privacy one wishes to protect, and teachers'
predictions supervise the training of a student model in an alternating manner
at each time step. Experiments on LibriSpeech datasets show that the proposed
method achieves superior privacy-preserving results than other counterparts. In
comparison with no prevention for unintended memorization, the overall utility
loss is small when training records are sufficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re3: Generating Longer Stories With Recursive Reprompting and Revision. (arXiv:2210.06774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06774">
<div class="article-summary-box-inner">
<span><p>We consider the problem of automatically generating longer stories of over
two thousand words. Compared to prior work on shorter stories, long-range plot
coherence and relevance are more central challenges here. We propose the
Recursive Reprompting and Revision framework (Re3) to address these challenges
by (a) prompting a general-purpose language model to construct a structured
overarching plan, and (b) generating story passages by repeatedly injecting
contextual information from both the plan and current story state into a
language model prompt. We then revise by (c) reranking different continuations
for plot coherence and premise relevance, and finally (d) editing the best
continuation for factual consistency. Compared to similar-length stories
generated directly from the same base model, human evaluators judged
substantially more of Re3's stories as having a coherent overarching plot (by
14% absolute increase), and relevant to the given initial premise (by 20%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closed-book Question Generation via Contrastive Learning. (arXiv:2210.06781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06781">
<div class="article-summary-box-inner">
<span><p>Question Generation (QG) is a fundamental NLP task for many downstream
applications. Recent studies on open-book QG, where supportive question-context
pairs are provided to models, have achieved promising progress. However,
generating natural questions under a more practical closed-book setting that
lacks these supporting documents still remains a challenge. In this work, to
learn better representations from semantic information hidden in
question-answer pairs under the closed-book setting, we propose a new QG model
empowered by a contrastive learning module and an answer reconstruction module.
We present a new closed-book QA dataset -- WikiCQA involving abstractive long
answers collected from a wiki-style website. In the experiments, we validate
the proposed QG model on both public datasets and the new WikiCQA dataset.
Empirical results show that the proposed QG model outperforms baselines in both
automatic evaluation and human evaluation. In addition, we show how to leverage
the proposed model to improve existing closed-book QA systems. We observe that
by pre-training a closed-book QA model on our generated synthetic QA pairs,
significant QA improvement can be achieved on both seen and unseen datasets,
which further demonstrates the effectiveness of our QG model for enhancing
unsupervised and semi-supervised QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous American Sign Language. (arXiv:2210.06791v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06791">
<div class="article-summary-box-inner">
<span><p>Despite tremendous progress in natural language processing using deep
learning techniques in recent years, sign language production and comprehension
has advanced very little. One critical barrier is the lack of largescale
datasets available to the public due to the unbearable cost of labeled data
generation. Efforts to provide public data for American Sign Language (ASL)
comprehension have yielded two datasets, comprising more than thousand video
clips. These datasets are large enough to enable a meaningful start to deep
learning research on sign languages but are far too small to lead to any
solution that can be practically deployed. So far, there is still no suitable
dataset for ASL production. We proposed a system that can generate large scale
ASL datasets for continuous ASL. It is suitable for general ASL processing and
is particularly useful for ASL production. The continuous ASL dataset contains
English labeled human articulations in condensed body pose data formats. To
better serve the research community, we are releasing the first version of our
ASL dataset, which contains 30k sentences, 416k words, a vocabulary of 18k
words, in a total of 104 hours. This is the largest continuous sign language
dataset published to date in terms of video duration. We also describe a system
that can evolve and expand the dataset to incorporate better data processing
techniques and more contents when available. It is our hope that the release of
this ASL dataset and the sustainable dataset generation system to the public
will propel better deep-learning research in ASL natural language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Long-tail Generalization with Likelihood Splits. (arXiv:2210.06799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06799">
<div class="article-summary-box-inner">
<span><p>In order to reliably process natural language, NLP systems must generalize to
the long tail of rare utterances. We propose a method to create challenging
benchmarks that require generalizing to the tail of the distribution by
re-splitting existing datasets. We create 'Likelihood splits' where examples
that are assigned lower likelihood by a pre-trained language model (LM) are
placed in the test set, and more likely examples are in the training set. This
simple approach can be customized to construct meaningful train-test splits for
a wide range of tasks. Likelihood splits are more challenging than random
splits: relative error rates of state-of-the-art models on our splits increase
by 59% for semantic parsing on Spider, 77% for natural language inference on
SNLI, and 38% for yes/no question answering on BoolQ compared with the
corresponding random splits. Moreover, Likelihood splits create fairer
benchmarks than adversarial filtering; when the LM used to create the splits is
used as the task model, our splits do not adversely penalize the LM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Finding Spans. (arXiv:2210.06824v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06824">
<div class="article-summary-box-inner">
<span><p>We present an empirical study on methods for span finding, the selection of
consecutive tokens in text for some downstream tasks. We focus on approaches
that can be employed in training end-to-end information extraction systems. We
recognize there is no silver bullet that can simply solve all downstream tasks
well without considering task properties and provide our observations to help
with design choices in the future: 1) tagging method usually yields a higher
precision while span enumeration and boundary prediction prefer a higher
recall; 2) span type information can benefit boundary prediction approach; 3)
additional contextualization does not help span finding in most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Annotation: Can Language Learners Contribute?. (arXiv:2210.06828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06828">
<div class="article-summary-box-inner">
<span><p>Researchers have traditionally recruited native speakers to provide
annotations for the widely used benchmark datasets. But there are languages for
which recruiting native speakers is difficult, and it would help to get
learners of those languages to annotate the data. In this paper, we investigate
whether language learners can contribute annotations to the benchmark datasets.
In a carefully controlled annotation experiment, we recruit 36 language
learners, provide two types of additional resources (dictionaries and
machine-translated sentences), and perform mini-tests to measure their language
proficiency. We target three languages, English, Korean, and Indonesian, and
four NLP tasks, sentiment analysis, natural language inference, named entity
recognition, and machine reading comprehension. We find that language learners,
especially those with intermediate or advanced language proficiency, are able
to provide fairly accurate labels with the help of additional resources.
Moreover, we show that data annotation improves learners' language proficiency
in terms of vocabulary and grammar. The implication of our findings is that
broadening the annotation task to include language learners can open up the
opportunity to build benchmark datasets for languages for which it is difficult
to recruit native speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Creation via Anchored Regularization for Unsupervised Aspect Extraction. (arXiv:2210.06829v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06829">
<div class="article-summary-box-inner">
<span><p>Aspect Based Sentiment Analysis is the most granular form of sentiment
analysis that can be performed on the documents / sentences. Besides delivering
the most insights at a finer grain, it also poses equally daunting challenges.
One of them being the shortage of labelled data. To bring in value right out of
the box for the text data being generated at a very fast pace in today's world,
unsupervised aspect-based sentiment analysis allows us to generate insights
without investing time or money in generating labels. From topic modelling
approaches to recent deep learning-based aspect extraction models, this domain
has seen a lot of development. One of the models that we improve upon is ABAE
that reconstructs the sentences as a linear combination of aspect terms present
in it, In this research we explore how we can use information from another
unsupervised model to regularize ABAE, leading to better performance. We
contrast it with baseline rule based ensemble and show that the ensemble
methods work better than the individual models and the regularization based
ensemble performs better than the rule-based one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of BioASQ 2022: The tenth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2210.06852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06852">
<div class="article-summary-box-inner">
<span><p>This paper presents an overview of the tenth edition of the BioASQ challenge
in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2022.
BioASQ is an ongoing series of challenges that promotes advances in the domain
of large-scale biomedical semantic indexing and question answering. In this
edition, the challenge was composed of the three established tasks a, b, and
Synergy, and a new task named DisTEMIST for automatic semantic annotation and
grounding of diseases from clinical content in Spanish, a key concept for
semantic indexing and search engines of literature and clinical records. This
year, BioASQ received more than 170 distinct systems from 38 teams in total for
the four different tasks of the challenge. As in previous years, the majority
of the competing systems outperformed the strong baselines, indicating the
continuous advancement of the state-of-the-art in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CS-Insights: A System for Analyzing Computer Science Research. (arXiv:2210.06878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06878">
<div class="article-summary-box-inner">
<span><p>This paper presents CS-Insights, an interactive web application to analyze
computer science publications from DBLP through multiple perspectives. The
dedicated interfaces allow its users to identify trends in research activity,
productivity, accessibility, author's productivity, venues' statistics, topics
of interest, and the impact of computer science research on other fields.
CS-Insightsis publicly available, and its modular architecture can be easily
adapted to domains other than computer science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithms for Weighted Pushdown Automata. (arXiv:2210.06884v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06884">
<div class="article-summary-box-inner">
<span><p>Weighted pushdown automata (WPDAs) are at the core of many natural language
processing tasks, like syntax-based statistical machine translation and
transition-based dependency parsing. As most existing dynamic programming
algorithms are designed for context-free grammars (CFGs), algorithms for PDAs
often resort to a PDA-to-CFG conversion. In this paper, we develop novel
algorithms that operate directly on WPDAs. Our algorithms are inspired by
Lang's algorithm, but use a more general definition of pushdown automaton and
either reduce the space requirements by a factor of $|\Gamma|$ (the size of the
stack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the
number of states). When run on the same class of PDAs as Lang's algorithm, our
algorithm is both more space-efficient by a factor of $|\Gamma|$ and more
time-efficient by a factor of $|Q| \cdot |\Gamma|$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evaluation of the Plausibility and Faithfulness of Sentiment Analysis Explanations. (arXiv:2210.06916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06916">
<div class="article-summary-box-inner">
<span><p>Current Explainable AI (ExAI) methods, especially in the NLP field, are
conducted on various datasets by employing different metrics to evaluate
several aspects. The lack of a common evaluation framework is hindering the
progress tracking of such methods and their wider adoption. In this work,
inspired by offline information retrieval, we propose different metrics and
techniques to evaluate the explainability of SA models from two angles. First,
we evaluate the strength of the extracted "rationales" in faithfully explaining
the predicted outcome. Second, we measure the agreement between ExAI methods
and human judgment on a homegrown dataset1 to reflect on the rationales
plausibility. Our conducted experiments comprise four dimensions: (1) the
underlying architectures of SA models, (2) the approach followed by the ExAI
method, (3) the reasoning difficulty, and (4) the homogeneity of the
ground-truth rationales. We empirically demonstrate that anchors explanations
are more aligned with the human judgment and can be more confident in
extracting supporting rationales. As can be foreseen, the reasoning complexity
of sentiment is shown to thwart ExAI methods from extracting supporting
evidence. Moreover, a remarkable discrepancy is discerned between the results
of different explainability methods on the various architectures suggesting the
need for consolidation to observe enhanced performance. Predominantly,
transformers are shown to exhibit better explainability than convolutional and
recurrent architectures. Our work paves the way towards designing more
interpretable NLP models and enabling a common evaluation ground for their
relative strengths and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automotive Multilingual Fault Diagnosis. (arXiv:2210.06918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06918">
<div class="article-summary-box-inner">
<span><p>Automated fault diagnosis can facilitate diagnostics assistance, speedier
troubleshooting, and better-organised logistics. Currently, AI-based
prognostics and health management in the automotive industry ignore the textual
descriptions of the experienced problems or symptoms. With this study, however,
we show that a multilingual pre-trained Transformer can effectively classify
the textual claims from a large company with vehicle fleets, despite the task's
challenging nature due to the 38 languages and 1,357 classes involved. Overall,
we report an accuracy of more than 80% for high-frequency classes and above 60%
for above-low-frequency classes, bringing novel evidence that multilingual
classification can benefit automotive troubleshooting management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Ambiguity, Grammaticality and Complexity Probes. (arXiv:2210.06928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06928">
<div class="article-summary-box-inner">
<span><p>It is unclear whether, how and where large pre-trained language models
capture subtle linguistic traits like ambiguity, grammaticality and sentence
complexity. We present results of automatic classification of these traits and
compare their viability and patterns across representation types. We
demonstrate that template-based datasets with surface-level artifacts should
not be used for probing, careful comparisons with baselines should be done and
that t-SNE plots should not be used to determine the presence of a feature
among dense vectors representations. We also show how features might be highly
localized in the layers for these models and get lost in the upper layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Explainability of Natural Language Processing Deep Models. (arXiv:2210.06929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06929">
<div class="article-summary-box-inner">
<span><p>While there has been a recent explosion of work on ExplainableAI ExAI on deep
models that operate on imagery and tabular data, textual datasets present new
challenges to the ExAI community. Such challenges can be attributed to the lack
of input structure in textual data, the use of word embeddings that add to the
opacity of the models and the difficulty of the visualization of the inner
workings of deep models when they are trained on textual data.
</p>
<p>Lately, methods have been developed to address the aforementioned challenges
and present satisfactory explanations on Natural Language Processing (NLP)
models. However, such methods are yet to be studied in a comprehensive
framework where common challenges are properly stated and rigorous evaluation
practices and metrics are proposed. Motivated to democratize ExAI methods in
the NLP field, we present in this work a survey that studies model-agnostic as
well as model-specific explainability methods on NLP models. Such methods can
either develop inherently interpretable NLP models or operate on pre-trained
models in a post-hoc manner. We make this distinction and we further decompose
the methods into three categories according to what they explain: (1) word
embeddings (input-level), (2) inner workings of NLP models (processing-level)
and (3) models' decisions (output-level). We also detail the different
evaluation approaches interpretability methods in the NLP field. Finally, we
present a case-study on the well-known neural machine translation in an
appendix and we propose promising future research directions for ExAI in the
NLP field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Zero Resource Speech Recognition Base on Self-Supervise Pre-Trained Acoustic Models. (arXiv:2210.06936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06936">
<div class="article-summary-box-inner">
<span><p>Labeled audio data is insufficient to build satisfying speech recognition
systems for most of the languages in the world. There have been some
zero-resource methods trying to perform phoneme or word-level speech
recognition without labeled audio data of the target language, but the error
rate of these methods is usually too high to be applied in real-world
scenarios. Recently, the representation ability of self-supervise pre-trained
models has been found to be extremely beneficial in zero-resource phoneme
recognition. As far as we are concerned, this paper is the first attempt to
extend the use of pre-trained models into word-level zero-resource speech
recognition. This is done by fine-tuning the pre-trained models on IPA phoneme
transcriptions and decoding with a language model trained on extra texts.
Experiments on Wav2vec 2.0 and HuBERT models show that this method can achieve
less than 20% word error rate on some languages, and the average error rate on
8 languages is 33.77%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differential Bias: On the Perceptibility of Stance Imbalance in Argumentation. (arXiv:2210.06970v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06970">
<div class="article-summary-box-inner">
<span><p>Most research on natural language processing treats bias as an absolute
concept: Based on a (probably complex) algorithmic analysis, a sentence, an
article, or a text is classified as biased or not. Given the fact that for
humans the question of whether a text is biased can be difficult to answer or
is answered contradictory, we ask whether an "absolute bias classification" is
a promising goal at all. We see the problem not in the complexity of
interpreting language phenomena but in the diversity of sociocultural
backgrounds of the readers, which cannot be handled uniformly: To decide
whether a text has crossed the proverbial line between non-biased and biased is
subjective. By asking "Is text X more [less, equally] biased than text Y?" we
propose to analyze a simpler problem, which, by its construction, is rather
independent of standpoints, views, or sociocultural aspects. In such a model,
bias becomes a preference relation that induces a partial ordering from least
biased to most biased texts without requiring a decision on where to draw the
line. A prerequisite for this kind of bias model is the ability of humans to
perceive relative bias differences in the first place. In our research, we
selected a specific type of bias in argumentation, the stance bias, and
designed a crowdsourcing study showing that differences in stance bias are
perceptible when (light) support is provided through training or visual aid.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tone prediction and orthographic conversion for Basaa. (arXiv:2210.06986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06986">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a seq2seq approach for transliterating missionary
Basaa orthographies into the official orthography. Our model uses pre-trained
Basaa missionary and official orthography corpora using BERT. Since Basaa is a
low-resource language, we have decided to use the mT5 model for our project.
Before training our model, we pre-processed our corpora by eliminating
one-to-one correspondences between spellings and unifying characters variably
containing either one to two characters into single-character form. Our best
mT5 model achieved a CER equal to 12.6747 and a WER equal to 40.1012.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text. (arXiv:2210.06990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06990">
<div class="article-summary-box-inner">
<span><p>Data sparsity is one of the main challenges posed by Code-switching (CS),
which is further exacerbated in the case of morphologically rich languages. For
the task of Machine Translation (MT), morphological segmentation has proven
successful in alleviating data sparsity in monolingual contexts; however, it
has not been investigated for CS settings. In this paper, we study the
effectiveness of different segmentation approaches on MT performance, covering
morphology-based and frequency-based segmentation techniques. We experiment on
MT from code-switched Arabic-English to English. We provide detailed analysis,
examining a variety of conditions, such as data size and sentences with
different degrees in CS. Empirical results show that morphology-aware
segmenters perform the best in segmentation tasks but under-perform in MT.
Nevertheless, we find that the choice of the segmentation setup to use for MT
is highly dependent on the data size. For extreme low-resource scenarios, a
combination of frequency and morphology-based segmentations is shown to perform
the best. For more resourced settings, such a combination does not bring
significant improvements over the use of frequency-based segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06996">
<div class="article-summary-box-inner">
<span><p>Domain-specific neural machine translation (NMT) systems (e.g., in
educational applications) are socially significant with the potential to help
make information accessible to a diverse set of users in multilingual
societies. It is desirable that such NMT systems be lexically constrained and
draw from domain-specific dictionaries. Dictionaries could present multiple
candidate translations for a source words/phrases on account of the polysemous
nature of words. The onus is then on the NMT model to choose the contextually
most appropriate candidate. Prior work has largely ignored this problem and
focused on the single candidate setting where the target word or phrase is
replaced by a single constraint. In this work we present DICTDIS, a lexically
constrained NMT system that disambiguates between multiple candidate
translations derived from dictionaries. We achieve this by augmenting training
data with multiple dictionary candidates to actively encourage disambiguation
during training. We demonstrate the utility of DICTDIS via extensive
experiments on English-Hindi sentences in a variety of domains including news,
finance, medicine and engineering. We obtain superior disambiguation
performance on all domains with improved fluency in some domains of up to 4
BLEU points, when compared with existing approaches for lexically constrained
and unconstrained NMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy. (arXiv:2210.07002v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07002">
<div class="article-summary-box-inner">
<span><p>In order to protect the privacy of speech data, speaker anonymization aims
for hiding the identity of a speaker by changing the voice in speech
recordings. This typically comes with a privacy-utility trade-off between
protection of individuals and usability of the data for downstream
applications. One of the challenges in this context is to create non-existent
voices that sound as natural as possible.
</p>
<p>In this work, we propose to tackle this issue by generating speaker
embeddings using a generative adversarial network with Wasserstein distance as
cost function. By incorporating these artificial embeddings into a
speech-to-text-to-speech pipeline, we outperform previous approaches in terms
of privacy and utility. According to standard objective metrics and human
evaluation, our approach generates intelligible and content-preserving yet
privacy-protecting versions of the original recordings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ComSearch: Equation Searching with Combinatorial Strategy for Solving Math Word Problems with Weak Supervision. (arXiv:2210.07017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07017">
<div class="article-summary-box-inner">
<span><p>Previous studies have introduced a weakly-supervised paradigm for solving
math word problems requiring only the answer value annotation. While these
methods search for correct value equation candidates as pseudo labels, they
search among a narrow sub-space of the enormous equation space. To address this
problem, we propose a novel search algorithm with combinatorial strategy
\textbf{ComSearch}, which can compress the search space by excluding
mathematically equivalent equations. The compression allows the searching
algorithm to enumerate all possible equations and obtain high-quality data. We
investigate the noise in the pseudo labels that hold wrong mathematical logic,
which we refer to as the \textit{false-matching} problem, and propose a ranking
model to denoise the pseudo labels. Our approach holds a flexible framework to
utilize two existing supervised math word problem solvers to train pseudo
labels, and both achieve state-of-the-art performance in the weak supervision
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation. (arXiv:2210.07022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07022">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) suffers from the scarcity of annotated
training data, especially for low-resource languages without labeled data.
Cross-lingual NER has been proposed to alleviate this issue by transferring
knowledge from high-resource languages to low-resource languages via aligned
cross-lingual representations or machine translation results. However, the
performance of cross-lingual NER methods is severely affected by the
unsatisfactory quality of translation or label projection. To address these
problems, we propose a Cross-lingual Entity Projection framework (CROP) to
enable zero-shot cross-lingual NER with the help of a multilingual labeled
sequence translation model. Specifically, the target sequence is first
translated into the source language and then tagged by a source NER model. We
further adopt a labeled sequence translation model to project the tagged
sequence back to the target language and label the target raw sentence.
Ultimately, the whole pipeline is integrated into an end-to-end model by the
way of self-training. Experimental results on two benchmarks demonstrate that
our method substantially outperforms the previous strong baseline by a large
margin of +3~7 F1 scores and achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition. (arXiv:2210.07032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07032">
<div class="article-summary-box-inner">
<span><p>Due to the absence of connectives, implicit discourse relation recognition
(IDRR) is still a challenging and crucial task in discourse analysis. Most of
the current work adopted multitask learning to aid IDRR through explicit
discourse relation recognition (EDRR) or utilized dependencies between
discourse relation labels to constrain model predictions. But these methods
still performed poorly on fine-grained IDRR and even utterly misidentified on
most of the few-shot discourse relation classes. To address these problems, we
propose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our
method instructs large-scale pre-trained models to use knowledge relevant to
discourse relation and utilizes the strong correlation between connectives and
discourse relation to help the model recognize implicit discourse relations.
Experimental results show that our method surpasses the current
state-of-the-art model and achieves significant improvements on those
fine-grained few-shot discourse relation. Moreover, our approach is able to be
transferred to EDRR and obtain acceptable results. Our code is released in
https://github.com/zh-i9/PCP-for-IDRR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spontaneous Emerging Preference in Two-tower Language Model. (arXiv:2210.07041v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07041">
<div class="article-summary-box-inner">
<span><p>The ever-growing size of the foundation language model has brought
significant performance gains in various types of downstream tasks. With the
existence of side-effects brought about by the large size of the foundation
language model such as deployment cost, availability issues, and environmental
cost, there is some interest in exploring other possible directions, such as a
divide-and-conquer scheme. In this paper, we are asking a basic question: are
language processes naturally dividable? We study this problem with a simple
two-tower language model setting, where two language models with identical
configurations are trained side-by-side cooperatively. With this setting, we
discover the spontaneous emerging preference phenomenon, where some of the
tokens are consistently better predicted by one tower while others by another
tower. This phenomenon is qualitatively stable, regardless of model
configuration and type, suggesting this as an intrinsic property of natural
language. This study suggests that interesting properties of natural language
are still waiting to be discovered, which may aid the future development of
natural language processing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation. (arXiv:2210.07054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07054">
<div class="article-summary-box-inner">
<span><p>Sign language gloss translation aims to translate the sign glosses into
spoken language texts, which is challenging due to the scarcity of labeled
gloss-text parallel data. Back translation (BT), which generates
pseudo-parallel data by translating in-domain spoken language texts into sign
glosses, has been applied to alleviate the data scarcity problem. However, the
lack of large-scale high-quality domain spoken language text data limits the
effect of BT. In this paper, to overcome the limitation, we propose a Prompt
based domain text Generation (PGEN) approach to produce the large-scale
in-domain spoken language text data. Specifically, PGEN randomly concatenates
sentences from the original in-domain spoken language text data as prompts to
induce a pre-trained language model (i.e., GPT-2) to generate spoken language
texts in a similar style. Experimental results on three benchmarks of sign
language gloss translation in varied languages demonstrate that BT with spoken
language texts generated by PGEN significantly outperforms the compared
methods. In addition, as the scale of spoken language texts generated by PGEN
increases, the BT technique can achieve further improvements, demonstrating the
effectiveness of our approach. We release the code and data for facilitating
future research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open-World Lottery Ticket for Out-of-Domain Intent Classification. (arXiv:2210.07071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07071">
<div class="article-summary-box-inner">
<span><p>Most existing methods of Out-of-Domain (OOD) intent classification, which
rely on extensive auxiliary OOD corpora or specific training paradigms, are
underdeveloped in the underlying principle that the models should have
differentiated confidence in In- and Out-of-domain intent. In this work, we
demonstrate that calibrated subnetworks can be uncovered by pruning the
(poor-calibrated) overparameterized model. Calibrated confidence provided by
the subnetwork can better distinguish In- and Out-of-domain. Furthermore, we
theoretically bring new insights into why temperature scaling can differentiate
In- and Out-of-Domain intent and empirically extend the Lottery Ticket
Hypothesis to the open-world setting. Extensive experiments on three real-world
datasets demonstrate our approach can establish consistent improvements
compared with a suite of competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing. (arXiv:2210.07074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07074">
<div class="article-summary-box-inner">
<span><p>A bottleneck to developing Semantic Parsing (SP) models is the need for a
large volume of human-labeled training data. Given the complexity and cost of
human annotation for SP, labeled data is often scarce, particularly in
multilingual settings. Large Language Models (LLMs) excel at SP given only a
few examples, however LLMs are unsuitable for runtime systems which require low
latency. In this work, we propose CLASP, a simple method to improve
low-resource SP for moderate-sized models: we generate synthetic data from
AlexaTM 20B to augment the training set for a model 40x smaller (500M
parameters). We evaluate on two datasets in low-resource settings: English
PIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual
zero-shot, where training data is available only in English, and the model must
generalize to four new languages. On both datasets, we show significant
improvements over strong baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Expansion Using Contextual Clue Sampling with Language Models. (arXiv:2210.07093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07093">
<div class="article-summary-box-inner">
<span><p>Query expansion is an effective approach for mitigating vocabulary mismatch
between queries and documents in information retrieval. One recent line of
research uses language models to generate query-related contexts for expansion.
Along this line, we argue that expansion terms from these contexts should
balance two key aspects: diversity and relevance. The obvious way to increase
diversity is to sample multiple contexts from the language model. However, this
comes at the cost of relevance, because there is a well-known tendency of
models to hallucinate incorrect or irrelevant contexts. To balance these two
considerations, we propose a combination of an effective filtering strategy and
fusion of the retrieved documents based on the generation probability of each
context. Our lexical matching based approach achieves a similar top-5/top-20
retrieval accuracy and higher top-100 accuracy compared with the
well-established dense retrieval model DPR, while reducing the index size by
more than 96%. For end-to-end QA, the reader model also benefits from our
method and achieves the highest Exact-Match score against several competitive
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Context into Subword Vocabularies. (arXiv:2210.07095v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07095">
<div class="article-summary-box-inner">
<span><p>Most current popular subword tokenizers are trained based on word frequency
statistics over a corpus, without considering information about co-occurrence
or context. Nevertheless, the resulting vocabularies are used in language
models' highly contextualized settings. We present SaGe, a tokenizer that
tailors subwords for their downstream use by baking in the contextualized
signal at the vocabulary creation phase. We show that SaGe does a better job
than current widespread tokenizers in keeping token contexts cohesive, while
not incurring a large price in terms of encoding efficiency or domain
robustness. SaGe improves performance on English GLUE classification tasks as
well as on NER, and on Inference and NER in Turkish, demonstrating its
robustness to language properties such as morphological exponence and
agglutination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence. (arXiv:2210.07109v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07109">
<div class="article-summary-box-inner">
<span><p>AI researchers have posited Dungeons and Dragons (D&amp;D) as a challenge problem
to test systems on various language-related capabilities. In this paper, we
frame D&amp;D specifically as a dialogue system challenge, where the tasks are to
both generate the next conversational turn in the game and predict the state of
the game given the dialogue history. We create a gameplay dataset consisting of
nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns,
500,000 dice rolls, and 58 million words. We automatically annotate the data
with partial state information about the game play. We train a large language
model (LM) to generate the next game turn, conditioning it on different
information. The LM can respond as a particular character or as the player who
runs the game--i.e., the Dungeon Master (DM). It is trained to produce dialogue
that is either in-character (roleplaying in the fictional world) or
out-of-character (discussing rules or strategy). We perform a human evaluation
to determine what factors make the generated output plausible and interesting.
We further perform an automatic evaluation to determine how well the model can
predict the game state given the history and examine how well tracking the game
state improves its ability to produce plausible conversational output.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models. (arXiv:2210.07111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07111">
<div class="article-summary-box-inner">
<span><p>Recent work on tokenizer-free multilingual pretrained models show promising
results in improving cross-lingual transfer and reducing engineering overhead
(Clark et al., 2022; Xue et al., 2022). However, these works mainly focus on
reporting accuracy on a limited set of tasks and data settings, placing less
emphasis on other important factors when tuning and deploying the models in
practice, such as memory usage, inference speed, and fine-tuning data
robustness. We attempt to fill this gap by performing a comprehensive empirical
comparison of multilingual tokenizer-free and subword-based models considering
these various dimensions. Surprisingly, we find that subword-based models might
still be the most practical choice in many settings, achieving better
performance for lower inference latency and memory usage. Based on these
results, we encourage future work in tokenizer-free methods to consider these
factors when designing and evaluating new models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-End Open Conversational Machine Reading. (arXiv:2210.07113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07113">
<div class="article-summary-box-inner">
<span><p>In open-retrieval conversational machine reading (OR-CMR) task, machines are
required to do multi-turn question answering given dialogue history and a
textual knowledge base. Existing works generally utilize two independent
modules to approach this problem's two successive sub-tasks: first with a
hard-label decision making and second with a question generation aided by
various entailment reasoning methods. Such usual cascaded modeling is
vulnerable to error propagation and prevents the two sub-tasks from being
consistently optimized. In this work, we instead model OR-CMR as a unified
text-to-text task in a fully end-to-end style. Experiments on the OR-ShARC
dataset show the effectiveness of our proposed end-to-end framework on both
sub-tasks by a large margin, achieving new state-of-the-art results. Further
ablation studies support that our framework can generalize to different
backbone models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How (Not) To Evaluate Explanation Quality. (arXiv:2210.07126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07126">
<div class="article-summary-box-inner">
<span><p>The importance of explainability is increasingly acknowledged in natural
language processing. However, it is still unclear how the quality of
explanations can be assessed effectively. The predominant approach is to
compare proxy scores (such as BLEU or explanation F1) evaluated against gold
explanations in the dataset. The assumption is that an increase of the proxy
score implies a higher utility of explanations to users. In this paper, we
question this assumption. In particular, we (i) formulate desired
characteristics of explanation quality that apply across tasks and domains,
(ii) point out how current evaluation practices violate those characteristics,
and (iii) propose actionable guidelines to overcome obstacles that limit
today's evaluation of explanation quality and to enable the development of
explainable systems that provide tangible benefits for human users. We
substantiate our theoretical claims (i.e., the lack of validity and temporal
decline of currently-used proxy scores) with empirical evidence from a
crowdsourcing case study in which we investigate the explanation quality of
state-of-the-art explainable question answering systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models of Code are Few-Shot Commonsense Learners. (arXiv:2210.07128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07128">
<div class="article-summary-box-inner">
<span><p>We address the general task of structured commonsense reasoning: given a
natural language input, the goal is to generate a graph such as an event -- or
a reasoning-graph. To employ large language models (LMs) for this task,
existing approaches ``serialize'' the output graph as a flat list of nodes and
edges. Although feasible, these serialized graphs strongly deviate from the
natural language corpora that LMs were pre-trained on, hindering LMs from
generating them correctly. In this paper, we show that when we instead frame
structured commonsense reasoning tasks as code generation tasks, pre-trained
LMs of code are better structured commonsense reasoners than LMs of natural
language, even when the downstream task does not involve source code at all. We
demonstrate our approach across three diverse structured commonsense reasoning
tasks. In all these natural language tasks, we show that using our approach, a
code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the
target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models. (arXiv:2210.07135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07135">
<div class="article-summary-box-inner">
<span><p>Multilingual models have been widely used for cross-lingual transfer to
low-resource languages. However, the performance on these languages is hindered
by their underrepresentation in the pretraining data. To alleviate this
problem, we propose a novel multilingual training technique based on
teacher-student knowledge distillation. In this setting, we utilize monolingual
teacher models optimized for their language. We use those teachers along with
balanced (sub-sampled) data to distill the teachers' knowledge into a single
multilingual student. Our method outperforms standard training methods in
low-resource languages and retrains performance on high-resource languages
while using the same amount of data. If applied widely, our approach can
increase the representation of low-resource languages in NLP systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning. (arXiv:2210.07138v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07138">
<div class="article-summary-box-inner">
<span><p>Multi-hop QA requires reasoning over multiple supporting facts to answer the
question. However, the existing QA models always rely on shortcuts, e.g.,
providing the true answer by only one fact, rather than multi-hop reasoning,
which is referred as $\textit{disconnected reasoning}$ problem. To alleviate
this issue, we propose a novel counterfactual multihop QA, a causal-effect
approach that enables to reduce the disconnected reasoning. It builds upon
explicitly modeling of causality: 1) the direct causal effects of disconnected
reasoning and 2) the causal effect of true multi-hop reasoning from the total
causal effect. With the causal graph, a counterfactual inference is proposed to
disentangle the disconnected reasoning from the total causal effect, which
provides us a new perspective and technology to learn a QA model that exploits
the true multi-hop reasoning instead of shortcuts. Extensive experiments have
conducted on the benchmark HotpotQA dataset, which demonstrate that the
proposed method can achieve notable improvement on reducing disconnected
reasoning. For example, our method achieves 5.8% higher points of its Supp$_s$
score on HotpotQA through true multihop reasoning. The code is available at
supplementary material.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Translating between Ancient Chinese and Contemporary Chinese with Limited Aligned Corpora. (arXiv:1803.01557v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1803.01557">
<div class="article-summary-box-inner">
<span><p>The Chinese language has evolved a lot during the long-term development.
Therefore, native speakers now have trouble in reading sentences written in
ancient Chinese. In this paper, we propose to build an end-to-end neural model
to automatically translate between ancient and contemporary Chinese. However,
the existing ancient-contemporary Chinese parallel corpora are not aligned at
the sentence level and sentence-aligned corpora are limited, which makes it
difficult to train the model. To build the sentence level parallel training
data for the model, we propose an unsupervised algorithm that constructs
sentence-aligned ancient-contemporary pairs by using the fact that the aligned
sentence pair shares many of the tokens. Based on the aligned corpus, we
propose an end-to-end neural model with copying mechanism and local attention
to translate between ancient and contemporary Chinese. Experiments show that
the proposed unsupervised algorithm achieves 99.4% F1 score for sentence
alignment, and the translation model achieves 26.95 BLEU from ancient to
contemporary, and 36.34 BLEU from contemporary to ancient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05502">
<div class="article-summary-box-inner">
<span><p>While a large body of work has scrutinized the meaning of conditional
sentences, considerably less attention has been paid to formal models of their
pragmatic use and interpretation. Here, we take a probabilistic approach to
pragmatic reasoning about indicative conditionals which flexibly integrates
gradient beliefs about richly structured world states. We model listeners'
update of their prior beliefs about the causal structure of the world and the
joint probabilities of the consequent and antecedent based on assumptions about
the speaker's utterance production protocol. We show that, when supplied with
natural contextual assumptions, our model uniformly explains a number of
inferences attested in the literature, including epistemic inferences,
conditional perfection and the dependency between antecedent and consequent of
a conditional. We argue that this approach also helps explain three puzzles
introduced by Douven (2012) about updating with conditionals: depending on the
utterance context, the listener's belief in the antecedent may increase,
decrease or remain unchanged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Graphical Conventions in a Visual Communication Game. (arXiv:2111.14210v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14210">
<div class="article-summary-box-inner">
<span><p>Humans communicate with graphical sketches apart from symbolic languages.
Primarily focusing on the latter, recent studies of emergent communication
overlook the sketches; they do not account for the evolution process through
which symbolic sign systems emerge in the trade-off between iconicity and
symbolicity. In this work, we take the very first step to model and simulate
this process via two neural agents playing a visual communication game; the
sender communicates with the receiver by sketching on a canvas. We devise a
novel reinforcement learning method such that agents are evolved jointly
towards successful communication and abstract graphical conventions. To inspect
the emerged conventions, we define three fundamental properties -- iconicity,
symbolicity, and semanticity -- and design evaluation methods accordingly. Our
experimental results under different controls are consistent with the
observation in studies of human graphical conventions. Of note, we find that
evolved sketches can preserve the continuum of semantics under proper
environmental pressures. More interestingly, co-evolved agents can switch
between conventionalized and iconic communication based on their familiarity
with referents. We hope the present research can pave the path for studying
emergent communication with the modality of sketches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Linking: Grounding Event Mentions to Wikipedia. (arXiv:2112.07888v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07888">
<div class="article-summary-box-inner">
<span><p>Comprehending an article requires understanding its constituent events.
However, the context where an event is mentioned often lacks the details of
this event. A question arises: how can the reader obtain more knowledge about
this particular event in addition to what is provided by the local context in
the article?
</p>
<p>This work defines Event Linking, a new natural language understanding task at
the event level. Event linking tries to link an event mention appearing in an
article to the most appropriate Wikipedia page. This page is expected to
provide rich knowledge about what the event mention refers to. To standardize
the research in this new direction, we contribute in four-fold. First, this is
the first work in the community that formally defines Event Linking task.
Second, we collect a dataset for this new task. Specifically, we automatically
gather training set from Wikipedia, and then create two evaluation sets: one
from the Wikipedia domain, reporting the in-domain performance, and a second
from the real-world news domain, to evaluate out-of-domain performance. Third,
we retrain and evaluate two state-of-the-art (SOTA) entity linking models,
showing the challenges of event linking, and we propose an event-specific
linking system EVELINK to set a competitive result for the new task. Fourth, we
conduct a detailed and insightful analysis to help understand the task and the
limitation of the current model. Overall, as our analysis shows, Event Linking
is a considerably challenging and essential task requiring more effort from the
community. Data and code are available here:
https://github.com/CogComp/event-linking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptBERT: Improving BERT Sentence Embeddings with Prompts. (arXiv:2201.04337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04337">
<div class="article-summary-box-inner">
<span><p>We propose PromptBERT, a novel contrastive learning method for learning
better sentence representation. We firstly analyze the drawback of current
sentence embedding from original BERT and find that it is mainly due to the
static token embedding bias and ineffective BERT layers. Then we propose the
first prompt-based sentence embeddings method and discuss two prompt
representing methods and three prompt searching methods to make BERT achieve
better sentence embeddings. Moreover, we propose a novel unsupervised training
objective by the technology of template denoising, which substantially shortens
the performance gap between the supervised and unsupervised settings. Extensive
experiments show the effectiveness of our method. Compared to SimCSE,
PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and
RoBERTa in the unsupervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers. (arXiv:2202.06690v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06690">
<div class="article-summary-box-inner">
<span><p>The applications of conversational agents for scientific disciplines (as
expert domains) are understudied due to the lack of dialogue data to train such
agents. While most data collection frameworks, such as Amazon Mechanical Turk,
foster data collection for generic domains by connecting crowd workers and task
designers, these frameworks are not much optimized for data collection in
expert domains. Scientists are rarely present in these frameworks due to their
limited time budget. Therefore, we introduce a novel framework to collect
dialogues between scientists as domain experts on scientific papers. Our
framework lets scientists present their scientific papers as groundings for
dialogues and participate in dialogue they like its paper title. We use our
framework to collect a novel argumentative dialogue dataset, ArgSciChat. It
consists of 498 messages collected from 41 dialogues on 20 scientific papers.
Alongside extensive analysis on ArgSciChat, we evaluate a recent conversational
agent on our dataset. Experimental results show that this agent poorly performs
on ArgSciChat, motivating further research on argumentative scientific agents.
We release our framework and the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VScript: Controllable Script Generation with Visual Presentation. (arXiv:2203.00314v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00314">
<div class="article-summary-box-inner">
<span><p>In order to offer a customized script tool and inspire professional
scriptwriters, we present VScript. It is a controllable pipeline that generates
complete scripts, including dialogues and scene descriptions, as well as
presents visually using video retrieval. With an interactive interface, our
system allows users to select genres and input starting words that control the
theme and development of the generated script. We adopt a hierarchical
structure, which first generates the plot, then the script and its visual
presentation. A novel approach is also introduced to plot-guided dialogue
generation by treating it as an inverse dialogue summarization. The experiment
results show that our approach outperforms the baselines on both automatic and
human evaluations, especially in genre control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linearizing Transformer with Key-Value Memory. (arXiv:2203.12644v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12644">
<div class="article-summary-box-inner">
<span><p>Efficient transformer variants with linear time complexity have been
developed to mitigate the quadratic computational overhead of the vanilla
transformer. Among them are low-rank projection methods such as Linformer and
kernel-based Transformers. Despite their unique merits, they usually suffer
from a performance drop comparing with the vanilla transformer on many sequence
generation tasks, and often fail to obtain computation gain when the generation
is short. We propose MemSizer, an approach towards closing the performance gap
while improving the efficiency even with short generation. It projects the
source sequences into lower dimension representations like Linformer, while
enjoying efficient recurrent-style incremental computation similar to
kernel-based transformers. This yields linear computation time and constant
memory complexity at inference time. MemSizer also employs a lightweight
multi-head mechanism which renders the computation as light as a single-head
model. We demonstrate that MemSizer provides an improved balance between
efficiency and accuracy over the vanilla transformer and other efficient
transformer variants in three typical sequence generation tasks, including
machine translation, abstractive text summarization, and language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. (arXiv:2203.14680v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14680">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) are at the core of modern NLP, but
their internal prediction construction process is opaque and largely not
understood. In this work, we make a substantial step towards unveiling this
underlying prediction process, by reverse-engineering the operation of the
feed-forward network (FFN) layers, one of the building blocks of transformer
models. We view the token representation as a changing distribution over the
vocabulary, and the output from each FFN layer as an additive update to that
distribution. Then, we analyze the FFN updates in the vocabulary space, showing
that each update can be decomposed to sub-updates corresponding to single FFN
parameter vectors, each promoting concepts that are often human-interpretable.
We then leverage these findings for controlling LM predictions, where we reduce
the toxicity of GPT2 by almost 50%, and for improving computation efficiency
with a simple early exit rule, saving 20% of computation on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASR data augmentation using cross-lingual multi-speaker TTS and cross-lingual voice conversion. (arXiv:2204.00618v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00618">
<div class="article-summary-box-inner">
<span><p>We explore cross-lingual multi-speaker speech synthesis and cross-lingual
voice conversion applied to data augmentation for automatic speech recognition
(ASR) systems. Through extensive experiments, we show that our approach permits
the application of speech synthesis and voice conversion to improve ASR systems
on a target language using only one target-language speaker during model
training. We managed to close the gap between ASR models trained with
synthesized versus human speech compared to other works that use many speakers.
Finally, we show that it is possible to obtain promising ASR training results
with our data augmentation method using only a single real speaker in a target
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06674">
<div class="article-summary-box-inner">
<span><p>Recent improvements in KG-to-text generation are due to additional auxiliary
pre-training tasks designed to give the fine-tune task a boost in performance.
These tasks require extensive computational resources while only suggesting
marginal improvements. Here, we demonstrate that by fusing graph-aware elements
into existing pre-trained language models, we are able to outperform
state-of-the-art models and close the gap imposed by additional pre-training
tasks. We do so by proposing a mask structure to capture neighborhood
information and a novel type encoder that adds a bias to the graph-attention
weights depending on the connection type. Experiments on two KG-to-text
benchmark datasets show our models are competitive while involving fewer
parameters and no additional pre-training tasks. By formulating the problem as
a framework, we can interchange the various proposed components and begin
interpreting KG-to-text generative models based on the topological and type
information found in a graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08790">
<div class="article-summary-box-inner">
<span><p>Learning visual representations from natural language supervision has
recently shown great promise in a number of pioneering works. In general, these
language-augmented visual models demonstrate strong transferability to a
variety of datasets and tasks. However, it remains challenging to evaluate the
transferablity of these models due to the lack of easy-to-use evaluation
toolkits and public benchmarks. To tackle this, we build ELEVATER (Evaluation
of Language-augmented Visual Task-level Transfer), the first benchmark and
toolkit for evaluating(pre-trained) language-augmented visual models. ELEVATER
is composed of three components. (i) Datasets. As downstream evaluation suites,
it consists of 20 image classification datasets and 35 object detection
datasets, each of which is augmented with external knowledge. (ii) Toolkit. An
automatic hyper-parameter tuning toolkit is developed to facilitate model
evaluation on downstream tasks. (iii) Metrics. A variety of evaluation metrics
are used to measure sample-efficiency (zero-shot and few-shot) and
parameter-efficiency (linear probing and full model fine-tuning). ELEVATER is a
platform for Computer Vision in the Wild (CVinW), and is publicly released at
at https://computer-vision-in-the-wild.github.io/ELEVATER/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models. (arXiv:2204.12130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12130">
<div class="article-summary-box-inner">
<span><p>The opaque nature and unexplained behavior of transformer-based language
models (LMs) have spurred a wide interest in interpreting their predictions.
However, current interpretation methods mostly focus on probing models from
outside, executing behavioral tests, and analyzing salience input features,
while the internal prediction construction process is largely not understood.
In this work, we introduce LM-Debugger, an interactive debugger tool for
transformer-based LMs, which provides a fine-grained interpretation of the
model's internal prediction process, as well as a powerful framework for
intervening in LM behavior. For its backbone, LM-Debugger relies on a recent
method that interprets the inner token representations and their updates by the
feed-forward layers in the vocabulary space. We demonstrate the utility of
LM-Debugger for single-prediction debugging, by inspecting the internal
disambiguation process done by GPT2. Moreover, we show how easily LM-Debugger
allows to shift model behavior in a direction of the user's choice, by
identifying a few vectors in the network and inducing effective interventions
to the prediction process. We release LM-Debugger as an open-source tool and a
demo over GPT2 models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Cross-Lingual Lexical Knowledge from Multilingual Sentence Encoders. (arXiv:2205.00267v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00267">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual language models (LMs) can be successfully transformed
into multilingual sentence encoders (SEs; e.g., LaBSE, xMPNet) via additional
fine-tuning or model distillation with parallel data. However, it remains
unclear how to best leverage them to represent sub-sentence lexical items
(i.e., words and phrases) in cross-lingual lexical tasks. In this work, we
probe SEs for the amount of cross-lingual lexical knowledge stored in their
parameters, and compare them against the original multilingual LMs. We also
devise a simple yet efficient method for exposing the cross-lingual lexical
knowledge by means of additional fine-tuning through inexpensive contrastive
learning that requires only a small amount of word translation pairs. Using
bilingual lexical induction (BLI), cross-lingual lexical semantic similarity,
and cross-lingual entity linking as lexical probing tasks, we report
substantial gains on standard benchmarks (e.g., +10 Precision@1 points in BLI).
The results indicate that the SEs such as LaBSE can be 'rewired' into effective
cross-lingual lexical encoders via the contrastive learning procedure, and that
they contain more cross-lingual lexical knowledge than what 'meets the eye'
when they are used as off-the-shelf SEs. This way, we also provide an effective
tool for harnessing 'covert' multilingual lexical knowledge hidden in
multilingual sentence encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Global and Local Hierarchies for Hierarchical Text Classification. (arXiv:2205.02613v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02613">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods.To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning. (arXiv:2205.03401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03401">
<div class="article-summary-box-inner">
<span><p>Does prompting a large language model (LLM) like GPT-3 with explanations
improve in-context learning? We study this question on two NLP tasks that
involve reasoning over text, namely question answering and natural language
inference. We test the performance of four LLMs on three textual reasoning
datasets using prompts that include explanations in multiple different styles.
For these tasks, we find that including explanations in the prompts for OPT,
GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to
moderate accuracy improvements over standard few-show learning. However,
text-davinci-002 is able to benefit more substantially.
</p>
<p>We further show that explanations generated by the LLMs may not entail the
models' predictions nor be factually grounded in the input, even on simple
tasks with extractive explanations. However, these flawed explanations can
still be useful as a way to verify LLMs' predictions post-hoc. Through analysis
in our three settings, we show that explanations judged by humans to be
good--logically consistent with the input and the prediction--more likely
cooccur with accurate predictions. Following these observations, we train
calibrators using automatically extracted scores that assess the reliability of
explanations, allowing us to improve performance post-hoc across all of our
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Modeling of Multi-Domain Multi-Device ASR Systems. (arXiv:2205.06655v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06655">
<div class="article-summary-box-inner">
<span><p>Modern Automatic Speech Recognition (ASR) systems often use a portfolio of
domain-specific models in order to get high accuracy for distinct user
utterance types across different devices. In this paper, we propose an
innovative approach that integrates the different per-domain per-device models
into a unified model, using a combination of domain embedding, domain experts,
mixture of experts and adversarial training. We run careful ablation studies to
show the benefit of each of these innovations in contributing to the accuracy
of the overall unified model. Experiments show that our proposed unified
modeling approach actually outperforms the carefully tuned per-domain models,
giving relative gains of up to 10% over a baseline model with negligible
increase in the number of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DDXPlus: A New Dataset For Automatic Medical Diagnosis. (arXiv:2205.09148v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09148">
<div class="article-summary-box-inner">
<span><p>There has been a rapidly growing interest in Automatic Symptom Detection
(ASD) and Automatic Diagnosis (AD) systems in the machine learning research
literature, aiming to assist doctors in telemedicine services. These systems
are designed to interact with patients, collect evidence about their symptoms
and relevant antecedents, and possibly make predictions about the underlying
diseases. Doctors would review the interactions, including the evidence and the
predictions, collect if necessary additional information from patients, before
deciding on next steps. Despite recent progress in this area, an important
piece of doctors' interactions with patients is missing in the design of these
systems, namely the differential diagnosis. Its absence is largely due to the
lack of datasets that include such information for models to train on. In this
work, we present a large-scale synthetic dataset of roughly 1.3 million
patients that includes a differential diagnosis, along with the ground truth
pathology, symptoms and antecedents for each patient. Unlike existing datasets
which only contain binary symptoms and antecedents, this dataset also contains
categorical and multi-choice symptoms and antecedents useful for efficient data
collection. Moreover, some symptoms are organized in a hierarchy, making it
possible to design systems able to interact with patients in a logical way. As
a proof-of-concept, we extend two existing AD and ASD systems to incorporate
the differential diagnosis, and provide empirical evidence that using
differentials as training signals is essential for the efficiency of such
systems or for helping doctors better understand the reasoning of those
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. (arXiv:2205.09921v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09921">
<div class="article-summary-box-inner">
<span><p>Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets. Our implementation and pretrained checkpoints are released
at https://github.com/chijames/KERPLE.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11490">
<div class="article-summary-box-inner">
<span><p>Subword tokenization schemes are the dominant technique used in current NLP
models. However, such schemes can be rigid and tokenizers built on one corpus
do not adapt well to other parallel corpora. It has also been observed that in
multilingual corpora, subword tokenization schemes over-segment low-resource
languages leading to a drop in translation performance. A simple alternative to
subword tokenizers is byte-based methods i.e. tokenization into byte sequences
using encoding schemes such as UTF-8. Byte tokens often represent inputs at a
sub-character granularity i.e. one character can be represented by a sequence
of multiple byte tokens. This results in byte sequences that are significantly
longer than character sequences. Enforcing aggregation of local information in
the lower layers can guide the model to build higher-level semantic
information. We propose a Local Byte Fusion (LOBEF) method for byte-based
machine translation -- utilizing byte $n$-gram and word boundaries -- to
aggregate local semantic information. Extensive experiments on multilingual
translation, zero-shot cross-lingual transfer, and domain adaptation reveal a
consistent improvement over traditional byte-based models and even over subword
techniques. Further analysis also indicates that our byte-based models are
parameter-efficient and can be trained faster than subword models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaskEval: Weighted MLM-Based Evaluation for Text Summarization and Simplification. (arXiv:2205.12394v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12394">
<div class="article-summary-box-inner">
<span><p>In text summarization and simplification, system outputs must be evaluated
along multiple dimensions such as relevance, factual consistency, fluency, and
grammaticality, and a wide range of possible outputs could be of high quality.
These properties make the development of an adaptable, reference-less
evaluation metric both necessary and challenging. We introduce MaskEval, a
reference-less metric for text summarization and simplification that operates
by performing masked language modeling (MLM) on the concatenation of the
candidate and the source texts. It features an attention-like weighting
mechanism to modulate the relative importance of each MLM step, which crucially
allows it to be adapted to evaluate different quality dimensions. We
demonstrate its effectiveness on English summarization and simplification in
terms of correlations with human judgments, and explore transfer scenarios
between the two tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT. (arXiv:2205.12399v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12399">
<div class="article-summary-box-inner">
<span><p>We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the
speed and stability of linear, mixing transformations to design the Sparse
Mixer encoder model. Sparse Mixer slightly outperforms (&lt;1%) BERT on GLUE and
SuperGLUE, but more importantly trains 65% faster and runs inference 61%
faster. We also present a faster variant, prosaically named Fast Sparse Mixer,
that marginally underperforms BERT on SuperGLUE, but trains and runs nearly
twice as fast. We justify the design of these two models by carefully ablating
through various mixing mechanisms, MoE configurations and hyperparameters.
Sparse Mixer overcomes many of the latency and stability concerns of MoE models
and offers the prospect of serving sparse student models, without resorting to
distilling them to dense variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Helpfulness and Fairness of Task-Oriented Dialogue Systems. (arXiv:2205.12554v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12554">
<div class="article-summary-box-inner">
<span><p>Goal-oriented dialogue systems aim to help users achieve certain goals.
Therefore, how humans perceive their helpfulness is important. However, neither
the human-perceived helpfulness of goal-oriented dialogue systems nor its
fairness implication has been well studied. In this paper, we study
computational measurements of helpfulness. We first formally define a dialogue
response as helpful if it is relevant &amp; coherent, useful, and informative to a
query. Then, we collect human annotations for the helpfulness of dialogue
responses based on our definition and build a classifier to automatically
determine the helpfulness of a response. We further propose to use the
helpfulness level of a dialogue system towards different user queries to
measure the fairness of a dialogue system. Experiments with state-of-the-art
dialogue systems under three information-seeking scenarios reveal that existing
systems tend to be more helpful for questions regarding concepts from
highly-developed countries than less-developed countries, uncovering potential
fairness concerns underlying the current goal-oriented dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbation Augmentation for Fairer NLP. (arXiv:2205.12586v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12586">
<div class="article-summary-box-inner">
<span><p>Unwanted and often harmful social biases are becoming ever more salient in
NLP research, affecting both models and datasets. In this work, we ask whether
training on demographically perturbed data leads to fairer language models. We
collect a large dataset of human annotated text perturbations and train a
neural perturbation model, which we show outperforms heuristic alternatives. We
find that (i) language models (LMs) pre-trained on demographically perturbed
corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE
datasets exhibit less demographic bias on downstream tasks, and (iii) fairness
improvements do not come at the expense of performance on downstream tasks.
Lastly, we discuss outstanding questions about how best to evaluate the
(un)fairness of large language models. We hope that this exploration of neural
demographic perturbation will help drive more improvement towards fairer NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuro-Symbolic Procedural Planning with Commonsense Prompting. (arXiv:2206.02928v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02928">
<div class="article-summary-box-inner">
<span><p>Procedural planning aims to implement complex high-level goals by
decomposition into sequential simpler low-level steps. Although procedural
planning is a basic skill set for humans in daily life, it remains a challenge
for large language models (LLMs) that lack a deep understanding of the
cause-effect relations in procedures. Previous methods require manual exemplars
to acquire procedural planning knowledge from LLMs in the zero-shot setting.
However, such elicited pre-trained knowledge in LLMs induces spurious
correlations between goals and steps, which impair the model generalization to
unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural
PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with
commonsense-infused prompting. To mitigate spurious goal-step correlations, we
use symbolic program executors on the latent procedural representations to
formalize prompts from commonsense knowledge bases as a causal intervention
toward the Structural Causal Model. Both automatic and human evaluations on
WikiHow and RobotHow show the superiority of PLAN on procedural planning
without further training or manual exemplars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Prompts for Dialogue Generation through Reinforcement Learning. (arXiv:2206.03931v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03931">
<div class="article-summary-box-inner">
<span><p>Much literature has shown that prompt-based learning is an efficient method
to make use of the large pre-trained language model. Recent works also exhibit
the possibility of steering a chatbot's output by plugging in an appropriate
prompt. Gradient-based methods are often used to perturb the prompts. However,
some language models are not even available to the public. In this work, we
first explored the combination of prompting and reinforcement learning (RL) to
steer models' generation without accessing any of the models' parameters.
Second, to reduce the training effort and enhance the generalizability to the
unseen task, we apply multi-task learning to make the model learn to generalize
to new tasks better. The experiment results show that our proposed method can
successfully control several state-of-the-art (SOTA) dialogue models without
accessing their parameters. Furthermore, the model demonstrates the strong
ability to quickly adapt to an unseen task in fewer steps than the baseline
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09674">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) in long horizon and sparse reward tasks is
notoriously difficult and requires a lot of training steps. A standard solution
to speed up the process is to leverage additional reward signals, shaping it to
better guide the learning process. In the context of language-conditioned RL,
the abstraction and generalisation properties of the language input provide
opportunities for more efficient ways of shaping the reward. In this paper, we
leverage this idea and propose an automated reward shaping method where the
agent extracts auxiliary objectives from the general language goal. These
auxiliary objectives use a question generation (QG) and question answering (QA)
system: they consist of questions leading the agent to try to reconstruct
partial information about the global goal using its own trajectory. When it
succeeds, it receives an intrinsic reward proportional to its confidence in its
answer. This incentivizes the agent to generate trajectories which
unambiguously explain various aspects of the general language goal. Our
experimental study shows that this approach, which does not require engineer
intervention to design the auxiliary objectives, improves sample efficiency by
effectively directing exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus-Driven Contrastive Learniang for Medical Question Summarization. (arXiv:2209.00484v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.00484">
<div class="article-summary-box-inner">
<span><p>Automatic medical question summarization can significantly help the system to
understand consumer health questions and retrieve correct answers. The Seq2Seq
model based on maximum likelihood estimation (MLE) has been applied in this
task, which faces two general problems: the model can not capture well question
focus and and the traditional MLE strategy lacks the ability to understand
sentence-level semantics. To alleviate these problems, we propose a novel
question focus-driven contrastive learning framework (QFCL). Specially, we
propose an easy and effective approach to generate hard negative samples based
on the question focus, and exploit contrastive learning at both encoder and
decoder to obtain better sentence level representations. On three medical
benchmark datasets, our proposed model achieves new state-of-the-art results,
and obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline
BART model on three datasets respectively. Further human judgement and detailed
analysis prove that our QFCL model learns better sentence representations with
the ability to distinguish different sentence meanings, and generates
high-quality summaries by capturing question focus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Large Pre-Trained Language Models for Machine Translation: What You Don't Know About It. (arXiv:2209.07417v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07417">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) often take advantage of the monolingual
and multilingual dataset that is freely available online to acquire general or
mixed domain knowledge before deployment into specific tasks. Extra-large PLMs
(xLPLMs) are proposed very recently to claim supreme performances over
smaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs
include Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this
work, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in
fine-tuning toward domain-specific MTs. We use two different in-domain data of
different sizes: commercial automotive in-house data and clinical shared task
data from the ClinSpEn2022 challenge at WMT2022. We choose popular Marian
Helsinki as smaller sized PLM and two massive-sized Mega-Transformers from
Meta-AI as xLPLMs.
</p>
<p>Our experimental investigation shows that 1) on smaller-sized in-domain
commercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much
better evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized
Marian, even though its score increase rate is lower than Marian after
fine-tuning; 2) on relatively larger-size well prepared clinical data
fine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized
Marian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn
offered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on
Task-1 (clinical cases) on all official metrics including SacreBLEU and BLEU;
3) metrics do not always agree with each other on the same tasks using the same
model outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SACREBLEU/BLEU) and
Task-3 (via METEOR and ROUGE) among all submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango. (arXiv:2209.07686v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07686">
<div class="article-summary-box-inner">
<span><p>The past decade has witnessed dramatic gains in natural language processing
and an unprecedented scaling of large language models. These developments have
been accelerated by the advent of few-shot techniques such as chain of thought
(CoT) prompting. Specifically, CoT pushes the performance of large language
models in a few-shot setup by augmenting the prompts with intermediate steps.
Despite impressive results across various tasks, the reasons behind their
success have not been explored. This work uses counterfactual prompting to
develop a deeper understanding of CoT-based few-shot prompting mechanisms in
large language models. We first systematically identify and define the key
components of a prompt: symbols, patterns, and text. Then, we devise and
conduct an exhaustive set of experiments across four different tasks, by
querying the model with counterfactual prompts where only one of these
components is altered. Our experiments across three models (PaLM, GPT-3, and
CODEX) reveal several surprising findings and brings into question the
conventional wisdom around few-shot prompting. First, the presence of factual
patterns in a prompt is practically immaterial to the success of CoT. Second,
our results conclude that the primary role of intermediate steps may not be to
facilitate learning how to solve a task. The intermediate steps are rather a
beacon for the model to realize what symbols to replicate in the output to form
a factual answer. Further, text imbues patterns with commonsense knowledge and
meaning. Our empirical and qualitative analysis reveals that a symbiotic
relationship between text and patterns explains the success of few-shot
prompting: text helps extract commonsense from the question to help patterns,
and patterns enforce task understanding and direct text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Interpretable Latent Dialogue Actions With Less Supervision. (arXiv:2209.11128v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.11128">
<div class="article-summary-box-inner">
<span><p>We present a novel architecture for explainable modeling of task-oriented
dialogues with discrete latent variables to represent dialogue actions. Our
model is based on variational recurrent neural networks (VRNN) and requires no
explicit annotation of semantic information. Unlike previous works, our
approach models the system and user turns separately and performs database
query modeling, which makes the model applicable to task-oriented dialogues
while producing easily interpretable action latent variables. We show that our
model outperforms previous approaches with less supervision in terms of
perplexity and BLEU on three datasets, and we propose a way to measure dialogue
success without the need for expert annotation. Finally, we propose a novel way
to explain semantics of the latent variables with respect to system actions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaPrompting: Learning to Learn Better Prompts. (arXiv:2209.11486v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.11486">
<div class="article-summary-box-inner">
<span><p>Prompting method is regarded as one of the crucial progress for few-shot
nature language processing. Recent research on prompting moves from discrete
tokens based ``hard prompts'' to continuous ``soft prompts'', which employ
learnable vectors as pseudo prompt tokens and achieve better performance.
Though showing promising prospects, these soft-prompting methods are observed
to rely heavily on good initialization to take effect. Unfortunately, obtaining
a perfect initialization for soft prompts requires understanding of inner
language models working and elaborate design, which is no easy task and has to
restart from scratch for each new task. To remedy this, we propose a
generalized soft prompting method called MetaPrompting, which adopts the
well-recognized model-agnostic meta-learning algorithm to automatically find
better prompt initialization that facilitates fast adaptation to new prompting
tasks.Extensive experiments show MetaPrompting tackles soft prompt
initialization problem and brings significant improvement on four different
datasets (over 6 points improvement in accuracy for 1-shot setting), achieving
new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Family Adapters for Multilingual Neural Machine Translation. (arXiv:2209.15236v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15236">
<div class="article-summary-box-inner">
<span><p>Massively multilingual models pretrained on abundant corpora with
self-supervision achieve state-of-the-art results in a wide range of natural
language processing tasks. In machine translation, multilingual pretrained
models are often fine-tuned on parallel data from one or multiple language
pairs. Multilingual fine-tuning improves performance on medium- and
low-resource languages but requires modifying the entire model and can be
prohibitively expensive. Training a new set of adapters on each language pair
or training a single set of adapters on all language pairs while keeping the
pretrained model's parameters frozen has been proposed as a parameter-efficient
alternative. However, the former do not permit any sharing between languages,
while the latter share parameters for all languages and have to deal with
negative interference. In this paper, we propose training language-family
adapters on top of a pretrained multilingual model to facilitate cross-lingual
transfer. Our model consistently outperforms other adapter-based approaches. We
also demonstrate that language-family adapters provide an effective method to
translate to languages unseen during pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding. (arXiv:2210.03304v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03304">
<div class="article-summary-box-inner">
<span><p>Automatic International Classification of Diseases (ICD) coding aims to
assign multiple ICD codes to a medical note with average length of 3,000+
tokens. This task is challenging due to a high-dimensional space of multi-label
assignment (tens of thousands of ICD codes) and the long-tail challenge: only a
few codes (common diseases) are frequently assigned while most codes (rare
diseases) are infrequently assigned. This study addresses the long-tail
challenge by adapting a prompt-based fine-tuning technique with label
semantics, which has been shown to be effective under few-shot setting. To
further enhance the performance in medical domain, we propose a
knowledge-enhanced longformer by injecting three domain-specific knowledge:
hierarchy, synonym, and abbreviation with additional pretraining using
contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of
code assignment, show that our proposed method outperforms previous
state-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P&lt;0.001). To
further test our model on few-shot setting, we created a new rare diseases
coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from
17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphorical Paraphrase Generation: Feeding Metaphorical Language Models with Literal Texts. (arXiv:2210.04756v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04756">
<div class="article-summary-box-inner">
<span><p>This study presents a new approach to metaphorical paraphrase generation by
masking literal tokens of literal sentences and unmasking them with
metaphorical language models. Unlike similar studies, the proposed algorithm
does not only focus on verbs but also on nouns and adjectives. Despite the fact
that the transfer rate for the former is the highest (56%), the transfer of the
latter is feasible (24% and 31%). Human evaluation showed that our
system-generated metaphors are considered more creative and metaphorical than
human-generated ones while when using our transferred metaphors for data
augmentation improves the state of the art in metaphorical sentence
classification by 3% in F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What the DAAM: Interpreting Stable Diffusion Using Cross Attention. (arXiv:2210.04885v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04885">
<div class="article-summary-box-inner">
<span><p>Large-scale diffusion neural networks represent a substantial milestone in
text-to-image generation, with some performing similar to real photographs in
human evaluation. However, they remain poorly understood, lacking
explainability and interpretability analyses, largely due to their proprietary,
closed-source nature. In this paper, to shine some much-needed light on
text-to-image diffusion models, we perform a text-image attribution analysis on
Stable Diffusion, a recently open-sourced large diffusion model. To produce
pixel-level attribution maps, we propose DAAM, a novel method based on
upscaling and aggregating cross-attention activations in the latent denoising
subnetwork. We support its correctness by evaluating its unsupervised semantic
segmentation quality on its own generated imagery, compared to supervised
segmentation models. We show that DAAM performs strongly on COCO
caption-generated images, achieving an mIoU of 61.0, and it outperforms
supervised models on open-vocabulary segmentation, for an mIoU of 51.5. We
further find that certain parts of speech, like punctuation and conjunctions,
influence the generated imagery most, which agrees with the prior literature,
while determiners and numerals the least, suggesting poor numeracy. To our
knowledge, we are the first to propose and study word-pixel attribution for
interpreting large-scale diffusion models. Our code and data are at
https://github.com/castorini/daam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05423">
<div class="article-summary-box-inner">
<span><p>We introduce a new task, named video corpus visual answer localization
(VCVAL), which aims to locate the visual answer in a large collection of
untrimmed, unsegmented instructional videos using a natural language question.
This task requires a range of skills - the interaction between vision and
language, video retrieval, passage comprehension, and visual answer
localization. In this paper, we propose a cross-modal contrastive global-span
(CCGS) method for the VCVAL, jointly training the video corpus retrieval and
visual answer localization subtasks. More precisely, we first enhance the video
question-answer semantic by adding element-wise visual information into the
pre-trained language model, and then design a novel global-span predictor
through fusion information to locate the visual answer point. The global-span
contrastive learning is adopted to sort the span point from the positive and
negative samples with the global-span matrix. We have reconstructed a dataset
named MedVidCQA, on which the VCVAL task is benchmarked. Experimental results
show that the proposed method outperforms other competitive methods both in the
video corpus retrieval and visual answer localization subtasks. Most
importantly, we perform detailed analyses on extensive experiments, paving a
new path for understanding the instructional videos, which ushers in further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers generalize differently from information stored in context vs in weights. (arXiv:2210.05675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05675">
<div class="article-summary-box-inner">
<span><p>Transformer models can use two fundamentally different kinds of information:
information stored in weights during training, and information provided
``in-context'' at inference time. In this work, we show that transformers
exhibit different inductive biases in how they represent and generalize from
the information in these two sources. In particular, we characterize whether
they generalize via parsimonious rules (rule-based generalization) or via
direct comparison with observed examples (exemplar-based generalization). This
is of important practical consequence, as it informs whether to encode
information in weights or in context, depending on how we want models to use
that information. In transformers trained on controlled stimuli, we find that
generalization from weights is more rule-based whereas generalization from
context is largely exemplar-based. In contrast, we find that in transformers
pre-trained on natural language, in-context learning is significantly
rule-based, with larger models showing more rule-basedness. We hypothesise that
rule-based generalization from in-context information might be an emergent
consequence of large-scale training on language, which has sparse rule-like
structure. Using controlled stimuli, we verify that transformers pretrained on
data containing sparse rule-like structure exhibit more rule-based
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Language Maps for Robot Navigation. (arXiv:2210.05714v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05714">
<div class="article-summary-box-inner">
<span><p>Grounding language to the visual observations of a navigating agent can be
performed using off-the-shelf visual-language models pretrained on
Internet-scale data (e.g., image captions). While this is useful for matching
images to natural language descriptions of object goals, it remains disjoint
from the process of mapping the environment, so that it lacks the spatial
precision of classic geometric maps. To address this problem, we propose
VLMaps, a spatial map representation that directly fuses pretrained
visual-language features with a 3D reconstruction of the physical world. VLMaps
can be autonomously built from video feed on robots using standard exploration
approaches and enables natural language indexing of the map without additional
labeled data. Specifically, when combined with large language models (LLMs),
VLMaps can be used to (i) translate natural language commands into a sequence
of open-vocabulary navigation goals (which, beyond prior work, can be spatial
by construction, e.g., "in between the sofa and TV" or "three meters to the
right of the chair") directly localized in the map, and (ii) can be shared
among multiple robots with different embodiments to generate new obstacle maps
on-the-fly (by using a list of obstacle categories). Extensive experiments
carried out in simulated and real world environments show that VLMaps enable
navigation according to more complex language instructions than existing
methods. Videos are available at https://vlmaps.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vote'n'Rank: Revision of Benchmarking with Social Choice Theory. (arXiv:2210.05769v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05769">
<div class="article-summary-box-inner">
<span><p>The development of state-of-the-art systems in different applied areas of
machine learning (ML) is driven by benchmarks, which have shaped the paradigm
of evaluating generalisation capabilities from multiple perspectives. Although
the paradigm is shifting towards more fine-grained evaluation across diverse
tasks, the delicate question of how to aggregate the performances has received
particular interest in the community. In general, benchmarks follow the
unspoken utilitarian principles, where the systems are ranked based on their
mean average score over task-specific metrics. Such aggregation procedure has
been viewed as a sub-optimal evaluation protocol, which may have created the
illusion of progress. This paper proposes Vote'n'Rank, a framework for ranking
systems in multi-task benchmarks under the principles of the social choice
theory. We demonstrate that our approach can be efficiently utilised to draw
new insights on benchmarking in several ML sub-fields and identify the
best-performing systems in research and development case studies. The
Vote'n'Rank's procedures are more robust than the mean average while being able
to handle missing performance scores and determine conditions under which the
system becomes the winner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05916">
<div class="article-summary-box-inner">
<span><p>Hateful memes are a growing menace on social media. While the image and its
corresponding text in a meme are related, they do not necessarily convey the
same meaning when viewed individually. Hence, detecting hateful memes requires
careful consideration of both visual and textual information. Multimodal
pre-training can be beneficial for this task because it effectively captures
the relationship between the image and the text by representing them in a
similar feature space. Furthermore, it is essential to model the interactions
between the image and text features through intermediate fusion. Most existing
methods either employ multimodal pre-training or intermediate fusion, but not
both. In this work, we propose the Hate-CLIPper architecture, which explicitly
models the cross-modal interactions between the image and text representations
obtained using Contrastive Language-Image Pre-training (CLIP) encoders via a
feature interaction matrix (FIM). A simple classifier based on the FIM
representation is able to achieve state-of-the-art performance on the Hateful
Memes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the
human performance of 82.65. Experiments on other meme datasets such as
Propaganda Memes and TamilMemes also demonstrate the generalizability of the
proposed approach. Finally, we analyze the interpretability of the FIM
representation and show that cross-modal interactions can indeed facilitate the
learning of meaningful concepts. The code for this work is available at
https://github.com/gokulkarthik/hateclipper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary on the ISCSLP 2022 Chinese-English Code-Switching ASR Challenge. (arXiv:2210.06091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06091">
<div class="article-summary-box-inner">
<span><p>Code-switching automatic speech recognition becomes one of the most
challenging and the most valuable scenarios of automatic speech recognition,
due to the code-switching phenomenon between multilingual language and the
frequent occurrence of code-switching phenomenon in daily life. The ISCSLP 2022
Chinese-English Code-Switching Automatic Speech Recognition (CSASR) Challenge
aims to promote the development of code-switching automatic speech recognition.
The ISCSLP 2022 CSASR challenge provided two training sets, TAL_CSASR corpus
and MagicData-RAMC corpus, a development and a test set for participants, which
are used for CSASR model training and evaluation. Along with the challenge, we
also provide the baseline system performance for reference. As a result, more
than 40 teams participated in this challenge, and the winner team achieved
16.70% Mixture Error Rate (MER) performance on the test set and has achieved
9.8% MER absolute improvement compared with the baseline system. In this paper,
we will describe the datasets, the associated baselines system and the
requirements, and summarize the CSASR challenge results and major techniques
and tricks used in the submitted systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors. (arXiv:2210.06340v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06340">
<div class="article-summary-box-inner">
<span><p>Current deep learning models trained to generate radiology reports from chest
radiographs are capable of producing clinically accurate, clear, and actionable
text that can advance patient care. However, such systems all succumb to the
same problem: making hallucinated references to non-existent prior reports.
Such hallucinations occur because these models are trained on datasets of
real-world patient reports that inherently refer to priors. To this end, we
propose two methods to remove references to priors in radiology reports: (1) a
GPT-3-based few-shot approach to rewrite medical reports without references to
priors; and (2) a BioBERT-based token classification approach to directly
remove words referring to priors. We use the aforementioned approaches to
modify MIMIC-CXR, a publicly available dataset of chest X-rays and their
associated free-text radiology reports; we then retrain CXR-RePaiR, a radiology
report generation system, on the adapted MIMIC-CXR dataset. We find that our
re-trained model--which we call CXR-ReDonE--outperforms previous report
generation methods on clinical metrics, achieving an average BERTScore of
0.2351 (2.57% absolute improvement). We expect our approach to be broadly
valuable in enabling current radiology report generation systems to be more
directly integrated into clinical pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GMP*: Well-Tuned Global Magnitude Pruning Can Outperform Most BERT-Pruning Methods. (arXiv:2210.06384v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06384">
<div class="article-summary-box-inner">
<span><p>We revisit the performance of the classic gradual magnitude pruning (GMP)
baseline for large language models, focusing on the classic BERT benchmark on
various popular tasks. Despite existing evidence in the literature that GMP
performs poorly, we show that a simple and general variant, which we call GMP*,
can match and sometimes outperform more complex state-of-the-art methods. Our
results provide a simple yet strong baseline for future work, highlight the
importance of parameter tuning for baselines, and even improve the performance
of the state-of-the-art second-order pruning method in this setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Graph Convolutional Neural Networks for Multihop Reasoning: A Comparative Study. (arXiv:2210.06418v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06418">
<div class="article-summary-box-inner">
<span><p>Multihop Question Answering is a complex Natural Language Processing task
that requires multiple steps of reasoning to find the correct answer to a given
question. Previous research has explored the use of models based on Graph
Neural Networks for tackling this task. Various architectures have been
proposed, including Relational Graph Convolutional Networks (RGCN). For these
many node types and relations between them have been introduced, such as simple
entity co-occurrences, modelling coreferences, or "reasoning paths" from
questions to answers via intermediary entities. Nevertheless, a thoughtful
analysis on which relations, node types, embeddings and architecture are the
most beneficial for this task is still missing. In this paper we explore a
number of RGCN-based Multihop QA models, graph relations, and node embeddings,
and empirically explore the influence of each on Multihop QA performance on the
WikiHop dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings. (arXiv:2210.06432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06432">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been extensively studied in sentence embedding
learning, which assumes that the embeddings of different views of the same
sentence are closer. The constraint brought by this assumption is weak, and a
good sentence representation should also be able to reconstruct the original
sentence fragments. Therefore, this paper proposes an information-aggregated
contrastive learning framework for learning unsupervised sentence embeddings,
termed InfoCSE. InfoCSE forces the representation of [CLS] positions to
aggregate denser sentence information by introducing an additional Masked
language model task and a well-designed network. We evaluate the proposed
InfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS)
task. Experimental results show that InfoCSE outperforms SimCSE by an average
Spearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving
state-of-the-art results among unsupervised sentence representation learning
methods. Our code are available at
https://github.com/caskcsg/sentemb/tree/main/InfoCSE.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-10-14 23:19:23.546201491 UTC">2022-10-14 23:19:23 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>