<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-17T01:30:00Z">05-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Watermarking Text Generated by Black-Box Language Models. (arXiv:2305.08883v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08883">
<div class="article-summary-box-inner">
<span><p>LLMs now exhibit human-like skills in various fields, leading to worries
about misuse. Thus, detecting generated text is crucial. However, passive
detection methods are stuck in domain specificity and limited adversarial
robustness. To achieve reliable detection, a watermark-based method was
proposed for white-box LLMs, allowing them to embed watermarks during text
generation. The method involves randomly dividing the model vocabulary to
obtain a special list and adjusting the probability distribution to promote the
selection of words in the list. A detection algorithm aware of the list can
identify the watermarked text. However, this method is not applicable in many
real-world scenarios where only black-box language models are available. For
instance, third-parties that develop API-based vertical applications cannot
watermark text themselves because API providers only supply generated text and
withhold probability distributions to shield their commercial interests. To
allow third-parties to autonomously inject watermarks into generated text, we
develop a watermarking framework for black-box language model usage scenarios.
Specifically, we first define a binary encoding function to compute a random
binary encoding corresponding to a word. The encodings computed for
non-watermarked text conform to a Bernoulli distribution, wherein the
probability of a word representing bit-1 being approximately 0.5. To inject a
watermark, we alter the distribution by selectively replacing words
representing bit-0 with context-based synonyms that represent bit-1. A
statistical test is then used to identify the watermark. Experiments
demonstrate the effectiveness of our method on both Chinese and English
datasets. Furthermore, results under re-translation, polishing, word deletion,
and synonym substitution attacks reveal that it is arduous to remove the
watermark without compromising the original semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An assessment of measuring local levels of homelessness through proxy social media signals. (arXiv:2305.08978v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08978">
<div class="article-summary-box-inner">
<span><p>Recent studies suggest social media activity can function as a proxy for
measures of state-level public health, detectable through natural language
processing. We present results of our efforts to apply this approach to
estimate homelessness at the state level throughout the US during the period
2010-2019 and 2022 using a dataset of roughly 1 million geotagged tweets
containing the substring ``homeless.'' Correlations between
homelessness-related tweet counts and ranked per capita homelessness volume,
but not general-population densities, suggest a relationship between the
likelihood of Twitter users to personally encounter or observe homelessness in
their everyday lives and their likelihood to communicate about it online. An
increase to the log-odds of ``homeless'' appearing in an English-language
tweet, as well as an acceleration in the increase in average tweet sentiment,
suggest that tweets about homelessness are also affected by trends at the
nation-scale. Additionally, changes to the lexical content of tweets over time
suggest that reversals to the polarity of national or state-level trends may be
detectable through an increase in political or service-sector language over the
semantics of charity or direct appeals. An analysis of user account type also
revealed changes to Twitter-use patterns by accounts authored by individuals
versus entities that may provide an additional signal to confirm changes to
homelessness density in a given jurisdiction. While a computational approach to
social media analysis may provide a low-cost, real-time dataset rich with
information about nationwide and localized impacts of homelessness and
homelessness policy, we find that practical issues abound, limiting the
potential of social media as a proxy to complement other measures of
homelessness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback. (arXiv:2305.08982v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08982">
<div class="article-summary-box-inner">
<span><p>Millions of users come to online peer counseling platforms to seek support on
diverse topics ranging from relationship stress to anxiety. However, studies
show that online peer support groups are not always as effective as expected
largely due to users' negative experiences with unhelpful counselors. Peer
counselors are key to the success of online peer counseling platforms, but most
of them often do not have systematic ways to receive guidelines or supervision.
In this work, we introduce CARE: an interactive AI-based tool to empower peer
counselors through automatic suggestion generation. During the practical
training stage, CARE helps diagnose which specific counseling strategies are
most suitable in the given context and provides tailored example responses as
suggestions. Counselors can choose to select, modify, or ignore any suggestion
before replying to the support seeker. Building upon the Motivational
Interviewing framework, CARE utilizes large-scale counseling conversation data
together with advanced natural language generation techniques to achieve these
functionalities. We demonstrate the efficacy of CARE by performing both
quantitative evaluations and qualitative user studies through simulated chats
and semi-structured interviews. We also find that CARE especially helps novice
counselors respond better in challenging situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance. (arXiv:2305.09022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09022">
<div class="article-summary-box-inner">
<span><p>Progress in NLP is increasingly measured through benchmarks; hence,
contextualizing progress requires understanding when and why practitioners may
disagree about the validity of benchmarks. We develop a taxonomy of
disagreement, drawing on tools from measurement modeling, and distinguish
between two types of disagreement: 1) how tasks are conceptualized and 2) how
measurements of model performance are operationalized. To provide evidence for
our taxonomy, we conduct a meta-analysis of relevant literature to understand
how NLP tasks are conceptualized, as well as a survey of practitioners about
their impressions of different factors that affect benchmark validity. Our
meta-analysis and survey across eight tasks, ranging from coreference
resolution to question answering, uncover that tasks are generally not clearly
and consistently conceptualized and benchmarks suffer from operationalization
disagreements. These findings support our proposed taxonomy of disagreement.
Finally, based on our taxonomy, we present a framework for constructing
benchmarks and documenting their limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft Prompt Decoding for Multilingual Dense Retrieval. (arXiv:2305.09025v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09025">
<div class="article-summary-box-inner">
<span><p>In this work, we explore a Multilingual Information Retrieval (MLIR) task,
where the collection includes documents in multiple languages. We demonstrate
that applying state-of-the-art approaches developed for cross-lingual
information retrieval to MLIR tasks leads to sub-optimal performance. This is
due to the heterogeneous and imbalanced nature of multilingual collections --
some languages are better represented in the collection and some benefit from
large-scale training data. To address this issue, we present KD-SPD, a novel
soft prompt decoding approach for MLIR that implicitly "translates" the
representation of documents in different languages into the same embedding
space. To address the challenges of data scarcity and imbalance, we introduce a
knowledge distillation strategy. The teacher model is trained on rich English
retrieval data, and by leveraging bi-text data, our distillation framework
transfers its retrieval knowledge to the multilingual document encoder.
Therefore, our approach does not require any multilingual retrieval training
data. Extensive experiments on three MLIR datasets with a total of 15 languages
demonstrate that KD-SPD significantly outperforms competitive baselines in all
cases. We conduct extensive analyses to show that our method has less language
bias and better zero-shot transfer ability towards new languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting. (arXiv:2305.09067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09067">
<div class="article-summary-box-inner">
<span><p>Building end-to-end task bots and maintaining their integration with new
functionalities using minimal human efforts is a long-standing challenge in
dialog research. Recently large language models (LLMs) have demonstrated
exceptional proficiency in conversational engagement and adherence to
instructions across various downstream tasks. In this work, we introduce
SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems
effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we
instruct fixed LLMs to generate appropriate responses on novel tasks,
circumventing the need for training data. Specifically, SGP-TOD comprises three
components: a LLM for engaging with users, a DST Prompter to aid the LLM with
dialog state tracking, which is then used to retrieve database items, and a
Policy Prompter to elicit proper responses adhering to the provided dialog
policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that
our training-free strategy SGP-TOD, without any task-specific data, yields
state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot
approaches. In a domain-extension setting, SGP-TOD aptly adapts to new
functionalities by merely adding supplementary schema rules. We make our code
and data publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09098">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is a predominant approach for BERT compression.
Previous KD-based methods focus on designing extra alignment losses for the
student model to mimic the behavior of the teacher model. These methods
transfer the knowledge in an indirect way. In this paper, we propose a novel
Weight-Inherited Distillation (WID), which directly transfers knowledge from
the teacher. WID does not require any additional alignment loss and trains a
compact student by inheriting the weights, showing a new perspective of
knowledge distillation. Specifically, we design the row compactors and column
compactors as mappings and then compress the weights via structural
re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show
that WID outperforms previous state-of-the-art KD-based baselines. Further
analysis indicates that WID can also learn the attention patterns from the
teacher model without any alignment loss on attention distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is a Video worth $n\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering. (arXiv:2305.09107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09107">
<div class="article-summary-box-inner">
<span><p>Conventional Transformer-based Video Question Answering (VideoQA) approaches
generally encode frames independently through one or more image encoders
followed by interaction between frames and question. However, such schema would
incur significant memory use and inevitably slow down the training and
inference speed. In this work, we present a highly efficient approach for
VideoQA based on existing vision-language pre-trained models where we
concatenate video frames to a $n\times n$ matrix and then convert it to one
image. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$
while maintaining the temporal structure of the original video. Experimental
results on MSRVTT and TrafficQA show that our proposed approach achieves
state-of-the-art performance with nearly $4\times$ faster speed and only 30%
memory use. We show that by integrating our approach into VideoQA systems we
can achieve comparable, even superior, performance with a significant speed up
for training and inference. We believe the proposed approach can facilitate
VideoQA-related research by reducing the computational requirements for those
who have limited access to budgets and resources. Our code will be made
publicly available for research use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Training to Learn in Context. (arXiv:2305.09137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09137">
<div class="article-summary-box-inner">
<span><p>In-context learning, where pre-trained language models learn to perform tasks
from task examples and instructions in their contexts, has attracted much
attention in the NLP community. However, the ability of in-context learning is
not fully exploited because language models are not explicitly trained to learn
in context. To this end, we propose PICL (Pre-training for In-Context
Learning), a framework to enhance the language models' in-context learning
ability by pre-training the model on a large collection of "intrinsic tasks" in
the general plain-text corpus using the simple language modeling objective.
PICL encourages the model to infer and perform tasks by conditioning on the
contexts while maintaining task generalization of pre-trained models. We
evaluate the in-context learning performance of the model trained with PICL on
seven widely-used text classification datasets and the Super-NaturalInstrctions
benchmark, which contains 100+ NLP tasks formulated to text generation. Our
experiments show that PICL is more effective and task-generalizable than a
range of baselines, outperforming larger language models with nearly 4x
parameters. The code is publicly available at https://github.com/thu-coai/PICL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. (arXiv:2305.09144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09144">
<div class="article-summary-box-inner">
<span><p>Memory is one of the most essential cognitive functions serving as a
repository of world knowledge and episodes of activities. In recent years,
large-scale pre-trained language models have shown remarkable memorizing
ability. On the contrary, vanilla neural networks without pre-training have
been long observed suffering from the catastrophic forgetting problem. To
investigate such a retentive-forgetful contradiction and understand the memory
mechanism of language models, we conduct thorough experiments by controlling
the target knowledge types, the learning strategies and the learning schedules.
We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads
to retentive language models; 3) Knowledge relevance and diversification
significantly influence the memory formation. These conclusions are useful for
understanding the abilities of pre-trained language models and shed light on
designing and evaluating new learning and inference algorithms of language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Alignment Pre-training for Cross-lingual Sentence Embedding. (arXiv:2305.09148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09148">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that dual encoder models trained with the
sentence-level translation ranking task are effective methods for cross-lingual
sentence embedding. However, our research indicates that token-level alignment
is also crucial in multilingual scenarios, which has not been fully explored
previously. Based on our findings, we propose a dual-alignment pre-training
(DAP) framework for cross-lingual sentence embedding that incorporates both
sentence-level and token-level alignment. To achieve this, we introduce a novel
representation translation learning (RTL) task, where the model learns to use
one-side contextualized token representation to reconstruct its translation
counterpart. This reconstruction objective encourages the model to embed
translation information into the token representation. Compared to other
token-level alignment methods such as translation language modeling, RTL is
more suitable for dual encoder architectures and is computationally efficient.
Extensive experiments on three sentence-level cross-lingual benchmarks
demonstrate that our approach can significantly improve sentence embedding. Our
code is available at https://github.com/ChillingDream/DAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences. (arXiv:2305.09154v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09154">
<div class="article-summary-box-inner">
<span><p>Previous studies show that intermediate supervision signals benefit various
Natural Language Processing tasks. However, it is not clear whether there exist
intermediate signals that benefit Neural Machine Translation (NMT). Borrowing
techniques from Statistical Machine Translation, we propose intermediate
signals which are intermediate sequences from the "source-like" structure to
the "target-like" structure. Such intermediate sequences introduce an inductive
bias that reflects a domain-agnostic principle of translation, which reduces
spurious correlations that are harmful to out-of-domain generalisation.
Furthermore, we introduce a full-permutation multi-task learning to alleviate
the spurious causal relations from intermediate sequences to the target, which
results from exposure bias. The Minimum Bayes Risk decoding algorithm is used
to pick the best candidate translation from all permutations to further improve
the performance. Experiments show that the introduced intermediate signals can
effectively improve the domain robustness of NMT and reduces the amount of
hallucinations on out-of-domain translation. Further analysis shows that our
methods are especially promising in low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Speaker Disentanglement Using Unannotated External Data for Self-supervised Representation Based Voice Conversion. (arXiv:2305.09167v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09167">
<div class="article-summary-box-inner">
<span><p>Nowadays, recognition-synthesis-based methods have been quite popular with
voice conversion (VC). By introducing linguistics features with good
disentangling characters extracted from an automatic speech recognition (ASR)
model, the VC performance achieved considerable breakthroughs. Recently,
self-supervised learning (SSL) methods trained with a large-scale unannotated
speech corpus have been applied to downstream tasks focusing on the content
information, which is suitable for VC tasks. However, a huge amount of speaker
information in SSL representations degrades timbre similarity and the quality
of converted speech significantly. To address this problem, we proposed a
high-similarity any-to-one voice conversion method with the input of SSL
representations. We incorporated adversarial training mechanisms in the
synthesis module using external unannotated corpora. Two auxiliary
discriminators were trained to distinguish whether a sequence of
mel-spectrograms has been converted by the acoustic model and whether a
sequence of content embeddings contains speaker information from external
corpora. Experimental results show that our proposed method achieves comparable
similarity and higher naturalness than the supervised method, which needs a
huge amount of annotated corpora for training and is applicable to improve
similarity for VC methods with other SSL representations as input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy-to-Hard Learning for Information Extraction. (arXiv:2305.09193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09193">
<div class="article-summary-box-inner">
<span><p>Information extraction (IE) systems aim to automatically extract structured
information, such as named entities, relations between entities, and events,
from unstructured texts. While most existing work addresses a particular IE
task, universally modeling various IE tasks with one model has achieved great
success recently. Despite their success, they employ a one-stage learning
strategy, i.e., directly learning to extract the target structure given the
input text, which contradicts the human learning process. In this paper, we
propose a unified easy-to-hard learning framework consisting of three stages,
i.e., the easy stage, the hard stage, and the main stage, for IE by mimicking
the human learning process. By breaking down the learning process into multiple
stages, our framework facilitates the model to acquire general IE task
knowledge and improve its generalization ability. Extensive experiments across
four IE tasks demonstrate the effectiveness of our framework. We achieve new
state-of-the-art results on 13 out of 17 datasets. Our code is available at
\url{https://github.com/DAMO-NLP-SG/IE-E2H}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Weighted M\"obius Score: A Unified Framework for Feature Attribution. (arXiv:2305.09204v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09204">
<div class="article-summary-box-inner">
<span><p>Feature attribution aims to explain the reasoning behind a black-box model's
prediction by identifying the impact of each feature on the prediction. Recent
work has extended feature attribution to interactions between multiple
features. However, the lack of a unified framework has led to a proliferation
of methods that are often not directly comparable. This paper introduces a
parameterized attribution framework -- the Weighted M\"obius Score -- and (i)
shows that many different attribution methods for both individual features and
feature interactions are special cases and (ii) identifies some new methods. By
studying the vector space of attribution methods, our framework utilizes
standard linear algebra tools and provides interpretations in various fields,
including cooperative game theory and causal mediation analysis. We empirically
demonstrate the framework's versatility and effectiveness by applying these
attribution methods to feature interactions in sentiment analysis and
chain-of-thought prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Speech Dialogue Translation Mediating Speakers of Different Languages. (arXiv:2305.09210v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09210">
<div class="article-summary-box-inner">
<span><p>We present a new task, speech dialogue translation mediating speakers of
different languages. We construct the SpeechBSD dataset for the task and
conduct baseline experiments. Furthermore, we consider context to be an
important aspect that needs to be addressed in this task and propose two ways
of utilizing context, namely monolingual context and bilingual context. We
conduct cascaded speech translation experiments using Whisper and mBART, and
show that bilingual context performs better in our settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unifying Multi-Lingual and Cross-Lingual Summarization. (arXiv:2305.09220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09220">
<div class="article-summary-box-inner">
<span><p>To adapt text summarization to the multilingual world, previous work proposes
multi-lingual summarization (MLS) and cross-lingual summarization (CLS).
However, these two tasks have been studied separately due to the different
definitions, which limits the compatible and systematic research on both of
them. In this paper, we aim to unify MLS and CLS into a more general setting,
i.e., many-to-many summarization (M2MS), where a single model could process
documents in any language and generate their summaries also in any language. As
the first step towards M2MS, we conduct preliminary studies to show that M2MS
can better transfer task knowledge across different languages than MLS and CLS.
Furthermore, we propose Pisces, a pre-trained M2MS model that learns language
modeling, cross-lingual ability and summarization ability via three-stage
pre-training. Experimental results indicate that our Pisces significantly
outperforms the state-of-the-art baselines, especially in the zero-shot
directions, where there is no training data from the source-language documents
to the target-language summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. (arXiv:2305.09246v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09246">
<div class="article-summary-box-inner">
<span><p>Instruction tuning for large language models (LLMs) has gained attention from
researchers due to its ability to unlock the potential of LLMs in following
instructions. While instruction tuning offers advantages for facilitating the
adaptation of large language models (LLMs) to downstream tasks as a fine-tuning
approach, training models with tens of millions or even billions of parameters
on large amounts of data results in unaffordable computational costs. To
address this, we focus on reducing the data used in LLM instruction tuning to
decrease training costs and improve data efficiency, dubbed as Low Training
Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper
conducts a preliminary exploration into reducing the data used in LLM training
and identifies several observations regarding task specialization for LLM
training, such as the optimization of performance for a specific task, the
number of instruction types required for instruction tuning, and the amount of
data required for task-specific models. The results suggest that task-specific
models can be trained using less than 0.5% of the original dataset, with a 2%
improvement in performance over those trained on full task-related data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xPQA: Cross-Lingual Product Question Answering across 12 Languages. (arXiv:2305.09249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09249">
<div class="article-summary-box-inner">
<span><p>Product Question Answering (PQA) systems are key in e-commerce applications
to provide responses to customers' questions as they shop for products. While
existing work on PQA focuses mainly on English, in practice there is need to
support multiple customer languages while leveraging product information
available in English. To study this practical industrial task, we present xPQA,
a large-scale annotated cross-lingual PQA dataset in 12 languages across 9
branches, and report results in (1) candidate ranking, to select the best
English candidate containing the information to answer a non-English question;
and (2) answer generation, to generate a natural-sounding non-English answer
based on the selected English candidate. We evaluate various approaches
involving machine translation at runtime or offline, leveraging multilingual
pre-trained LMs, and including or excluding xPQA training data. We find that
(1) In-domain data is essential as cross-lingual rankers trained on other
domains perform poorly on the PQA task; (2) Candidate ranking often prefers
runtime-translation approaches while answer generation prefers multilingual
approaches; (3) Translating offline to augment multilingual models helps
candidate ranking mainly on languages with non-Latin scripts; and helps answer
generation mainly on languages with Latin scripts. Still, there remains a
significant performance gap between the English and the cross-lingual test
sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyHTM: Hyperbolic Geometry based Hierarchical Topic Models. (arXiv:2305.09258v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09258">
<div class="article-summary-box-inner">
<span><p>Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies
in a collection of documents. However, traditional HTMs often produce
hierarchies where lowerlevel topics are unrelated and not specific enough to
their higher-level topics. Additionally, these methods can be computationally
expensive. We present HyHTM - a Hyperbolic geometry based Hierarchical Topic
Models - that addresses these limitations by incorporating hierarchical
information from hyperbolic geometry to explicitly model hierarchies in topic
models. Experimental results with four baselines show that HyHTM can better
attend to parent-child relationships among topics. HyHTM produces coherent
topic hierarchies that specialise in granularity from generic higher-level
topics to specific lowerlevel topics. Further, our model is significantly
faster and leaves a much smaller memory footprint than our best-performing
baseline.We have made the source code for our algorithm publicly accessible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification. (arXiv:2305.09269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09269">
<div class="article-summary-box-inner">
<span><p>Few-shot text classification has recently been promoted by the meta-learning
paradigm which aims to identify target classes with knowledge transferred from
source classes with sets of small tasks named episodes. Despite their success,
existing works building their meta-learner based on Prototypical Networks are
unsatisfactory in learning discriminative text representations between similar
classes, which may lead to contradictions during label prediction. In addition,
the tasklevel and instance-level overfitting problems in few-shot text
classification caused by a few training examples are not sufficiently tackled.
In this work, we propose a contrastive learning framework named ContrastNet to
tackle both discriminative representation and overfitting problems in few-shot
text classification. ContrastNet learns to pull closer text representations
belonging to the same class and push away text representations belonging to
different classes, while simultaneously introducing unsupervised contrastive
regularization at both task-level and instance-level to prevent overfitting.
Experiments on 8 few-shot text classification datasets show that ContrastNet
outperforms the current state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Origins of Bias in NLP through the Lens of the Jim Code. (arXiv:2305.09281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09281">
<div class="article-summary-box-inner">
<span><p>In this paper, we trace the biases in current natural language processing
(NLP) models back to their origins in racism, sexism, and homophobia over the
last 500 years. We review literature from critical race theory, gender studies,
data ethics, and digital humanities studies, and summarize the origins of bias
in NLP models from these social science perspective. We show how the causes of
the biases in the NLP pipeline are rooted in social issues. Finally, we argue
that the only way to fix the bias and unfairness in NLP is by addressing the
social problems that caused them in the first place and by incorporating social
sciences and social scientists in efforts to mitigate bias in NLP models. We
provide actionable recommendations for the NLP research community to do so.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdversarialWord Dilution as Text Data Augmentation in Low-Resource Regime. (arXiv:2305.09287v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09287">
<div class="article-summary-box-inner">
<span><p>Data augmentation is widely used in text classification, especially in the
low-resource regime where a few examples for each class are available during
training. Despite the success, generating data augmentations as hard positive
examples that may increase their effectiveness is under-explored. This paper
proposes an Adversarial Word Dilution (AWD) method that can generate hard
positive examples as text data augmentations to train the low-resource text
classification model efficiently. Our idea of augmenting the text data is to
dilute the embedding of strong positive words by weighted mixing with
unknown-word embedding, making the augmented inputs hard to be recognized as
positive by the classification model. We adversarially learn the dilution
weights through a constrained min-max optimization process with the guidance of
the labels. Empirical studies on three benchmark datasets show that AWD can
generate more effective data augmentations and outperform the state-of-the-art
text data augmentation methods. The additional analysis demonstrates that the
data augmentations generated by AWD are interpretable and can flexibly extend
to new examples without further training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning. (arXiv:2305.09299v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09299">
<div class="article-summary-box-inner">
<span><p>Multimodal learning aims to imitate human beings to acquire complementary
information from multiple modalities for various downstream tasks. However,
traditional aggregation-based multimodal fusion methods ignore the
inter-modality relationship, treat each modality equally, suffer sensor noise,
and thus reduce multimodal learning performance. In this work, we propose a
novel multimodal contrastive method to explore more reliable multimodal
representations under the weak supervision of unimodal predicting.
Specifically, we first capture task-related unimodal representations and the
unimodal predictions from the introduced unimodal predicting task. Then the
unimodal representations are aligned with the more effective one by the
designed multimodal contrastive method under the supervision of the unimodal
predictions. Experimental results with fused features on two image-text
classification benchmarks UPMC-Food-101 and N24News show that our proposed
Unimodality-Supervised MultiModal Contrastive UniS-MMC learning method
outperforms current state-of-the-art multimodal methods. The detailed ablation
study and analysis further demonstrate the advantage of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation. (arXiv:2305.09312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09312">
<div class="article-summary-box-inner">
<span><p>This paper studies the impact of layer normalization (LayerNorm) on zero-shot
translation (ZST). Recent efforts for ZST often utilize the Transformer
architecture as the backbone, with LayerNorm at the input of layers (PreNorm)
set as the default. However, Xu et al. (2019) has revealed that PreNorm carries
the risk of overfitting the training data. Based on this, we hypothesize that
PreNorm may overfit supervised directions and thus have low generalizability
for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST
directions, we demonstrate that the original Transformer setting of LayerNorm
after residual connections (PostNorm) consistently outperforms PreNorm by up to
12.3 BLEU points. We then study the performance disparities by analyzing the
differences in off-target rates and structural variations between PreNorm and
PostNorm. This study highlights the need for careful consideration of the
LayerNorm setting for ZST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid and Collaborative Passage Reranking. (arXiv:2305.09313v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09313">
<div class="article-summary-box-inner">
<span><p>In passage retrieval system, the initial passage retrieval results may be
unsatisfactory, which can be refined by a reranking scheme. Existing solutions
to passage reranking focus on enriching the interaction between query and each
passage separately, neglecting the context among the top-ranked passages in the
initial retrieval list. To tackle this problem, we propose a Hybrid and
Collaborative Passage Reranking (HybRank) method, which leverages the
substantial similarity measurements of upstream retrievers for passage
collaboration and incorporates the lexical and semantic properties of sparse
and dense retrievers for reranking. Besides, built on off-the-shelf retriever
features, HybRank is a plug-in reranker capable of enhancing arbitrary passage
lists including previously reranked ones. Extensive experiments demonstrate the
stable improvements of performance over prevalent retrieval and reranking
methods, and verify the effectiveness of the core components of HybRank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings. (arXiv:2305.09316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09316">
<div class="article-summary-box-inner">
<span><p>In this study, we investigate using graph neural network (GNN)
representations to enhance contextualized representations of pre-trained
language models (PLMs) for keyphrase extraction from lengthy documents. We show
that augmenting a PLM with graph embeddings provides a more comprehensive
semantic understanding of words in a document, particularly for long documents.
We construct a co-occurrence graph of the text and embed it using a graph
convolutional network (GCN) trained on the task of edge prediction. We propose
a graph-enhanced sequence tagging architecture that augments contextualized PLM
embeddings with graph representations. Evaluating on benchmark datasets, we
demonstrate that enhancing PLMs with graph embeddings outperforms
state-of-the-art models on long documents, showing significant improvements in
F1 scores across all the datasets. Our study highlights the potential of GNN
representations as a complementary approach to improve PLM performance for
keyphrase extraction from long documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09329">
<div class="article-summary-box-inner">
<span><p>With the development of neural topic models in recent years, topic modelling
is playing an increasingly important role in natural language understanding.
However, most existing topic models still rely on bag-of-words (BoW)
information, either as training input or training target. This limits their
ability to capture word order information in documents and causes them to
suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle
unobserved words in new documents. Contextualized word embeddings from
pre-trained language models show superiority in the ability of word sense
disambiguation and prove to be effective in dealing with OOV words. In this
work, we developed a novel neural topic model combining contextualized word
embeddings from the pre-trained language model BERT. The model can infer the
topic distribution of a document without using any BoW information. In
addition, the model can infer the topic distribution of each word in a document
directly from the contextualized word embeddings. Experiments on several
datasets show that our model outperforms existing topic models in terms of both
document classification and topic coherence metrics and can accommodate unseen
words from newly arrived documents. Experiments on the NER dataset also show
that our model can produce high-quality word topic representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image. (arXiv:2305.09333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09333">
<div class="article-summary-box-inner">
<span><p>Multi-modal visual understanding of images with prompts involves using
various visual and textual cues to enhance the semantic understanding of
images. This approach combines both vision and language processing to generate
more accurate predictions and recognition of images. By utilizing prompt-based
techniques, models can learn to focus on certain features of an image to
extract useful information for downstream tasks. Additionally, multi-modal
understanding can improve upon single modality models by providing more robust
representations of images. Overall, the combination of visual and textual
information is a promising area of research for advancing image recognition and
understanding. In this paper we will try an amount of prompt design methods and
propose a new method for better extraction of semantic information
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MsPrompt: Multi-step Prompt Learning for Debiasing Few-shot Event Detection. (arXiv:2305.09335v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09335">
<div class="article-summary-box-inner">
<span><p>Event detection (ED) is aimed to identify the key trigger words in
unstructured text and predict the event types accordingly. Traditional ED
models are too data-hungry to accommodate real applications with scarce labeled
data. Besides, typical ED models are facing the context-bypassing and disabled
generalization issues caused by the trigger bias stemming from ED datasets.
Therefore, we focus on the true few-shot paradigm to satisfy the low-resource
scenarios. In particular, we propose a multi-step prompt learning model
(MsPrompt) for debiasing few-shot event detection, that consists of the
following three components: an under-sampling module targeting to construct a
novel training set that accommodates the true few-shot setting, a multi-step
prompt module equipped with a knowledge-enhanced ontology to leverage the event
semantics and latent prior knowledge in the PLMs sufficiently for tackling the
context-bypassing problem, and a prototypical module compensating for the
weakness of classifying events with sparse data and boost the generalization
performance. Experiments on two public datasets ACE-2005 and FewEvent show that
MsPrompt can outperform the state-of-the-art models, especially in the strict
low-resource scenarios reporting 11.43% improvement in terms of weighted
F1-score against the best-performing baseline and achieving an outstanding
debiasing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing and Interpreting Causal Knowledge Graphs from News. (arXiv:2305.09359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09359">
<div class="article-summary-box-inner">
<span><p>Many jobs rely on news to learn about causal events in the past and present,
to make informed decisions and predictions about the future. With the
ever-increasing amount of news and text available on the internet, there is a
need to automate the extraction of causal events from unstructured texts. In
this work, we propose a methodology to construct causal knowledge graphs (KGs)
from news using two steps: (1) Extraction of Causal Relations, and (2) Argument
Clustering and Representation into KG. We aim to build graphs that emphasize on
recall, precision and interpretability. For extraction, although many earlier
works already construct causal KGs from text, most adopt rudimentary
pattern-based methods. We close this gap by using the latest BERT-based
extraction models alongside pattern-based ones. As a result, we achieved a high
recall, while still maintaining a high precision. For clustering, we utilized a
topic modelling approach to cluster our arguments, so as to increase the
connectivity of our graph. As a result, instead of 15,686 disconnected
subgraphs, we were able to obtain 1 connected graph that enables users to infer
more causal relationships from. Our final KG effectively captures and conveys
causal relationships, validated through multiple use cases and user feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09360">
<div class="article-summary-box-inner">
<span><p>Addressing the issues of who saying what to whom in multi-party conversations
(MPCs) has recently attracted a lot of research attention. However, existing
methods on MPC understanding typically embed interlocutors and utterances into
sequential information flows, or utilize only the superficial of inherent graph
structures in MPCs. To this end, we present a plug-and-play and lightweight
method named graph-induced fine-tuning (GIFT) which can adapt various
Transformer-based pre-trained language models (PLMs) for universal MPC
understanding. In detail, the full and equivalent connections among utterances
in regular Transformer ignore the sparse but distinctive dependency of an
utterance on another in MPCs. To distinguish different relationships between
utterances, four types of edges are designed to integrate graph-induced signals
into attention mechanisms to refine PLMs originally designed for processing
sequential texts. We evaluate GIFT by implementing it into three PLMs, and test
the performance on three downstream tasks including addressee recognition,
speaker identification and response selection. Experimental results show that
GIFT can significantly improve the performance of three PLMs on three
downstream tasks and two benchmarks with only 4 additional parameters per
encoding layer, achieving new state-of-the-art performance on MPC
understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification. (arXiv:2305.09400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09400">
<div class="article-summary-box-inner">
<span><p>The success of deep learning models on multi-hop fact verification has
prompted researchers to understand the behavior behind their veracity. One
possible way is erasure search: obtaining the rationale by entirely removing a
subset of input without compromising the veracity prediction. Although
extensively explored, existing approaches fall within the scope of the
single-granular (tokens or sentences) explanation, which inevitably leads to
explanation redundancy and inconsistency. To address such issues, this paper
explores the viability of multi-granular rationale extraction with consistency
and faithfulness for explainable multi-hop fact verification. In particular,
given a pretrained veracity prediction model, both the token-level explainer
and sentence-level explainer are trained simultaneously to obtain
multi-granular rationales via differentiable masking. Meanwhile, three
diagnostic properties (fidelity, consistency, salience) are introduced and
applied to the training process, to ensure that the extracted rationales
satisfy faithfulness and consistency. Experimental results on three multi-hop
fact verification datasets show that the proposed approach outperforms some
state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5 and Bard AI Models for Java Functions. (arXiv:2305.09402v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09402">
<div class="article-summary-box-inner">
<span><p>This paper evaluates the capability of two state-of-the-art artificial
intelligence (AI) models, GPT-3.5 and Bard, in generating Java code given a
function description. We sourced the descriptions from CodingBat.com, a popular
online platform that provides practice problems to learn programming. We
compared the Java code generated by both models based on correctness, verified
through the platform's own test cases. The results indicate clear differences
in the capabilities of the two models. GPT-3.5 demonstrated superior
performance, generating correct code for approximately 90.6% of the function
descriptions, whereas Bard produced correct code for 53.1% of the functions.
While both models exhibited strengths and weaknesses, these findings suggest
potential avenues for the development and refinement of more advanced
AI-assisted code generation tools. The study underlines the potential of AI in
automating and supporting aspects of software development, although further
research is required to fully realize this potential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">About Evaluation of F1 Score for RECENT Relation Extraction System. (arXiv:2305.09410v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09410">
<div class="article-summary-box-inner">
<span><p>This document contains a discussion of the F1 score evaluation used in the
article 'Relation Classification with Entity Type Restriction' by Shengfei Lyu,
Huanhuan Chen published on Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and
claim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on
the TACRED dataset, while after correcting errors and reevaluation the final
result is 65.16
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09438">
<div class="article-summary-box-inner">
<span><p>Automatic source-to-source parallelization of serial code for shared and
distributed memory systems is a challenging task in high-performance computing.
While many attempts were made to translate serial code into parallel code for a
shared memory environment (usually using OpenMP), none has managed to do so for
a distributed memory environment. In this paper, we propose a novel approach,
called MPI-rical, for automated MPI code generation using a transformer-based
model trained on approximately 25,000 serial code snippets and their
corresponding parallelized MPI code out of more than 50,000 code snippets in
our corpus (MPICodeCorpus). To evaluate the performance of the model, we first
break down the serial code to MPI-based parallel code translation problem into
two sub-problems and develop two research objectives: code completion defined
as given a location in the source code, predict the MPI function for that
location, and code translation defined as predicting an MPI function as well as
its location in the source code. We evaluate MPI-rical on MPICodeCorpus dataset
and on real-world scientific code benchmarks and compare its performance
between the code completion and translation tasks. Our experimental results
show that while MPI-rical performs better on the code completion task than the
code translation task, the latter is better suited for real-world programming
assistance, in which the tool suggests the need for an MPI function regardless
of prior knowledge. Overall, our approach represents a significant step forward
in automating the parallelization of serial code for distributed memory
systems, which can save valuable time and resources for software developers and
researchers. The source code used in this work, as well as other relevant
sources, are available at:
https://github.com/Scientific-Computing-Lab-NRCN/MPI-rical
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fuzzy Temporal Protoforms for the Quantitative Description of Processes in Natural Language. (arXiv:2305.09506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09506">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a series of fuzzy temporal protoforms in the
framework of the automatic generation of quantitative and qualitative natural
language descriptions of processes. The model includes temporal and causal
information from processes and attributes, quantifies attributes in time during
the process life-span and recalls causal relations and temporal distances
between events, among other features. Through integrating process mining
techniques and fuzzy sets within the usual Data-to-Text architecture, our
framework is able to extract relevant quantitative temporal as well as
structural information from a process and describe it in natural language
involving uncertain terms. A real use-case in the cardiology domain is
presented, showing the potential of our model for providing natural language
explanations addressed to domain experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis. (arXiv:2305.09509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09509">
<div class="article-summary-box-inner">
<span><p>Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various
fine-grained sentiment analysis tasks on a target domain by transferring
knowledge from a source domain. Since labeled data only exists in the source
domain, a model is expected to bridge the domain gap for tackling cross-domain
ABSA. Though domain adaptation methods have proven to be effective, most of
them are based on a discriminative model, which needs to be specifically
designed for different ABSA tasks. To offer a more general solution, we propose
a unified bidirectional generative framework to tackle various cross-domain
ABSA tasks. Specifically, our framework trains a generative model in both
text-to-label and label-to-text directions. The former transforms each task
into a unified format to learn domain-agnostic features, and the latter
generates natural sentences from noisy labels for data augmentation, with which
a more accurate model can be trained. To investigate the effectiveness and
generality of our framework, we conduct extensive experiments on four
cross-domain ABSA tasks and present new state-of-the-art results on all tasks.
Our data and code are publicly available at
\url{https://github.com/DAMO-NLP-SG/BGCA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09515">
<div class="article-summary-box-inner">
<span><p>Diffusion models have gained significant attention in the realm of image
generation due to their exceptional performance. Their success has been
recently expanded to text generation via generating all tokens within a
sequence concurrently. However, natural language exhibits a far more pronounced
sequential dependency in comparison to images, and the majority of existing
language models are trained utilizing a left-to-right auto-regressive approach.
To account for the inherent sequential characteristic of natural language, we
introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that
the generation of tokens on the right depends on the generated ones on the
left, a mechanism achieved through employing a dynamic number of denoising
steps that vary based on token position. This results in tokens on the left
undergoing fewer denoising steps than those on the right, thereby enabling them
to generate earlier and subsequently influence the generation of tokens on the
right. In a series of experiments on various text generation tasks including
text summarization, machine translation, and common sense generation,
AR-Diffusion clearly demonstrated the superiority over existing diffusion
language models and that it can be $100\times\sim600\times$ faster when
achieving comparable results. Our code will be publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DLUE: Benchmarking Document Language Understanding. (arXiv:2305.09520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09520">
<div class="article-summary-box-inner">
<span><p>Understanding documents is central to many real-world tasks but remains a
challenging topic. Unfortunately, there is no well-established consensus on how
to comprehensively evaluate document understanding abilities, which
significantly hinders the fair comparison and measuring the progress of the
field. To benchmark document understanding researches, this paper summarizes
four representative abilities, i.e., document classification, document
structural analysis, document information extraction, and document
transcription. Under the new evaluation framework, we propose \textbf{Document
Language Understanding Evaluation} -- \textbf{DLUE}, a new task suite which
covers a wide-range of tasks in various forms, domains and document genres. We
also systematically evaluate six well-established transformer models on DLUE,
and find that due to the lengthy content, complicated underlying structure and
dispersed knowledge, document understanding is still far from being solved, and
currently there is no neural architecture that dominates all tasks, raising
requirements for a universal document understanding architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaSRL++: A Uniform Scheme for Modelling Deeper Semantics. (arXiv:2305.09534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09534">
<div class="article-summary-box-inner">
<span><p>Despite enormous progress in Natural Language Processing (NLP), our field is
still lacking a common deep semantic representation scheme. As a result, the
problem of meaning and understanding is typically sidestepped through more
simple, approximative methods. This paper argues that in order to arrive at
such a scheme, we also need a common modelling scheme. It therefore introduces
MetaSRL++, a uniform, language- and modality-independent modelling scheme based
on Semantic Graphs, as a step towards a common representation scheme; as well
as a method for defining the concepts and entities that are used in these
graphs. Our output is twofold. First, we illustrate MetaSRL++ through concrete
examples. Secondly, we discuss how it relates to existing work in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Stereotypes using Entity-Centric Data. (arXiv:2305.09548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09548">
<div class="article-summary-box-inner">
<span><p>Stereotypes inform how we present ourselves and others, and in turn how we
behave. They are thus important to measure. Recent work has used projections of
embeddings from Distributional Semantic Models (DSMs), such as BERT, to perform
these measurements. However, DSMs capture cognitive associations that are not
necessarily relevant to the interpersonal nature of stereotyping. Here, we
propose and evaluate three novel, entity-centric methods for learning
stereotypes from Twitter and Wikipedia biographies. Models are trained by
leveraging the fact that multiple phrases are applied to the same person,
magnifying the person-centric nature of the learned associations. We show that
these models outperform existing approaches to stereotype measurement with
respect to 1) predicting which identities people apply to themselves and
others, and 2) quantifying stereotypes on salient social dimensions (e.g.
gender). Via a case study, we also show the utility of these models for future
questions in computational social science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09550">
<div class="article-summary-box-inner">
<span><p>Protecting sensitive information is crucial in today's world of Large
Language Models (LLMs) and data-driven services. One common method used to
preserve privacy is by using data perturbation techniques to reduce
overreaching utility of (sensitive) Personal Identifiable Information (PII)
data while maintaining its statistical and semantic properties. Data
perturbation methods often result in significant information loss, making them
impractical for use. In this paper, we propose 'Life of PII', a novel
Obfuscation Transformer framework for transforming PII into faux-PII while
preserving the original information, intent, and context as much as possible.
Our approach includes an API to interface with the given document, a
configuration-based obfuscator, and a model based on the Transformer
architecture, which has shown high context preservation and performance in
natural language processing tasks and LLMs.
</p>
<p>Our Transformer-based approach learns mapping between the original PII and
its transformed faux-PII representation, which we call "obfuscated" data. Our
experiments demonstrate that our method, called Life of PII, outperforms
traditional data perturbation techniques in terms of both utility preservation
and privacy protection. We show that our approach can effectively reduce
utility loss while preserving the original information, offering greater
flexibility in the trade-off between privacy protection and data utility. Our
work provides a solution for protecting PII in various real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09556">
<div class="article-summary-box-inner">
<span><p>Learning effective sentence representations is crucial for many Natural
Language Processing (NLP) tasks, including semantic search, semantic textual
similarity (STS), and clustering. While multiple transformer models have been
developed for sentence embedding learning, these models may not perform
optimally when dealing with specialized domains like aviation, which has unique
characteristics such as technical jargon, abbreviations, and unconventional
grammar. Furthermore, the absence of labeled datasets makes it difficult to
train models specifically for the aviation domain. To address these challenges,
we propose a novel approach for adapting sentence transformers for the aviation
domain. Our method is a two-stage process consisting of pre-training followed
by fine-tuning. During pre-training, we use Transformers and Sequential
Denoising AutoEncoder (TSDAE) with aviation text data as input to improve the
initial model performance. Subsequently, we fine-tune our models using a
Natural Language Inference (NLI) dataset in the Sentence Bidirectional Encoder
Representations from Transformers (SBERT) architecture to mitigate overfitting
issues. Experimental results on several downstream tasks show that our adapted
sentence transformers significantly outperform general-purpose transformers,
demonstrating the effectiveness of our approach in capturing the nuances of the
aviation domain. Overall, our work highlights the importance of domain-specific
adaptation in developing high-quality NLP solutions for specialized industries
like aviation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UOR: Universal Backdoor Attacks on Pre-trained Language Models. (arXiv:2305.09574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09574">
<div class="article-summary-box-inner">
<span><p>Backdoors implanted in pre-trained language models (PLMs) can be transferred
to various downstream tasks, which exposes a severe security threat. However,
most existing backdoor attacks against PLMs are un-targeted and task-specific.
Few targeted and task-agnostic methods use manually pre-defined triggers and
output representations, which prevent the attacks from being more effective and
general. In this paper, we first summarize the requirements that a more
threatening backdoor attack against PLMs should satisfy, and then propose a new
backdoor attack method called UOR, which breaks the bottleneck of the previous
approach by turning manual selection into automatic optimization. Specifically,
we define poisoned supervised contrastive learning which can automatically
learn the more uniform and universal output representations of triggers for
various PLMs. Moreover, we use gradient search to select appropriate trigger
words which can be adaptive to different PLMs and vocabularies. Experiments
show that our method can achieve better attack performance on various text
classification tasks compared to manual methods. Further, we tested our method
on PLMs with different architectures, different usage paradigms, and more
difficult tasks, which demonstrated the universality of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Event Extraction with Denoised Structure-to-Text Augmentation. (arXiv:2305.09598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09598">
<div class="article-summary-box-inner">
<span><p>Event extraction aims to recognize pre-defined event triggers and arguments
from texts, which suffer from the lack of high-quality annotations. In most NLP
applications, involving a large scale of synthetic training data is a practical
and effective approach to alleviate the problem of data scarcity. However, when
applying to the task of event extraction, recent data augmentation methods
often neglect the problem of grammatical incorrectness, structure misalignment,
and semantic drifting, leading to unsatisfactory performances. In order to
solve these problems, we propose a denoised structure-to-text augmentation
framework for event extraction DAEE, which generates additional training data
through the knowledge-based structure-to-text generation model and selects the
effective subset from the generated data iteratively with a deep reinforcement
learning agent. Experimental results on several datasets demonstrate that the
proposed method generates more diverse text representations for event
extraction and achieves comparable results with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Built-in Autoregressive Search Engines. (arXiv:2305.09612v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09612">
<div class="article-summary-box-inner">
<span><p>Document retrieval is a key stage of standard Web search engines. Existing
dual-encoder dense retrievers obtain representations for questions and
documents independently, allowing for only shallow interactions between them.
To overcome this limitation, recent autoregressive search engines replace the
dual-encoder architecture by directly generating identifiers for relevant
documents in the candidate pool. However, the training cost of such
autoregressive search engines rises sharply as the number of candidate
documents increases. In this paper, we find that large language models (LLMs)
can follow human instructions to directly generate URLs for document retrieval.
</p>
<p>Surprisingly, when providing a few {Query-URL} pairs as in-context
demonstrations, LLMs can generate Web URLs where nearly 90\% of the
corresponding documents contain correct answers to open-domain questions. In
this way, LLMs can be thought of as built-in search engines, since they have
not been explicitly trained to map questions to document identifiers.
Experiments demonstrate that our method can consistently achieve better
retrieval performance than existing retrieval approaches by a significant
margin on three open-domain question answering benchmarks, under both zero and
few-shot settings. The code for this work can be found at
\url{https://github.com/Ziems/llm-url}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Expert-Level Medical Question Answering with Large Language Models. (arXiv:2305.09617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09617">
<div class="article-summary-box-inner">
<span><p>Recent artificial intelligence (AI) systems have reached milestones in "grand
challenges" ranging from Go to protein-folding. The capability to retrieve
medical knowledge, reason over it, and answer medical questions comparably to
physicians has long been viewed as one such grand challenge.
</p>
<p>Large language models (LLMs) have catalyzed significant progress in medical
question answering; Med-PaLM was the first model to exceed a "passing" score in
US Medical Licensing Examination (USMLE) style questions with a score of 67.2%
on the MedQA dataset. However, this and other prior work suggested significant
room for improvement, especially when models' answers were compared to
clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by
leveraging a combination of base LLM improvements (PaLM 2), medical domain
finetuning, and prompting strategies including a novel ensemble refinement
approach.
</p>
<p>Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM
by over 19% and setting a new state-of-the-art. We also observed performance
approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU
clinical topics datasets.
</p>
<p>We performed detailed human evaluations on long-form questions along multiple
axes relevant to clinical applications. In pairwise comparative ranking of 1066
consumer medical questions, physicians preferred Med-PaLM 2 answers to those
produced by physicians on eight of nine axes pertaining to clinical utility (p
&lt; 0.001). We also observed significant improvements compared to Med-PaLM on
every evaluation axis (p &lt; 0.001) on newly introduced datasets of 240 long-form
"adversarial" questions to probe LLM limitations.
</p>
<p>While further studies are necessary to validate the efficacy of these models
in real-world settings, these results highlight rapid progress towards
physician-level performance in medical question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09620">
<div class="article-summary-box-inner">
<span><p>How can we use large language models (LLMs) to augment surveys? This paper
investigates three distinct applications of LLMs fine-tuned by nationally
representative surveys for opinion prediction -- missing data imputation,
retrodiction, and zero-shot prediction. We present a new methodological
framework that incorporates neural embeddings of survey questions, individual
beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among
3,110 binarized opinions from 68,846 Americans in the General Social Survey
from 1972 to 2021, our best models based on Alpaca-7b excels in missing data
imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for
public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These
remarkable prediction capabilities allow us to fill in missing trends with high
confidence and pinpoint when public attitudes changed, such as the rising
support for same-sex marriage. However, the models show limited performance in
a zero-shot prediction task (AUC = 0.73, $\rho$ = 0.67), highlighting
challenges presented by LLMs without human responses. Further, we find that the
best models' accuracy is lower for individuals with low socioeconomic status,
racial minorities, and non-partisan affiliations but higher for ideologically
sorted opinions in contemporary periods. We discuss practical constraints,
socio-demographic representation, and ethical concerns regarding individual
autonomy and privacy when using LLMs for opinion prediction. This paper
showcases a new approach for leveraging LLMs to enhance nationally
representative surveys by predicting missing responses and trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructGPT: A General Framework for Large Language Model to Reason over Structured Data. (arXiv:2305.09645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09645">
<div class="article-summary-box-inner">
<span><p>In this paper, we study how to improve the zero-shot reasoning ability of
large language models~(LLMs) over structured data in a unified way. Inspired by
the study on tool augmentation for LLMs, we develop an \emph{Iterative
Reading-then-Reasoning~(IRR)} approach for solving question answering tasks
based on structured data, called \textbf{StructGPT}. In our approach, we
construct the specialized function to collect relevant evidence from structured
data (\ie \emph{reading}), and let LLMs concentrate the reasoning task based on
the collected information (\ie \emph{reasoning}). Specially, we propose an
\emph{invoking-linearization-generation} procedure to support LLMs in reasoning
on the structured data with the help of the external interfaces. By iterating
this procedures with provided interfaces, our approach can gradually approach
the target answer to a given query. Extensive experiments conducted on three
types of structured data demonstrate the effectiveness of our approach, which
can significantly boost the performance of ChatGPT and achieve comparable
performance against the full-data supervised-tuning baselines. Our codes and
data are publicly available at~\url{https://github.com/RUCAIBox/StructGPT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09651">
<div class="article-summary-box-inner">
<span><p>It has been commonly observed that a teacher model with superior performance
does not necessarily result in a stronger student, highlighting a discrepancy
between current teacher training practices and effective knowledge transfer. In
order to enhance the guidance of the teacher training process, we introduce the
concept of distillation influence to determine the impact of distillation from
each training sample on the student's generalization ability. In this paper, we
propose Learning Good Teacher Matters (LGTM), an efficient training technique
for incorporating distillation influence into the teacher's learning process.
By prioritizing samples that are likely to enhance the student's generalization
ability, our LGTM outperforms 10 common knowledge distillation baselines on 6
text classification tasks in the GLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09652">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) remains elusive even with
current large pretrained language models on text and speech, especially in
multilingual cases. Machine translation has been established as a powerful
pretraining objective on text as it enables the model to capture high-level
semantics of the input utterance and associations between different languages,
which is desired for speech models that work on lower-level acoustic frames.
Motivated particularly by the task of cross-lingual SLU, we demonstrate that
the task of speech translation (ST) is a good means of pretraining speech
models for end-to-end SLU on both monolingual and cross-lingual scenarios.
</p>
<p>By introducing ST, our models give higher performance over current baselines
on monolingual and multilingual intent classification as well as spoken
question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the
effectiveness of our methods, we also release two new benchmark datasets from
both synthetic and real sources, for the tasks of abstractive summarization
from speech and low-resource or zero-shot transfer from English to French. We
further show the value of preserving knowledge from the pretraining task, and
explore Bayesian transfer learning on pretrained speech models based on
continual learning regularizers for that.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09656">
<div class="article-summary-box-inner">
<span><p>Prior work has combined chain-of-thought prompting in large language models
(LLMs) with programmatic representations to perform effective and transparent
reasoning. While such an approach works very well for tasks that only require
forward reasoning (e.g., straightforward arithmetic), it is less effective for
constraint solving tasks that require more sophisticated planning and search.
In this paper, we propose a new satisfiability-aided language modeling approach
for improving the reasoning capabilities of LLMs. We use an LLM to generate a
declarative task specification rather than an imperative program and leverage
an off-the-shelf automated theorem prover to derive the final answer. This
approach has two key advantages. The declarative specification is closer to the
problem description than the reasoning steps are, so the LLM can parse it more
accurately. Furthermore, by offloading the actual reasoning task to an
automated theorem prover, our approach can guarantee the correctness of the
answer with respect to the parsed specification and avoid planning errors in
the reasoning process. We evaluate SATLM on 6 different datasets and show that
it consistently outperforms program-aided LMs in an imperative paradigm
(PROGLM). In particular, SATLM outperforms PROGLM by 23% on a challenging
subset of GSM; SATLM also achieves a new SoTA on LSAT, surpassing previous
models that are trained on the full training set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Korean Corpora: A Practical Report. (arXiv:2012.15621v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15621">
<div class="article-summary-box-inner">
<span><p>Korean is often referred to as a low-resource language in the research
community. While this claim is partially true, it is also because the
availability of resources is inadequately advertised and curated. This work
curates and reviews a list of Korean corpora, first describing
institution-level resource development, then further iterate through a list of
current open datasets for different types of tasks. We then propose a direction
on how open-source dataset construction and releases should be done for
less-resourced languages to promote research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v6 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08614">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, systems from the IR
and NLP communities have addressed QA over text, but such systems barely
utilize semantic data and knowledge. This paper presents a method for complex
questions that can seamlessly operate over a mixture of RDF datasets and text
corpora, or individual sources, in a unified framework. Our method, called
UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant
evidences from the RDF data and/or a text corpus, using fine-tuned BERT models.
The resulting graph is typically contains all question-relevant evidences but
also a lot of noise. UNIQORN copes with this input by a graph algorithm for
Group Steiner Trees, that identifies the best answer candidates in the context
graph. Experimental results on several benchmarks of complex questions with
multiple entities and relations, show that UNIQORN significantly outperforms
state-of-the-art methods for heterogeneous QA. The graph-based methodology
provides user-interpretable evidence for the complete answering process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Implicit Sentiment Learning via Local Sentiment Aggregation. (arXiv:2110.08604v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08604">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment classification (ABSC) has revealed the potential
dependency of sentiment polarities among different aspects. Our study further
explores this phenomenon, positing that adjacent aspects often exhibit similar
sentiments, a concept we term "aspect sentiment coherency." We argue that the
current research landscape has not fully appreciated the significance of
modeling aspect sentiment coherency. To address this gap, we introduce a local
sentiment aggregation paradigm (LSA) that facilitates fine-grained sentiment
coherency modeling. This approach enables the extraction of implicit sentiments
for aspects lacking explicit sentiment descriptions. Leveraging gradient
descent, we design a differential-weighted sentiment aggregation window that
guides the modeling of aspect sentiment coherency. Experimental results affirm
the efficacy of LSA in learning sentiment coherency, as it achieves
state-of-the-art performance across three public datasets, thus significantly
enhancing existing ABSC models. We have made our code available, providing a
ready tool for existing methods to harness the potential of sentiment coherency
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation. (arXiv:2203.05386v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05386">
<div class="article-summary-box-inner">
<span><p>Despite recent advances in detecting fake news generated by neural models,
their results are not readily applicable to effective detection of
human-written disinformation. What limits the successful transfer between them
is the sizable gap between machine-generated fake news and human-authored ones,
including the notable differences in terms of style and underlying intent. With
this in mind, we propose a novel framework for generating training examples
that are informed by the known styles and strategies of human-authored
propaganda. Specifically, we perform self-critical sequence training guided by
natural language inference to ensure the validity of the generated articles,
while also incorporating propaganda techniques, such as appeal to authority and
loaded language. In particular, we create a new training dataset, PropaNews,
with 2,256 examples, which we release for future use. Our experimental results
show that fake news detectors trained on PropaNews are better at detecting
human-written disinformation by 3.62 - 7.69% F1 score on two public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Aligned Simple German Corpus. (arXiv:2209.01106v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.01106">
<div class="article-summary-box-inner">
<span><p>"Leichte Sprache", the German counterpart to Simple English, is a regulated
language aiming to facilitate complex written language that would otherwise
stay inaccessible to different groups of people. We present a new
sentence-aligned monolingual corpus for Simple German -- German. It contains
multiple document-aligned sources which we have aligned using automatic
sentence-alignment methods. We evaluate our alignments based on a manually
labelled subset of aligned documents. The quality of our sentence alignments,
as measured by F1-score, surpasses previous work. We publish the dataset under
CC BY-SA and the accompanying code under MIT license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Temporal Adaptation for Social Media Topic Classification. (arXiv:2209.05706v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05706">
<div class="article-summary-box-inner">
<span><p>User-generated social media data is constantly changing as new trends
influence online discussion and personal information is deleted due to privacy
concerns. However, most current NLP models are static and rely on fixed
training data, which means they are unable to adapt to temporal change -- both
test distribution shift and deleted training data -- without frequent, costly
re-training. In this paper, we study temporal adaptation through the task of
longitudinal hashtag prediction and propose a non-parametric dense retrieval
technique, which does not require re-training, as a simple but effective
solution. In experiments on a newly collected, publicly available, year-long
Twitter dataset exhibiting temporal distribution shift, our method improves by
64.12% over the best parametric baseline without any of its costly
gradient-based updating. Our dense retrieval approach is also particularly
well-suited to dynamically deleted user data in line with data privacy laws,
with negligible computational cost and performance loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10372">
<div class="article-summary-box-inner">
<span><p>Large Language Models pre-trained with self-supervised learning have
demonstrated impressive zero-shot generalization capabilities on a wide
spectrum of tasks. In this work, we present WeLM: a well-read pre-trained
language model for Chinese that is able to seamlessly perform different types
of tasks with zero or few-shot demonstrations. WeLM is trained with 10B
parameters by "reading" a curated high-quality corpus covering a wide range of
topics. We show that WeLM is equipped with broad knowledge on various domains
and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly
outperform existing pre-trained models with similar sizes and match the
performance of models up to 25 times larger. WeLM also exhibits strong
capabilities in multi-lingual and code-switching understanding, outperforming
existing multilingual language models pre-trained on 30 languages. Furthermore,
We collected human-written prompts for a large set of supervised datasets in
Chinese and fine-tuned WeLM with multi-prompted training. The resulting model
can attain strong generalization on unseen types of tasks and outperform the
unsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has
basic skills at explaining and calibrating the decisions from itself, which can
be promising directions for future research. Our models can be applied from
https://welm.weixin.qq.com/docs/api/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers. (arXiv:2209.12816v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.12816">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models utilize the attention mechanism for
substantial performance improvements in almost all natural language processing
(NLP) tasks. Similar attention structures are also extensively studied in
several other areas. Although the attention mechanism enhances the model
performances significantly, its quadratic complexity prevents efficient
processing of long sequences. Recent works focused on eliminating the
disadvantages of computational inefficiency and showed that transformer-based
models can still reach competitive results without the attention layer. A
pioneering study proposed the FNet, which replaces the attention layer with the
Fourier Transform (FT) in the transformer encoder architecture. FNet achieves
competitive performances concerning the original transformer encoder model
while accelerating training process by removing the computational burden of the
attention mechanism. However, the FNet model ignores essential properties of
the FT from the classical signal processing that can be leveraged to increase
model efficiency further. We propose different methods to deploy FT efficiently
in transformer encoder models. Our proposed architectures have smaller number
of model parameters, shorter training times, less memory usage, and some
additional performance improvements. We demonstrate these improvements through
extensive experiments on common benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15206">
<div class="article-summary-box-inner">
<span><p>Current methods for prompt learning in zeroshot scenarios widely rely on a
development set with sufficient human-annotated data to select the
best-performing prompt template a posteriori. This is not ideal because in a
realworld zero-shot scenario of practical relevance, no labelled data is
available. Thus, we propose a simple yet effective method for screening
reasonable prompt templates in zero-shot text classification: Perplexity
Selection (Perplection). We hypothesize that language discrepancy can be used
to measure the efficacy of prompt templates, and thereby develop a
substantiated perplexity-based scheme allowing for forecasting the performance
of prompt templates in advance. Experiments show that our method leads to
improved prediction performance in a realistic zero-shot setting, eliminating
the need for any labelled examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06210">
<div class="article-summary-box-inner">
<span><p>To overcome the overparameterized problem in Pre-trained Language Models
(PLMs), pruning is widely used as a simple and straightforward compression
method by directly removing unimportant weights. Previous first-order methods
successfully compress PLMs to extremely high sparsity with little performance
drop. These methods, such as movement pruning, use first-order information to
prune PLMs while fine-tuning the remaining weights. In this work, we argue
fine-tuning is redundant for first-order pruning, since first-order pruning is
sufficient to converge PLMs to downstream tasks without fine-tuning. Under this
motivation, we propose Static Model Pruning (SMP), which only uses first-order
pruning to adapt PLMs to downstream tasks while achieving the target sparsity
level. In addition, we also design a new masking function and training
objective to further improve SMP. Extensive experiments at various sparsity
levels show SMP has significant improvements over first-order and zero-order
methods. Unlike previous first-order methods, SMP is also applicable to low
sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter
efficient than other methods due to it does not require fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All. (arXiv:2211.15199v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15199">
<div class="article-summary-box-inner">
<span><p>We present a new pre-trained language model (PLM) for modern Hebrew, termed
AlephBERTGimmel, which employs a much larger vocabulary (128K items) than
standard Hebrew PLMs before. We perform a contrastive analysis of this model
against all previous Hebrew PLMs (mBERT, heBERT, AlephBERT) and assess the
effects of larger vocabularies on task performance. Our experiments show that
larger vocabularies lead to fewer splits, and that reducing splits is better
for model performance, across different tasks. All in all this new model
achieves new SOTA on all available Hebrew benchmarks, including Morphological
Segmentation, POS Tagging, Full Morphological Analysis, NER, and Sentiment
Analysis. Subsequently we advocate for PLMs that are larger not only in terms
of number of layers or training data, but also in terms of their vocabulary. We
release the new model publicly for unrestricted use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15613">
<div class="article-summary-box-inner">
<span><p>Translating training data into many languages has emerged as a practical
solution for improving cross-lingual transfer. For tasks that involve
span-level annotations, such as information extraction or question answering,
an additional label projection step is required to map annotated spans onto the
translated texts. Recently, a few efforts have utilized a simple
mark-then-translate method to jointly perform translation and projection by
inserting special markers around the labeled spans in the original sentence.
However, as far as we are aware, no empirical analysis has been conducted on
how this approach compares to traditional annotation projection based on word
alignment. In this paper, we present an extensive empirical study across 57
languages and three tasks (QA, NER, and Event Extraction) to evaluate the
effectiveness and limitations of both methods, filling an important gap in the
literature. Experimental results show that our optimized version of
mark-then-translate, which we call EasyProject, is easily applied to many
languages and works surprisingly well, outperforming the more complex word
alignment-based methods. We analyze several key factors that affect the
end-task performance, and show EasyProject works well because it can accurately
preserve label span boundaries after translation. We will publicly release all
our code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16944">
<div class="article-summary-box-inner">
<span><p>Biomedical named entity recognition (BioNER) seeks to automatically recognize
biomedical entities in natural language text, serving as a necessary foundation
for downstream text mining tasks and applications such as information
extraction and question answering. Manually labeling training data for the
BioNER task is costly, however, due to the significant domain expertise
required for accurate annotation. The resulting data scarcity causes current
BioNER approaches to be prone to overfitting, to suffer from limited
generalizability, and to address a single entity type at a time (e.g., gene or
disease). We therefore propose a novel all-in-one (AIO) scheme that uses
external data from existing annotated resources to enhance the accuracy and
stability of BioNER models. We further present AIONER, a general-purpose BioNER
tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER
on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and
compares favorably to other state-of-the-art approaches such as multi-task
learning. We further demonstrate the practical utility of AIONER in three
independent tasks to recognize entity types not previously seen in training
data, as well as the advantages of AIONER over existing methods for processing
biomedical text at a large scale (e.g., the entire PubMed data).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07796">
<div class="article-summary-box-inner">
<span><p>A fundamental characteristic common to both human vision and natural language
is their compositional nature. Yet, despite the performance gains contributed
by large vision and language pretraining, we find that: across 7 architectures
trained with 4 algorithms on massive datasets, they struggle at
compositionality. To arrive at this conclusion, we introduce a new
compositionality evaluation benchmark, CREPE, which measures two important
aspects of compositionality identified by cognitive science literature:
systematicity and productivity. To measure systematicity, CREPE consists of a
test dataset containing over $370K$ image-text pairs and three different
seen-unseen splits. The three splits are designed to test models trained on
three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also
generate $325K$, $316K$, and $309K$ hard negative captions for a subset of the
pairs. To test productivity, CREPE contains $17K$ image-text pairs with nine
different complexities plus $183K$ hard negative captions with atomic, swapping
and negation foils. The datasets are generated by repurposing the Visual Genome
scene graphs and region descriptions and applying handcrafted templates and
GPT-3. For systematicity, we find that model performance decreases consistently
when novel compositions dominate the retrieval set, with Recall@1 dropping by
up to $12\%$. For productivity, models' retrieval success decays as complexity
increases, frequently nearing random chance at high complexity. These results
hold regardless of model and training dataset size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. (arXiv:2212.08635v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08635">
<div class="article-summary-box-inner">
<span><p>Open-Domain Question Answering (ODQA) aims at answering factoid questions
without explicitly providing specific background documents. In a zero-shot
setting, this task is more challenging since no data is available to train
customized models like Retriever-Readers. Recently, Large Language Models
(LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct
prompting methods, but these methods are still far from releasing the full
powerfulness of LLMs only in an implicitly invoking way. In this paper, we
propose a Self-Prompting framework to explicitly utilize the massive knowledge
stored in the parameters of LLMs and their strong instruction understanding
abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo
QA pairs with background passages and explanations from scratch and then use
those generated elements for in-context learning. Experimental results show our
method surpasses previous SOTA methods significantly on three widely-used ODQA
datasets, and even achieves comparable performance with some Retriever-Reader
models fine-tuned on full training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09086">
<div class="article-summary-box-inner">
<span><p>We investigate response generation for multi-turn dialogue in
generative-based chatbots. Existing generative models based on RNNs (Recurrent
Neural Networks) usually employ the last hidden state to summarize the
sequences, which makes models unable to capture the subtle variability observed
in different dialogues and cannot distinguish the differences between dialogues
that are similar in composition. In this paper, we propose a Pseudo-Variational
Gated Recurrent Unit (PVGRU) component without posterior knowledge through
introducing a recurrent summarizing variable into the GRU, which can aggregate
the accumulated distribution variations of subsequences. PVGRU can perceive the
subtle semantic variability through summarizing variables that are optimized by
the devised distribution consistency and reconstruction objectives. In
addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model
based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve
the diversity and relevance of responses on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CiteBench: A benchmark for Scientific Citation Text Generation. (arXiv:2212.09577v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09577">
<div class="article-summary-box-inner">
<span><p>Science progresses by incrementally building upon the prior body of knowledge
documented in scientific publications. The acceleration of research across many
fields makes it hard to stay up-to-date with the recent developments and to
summarize the ever-growing body of prior work. To target this issue, the task
of citation text generation aims to produce accurate textual summaries given a
set of papers-to-cite and the citing paper context. Existing studies in
citation text generation are based upon widely diverging task definitions,
which makes it hard to study this task systematically. To address this
challenge, we propose CiteBench: a benchmark for citation text generation that
unifies multiple diverse datasets and enables standardized evaluation of
citation text generation models across task designs and domains. Using the new
benchmark, we investigate the performance of multiple strong baselines, test
their transferability between the datasets, and deliver new insights into the
task definition and evaluation to guide future research in citation text
generation. We make the code for CiteBench publicly available at
https://github.com/UKPLab/citebench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships. (arXiv:2212.10545v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10545">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose DimonGen, which aims to generate diverse sentences
describing concept relationships in various everyday scenarios. To support
this, we first create a benchmark dataset for this task by adapting the
existing CommonGen dataset. We then propose a two-stage model called MoREE to
generate the target sentences. MoREE consists of a mixture of retrievers model
that retrieves diverse context sentences related to the given concepts, and a
mixture of generators model that generates diverse sentences based on the
retrieved contexts. We conduct experiments on the DimonGen task and show that
MoREE outperforms strong baselines in terms of both the quality and diversity
of the generated sentences. Our results demonstrate that MoREE is able to
generate diverse sentences that reflect different relationships between
concepts, leading to a comprehensive understanding of concept relationships.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.11261">
<div class="article-summary-box-inner">
<span><p>Nine language-vision AI models trained on web scrapes with the Contrastive
Language-Image Pretraining (CLIP) objective are evaluated for evidence of a
bias studied by psychologists: the sexual objectification of girls and women,
which occurs when a person's human characteristics, such as emotions, are
disregarded and the person is treated as a body. We replicate three experiments
in psychology quantifying sexual objectification and show that the phenomena
persist in AI. A first experiment uses standardized images of women from the
Sexual OBjectification and EMotion Database, and finds that human
characteristics are disassociated from images of objectified women: the model's
recognition of emotional state is mediated by whether the subject is fully or
partially clothed. Embedding association tests (EATs) return significant effect
sizes for both anger (d &gt;0.80) and sadness (d &gt;0.50), associating images of
fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that
CLIP gets distracted from emotional expressions in objectified images. A second
experiment measures the effect in a representative application: an automatic
image captioner (Antarctic Captions) includes words denoting emotion less than
50% as often for images of partially clothed women than for images of fully
clothed women. A third experiment finds that images of female professionals
(scientists, doctors, executives) are likely to be associated with sexual
descriptions relative to images of male professionals. A fourth experiment
shows that a prompt of "a [age] year old girl" generates sexualized images (as
determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP and
Stable Diffusion; the corresponding rate for boys never surpasses 9%. The
evidence indicates that language-vision AI models trained on web scrapes learn
biases of sexual objectification, which propagate to downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation. (arXiv:2301.05948v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.05948">
<div class="article-summary-box-inner">
<span><p>The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting
opportunities for language model training and evaluation. However, datasets for
a specific task type often have different schemas, making harmonization
challenging. Multi-task training or evaluation necessitates manual work to fit
data into task templates. Several initiatives independently tackle this issue
by releasing harmonized datasets or providing harmonization codes to preprocess
datasets into a consistent format. We identify patterns across previous
preprocessing efforts, such as column name mapping and extracting specific
sub-fields from structured data in a column. We then propose a structured
annotation framework that ensures our annotations are fully exposed and not
hidden within unstructured code. We release a dataset annotation framework and
dataset annotations for more than 500 English
tasks\footnote{\url{https://github.com/sileod/tasksource}}. These annotations
include metadata, such as the names of columns to be used as input or labels
for all datasets, which can save time for future dataset preprocessing,
regardless of whether our framework is utilized. We fine-tune a multi-task text
encoder on all tasksource tasks, outperforming every publicly available text
encoder of comparable size in an external evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10856">
<div class="article-summary-box-inner">
<span><p>In response to disinformation and propaganda from Russian online media
following the Russian invasion of Ukraine, Russian outlets including Russia
Today and Sputnik News were banned throughout Europe. To maintain viewership,
many of these Russian outlets began to heavily promote their content on
messaging services like Telegram. In this work, we study how 16 Russian media
outlets interacted with and utilized 732 Telegram channels throughout 2022.
Leveraging the foundational model MPNet, DP-means clustering, and Hawkes
Processes, we trace how narratives spread between news sites and Telegram
channels. We show that news outlets not only propagate existing narratives
through Telegram, but that they source material from the messaging platform.
Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru)
of articles discuss content that originated/resulted from activity on Telegram.
Finally, tracking the spread of individual topics, we measure the rate at which
news websites and their Telegram channels disseminate content within the
Russian media ecosystem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16416">
<div class="article-summary-box-inner">
<span><p>In this study, we investigated the potential of ChatGPT, a large language
model developed by OpenAI, for the clinical named entity recognition task
defined in the 2010 i2b2 challenge, in a zero-shot setting with two different
prompt strategies. We compared its performance with GPT-3 in a similar
zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of
synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT
outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250)
and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover,
prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores
of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's
performance was still lower than that of the supervised BioClinicalBERT model
(i.e., relaxed-matching F1 scores of 0.620 vs. 0.888), our study demonstrates
the great potential of ChatGPT for clinical NER tasks in a zero-shot setting,
which is much more appealing as it does not require any annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00020">
<div class="article-summary-box-inner">
<span><p>The prevalence of memes on social media has created the need to sentiment
analyze their underlying meanings for censoring harmful content. Meme censoring
systems by machine learning raise the need for a semi-supervised learning
solution to take advantage of the large number of unlabeled memes available on
the internet and make the annotation process less challenging. Moreover, the
approach needs to utilize multimodal data as memes' meanings usually come from
both images and texts. This research proposes a multimodal semi-supervised
learning approach that outperforms other multimodal semi-supervised learning
and supervised learning state-of-the-art models on two datasets, the Multimedia
Automatic Misogyny Identification and Hateful Memes dataset. Building on the
insights gained from Contrastive Language-Image Pre-training, which is an
effective multimodal learning technique, this research introduces SemiMemes, a
novel training method that combines auto-encoder and classification task to
make use of the resourceful unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02481">
<div class="article-summary-box-inner">
<span><p>Training and inference on edge devices often requires an efficient setup due
to computational limitations. While pre-computing data representations and
caching them on a server can mitigate extensive edge device computation, this
leads to two challenges. First, the amount of storage required on the server
that scales linearly with the number of instances. Second, the bandwidth
required to send extensively large amounts of data to an edge device. To reduce
the memory footprint of pre-computed data representations, we propose a simple,
yet effective approach that uses randomly initialized hyperplane projections.
To further reduce their size by up to 98.96%, we quantize the resulting
floating-point representations into binary vectors. Despite the greatly reduced
size, we show that the embeddings remain effective for training models across
various English and German sentence classification tasks that retain 94%--99%
of their floating-point.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Interpretable Mental Health Analysis with ChatGPT. (arXiv:2304.03347v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03347">
<div class="article-summary-box-inner">
<span><p>Automated mental health analysis shows great potential for enhancing the
efficiency and accessibility of mental health care, with recent methods using
pre-trained language models (PLMs) and incorporated emotional information. The
latest large language models (LLMs), such as ChatGPT, exhibit dramatic
capabilities on diverse natural language processing tasks. However, existing
studies on ChatGPT for mental health analysis bear limitations in inadequate
evaluations, ignorance of emotional information, and lack of explainability. To
bridge these gaps, we comprehensively evaluate the mental health analysis and
emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, and
analyze the effects of various emotion-based prompting strategies. Based on
these prompts, we further explore LLMs for interpretable mental health analysis
by instructing them to also generate explanations for each of their decisions.
With an annotation protocol designed by domain experts, we convey human
evaluations to assess the quality of explanations generated by ChatGPT and
GPT-3. The annotated corpus will be released for future research. Experimental
results show that ChatGPT outperforms traditional neural network-based methods
but still has a significant gap with advanced task-specific methods. Prompt
engineering with emotional cues can be effective in improving performance on
mental health analysis but suffers from a lack of robustness and inaccurate
reasoning. In addition, ChatGPT significantly outperforms GPT-3 on all criteria
in human evaluations of the explanations and approaches to human performance,
showing its great potential in explainable mental health analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information. (arXiv:2304.09667v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09667">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) have been successfully applied to various
tasks, they still face challenges with hallucinations. Augmenting LLMs with
domain-specific tools such as database utilities can facilitate easier and more
precise access to specialized knowledge. In this paper, we present GeneGPT, a
novel method for teaching LLMs to use the Web APIs of the National Center for
Biotechnology Information (NCBI) for answering genomics questions.
Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs
by in-context learning and an augmented decoding algorithm that can detect and
execute API calls. Experimental results show that GeneGPT achieves
state-of-the-art performance on eight tasks in the GeneTuring benchmark with an
average score of 0.83, largely surpassing retrieval-augmented LLMs such as the
new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as
well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)
API demonstrations have good cross-task generalizability and are more useful
than documentations for in-context learning; (2) GeneGPT can generalize to
longer chains of API calls and answer multi-hop questions in GeneHop, a novel
dataset introduced in this work; (3) Different types of errors are enriched in
different tasks, providing valuable insights for future improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00586">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as "The war lasted from the year 1732 to the year 17", and predict valid
two-digit end years (years &gt; 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v3 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02783">
<div class="article-summary-box-inner">
<span><p>The recent improvement in code generation capabilities due to the use of
large language models has mainly benefited general purpose programming
languages. Domain specific languages, such as the ones used for IT Automation,
have received far less attention, despite involving many active developers and
being an essential component of modern cloud platforms. This work focuses on
the generation of Ansible-YAML, a widely used markup language for IT
Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code
generation tool, aimed at improving IT automation productivity. Ansible Wisdom
is a transformer-based model, extended by training with a new dataset
containing Ansible-YAML. We also develop two novel performance metrics for YAML
and Ansible to capture the specific characteristics of this domain. Results
show that Ansible Wisdom can accurately generate Ansible script from natural
language prompts with performance comparable or better than existing state of
the art code generation models. In few-shot settings we asses the impact of
training with Ansible, YAML data and compare with different baselines including
Codex-Davinci-002. We also show that after finetuning, our Ansible specific
model can beat the performance of a much larger Codex-Davinci-002 in few shot
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03445">
<div class="article-summary-box-inner">
<span><p>Figurative language is a challenge for language models since its
interpretation is based on the use of words in a way that deviates from their
conventional order and meaning. Yet, humans can easily understand and interpret
metaphors, similes or idioms as they can be derived from embodied metaphors.
Language is a proxy for embodiment and if a metaphor is conventional and
lexicalised, it becomes easier for a system without a body to make sense of
embodied concepts. Yet, the intricate relation between embodiment and features
such as concreteness or age of acquisition has not been studied in the context
of figurative language interpretation concerning language models. Hence, the
presented study shows how larger language models perform better at interpreting
metaphoric sentences when the action of the metaphorical sentence is more
embodied. The analysis rules out multicollinearity with other features (e.g.
word length or concreteness) and provides initial evidence that larger language
models conceptualise embodied concepts to a degree that facilitates figurative
language understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive loose optimization for robust question answering. (arXiv:2305.03971v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03971">
<div class="article-summary-box-inner">
<span><p>Question answering methods are well-known for leveraging data bias, such as
the language prior in visual question answering and the position bias in
machine reading comprehension (extractive question answering). Current
debiasing methods often come at the cost of significant in-distribution
performance to achieve favorable out-of-distribution generalizability, while
non-debiasing methods sacrifice a considerable amount of out-of-distribution
performance in order to obtain high in-distribution performance. Therefore, it
is challenging for them to deal with the complicated changing real-world
situations. In this paper, we propose a simple yet effective novel loss
function with adaptive loose optimization, which seeks to make the best of both
worlds for question answering. Our main technical contribution is to reduce the
loss adaptively according to the ratio between the previous and current
optimization state on mini-batch training data. This loose optimization can be
used to prevent non-debiasing methods from overlearning data bias while
enabling debiasing methods to maintain slight bias learning. Experiments on the
visual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,
GQA-OOD, and the extractive question answering dataset SQuAD demonstrate that
our approach enables QA methods to obtain state-of-the-art in- and
out-of-distribution performance in most cases. The source code has been
released publicly in \url{https://github.com/reml-group/ALO}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Demonstration Retriever for In-Context Learning. (arXiv:2305.04320v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04320">
<div class="article-summary-box-inner">
<span><p>In-context learning is a new learning paradigm where a language model
conditions on a few input-output pairs (demonstrations) and a test input, and
directly outputs the prediction. It has been shown highly dependent on the
provided demonstrations and thus promotes the research of demonstration
retrieval: given a test input, relevant examples are retrieved from the
training set to serve as informative demonstrations for in-context learning.
While previous works focus on training task-specific retrievers for several
tasks separately, these methods are often hard to transfer and scale on various
tasks, and separately trained retrievers incur a lot of parameter storage and
deployment cost. In this paper, we propose Unified Demonstration Retriever
(\textbf{UDR}), a single model to retrieve demonstrations for a wide range of
tasks. To train UDR, we cast various tasks' training signals into a unified
list-wise ranking formulation by language model's feedback. Then we propose a
multi-task list-wise ranking training framework, with an iterative mining
strategy to find high-quality candidates, which can help UDR fully incorporate
various tasks' signals. Experiments on 30+ tasks across 13 task families and
multiple data domains show that UDR significantly outperforms baselines.
Further analyses show the effectiveness of each proposed component and UDR's
strong ability in various scenarios including different LMs (1.3B - 175B),
unseen datasets, varying demonstration quantities, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06335">
<div class="article-summary-box-inner">
<span><p>We present in this work a new Universal Morphology dataset for Korean.
Previously, the Korean language has been underrepresented in the field of
morphological paradigms amongst hundreds of diverse world languages. Hence, we
propose this Universal Morphological paradigms for the Korean language that
preserve its distinct characteristics. For our K-UniMorph dataset, we outline
each grammatical criterion in detail for the verbal endings, clarify how to
extract inflected forms, and demonstrate how we generate the morphological
schemata. This dataset adopts morphological feature schema from Sylak-Glassman
et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract
inflected verb forms from the Sejong morphologically analyzed corpus that is
one of the largest annotated corpora for Korean. During the data creation, our
methodology also includes investigating the correctness of the conversion from
the Sejong corpus. Furthermore, we carry out the inflection task using three
different Korean word forms: letters, syllables and morphemes. Finally, we
discuss and describe future perspectives on Korean morphological paradigms and
the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06424">
<div class="article-summary-box-inner">
<span><p>Large language models like ChatGPT have recently demonstrated impressive
capabilities in natural language understanding and generation, enabling various
applications including translation, essay writing, and chit-chatting. However,
there is a concern that they can be misused for malicious purposes, such as
fraud or denial-of-service attacks. Therefore, it is crucial to develop methods
for detecting whether the party involved in a conversation is a bot or a human.
In this paper, we propose a framework named FLAIR, Finding Large language model
Authenticity via a single Inquiry and Response, to detect conversational bots
in an online manner. Specifically, we target a single question scenario that
can effectively differentiate human users from bots. The questions are divided
into two categories: those that are easy for humans but difficult for bots
(e.g., counting, substitution, positioning, noise filtering, and ASCII art),
and those that are easy for bots but difficult for humans (e.g., memorization
and computation). Our approach shows different strengths of these questions in
their effectiveness, providing a new way for online service providers to
protect themselves against nefarious activities and ensure that they are
serving real users. We open-sourced our dataset on
https://github.com/hongwang600/FLAIR and welcome contributions from the
community to enrich such detection datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2305.06655v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06655">
<div class="article-summary-box-inner">
<span><p>Context-dependent Text-to-SQL aims to translate multi-turn natural language
questions into SQL queries. Despite various methods have exploited
context-dependence information implicitly for contextual SQL parsing, there are
few attempts to explicitly address the dependencies between current question
and question context. This paper presents QURG, a novel Question Rewriting
Guided approach to help the models achieve adequate contextual understanding.
Specifically, we first train a question rewriting model to complete the current
question based on question context, and convert them into a rewriting edit
matrix. We further design a two-stream matrix encoder to jointly model the
rewriting relations between question and context, and the schema linking
relations between natural language and structured schema. Experimental results
show that QURG significantly improves the performances on two large-scale
context-dependent datasets SParC and CoSQL, especially for hard and long-turn
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07637">
<div class="article-summary-box-inner">
<span><p>The Imaging Data Commons (IDC) is a cloud-based database that provides
researchers with open access to cancer imaging data, with the goal of
facilitating collaboration in medical imaging research. However, querying the
IDC database for cohort discovery and access to imaging data has a significant
learning curve for researchers due to its complex nature. We developed
Text2Cohort, a large language model (LLM) based toolkit to facilitate
user-friendly and intuitive natural language cohort discovery in the IDC.
Text2Cohorts translates user input into IDC database queries using prompt
engineering and autocorrection and returns the query's response to the user.
Autocorrection resolves errors in queries by passing the errors back to the
model for interpretation and correction. We evaluate Text2Cohort on 50 natural
language user inputs ranging from information extraction to cohort discovery.
The resulting queries and outputs were verified by two computer scientists to
measure Text2Cohort's accuracy and F1 score. Text2Cohort successfully generated
queries and their responses with an 88% accuracy and F1 score of 0.94. However,
it failed to generate queries for 6/50 (12%) user inputs due to syntax and
semantic errors. Our results indicate that Text2Cohort succeeded at generating
queries with correct responses, but occasionally failed due to a lack of
understanding of the data schema. Despite these shortcomings, Text2Cohort
demonstrates the utility of LLMs to enable researchers to discover and curate
cohorts using data hosted on IDC with high levels of accuracy using natural
language in a more intuitive and user-friendly way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08503">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have accomplished impressive achievements
in abstractive single-document summarization (SDS). However, such benefits may
not be readily extended to muti-document summarization (MDS), where the
interactions among documents are more complex. Previous works either design new
architectures or new pre-training objectives for MDS, or apply PLMs to MDS
without considering the complex document interactions. While the former does
not make full use of previous pre-training efforts and may not generalize well
across multiple domains, the latter cannot fully attend to the intricate
relationships unique to MDS tasks. In this paper, we enforce hierarchy on both
the encoder and decoder and seek to make better use of a PLM to facilitate
multi-document interactions for the MDS task. We test our design on 10 MDS
datasets across a wide range of domains. Extensive experiments show that our
proposed method can achieve consistent improvements on all these datasets,
outperforming the previous best models, and even achieving better or
competitive results as compared to some models with additional MDS pre-training
or larger model parameters.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-17 23:10:53.936055848 UTC">2023-05-17 23:10:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>