<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-17T01:30:00Z">04-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. (arXiv:2304.06762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06762">
<div class="article-summary-box-inner">
<span><p>Large decoder-only language models (LMs) can be largely improved in terms of
perplexity by retrieval (e.g., RETRO), but its impact on text generation
quality and downstream task accuracy is unclear. Thus, it is still an open
question: shall we pretrain large autoregressive LMs with retrieval? To answer
it, we perform a comprehensive study on a scalable pre-trained
retrieval-augmented LM (i.e., RETRO) compared with standard GPT and
retrieval-augmented GPT incorporated at fine-tuning or inference stages. We
first provide the recipe to reproduce RETRO up to 9.5B parameters while
retrieving a text corpus with 330B tokens. Based on that, we have the following
novel findings: i) RETRO outperforms GPT on text generation with much less
degeneration (i.e., repetition), moderately higher factual accuracy, and
slightly lower toxicity with a nontoxic retrieval database. ii) On the LM
Evaluation Harness benchmark, RETRO largely outperforms GPT on
knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,
we introduce a simple variant of the model, RETRO++, which largely improves
open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural
Question) and significantly outperforms retrieval-augmented GPT across
different model sizes. Our findings highlight the promising direction of
pretraining autoregressive LMs with retrieval as future foundation models. We
release our implementation at: https://github.com/NVIDIA/Megatron-LM#retro
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06767">
<div class="article-summary-box-inner">
<span><p>Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially
significant repercussions. Consequently, aligning these models with human
ethics and preferences is an essential step toward ensuring their responsible
and effective deployment in real-world applications. Prior research has
primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means
of addressing this problem, wherein generative models are fine-tuned using RL
algorithms guided by a human-feedback-informed reward model. However, the
inefficiencies and instabilities associated with RL algorithms frequently
present substantial obstacles to the successful alignment of generative models,
necessitating the development of a more robust and streamlined approach. To
this end, we introduce a new framework, Reward rAnked FineTuning (RAFT),
designed to align generative models more effectively. Utilizing a reward model
and a sufficient number of samples, our approach selects the high-quality
samples, discarding those that exhibit undesired behavior, and subsequently
assembles a streaming dataset. This dataset serves as the basis for aligning
the generative model and can be employed under both offline and online
settings. Notably, the sample generation process within RAFT is gradient-free,
rendering it compatible with black-box generators. Through extensive
experiments, we demonstrate that our proposed algorithm exhibits strong
performance in the context of both large language models and diffusion models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06795">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel Token-and-Duration Transducer (TDT)
architecture for sequence-to-sequence tasks. TDT extends conventional
RNN-Transducer architectures by jointly predicting both a token and its
duration, i.e. the number of input frames covered by the emitted token. This is
achieved by using a joint network with two outputs which are independently
normalized to generate distributions over tokens and durations. During
inference, TDT models can skip input frames guided by the predicted duration
output, which makes them significantly faster than conventional Transducers
which process the encoder output frame by frame. TDT models achieve both better
accuracy and significantly faster inference than conventional Transducers on
different sequence transduction tasks. TDT models for Speech Recognition
achieve better accuracy and up to 2.82X faster inference than RNN-Transducers.
TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on
the MUST-C test compared with conventional Transducers, and its inference is
2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT
models improve the intent accuracy up to over 1% (absolute) over conventional
Transducers, while running up to 1.28X faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence. (arXiv:2304.06798v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06798">
<div class="article-summary-box-inner">
<span><p>Large pre-trained models, also known as foundation models (FMs), are trained
in a task-agnostic manner on large-scale data and can be adapted to a wide
range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.
Despite their successes in language and vision tasks, we have yet seen an
attempt to develop foundation models for geospatial artificial intelligence
(GeoAI). In this work, we explore the promises and challenges of developing
multimodal foundation models for GeoAI. We first investigate the potential of
many existing FMs by testing their performances on seven tasks across multiple
geospatial subdomains including Geospatial Semantics, Health Geography, Urban
Geography, and Remote Sensing. Our results indicate that on several geospatial
tasks that only involve text modality such as toponym recognition, location
description recognition, and US state-level/county-level dementia time series
forecasting, these task-agnostic LLMs can outperform task-specific
fully-supervised models in a zero-shot or few-shot learning setting. However,
on other geospatial tasks, especially tasks that involve multiple data
modalities (e.g., POI-based urban function classification, street view
image-based urban noise intensity classification, and remote sensing image
scene classification), existing foundation models still underperform
task-specific models. Based on these observations, we propose that one of the
major challenges of developing a FM for GeoAI is to address the multimodality
nature of geospatial tasks. After discussing the distinct challenges of each
geospatial data modality, we suggest the possibility of a multimodal foundation
model which can reason over various types of geospatial data through geospatial
alignments. We conclude this paper by discussing the unique risks and
challenges to develop such a model for GeoAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). (arXiv:2304.06845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06845">
<div class="article-summary-box-inner">
<span><p>We present the first Africentric SemEval Shared task, Sentiment Analysis for
African Languages (AfriSenti-SemEval) - the dataset is available at
https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval
is a sentiment classification challenge in 14 African languages - Amharic,
Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican
Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and
Yor\`ub\'a (Muhammad et al., 2023), using a 3-class labeled data: positive,
negative, and neutral. We present three subtasks: (1) Task A: monolingual
classification, which received 44 submissions; (2) Task B: multilingual
classification, which received 32 submissions; and (3) Task C: zero-shot
classification, which received 34 submissions. The best system for tasks A and
B was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively.
UCAS-IIE-NLP achieved the best system on average for task C with 58.15 weighted
F1. We describe the various approaches adopted by the top 10 systems and their
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06858">
<div class="article-summary-box-inner">
<span><p>Vaccine hesitancy continues to be a main challenge for public health
officials during the COVID-19 pandemic. As this hesitancy undermines vaccine
campaigns, many researchers have sought to identify its root causes, finding
that the increasing volume of anti-vaccine misinformation on social media
platforms is a key element of this problem. We explored Twitter as a source of
misleading content with the goal of extracting overlapping cultural and
political beliefs that motivate the spread of vaccine misinformation. To do
this, we have collected a data set of vaccine-related Tweets and annotated them
with the help of a team of annotators with a background in communications and
journalism. Ultimately we hope this can lead to effective and targeted public
health communication strategies for reaching individuals with anti-vaccine
beliefs. Moreover, this information helps with developing Machine Learning
models to automatically detect vaccine misinformation posts and combat their
negative impacts. In this paper, we present Vax-Culture, a novel Twitter
COVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by an
extensive set of human-provided annotations including vaccine-hesitancy stance,
indication of any misinformation in tweets, the entities criticized and
supported in each tweet and the communicated message of each tweet. Moreover,
we define five baseline tasks including four classification and one sequence
generation tasks, and report the results of a set of recent transformer-based
models for them. The dataset and code are publicly available at
https://github.com/mrzarei5/Vax-Culture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06861">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models are widely used in the community. These
models are usually trained on unmoderated and unfiltered data from open sources
like the Internet. Due to this, biases that we see in platforms online which
are a reflection of those in society are in turn captured and learned by these
models. These models are deployed in applications that affect millions of
people and their inherent biases are harmful to the targeted social groups. In
this work, we study the general trend in bias reduction as newer pre-trained
models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT)
are chosen and evaluated against two bias benchmarks, StereoSet and
CrowS-Pairs. They are compared to the baseline of BERT using the associated
metrics. We explore whether as advancements are made and newer, faster, lighter
models are released: are they being developed responsibly such that their
inherent social biases have been reduced compared to their older counterparts?
The results are compiled and we find that all the models under study do exhibit
biases but have generally improved as compared to BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06875">
<div class="article-summary-box-inner">
<span><p>As language models scale up, it becomes increasingly expensive to verify
research ideas because conclusions on small models do not trivially transfer to
large ones. A possible solution is to establish a generic system that directly
predicts some metrics for large models solely based on the results and
hyperparameters from small models. Existing methods based on scaling laws
require hyperparameter search on the largest models, which is impractical with
limited resources. We address this issue by presenting our discoveries
indicating that Maximal Update parametrization (muP) enables accurate fitting
of scaling laws for hyperparameters close to common loss basins, without any
search. Thus, different models can be directly compared on large scales with
loss prediction even before the training starts. We propose a new paradigm as a
first step towards reliable academic research for any model scale without heavy
computation. Code will be publicly available shortly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06910">
<div class="article-summary-box-inner">
<span><p>Emotion recognition in conversations is challenging due to the multi-modal
nature of the emotion expression. We propose a hierarchical cross-attention
model (HCAM) approach to multi-modal emotion recognition using a combination of
recurrent and co-attention neural network models. The input to the model
consists of two modalities, i) audio data, processed through a learnable
wav2vec approach and, ii) text data represented using a bidirectional encoder
representations from transformers (BERT) model. The audio and text
representations are processed using a set of bi-directional recurrent neural
network layers with self-attention that converts each utterance in a given
conversation to a fixed dimensional embedding. In order to incorporate
contextual knowledge and the information across the two modalities, the audio
and text embeddings are combined using a co-attention layer that attempts to
weigh the utterance level embeddings relevant to the task of emotion
recognition. The neural network parameters in the audio layers, text layers as
well as the multi-modal co-attention layers, are hierarchically trained for the
emotion classification task. We perform experiments on three established
datasets namely, IEMOCAP, MELD and CMU-MOSI, where we illustrate that the
proposed model improves significantly over other benchmarks and helps achieve
state-of-art results on all these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text. (arXiv:2304.06939v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06939">
<div class="article-summary-box-inner">
<span><p>In-context vision and language models like Flamingo support arbitrarily
interleaved sequences of images and text as input. This format not only enables
few-shot learning via interleaving independent supervised (image, text)
examples, but also, more complex prompts involving interaction between images,
e.g., "What do image A and image B have in common?" To support this interface,
pretraining occurs over web corpora that similarly contain interleaved
images+text. To date, however, large-scale data of this form have not been
publicly available.
</p>
<p>We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4
corpus with images interleaved. We use a linear assignment algorithm to place
images into longer bodies of text using CLIP features, a process that we show
outperforms alternatives. mmc4 spans everyday topics like cooking, travel,
technology, etc. A manual inspection of a random sample of documents shows that
a vast majority (90%) of images are topically relevant, and that linear
assignment frequently selects individual sentences specifically well-aligned
with each image (78%). After filtering NSFW images, ads, etc., the corpus
contains 103M documents containing 585M images interleaved with 43B English
tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning. (arXiv:2304.06962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06962">
<div class="article-summary-box-inner">
<span><p>Prompt engineering and calibration make large language models excel at
reasoning tasks, including multiple choice commonsense reasoning. From a
practical perspective, we investigate and evaluate these strategies on smaller
language models. Through experiments on five commonsense reasoning benchmarks,
we find that each strategy favors certain models, but their joint effects are
mostly negative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge. (arXiv:2304.06975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06975">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), such as the LLaMA model, have demonstrated
their effectiveness in various general-domain natural language processing (NLP)
tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain
tasks due to the need for medical expertise in the responses. In response to
this challenge, we propose HuaTuo, a LLaMA-based model that has been
supervised-fine-tuned with generated QA (Question-Answer) instances. The
experimental results demonstrate that HuaTuo generates responses that possess
more reliable medical knowledge. Our proposed HuaTuo model is accessible at
https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimpLex: a lexical text simplification architecture. (arXiv:2304.07002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07002">
<div class="article-summary-box-inner">
<span><p>Text simplification (TS) is the process of generating easy-to-understand
sentences from a given sentence or piece of text. The aim of TS is to reduce
both the lexical (which refers to vocabulary complexity and meaning) and
syntactic (which refers to the sentence structure) complexity of a given text
or sentence without the loss of meaning or nuance. In this paper, we present
\textsc{SimpLex}, a novel simplification architecture for generating simplified
English sentences. To generate a simplified sentence, the proposed architecture
uses either word embeddings (i.e., Word2Vec) and perplexity, or sentence
transformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The
solution is incorporated into a user-friendly and simple-to-use software. We
evaluate our system using two metrics, i.e., SARI, and Perplexity Decrease.
Experimentally, we observe that the transformer models outperform the other
models in terms of the SARI score. However, in terms of Perplexity, the
Word-Embeddings-based models achieve the biggest decrease. Thus, the main
contributions of this paper are: (1) We propose a new Word Embedding and
Transformer based algorithm for text simplification; (2) We design
\textsc{SimpLex} -- a modular novel text simplification system -- that can
provide a baseline for further research; and (3) We perform an in-depth
analysis of our solution and compare our results with two state-of-the-art
models, i.e., LightLS [19] and NTS-w2v [44]. We also make the code publicly
available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy. (arXiv:2304.07007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07007">
<div class="article-summary-box-inner">
<span><p>How does one measure "ability to understand language"? If it is a person's
ability that is being measured, this is a question that almost never poses
itself in an unqualified manner: Whatever formal test is applied, it takes
place on the background of the person's language use in daily social practice,
and what is measured is a specialised variety of language understanding (e.g.,
of a second language; or of written, technical language). Computer programs do
not have this background. What does that mean for the applicability of formal
tests of language understanding? I argue that such tests need to be
complemented with tests of language use embedded in a practice, to arrive at a
more comprehensive evaluation of "artificial language understanding". To do
such tests systematically, I propose to use "Dialogue Games" -- constructed
activities that provide a situational embedding for language use. I describe a
taxonomy of Dialogue Game types, linked to a model of underlying capabilites
that are tested, and thereby giving an argument for the \emph{construct
validity} of the test. I close with showing how the internal structure of the
taxonomy suggests an ordering from more specialised to more general situational
language understanding, which potentially can provide some strategic guidance
for development in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification. (arXiv:2304.07022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07022">
<div class="article-summary-box-inner">
<span><p>Multi-label text classification aims to extract all the related labels from a
sentence, which can be viewed as a sequence generation problem. However, the
labels in training dataset are unordered. We propose to treat it as a direct
set prediction problem and don't need to consider the order of labels. Besides,
in order to model the correlation between labels, the adjacency matrix is
constructed through the statistical relations between labels and GCN is
employed to learn the label information. Based on the learned label
information, the set prediction networks can both utilize the sentence
information and label information for multi-label text classification
simultaneously. Furthermore, the Bhattacharyya distance is imposed on the
output probability distributions of the set prediction networks to increase the
recall ability. Experimental results on four multi-label datasets show the
effectiveness of the proposed method and it outperforms previous method a
substantial margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEA: A Scalable Entity Alignment System. (arXiv:2304.07065v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07065">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA) aims to find equivalent entities in different knowledge
graphs (KGs). State-of-the-art EA approaches generally use Graph Neural
Networks (GNNs) to encode entities. However, most of them train the models and
evaluate the results in a fullbatch fashion, which prohibits EA from being
scalable on largescale datasets. To enhance the usability of GNN-based EA
models in real-world applications, we present SEA, a scalable entity alignment
system that enables to (i) train large-scale GNNs for EA, (ii) speed up the
normalization and the evaluation process, and (iii) report clear results for
users to estimate different models and parameter settings. SEA can be run on a
computer with merely one graphic card. Moreover, SEA encompasses six
state-of-the-art EA models and provides access for users to quickly establish
and evaluate their own models. Thus, SEA allows users to perform EA without
being involved in tedious implementations, such as negative sampling and
GPU-accelerated evaluation. With SEA, users can gain a clear view of the model
performance. In the demonstration, we show that SEA is user-friendly and is of
high scalability even on computers with limited computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10. (arXiv:2304.07101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07101">
<div class="article-summary-box-inner">
<span><p>This paper summarizes our contributions to the document-grounded dialog tasks
at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In
both iterations the task consists of three subtasks: first detect whether the
current turn is knowledge seeking, second select a relevant knowledge document,
and third generate a response grounded on the selected document. For DSTC9 we
proposed different approaches to make the selection task more efficient. The
best method, Hierarchical Selection, actually improves the results compared to
the original baseline and gives a speedup of 24x. In the DSTC10 iteration of
the task, the challenge was to adapt systems trained on written dialogs to
perform well on noisy automatic speech recognition transcripts. Therefore, we
proposed data augmentation techniques to increase the robustness of the models
as well as methods to adapt the style of generated responses to fit well into
the proceeding dialog. Additionally, we proposed a noisy channel model that
allows for increasing the factuality of the generated responses. In addition to
summarizing our previous contributions, in this work, we also report on a few
small improvements and reconsider the automatic evaluation metrics for the
generation task which have shown a low correlation to human judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keeping the Questions Conversational: Using Structured Representations to Resolve Dependency in Conversational Question Answering. (arXiv:2304.07125v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07125">
<div class="article-summary-box-inner">
<span><p>Having an intelligent dialogue agent that can engage in conversational
question answering (ConvQA) is now no longer limited to Sci-Fi movies only and
has, in fact, turned into a reality. These intelligent agents are required to
understand and correctly interpret the sequential turns provided as the context
of the given question. However, these sequential questions are sometimes left
implicit and thus require the resolution of some natural language phenomena
such as anaphora and ellipsis. The task of question rewriting has the potential
to address the challenges of resolving dependencies amongst the contextual
turns by transforming them into intent-explicit questions. Nonetheless, the
solution of rewriting the implicit questions comes with some potential
challenges such as resulting in verbose questions and taking conversational
aspect out of the scenario by generating self-contained questions. In this
paper, we propose a novel framework, CONVSR (CONVQA using Structured
Representations) for capturing and generating intermediate representations as
conversational cues to enhance the capability of the QA model to better
interpret the incomplete questions. We also deliberate how the strengths of
this task could be leveraged in a bid to design more engaging and eloquent
conversational agents. We test our model on the QuAC and CANARD datasets and
illustrate by experimental results that our proposed framework achieves a
better F1 score than the standard question rewriting model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPI at SemEval 2023 Task 1: Image-Text Embeddings and Multimodal Information Retrieval for Visual Word Sense Disambiguation. (arXiv:2304.07127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07127">
<div class="article-summary-box-inner">
<span><p>The goal of visual word sense disambiguation is to find the image that best
matches the provided description of the word's meaning. It is a challenging
problem, requiring approaches that combine language and image understanding. In
this paper, we present our submission to SemEval 2023 visual word sense
disambiguation shared task. The proposed system integrates multimodal
embeddings, learning to rank methods, and knowledge-based approaches. We build
a classifier based on the CLIP model, whose results are enriched with
additional information retrieved from Wikipedia and lexical databases. Our
solution was ranked third in the multilingual task and won in the Persian
track, one of the three language subtasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPI at SemEval 2023 Task 9: A Simple But Effective Approach to Multilingual Tweet Intimacy Analysis. (arXiv:2304.07130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07130">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the SemEval 2023 multilingual tweet
intimacy analysis shared task. The goal of the task was to assess the level of
intimacy of Twitter posts in ten languages. The proposed approach consists of
several steps. First, we perform in-domain pre-training to create a language
model adapted to Twitter data. In the next step, we train an ensemble of
regression models to expand the training set with pseudo-labeled examples. The
extended dataset is used to train the final solution. Our method was ranked
first in five out of ten language subtasks, obtaining the highest average score
across all languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Tell Me: Prompt Engineering in Business Process Management. (arXiv:2304.07183v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07183">
<div class="article-summary-box-inner">
<span><p>GPT-3 and several other language models (LMs) can effectively address various
natural language processing (NLP) tasks, including machine translation and text
summarization. Recently, they have also been successfully employed in the
business process management (BPM) domain, e.g., for predictive process
monitoring and process extraction from text. This, however, typically requires
fine-tuning the employed LM, which, among others, necessitates large amounts of
suitable training data. A possible solution to this problem is the use of
prompt engineering, which leverages pre-trained LMs without fine-tuning them.
Recognizing this, we argue that prompt engineering can help bring the
capabilities of LMs to BPM research. We use this position paper to develop a
research agenda for the use of prompt engineering for BPM research by
identifying the associated potentials and challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07235">
<div class="article-summary-box-inner">
<span><p>Transformers are the type of neural networks that has revolutionised natural
language processing and protein science. Their key building block is a
mechanism called self-attention which is trained to predict missing words in
sentences. Despite the practical success of transformers in applications it
remains unclear what self-attention learns from data, and how. Here, we give a
precise analytical and numerical characterisation of transformers trained on
data drawn from a generalised Potts model with interactions between sites and
Potts colours. While an off-the-shelf transformer requires several layers to
learn this distribution, we show analytically that a single layer of
self-attention with a small modification can learn the Potts model exactly in
the limit of infinite sampling. We show that this modified self-attention, that
we call ``factored'', has the same functional form as the conditional
probability of a Potts spin given the other spins, compute its generalisation
error using the replica method from statistical physics, and derive an exact
mapping to pseudo-likelihood methods for solving the inverse Ising and Potts
problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Covidia: COVID-19 Interdisciplinary Academic Knowledge Graph. (arXiv:2304.07242v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07242">
<div class="article-summary-box-inner">
<span><p>The pandemic of COVID-19 has inspired extensive works across different
research fields. Existing literature and knowledge platforms on COVID-19 only
focus on collecting papers on biology and medicine, neglecting the
interdisciplinary efforts, which hurdles knowledge sharing and research
collaborations between fields to address the problem. Studying
interdisciplinary researches requires effective paper category classification
and efficient cross-domain knowledge extraction and integration. In this work,
we propose Covidia, COVID-19 interdisciplinary academic knowledge graph to
bridge the gap between knowledge of COVID-19 on different domains. We design
frameworks based on contrastive learning for disciplinary classification, and
propose a new academic knowledge graph scheme for entity extraction, relation
classification and ontology management in accordance with interdisciplinary
researches. Based on Covidia, we also establish knowledge discovery benchmarks
for finding COVID-19 research communities and predicting potential links.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games. (arXiv:2304.07258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07258">
<div class="article-summary-box-inner">
<span><p>Language models pre-trained on large self-supervised corpora, followed by
task-specific fine-tuning has become the dominant paradigm in NLP. These
pre-training datasets often have a one-to-many structure--e.g. in dialogue
there are many valid responses for a given context. However, only some of these
responses will be desirable in our downstream task. This raises the question of
how we should train the model such that it can emulate the desirable
behaviours, but not the undesirable ones. Current approaches train in a
one-to-one setup--only a single target response is given for a single dialogue
context--leading to models only learning to predict the average response, while
ignoring the full range of possible responses. Using text-based games as a
testbed, our approach, PASA, uses discrete latent variables to capture the
range of different behaviours represented in our larger pre-training dataset.
We then use knowledge distillation to distil the posterior probability
distribution into a student model. This probability distribution is far richer
than learning from only the hard targets of the dataset, and thus allows the
student model to benefit from the richer range of actions the teacher model has
learned. Results show up to 49% empirical improvement over the previous
state-of-the-art model on the Jericho Walkthroughs dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniCausal: Unified Benchmark and Repository for Causal Text Mining. (arXiv:2208.09163v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.09163">
<div class="article-summary-box-inner">
<span><p>Current causal text mining datasets vary in objectives, data coverage, and
annotation schemes. These inconsistent efforts prevent modeling capabilities
and fair comparisons of model performance. Furthermore, few datasets include
cause-effect span annotations, which are needed for end-to-end causal relation
extraction. To address these issues, we propose UniCausal, a unified benchmark
for causal text mining across three tasks: (I) Causal Sequence Classification,
(II) Cause-Effect Span Detection and (III) Causal Pair Classification. We
consolidated and aligned annotations of six high quality, mainly
human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165
examples for each task respectively. Since the definition of causality can be
subjective, our framework was designed to allow researchers to work on some or
all datasets and tasks. To create an initial benchmark, we fine-tuned BERT
pre-trained language models to each task, achieving 70.10% Binary F1, 52.42%
Macro F1, and 84.68% Binary F1 scores respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03454">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained language models such as BERT have achieved
remarkable results in Semantic Sentence Matching. However, existing models
still suffer from insufficient ability to capture subtle differences. Minor
noise like word addition, deletion, and modification of sentences may cause
flipped predictions. To alleviate this problem, we propose a novel Dual
Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture
fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention
module, which measures soft word matches by introducing a new dual channel
alignment mechanism to model affinity and difference attention. (2) Adaptive
Fusion module, this module uses attention to learn the aggregation of
difference and affinity features, and generates a vector describing the
matching details of sentence pairs. We conduct extensive experiments on
well-studied semantic matching and robustness test datasets, and the
experimental results show the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyLEx: Explaining Style Using Human Lexical Annotations. (arXiv:2210.07469v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07469">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models have achieved impressive results on various
style classification tasks, but they often learn spurious domain-specific words
to make predictions (Hayati et al., 2021). While human explanation highlights
stylistic tokens as important features for this task, we observe that model
explanations often do not align with them. To tackle this issue, we introduce
StyLEx, a model that learns from human-annotated explanations of stylistic
features and jointly learns to perform the task and predict these features as
model explanations. Our experiments show that StyLEx can provide human-like
stylistic lexical explanations without sacrificing the performance of
sentence-level style prediction on both in-domain and out-of-domain datasets.
Explanations from StyLEx show significant improvements in explanation metrics
(sufficiency, plausibility) and when evaluated with human annotations. They are
also more understandable by human judges compared to the widely-used
saliency-based explanation baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08471">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10260">
<div class="article-summary-box-inner">
<span><p>Named entity recognition is a traditional task in natural language
processing. In particular, nested entity recognition receives extensive
attention for the widespread existence of the nesting scenario. The latest
research migrates the well-established paradigm of set prediction in object
detection to cope with entity nesting. However, the manual creation of query
vectors, which fail to adapt to the rich semantic information in the context,
limits these approaches. An end-to-end entity detection approach with proposer
and regressor is presented in this paper to tackle the issues. First, the
proposer utilizes the feature pyramid network to generate high-quality entity
proposals. Then, the regressor refines the proposals for generating the final
prediction. The model adopts encoder-only architecture and thus obtains the
advantages of the richness of query semantics, high precision of entity
localization, and easiness of model training. Moreover, we introduce the novel
spatially modulated attention and progressive refinement for further
improvement. Extensive experiments demonstrate that our model achieves advanced
performance in flat and nested NER, achieving a new state-of-the-art F1 score
of 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15398">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a technique to generate new training data based on
existing data. We evaluate the simple and cost-effective method of
concatenating the original data examples to build new training instances.
Continued training with such augmented data is able to improve off-the-shelf
Transformer and Conformer models that were optimized on the original data only.
We demonstrate considerable improvements on the LibriSpeech-960h test sets (WER
2.83 and 6.87 for test-clean and test-other), which carry over to models
combined with shallow fusion (WER 2.55 and 6.27). Our method of continued
training also leads to improvements of up to 0.9 WER on the ASR part of
CoVoST-2 for four non English languages, and we observe that the gains are
highly dependent on the size of the original training data. We compare
different concatenation strategies and found that our method does not need
speaker information to achieve its improvements. Finally, we demonstrate on two
datasets that our methods also works for speech translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04800">
<div class="article-summary-box-inner">
<span><p>Current work in named entity recognition (NER) uses either cross entropy (CE)
or conditional random fields (CRF) as the objective/loss functions to optimize
the underlying NER model. Both of these traditional objective functions for the
NER problem generally produce adequate performance when the data distribution
is balanced and there are sufficient annotated training examples. But since NER
is inherently an imbalanced tagging problem, the model performance under the
low-resource settings could suffer using these standard objective functions.
Based on recent advances in area under the ROC curve (AUC) maximization, we
propose to optimize the NER model by maximizing the AUC score. We give evidence
that by simply combining two binary-classifiers that maximize the AUC score,
significant performance improvement over traditional loss functions is achieved
under low-resource NER settings. We also conduct extensive experiments to
demonstrate the advantages of our method under the low-resource and
highly-imbalanced data distribution settings. To the best of our knowledge,
this is the first work that brings AUC maximization to the NER setting.
Furthermore, we show that our method is agnostic to different types of NER
embeddings, models and domains. The code to replicate this work will be
provided upon request.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record. (arXiv:2212.07538v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07538">
<div class="article-summary-box-inner">
<span><p>Objective: Social determinants of health (SDOH) impact health outcomes and
are documented in the electronic health record (EHR) through structured data
and unstructured clinical notes. However, clinical notes often contain more
comprehensive SDOH information, detailing aspects such as status, severity, and
temporality. This work has two primary objectives: i) develop a natural
language processing (NLP) information extraction model to capture detailed SDOH
information and ii) evaluate the information gain achieved by applying the SDOH
extractor to clinical narratives and combining the extracted representations
with existing structured data.
</p>
<p>Materials and Methods: We developed a novel SDOH extractor using a deep
learning entity and relation extraction architecture to characterize SDOH
across various dimensions. In an EHR case study, we applied the SDOH extractor
to a large clinical data set with 225,089 patients and 430,406 notes with
social history sections and compared the extracted SDOH information with
existing structured data.
</p>
<p>Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the
EHR case study, we found extracted SDOH information complements existing
structured data with 32% of homeless patients, 19% of current tobacco users,
and 10% of drug users only having these health risk factors documented in the
clinical narrative.
</p>
<p>Conclusions: Utilizing EHR data to identify SDOH health risk factors and
social needs may improve patient care and outcomes. Semantic representations of
text-encoded SDOH information can augment existing structured data, and this
more comprehensive SDOH representation can assist health systems in identifying
and addressing these social needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Active Learning Methods to Strategically Select Essays for Automated Scoring. (arXiv:2301.00628v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00628">
<div class="article-summary-box-inner">
<span><p>Research on automated essay scoring has become increasing important because
it serves as a method for evaluating students' written-responses at scale.
Scalable methods for scoring written responses are needed as students migrate
to online learning environments resulting in the need to evaluate large numbers
of written-response assessments. The purpose of this study is to describe and
evaluate three active learning methods than can be used to minimize the number
of essays that must be scored by human raters while still providing the data
needed to train a modern automated essay scoring system. The three active
learning methods are the uncertainty-based, the topological-based, and the
hybrid method. These three methods were used to select essays included as part
of the Automated Student Assessment Prize competition that were then classified
using a scoring model that was training with the bidirectional encoder
representations from transformer language model. All three active learning
methods produced strong results, with the topological-based method producing
the most efficient classification. Growth rate accuracy was also evaluated. The
active learning methods produced different levels of efficiency under different
sample size allocations but, overall, all three methods were highly efficient
and produced classifications that were similar to one another.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04647">
<div class="article-summary-box-inner">
<span><p>We learn a visual representation that captures information about the camera
that recorded a given photo. To do this, we train a multimodal embedding
between image patches and the EXIF metadata that cameras automatically insert
into image files. Our model represents this metadata by simply converting it to
text and then processing it with a transformer. The features that we learn
significantly outperform other self-supervised and supervised features on
downstream image forensics and calibration tasks. In particular, we
successfully localize spliced image regions "zero shot" by clustering the
visual embeddings for all of the patches within an image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alloprof: a new French question-answer education dataset and its use in an information retrieval case study. (arXiv:2302.07738v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07738">
<div class="article-summary-box-inner">
<span><p>Teachers and students are increasingly relying on online learning resources
to supplement the ones provided in school. This increase in the breadth and
depth of available resources is a great thing for students, but only provided
they are able to find answers to their queries. Question-answering and
information retrieval systems have benefited from public datasets to train and
evaluate their algorithms, but most of these datasets have been in English text
written by and for adults. We introduce a new public French question-answering
dataset collected from Alloprof, a Quebec-based primary and high-school help
website, containing 29 349 questions and their explanations in a variety of
school subjects from 10 368 students, with more than half of the explanations
containing links to other questions or some of the 2 596 reference pages on the
website. We also present a case study of this dataset in an information
retrieval task. This dataset was collected on the Alloprof public forum, with
all questions verified for their appropriateness and the explanations verified
both for their appropriateness and their relevance to the question. To predict
relevant documents, architectures using pre-trained BERT models were fine-tuned
and evaluated. This dataset will allow researchers to develop
question-answering, information retrieval and other algorithms specifically for
the French speaking education context. Furthermore, the range of language
proficiency, images, mathematical symbols and spelling mistakes will
necessitate algorithms based on a multimodal comprehension. The case study we
present as a baseline shows an approach that relies on recent techniques
provides an acceptable performance level, but more work is necessary before it
can reliably be used and trusted in a production setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13942">
<div class="article-summary-box-inner">
<span><p>Past work in natural language processing interpretability focused mainly on
popular classification tasks while largely overlooking generation settings,
partly due to a lack of dedicated tools. In this work, we introduce Inseq, a
Python library to democratize access to interpretability analyses of sequence
generation models. Inseq enables intuitive and optimized extraction of models'
internal information and feature importance scores for popular decoder-only and
encoder-decoder Transformers architectures. We showcase its potential by
adopting it to highlight gender biases in machine translation models and locate
factual knowledge inside GPT-2. Thanks to its extensible interface supporting
cutting-edge techniques such as contrastive feature attribution, Inseq can
drive future advances in explainable natural language generation, centralizing
good practices and enabling fair and reproducible model evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09752">
<div class="article-summary-box-inner">
<span><p>Many natural language processing tasks benefit from long inputs, but
processing long documents with Transformers is expensive -- not only due to
quadratic attention complexity but also from applying feedforward and
projection layers to every token. However, not all tokens are equally
important, especially for longer documents. We propose CoLT5, a long-input
Transformer model that builds on this intuition by employing conditional
computation, devoting more resources to important tokens in both feedforward
and attention layers. We show that CoLT5 achieves stronger performance than
LongT5 with much faster training and inference, achieving SOTA on the
long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably
make use of extremely long inputs, showing strong gains up to 64k input length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10475">
<div class="article-summary-box-inner">
<span><p>Task semantics can be expressed by a set of input-to-output examples or a
piece of textual instruction. Conventional machine learning approaches for
natural language processing (NLP) mainly rely on the availability of
large-scale sets of task-specific examples. Two issues arise: first, collecting
task-specific labeled examples does not apply to scenarios where tasks may be
too complicated or costly to annotate, or the system is required to handle a
new task immediately; second, this is not user-friendly since end-users are
probably more willing to provide task description rather than a set of examples
before using the system. Therefore, the community is paying increasing interest
in a new supervision-seeking paradigm for NLP: learning from task instructions.
Despite its impressive progress, there are some common issues that the
community struggles with. This survey paper tries to summarize the current
research on instruction learning, particularly, by answering the following
questions: (i) what is task instruction, and what instruction types exist? (ii)
how to model instructions? (iii) what factors influence and explain the
instructions' performance? (iv) what challenges remain in instruction learning?
To our knowledge, this is the first comprehensive survey about textual
instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12712">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17876">
<div class="article-summary-box-inner">
<span><p>We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading
dataset, designed to support the development of fair and transparent NLP
models. WebQAmGaze includes webcam eye-tracking data from 332 participants
naturally reading English, Spanish, and German texts. Each participant performs
two reading tasks composed of five texts, a normal reading and an
information-seeking task. After preprocessing the data, we find that fixations
on relevant spans seem to indicate correctness when answering the comprehension
questions. Additionally, we perform a comparative analysis of the data
collected to high-quality eye-tracking data. The results show a moderate
correlation between the features obtained with the webcam-ET compared to those
of a commercial ET device. We believe this data can advance webcam-based
reading studies and open a way to cheaper and more accessible data collection.
WebQAmGaze is useful to learn about the cognitive processes behind question
answering (QA) and to apply these insights to computational models of language
understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PEACH: Pre-Training Sequence-to-Sequence Multilingual Models for Translation with Semi-Supervised Pseudo-Parallel Document Generation. (arXiv:2304.01282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01282">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-training significantly improves many multilingual NLP tasks,
including machine translation. Most existing methods are based on some variants
of masked language modeling and text-denoising objectives on monolingual data.
Multilingual pre-training on monolingual data ignores the availability of
parallel data in many language pairs. Also, some other works integrate the
available human-generated parallel translation data in their pre-training. This
kind of parallel data is definitely helpful, but it is limited even in
high-resource language pairs. This paper introduces a novel semi-supervised
method, SPDG, that generates high-quality pseudo-parallel data for multilingual
pre-training. First, a denoising model is pre-trained on monolingual data to
reorder, add, remove, and substitute words, enhancing the pre-training
documents' quality. Then, we generate different pseudo-translations for each
pre-training document using dictionaries for word-by-word translation and
applying the pre-trained denoising model. The resulting pseudo-parallel data is
then used to pre-train our multilingual sequence-to-sequence model, PEACH. Our
experiments show that PEACH outperforms existing approaches used in training
mT5 and mBART on various translation tasks, including supervised, zero- and
few-shot scenarios. Moreover, PEACH's ability to transfer knowledge between
similar languages makes it particularly useful for low-resource languages. Our
results demonstrate that with high-quality dictionaries for generating accurate
pseudo-parallel, PEACH can be valuable for low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering. (arXiv:2304.02138v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02138">
<div class="article-summary-box-inner">
<span><p>The widespread adoption of large language models (LLMs), such as OpenAI's
ChatGPT, could revolutionize various industries, including geotechnical
engineering. However, GPT models can sometimes generate plausible-sounding but
false outputs, leading to hallucinations. In this article, we discuss the
importance of prompt engineering in mitigating these risks and harnessing the
full potential of GPT for geotechnical applications. We explore the challenges
and pitfalls associated with LLMs and highlight the role of context in ensuring
accurate and valuable responses. Furthermore, we examine the development of
context-specific search engines and the potential of LLMs to become a natural
interface for complex tasks, such as data analysis and design. We also develop
a unified interface using natural language to handle complex geotechnical
engineering tasks and data analysis. By integrating GPT into geotechnical
engineering workflows, professionals can streamline their work and develop
sustainable and resilient infrastructure systems for the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04933">
<div class="article-summary-box-inner">
<span><p>Resource limitations make it hard to provide all students with one of the
most effective educational interventions: personalized instruction.
Reinforcement learning could be a key tool to reduce the development cost and
improve the effectiveness of intelligent tutoring software that aims to provide
the right support, at the right time, to a student. Here we illustrate that
deep reinforcement learning can be used to provide adaptive pedagogical support
to students learning about the concept of volume in a narrative storyline
software. Using explainable artificial intelligence tools, we extracted
interpretable insights about the pedagogical policy learned and demonstrated
that the resulting policy had similar performance in a different student
population. Most importantly, in both studies, the reinforcement-learning
narrative system had the largest benefit for those students with the lowest
initial pretest scores, suggesting the opportunity for AI to adapt and provide
support for those most in need.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign Language Translation from Instructional Videos. (arXiv:2304.06371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06371">
<div class="article-summary-box-inner">
<span><p>The advances in automatic sign language translation (SLT) to spoken languages
have been mostly benchmarked with datasets of limited size and restricted
domains. Our work advances the state of the art by providing the first baseline
results on How2Sign, a large and broad dataset.
</p>
<p>We train a Transformer over I3D video features, using the reduced BLEU as a
reference metric for validation, instead of the widely used BLEU score. We
report a result of 8.03 on the BLEU score, and publish the first open-source
implementation of its kind to promote further advances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06447">
<div class="article-summary-box-inner">
<span><p>Document-based Visual Question Answering examines the document understanding
of document images in conditions of natural language questions. We proposed a
new document-based VQA dataset, PDF-VQA, to comprehensively examine the
document understanding from various aspects, including document element
recognition, document layout structural understanding as well as contextual
understanding and key information extraction. Our PDF-VQA dataset extends the
current scale of document understanding that limits on the single document page
to the new scale that asks questions over the full document of multiple pages.
We also propose a new graph-based VQA model that explicitly integrates the
spatial and hierarchically structural relationships between different document
elements to boost the document structural understanding. The performances are
compared with several baselines over different question types and
tasks\footnote{The full dataset will be released after paper acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06623">
<div class="article-summary-box-inner">
<span><p>Answering questions related to the legal domain is a complex task, primarily
due to the intricate nature and diverse range of legal document systems.
Providing an accurate answer to a legal query typically necessitates
specialized knowledge in the relevant domain, which makes this task all the
more challenging, even for human experts. Question answering (QA) systems are
designed to generate answers to questions asked in human languages. QA uses
natural language processing to understand questions and search through
information to find relevant answers. QA has various practical applications,
including customer service, education, research, and cross-lingual
communication. However, QA faces challenges such as improving natural language
understanding and handling complex and ambiguous questions. Answering questions
related to the legal domain is a complex task, primarily due to the intricate
nature and diverse range of legal document systems. Providing an accurate
answer to a legal query typically necessitates specialized knowledge in the
relevant domain, which makes this task all the more challenging, even for human
experts. At this time, there is a lack of surveys that discuss legal question
answering. To address this problem, we provide a comprehensive survey that
reviews 14 benchmark datasets for question-answering in the legal field as well
as presents a comprehensive review of the state-of-the-art Legal Question
Answering deep learning models. We cover the different architectures and
techniques used in these studies and the performance and limitations of these
models. Moreover, we have established a public GitHub repository where we
regularly upload the most recent articles, open data, and source code. The
repository is available at:
\url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">G2T: A Simple but Effective Framework for Topic Modeling based on Pretrained Language Model and Community Detection. (arXiv:2304.06653v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06653">
<div class="article-summary-box-inner">
<span><p>It has been reported that clustering-based topic models, which cluster
high-quality sentence embeddings with an appropriate word selection method, can
generate better topics than generative probabilistic topic models. However,
these approaches suffer from the inability to select appropriate parameters and
incomplete models that overlook the quantitative relation between words with
topics and topics with text. To solve these issues, we propose graph to topic
(G2T), a simple but effective framework for topic modelling. The framework is
composed of four modules. First, document representation is acquired using
pretrained language models. Second, a semantic graph is constructed according
to the similarity between document representations. Third, communities in
document semantic graphs are identified, and the relationship between topics
and documents is quantified accordingly. Fourth, the word--topic distribution
is computed based on a variant of TFIDF. Automatic evaluation suggests that G2T
achieved state-of-the-art performance on both English and Chinese documents
with different lengths. Human judgements demonstrate that G2T can produce
topics with better interpretability and coverage than baselines. In addition,
G2T can not only determine the topic number automatically but also give the
probabilistic distribution of words in topics and topics in documents. Finally,
G2T is publicly available, and the distillation experiments provide instruction
on how it works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06671">
<div class="article-summary-box-inner">
<span><p>Spatial control is a core capability in controllable image generation.
Advancements in layout-guided image generation have shown promising results on
in-distribution (ID) datasets with similar spatial configurations. However, it
is unclear how these models perform when facing out-of-distribution (OOD)
samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,
a diagnostic benchmark for layout-guided image generation that examines four
categories of spatial control skills: number, position, size, and shape. We
benchmark two recent representative layout-guided image generation methods and
observe that the good ID layout control may not generalize well to arbitrary
layouts in the wild (e.g., objects at the boundary). Next, we propose
IterInpaint, a new baseline that generates foreground and background regions in
a step-by-step manner via inpainting, demonstrating stronger generalizability
than existing models on OOD layouts in LayoutBench. We perform quantitative and
qualitative evaluation and fine-grained analysis on the four LayoutBench skills
to pinpoint the weaknesses of existing models. Lastly, we show comprehensive
ablation studies on IterInpaint, including training task ratio, crop&amp;paste vs.
repaint, and generation order. Project website: https://layoutbench.github.io
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-17 23:11:39.629543330 UTC">2023-04-17 23:11:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>