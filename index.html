<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-20T01:30:00Z">09-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Representation Pre-training with Complements from Program Executions. (arXiv:2309.09980v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09980">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) for natural language processing have been
grafted onto programming language modeling for advancing code intelligence.
Although it can be represented in the text format, code is syntactically more
rigorous in order to be properly compiled or interpreted to perform a desired
set of behaviors given any inputs. In this case, existing works benefit from
syntactic representations to learn from code less ambiguously in the forms of
abstract syntax tree, control-flow graph, etc. However, programs with the same
purpose can be implemented in various ways showing different syntactic
representations while the ones with similar implementations can have distinct
behaviors. Though trivially demonstrated during executions, such semantics
about functionality are challenging to be learned directly from code,
especially in an unsupervised manner. Hence, in this paper, we propose
FuzzPretrain to explore the dynamic information of programs revealed by their
test cases and embed it into the feature representations of code as
complements. The test cases are obtained with the assistance of a customized
fuzzer and are only required during pre-training. FuzzPretrain yielded more
than 6%/9% mAP improvements on code search over its counterparts trained with
only source code or AST, respectively. Our extensive experimental results show
the benefits of learning discriminative code representations with program
executions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09992">
<div class="article-summary-box-inner">
<span><p>The authors explain where OpenAI got the tax law example in its livestream
demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to
reliably calculate taxes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speech Recognition for African American English With Audio Classification. (arXiv:2309.09996v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09996">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) systems have been shown to have large
quality disparities between the language varieties they are intended or
expected to recognize. One way to mitigate this is to train or fine-tune models
with more representative datasets. But this approach can be hindered by limited
in-domain data for training and evaluation. We propose a new way to improve the
robustness of a US English short-form speech recognizer using a small amount of
out-of-domain (long-form) African American English (AAE) data. We use CORAAL,
YouTube and Mozilla Common Voice to train an audio classifier to approximately
output whether an utterance is AAE or some other variety including Mainstream
American English (MAE). By combining the classifier output with coarse
geographic information, we can select a subset of utterances from a large
corpus of untranscribed short-form queries for semi-supervised learning at
scale. Fine-tuning on this data results in a 38.5% relative word error rate
disparity reduction between AAE and MAE without reducing MAE quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting covariate drift in text data using document embeddings and dimensionality reduction. (arXiv:2309.10000v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10000">
<div class="article-summary-box-inner">
<span><p>Detecting covariate drift in text data is essential for maintaining the
reliability and performance of text analysis models. In this research, we
investigate the effectiveness of different document embeddings, dimensionality
reduction techniques, and drift detection methods for identifying covariate
drift in text data. We explore three popular document embeddings: term
frequency-inverse document frequency (TF-IDF) using Latent semantic
analysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings,
with and without using principal component analysis (PCA) for dimensionality
reduction. To quantify the divergence between training and test data
distributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum
Mean Discrepancy (MMD) test as drift detection methods. Experimental results
demonstrate that certain combinations of embeddings, dimensionality reduction
techniques, and drift detection methods outperform others in detecting
covariate drift. Our findings contribute to the advancement of reliable text
analysis models by providing insights into effective approaches for addressing
covariate drift in text data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10003">
<div class="article-summary-box-inner">
<span><p>This work proposes to measure the scope of a patent claim as the reciprocal
of the self-information contained in this claim. Grounded in information
theory, this approach is based on the assumption that a rare concept is more
informative than a usual concept, inasmuch as it is more surprising. The
self-information is calculated from the probability of occurrence of that
claim, where the probability is calculated in accordance with a language model.
Five language models are considered, ranging from the simplest models (each
word or character is drawn from a uniform distribution) to intermediate models
(using average word or character frequencies), to a large language model
(GPT2). Interestingly, the simplest language models reduce the scope measure to
the reciprocal of the word or character count, a metric already used in
previous works. Application is made to nine series of patent claims directed to
distinct inventions, where the claims in each series have a gradually
decreasing scope. The performance of the language models is then assessed with
respect to several ad hoc tests. The more sophisticated the model, the better
the results. The GPT2 model outperforms models based on word and character
frequencies, which are themselves ahead of models based on word and character
counts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback. (arXiv:2309.10015v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10015">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning is a critical aspect of human communication. Despite
recent advances in conversational AI driven by large language models,
commonsense reasoning remains a challenging task. In this work, we introduce
SYNDICOM - a method for improving commonsense in dialogue response generation.
SYNDICOM consists of two components. The first component is a dataset composed
of commonsense dialogues created from a knowledge graph and synthesized into
natural language. This dataset includes both valid and invalid responses to
dialogue contexts, along with natural language feedback (NLF) for the invalid
responses. The second contribution is a two-step procedure: training a model to
predict natural language feedback (NLF) for invalid responses, and then
training a response generation model conditioned on the predicted NLF, the
invalid response, and the dialogue. SYNDICOM is scalable and does not require
reinforcement learning. Empirical results on three tasks are evaluated using a
broad range of metrics. SYNDICOM achieves a relative improvement of 53% over
ChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the
time. We will publicly release the code and the full dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Foundation Models: From Specialists to General-Purpose Assistants. (arXiv:2309.10020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10020">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of the taxonomy and evolution of
multimodal foundation models that demonstrate vision and vision-language
capabilities, focusing on the transition from specialist models to
general-purpose assistants. The research landscape encompasses five core
topics, categorized into two classes. (i) We start with a survey of
well-established research areas: multimodal foundation models pre-trained for
specific purposes, including two topics -- methods of learning vision backbones
for visual understanding and text-to-image generation. (ii) Then, we present
recent advances in exploratory, open research areas: multimodal foundation
models that aim to play the role of general-purpose assistants, including three
topics -- unified vision models inspired by large language models (LLMs),
end-to-end training of multimodal LLMs, and chaining multimodal tools with
LLMs. The target audiences of the paper are researchers, graduate students, and
professionals in computer vision and vision-language multimodal communities who
are eager to learn the basics and recent advances in multimodal foundation
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation. (arXiv:2309.10057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10057">
<div class="article-summary-box-inner">
<span><p>Information extraction systems often produce hundreds to thousands of strings
on a specific topic. We present a method that facilitates better consumption of
these strings, in an exploratory setting in which a user wants to both get a
broad overview of what's available, and a chance to dive deeper on some
aspects. The system works by grouping similar items together and arranging the
remaining items into a hierarchical navigable DAG structure. We apply the
method to medical information extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10066">
<div class="article-summary-box-inner">
<span><p>Purpose: To determine if fine-tuned large language models (LLMs) can generate
accurate, personalized impressions for whole-body PET reports. Materials and
Methods: Twelve language models were trained on a corpus of PET reports using
the teacher-forcing algorithm, with the report findings as input and the
clinical impressions as reference. An extra input token encodes the reading
physician's identity, allowing models to learn physician-specific reporting
styles. Our corpus comprised 37,370 retrospective PET reports collected from
our institution between 2010 and 2022. To identify the best LLM, 30 evaluation
metrics were benchmarked against quality scores from two nuclear medicine (NM)
physicians, with the most aligned metrics selecting the model for expert
evaluation. In a subset of data, model-generated impressions and original
clinical impressions were assessed by three NM physicians according to 6
quality dimensions and an overall utility score (5-point scale). Each physician
reviewed 12 of their own reports and 12 reports from other physicians.
Bootstrap resampling was used for statistical analysis. Results: Of all
evaluation metrics, domain-adapted BARTScore and PEGASUSScore showed the
highest Spearman's rho correlations (0.568 and 0.563) with physician
preferences. Based on these metrics, the fine-tuned PEGASUS model was selected
as the top LLM. When physicians reviewed PEGASUS-generated impressions in their
own style, 89% were considered clinically acceptable, with a mean utility score
of 4.08/5. Physicians rated these personalized impressions as comparable in
overall utility to the impressions dictated by other physicians (4.03, P=0.41).
Conclusion: Personalized impressions generated by PEGASUS were clinically
useful, highlighting its potential to expedite PET reporting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HTEC: Human Transcription Error Correction. (arXiv:2309.10089v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10089">
<div class="article-summary-box-inner">
<span><p>High-quality human transcription is essential for training and improving
Automatic Speech Recognition (ASR) models. Recent study~\cite{libricrowd} has
found that every 1% worse transcription Word Error Rate (WER) increases
approximately 2% ASR WER by using the transcriptions to train ASR models.
Transcription errors are inevitable for even highly-trained annotators.
However, few studies have explored human transcription correction. Error
correction methods for other problems, such as ASR error correction and
grammatical error correction, do not perform sufficiently for this problem.
Therefore, we propose HTEC for Human Transcription Error Correction. HTEC
consists of two stages: Trans-Checker, an error detection model that predicts
and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative
model that fills masked positions. We propose a holistic list of correction
operations, including four novel operations handling deletion errors. We
further propose a variant of embeddings that incorporates phoneme information
into the input of the transformer. HTEC outperforms other methods by a large
margin and surpasses human annotators by 2.2% to 4.5% in WER. Finally, we
deployed HTEC to assist human annotators and showed HTEC is particularly
effective as a co-pilot, which improves transcription quality by 15.1% without
sacrificing transcription velocity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Coarse-to-Fine Alignment for Video-Text Retrieval. (arXiv:2309.10091v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10091">
<div class="article-summary-box-inner">
<span><p>The canonical approach to video-text retrieval leverages a coarse-grained or
fine-grained alignment between visual and textual information. However,
retrieving the correct video according to the text query is often challenging
as it requires the ability to reason about both high-level (scene) and
low-level (object) visual clues and how they relate to the text query. To this
end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.
Specifically, our model captures the cross-modal similarity information at
different granularity levels. To alleviate the effect of irrelevant visual
clues, we also apply an Interactive Similarity Aggregation module (ISA) to
consider the importance of different visual features while aggregating the
cross-modal similarity to obtain a similarity score for each granularity.
Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of
each level before summing them, alleviating over- and under-representation
issues at different levels. By jointly considering the crossmodal similarity of
different granularity, UCoFiA allows the effective unification of multi-grained
alignments. Empirically, UCoFiA outperforms previous state-of-the-art
CLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,
1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,
Activity-Net, and DiDeMo, respectively. Our code is publicly available at
https://github.com/Ziyang412/UCoFiA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Catastrophic Forgetting in Language Models via Implicit Inference. (arXiv:2309.10105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10105">
<div class="article-summary-box-inner">
<span><p>Fine-tuning (via methods such as instruction-tuning or reinforcement learning
from human feedback) is a crucial step in training language models to robustly
carry out tasks of interest. However, we lack a systematic understanding of the
effects of fine-tuning, particularly on tasks outside the narrow fine-tuning
distribution. In a simplified scenario, we demonstrate that improving
performance on tasks within the fine-tuning data distribution comes at the
expense of suppressing model capabilities on other tasks. This degradation is
especially pronounced for tasks "closest" to the fine-tuning distribution. We
hypothesize that language models implicitly infer the task of the prompt
corresponds, and the fine-tuning process predominantly skews this task
inference towards tasks in the fine-tuning distribution. To test this
hypothesis, we propose Conjugate Prompting to see if we can recover pretrained
capabilities. Conjugate prompting artificially makes the task look farther from
the fine-tuning distribution while requiring the same capability. We find that
conjugate prompting systematically recovers some of the pretraining
capabilities on our synthetic setup. We then apply conjugate prompting to
real-world LLMs using the observation that fine-tuning distributions are
typically heavily skewed towards English. We find that simply translating the
prompts to different languages can cause the fine-tuned models to respond like
their pretrained counterparts instead. This allows us to recover the in-context
learning abilities lost via instruction tuning, and more concerningly, to
recover harmful content generation suppressed by safety fine-tuning in chatbots
like ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Adaptation for Parsing Contextual Utterances with LLMs. (arXiv:2309.10168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10168">
<div class="article-summary-box-inner">
<span><p>We evaluate the ability of semantic parsers based on large language models
(LLMs) to handle contextual utterances. In real-world settings, there typically
exists only a limited number of annotated contextual utterances due to
annotation cost, resulting in an imbalance compared to non-contextual
utterances. Therefore, parsers must adapt to contextual utterances with a few
training examples. We examine four major paradigms for doing so in
conversational semantic parsing i.e., Parse-with-Utterance-History,
Parse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. To
facilitate such cross-paradigm comparisons, we construct
SMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with
additional annotations. Experiments with in-context learning and fine-tuning
suggest that Rewrite-then-Parse is the most promising paradigm when
holistically considering parsing accuracy, annotation cost, and error types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10182">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a novel research problem: assessing positive and
risky messages from music products. We first establish a benchmark for
multi-angle multi-level music content assessment and then present an effective
multi-task prediction model with ordinality-enforcement to solve this problem.
Our result shows the proposed method not only significantly outperforms strong
task-specific counterparts but can concurrently evaluate multiple aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stabilizing RLHF through Advantage Model and Selective Rehearsal. (arXiv:2309.10202v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10202">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have revolutionized natural language processing,
yet aligning these models with human values and preferences using RLHF remains
a significant challenge. This challenge is characterized by various
instabilities, such as reward hacking and catastrophic forgetting. In this
technical report, we propose two innovations to stabilize RLHF training: 1)
Advantage Model, which directly models advantage score i.e., extra reward
compared to the expected rewards and regulates score distributions across tasks
to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic
forgetting by strategically selecting data for PPO training and knowledge
rehearsing. Our experimental analysis on public and proprietary datasets
reveals that the proposed methods not only increase stability in RLHF training
but also achieve higher reward scores and win rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models. (arXiv:2309.10238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10238">
<div class="article-summary-box-inner">
<span><p>Privacy policies serve as the primary conduit through which online service
providers inform users about their data collection and usage procedures.
However, in a bid to be comprehensive and mitigate legal risks, these policy
documents are often quite verbose. In practical use, users tend to click the
Agree button directly rather than reading them carefully. This practice exposes
users to risks of privacy leakage and legal issues. Recently, the advent of
Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new
possibilities for text analysis, especially for lengthy documents like privacy
policies. In this study, we investigate a privacy policy text analysis
framework PolicyGPT based on the LLM. This framework was tested using two
datasets. The first dataset comprises of privacy policies from 115 websites,
which were meticulously annotated by legal experts, categorizing each segment
into one of 10 classes. The second dataset consists of privacy policies from
304 popular mobile applications, with each sentence manually annotated and
classified into one of another 10 categories. Under zero-shot learning
conditions, PolicyGPT demonstrated robust performance. For the first dataset,
it achieved an accuracy rate of 97%, while for the second dataset, it attained
an 87% accuracy rate, surpassing that of the baseline machine learning and
neural network models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is the Best Automated Metric for Text to Motion Generation?. (arXiv:2309.10248v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10248">
<div class="article-summary-box-inner">
<span><p>There is growing interest in generating skeleton-based human motions from
natural language descriptions. While most efforts have focused on developing
better neural architectures for this task, there has been no significant work
on determining the proper evaluation metric. Human evaluation is the ultimate
accuracy measure for this task, and automated metrics should correlate well
with human quality judgments. Since descriptions are compatible with many
motions, determining the right metric is critical for evaluating and designing
effective generative models. This paper systematically studies which metrics
best align with human evaluations and proposes new metrics that align even
better. Our findings indicate that none of the metrics currently used for this
task show even a moderate correlation with human judgments on a sample level.
However, for assessing average model performance, commonly used metrics such as
R-Precision and less-used coordinate errors show strong correlations.
Additionally, several recently developed metrics are not recommended due to
their low correlation compared to alternatives. We also introduce a novel
metric based on a multimodal BERT-like model, MoBERT, which offers strongly
human-correlated sample-level evaluations while maintaining near-perfect
model-level correlation. Our results demonstrate that this new metric exhibits
extensive benefits over all current alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10254">
<div class="article-summary-box-inner">
<span><p>Large language model (LLM) platforms, such as ChatGPT, have recently begun
offering a plugin ecosystem to interface with third-party services on the
internet. While these plugins extend the capabilities of LLM platforms, they
are developed by arbitrary third parties and thus cannot be implicitly trusted.
Plugins also interface with LLM platforms and users using natural language,
which can have imprecise interpretations. In this paper, we propose a framework
that lays a foundation for LLM platform designers to analyze and improve the
security, privacy, and safety of current and future plugin-integrated LLM
platforms. Our framework is a formulation of an attack taxonomy that is
developed by iteratively exploring how LLM platform stakeholders could leverage
their capabilities and responsibilities to mount attacks against each other. As
part of our iterative process, we apply our framework in the context of
OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the
potential for the types of issues that we outline in our attack taxonomy. We
conclude by discussing novel challenges and by providing recommendations to
improve the security, privacy, and safety of present and future LLM-based
computing platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi. (arXiv:2309.10272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10272">
<div class="article-summary-box-inner">
<span><p>One of the most popular downstream tasks in the field of Natural Language
Processing is text classification. Text classification tasks have become more
daunting when the texts are code-mixed. Though they are not exposed to such
text during pre-training, different BERT models have demonstrated success in
tackling Code-Mixed NLP challenges. Again, in order to enhance their
performance, Code-Mixed NLP models have depended on combining synthetic data
with real-world data. It is crucial to understand how the BERT models'
performance is impacted when they are pretrained using corresponding code-mixed
languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model
pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model
fine-tuned on code-mixed data. Both models are evaluated across multiple NLP
tasks and demonstrate competitive performance against larger models like mBERT
and XLM-R. Our two-tiered pre-training approach offers efficient alternatives
for multilingual and code-mixed language understanding, contributing to
advancements in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition. (arXiv:2309.10294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10294">
<div class="article-summary-box-inner">
<span><p>In this paper, we explored how to boost speech emotion recognition (SER) with
the state-of-the-art speech pre-trained model (PTM), data2vec, text generation
technique, GPT-4, and speech synthesis technique, Azure TTS. First, we
investigated the representation ability of different speech self-supervised
pre-trained models, and we found that data2vec has a good representation
ability on the SER task. Second, we employed a powerful large language model
(LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate
emotionally congruent text and speech. We carefully designed the text prompt
and dataset construction, to obtain the synthetic emotional speech data with
high quality. Third, we studied different ways of data augmentation to promote
the SER task with synthetic speech, including random mixing, adversarial
training, transfer learning, and curriculum learning. Experiments and ablation
studies on the IEMOCAP dataset demonstrate the effectiveness of our method,
compared with other data augmentation methods, and data augmentation with other
synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using fine-tuning and min lookahead beam search to improve Whisper. (arXiv:2309.10299v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10299">
<div class="article-summary-box-inner">
<span><p>The performance of Whisper in low-resource languages is still far from
perfect. In addition to a lack of training data on low-resource languages, we
identify some limitations in the beam search algorithm used in Whisper. To
address these issues, we fine-tune Whisper on additional data and propose an
improved decoding algorithm. On the Vietnamese language, fine-tuning
Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the
zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to
full-parameter fine-tuning. Additionally, by using Filter-Ends and Min
Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range
of languages compared to standard beam search. These results generalise to
larger Whisper model sizes. We also prove a theorem that Min Lookahead
outperforms the standard beam search algorithm used in Whisper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10305">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable performance on a
variety of natural language tasks based on just a few examples of natural
language instructions, reducing the need for extensive feature engineering.
However, most powerful LLMs are closed-source or limited in their capability
for languages other than English. In this technical report, we present Baichuan
2, a series of large-scale multilingual language models containing 7 billion
and 13 billion parameters, trained from scratch, on 2.6 trillion tokens.
Baichuan 2 matches or outperforms other open-source models of similar size on
public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan
2 excels in vertical domains such as medicine and law. We will release all
pre-training model checkpoints to benefit the research community in better
understanding the training dynamics of Baichuan 2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rigorously Assessing Natural Language Explanations of Neurons. (arXiv:2309.10312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10312">
<div class="article-summary-box-inner">
<span><p>Natural language is an appealing medium for explaining how large language
models process and store information, but evaluating the faithfulness of such
explanations is challenging. To help address this, we develop two modes of
evaluation for natural language explanations that claim individual neurons
represent a concept in a text input. In the observational mode, we evaluate
claims that a neuron $a$ activates on all and only input strings that refer to
a concept picked out by the proposed explanation $E$. In the intervention mode,
we construe $E$ as a claim that the neuron $a$ is a causal mediator of the
concept denoted by $E$. We apply our framework to the GPT-4-generated
explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the
most confident explanations have high error rates and little to no causal
efficacy. We close the paper by critically assessing whether natural language
is a good choice for explanations and whether neurons are the best level of
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10313">
<div class="article-summary-box-inner">
<span><p>Following the success of GPT4, there has been a surge in interest in
multimodal large language model (MLLM) research. This line of research focuses
on developing general-purpose LLMs through fine-tuning pre-trained LLMs and
vision models. However, catastrophic forgetting, a notorious phenomenon where
the fine-tuned model fails to retain similar performance compared to the
pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).
In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the
catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.
We first apply EMT to evaluate several open-source fine-tuned MLLMs and we
discover that almost all evaluated MLLMs fail to retain the same performance
levels as their vision encoders on standard image classification tasks.
Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess
performance throughout the fine-tuning. Interestingly, our results suggest that
early-stage fine-tuning on an image dataset improves performance across other
image datasets, by enhancing the alignment of text and visual features.
However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in
a significant loss of generalizability, even when the image encoder remains
frozen. Our results suggest that MLLMs have yet to demonstrate performance on
par with their vision models on standard image classification tasks and the
current MLLM fine-tuning procedure still has room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10326">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the success of question answering (QA),
especially its potential to be a foundation paradigm for tackling diverse NLP
tasks. However, obtaining sufficient data to build an effective and stable QA
system still remains an open problem. For this problem, we introduce an
iterative bootstrapping framework for QA data augmentation (named QASnowball),
which can iteratively generate large-scale high-quality QA data based on a seed
set of supervised examples. Specifically, QASnowball consists of three modules,
an answer extractor to extract core phrases in unlabeled documents as candidate
answers, a question generator to generate questions based on documents and
candidate answers, and a QA data filter to filter out high-quality QA data.
Moreover, QASnowball can be self-enhanced by reseeding the seed set to
fine-tune itself in different iterations, leading to continual improvements in
the generation quality. We conduct experiments in the high-resource English
scenario and the medium-resource Chinese scenario, and the experimental results
show that the data generated by QASnowball can facilitate QA models: (1)
training models on the generated data achieves comparable results to using
supervised data, and (2) pre-training on the generated data and fine-tuning on
supervised data can achieve better performance. Our code and generated data
will be released to advance further work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KoBigBird-large: Transformation of Transformer for Korean Language Understanding. (arXiv:2309.10339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10339">
<div class="article-summary-box-inner">
<span><p>This work presents KoBigBird-large, a large size of Korean BigBird that
achieves state-of-the-art performance and allows long sequence processing for
Korean language understanding. Without further pretraining, we only transform
the architecture and extend the positional encoding with our proposed Tapered
Absolute Positional Encoding Representations (TAPER). In experiments,
KoBigBird-large shows state-of-the-art overall performance on Korean language
understanding benchmarks and the best performance on document classification
and question answering tasks for longer sequences against the competitive
baseline models. We publicly release our model here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Agent Behavior with Large Language Models. (arXiv:2309.10346v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10346">
<div class="article-summary-box-inner">
<span><p>Intelligent agents such as robots are increasingly deployed in real-world,
safety-critical settings. It is vital that these agents are able to explain the
reasoning behind their decisions to human counterparts, however, their behavior
is often produced by uninterpretable models such as deep neural networks. We
propose an approach to generate natural language explanations for an agent's
behavior based only on observations of states and actions, agnostic to the
underlying model representation. We show how a compact representation of the
agent's behavior can be learned and used to produce plausible explanations with
minimal hallucination while affording user interaction with a pre-trained large
language model. Through user studies and empirical experiments, we show that
our approach generates explanations as helpful as those generated by a human
domain expert while enabling beneficial interactions such as clarification and
counterfactual queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning. (arXiv:2309.10359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10359">
<div class="article-summary-box-inner">
<span><p>Unsupported and unfalsifiable claims we encounter in our daily lives can
influence our view of the world. Characterizing, summarizing, and -- more
generally -- making sense of such claims, however, can be challenging. In this
work, we focus on fine-grained debate topics and formulate a new task of
distilling, from such claims, a countable set of narratives. We present a
crowdsourced dataset of 12 controversial topics, comprising more than 120k
arguments, claims, and comments from heterogeneous sources, each annotated with
a narrative label. We further investigate how large language models (LLMs) can
be used to synthesise claims using In-Context Learning. We find that generated
claims with supported evidence can be used to improve the performance of
narrative classification models and, additionally, that the same model can
infer the stance and aspect using a few training examples. Such a model can be
useful in applications which rely on narratives , e.g. fact-checking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10400">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce Positional Skip-wisE (PoSE) training for
efficient adaptation of large language models~(LLMs) to extremely long context
windows. PoSE decouples train length from target context window size by
simulating long inputs using a fixed context window with manipulated position
indices during training. Concretely, we select several short chunks from a long
input sequence, and introduce distinct skipping bias terms to modify the
position indices of each chunk. These bias terms, along with the length of each
chunk, are altered for each training example, allowing the model to adapt to
all positions within the target context window without training on full length
inputs. Experiments show that, compared with fine-tuning on the full length,
PoSE greatly reduces memory and time overhead with minimal impact on
performance. Leveraging this advantage, we have successfully extended the LLaMA
model to 128k tokens. Furthermore, we empirically confirm that PoSE is
compatible with all RoPE-based LLMs and various position interpolation
strategies. Notably, by decoupling fine-tuning length from target context
window, PoSE can theoretically extend the context window infinitely,
constrained only by memory usage for inference. With ongoing advancements for
efficient inference, we believe PoSE holds great promise for scaling the
context window even further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PICK: Polished & Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems. (arXiv:2309.10413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10413">
<div class="article-summary-box-inner">
<span><p>Grounding dialogue response generation on external knowledge is proposed to
produce informative and engaging responses. However, current knowledge-grounded
dialogue (KGD) systems often fail to align the generated responses with
human-preferred qualities due to several issues like hallucination and the lack
of coherence. Upon analyzing multiple language model generations, we observe
the presence of alternative generated responses within a single decoding
process. These alternative responses are more faithful and exhibit a comparable
or higher level of relevance to prior conversational turns compared to the
optimal responses prioritized by the decoding processes. To address these
challenges and driven by these observations, we propose Polished \&amp; Informed
Candidate Scoring (PICK), a generation re-scoring framework that empowers
models to generate faithful and relevant responses without requiring additional
labeled data or model tuning. Through comprehensive automatic and human
evaluations, we demonstrate the effectiveness of PICK in generating responses
that are more faithful while keeping them relevant to the dialogue history.
Furthermore, PICK consistently improves the system's performance with both
oracle and retrieved knowledge in all decoding strategies. We provide the
detailed implementation in https://github.com/bryanwilie/pick .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Writer-Defined AI Personas for On-Demand Feedback Generation. (arXiv:2309.10433v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10433">
<div class="article-summary-box-inner">
<span><p>Compelling writing is tailored to its audience. This is challenging, as
writers may struggle to empathize with readers, get feedback in time, or gain
access to the target group. We propose a concept that generates on-demand
feedback, based on writer-defined AI personas of any target audience. We
explore this concept with a prototype (using GPT-3.5) in two user studies (N=5
and N=11): Writers appreciated the concept and strategically used personas for
getting different perspectives. The feedback was seen as helpful and inspired
revisions of text and personas, although it was often verbose and unspecific.
We discuss the impact of on-demand feedback, the limited representativity of
contemporary AI systems, and further ideas for defining AI personas. This work
contributes to the vision of supporting writers with AI by expanding the
socio-technical perspective in AI tool design: To empower creators, we also
need to keep in mind their relationship to an audience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10435">
<div class="article-summary-box-inner">
<span><p>Recommender systems are essential for online applications, and sequential
recommendation has enjoyed significant prevalence due to its expressive ability
to capture dynamic user interests. However, previous sequential modeling
methods still have limitations in capturing contextual information. The primary
reason for this issue is that language models often lack an understanding of
domain-specific knowledge and item-related textual content. To address this
issue, we adopt a new sequential recommendation paradigm and propose LANCER,
which leverages the semantic understanding capabilities of pre-trained language
models to generate personalized recommendations. Our approach bridges the gap
between language models and recommender systems, resulting in more human-like
recommendations. We demonstrate the effectiveness of our approach through
experiments on several benchmark datasets, showing promising results and
providing valuable insights into the influence of our model on sequential
recommendation tasks. Furthermore, our experimental codes are publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10444">
<div class="article-summary-box-inner">
<span><p>Learnersourcing involves students generating and sharing learning resources
with their peers. When learnersourcing multiple-choice questions, creating
explanations for the generated questions is a crucial step as it facilitates a
deeper understanding of the related concepts. However, it is often difficult
for students to craft effective explanations due to limited subject
understanding and a tendency to merely restate the question stem, distractors,
and correct answer. To help scaffold this task, in this work we propose a
self-reinforcement large-language-model framework, with the goal of generating
and evaluating explanations automatically. Comprising three modules, the
framework generates student-aligned explanations, evaluates these explanations
to ensure their quality and iteratively enhances the explanations. If an
explanation's evaluation score falls below a defined threshold, the framework
iteratively refines and reassesses the explanation. Importantly, our framework
emulates the manner in which students compose explanations at the relevant
grade level. For evaluation, we had a human subject-matter expert compare the
explanations generated by students with the explanations created by the
open-source large language model Vicuna-13B, a version of Vicuna-13B that had
been fine-tuned using our method, and by GPT-4. We observed that, when compared
to other large language models, GPT-4 exhibited a higher level of creativity in
generating explanations. We also found that explanations generated by GPT-4
were ranked higher by the human expert than both those created by the other
models and the original student-created explanations. Our findings represent a
significant advancement in enriching the learnersourcing experience for
students and enhancing the capabilities of large language models in educational
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10447">
<div class="article-summary-box-inner">
<span><p>Controllable text generation is a fundamental aspect of natural language
generation, with numerous methods proposed for different constraint types.
However, these approaches often require significant architectural or decoding
modifications, making them challenging to apply to additional constraints or
resolve different constraint combinations. To address this, our paper
introduces Regular Expression Instruction (REI), which utilizes an
instruction-based mechanism to fully exploit regular expressions' advantages to
uniformly model diverse constraints. Specifically, our REI supports all popular
fine-grained controllable generation constraints, i.e., lexical, positional,
and length, as well as their complex combinations, via regular expression-style
instructions. Our method only requires fine-tuning on medium-scale language
models or few-shot, in-context learning on large language models, and requires
no further adjustment when applied to various constraint combinations.
Experiments demonstrate that our straightforward approach yields high success
rates and adaptability to various constraints while maintaining competitiveness
in automatic metrics and outperforming most previous baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation. (arXiv:2309.10456v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10456">
<div class="article-summary-box-inner">
<span><p>Speaker diarization has gained considerable attention within speech
processing research community. Mainstream speaker diarization rely primarily on
speakers' voice characteristics extracted from acoustic signals and often
overlook the potential of semantic information. Considering the fact that
speech signals can efficiently convey the content of a speech, it is of our
interest to fully exploit these semantic cues utilizing language models. In
this work we propose a novel approach to effectively leverage semantic
information in clustering-based speaker diarization systems. Firstly, we
introduce spoken language understanding modules to extract speaker-related
semantic information and utilize these information to construct pairwise
constraints. Secondly, we present a novel framework to integrate these
constraints into the speaker diarization pipeline, enhancing the performance of
the entire system. Extensive experiments conducted on the public dataset
demonstrate the consistent superiority of our proposed approach over
acoustic-only speaker diarization systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation of GPT-4 on the ETHICS Dataset. (arXiv:2309.10492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10492">
<div class="article-summary-box-inner">
<span><p>This report summarizes a short study of the performance of GPT-4 on the
ETHICS dataset. The ETHICS dataset consists of five sub-datasets covering
different fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism,
and Commonsense Ethics. The moral judgments were curated so as to have a high
degree of agreement with the aim of representing shared human values rather
than moral dilemmas. GPT-4's performance is much better than that of previous
models and suggests that learning to work with common human values is not the
hard problem for AI ethics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval. (arXiv:2309.10506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10506">
<div class="article-summary-box-inner">
<span><p>Open-domain table question answering aims to provide answers to a question by
retrieving and extracting information from a large collection of tables.
Existing studies of open-domain table QA either directly adopt text retrieval
methods or consider the table structure only in the encoding layer for table
retrieval, which may cause syntactical and structural information loss during
table scoring. To address this issue, we propose a syntax- and structure-aware
retrieval method for the open-domain table QA task. It provides syntactical
representations for the question and uses the structural header and value
representations for the tables to avoid the loss of fine-grained syntactical
and structural information. Then, a syntactical-to-structural aggregator is
used to obtain the matching score between the question and a candidate table by
mimicking the human retrieval process. Experimental results show that our
method achieves the state-of-the-art on the NQ-tables dataset and overwhelms
strong baselines on a newly curated open-domain Text-to-SQL dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition. (arXiv:2309.10524v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10524">
<div class="article-summary-box-inner">
<span><p>We present a novel integration of an instruction-tuned large language model
(LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can
perform a wide range of linguistic tasks within zero-shot learning when
provided with a precise instruction or a prompt to guide the text generation
process towards the desired task. We explore using this zero-shot capability of
LLMs to extract linguistic information that can contribute to improving ASR
performance. Specifically, we direct an LLM to correct grammatical errors in an
ASR hypothesis and harness the embedded linguistic knowledge to conduct
end-to-end ASR. The proposed model is built on the hybrid connectionist
temporal classification (CTC) and attention architecture, where an
instruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder.
An ASR hypothesis, subject to correction, is obtained from the encoder via CTC
decoding, which is then fed into the LLM along with an instruction. The decoder
subsequently takes as input the LLM embeddings to perform sequence generation,
incorporating acoustic information from the encoder output. Experimental
results and analyses demonstrate that the proposed integration yields promising
performance improvements, and our approach largely benefits from LLM-based
rescoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NSOAMT -- New Search Only Approach to Machine Translation. (arXiv:2309.10526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10526">
<div class="article-summary-box-inner">
<span><p>Translation automation mechanisms and tools have been developed for several
years to bring people who speak different languages together. A "new search
only approach to machine translation" was adopted to tackle some of the
slowness and inaccuracy of the other technologies. The idea is to develop a
solution that, by indexing an incremental set of words that combine a certain
semantic meaning, makes it possible to create a process of correspondence
between their native language record and the language of translation. This
research principle assumes that the vocabulary used in a given type of
publication/document is relatively limited in terms of language style and word
diversity, which enhances the greater effect of instantaneously and rigor in
the translation process through the indexing process. A volume of electronic
text documents where processed and loaded into a database, and analyzed and
measured in order confirm the previous premise. Although the observed and
projected metric values did not give encouraging results, it was possible to
develop and make available a translation tool using this approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement. (arXiv:2309.10539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10539">
<div class="article-summary-box-inner">
<span><p>We develop and evaluate multilingual scientific documents similarity
measurement models in this work. Such models can be used to find related works
in different languages, which can help multilingual researchers find and
explore papers more efficiently. We propose the first multilingual scientific
documents dataset, Open-access Multilingual Scientific Documents (OpenMSD),
which has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we
pretrain science-specialized language models, and explore different strategies
to derive "related" paper pairs to fine-tune the models, including using a
mixture of citation, co-citation, and bibliographic-coupling pairs. To further
improve the models' performance for non-English papers, we explore the use of
generative language models to enrich the non-English papers with English
summaries. This allows us to leverage the models' English capabilities to
create better representations for non-English papers. Our best model
significantly outperforms strong baselines by 7-16% (in mean average
precision).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Leeching: An Extraction Attack Targeting LLMs. (arXiv:2309.10544v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10544">
<div class="article-summary-box-inner">
<span><p>Model Leeching is a novel extraction attack targeting Large Language Models
(LLMs), capable of distilling task-specific knowledge from a target LLM into a
reduced parameter model. We demonstrate the effectiveness of our attack by
extracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match
(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,
respectively for only $50 in API cost. We further demonstrate the feasibility
of adversarial attack transferability from an extracted model extracted via
Model Leeching to perform ML attack staging against a target LLM, resulting in
an 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings. (arXiv:2309.10551v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10551">
<div class="article-summary-box-inner">
<span><p>We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism
considering the neighbourhood of a word in a pretrained static word embedding
space to determine the minimal amount of noise required to guarantee a
specified privacy level. We first construct a nearest neighbour graph over the
words using their embeddings, and factorise it into a set of connected
components (i.e. neighbourhoods). We then separately apply different levels of
Gaussian noise to the words in each neighbourhood, determined by the set of
words in that neighbourhood. Experiments show that our proposed NADP mechanism
consistently outperforms multiple previously proposed DP mechanisms such as
Laplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while
guaranteeing higher levels of privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Modeling For Spoken Language Identification. (arXiv:2309.10567v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10567">
<div class="article-summary-box-inner">
<span><p>Spoken language identification refers to the task of automatically predicting
the spoken language in a given utterance. Conventionally, it is modeled as a
speech-based language identification task. Prior techniques have been
constrained to a single modality; however in the case of video data there is a
wealth of other metadata that may be beneficial for this task. In this work, we
propose MuSeLI, a Multimodal Spoken Language Identification method, which
delves into the use of various metadata sources to enhance language
identification. Our study reveals that metadata such as video title,
description and geographic location provide substantial information to identify
the spoken language of the multimedia recording. We conduct experiments using
two diverse public datasets of YouTube videos, and obtain state-of-the-art
results on the language identification task. We additionally conduct an
ablation study that describes the distinct contribution of each modality for
language recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Deep Cross-Language Entity Alignment. (arXiv:2309.10598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10598">
<div class="article-summary-box-inner">
<span><p>Cross-lingual entity alignment is the task of finding the same semantic
entities from different language knowledge graphs. In this paper, we propose a
simple and novel unsupervised method for cross-language entity alignment. We
utilize the deep learning multi-language encoder combined with a machine
translator to encode knowledge graph text, which reduces the reliance on label
data. Unlike traditional methods that only emphasize global or local alignment,
our method simultaneously considers both alignment strategies. We first view
the alignment task as a bipartite matching problem and then adopt the
re-exchanging idea to accomplish alignment. Compared with the traditional
bipartite matching algorithm that only gives one optimal solution, our
algorithm generates ranked matching results which enabled many potentials
downstream tasks. Additionally, our method can adapt two different types of
optimization (minimal and maximal) in the bipartite matching process, which
provides more flexibility. Our evaluation shows, we each scored 0.966, 0.990,
and 0.996 Hits@1 rates on the DBP15K dataset in Chinese, Japanese, and French
to English alignment tasks. We outperformed the state-of-the-art method in
unsupervised and semi-supervised categories. Compared with the state-of-the-art
supervised method, our method outperforms 2.6% and 0.4% in Ja-En and Fr-En
alignment tasks while marginally lower by 0.2% in the Zh-En alignment task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRACAS: A FRench Annotated Corpus of Attribution relations in newS. (arXiv:2309.10604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10604">
<div class="article-summary-box-inner">
<span><p>Quotation extraction is a widely useful task both from a sociological and
from a Natural Language Processing perspective. However, very little data is
available to study this task in languages other than English. In this paper, we
present a manually annotated corpus of 1676 newswire texts in French for
quotation extraction and source attribution. We first describe the composition
of our corpus and the choices that were made in selecting the data. We then
detail the annotation guidelines and annotation process, as well as a few
statistics about the final corpus and the obtained balance between quote types
(direct, indirect and mixed, which are particularly challenging). We end by
detailing our inter-annotator agreement between the 8 annotators who worked on
manual labelling, which is substantially high for such a difficult linguistic
phenomenon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Medical Dialogue Generation with Abstract Meaning Representations. (arXiv:2309.10608v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10608">
<div class="article-summary-box-inner">
<span><p>Medical Dialogue Generation serves a critical role in telemedicine by
facilitating the dissemination of medical expertise to patients. Existing
studies focus on incorporating textual representations, which have limited
their ability to represent the semantics of text, such as ignoring important
medical entities. To enhance the model's understanding of the textual semantics
and the medical knowledge including entities and relations, we introduce the
use of Abstract Meaning Representations (AMR) to construct graphical
representations that delineate the roles of language constituents and medical
entities within the dialogues. In this paper, We propose a novel framework that
models dialogues between patients and healthcare professionals using AMR
graphs, where the neural networks incorporate textual and graphical knowledge
with a dual attention mechanism. Experimental results show that our framework
outperforms strong baseline models in medical dialogue generation,
demonstrating the effectiveness of AMR graphs in enhancing the representations
of medical knowledge and logical relationships. Furthermore, to support future
research in this domain, we provide the corresponding source code at
https://github.com/Bernard-Yang/MedDiaAMR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10621">
<div class="article-summary-box-inner">
<span><p>Relevance labels, which indicate whether a search result is valuable to a
searcher, are key to evaluating and optimising search systems. The best way to
capture the true preferences of users is to ask them for their careful feedback
on which results would be useful, but this approach does not scale to produce a
large number of labels. Getting relevance labels at scale is usually done with
third-party labellers, who judge on behalf of the user, but there is a risk of
low-quality data if the labeller doesn't understand user needs. To improve
quality, one standard approach is to study real users through interviews, user
studies and direct feedback, find areas where labels are systematically
disagreeing with users, then educate labellers about user needs through judging
guidelines, training and monitoring. This paper introduces an alternate
approach for improving label quality. It takes careful feedback from real
users, which by definition is the highest-quality first-party gold data that
can be derived, and develops an large language model prompt that agrees with
that data.
</p>
<p>We present ideas and observations from deploying language models for
large-scale relevance labelling at Bing, and illustrate with data from TREC. We
have found large language models can be effective, with accuracy as good as
human labellers and similar capability to pick the hardest queries, best runs,
and best groups. Systematic changes to the prompts make a difference in
accuracy, but so too do simple paraphrases. To measure agreement with real
searchers needs high-quality ``gold'' labels, but with these we find that
models produce better labels than third-party workers, for a fraction of the
cost, and these labels let us train notably better rankers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.10654">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated great potential in natural
language processing tasks within the financial domain. In this work, we present
a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT,
which includes a dataset~(CFData) for pre-training and supervised fine-tuning,
a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment
framework~(CFAPP) designed to navigate real-world financial applications. The
CFData comprising both a pre-training dataset and a supervised fine-tuning
dataset, where the pre-training dataset collates Chinese financial data and
analytics, alongside a smaller subset of general-purpose text with 584M
documents and 141B tokens in total, and the supervised fine-tuning dataset is
tailored for six distinct financial tasks, embodying various facets of
financial analysis and decision-making with 1.5M instruction pairs and 1.5B
tokens in total. The CFLLM, which is based on InternLM-7B to balance the model
capability and size, is trained on CFData in two stage, continued pre-training
and supervised fine-tuning. The CFAPP is centered on large language models
(LLMs) and augmented with additional modules to ensure multifaceted
functionality in real-world application. Our codes are released at
https://github.com/TongjiFinLab/CFGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.10683">
<div class="article-summary-box-inner">
<span><p>Transfer learning, where a model is first pre-trained on a data-rich task
before being fine-tuned on a downstream task, has emerged as a powerful
technique in natural language processing (NLP). The effectiveness of transfer
learning has given rise to a diversity of approaches, methodology, and
practice. In this paper, we explore the landscape of transfer learning
techniques for NLP by introducing a unified framework that converts all
text-based language problems into a text-to-text format. Our systematic study
compares pre-training objectives, architectures, unlabeled data sets, transfer
approaches, and other factors on dozens of language understanding tasks. By
combining the insights from our exploration with scale and our new ``Colossal
Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks
covering summarization, question answering, text classification, and more. To
facilitate future work on transfer learning for NLP, we release our data set,
pre-trained models, and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04392">
<div class="article-summary-box-inner">
<span><p>Pretrained language models can be effectively stimulated by textual prompts
or demonstrations, especially in low-data scenarios. Recent works have focused
on automatically searching discrete or continuous prompts or optimized
verbalizers, yet studies for the demonstration are still limited. Concretely,
the demonstration examples are crucial for an excellent final performance of
prompt-tuning. In this paper, we propose a novel pluggable, extensible, and
efficient approach named contrastive demonstration tuning, which is free of
demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged
into any previous prompt-tuning approaches; (ii) Extended to widespread
classification tasks with a large number of categories. Experimental results on
16 datasets illustrate that our method integrated with previous approaches
LM-BFF and P-tuning can yield better performance. Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02355">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have contributed significantly to relation
extraction by demonstrating remarkable few-shot learning abilities. However,
prompt tuning methods for relation extraction may still fail to generalize to
those rare or hard patterns. Note that the previous parametric learning
paradigm can be viewed as memorization regarding training data as a book and
inference as the close-book test. Those long-tailed or hard patterns can hardly
be memorized in parameters given few-shot instances. To this end, we regard RE
as an open-book examination and propose a new semiparametric paradigm of
retrieval-enhanced prompt tuning for relation extraction. We construct an
open-book datastore for retrieval regarding prompt-based instance
representations and corresponding relation labels as memorized key-value pairs.
During inference, the model can infer relations by linearly interpolating the
base output of PLM with the non-parametric nearest neighbor distribution over
the datastore. In this way, our model not only infers relation through
knowledge stored in the weights during training but also assists
decision-making by unwinding and querying examples in the open-book datastore.
Extensive experiments on benchmark datasets show that our method can achieve
state-of-the-art in both standard supervised and few-shot settings. Code are
available in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14704">
<div class="article-summary-box-inner">
<span><p>Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation. (arXiv:2209.08738v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.08738">
<div class="article-summary-box-inner">
<span><p>K-Nearest Neighbor Neural Machine Translation (kNN-MT) successfully
incorporates external corpus by retrieving word-level representations at test
time. Generally, kNN-MT borrows the off-the-shelf context representation in the
translation task, e.g., the output of the last decoder layer, as the query
vector of the retrieval task. In this work, we highlight that coupling the
representations of these two tasks is sub-optimal for fine-grained retrieval.
To alleviate it, we leverage supervised contrastive learning to learn the
distinctive retrieval representation derived from the original context
representation. We also propose a fast and effective approach to constructing
hard negative samples. Experimental results on five domains show that our
approach improves the retrieval accuracy and BLEU score compared to vanilla
kNN-MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.00038">
<div class="article-summary-box-inner">
<span><p>Differentially private (DP) optimization is the standard paradigm to learn
large neural networks that are accurate and privacy-preserving. The
computational cost for DP deep learning, however, is notoriously heavy due to
the per-sample gradient clipping. Existing DP implementations are 2-1000X more
costly in time and space complexity than the standard (non-private) training.
In this work, we develop a novel Book-Keeping (BK) technique that implements
existing DP optimizers (thus achieving the same accuracy), with a substantial
improvement on the computational cost. Specifically, BK enables DP training on
large models and high dimensional data to be roughly as fast and memory-saving
as the standard training, whereas previous DP algorithms can be inefficient or
incapable of training due to memory error. The computational advantage of BK is
supported by the complexity analysis as well as extensive experiments on vision
and language tasks. Our implementation achieves state-of-the-art (SOTA)
accuracy with very small extra cost: on GPT2 and at almost the same memory cost
(&lt;1% overhead), BK has 1.03X the time complexity of the standard training
(0.83X training speed in practice), and 0.61X the time complexity of the most
efficient DP implementation (1.36X training speed in practice). We open-source
the codebase for the BK algorithm at the FastDP library
(https://github.com/awslabs/fast-differential-privacy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Study Incorporating Linguistic Knowledge on Filled Pauses for Personalized Spontaneous Speech Synthesis. (arXiv:2210.07559v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07559">
<div class="article-summary-box-inner">
<span><p>We present a comprehensive empirical study for personalized spontaneous
speech synthesis on the basis of linguistic knowledge. With the advent of voice
cloning for reading-style speech synthesis, a new voice cloning paradigm for
human-like and spontaneous speech synthesis is required. We, therefore, focus
on personalized spontaneous speech synthesis that can clone both the
individual's voice timbre and speech disfluency. Specifically, we deal with
filled pauses, a major source of speech disfluency, which is known to play an
important role in speech generation and communication in psychology and
linguistics. To comparatively evaluate personalized filled pause insertion and
non-personalized filled pause prediction methods, we developed a speech
synthesis method with a non-personalized external filled pause predictor
trained with a multi-speaker corpus. The results clarify the position-word
entanglement of filled pauses, i.e., the necessity of precisely predicting
positions for naturalness and the necessity of precisely predicting words for
individuality on the evaluation of synthesized speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Automated Machine Translation to Educational Video Courses. (arXiv:2301.03141v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03141">
<div class="article-summary-box-inner">
<span><p>We studied the capability of automated machine translation in the online
video education space by automatically translating Khan Academy videos with
state-of-the-art translation models and applying text-to-speech synthesis and
audio/video synchronization to build engaging videos in target languages. We
also analyzed and established two reliable translation confidence estimators
based on round-trip translations in order to efficiently manage translation
quality and reduce human translation effort. Finally, we developed a deployable
system to deliver translated videos to end users and collect user corrections
for iterative improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reliable Neural Machine Translation with Consistency-Aware Meta-Learning. (arXiv:2303.10966v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10966">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) has achieved remarkable success in producing
high-quality translations. However, current NMT systems suffer from a lack of
reliability, as their outputs that are often affected by lexical or syntactic
changes in inputs, resulting in large variations in quality. This limitation
hinders the practicality and trustworthiness of NMT. A contributing factor to
this problem is that NMT models trained with the one-to-one paradigm struggle
to handle the source diversity phenomenon, where inputs with the same meaning
can be expressed differently. In this work, we treat this problem as a bilevel
optimization problem and present a consistency-aware meta-learning (CAML)
framework derived from the model-agnostic meta-learning (MAML) algorithm to
address it. Specifically, the NMT model with CAML (named CoNMT) first learns a
consistent meta representation of semantically equivalent sentences in the
outer loop. Subsequently, a mapping from the meta representation to the output
sentence is learned in the inner loop, allowing the NMT model to translate
semantically equivalent sentences to the same target sentence. We conduct
experiments on the NIST Chinese to English task, three WMT translation tasks,
and the TED M2O task. The results demonstrate that CoNMT effectively improves
overall translation quality and reliably handles diverse inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15078">
<div class="article-summary-box-inner">
<span><p>Text summarization has a wide range of applications in many scenarios. The
evaluation of the quality of the generated text is a complex problem. A big
challenge to language evaluation is that there is a clear divergence between
existing metrics and human evaluation. A document summary's quality can be
assessed by human annotators on various criteria, both objective ones like
grammar and correctness, and subjective ones like informativeness,
succinctness, and appeal. Most of the automatic evaluation methods like
BLUE/ROUGE may be not able to adequately capture the above dimensions. In this
paper, we propose a new evaluation framework based on LLMs, which provides a
comprehensive evaluation framework by comparing generated text and reference
text from both objective and subjective aspects. First, we propose to model
objective and subjective dimensions of generated text based on roleplayers
prompting mechanism. Furthermore, we introduce a context-based prompting
mechanism that is able to generate dynamic roleplayer profiles based on input
context. Finally, we design a multi-roleplayer prompting technology based on
batch prompting and integrate multiple outputs into the final evaluation
results. Experimental results on three real datasets for summarization show
that our model is highly competitive and has a very high consistency with human
annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03513">
<div class="article-summary-box-inner">
<span><p>ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Speaking Styles Using a Large Language Model. (arXiv:2305.10321v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10321">
<div class="article-summary-box-inner">
<span><p>Reference-based Text-to-Speech (TTS) models can generate multiple,
prosodically-different renditions of the same target text. Such models jointly
learn a latent acoustic space during training, which can be sampled from during
inference. Controlling these models during inference typically requires finding
an appropriate reference utterance, which is non-trivial.
</p>
<p>Large generative language models (LLMs) have shown excellent performance in
various language-related tasks. Given only a natural language query text (the
prompt), such models can be used to solve specific, context-dependent tasks.
Recent work in TTS has attempted similar prompt-based control of novel speaking
style generation. Those methods do not require a reference utterance and can,
under ideal conditions, be controlled with only a prompt. But existing methods
typically require a prompt-labelled speech corpus for jointly training a
prompt-conditioned encoder.
</p>
<p>In contrast, we instead employ an LLM to directly suggest prosodic
modifications for a controllable TTS model, using contextual information
provided in the prompt. The prompt can be designed for a multitude of tasks.
Here, we give two demonstrations: control of speaking style; prosody
appropriate for a given dialogue context. The proposed method is rated most
appropriate in 50% of cases vs. 31% for a baseline model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v4 [q-fin.ST] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03763">
<div class="article-summary-box-inner">
<span><p>ChatGPT has demonstrated remarkable capabilities across various natural
language processing (NLP) tasks. However, its potential for inferring dynamic
network structures from temporal textual data, specifically financial news,
remains an unexplored frontier. In this research, we introduce a novel
framework that leverages ChatGPT's graph inference capabilities to enhance
Graph Neural Networks (GNN). Our framework adeptly extracts evolving network
structures from textual data, and incorporates these networks into graph neural
networks for subsequent predictive tasks. The experimental results from stock
movement forecasting indicate our model has consistently outperformed the
state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios
constructed based on our model's outputs demonstrate higher annualized
cumulative returns, alongside reduced volatility and maximum drawdown. This
superior performance highlights the potential of ChatGPT for text-based network
inferences and underscores its promising implications for the financial sector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Large Enterprise Language Models via Ontological Reasoning. (arXiv:2306.10723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.10723">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to
diverse goals, thanks to task-specific training data. Task specificity should
go hand in hand with domain orientation, that is, the specialization of an LLM
to accurately address the tasks of a given realm of interest. However, models
are usually fine-tuned over publicly available data or, at most, over ground
data from databases, ignoring business-level definitions and domain experience.
On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and
augment such domain knowledge via ontological reasoning. With the goal of
combining LLM flexibility with the domain orientation of EKGs, we propose a
novel neurosymbolic architecture that leverages the power of ontological
reasoning to build task- and domain-specific corpora for LLM fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06985">
<div class="article-summary-box-inner">
<span><p>Aiming to populate generalizable engineering design knowledge, we propose a
method to extract facts of the form &lt;head entity, relationship, tail entity&gt;
from sentences found in patent documents. These facts could be combined within
and across patent documents to form knowledge graphs that serve as schemes for
representing as well as storing design knowledge. Existing methods in
engineering design literature often utilise a set of predefined relationships
to populate triples that are statistical approximations rather than facts. In
our method, we train a tagger to identify both entities and relationships from
a sentence. Given a pair of entities, we train another tagger to identify the
specific relationship tokens. For training these taggers, we manually construct
a dataset of 44,227 sentences and corresponding facts. We benchmark our method
against two typically recommended approaches. We apply our method by extracting
facts from sentences found in patents related to fan systems. We build a
knowledge base using these facts to demonstrate how domain ontologies could be
constructed and contextualised knowledge of subsystems could be visualised. We
then search the knowledge base for key issues prevailing in fan systems. We
organize the responses into knowledge graphs and hold a comparative discussion
against the opinions about the key issues from ChatGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07697">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have made significant strides in various tasks,
yet they often struggle with complex reasoning and exhibit poor performance in
scenarios where knowledge traceability, timeliness, and accuracy are crucial.
To address these limitations, we present Think-on-Graph (ToG), a novel
framework that leverages knowledge graphs to enhance LLMs' ability for deep and
responsible reasoning. By employing ToG, we can identify entities relevant to a
given question and conduct exploration and reasoning to retrieve related
triples from an external knowledge database. This iterative procedure generates
multiple reasoning pathways consisting of sequentially connected triplets until
sufficient information is gathered to answer the question or the maximum depth
is reached. Through experiments on complex multi-hop reasoning
question-answering tasks, we demonstrate that ToG outperforms existing methods,
effectively addressing the aforementioned limitations of LLMs without incurring
additional training costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04306">
<div class="article-summary-box-inner">
<span><p>The history of metaphor research also marks the evolution of knowledge
infusion research. With the continued advancement of deep learning techniques
in recent years, the natural language processing community has shown great
interest in applying knowledge to successful results in metaphor recognition
tasks. Although there has been a gradual increase in the number of approaches
involving knowledge injection in the field of metaphor recognition, there is a
lack of a complete review article on knowledge injection based approaches.
Therefore, the goal of this paper is to provide a comprehensive review of
research advances in the application of deep learning for knowledge injection
in metaphor recognition tasks. In this paper, we systematically summarize and
generalize the mainstream knowledge and knowledge injection principles, as well
as review the datasets, evaluation metrics, and benchmark models used in
metaphor recognition tasks. Finally, we explore the current issues facing
knowledge injection methods and provide an outlook on future research
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Is Not All You Need Anymore. (arXiv:2308.07661v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07661">
<div class="article-summary-box-inner">
<span><p>In recent years, the popular Transformer architecture has achieved great
success in many application areas, including natural language processing and
computer vision. Many existing works aim to reduce the computational and memory
complexity of the self-attention mechanism in the Transformer by trading off
performance. However, performance is key for the continuing success of the
Transformer. In this paper, a family of drop-in replacements for the
self-attention mechanism in the Transformer, called the Extractors, is
proposed. Four types of the Extractors, namely the super high-performance
Extractor (SHE), the higher-performance Extractor (HE), the worthwhile
Extractor (WE), and the minimalist Extractor (ME), are proposed as examples.
Experimental results show that replacing the self-attention mechanism with the
SHE evidently improves the performance of the Transformer, whereas the
simplified versions of the SHE, i.e., the HE, the WE, and the ME, perform close
to or better than the self-attention mechanism with less computational and
memory complexity. Furthermore, the proposed Extractors have the potential or
are able to run faster than the self-attention mechanism since their critical
paths of computation are much shorter. Additionally, the sequence prediction
problem in the context of text generation is formulated using variable-length
discrete-time Markov chains, and the Transformer is reviewed based on our
understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15126">
<div class="article-summary-box-inner">
<span><p>Large Vision-Language Models (LVLMs) have recently achieved remarkable
success. However, LVLMs are still plagued by the hallucination problem, which
limits the practicality in many scenarios. Hallucination refers to the
information of LVLMs' responses that does not exist in the visual input, which
poses potential risks of substantial consequences. There has been limited work
studying hallucination evaluation in LVLMs. In this paper, we propose
Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based
hallucination evaluation framework. HaELM achieves an approximate 95%
performance comparable to ChatGPT and has additional advantages including low
cost, reproducibility, privacy preservation and local deployment. Leveraging
the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we
analyze the factors contributing to hallucination in LVLMs and offer helpful
suggestions to mitigate the hallucination problem. Our training data and human
annotation hallucination data will be made public soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05007">
<div class="article-summary-box-inner">
<span><p>Humans ask follow-up questions driven by curiosity, which reflects a creative
human cognitive process. We introduce the task of real-world
information-seeking follow-up question generation (FQG), which aims to generate
follow-up questions seeking a more in-depth understanding of an initial
question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world
(initial question, answer, follow-up question) tuples collected from a Reddit
forum providing layman-friendly explanations for open-ended questions. In
contrast to existing datasets, questions in FOLLOWUPQG use more diverse
pragmatic strategies to seek information, and they also show higher-order
cognitive skills (such as applying and relating). We evaluate current question
generation models on their efficacy for generating follow-up questions,
exploring how to generate specific types of follow-up questions based on
step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging
benchmark, as model-generated questions are adequate but far from human-raised
questions in terms of informativeness and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05557">
<div class="article-summary-box-inner">
<span><p>Nowadays, the versatile capabilities of Pre-trained Large Language Models
(LLMs) have attracted much attention from the industry. However, some vertical
domains are more interested in the in-domain capabilities of LLMs. For the
Networks domain, we present NetEval, an evaluation set for measuring the
comprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is
designed for evaluating the commonsense knowledge and inference ability in
NetOps in a multi-lingual context. NetEval consists of 5,732 questions about
NetOps, covering five different sub-domains of NetOps. With NetEval, we
systematically evaluate the NetOps capability of 26 publicly available LLMs.
The results show that only GPT-4 can achieve a performance competitive to
humans. However, some open models like LLaMA 2 demonstrate significant
potential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models. (arXiv:2309.06085v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06085">
<div class="article-summary-box-inner">
<span><p>The rapid development of Large Language Models (LLMs) and the emergence of
novel abilities with scale have necessitated the construction of holistic,
diverse and challenging benchmarks such as HELM and BIG-bench. However, at the
moment, most of these benchmarks focus only on performance in English and
evaluations that include Southeast Asian (SEA) languages are few in number. We
therefore propose BHASA, a holistic linguistic and cultural evaluation suite
for LLMs in SEA languages. It comprises three components: (1) a NLP benchmark
covering eight tasks across Natural Language Understanding (NLU), Generation
(NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit
that spans the gamut of linguistic phenomena including syntax, semantics and
pragmatics, and (3) a cultural diagnostics dataset that probes for both
cultural representation and sensitivity. For this preliminary effort, we
implement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil,
and we only include Indonesian and Tamil for LINDSEA and the cultural
diagnostics dataset. As GPT-4 is purportedly one of the best-performing
multilingual LLMs at the moment, we use it as a yardstick to gauge the
capabilities of LLMs in the context of SEA languages. Our initial experiments
on GPT-4 with BHASA find it lacking in various aspects of linguistic
capabilities, cultural representation and sensitivity in the targeted SEA
languages. BHASA is a work in progress and will continue to be improved and
expanded in the future. The repository for this paper can be found at:
https://github.com/aisingapore/BHASA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07315">
<div class="article-summary-box-inner">
<span><p>Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07864">
<div class="article-summary-box-inner">
<span><p>For a long time, humanity has pursued artificial intelligence (AI) equivalent
to or surpassing the human level, with AI agents considered a promising vehicle
for this pursuit. AI agents are artificial entities that sense their
environment, make decisions, and take actions. Many efforts have been made to
develop intelligent agents, but they mainly focus on advancement in algorithms
or training strategies to enhance specific capabilities or performance on
particular tasks. Actually, what the community lacks is a general and powerful
model to serve as a starting point for designing AI agents that can adapt to
diverse scenarios. Due to the versatile capabilities they demonstrate, large
language models (LLMs) are regarded as potential sparks for Artificial General
Intelligence (AGI), offering hope for building general AI agents. Many
researchers have leveraged LLMs as the foundation to build AI agents and have
achieved significant progress. In this paper, we perform a comprehensive survey
on LLM-based agents. We start by tracing the concept of agents from its
philosophical origins to its development in AI, and explain why LLMs are
suitable foundations for agents. Building upon this, we present a general
framework for LLM-based agents, comprising three main components: brain,
perception, and action, and the framework can be tailored for different
applications. Subsequently, we explore the extensive applications of LLM-based
agents in three aspects: single-agent scenarios, multi-agent scenarios, and
human-agent cooperation. Following this, we delve into agent societies,
exploring the behavior and personality of LLM-based agents, the social
phenomena that emerge from an agent society, and the insights they offer for
human society. Finally, we discuss several key topics and open problems within
the field. A repository for the related papers at
https://github.com/WooooDyy/LLM-Agent-Paper-List.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08600">
<div class="article-summary-box-inner">
<span><p>One of the roadblocks to a better understanding of neural networks' internals
is \textit{polysemanticity}, where neurons appear to activate in multiple,
semantically distinct contexts. Polysemanticity prevents us from identifying
concise, human-understandable explanations for what neural networks are doing
internally. One hypothesised cause of polysemanticity is
\textit{superposition}, where neural networks represent more features than they
have neurons by assigning features to an overcomplete set of directions in
activation space, rather than to individual neurons. Here, we attempt to
identify those directions, using sparse autoencoders to reconstruct the
internal activations of a language model. These autoencoders learn sets of
sparsely activating features that are more interpretable and monosemantic than
directions identified by alternative approaches, where interpretability is
measured by automated methods. Ablating these features enables precise model
editing, for example, by removing capabilities such as pronoun prediction,
while disrupting model behaviour less than prior techniques. This work
indicates that it is possible to resolve superposition in language models using
a scalable, unsupervised method. Our method may serve as a foundation for
future mechanistic interpretability work, which we hope will enable greater
model transparency and steerability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Media of Langue. (arXiv:2309.08609v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08609">
<div class="article-summary-box-inner">
<span><p>This paper aims to archive the materials behind "Media of Langue" by Goki
Muramoto et al. Media of Langue is a new dictionary and public sculpture that
depicts the map of meaning on the boundary between languages solely from the
vast events of "this word was translated into that word" and two forces:
repulsion between all words in the same language and attraction between
translated words in different languages. First, the three new concepts
proposed, Inter-Langue Map/Dictionary, Inter-Langue Space, and then
Inter-Langue Network, are introduced, comparing them to the three domains of
dictionary, semantic space, and semantic network. The specific algorithms and
designs implemented in the work are then described.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild. (arXiv:2309.08637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08637">
<div class="article-summary-box-inner">
<span><p>Large language models with instruction-following abilities have
revolutionized the field of artificial intelligence. These models show
exceptional generalizability to tackle various real-world tasks through their
natural language interfaces. However, their performance heavily relies on
high-quality exemplar data, which is often difficult to obtain. This challenge
is further exacerbated when it comes to multimodal instruction following. We
introduce TextBind, an almost annotation-free framework for empowering larger
language models with the multi-turn interleaved multimodal
instruction-following capabilities. Our approach requires only image-caption
pairs and generates multi-turn multimodal instruction-response conversations
from a language model. To accommodate interleaved image-text inputs and
outputs, we devise MIM, a language model-centric architecture that seamlessly
integrates image encoder and decoder models. We release our dataset, model, and
demo to foster future research in the area of multimodal instruction following.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?. (arXiv:2309.08963v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08963">
<div class="article-summary-box-inner">
<span><p>Despite the power of Large Language Models (LLMs) like GPT-4, they still
struggle with tasks that require generating complex, structured outputs. In
this study, we assess the capability of Current LLMs in generating complex
structured data and propose a structure-aware fine-tuning approach as a
solution to improve this ability. To perform a comprehensive evaluation, we
propose Struc-Bench, include five representative LLMs (i.e., GPT-NeoX 20B,
GPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed
datasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of
current model performance, we identify specific common formatting errors and
areas of potential improvement. To address complex formatting requirements, we
utilize FormatCoT (Chain-of-Thought) to generate format instructions from
target outputs. Our experiments show that our structure-aware fine-tuning
method, when applied to LLaMA-7B, significantly improves adherence to natural
language constraints, outperforming other evaluated LLMs. Based on these
results, we present an ability map of model capabilities from six dimensions
(i.e., coverage, formatting, reasoning, comprehension, pragmatics, and
hallucination). This map highlights the weaknesses of LLMs in handling complex
structured outputs and suggests promising directions for future work. Our code
and models can be found at https://github.com/gersteinlab/Struc-Bench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter. (arXiv:2309.09443v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09443">
<div class="article-summary-box-inner">
<span><p>Multilingual intelligent assistants, such as ChatGPT, have recently gained
popularity. To further expand the applications of multilingual artificial
intelligence assistants and facilitate international communication, it is
essential to enhance the performance of multilingual speech recognition, which
is a crucial component of speech interaction. In this paper, we propose two
simple and parameter-efficient methods: language prompt tuning and frame-level
language adapter, to respectively enhance language-configurable and
language-agnostic multilingual speech recognition. Additionally, we explore the
feasibility of integrating these two approaches using parameter-efficient
fine-tuning methods. Our experiments demonstrate significant performance
improvements across seven languages using our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models. (arXiv:2309.09506v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09506">
<div class="article-summary-box-inner">
<span><p>Graphic layout generation, a growing research field, plays a significant role
in user engagement and information perception. Existing methods primarily treat
layout generation as a numerical optimization task, focusing on quantitative
aspects while overlooking the semantic information of layout, such as the
relationship between each layout element. In this paper, we propose LayoutNUWA,
the first model that treats layout generation as a code generation task to
enhance semantic information and harness the hidden layout expertise of large
language models~(LLMs). More concretely, we develop a Code Instruct Tuning
(CIT) approach comprising three interconnected modules: 1) the Code
Initialization (CI) module quantifies the numerical conditions and initializes
them as HTML code with strategically placed masks; 2) the Code Completion (CC)
module employs the formatting knowledge of LLMs to fill in the masked portions
within the HTML code; 3) the Code Rendering (CR) module transforms the
completed code into the final layout output, ensuring a highly interpretable
and transparent layout generation procedure that directly maps code to a
visualized layout. We attain significant state-of-the-art performance (even
over 50\% improvements) on multiple datasets, showcasing the strong
capabilities of LayoutNUWA. Our code is available at
https://github.com/ProjectNUWA/LayoutNUWA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models. (arXiv:2309.09708v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09708">
<div class="article-summary-box-inner">
<span><p>Automated occupation extraction and standardization from free-text job
postings and resumes are crucial for applications like job recommendation and
labor market policy formation. This paper introduces LLM4Jobs, a novel
unsupervised methodology that taps into the capabilities of large language
models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the
natural language understanding and generation capacities of LLMs. Evaluated on
rigorous experimentation on synthetic and real-world datasets, we demonstrate
that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks,
demonstrating its versatility across diverse datasets and granularities. As a
side result of our work, we present both synthetic and real-world datasets,
which may be instrumental for subsequent research in this domain. Overall, this
investigation highlights the promise of contemporary LLMs for the intricate
task of occupation extraction and standardization, laying the foundation for a
robust and adaptable framework relevant to both research and industrial
contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation. (arXiv:2309.09749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09749">
<div class="article-summary-box-inner">
<span><p>NSFW (Not Safe for Work) content, in the context of a dialogue, can have
severe side effects on users in open-domain dialogue systems. However, research
on detecting NSFW language, especially sexually explicit content, within a
dialogue context has significantly lagged behind. To address this issue, we
introduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue
detection. Leveraging knowledge distillation techniques involving GPT-4 and
ChatGPT, this dataset offers a cost-effective means of constructing NSFW
content detectors. The process entails collecting real-life human-machine
interaction data and breaking it down into single utterances and single-turn
dialogues, with the chatbot delivering the final utterance. ChatGPT is employed
to annotate unlabeled data, serving as a training set. Rationale validation and
test sets are constructed using ChatGPT and GPT-4 as annotators, with a
self-criticism strategy for resolving discrepancies in labeling. A BERT model
is fine-tuned as a text classifier on pseudo-labeled data, and its performance
is assessed. The study emphasizes the importance of AI systems prioritizing
user safety and well-being in digital conversations while respecting freedom of
expression. The proposed approach not only advances NSFW content detection but
also aligns with evolving user protection needs in AI-driven dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion Recognition in Conversation With Emotion Disentanglement. (arXiv:2309.09799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09799">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversation (ERC) has attracted widespread attention
in the natural language processing field due to its enormous potential for
practical applications. Existing ERC methods face challenges in achieving
generalization to diverse scenarios due to insufficient modeling of context,
ambiguous capture of dialogue relationships and overfitting in speaker
modeling. In this work, we present a Hybrid Continuous Attributive Network
(HCAN) to address these issues in the perspective of emotional continuation and
emotional attribution. Specifically, HCAN adopts a hybrid recurrent and
attention-based module to model global emotion continuity. Then a novel
Emotional Attribution Encoding (EAE) is proposed to model intra- and
inter-emotional attribution for each utterance. Moreover, aiming to enhance the
robustness of the model in speaker modeling and improve its performance in
different scenarios, A comprehensive loss function emotional cognitive loss
$\mathcal{L}_{\rm EC}$ is proposed to alleviate emotional drift and overcome
the overfitting of the model to speaker modeling. Our model achieves
state-of-the-art performance on three datasets, demonstrating the superiority
of our work. Another extensive comparative experiments and ablation studies on
three benchmarks are conducted to provided evidence to support the efficacy of
each module. Further exploration of generalization ability experiments shows
the plug-and-play nature of the EAE module in our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HypR: A comprehensive study for ASR hypothesis revising with a reference corpus. (arXiv:2309.09838v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09838">
<div class="article-summary-box-inner">
<span><p>With the development of deep learning, automatic speech recognition (ASR) has
made significant progress. To further enhance the performance, revising
recognition results is one of the lightweight but efficient manners. Various
methods can be roughly classified into N-best reranking methods and error
correction models. The former aims to select the hypothesis with the lowest
error rate from a set of candidates generated by ASR for a given input speech.
The latter focuses on detecting recognition errors in a given hypothesis and
correcting these errors to obtain an enhanced result. However, we observe that
these studies are hardly comparable to each other as they are usually evaluated
on different corpora, paired with different ASR models, and even use different
datasets to train the models. Accordingly, we first concentrate on releasing an
ASR hypothesis revising (HypR) dataset in this study. HypR contains several
commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provides 50
recognition hypotheses for each speech utterance. The checkpoint models of the
ASR are also published. In addition, we implement and compare several classic
and representative methods, showing the recent research progress in revising
speech recognition results. We hope the publicly available HypR dataset can
become a reference benchmark for subsequent research and promote the school of
research to an advanced level.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-20 23:11:00.554185442 UTC">2023-09-20 23:11:00 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>