<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-30T01:30:00Z">01-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems. (arXiv:2301.11333v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11333">
<div class="article-summary-box-inner">
<span><p>The dazzling promises of AI systems to augment humans in various tasks hinge
on whether humans can appropriately rely on them. Recent research has shown
that appropriate reliance is the key to achieving complementary team
performance in AI-assisted decision making. This paper addresses an
under-explored problem of whether the Dunning-Kruger Effect (DKE) among people
can hinder their appropriate reliance on AI systems. DKE is a metacognitive
bias due to which less-competent individuals overestimate their own skill and
performance. Through an empirical study (N = 249), we explored the impact of
DKE on human reliance on an AI system, and whether such effects can be
mitigated using a tutorial intervention that reveals the fallibility of AI
advice, and exploiting logic units-based explanations to improve user
understanding of AI advice. We found that participants who overestimate their
performance tend to exhibit under-reliance on AI systems, which hinders optimal
team performance. Logic units-based explanations did not help users in either
improving the calibration of their competence or facilitating appropriate
reliance. While the tutorial intervention was highly effective in helping users
calibrate their self-assessment and facilitating appropriate reliance among
participants with overestimated self-assessment, we found that it can
potentially hurt the appropriate reliance of participants with underestimated
self-assessment. Our work has broad implications on the design of methods to
tackle user cognitive biases while facilitating appropriate reliance on AI
systems. Our findings advance the current understanding of the role of
self-assessment in shaping trust and reliance in human-AI decision making. This
lays out promising future directions for relevant HCI research in this
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using novel data and ensemble models to improve automated labeling of Sustainable Development Goals. (arXiv:2301.11353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11353">
<div class="article-summary-box-inner">
<span><p>A number of labeling systems based on text have been proposed to help monitor
work on the United Nations (UN) Sustainable Development Goals (SDGs). Here, we
present a systematic comparison of systems using a variety of text sources and
show that systems differ considerably in their specificity (i.e., true-positive
rate) and sensitivity (i.e., true-negative rate), have systematic biases (e.g.,
are more sensitive to specific SDGs relative to others), and are susceptible to
the type and amount of text analyzed. We then show that an ensemble model that
pools labeling systems alleviates some of these limitations, exceeding the
labeling performance of all currently available systems. We conclude that
researchers and policymakers should care about the choice of labeling system
and that ensemble methods should be favored when drawing conclusions about the
absolute and relative prevalence of work on the SDGs based on automated
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Event Transformer for Image-guided Story Ending Generation. (arXiv:2301.11357v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11357">
<div class="article-summary-box-inner">
<span><p>Image-guided story ending generation (IgSEG) is to generate a story ending
based on given story plots and ending image. Existing methods focus on
cross-modal feature fusion but overlook reasoning and mining implicit
information from story plots and ending image. To tackle this drawback, we
propose a multimodal event transformer, an event-based reasoning framework for
IgSEG. Specifically, we construct visual and semantic event graphs from story
plots and ending image, and leverage event-based reasoning to reason and mine
implicit information in a single modality. Next, we connect visual and semantic
event graphs and utilize cross-modal fusion to integrate different-modality
features. In addition, we propose a multimodal injector to adaptive pass
essential information to decoder. Besides, we present an incoherence detection
to enhance the understanding context of a story plot and the robustness of
graph modeling for our model. Experimental results show that our method
achieves state-of-the-art performance for the image-guided story ending
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Cross-modal Alignment for Text-Guided Image Inpainting. (arXiv:2301.11362v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11362">
<div class="article-summary-box-inner">
<span><p>Text-guided image inpainting (TGII) aims to restore missing regions based on
a given text in a damaged image. Existing methods are based on a strong vision
encoder and a cross-modal fusion model to integrate cross-modal features.
However, these methods allocate most of the computation to visual encoding,
while light computation on modeling modality interactions. Moreover, they take
cross-modal fusion for depth features, which ignores a fine-grained alignment
between text and image. Recently, vision-language pre-trained models (VLPM),
encapsulating rich cross-modal alignment knowledge, have advanced in most
multimodal tasks. In this work, we propose a novel model for TGII by improving
cross-modal alignment (CMA). CMA model consists of a VLPM as a vision-language
encoder, an image generator and global-local discriminators. To explore
cross-modal alignment knowledge for image restoration, we introduce cross-modal
alignment distillation and in-sample distribution distillation. In addition, we
employ adversarial training to enhance the model to fill the missing region in
complicated structures effectively. Experiments are conducted on two popular
vision-language datasets. Results show that our model achieves state-of-the-art
performance compared with other strong competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Aware Contrastive Learning for Multi-Style Image Captioning. (arXiv:2301.11367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11367">
<div class="article-summary-box-inner">
<span><p>Existing multi-style image captioning methods show promising results in
generating a caption with accurate visual content and desired linguistic style.
However, existing methods overlook the relationship between linguistic style
and visual content. To overcome this drawback, we propose style-aware
contrastive learning for multi-style image captioning. First, we present a
style-aware visual encoder with contrastive learning to mine potential visual
content relevant to style. Moreover, we propose a style-aware triplet contrast
objective to distinguish whether the image, style and caption matched. To
provide positive and negative samples for contrastive learning, we present
three retrieval schemes: object-based retrieval, RoI-based retrieval and
triplet-based retrieval, and design a dynamic trade-off function to calculate
retrieval scores. Experimental results demonstrate that our approach achieves
state-of-the-art performance. In addition, we conduct an extensive analysis to
verify the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task formulation for Extracting Social Determinants of Health from Clinical Narratives. (arXiv:2301.11386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11386">
<div class="article-summary-box-inner">
<span><p>Objective: The 2022 n2c2 NLP Challenge posed identification of social
determinants of health (SDOH) in clinical narratives. We present three systems
that we developed for the Challenge and discuss the distinctive task
formulation used in each of the three systems. Materials and Methods: The first
system identifies target pieces of information independently using machine
learning classifiers. The second system uses a large language model (LLM) to
extract complete structured outputs per document. The third system extracts
candidate phrases using machine learning and identifies target relations with
hand-crafted rules. Results: The three systems achieved F1 scores of 0.884,
0.831, and 0.663 in the Subtask A of the Challenge, which are ranked third,
seventh, and eighth among the 15 participating teams. The review of the
extraction results from our systems reveals characteristics of each approach
and those of the SODH extraction task. Discussion: Phrases and relations
annotated in the task is unique and diverse, not conforming to the conventional
event extraction task. These annotations are difficult to model with limited
training data. The system that extracts information independently, ignoring the
annotated relations, achieves the highest F1 score. Meanwhile, LLM with its
versatile capability achieves the high F1 score, while respecting the annotated
relations. The rule-based system tackling relation extraction obtains the low
F1 score, while it is the most explainable approach. Conclusion: The F1 scores
of the three systems vary in this challenge setting, but each approach has
advantages and disadvantages in a practical application. The selection of the
approach depends not only on the F1 score but also on the requirements in the
application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Pump&Dump Stock Market Manipulation from Online Forums. (arXiv:2301.11403v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11403">
<div class="article-summary-box-inner">
<span><p>The intersection of social media, low-cost trading platforms, and naive
investors has created an ideal situation for information-based market
manipulations, especially pump&amp;dumps. Manipulators accumulate small-cap stocks,
disseminate false information on social media to inflate their price, and sell
at the peak. We collect a dataset of stocks whose price and volume profiles
have the characteristic shape of a pump&amp;dump, and social media posts for those
same stocks that match the timing of the initial price rises. From these we
build predictive models for pump&amp;dump events based on the language used in the
social media posts.
</p>
<p>There are multiple difficulties: not every post will cause the intended
market reaction, some pump&amp;dump events may be triggered by posts in other
forums, and there may be accidental confluences of post timing and market
movements. Nevertheless, our best model achieves a prediction accuracy of 85%
and an F1-score of 62%. Such a tool can provide early warning to investors and
regulators that a pump&amp;dump may be underway.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Arabic: Software for Perso-Arabic Script Manipulation. (arXiv:2301.11406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11406">
<div class="article-summary-box-inner">
<span><p>This paper presents an open-source software library that provides a set of
finite-state transducer (FST) components and corresponding utilities for
manipulating the writing systems of languages that use the Perso-Arabic script.
The operations include various levels of script normalization, including visual
invariance-preserving operations that subsume and go beyond the standard
Unicode normalization forms, as well as transformations that modify the visual
appearance of characters in accordance with the regional orthographies for
eleven contemporary languages from diverse language families. The library also
provides simple FST-based romanization and transliteration. We additionally
attempt to formalize the typology of Perso-Arabic characters by providing
one-to-many mappings from Unicode code points to the languages that use them.
While our work focuses on the Arabic script diaspora rather than Arabic itself,
this approach could be adopted for any language that uses the Arabic script,
thus providing a unified framework for treating a script family used by close
to a billion people.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification. (arXiv:2301.11459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11459">
<div class="article-summary-box-inner">
<span><p>Pre-trained seq2seq models excel at graph semantic parsing with rich
annotated data, but generalize worse to out-of-distribution (OOD) and long-tail
examples. In comparison, symbolic parsers under-perform on population-level
metrics, but exhibit unique strength in OOD and tail generalization. In this
work, we study compositionality-aware approach to neural-symbolic inference
informed by model confidence, performing fine-grained neural-symbolic reasoning
at subgraph level (i.e., nodes and edges) and precisely targeting subgraph
components with high uncertainty in the neural parser. As a result, the method
combines the distinct strength of the neural and symbolic approaches in
capturing different aspects of the graph prediction, leading to well-rounded
generalization performance both across domains and in the tail. We empirically
investigate the approach in the English Resource Grammar (ERG) parsing problem
on a diverse suite of standard in-domain and seven OOD corpora. Our approach
leads to 35.26% and 35.60% error reduction in aggregated Smatch score over
neural and symbolic approaches respectively, and 14% absolute accuracy gain in
key tail linguistic categories over the neural model, outperforming prior
state-of-art methods that do not account for compositionality or uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech. (arXiv:2301.11462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11462">
<div class="article-summary-box-inner">
<span><p>When acquiring syntax, children consistently choose hierarchical rules over
competing non-hierarchical possibilities. Is this preference due to a learning
bias for hierarchical structure, or due to more general biases that interact
with hierarchical cues in children's linguistic input? We explore these
possibilities by training LSTMs and Transformers - two types of neural networks
without a hierarchical bias - on data similar in quantity and content to
children's linguistic input: text from the CHILDES corpus. We then evaluate
what these models have learned about English yes/no questions, a phenomenon for
which hierarchical structure is crucial. We find that, though they perform well
at capturing the surface statistics of child-directed speech (as measured by
perplexity), both model types generalize in a way more consistent with an
incorrect linear rule than the correct hierarchical rule. These results suggest
that human-like generalization from text alone requires stronger biases than
the general sequence-processing biases of standard neural network
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models. (arXiv:2301.11489v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11489">
<div class="article-summary-box-inner">
<span><p>Conversational recommendation systems (CRSs) enable users to use natural
language feedback to control their recommendations, overcoming many of the
challenges of traditional recommendation systems. However, the practical
adoption of CRSs remains limited due to a lack of rich and diverse
conversational training data that pairs user utterances with recommendations.
To address this problem, we introduce a new method to generate synthetic
training data by transforming curated item collections, such as playlists or
movie watch lists, into item-seeking conversations. First, we use a biased
random walk to generate a sequence of slates, or sets of item recommendations;
then, we use a language model to generate corresponding user utterances. We
demonstrate our approach by generating a conversational music recommendation
dataset with over one million conversations, which were found to be consistent
with relevant recommendations by a crowdsourced evaluation. Using the synthetic
data to train a CRS, we significantly outperform standard retrieval baselines
in offline and online evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation. (arXiv:2301.11503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11503">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive translation (NAT) model achieves a much faster inference
speed than the autoregressive translation (AT) model because it can
simultaneously predict all tokens during inference. However, its translation
quality suffers from degradation compared to AT. And existing NAT methods only
focus on improving the NAT model's performance but do not fully utilize it. In
this paper, we propose a simple but effective method called "Candidate Soups,"
which can obtain high-quality translations while maintaining the inference
speed of NAT models. Unlike previous approaches that pick the individual result
and discard the remainders, Candidate Soups (CDS) can fully use the valuable
information in the different candidate translations through model uncertainty.
Extensive experiments on two benchmarks (WMT'14 EN-DE and WMT'16 EN-RO)
demonstrate the effectiveness and generality of our proposed method, which can
significantly improve the translation quality of various base models. More
notably, our best variant outperforms the AT model on three translation tasks
with 7.6 times speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Parametric Video-Grounded Text Generation. (arXiv:2301.11507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11507">
<div class="article-summary-box-inner">
<span><p>Efficient video-language modeling should consider the computational cost
because of a large, sometimes intractable, number of video frames. Parametric
approaches such as the attention mechanism may not be ideal since its
computational cost quadratically increases as the video length increases.
Rather, previous studies have relied on offline feature extraction or frame
sampling to represent the video efficiently, focusing on cross-modal modeling
in short video clips. In this paper, we propose a semi-parametric
video-grounded text generation model, SeViT, a novel perspective on scalable
video-language modeling toward long untrimmed videos. Treating a video as an
external data store, SeViT includes a non-parametric frame retriever to select
a few query-relevant frames from the data store for a given query and a
parametric generator to effectively aggregate the frames with the query via
late fusion methods. Experimental results demonstrate our method has a
significant advantage in longer videos and causal video understanding.
Moreover, our model achieves the new state of the art on four video-language
datasets, iVQA (+4.8), Next-QA (+6.9), and Activitynet-QA (+4.8) in accuracy,
and MSRVTT-Caption (+3.6) in CIDEr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theme-driven Keyphrase Extraction from Social Media on Opioid Recovery. (arXiv:2301.11508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11508">
<div class="article-summary-box-inner">
<span><p>An emerging trend on social media platforms is their use as safe spaces for
peer support. Particularly in healthcare, where many medical conditions contain
harsh stigmas, social media has become a stigma-free way to engage in dialogues
regarding symptoms, treatments, and personal experiences. Many existing works
have employed NLP algorithms to facilitate quantitative analysis of health
trends. Notably absent from existing works are keyphrase extraction (KE) models
for social health posts-a task crucial to discovering emerging public health
trends. This paper presents a novel, theme-driven KE dataset, SuboxoPhrase, and
a qualitative annotation scheme with an overarching goal of extracting targeted
clinically-relevant keyphrases. To the best of our knowledge, this is the first
study to design a KE schema for social media healthcare texts. To demonstrate
the value of this approach, this study analyzes Reddit posts regarding
medications for opioid use disorder, a paramount health concern worldwide.
Additionally, we benchmark ten off-the-shelf KE models on our new dataset,
demonstrating the unique extraction challenges in modeling user-generated
health texts. The proposed theme-driven KE approach lays the foundation of
future work on efficient, large-scale analysis of social health texts, allowing
researchers to surface useful public health trends, patterns, and knowledge
gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding. (arXiv:2301.11564v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11564">
<div class="article-summary-box-inner">
<span><p>Robotic grasping is a fundamental ability for a robot to interact with the
environment. Current methods focus on how to obtain a stable and reliable
grasping pose in object wise, while little work has been studied on part
(shape)-wise grasping which is related to fine-grained grasping and robotic
affordance. Parts can be seen as atomic elements to compose an object, which
contains rich semantic knowledge and a strong correlation with affordance.
However, lacking a large part-wise 3D robotic dataset limits the development of
part representation learning and downstream application. In this paper, we
propose a new large Language-guided SHape grAsPing datasEt (named Lang-SHAPE)
to learn 3D part-wise affordance and grasping ability. We design a novel
two-stage fine-grained robotic grasping network (named PIONEER), including a
novel 3D part language grounding model, and a part-aware grasp pose detection
model. To evaluate the effectiveness, we perform multi-level difficulty part
language grounding grasping experiments and deploy our proposed model on a real
robot. Results show our method achieves satisfactory performance and efficiency
in reference identification, affordance inference, and 3D part-aware grasping.
Our dataset and code are available on our project website
https://sites.google.com/view/lang-shape
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech. (arXiv:2301.11579v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11579">
<div class="article-summary-box-inner">
<span><p>Social media is a modern person's digital voice to project and engage with
new ideas and mobilise communities $\unicode{x2013}$ a power shared with
extremists. Given the societal risks of unvetted content-moderating algorithms
for Extremism, Radicalisation, and Hate speech (ERH) detection, responsible
software engineering must understand the who, what, when, where, and why such
models are necessary to protect user safety and free expression. Hence, we
propose and examine the unique research field of ERH context mining to unify
disjoint studies. Specifically, we evaluate the start-to-finish design process
from socio-technical definition-building and dataset collection strategies to
technical algorithm design and performance. Our 2015-2021 51-study Systematic
Literature Review (SLR) provides the first cross-examination of textual,
network, and visual approaches to detecting extremist affiliation, hateful
content, and radicalisation towards groups and movements. We identify
consensus-driven ERH definitions and propose solutions to existing ideological
and geographic biases, particularly due to the lack of research in
Oceania/Australasia. Our hybridised investigation on Natural Language
Processing, Community Detection, and visual-text models demonstrates the
dominating performance of textual transformer-based algorithms. We conclude
with vital recommendations for ERH context mining researchers and propose an
uptake roadmap with guidelines for researchers, industries, and governments to
enable a safer cyberspace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11596">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as GPT-3 and ChatGPT have recently
demonstrated impressive results across a wide range of tasks. LLMs are still
limited, however, in that they frequently fail at complex reasoning, their
reasoning processes are opaque, they are prone to 'hallucinate' facts, and
there are concerns about their underlying biases. Letting models verbalize
reasoning steps as natural language, a technique known as chain-of-thought
prompting, has recently been proposed as a way to address some of these issues.
Here we present the first release of ThoughtSource, a meta-dataset and software
library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to
improve future artificial intelligence systems by facilitating qualitative
understanding of CoTs, enabling empirical evaluations, and providing training
data. This first release of ThoughtSource integrates six scientific/medical,
three general-domain and five math word question answering datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks. (arXiv:2301.11608v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11608">
<div class="article-summary-box-inner">
<span><p>Learning to represent free text is a core task in many clinical machine
learning (ML) applications, as clinical text contains observations and plans
not otherwise available for inference. State-of-the-art methods use large
language models developed with immense computational resources and training
data; however, applying these models is challenging because of the highly
varying syntax and vocabulary in clinical free text. Structured information
such as International Classification of Disease (ICD) codes often succinctly
abstracts the most important facts of a clinical encounter and yields good
performance, but is often not as available as clinical text in real-world
scenarios. We propose a \textbf{multi-view learning framework} that jointly
learns from codes and text to combine the availability and forward-looking
nature of text and better performance of ICD codes. The learned text embeddings
can be used as inputs to predictive algorithms independent of the ICD codes
during inference. Our approach uses a Graph Neural Network (GNN) to process ICD
codes, and Bi-LSTM to process text. We apply Deep Canonical Correlation
Analysis (DCCA) to enforce the two views to learn a similar representation of
each patient. In experiments using planned surgical procedure text, our model
outperforms BERT models fine-tuned to clinical data, and in experiments using
diverse text in MIMIC-III, our model is competitive to a fine-tuned BERT at a
tiny fraction of its computational effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Causality Extraction with Event Argument Correlations. (arXiv:2301.11621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11621">
<div class="article-summary-box-inner">
<span><p>Event Causality Identification (ECI), which aims to detect whether a
causality relation exists between two given textual events, is an important
task for event causality understanding. However, the ECI task ignores crucial
event structure and cause-effect causality component information, making it
struggle for downstream applications. In this paper, we explore a novel task,
namely Event Causality Extraction (ECE), aiming to extract the cause-effect
event causality pairs with their structured event information from plain texts.
The ECE task is more challenging since each event can contain multiple event
arguments, posing fine-grained correlations between events to decide the
causeeffect event pair. Hence, we propose a method with a dual grid tagging
scheme to capture the intra- and inter-event argument correlations for ECE.
Further, we devise a event type-enhanced model architecture to realize the dual
grid tagging scheme. Experiments demonstrate the effectiveness of our method,
and extensive analyses point out several future directions for ECE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning Methods. (arXiv:2301.11660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11660">
<div class="article-summary-box-inner">
<span><p>As the size of the pre-trained language model (PLM) continues to increase,
numerous parameter-efficient transfer learning methods have been proposed
recently to compensate for the tremendous cost of fine-tuning. Despite the
impressive results achieved by large pre-trained language models (PLMs) and
various parameter-efficient transfer learning (PETL) methods on sundry
benchmarks, it remains unclear if they can handle inputs that have been
distributionally shifted effectively. In this study, we systematically explore
how the ability to detect out-of-distribution (OOD) changes as the size of the
PLM grows or the transfer methods are altered. Specifically, we evaluated
various PETL techniques, including fine-tuning, Adapter, LoRA, and
prefix-tuning, on three different intention classification tasks, each
utilizing various language models with different scales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Personalized Review Summarization by Modeling Historical Reviews from Customer and Product Separately. (arXiv:2301.11682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11682">
<div class="article-summary-box-inner">
<span><p>Review summarization is a non-trivial task that aims to summarize the main
idea of the product review in the E-commerce website. Different from the
document summary which only needs to focus on the main facts described in the
document, review summarization should not only summarize the main aspects
mentioned in the review but also reflect the personal style of the review
author. Although existing review summarization methods have incorporated the
historical reviews of both customer and product, they usually simply
concatenate and indiscriminately model this two heterogeneous information into
a long sequence. Moreover, the rating information can also provide a high-level
abstraction of customer preference, it has not been used by the majority of
methods. In this paper, we propose the Heterogeneous Historical Review aware
Review Summarization Model (HHRRS) which separately models the two types of
historical reviews with the rating information by a graph reasoning module with
a contrastive loss. We employ a multi-task framework that conducts the review
sentiment classification and summarization jointly. Extensive experiments on
four benchmark datasets demonstrate the superiority of HHRRS on both tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can We Use Probing to Better Understand Fine-tuning and Knowledge Distillation of the BERT NLU?. (arXiv:2301.11688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11688">
<div class="article-summary-box-inner">
<span><p>In this article, we use probing to investigate phenomena that occur during
fine-tuning and knowledge distillation of a BERT-based natural language
understanding (NLU) model. Our ultimate purpose was to use probing to better
understand practical production problems and consequently to build better NLU
models. We designed experiments to see how fine-tuning changes the linguistic
capabilities of BERT, what the optimal size of the fine-tuning dataset is, and
what amount of information is contained in a distilled NLU based on a tiny
Transformer. The results of the experiments show that the probing paradigm in
its current form is not well suited to answer such questions. Structural, Edge
and Conditional probes do not take into account how easy it is to decode probed
information. Consequently, we conclude that quantification of information
decodability is critical for many practical applications of the probing
paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLCNN: Sentence-Level Convolutional Neural Network for Text Classification. (arXiv:2301.11696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11696">
<div class="article-summary-box-inner">
<span><p>Text classification is a fundamental task in natural language processing
(NLP). Several recent studies show the success of deep learning on text
processing. Convolutional neural network (CNN), as a popular deep learning
model, has shown remarkable success in the task of text classification. In this
paper, new baseline models have been studied for text classification using CNN.
In these models, documents are fed to the network as a three-dimensional tensor
representation to provide sentence-level analysis. Applying such a method
enables the models to take advantage of the positional information of the
sentences in the text. Besides, analysing adjacent sentences allows extracting
additional features. The proposed models have been compared with the
state-of-the-art models using several datasets. The results have shown that the
proposed models have better performance, particularly in the longer documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Network Model for Sign Language Comprehension. (arXiv:2301.11709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11709">
<div class="article-summary-box-inner">
<span><p>In this study, the authors propose a computational cognitive model for sign
language (SL) perception and comprehension with detailed algorithmic
descriptions based on cognitive functionalities in human language processing.
The semantic network model (SNM) that represents semantic relations between
concepts, it is used as a form of knowledge representation. The proposed model
is applied in the comprehension of sign language for classifier predicates. The
spreading activation search method is initiated by labeling a set of source
nodes (e.g. concepts in the semantic network) with weights or "activation" and
then iteratively propagating or "spreading" that activation out to other nodes
linked to the source nodes. The results demonstrate that the proposed search
method improves the performance of sign language comprehension in the SNM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11716">
<div class="article-summary-box-inner">
<span><p>The gap between speech and text modalities is a major challenge in
speech-to-text translation (ST). Different methods have been proposed for
reducing this gap, but most of them require architectural changes in ST
training. In this work, we propose to mitigate this issue at the pre-training
stage, requiring no change in the ST model. First, we show that the
connectionist temporal classification (CTC) loss can reduce the modality gap by
design. We provide a quantitative comparison with the more common cross-entropy
loss, showing that pre-training with CTC consistently achieves better final ST
accuracy. Nevertheless, CTC is only a partial solution and thus, in our second
contribution, we propose a novel pre-training method combining CTC and optimal
transport to further reduce this gap. Our method pre-trains a Siamese-like
model composed of two encoders, one for acoustic inputs and the other for
textual inputs, such that they produce representations that are close to each
other in the Wasserstein space. Extensive experiments on the standard CoVoST-2
and MuST-C datasets show that our pre-training method applied to the vanilla
encoder-decoder Transformer achieves state-of-the-art performance under the
no-external-data setting, and performs on par with recent strong multi-task
learning systems trained with external data. Finally, our method can also be
applied on top of these multi-task systems, leading to further improvements for
these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Knowledge into Document Summarization: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11719">
<div class="article-summary-box-inner">
<span><p>Despite the great development of document summarization techniques nowadays,
factual inconsistencies between the generated summaries and the original text
still occur from time to time. This paper proposes a prefix-tuning-based
approach that uses a set of trainable continuous prefix prompt together with
discrete prompts to aid model generation, which makes a significant impact on
both CNN/Daily Mail and XSum summaries generated using GPT-2. The improvements
on fact preservation in the generated summaries indicates the effectiveness of
adopting this prefix-tuning-based method in knowledge-enhanced document
summarization, and also shows a great potential on other natural language
processing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-task Multi-stage Transitional Training Framework for Neural Chat Translation. (arXiv:2301.11749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11749">
<div class="article-summary-box-inner">
<span><p>Neural chat translation (NCT) aims to translate a cross-lingual chat between
speakers of different languages. Existing context-aware NMT models cannot
achieve satisfactory performances due to the following inherent problems: 1)
limited resources of annotated bilingual dialogues; 2) the neglect of modelling
conversational properties; 3) training discrepancy between different stages. To
address these issues, in this paper, we propose a multi-task multi-stage
transitional (MMT) training framework, where an NCT model is trained using the
bilingual chat translation dataset and additional monolingual dialogues. We
elaborately design two auxiliary tasks, namely utterance discrimination and
speaker discrimination, to introduce the modelling of dialogue coherence and
speaker characteristic into the NCT model. The training process consists of
three stages: 1) sentence-level pre-training on large-scale parallel corpus; 2)
intermediate training with auxiliary tasks using additional monolingual
dialogues; 3) context-aware fine-tuning with gradual transition. Particularly,
the second stage serves as an intermediate phase that alleviates the training
discrepancy between the pre-training and fine-tuning stages. Moreover, to make
the stage transition smoother, we train the NCT model using a gradual
transition strategy, i.e., gradually transiting from using monolingual to
bilingual dialogues. Extensive experiments on two language pairs demonstrate
the effectiveness and superiority of our proposed training framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11757">
<div class="article-summary-box-inner">
<span><p>The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
synthesis. One area that has yet to be fully explored is the application of
diffusion models to music generation. Music generation requires to handle
multiple aspects, including the temporal dimension, long-term structure,
multiple layers of overlapping sounds, and nuances that only trained listeners
can detect. In our work, we investigate the potential of diffusion models for
text-conditional music generation. We develop a cascading latent diffusion
approach that can generate multiple minutes of high-quality stereo music at
48kHz from textual descriptions. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open-source libraries
with the hope of facilitating future work in the field.
</p>
<p>We open-source the following: - Music samples for this paper:
https://bit.ly/anonymous-mousai - All music samples for all models:
https://bit.ly/audio-diffusion - Codes:
https://github.com/archinetai/audio-diffusion-pytorch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Attention with Hierarchies for Multi-hop Question Answering. (arXiv:2301.11792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11792">
<div class="article-summary-box-inner">
<span><p>Multi-hop QA (Question Answering) is the task of finding the answer to a
question across multiple documents. In recent years, a number of Deep
Learning-based approaches have been proposed to tackle this complex task, as
well as a few standard benchmarks to assess models Multi-hop QA capabilities.
In this paper, we focus on the well-established HotpotQA benchmark dataset,
which requires models to perform answer span extraction as well as support
sentence prediction. We present two extensions to the SOTA Graph Neural Network
(GNN) based model for HotpotQA, Hierarchical Graph Network (HGN): (i) we
complete the original hierarchical structure by introducing new edges between
the query and context sentence nodes; (ii) in the graph propagation step, we
propose a novel extension to Hierarchical Graph Attention Network GATH (Graph
ATtention with Hierarchies) that makes use of the graph hierarchy to update the
node representations in a sequential fashion. Experiments on HotpotQA
demonstrate the efficiency of the proposed modifications and support our
assumptions about the effects of model related variables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Call for Papers -- The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. (arXiv:2301.11796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11796">
<div class="article-summary-box-inner">
<span><p>We present the call for papers for the BabyLM Challenge: Sample-efficient
pretraining on a developmentally plausible corpus. This shared task is intended
for participants with an interest in small scale language modeling, human
language acquisition, low-resource NLP, and cognitive modeling. In partnership
with CoNLL and CMCL, we provide a platform for approaches to pretraining with a
limited-size corpus sourced from data inspired by the input to children. The
task has three tracks, two of which restrict the training data to pre-released
datasets of 10M and 100M words and are dedicated to explorations of approaches
such as architectural variations, self-supervised objectives, or curriculum
learning. The final track only restricts the amount of text used, allowing
innovation in the choice of the data, its domain, and even its modality (i.e.,
data from sources other than text is welcome). We will release a shared
evaluation pipeline which scores models on a variety of benchmarks and tasks,
including targeted syntactic evaluations and natural language understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking. (arXiv:2301.11843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11843">
<div class="article-summary-box-inner">
<span><p>Evidence data for automated fact-checking (AFC) can be in multiple modalities
such as text, tables, images, audio, or video. While there is increasing
interest in using images for AFC, previous works mostly focus on detecting
manipulated or fake images. We propose a novel task, chart-based fact-checking,
and introduce ChartBERT as the first model for AFC against chart evidence.
ChartBERT leverages textual, structural and visual information of charts to
determine the veracity of textual claims. For evaluation, we create ChartFC, a
new dataset of 15, 886 charts. We systematically evaluate 75 different
vision-language (VL) baselines and show that ChartBERT outperforms VL models,
achieving 63.8% accuracy. Our results suggest that the task is complex yet
feasible, with many challenges ahead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Effects of Physical Actions in a Multi-modal Environment. (arXiv:2301.11845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11845">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) handle physical commonsense information
inadequately. As a result of being trained in a disembodied setting, LLMs often
fail to predict an action's outcome in a given environment. However, predicting
the effects of an action before it is executed is crucial in planning, where
coherent sequences of actions are often needed to achieve a goal. Therefore, we
introduce the multi-modal task of predicting the outcomes of actions solely
from realistic sensory inputs (images and text). Next, we extend an LLM to
model latent representations of objects to better predict action outcomes in an
environment. We show that multi-modal models can capture physical commonsense
when augmented with visual information. Finally, we evaluate our model's
performance on novel actions and objects and find that combining modalities
help models to generalize and learn physical commonsense reasoning better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Pretrained Language Models for Long Clinical Text. (arXiv:2301.11847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11847">
<div class="article-summary-box-inner">
<span><p>Objective: Clinical knowledge enriched transformer models (e.g.,
ClinicalBERT) have state-of-the-art results on clinical NLP (natural language
processing) tasks. One of the core limitations of these transformer models is
the substantial memory consumption due to their full self-attention mechanism,
which leads to the performance degradation in long clinical texts. To overcome
this, we propose to leverage long-sequence transformer models (e.g., Longformer
and BigBird), which extend the maximum input sequence length from 512 to 4096,
to enhance the ability to model long-term dependencies in long clinical texts.
</p>
<p>Materials and Methods: Inspired by the success of long sequence transformer
models and the fact that clinical notes are mostly long, we introduce two
domain enriched language models, Clinical-Longformer and Clinical-BigBird,
which are pre-trained on a large-scale clinical corpus. We evaluate both
language models using 10 baseline tasks including named entity recognition,
question answering, natural language inference, and document classification
tasks.
</p>
<p>Results: The results demonstrate that Clinical-Longformer and
Clinical-BigBird consistently and significantly outperform ClinicalBERT and
other short-sequence transformers in all 10 downstream tasks and achieve new
state-of-the-art results.
</p>
<p>Discussion: Our pre-trained language models provide the bedrock for clinical
NLP using long texts. We have made our source code available at
https://github.com/luoyuanlab/Clinical-Longformer, and the pre-trained models
available for public download at:
https://huggingface.co/yikuan8/Clinical-Longformer.
</p>
<p>Conclusion: This study demonstrates that clinical knowledge enriched
long-sequence transformers are able to learn long-term dependencies in long
clinical text. Our methods can also inspire the development of other
domain-enriched long-sequence transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factual or Biased? Predicting Sentence-Level Factuality and Bias of News. (arXiv:2301.11850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11850">
<div class="article-summary-box-inner">
<span><p>We present a study on sentence-level factuality and bias of news articles
across domains. While prior work in NLP has mainly focused on predicting the
factuality of article-level news reporting and political-ideological bias of
news media, we investigated the effects of framing bias in factual reporting
across domains so as to predict factuality and bias at the sentence level,
which may explain more accurately the overall reliability of the entire
document. First, we manually produced a large sentence-level annotated dataset,
titled FactNews, composed of 6,191 sentences from 100 news stories by three
different outlets, resulting in 300 news articles. Further, we studied how
biased and factual spans surface in news articles from different media outlets
and different domains. Lastly, a baseline model for factual sentence prediction
was presented by fine-tuning BERT. We also provide a detailed analysis of data
demonstrating the reliability of the annotation and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Case-Based Reasoning with Language Models for Classification of Logical Fallacies. (arXiv:2301.11879v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11879">
<div class="article-summary-box-inner">
<span><p>The ease and the speed of spreading misinformation and propaganda on the Web
motivate the need to develop trustworthy technology for detecting fallacies in
natural language arguments. However, state-of-the-art language modeling methods
exhibit a lack of robustness on tasks like logical fallacy classification that
require complex reasoning. In this paper, we propose a Case-Based Reasoning
method that classifies new cases of logical fallacy by language-modeling-driven
retrieval and adaptation of historical cases. We design four complementary
strategies to enrich the input representation for our model, based on external
information about goals, explanations, counterarguments, and argument
structure. Our experiments in in-domain and out-of-domain settings indicate
that Case-Based Reasoning improves the accuracy and generalizability of
language models. Our ablation studies confirm that the representations of
similar cases have a strong impact on the model performance, that models
perform well with fewer retrieved cases, and that the size of the case database
has a negligible effect on the performance. Finally, we dive deeper into the
relationship between the properties of the retrieved cases and the model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11916">
<div class="article-summary-box-inner">
<span><p>In recent years, pre-trained large language models have demonstrated
remarkable efficiency in achieving an inference-time few-shot learning
capability known as in-context learning. However, existing literature has
highlighted the sensitivity of this capability to the selection of few-shot
demonstrations. The underlying mechanisms by which this capability arises from
regular language model pretraining objectives remain poorly understood. In this
study, we aim to examine the in-context learning phenomenon through a Bayesian
lens, viewing large language models as topic models that implicitly infer
task-related information from demonstrations. On this premise, we propose an
algorithm for selecting optimal demonstrations from a set of annotated data and
demonstrate a significant 12.5% improvement relative to the random selection
baseline, averaged over eight GPT2 and GPT3 models on eight different
real-world text classification datasets. Our empirical findings support our
hypothesis that large language models implicitly infer a latent concept
variable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Identifying and Fixing Inconsistent Readings from Information Extraction Systems. (arXiv:1808.04816v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.04816">
<div class="article-summary-box-inner">
<span><p>KGCleaner is a framework to identify and correct errors in data produced and
delivered by an information extraction system. These tasks have been
understudied and KGCleaner is the first to address both. We introduce a
multi-task model that jointly learns to predict if an extracted relation is
credible and repair it if not. We evaluate our approach and other models as
instance of our framework on two collections: a Wikidata corpus of nearly 700K
facts and 5M fact-relevant sentences and a collection of 30K facts from the
2015 TAC Knowledge Base Population task. For credibility classification,
parameter efficient simple shallow neural network can achieve an absolute
performance gain of 30 $F_1$ points on Wikidata and comparable performance on
TAC. For the repair task, significant performance (at more than twice) gain can
be obtained depending on the nature of the dataset and the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsically Motivated Compositional Language Emergence. (arXiv:2012.05011v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05011">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a great deal of research in emergent communication
on artificial agents interacting in simulated environments. Recent studies have
revealed that, in general, emergent languages do not follow the
compositionality patterns of natural language. To deal with this, existing
works have proposed a limited channel capacity as an important constraint for
learning highly compositional languages. In this paper, we show that this is
not a sufficient condition and propose an intrinsic reward framework for
improving compositionality in emergent communication. We use a reinforcement
learning setting with two agents -- a \textit{task-aware} Speaker and a
\textit{state-aware} Listener that are required to communicate to perform a set
of tasks. Through our experiments on three different referential game setups,
including a novel environment gComm, we show intrinsic rewards improve
compositionality scores by $\approx \mathbf{1.5-2}$ times that of existing
frameworks that use limited channel capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems. (arXiv:2109.04314v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04314">
<div class="article-summary-box-inner">
<span><p>Recently, two approaches, fine-tuning large pre-trained language models and
variational training, have attracted significant interests, separately, for
semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper,
we propose Variational Latent-State GPT model (VLS-GPT), which is the first to
combine the strengths of the two approaches. Among many options of models, we
propose the generative model and the inference model for variational learning
of the end-to-end TOD system, both as auto-regressive language models based on
GPT-2, which can be further trained over a mix of labeled and unlabeled dialog
data in a semi-supervised manner. Variational training of VLS-GPT is both
statistically and computationally more challenging than previous variational
learning works for sequential latent variable models, which use turn-level
first-order Markovian. The inference model in VLS-GPT is non-Markovian due to
the use of the Transformer architecture. In this work, we establish Recursive
Monte Carlo Approximation (RMCA) to the variational objective with
non-Markovian inference model and prove its unbiasedness. Further, we develop
the computational strategy of sampling-then-forward-computation to realize
RMCA, which successfully overcomes the memory explosion issue of using GPT in
variational learning and speeds up training. Semi-supervised TOD experiments
are conducted on two benchmark multi-domain datasets of different languages -
MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both
supervised-only and semi-supervised self-training baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric. (arXiv:2203.08299v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08299">
<div class="article-summary-box-inner">
<span><p>Syntax is a fundamental component of language, yet few metrics have been
employed to capture syntactic similarity or coherence at the utterance- and
document-level. The existing standard document-level syntactic similarity
metric is computationally expensive and performs inconsistently when faced with
syntactically dissimilar documents. To address these challenges, we present
FastKASSIM, a metric for utterance- and document-level syntactic similarity
which pairs and averages the most similar constituency parse trees between a
pair of documents based on tree kernels. FastKASSIM is more robust to syntactic
dissimilarities and runs up to to 5.32 times faster than its predecessor over
documents in the r/ChangeMyView corpus. FastKASSIM's improvements allow us to
examine hypotheses in two settings with large documents. We find that
syntactically similar arguments on r/ChangeMyView tend to be more persuasive,
and that syntax is predictive of authorship attribution in the Australian High
Court Judgment corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP. (arXiv:2206.10265v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10265">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the data augmentation for low-resource NLP tasks where
the training set is limited. The existing solutions either leverage
task-independent heuristic rules (e.g., Synonym Replacement) or fine-tune
general-purpose pre-trained language models (e.g., GPT2) using the limited
training instances to produce new synthetic data. Consequently, they have
trivial task-specific knowledge and are limited to yielding low-quality
synthetic data. To combat this issue, we propose Knowledge Mixture Data
Augmentation Model (KnowDA) which is an Seq2Seq language model pre-trained on a
mixture of diverse NLP tasks under a novel framework of Knowledge Mixture
Training (KoMT). The goal of KoMT is to condense diverse NLP task-specific
knowledge into the single KnowDA model (i.e., all-in-one) such that KnowDA
could utilize these knowledge to quickly grasp the inherent synthesis law of
the target task through limited training instances. Specifically, KoMT
reformulates input examples from various heterogeneous NLP tasks into a unified
text-to-text format, and employs denoising training objectives in different
granularity to learn to reconstruct partial or complete samples. To the best of
our knowledge, we are the first attempt to apply 100+ NLP multi-task training
for data augmentation. Extensive experiments show that i) the synthetic data
produced by KnowDA successfully improves performance of the strong pre-trained
language models (i.e., Bert, ALBert and Deberta) by a large margin on the
low-resource NLP benchmark FewGLUE, CoNLL'03 and WikiAnn; ii) KnowDA
successfully transfers the task knowledge to NLP tasks whose types are seen and
unseen in KoMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09068">
<div class="article-summary-box-inner">
<span><p>While contextualized word embeddings have been a de-facto standard, learning
contextualized phrase embeddings is less explored and being hindered by the
lack of a human-annotated benchmark that tests machine understanding of phrase
semantics given a context sentence or paragraph (instead of phrases alone). To
fill this gap, we propose PiC -- a dataset of ~28K of noun phrases accompanied
by their contextual Wikipedia pages and a suite of three tasks for training and
evaluating phrase embeddings. Training on PiC improves ranking models' accuracy
and remarkably pushes span-selection (SS) models (i.e., predicting the start
and end index of the target phrase) near-human accuracy, which is 95% Exact
Match (EM) on semantic search given a query phrase and a passage.
Interestingly, we find evidence that such impressive performance is because the
SS models learn to better capture the common meaning of a phrase regardless of
its actual context. SotA models perform poorly in distinguishing two senses of
the same phrase in two contexts (~60% EM) and in estimating the similarity
between two different phrases in the same context (~70% EM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Patterns in Data with Language Models via Interpretable Autoprompting. (arXiv:2210.01848v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01848">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have displayed an impressive ability to harness
natural language to perform complex tasks. In this work, we explore whether we
can leverage this learned ability to find and explain patterns in data.
Specifically, given a pre-trained LLM and data examples, we introduce
interpretable autoprompting (iPrompt), an algorithm that generates a
natural-language string explaining the data. iPrompt iteratively alternates
between generating explanations with an LLM and reranking them based on their
performance when used as a prompt. Experiments on a wide range of datasets,
from synthetic mathematics to natural-language understanding, show that iPrompt
can yield meaningful insights by accurately finding groundtruth dataset
descriptions. Moreover, the prompts produced by iPrompt are simultaneously
human-interpretable and highly effective for generalization: on real-world
sentiment classification datasets, iPrompt produces prompts that match or even
improve upon human-written prompts for GPT-3. Finally, experiments with an fMRI
dataset show the potential for iPrompt to aid in scientific discovery. All code
for using the methods and data here is made available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAL: Program-aided Language Models. (arXiv:2211.10435v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10435">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently demonstrated an impressive ability
to perform arithmetic and symbolic reasoning tasks, when provided with a few
examples at test time ("few-shot prompting"). Much of this success can be
attributed to prompting methods such as "chain-of-thought'', which employ LLMs
for both understanding the problem description by decomposing it into steps, as
well as solving each step of the problem. While LLMs seem to be adept at this
sort of step-by-step decomposition, LLMs often make logical and arithmetic
mistakes in the solution part, even when the problem is decomposed correctly.
In this paper, we present Program-Aided Language models (PAL): a novel approach
that uses the LLM to read natural language problems and generate programs as
the intermediate reasoning steps, but offloads the solution step to a runtime
such as a Python interpreter. With PAL, decomposing the natural language
problem into runnable steps remains the only learning task for the LLM, while
solving is delegated to the interpreter. We demonstrate this synergy between a
neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and
algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all
these natural language reasoning tasks, generating code using an LLM and
reasoning using a Python interpreter leads to more accurate results than much
larger models. For example, PAL using Codex achieves state-of-the-art few-shot
accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B
which uses chain-of-thought by absolute 15% top-1. Our code and data are
publicly available at <a href="http://reasonwithpal.com/">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05612">
<div class="article-summary-box-inner">
<span><p>Warning: this paper contains content that may be offensive or upsetting. In
the current context where online platforms have been effectively weaponized in
a variety of geo-political events and social issues, Internet memes make fair
content moderation at scale even more difficult. Existing work on meme
classification and tracking has focused on black-box methods that do not
explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Slang Representation Methods. (arXiv:2212.05613v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05613">
<div class="article-summary-box-inner">
<span><p>Warning: this paper contains content that may be offensive or upsetting.
Considering the large amount of content created online by the minute,
slang-aware automatic tools are critically needed to promote social good, and
assist policymakers and moderators in restricting the spread of offensive
language, abuse, and hate speech. Despite the success of large language models
and the spontaneous emergence of slang dictionaries, it is unclear how far
their combination goes in terms of slang understanding for downstream social
good tasks. In this paper, we provide a framework to study different
combinations of representation learning models and knowledge resources for a
variety of downstream tasks that rely on slang understanding. Our experiments
show the superiority of models that have been pre-trained on social media data,
while the impact of dictionaries is positive only for static word embeddings.
Our error analysis identifies core challenges for slang representation
learning, including out-of-vocabulary words, polysemy, variance, and annotation
disagreements, which can be traced to characteristics of slang as a quickly
evolving and highly subjective language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Watermark for Large Language Models. (arXiv:2301.10226v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10226">
<div class="article-summary-box-inner">
<span><p>Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of "green" tokens
before a word is generated, and then softly promoting use of green tokens
during sampling. We propose a statistical test for detecting the watermark with
interpretable p-values, and derive an information-theoretic framework for
analyzing the sensitivity of the watermark. We test the watermark using a
multi-billion parameter model from the Open Pretrained Transformer (OPT)
family, and discuss robustness and security.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in E-commerce. (arXiv:2205.10843v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10843">
<div class="article-summary-box-inner">
<span><p>In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for
widespread applications such as product search and recommendation. For example,
when users search for ``running'' in e-commerce, they would like to find
products highly related to running, such as ``running shoes'' rather than
``shoes''. Nevertheless, many existing CSK collections rank statements solely
by confidence scores, and there is no information about which ones are salient
from a human perspective. In this work, we define the task of supervised
salience evaluation, where given a CSK triple, the model is required to learn
whether the triple is salient or not. In addition to formulating the new task,
we also release a new Benchmark dataset of Salience Evaluation in E-commerce
(BSEE) and hope to promote related research on commonsense knowledge salience
evaluation. We conduct experiments in the dataset with several representative
baseline models. The experimental results show that salience evaluation is a
challenging task where models perform poorly on our evaluation set. We further
propose a simple but effective approach, PMI-tuning, which shows promise for
solving this novel problem. Code is available in
\url{https://github.com/OpenBGBenchmark/OpenBG-CSK.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-30 23:13:22.664973839 UTC">2023-01-30 23:13:22 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>