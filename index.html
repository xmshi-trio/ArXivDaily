<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-29T01:30:00Z">11-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Bidirectional Representations for Low Resource Spoken Language Understanding. (arXiv:2211.14320v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14320">
<div class="article-summary-box-inner">
<span><p>Most spoken language understanding systems use a pipeline approach composed
of an automatic speech recognition interface and a natural language
understanding module. This approach forces hard decisions when converting
continuous inputs into discrete language symbols. Instead, we propose a
representation model to encode speech in rich bidirectional encodings that can
be used for downstream tasks such as intent prediction. The approach uses a
masked language modelling objective to learn the representations, and thus
benefits from both the left and right contexts. We show that the performance of
the resulting encodings before fine-tuning is better than comparable models on
multiple datasets, and that fine-tuning the top layers of the representation
model improves the current state of the art on the Fluent Speech Command
dataset, also in a low-data regime, when a limited amount of labelled data is
used for training. Furthermore, we propose class attention as a spoken language
understanding module, efficient both in terms of speed and number of
parameters. Class attention can be used to visually explain the predictions of
our model, which goes a long way in understanding how the model makes
predictions. We perform experiments in English and in Dutch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Machine Learning, Natural Language Processing Analysis of Youth Perspectives: Key Trends and Focus Areas for Sustainable Youth Development Policies. (arXiv:2211.14321v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14321">
<div class="article-summary-box-inner">
<span><p>Investing in children and youth is a critical step towards inclusive,
equitable, and sustainable development for current and future generations.
Several international agendas for accomplishing common global goals emphasize
the need for active youth participation and engagement for sustainable
development. The 2030 Agenda for Sustainable Development emphasizes the need
for youth engagement and the inclusion of youth perspectives as an important
step toward addressing each of the 17 Sustainable Development Goals. The aim of
this study is to analyze youth perspectives, values, and sentiments towards
issues addressed by the 17 Sustainable Development Goals through social network
analysis using machine learning. Social network data collected during 7 major
sustainability conferences aimed at engaging children and youth is analyzed
using natural language processing techniques for sentiment analysis. This data
categorized using a natural language processing text classifier trained on a
sample dataset of social network data during the 7 youth sustainability
conferences for deeper understanding of youth perspectives in relation to the
SDGs. Machine learning identified demographic and location attributes and
features are utilized in order to identify bias and demographic differences
between ages, gender, and race among youth. Using natural language processing,
the qualitative data collected from over 7 different countries in 3 languages
are systematically translated, categorized, and analyzed, revealing key trends
and focus areas for sustainable youth development policies. The obtained
results reveal the general youth's depth of knowledge on sustainable
development and their attitudes towards each of the 17 SDGs. The findings of
this study serve as a guide toward better understanding the interests, roles,
and perspectives of children and youth in achieving the goals of Agenda 2030.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Moral- and Event- Centric Inspection of Gender Bias in Fairy Tales at A Large Scale. (arXiv:2211.14358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14358">
<div class="article-summary-box-inner">
<span><p>Fairy tales are a common resource for young children to learn a language or
understand how a society works. However, gender bias, e.g., stereotypical
gender roles, in this literature may cause harm and skew children's world view.
Instead of decades of qualitative and manual analysis of gender bias in fairy
tales, we computationally analyze gender bias in a fairy tale dataset
containing 624 fairy tales from 7 different cultures. We specifically examine
gender difference in terms of moral foundations, which are measures of human
morality, and events, which reveal human activities associated with each
character. We find that the number of male characters is two times that of
female characters, showing a disproportionate gender representation. Our
analysis further reveal stereotypical portrayals of both male and female
characters in terms of moral foundations and events. Female characters turn out
more associated with care-, loyalty- and sanctity- related moral words, while
male characters are more associated with fairness- and authority- related moral
words. Female characters' events are often about emotion (e.g., weep),
appearance (e.g., comb), household (e.g., bake), etc.; while male characters'
events are more about profession (e.g., hunt), violence (e.g., destroy),
justice (e.g., judge), etc. Gender bias in terms of moral foundations shows an
obvious difference across cultures. For example, female characters are more
associated with care and sanctity in high uncertainty-avoidance cultures which
are less open to changes and unpredictability. Based on the results, we propose
implications for children's literature and early literacy research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning BERT on Partially Annotated NER Corpora. (arXiv:2211.14360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14360">
<div class="article-summary-box-inner">
<span><p>Most Named Entity Recognition (NER) models operate under the assumption that
training datasets are fully labelled. While it is valid for established
datasets like CoNLL 2003 and OntoNotes, sometimes it is not feasible to obtain
the complete dataset annotation. These situations may occur, for instance,
after selective annotation of entities for cost reduction. This work presents
an approach to finetuning BERT on such partially labelled datasets using
self-supervision and label preprocessing. Our approach outperforms the previous
LSTM-based label preprocessing baseline, significantly improving the
performance on poorly labelled datasets. We demonstrate that following our
approach while finetuning RoBERTa on CoNLL 2003 dataset with only 10% of total
entities labelled is enough to reach the performance of the baseline trained on
the same dataset with 50% of the entities labelled.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Naughtyformer: A Transformer Understands Offensive Humor. (arXiv:2211.14369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14369">
<div class="article-summary-box-inner">
<span><p>Jokes are intentionally written to be funny, but not all jokes are created
the same. Some jokes may be fit for a classroom of kindergarteners, but others
are best reserved for a more mature audience. While recent work has shown
impressive results on humor detection in text, here we instead investigate the
more nuanced task of detecting humor subtypes, especially of the less innocent
variety. To that end, we introduce a novel jokes dataset filtered from Reddit
and solve the subtype classification task using a finetuned Transformer dubbed
the Naughtyformer. Moreover, we show that our model is significantly better at
detecting offensiveness in jokes compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretability Analysis of Deep Models for COVID-19 Detection. (arXiv:2211.14372v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14372">
<div class="article-summary-box-inner">
<span><p>During the outbreak of COVID-19 pandemic, several research areas joined
efforts to mitigate the damages caused by SARS-CoV-2. In this paper we present
an interpretability analysis of a convolutional neural network based model for
COVID-19 detection in audios. We investigate which features are important for
model decision process, investigating spectrograms, F0, F0 standard deviation,
sex and age. Following, we analyse model decisions by generating heat maps for
the trained models to capture their attention during the decision process.
Focusing on a explainable Inteligence Artificial approach, we show that studied
models can taken unbiased decisions even in the presence of spurious data in
the training set, given the adequate preprocessing steps. Our best model has
94.44% of accuracy in detection, with results indicating that models favors
spectrograms for the decision process, particularly, high energy areas in the
spectrogram related to prosodic domains, while F0 also leads to efficient
COVID-19 detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of Social Biases Present in BERT Variants Across Multiple Languages. (arXiv:2211.14402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14402">
<div class="article-summary-box-inner">
<span><p>Although large pre-trained language models have achieved great success in
many NLP tasks, it has been shown that they reflect human biases from their
pre-training corpora. This bias may lead to undesirable outcomes when these
models are applied in real-world settings. In this paper, we investigate the
bias present in monolingual BERT models across a diverse set of languages
(English, Greek, and Persian). While recent research has mostly focused on
gender-related biases, we analyze religious and ethnic biases as well and
propose a template-based method to measure any kind of bias, based on sentence
pseudo-likelihood, that can handle morphologically complex languages with
gender-based adjective declensions. We analyze each monolingual model via this
method and visualize cultural similarities and differences across different
dimensions of bias. Ultimately, we conclude that current methods of probing for
bias are highly language-dependent, necessitating cultural insights regarding
the unique ways bias is expressed in each language and culture (e.g. through
coded language, synecdoche, and other similar linguistic concepts). We also
hypothesize that higher measured social biases in the non-English BERT models
correlate with user-generated content in their training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLAMI-1M: A Multilingual Image-Text Fashion Dataset. (arXiv:2211.14451v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14451">
<div class="article-summary-box-inner">
<span><p>We introduce GLAMI-1M: the largest multilingual image-text classification
dataset and benchmark. The dataset contains images of fashion products with
item descriptions, each in 1 of 13 languages. Categorization into 191 classes
has high-quality annotations: all 100k images in the test set and 75% of the 1M
training set were human-labeled. The paper presents baselines for image-text
classification showing that the dataset presents a challenging fine-grained
classification problem: The best scoring EmbraceNet model using both visual and
textual features achieves 69.7% accuracy. Experiments with a modified Imagen
model show the dataset is also suitable for image generation conditioned on
text. The dataset, source code and model checkpoints are published at
https://github.com/glami/glami-1m
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Model for Word Level Language Identification in Code-mixed Kannada-English Texts. (arXiv:2211.14459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14459">
<div class="article-summary-box-inner">
<span><p>Using code-mixed data in natural language processing (NLP) research currently
gets a lot of attention. Language identification of social media code-mixed
text has been an interesting problem of study in recent years due to the
advancement and influences of social media in communication. This paper
presents the Instituto Polit\'ecnico Nacional, Centro de Investigaci\'on en
Computaci\'on (CIC) team's system description paper for the CoLI-Kanglish
shared task at ICON2022. In this paper, we propose the use of a Transformer
based model for word-level language identification in code-mixed Kannada
English texts. The proposed model on the CoLI-Kenglish dataset achieves a
weighted F1-score of 0.84 and a macro F1-score of 0.61.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SKDBERT: Compressing BERT via Stochastic Knowledge Distillation. (arXiv:2211.14466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14466">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain
compact BERT-style language model dubbed SKDBERT. In each iteration, SKD
samples a teacher model from a pre-defined teacher ensemble, which consists of
multiple teacher models with multi-level capacities, to transfer knowledge into
student model in an one-to-one manner. Sampling distribution plays an important
role in SKD. We heuristically present three types of sampling distributions to
assign appropriate probabilities for multi-level teacher models. SKD has two
advantages: 1) it can preserve the diversities of multi-level teacher models
via stochastically sampling single teacher model in each iteration, and 2) it
can also improve the efficacy of knowledge distillation via multi-level teacher
models when large capacity gap exists between the teacher model and the student
model. Experimental results on GLUE benchmark show that SKDBERT reduces the
size of a BERT$_{\rm BASE}$ model by 40% while retaining 99.5% performances of
language understanding and being 100% faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Document-level Relation Extraction via Iterative Inference. (arXiv:2211.14470v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14470">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (RE) aims to extract the relations between
entities from the input document that usually containing many
difficultly-predicted entity pairs whose relations can only be predicted
through relational inference. Existing methods usually directly predict the
relations of all entity pairs of input document in a one-pass manner, ignoring
the fact that predictions of some entity pairs heavily depend on the predicted
results of other pairs. To deal with this issue, in this paper, we propose a
novel document-level RE model with iterative inference. Our model is mainly
composed of two modules: 1) a base module expected to provide preliminary
relation predictions on entity pairs; 2) an inference module introduced to
refine these preliminary predictions by iteratively dealing with
difficultly-predicted entity pairs depending on other pairs in an easy-to-hard
manner. Unlike previous methods which only consider feature information of
entity pairs, our inference module is equipped with two Extended Cross
Attention units, allowing it to exploit both feature information and previous
predictions of entity pairs during relational inference. Furthermore, we adopt
a two-stage strategy to train our model. At the first stage, we only train our
base module. During the second stage, we train the whole model, where
contrastive learning is introduced to enhance the training of inference module.
Experimental results on three commonly-used datasets show that our model
consistently outperforms other competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection. (arXiv:2211.14477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14477">
<div class="article-summary-box-inner">
<span><p>Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation
triplets from unstructured texts, while the relation sets at the training and
testing stages are disjoint. Previous state-of-the-art method handles this
challenging task by leveraging pretrained language models to generate data as
additional training samples, which increases the training cost and severely
constrains the model performance. We tackle this task from a new perspective
and propose a novel method named PCRED for ZeroRTE with Potential Candidate
Relation selection and Entity boundary Detection. The model adopts a
relation-first paradigm, which firstly recognizes unseen relations through
candidate relation selection. By this approach, the semantics of relations are
naturally infused in the context. Entities are extracted based on the context
and the semantics of relations subsequently. We evaluate our model on two
ZeroRTE datasets. The experiment result shows that our method consistently
outperforms previous works. Besides, our model does not rely on any additional
data, which boasts the advantages of simplicity and effectiveness. Our code is
available at https://anonymous.4open.science/r/PCRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predictive linguistic cues for fake news: a societal artificial intelligence problem. (arXiv:2211.14505v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14505">
<div class="article-summary-box-inner">
<span><p>Media news are making a large part of public opinion and, therefore, must not
be fake. News on web sites, blogs, and social media must be analyzed before
being published. In this paper, we present linguistic characteristics of media
news items to differentiate between fake news and real news using machine
learning algorithms. Neural fake news generation, headlines created by
machines, semantic incongruities in text and image captions generated by
machine are other types of fake news problems. These problems use neural
networks which mainly control distributional features rather than evidence. We
propose applying correlation between features set and class, and correlation
among the features to compute correlation attribute evaluation metric and
covariance metric to compute variance of attributes over the news items.
Features unique, negative, positive, and cardinal numbers with high values on
the metrics are observed to provide a high area under the curve (AUC) and
F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexicon-injected Semantic Parsing for Task-Oriented Dialog. (arXiv:2211.14508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14508">
<div class="article-summary-box-inner">
<span><p>Recently, semantic parsing using hierarchical representations for dialog
systems has captured substantial attention. Task-Oriented Parse (TOP), a tree
representation with intents and slots as labels of nested tree nodes, has been
proposed for parsing user utterances. Previous TOP parsing methods are limited
on tackling unseen dynamic slot values (e.g., new songs and locations added),
which is an urgent matter for real dialog systems. To mitigate this issue, we
first propose a novel span-splitting representation for span-based parser that
outperforms existing methods. Then we present a novel lexicon-injected semantic
parser, which collects slot labels of tree representation as a lexicon, and
injects lexical features to the span representation of parser. An additional
slot disambiguation technique is involved to remove inappropriate span match
occurrences from the lexicon. Our best parser produces a new state-of-the-art
result (87.62%) on the TOP dataset, and demonstrates its adaptability to
frequently updated slot lexicon entries in real task-oriented dialog, with no
need of retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automatic SOAP Classification System Using Weakly Supervision And Transfer Learning. (arXiv:2211.14539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14539">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a comprehensive framework for developing a
machine learning-based SOAP (Subjective, Objective, Assessment, and Plan)
classification system without manually SOAP annotated training data or with
less manually SOAP annotated training data. The system is composed of the
following two parts: 1) Data construction, 2) A neural network-based SOAP
classifier, and 3) Transfer learning framework. In data construction, since a
manual construction of a large size training dataset is expensive, we propose a
rule-based weak labeling method utilizing the structured information of an EHR
note. Then, we present a SOAP classifier composed of a pre-trained language
model and bi-directional long-short term memory with conditional random field
(Bi-LSTM-CRF). Finally, we propose a transfer learning framework that re-uses
the trained parameters of the SOAP classifier trained with the weakly labeled
dataset for datasets collected from another hospital. The proposed weakly
label-based learning model successfully performed SOAP classification (89.99
F1-score) on the notes collected from the target hospital. Otherwise, in the
notes collected from other hospitals and departments, the performance
dramatically decreased. Meanwhile, we verified that the transfer learning
framework is advantageous for inter-hospital adaptation of the model increasing
the models' performance in every cases. In particular, the transfer learning
approach was more efficient when the manually annotated data size was smaller.
We showed that SOAP classification models trained with our weakly labeling
algorithm can perform SOAP classification without manually annotated data on
the EHR notes from the same hospital. The transfer learning framework helps
SOAP classification model's inter-hospital migration with a minimal size of the
manually annotated dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexical Complexity Controlled Sentence Generation. (arXiv:2211.14540v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14540">
<div class="article-summary-box-inner">
<span><p>Text generation rarely considers the control of lexical complexity, which
limits its more comprehensive practical application. We introduce a novel task
of lexical complexity controlled sentence generation, which aims at keywords to
sentence generation with desired complexity levels. It has enormous potential
in domains such as grade reading, language teaching and acquisition. The
challenge of this task is to generate fluent sentences only using the words of
given complexity levels. We propose a simple but effective approach for this
task based on complexity embedding. Compared with potential solutions, our
approach fuses the representations of the word complexity levels into the model
to get better control of lexical complexity. And we demonstrate the feasibility
of the approach for both training models from scratch and fine-tuning the
pre-trained models. To facilitate the research, we develop two datasets in
English and Chinese respectively, on which extensive experiments are conducted.
Results show that our approach better controls lexical complexity and generates
higher quality sentences than baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Expressive Text-to-Speech. (arXiv:2211.14548v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14548">
<div class="article-summary-box-inner">
<span><p>The goal of expressive Text-to-speech (TTS) is to synthesize natural speech
with desired content, prosody, emotion, or timbre, in high expressiveness. Most
of previous studies attempt to generate speech from given labels of styles and
emotions, which over-simplifies the problem by classifying styles and emotions
into a fixed number of pre-defined categories. In this paper, we introduce a
new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a
person speaks depends on the particular context she is in, where the context
can typically be represented as text. Thus, in the CTTS task, we propose to
utilize such context to guide the speech synthesis process instead of relying
on explicit labels of styles and emotions. To achieve this task, we construct a
synthetic dataset and develop an effective framework. Experiments show that our
framework can generate high-quality expressive speech based on the given
context both in synthetic datasets and real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who are you referring to? Weakly supervised coreference resolution with multimodal grounding. (arXiv:2211.14563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14563">
<div class="article-summary-box-inner">
<span><p>Coreference resolution aims at identifying words and phrases which refer to
same entity in a text, a core tool in natural language processing. In this
paper, we propose a novel task, resolving coreferences in multimodal data,
long-form textual descriptions of visual scenes. Most existing image-text
datasets only contain short sentences without coreferent expressions, or
coreferences are not annotated. To this end, we first introduce a new dataset,
Flickr30k-Coref in which coreference chains and bounding box localization of
these chains are annotated. We propose a new technique that learns to identify
coreference chains through weakly supervised grounding from image-text pairs
and a regularization using prior linguistic knowledge. Our model yields large
performance gains over prior work in coreference resolution and weakly
supervised grounding of long-form text descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Text Representation Methods and Their Genealogy. (arXiv:2211.14591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14591">
<div class="article-summary-box-inner">
<span><p>In recent years, with the advent of highly scalable
artificial-neural-network-based text representation methods the field of
natural language processing has seen unprecedented growth and sophistication.
It has become possible to distill complex linguistic information of text into
multidimensional dense numeric vectors with the use of the distributional
hypothesis. As a consequence, text representation methods have been evolving at
such a quick pace that the research community is struggling to retain knowledge
of the methods and their interrelations. We contribute threefold to this lack
of compilation, composition, and systematization by providing a survey of
current approaches, by arranging them in a genealogy, and by conceptualizing a
taxonomy of text representation methods to examine and explain the
state-of-the-art. Our research is a valuable guide and reference for artificial
intelligence researchers and practitioners interested in natural language
processing applications such as recommender systems, chatbots, and sentiment
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The distribution of syntactic dependency distances. (arXiv:2211.14620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14620">
<div class="article-summary-box-inner">
<span><p>The syntactic structure of a sentence can be represented as a graph where
vertices are words and edges indicate syntactic dependencies between them. In
this setting, the distance between two syntactically linked words can be
defined as the difference between their positions. Here we want to contribute
to the characterization of the actual distribution of syntactic dependency
distances, and unveil its relationship with short-term memory limitations. We
propose a new double-exponential model in which decay in probability is allowed
to change after a break-point. This transition could mirror the transition from
the processing of words chunks to higher-level structures. We find that a
two-regime model -- where the first regime follows either an exponential or a
power-law decay -- is the most likely one in all 20 languages we considered,
independently of sentence length and annotation style. Moreover, the
break-point is fairly stable across languages and averages values of 4-5 words,
suggesting that the amount of words that can be simultaneously processed
abstracts from the specific language to a high degree. Finally, we give an
account of the relation between the best estimated model and the closeness of
syntactic dependencies, as measured by a recently introduced optimality score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for Discriminative Words in Multidimensional Continuous Feature Space. (arXiv:2211.14631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14631">
<div class="article-summary-box-inner">
<span><p>Word feature vectors have been proven to improve many NLP tasks. With recent
advances in unsupervised learning of these feature vectors, it became possible
to train it with much more data, which also resulted in better quality of
learned features. Since it learns joint probability of latent features of
words, it has the advantage that we can train it without any prior knowledge
about the goal task we want to solve. We aim to evaluate the universal
applicability property of feature vectors, which has been already proven to
hold for many standard NLP tasks like part-of-speech tagging or syntactic
parsing. In our case, we want to understand the topical focus of text documents
and design an efficient representation suitable for discriminating different
topics. The discriminativeness can be evaluated adequately on text
categorisation task. We propose a novel method to extract discriminative
keywords from documents. We utilise word feature vectors to understand the
relations between words better and also understand the latent topics which are
discussed in the text and not mentioned directly but inferred logically. We
also present a simple way to calculate document feature vectors out of
extracted discriminative words. We evaluate our method on the four most popular
datasets for text categorisation. We show how different discriminative metrics
influence the overall results. We demonstrate the effectiveness of our approach
by achieving state-of-the-art results on text categorisation task using just a
small number of extracted keywords. We prove that word feature vectors can
substantially improve the topical inference of documents' meaning. We conclude
that distributed representation of words can be used to build higher levels of
abstraction as we demonstrate and build feature vectors of documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Biases Unexpectedly Fluctuate in the Pre-training Stage of Masked Language Models. (arXiv:2211.14639v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14639">
<div class="article-summary-box-inner">
<span><p>Masked language models pick up gender biases during pre-training. Such biases
are usually attributed to a certain model architecture and its pre-training
corpora, with the implicit assumption that other variations in the pre-training
process, such as the choices of the random seed or the stopping point, have no
effect on the biases measured. However, we show that severe fluctuations exist
at the fundamental level of individual templates, invalidating the assumption.
Further against the intuition of how humans acquire biases, these fluctuations
are not correlated with the certainty of the predicted pronouns or the
profession frequencies in pre-training corpora. We release our code and data to
benefit future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel multimodal dynamic fusion network for disfluency detection in spoken utterances. (arXiv:2211.14700v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14700">
<div class="article-summary-box-inner">
<span><p>Disfluency, though originating from human spoken utterances, is primarily
studied as a uni-modal text-based Natural Language Processing (NLP) task. Based
on early-fusion and self-attention-based multimodal interaction between text
and acoustic modalities, in this paper, we propose a novel multimodal
architecture for disfluency detection from individual utterances. Our
architecture leverages a multimodal dynamic fusion network that adds minimal
parameters over an existing text encoder commonly used in prior art to leverage
the prosodic and acoustic cues hidden in speech. Through experiments, we show
that our proposed model achieves state-of-the-art results on the widely used
English Switchboard for disfluency detection and outperforms prior unimodal and
multimodal systems in literature by a significant margin. In addition, we make
a thorough qualitative analysis and show that, unlike text-only systems, which
suffer from spurious correlations in the data, our system overcomes this
problem through additional cues from speech signals. We make all our codes
publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BadPrompt: Backdoor Attacks on Continuous Prompts. (arXiv:2211.14719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14719">
<div class="article-summary-box-inner">
<span><p>The prompt-based learning paradigm has gained much research attention
recently. It has achieved state-of-the-art performance on several NLP tasks,
especially in the few-shot scenarios. While steering the downstream tasks, few
works have been reported to investigate the security problems of the
prompt-based models. In this paper, we conduct the first study on the
vulnerability of the continuous prompt learning algorithm to backdoor attacks.
We observe that the few-shot scenarios have posed a great challenge to backdoor
attacks on the prompt-based models, limiting the usability of existing NLP
backdoor methods. To address this challenge, we propose BadPrompt, a
lightweight and task-adaptive algorithm, to backdoor attack continuous prompts.
Specially, BadPrompt first generates candidate triggers which are indicative
for predicting the targeted label and dissimilar to the samples of the
non-targeted labels. Then, it automatically selects the most effective and
invisible trigger for each sample with an adaptive trigger optimization
algorithm. We evaluate the performance of BadPrompt on five datasets and two
continuous prompt models. The results exhibit the abilities of BadPrompt to
effectively attack continuous prompts while maintaining high performance on the
clean test sets, outperforming the baseline models by a large margin. The
source code of BadPrompt is publicly available at
https://github.com/papersPapers/BadPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-PuDu at SemEval-2022 Task 7: A Replaced Token Detection Task Pre-trained Model with Pattern-aware Ensembling for Identifying Plausible Clarifications. (arXiv:2211.14734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14734">
<div class="article-summary-box-inner">
<span><p>This paper describes our winning system on SemEval 2022 Task 7: Identifying
Plausible Clarifications of Implicit and Underspecified Phrases in
Instructional Texts. A replaced token detection pre-trained model is utilized
with minorly different task-specific heads for SubTask-A: Multi-class
Classification and SubTask-B: Ranking. Incorporating a pattern-aware ensemble
method, our system achieves a 68.90% accuracy score and 0.8070 spearman's rank
correlation score surpassing the 2nd place with a large margin by 2.7 and 2.2
percent points for SubTask-A and SubTask-B, respectively. Our approach is
simple and easy to implement, and we conducted ablation studies and qualitative
and quantitative analyses for the working strategies used in our system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding. (arXiv:2211.14739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14739">
<div class="article-summary-box-inner">
<span><p>Multimodal named entity recognition (MNER) is a critical step in information
extraction, which aims to detect entity spans and classify them to
corresponding entity types given a sentence-image pair. Existing methods either
(1) obtain named entities with coarse-grained visual clues from attention
mechanisms, or (2) first detect fine-grained visual regions with toolkits and
then recognize named entities. However, they suffer from improper alignment
between entity types and visual regions or error propagation in the two-stage
manner, which finally imports irrelevant visual information into texts. In this
paper, we propose a novel end-to-end framework named MNER-QG that can
simultaneously perform MRC-based multimodal named entity recognition and query
grounding. Specifically, with the assistance of queries, MNER-QG can provide
prior knowledge of entity types and visual regions, and further enhance
representations of both texts and images. To conduct the query grounding task,
we provide manual annotations and weak supervisions that are obtained via
training a highly flexible visual grounding model with transfer learning. We
conduct extensive experiments on two public MNER datasets, Twitter2015 and
Twitter2017. Experimental results show that MNER-QG outperforms the current
state-of-the-art models on the MNER task, and also improves the query grounding
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigation as the Attacker Wishes? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14769">
<div class="article-summary-box-inner">
<span><p>Federated embodied agent learning protects the data privacy of individual
visual environments by keeping data locally at each client (the individual
environment) during training. However, since the local data is inaccessible to
the server under federated learning, attackers may easily poison the training
data of the local client to build a backdoor in the agent without notice.
Deploying such an agent raises the risk of potential harm to humans, as the
attackers may easily navigate and control the agent as they wish via the
backdoor. Towards Byzantine-robust federated embodied agent learning, in this
paper, we study the attack and defense for the task of vision-and-language
navigation (VLN), where the agent is required to follow natural language
instructions to navigate indoor environments. First, we introduce a simple but
effective attack strategy, Navigation as Wish (NAW), in which the malicious
client manipulates local trajectory data to implant a backdoor into the global
model. Results on two VLN datasets (R2R and RxR) show that NAW can easily
navigate the deployed VLN agent regardless of the language instruction, without
affecting its performance on normal test sets. Then, we propose a new
Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated
VLN, which provides the server with a ''prompt'' of the vision-and-language
alignment variance between the benign and malicious clients so that they can be
distinguished during training. We validate the effectiveness of the PBA method
on protecting the global model from the NAW attack, which outperforms other
state-of-the-art defense methods by a large margin in the defense metrics on
R2R and RxR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models. (arXiv:2211.14777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14777">
<div class="article-summary-box-inner">
<span><p>Alignment between image and text has shown promising improvements on
patch-level pre-trained document image models. However, investigating more
effective or finer-grained alignment techniques during pre-training requires a
large amount of computation cost and time. Thus, a question naturally arises:
Could we fine-tune the pre-trained models adaptive to downstream tasks with
alignment objectives and achieve comparable or better performance? In this
paper, we propose a new model architecture with alignment-enriched tuning
(dubbed AETNet) upon pre-trained document image models, to adapt downstream
tasks with the joint task-specific supervised and alignment-aware contrastive
objective. Specifically, we introduce an extra visual transformer as the
alignment-ware image encoder and an extra text transformer as the
alignment-ware text encoder before multimodal fusion. We consider alignment in
the following three aspects: 1) document-level alignment by leveraging the
cross-modal and intra-modal contrastive loss; 2) global-local alignment for
modeling localized and structural information in document images; and 3)
local-level alignment for more accurate patch-level information. Experiments on
various downstream tasks show that AETNet can achieve state-of-the-art
performance on various downstream tasks. Notably, AETNet consistently
outperforms state-of-the-art pre-trained models, such as LayoutLMv3 with
fine-tuning techniques, on three different downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Financial Event Extraction Using Wikipedia-Based Weak Supervision. (arXiv:1911.10783v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.10783">
<div class="article-summary-box-inner">
<span><p>Extraction of financial and economic events from text has previously been
done mostly using rule-based methods, with more recent works employing machine
learning techniques. This work is in line with this latter approach, leveraging
relevant Wikipedia sections to extract weak labels for sentences describing
economic events. Whereas previous weakly supervised approaches required a
knowledge-base of such events, or corresponding financial figures, our approach
requires no such additional data, and can be employed to extract economic
events related to companies which are not even mentioned in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation. (arXiv:2004.08694v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08694">
<div class="article-summary-box-inner">
<span><p>Question Generation (QG) is fundamentally a simple syntactic transformation;
however, many aspects of semantics influence what questions are good to form.
We implement this observation by developing SynQG, a set of transparent
syntactic rules leveraging universal dependencies, shallow semantic parsing,
lexical resources, and custom rules which transform declarative sentences into
question-answer pairs. We utilize PropBank argument descriptions and VerbNet
state predicates to incorporate shallow semantic content, which helps generate
questions of a descriptive nature and produce inferential and semantically
richer questions than existing systems. In order to improve syntactic fluency
and eliminate grammatically incorrect questions, we employ back-translation
over the output of these syntactic rules. A set of crowd-sourced evaluations
shows that our system can generate a larger number of highly grammatical and
relevant questions than previous QG systems and that back-translation
drastically improves grammaticality at a slight cost of generating irrelevant
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forecasting financial markets with semantic network analysis in the COVID-19 crisis. (arXiv:2009.04975v3 [q-fin.GN] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.04975">
<div class="article-summary-box-inner">
<span><p>This paper uses a new textual data index for predicting stock market data.
The index is applied to a large set of news to evaluate the importance of one
or more general economic-related keywords appearing in the text. The index
assesses the importance of the economic-related keywords, based on their
frequency of use and semantic network position. We apply it to the Italian
press and construct indices to predict Italian stock and bond market returns
and volatilities in a recent sample period, including the COVID-19 crisis. The
evidence shows that the index captures the different phases of financial time
series well. Moreover, results indicate strong evidence of predictability for
bond market data, both returns and volatilities, short and long maturities, and
stock market volatility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierachical Delta-Attention Method for Multimodal Fusion. (arXiv:2011.10916v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10916">
<div class="article-summary-box-inner">
<span><p>In vision and linguistics; the main input modalities are facial expressions,
speech patterns, and the words uttered. The issue with analysis of any one mode
of expression (Visual, Verbal or Vocal) is that lot of contextual information
can get lost. This asks researchers to inspect multiple modalities to get a
thorough understanding of the cross-modal dependencies and temporal context of
the situation to analyze the expression. This work attempts at preserving the
long-range dependencies within and across different modalities, which would be
bottle-necked by the use of recurrent networks and adds the concept of
delta-attention to focus on local differences per modality to capture the
idiosyncrasy of different people. We explore a cross-attention fusion technique
to get the global view of the emotion expressed through these
delta-self-attended modalities, in order to fuse all the local nuances and
global context together. The addition of attention is new to the multi-modal
fusion field and currently being scrutinized for on what stage the attention
mechanism should be used, this work achieves competitive accuracy for overall
and per-class classification which is close to the current state-of-the-art
with almost half number of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment Analysis. (arXiv:2204.00791v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00791">
<div class="article-summary-box-inner">
<span><p>As an extensive research in the field of natural language processing (NLP),
aspect-based sentiment analysis (ABSA) is the task of predicting the sentiment
expressed in a text relative to the corresponding aspect. Unfortunately, most
languages lack sufficient annotation resources, thus more and more recent
researchers focus on cross-lingual aspect-based sentiment analysis (XABSA).
However, most recent researches only concentrate on cross-lingual data
alignment instead of model alignment. To this end, we propose a novel
framework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based
Sentiment Analysis. Based on contrastive learning, we close the distance
between samples with the same label in different semantic spaces, thus
achieving a convergence of semantic spaces of different languages.
Specifically, we design two contrastive strategies, token level contrastive
learning of token embeddings (TL-CTE) and sentiment level contrastive learning
of token embeddings (SL-CTE), to regularize the semantic space of source and
target language to be more uniform. Since our framework can receive datasets in
multiple languages during training, our framework can be adapted not only for
XABSA task but also for multilingual aspect-based sentiment analysis (MABSA).
To further improve the performance of our model, we perform knowledge
distillation technology leveraging data from unlabeled target language. In the
distillation XABSA task, we further explore the comparative effectiveness of
different data (source dataset, translated dataset, and code-switched dataset).
The results demonstrate that the proposed method has a certain improvement in
the three tasks of XABSA, distillation XABSA and MABSA. For reproducibility,
our code for this paper is available at https://github.com/GKLMIP/CL-XABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07496">
<div class="article-summary-box-inner">
<span><p>We propose a simple and effective re-ranking method for improving passage
retrieval in open question answering. The re-ranker re-scores retrieved
passages with a zero-shot question generation model, which uses a pre-trained
language model to compute the probability of the input question conditioned on
a retrieved passage. This approach can be applied on top of any retrieval
method (e.g. neural or keyword-based), does not require any domain- or
task-specific training (and therefore is expected to generalize better to data
distribution shifts), and provides rich cross-attention between query and
passage (i.e. it must explain every token in the question). When evaluated on a
number of open-domain retrieval datasets, our re-ranker improves strong
unsupervised retrieval models by 6%-18% absolute and strong supervised models
by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new
state-of-the-art results on full open-domain question answering by simply
adding the new re-ranker to existing models with no further changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unbiased and Efficient Sampling of Dependency Trees. (arXiv:2205.12621v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12621">
<div class="article-summary-box-inner">
<span><p>Most computational models of dependency syntax consist of distributions over
spanning trees. However, the majority of dependency treebanks require that
every valid dependency tree has a single edge coming out of the ROOT node, a
constraint that is not part of the definition of spanning trees. For this
reason all standard inference algorithms for spanning trees are suboptimal for
inference over dependency trees.
</p>
<p>Zmigrod et al. (2021b) proposed algorithms for sampling with and without
replacement from the dependency tree distribution that incorporate the
single-root constraint. In this paper we show that their fastest algorithm for
sampling with replacement, Wilson-RC, is in fact producing biased samples and
we provide two alternatives that are unbiased. Additionally, we propose two
algorithms (one incremental, one parallel) that reduce the asymptotic runtime
of algorithm for sampling k trees without replacement to O(kn3). These
algorithms are both asymptotically and practically more efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIRECTOR: Generator-Classifiers For Supervised Language Modeling. (arXiv:2206.07694v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07694">
<div class="article-summary-box-inner">
<span><p>Current language models achieve low perplexity but their resulting
generations still suffer from toxic responses, repetitiveness and
contradictions. The standard language modeling setup fails to address these
issues. In this paper, we introduce a new architecture, {\sc Director}, that
consists of a unified generator-classifier with both a language modeling and a
classification head for each output token. Training is conducted jointly using
both standard language modeling data, and data labeled with desirable and
undesirable sequences. Experiments in several settings show that the model has
competitive training and decoding speed compared to standard language models
while yielding superior results, alleviating known issues while maintaining
generation quality. It also outperforms existing model guiding approaches in
terms of both accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality. (arXiv:2207.07036v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07036">
<div class="article-summary-box-inner">
<span><p>While audio-visual speech models can yield superior performance and
robustness compared to audio-only models, their development and adoption are
hindered by the lack of labeled and unlabeled audio-visual data and the cost to
deploy one model per modality. In this paper, we present u-HuBERT, a
self-supervised pre-training framework that can leverage both multimodal and
unimodal speech with a unified masked cluster prediction objective. By
utilizing modality dropout during pre-training, we demonstrate that a single
fine-tuned model can achieve performance on par or better than the
state-of-the-art modality-specific models. Moreover, our model fine-tuned only
on audio can perform well with audio-visual and visual speech input, achieving
zero-shot modality generalization for multiple speech processing tasks. In
particular, our single model yields 1.2%/1.4%/27.2% speech recognition word
error rate on LRS3 with audio-visual/audio/visual input. Codes and models are
available at https://github.com/facebookresearch/av_hubert
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSRFormer: Grounded Situation Recognition Transformer with Alternate Semantic Attention Refinement. (arXiv:2208.08965v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.08965">
<div class="article-summary-box-inner">
<span><p>Grounded Situation Recognition (GSR) aims to generate structured semantic
summaries of images for "human-like" event understanding. Specifically, GSR
task not only detects the salient activity verb (e.g. buying), but also
predicts all corresponding semantic roles (e.g. agent and goods). Inspired by
object detection and image captioning tasks, existing methods typically employ
a two-stage framework: 1) detect the activity verb, and then 2) predict
semantic roles based on the detected verb. Obviously, this illogical framework
constitutes a huge obstacle to semantic understanding. First, pre-detecting
verbs solely without semantic roles inevitably fails to distinguish many
similar daily activities (e.g., offering and giving, buying and selling).
Second, predicting semantic roles in a closed auto-regressive manner can hardly
exploit the semantic relations among the verb and roles. To this end, in this
paper we propose a novel two-stage framework that focuses on utilizing such
bidirectional relations within verbs and roles. In the first stage, instead of
pre-detecting the verb, we postpone the detection step and assume a pseudo
label, where an intermediate representation for each corresponding semantic
role is learned from images. In the second stage, we exploit transformer layers
to unearth the potential semantic relations within both verbs and semantic
roles. With the help of a set of support images, an alternate learning scheme
is designed to simultaneously optimize the results: update the verb using nouns
corresponding to the image, and update nouns using verbs from support images.
Extensive experimental results on challenging SWiG benchmarks show that our
renovated framework outperforms other state-of-the-art methods under various
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unit Testing for Concepts in Neural Networks. (arXiv:2208.10244v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.10244">
<div class="article-summary-box-inner">
<span><p>Many complex problems are naturally understood in terms of symbolic concepts.
For example, our concept of "cat" is related to our concepts of "ears" and
"whiskers" in a non-arbitrary way. Fodor (1998) proposes one theory of
concepts, which emphasizes symbolic representations related via constituency
structures. Whether neural networks are consistent with such a theory is open
for debate. We propose unit tests for evaluating whether a system's behavior is
consistent with several key aspects of Fodor's criteria. Using a simple visual
concept learning task, we evaluate several modern neural architectures against
this specification. We find that models succeed on tests of groundedness,
modularlity, and reusability of concepts, but that important questions about
causality remain open. Resolving these will require new methods for analyzing
models' internal states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models. (arXiv:2208.11445v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.11445">
<div class="article-summary-box-inner">
<span><p>The ability to extrapolate, i.e., to make predictions on sequences that are
longer than those presented as training examples, is a challenging problem for
current deep learning models. Recent work shows that this limitation persists
in state-of-the-art Transformer-based models. Most solutions to this problem
use specific architectures or training methods that do not generalize to other
tasks. We demonstrate that large language models can succeed in extrapolation
without modifying their architecture or training procedure. Our experimental
results show that generating step-by-step rationales and introducing marker
tokens are both required for effective extrapolation. First, we induce a
language model to produce step-by-step rationales before outputting the answer
to effectively communicate the task to the model. However, as sequences become
longer, we find that current models struggle to keep track of token positions.
To address this issue, we interleave output tokens with markup tokens that act
as explicit positional and counting symbols. Our findings show how these two
complementary approaches enable remarkable sequence extrapolation and highlight
a limitation of current architectures to effectively generalize without
explicit surface form guidance. Code available at
https://github.com/MirelleB/induced-rationales-markup-tokens
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extend and Explain: Interpreting Very Long Language Models. (arXiv:2209.01174v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.01174">
<div class="article-summary-box-inner">
<span><p>While Transformer language models (LMs) are state-of-the-art for information
extraction, long text introduces computational challenges requiring suboptimal
preprocessing steps or alternative model architectures. Sparse attention LMs
can represent longer sequences, overcoming performance hurdles. However, it
remains unclear how to explain predictions from these models, as not all tokens
attend to each other in the self-attention layers, and long sequences pose
computational challenges for explainability algorithms when runtime depends on
document length. These challenges are severe in the medical context where
documents can be very long, and machine learning (ML) models must be auditable
and trustworthy. We introduce a novel Masked Sampling Procedure (MSP) to
identify the text blocks that contribute to a prediction, apply MSP in the
context of predicting diagnoses from medical text, and validate our approach
with a blind review by two clinicians. Our method identifies about 1.7x more
clinically informative text blocks than the previous state-of-the-art, runs up
to 100x faster, and is tractable for generating important phrase pairs. MSP is
particularly well-suited to long LMs but can be applied to any text classifier.
We provide a general implementation of MSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvNeXt Based Neural Network for Audio Anti-Spoofing. (arXiv:2209.06434v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06434">
<div class="article-summary-box-inner">
<span><p>Automatic speaker verification (ASV) has been widely used in the real life
for identity authentication. However, with the rapid development of speech
conversion and speech synthesis algorithms, ASV systems are vulnerable to
spoofing attacks. In recent years, there have many works about synthetic speech
detection, researchers had proposed a number of anti-spoofing methods based on
hand-crafted features to improve the detection accuracy and robustness of ASV
systems. However, using hand-crafted features rather than raw waveform would
lose certain information for anti-spoofing, which will reduce the detection
performance of the system. Inspired by the promising performance of ConvNeXt in
image classification tasks, we revise the ConvNeXt network architecture
accordingly for spoof attacks detection task and propose a lightweight
end-to-end anti-spoofing model. By integrating the revised architecture with
the channel attention block and using the focal loss function, the proposed
model can focus on the most informative sub-bands of speech representations and
the difficult samples that are hard for models to classify. Experiments show
that our proposed best single system could achieve an equal error rate of 0.75%
and min-tDCF of 0.0212 for the ASVSpoof 2019 LA evaluation dataset, which
outperforms the state-of-the-art systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attention Matrix for Every Decision: Faithfulness-based Arbitration Among Multiple Attention-Based Interpretations of Transformers in Text Classification. (arXiv:2209.10876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10876">
<div class="article-summary-box-inner">
<span><p>Transformers are widely used in natural language processing, where they
consistently achieve state-of-the-art performance. This is mainly due to their
attention-based architecture, which allows them to model rich linguistic
relations between (sub)words. However, transformers are difficult to interpret.
Being able to provide reasoning for its decisions is an important property for
a model in domains where human lives are affected. With transformers finding
wide use in such fields, the need for interpretability techniques tailored to
them arises. We propose a new technique that selects the most faithful
attention-based interpretation among the several ones that can be obtained by
combining different head, layer and matrix operations. In addition, two
variations are introduced towards (i) reducing the computational complexity,
thus being faster and friendlier to the environment, and (ii) enhancing the
performance in multi-label data. We further propose a new faithfulness metric
that is more suitable for transformer models and exhibits high correlation with
the area under the precision-recall curve based on ground truth rationales. We
validate the utility of our contributions with a series of quantitative and
qualitative experiments on seven datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory in humans and deep language models: Linking hypotheses for model augmentation. (arXiv:2210.01869v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01869">
<div class="article-summary-box-inner">
<span><p>The computational complexity of the self-attention mechanism in Transformer
models significantly limits their ability to generalize over long temporal
durations. Memory-augmentation, or the explicit storing of past information in
external memory for subsequent predictions, has become a constructive avenue
for mitigating this limitation. We argue that memory-augmented Transformers can
benefit substantially from considering insights from the memory literature in
humans. We detail an approach for integrating evidence from the human memory
system through the specification of cross-domain linking hypotheses. We then
provide an empirical demonstration to evaluate the use of surprisal as a
linking hypothesis, and further identify the limitations of this approach to
inform future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReAct: Synergizing Reasoning and Acting in Language Models. (arXiv:2210.03629v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03629">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation. (arXiv:2210.15461v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15461">
<div class="article-summary-box-inner">
<span><p>Multimodal Machine Translation (MMT) focuses on enhancing text-only
translation with visual features, which has attracted considerable attention
from both natural language processing and computer vision communities. Recent
advances still struggle to train a separate model for each language pair, which
is costly and unaffordable when the number of languages increases in the real
world. In other words, the multilingual multimodal machine translation
(Multilingual MMT) task has not been investigated, which aims to handle the
aforementioned issues by providing a shared semantic space for multiple
languages. Besides, the image modality has no language boundaries, which is
superior to bridging the semantic gap between languages. To this end, we first
propose the Multilingual MMT task by establishing two new Multilingual MMT
benchmark datasets covering seven languages. Then, an effective baseline LVP-M3
using visual prompts is proposed to support translations between different
languages, which includes three stages (token encoding, language-aware visual
prompt generation, and language translation). Extensive experimental results on
our constructed benchmark datasets demonstrate the effectiveness of LVP-M3
method for Multilingual MMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intriguing Properties of Compression on Multilingual Models. (arXiv:2211.02738v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02738">
<div class="article-summary-box-inner">
<span><p>Multilingual models are often particularly dependent on scaling to generalize
to a growing number of languages. Compression techniques are widely relied upon
to reconcile the growth in model size with real world resource constraints, but
compression can have a disparate effect on model performance for low-resource
languages. It is thus crucial to understand the trade-offs between scale,
multilingualism, and compression. In this work, we propose an experimental
framework to characterize the impact of sparsifying multilingual pre-trained
language models during fine-tuning. Applying this framework to mBERT named
entity recognition models across 40 languages, we find that compression confers
several intriguing and previously unknown generalization properties. In
contrast to prior findings, we find that compression may improve model
robustness over dense models. We additionally observe that under certain
sparsification regimes compression may aid, rather than disproportionately
impact the performance of low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech separation with large-scale self-supervised learning. (arXiv:2211.05172v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05172">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) methods such as WavLM have shown promising
speech separation (SS) results in small-scale simulation-based experiments. In
this work, we extend the exploration of the SSL-based SS by massively scaling
up both the pre-training data (more than 300K hours) and fine-tuning data (10K
hours). We also investigate various techniques to efficiently integrate the
pre-trained model with the SS network under a limited computation budget,
including a low frame rate SSL model training setup and a fine-tuning scheme
using only the part of the pre-trained model. Compared with a supervised
baseline and the WavLM-based SS model using feature embeddings obtained with
the previously released 94K hours trained WavLM, our proposed model obtains
15.9% and 11.2% of relative word error rate (WER) reductions, respectively, for
a simulated far-field speech mixture test set. For conversation transcription
on real meeting recordings using continuous speech separation, the proposed
model achieves 6.8% and 10.6% of relative WER reductions over the purely
supervised baseline on AMI and ICSI evaluation sets, respectively, while
reducing the computational cost by 38%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning. (arXiv:2211.05610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.05610">
<div class="article-summary-box-inner">
<span><p>Current pre-trained language models rely on large datasets for achieving
state-of-the-art performance. However, past research has shown that not all
examples in a dataset are equally important during training. In fact, it is
sometimes possible to prune a considerable fraction of the training set while
maintaining the test performance. Established on standard vision benchmarks,
two gradient-based scoring metrics for finding important examples are GraNd and
its estimated version, EL2N. In this work, we employ these two metrics for the
first time in NLP. We demonstrate that these metrics need to be computed after
at least one epoch of fine-tuning and they are not reliable in early steps.
Furthermore, we show that by pruning a small portion of the examples with the
highest GraNd/EL2N scores, we can not only preserve the test accuracy, but also
surpass it. This paper details adjustments and implementation choices which
enable GraNd and EL2N to be applied to NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10438">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) show excellent performance but are compute- and
memory-intensive. Quantization can reduce memory and accelerate inference.
However, for LLMs beyond 100 billion parameters, existing methods cannot
maintain accuracy or do not run efficiently on hardware. We propose
SmoothQuant, a training-free, accuracy-preserving, and general-purpose
post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit
activation (W8A8) quantization for LLMs that can be implemented efficiently. We
observe that systematic outliers appear at fixed activation channels. Based on
the fact that weights are easy to quantize while activations are not,
SmoothQuant smooths the activation outliers by offline migrating the
quantization difficulty from activations to weights with a mathematically
equivalent transformation. SmoothQuant enables an INT8 quantization of both
weights and activations for all the GEMMs in LLMs, including OPT-175B,
BLOOM-176B, and GLM-130B. SmoothQuant has better hardware efficiency than
existing techniques using mixed-precision activation quantization or
weight-only quantization. We demonstrate up to 1.56x speedup and 2x memory
reduction for LLMs with negligible loss in accuracy. Thanks to the
hardware-friendly design, we integrate SmoothQuant into FasterTransformer, a
state-of-the-art LLM serving framework, and achieve faster inference speed with
half the number of GPUs compared to FP16. Our work offers a turn-key solution
that reduces hardware costs and democratizes LLMs. Code is available at:
https://github.com/mit-han-lab/smoothquant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-HindBERT and DevBERT: Pre-Trained BERT Transformer models for Devanagari based Hindi and Marathi Languages. (arXiv:2211.11418v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11418">
<div class="article-summary-box-inner">
<span><p>The monolingual Hindi BERT models currently available on the model hub do not
perform better than the multi-lingual models on downstream tasks. We present
L3Cube-HindBERT, a Hindi BERT model pre-trained on Hindi monolingual corpus.
</p>
<p>Further, since Indic languages, Hindi and Marathi share the Devanagari
script, we train a single model for both languages. We release DevBERT, a
Devanagari BERT model trained on both Marathi and Hindi monolingual datasets.
We evaluate these models on downstream Hindi and Marathi text classification
and named entity recognition tasks. The HindBERT and DevBERT-based models show
superior performance compared to their multi-lingual counterparts. These models
are shared at https://huggingface.co/l3cube-pune .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acceptability Judgements via Examining the Topology of Attention Maps. (arXiv:2205.09630v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09630">
<div class="article-summary-box-inner">
<span><p>The role of the attention mechanism in encoding linguistic knowledge has
received special interest in NLP. However, the ability of the attention heads
to judge the grammatical acceptability of a sentence has been underexplored.
This paper approaches the paradigm of acceptability judgments with topological
data analysis (TDA), showing that the geometric properties of the attention
graph can be efficiently exploited for two standard practices in linguistics:
binary judgments and linguistic minimal pairs. Topological features enhance the
BERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three
languages (English, Italian, and Swedish). By revealing the topological
discrepancy between attention maps of minimal pairs, we achieve the human-level
performance on the BLiMP benchmark, outperforming nine statistical and
Transformer LM baselines. At the same time, TDA provides the foundation for
analyzing the linguistic functions of attention heads and interpreting the
correspondence between the graph features and grammatical phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review. (arXiv:2211.09511v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09511">
<div class="article-summary-box-inner">
<span><p>Speech is the fundamental means of communication between humans. The advent
of AI and sophisticated speech technologies have led to the rapid proliferation
of human-to-computer-based interactions, fueled primarily by Automatic Speech
Recognition (ASR) systems. ASR systems normally take human speech in the form
of audio and convert it into words, but for some users, it cannot decode the
speech, and any output text is filled with errors that are incomprehensible to
the human reader. These systems do not work equally for everyone and actually
hinder the productivity of some users. In this paper, we present research that
addresses ASR biases against gender, race, and the sick and disabled, while
exploring studies that propose ASR debiasing techniques for mitigating these
discriminations. We also discuss techniques for designing a more accessible and
inclusive ASR technology. For each approach surveyed, we also provide a summary
of the investigation and methods applied, the ASR systems and corpora used, and
the research findings, and highlight their strengths and/or weaknesses.
Finally, we propose future opportunities for Natural Language Processing
researchers to explore in the next level creation of ASR technologies.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-29 23:14:23.932848091 UTC">2022-11-29 23:14:23 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>