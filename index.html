<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-17T01:30:00Z">08-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07931">
<div class="article-summary-box-inner">
<span><p>Self-supervised and language-supervised image models contain rich knowledge
of the world that is important for generalization. Many robotic tasks, however,
require a detailed understanding of 3D geometry, which is often lacking in 2D
image features. This work bridges this 2D-to-3D gap for robotic manipulation by
leveraging distilled feature fields to combine accurate 3D geometry with rich
semantics from 2D foundation models. We present a few-shot learning method for
6-DOF grasping and placing that harnesses these strong spatial and semantic
priors to achieve in-the-wild generalization to unseen objects. Using features
distilled from a vision-language model, CLIP, we present a way to designate
novel objects for manipulation via free-text natural language, and demonstrate
its ability to generalize to unseen expressions and novel categories of
objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment. (arXiv:2308.07933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07933">
<div class="article-summary-box-inner">
<span><p>Using picture description speech for dementia detection has been studied for
30 years. Despite the long history, previous models focus on identifying the
differences in speech patterns between healthy subjects and patients with
dementia but do not utilize the picture information directly. In this paper, we
propose the first dementia detection models that take both the picture and the
description texts as inputs and incorporate knowledge from large pre-trained
image-text alignment models. We observe the difference between dementia and
healthy samples in terms of the text's relevance to the picture and the focused
area of the picture. We thus consider such a difference could be used to
enhance dementia detection accuracy. Specifically, we use the text's relevance
to the picture to rank and filter the sentences of the samples. We also
identified focused areas of the picture as topics and categorized the sentences
according to the focused areas. We propose three advanced models that
pre-processed the samples based on their relevance to the picture, sub-image,
and focused areas. The evaluation results show that our advanced models, with
knowledge of the picture and large image-text alignment models, achieve
state-of-the-art performance with the best detection accuracy at 83.44%, which
is higher than the text-only baseline model at 79.91%. Lastly, we visualize the
sample and picture results to explain the advantages of our models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Sentiment Analysis in the Financial Domain with ChatGPT. (arXiv:2308.07935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07935">
<div class="article-summary-box-inner">
<span><p>Financial sentiment analysis plays a crucial role in decoding market trends
and guiding strategic trading decisions. Despite the deployment of advanced
deep learning techniques and language models to refine sentiment analysis in
finance, this study breaks new ground by investigating the potential of large
language models, particularly ChatGPT 3.5, in financial sentiment analysis,
with a strong emphasis on the foreign exchange market (forex). Employing a
zero-shot prompting approach, we examine multiple ChatGPT prompts on a
meticulously curated dataset of forex-related news headlines, measuring
performance using metrics such as precision, recall, f1-score, and Mean
Absolute Error (MAE) of the sentiment class. Additionally, we probe the
correlation between predicted sentiment and market returns as an additional
evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment
analysis model for financial texts, exhibited approximately 35\% enhanced
performance in sentiment classification and a 36\% higher correlation with
market returns. By underlining the significance of prompt engineering,
particularly in zero-shot contexts, this study spotlights ChatGPT's potential
to substantially boost sentiment analysis in financial applications. By sharing
the utilized dataset, our intention is to stimulate further research and
advancements in the field of financial services.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Testing and Improvement of Named Entity Recognition Systems. (arXiv:2308.07937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07937">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) systems have seen rapid progress in recent
years due to the development of deep neural networks. These systems are widely
used in various natural language processing applications, such as information
extraction, question answering, and sentiment analysis. However, the complexity
and intractability of deep neural networks can make NER systems unreliable in
certain circumstances, resulting in incorrect predictions. For example, NER
systems may misidentify female names as chemicals or fail to recognize the
names of minority groups, leading to user dissatisfaction. To tackle this
problem, we introduce TIN, a novel, widely applicable approach for
automatically testing and repairing various NER systems. The key idea for
automated testing is that the NER predictions of the same named entities under
similar contexts should be identical. The core idea for automated repairing is
that similar named entities should have the same NER prediction under the same
context. We use TIN to test two SOTA NER models and two commercial NER APIs,
i.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues
reported by TIN and find that 702 are erroneous issues, leading to high
precision (85.0%-93.4%) across four categories of NER errors: omission,
over-labeling, incorrect category, and range error. For automated repairing,
TIN achieves a high error reduction rate (26.8%-50.6%) over the four systems
under test, which successfully repairs 1,056 out of the 1,877 reported NER
errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teach LLMs to Personalize -- An Approach inspired by Writing Education. (arXiv:2308.07968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07968">
<div class="article-summary-box-inner">
<span><p>Personalized text generation is an emerging research area that has attracted
much attention in recent years. Most studies in this direction focus on a
particular domain by designing bespoke features or models. In this work, we
propose a general approach for personalized text generation using large
language models (LLMs). Inspired by the practice of writing education, we
develop a multistage and multitask framework to teach LLMs for personalized
generation. In writing instruction, the task of writing from sources is often
decomposed into multiple steps that involve finding, evaluating, summarizing,
synthesizing, and integrating information. Analogously, our approach to
personalized text generation consists of multiple stages: retrieval, ranking,
summarization, synthesis, and generation. In addition, we introduce a multitask
setting that helps the model improve its generation ability further, which is
inspired by the observation in education that a student's reading proficiency
and writing ability are often correlated. We evaluate our approach on three
public datasets, each of which covers a different and representative domain.
Our results show significant improvements over a variety of baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction. (arXiv:2308.07971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07971">
<div class="article-summary-box-inner">
<span><p>Automatic assessment of the quality of scholarly documents is a difficult
task with high potential impact. Multimodality, in particular the addition of
visual information next to text, has been shown to improve the performance on
scholarly document quality prediction (SDQP) tasks. We propose the multimodal
predictive model MultiSChuBERT. It combines a textual model based on chunking
full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with
a visual model based on Inception V3.Our work contributes to the current
state-of-the-art in SDQP in three ways. First, we show that the method of
combining visual and textual embeddings can substantially influence the
results. Second, we demonstrate that gradual-unfreezing of the weights of the
visual sub-model, reduces its tendency to ovefit the data, improving results.
Third, we show the retained benefit of multimodality when replacing standard
BERT$_{\textrm{BASE}}$ embeddings with more recent state-of-the-art text
embedding models.
</p>
<p>Using BERT$_{\textrm{BASE}}$ embeddings, on the (log) number of citations
prediction task with the ACL-BiblioMetry dataset, our MultiSChuBERT
(text+visual) model obtains an $R^{2}$ score of 0.454 compared to 0.432 for the
SChuBERT (text only) model. Similar improvements are obtained on the PeerRead
accept/reject prediction task. In our experiments using SciBERT, scincl,
SPECTER and SPECTER2.0 embeddings, we show that each of these tailored
embeddings adds further improvements over the standard BERT$_{\textrm{BASE}}$
embeddings, with the SPECTER2.0 embeddings performing best.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Beware of deception": Detecting Half-Truth and Debunking it through Controlled Claim Editing. (arXiv:2308.07973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07973">
<div class="article-summary-box-inner">
<span><p>The prevalence of half-truths, which are statements containing some truth but
that are ultimately deceptive, has risen with the increasing use of the
internet. To help combat this problem, we have created a comprehensive pipeline
consisting of a half-truth detection model and a claim editing model. Our
approach utilizes the T5 model for controlled claim editing; "controlled" here
means precise adjustments to select parts of a claim. Our methodology achieves
an average BLEU score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of
85% on edited claims. Significantly, our T5-based approach outperforms other
Language Models such as GPT2, RoBERTa, PEGASUS, and Tailor, with average
improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively.
By extending the LIAR PLUS dataset, we achieve an F1 score of 82% for the
half-truth detection model, setting a new benchmark in the field. While
previous attempts have been made at half-truth detection, our approach is, to
the best of our knowledge, the first to attempt to debunk half-truths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anaphoric Structure Emerges Between Neural Networks. (arXiv:2308.07984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07984">
<div class="article-summary-box-inner">
<span><p>Pragmatics is core to natural language, enabling speakers to communicate
efficiently with structures like ellipsis and anaphora that can shorten
utterances without loss of meaning. These structures require a listener to
interpret an ambiguous form - like a pronoun - and infer the speaker's intended
meaning - who that pronoun refers to. Despite potential to introduce ambiguity,
anaphora is ubiquitous across human language. In an effort to better understand
the origins of anaphoric structure in natural language, we look to see if
analogous structures can emerge between artificial neural networks trained to
solve a communicative task. We show that: first, despite the potential for
increased ambiguity, languages with anaphoric structures are learnable by
neural models. Second, anaphoric structures emerge between models 'naturally'
without need for additional constraints. Finally, introducing an explicit
efficiency pressure on the speaker increases the prevalence of these
structures. We conclude that certain pragmatic structures straightforwardly
emerge between neural networks, without explicit efficiency pressures, but that
the competing needs of speakers and listeners conditions the degree and nature
of their emergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations. (arXiv:2308.08027v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08027">
<div class="article-summary-box-inner">
<span><p>Conventional keyword search systems operate on automatic speech recognition
(ASR) outputs, which causes them to have a complex indexing and search
pipeline. This has led to interest in ASR-free approaches to simplify the
search procedure. We recently proposed a neural ASR-free keyword search model
which achieves competitive performance while maintaining an efficient and
simplified pipeline, where queries and documents are encoded with a pair of
recurrent neural network encoders and the encodings are combined with a
dot-product. In this article, we extend this work with multilingual pretraining
and detailed analysis of the model. Our experiments show that the proposed
multilingual training significantly improves the model performance and that
despite not matching a strong ASR-based conventional keyword search system for
short queries and queries comprising in-vocabulary words, the proposed model
outperforms the ASR-based system for long queries and queries that do not
appear in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Artificial Populations to Study Psychological Phenomena in Neural Models. (arXiv:2308.08032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08032">
<div class="article-summary-box-inner">
<span><p>The recent proliferation of research into transformer based natural language
processing has led to a number of studies which attempt to detect the presence
of human-like cognitive behavior in the models. We contend that, as is true of
human psychology, the investigation of cognitive behavior in language models
must be conducted in an appropriate population of an appropriate size for the
results to be meaningful. We leverage work in uncertainty estimation in a novel
approach to efficiently construct experimental populations. The resultant tool,
PopulationLM, has been made open source. We provide theoretical grounding in
the uncertainty estimation literature and motivation from current cognitive
work regarding language models. We discuss the methodological lessons from
other scientific communities and attempt to demonstrate their application to
two artificial population studies. Through population based experimentation we
find that language models exhibit behavior consistent with typicality effects
among categories highly represented in training. However, we find that language
models don't tend to exhibit structural priming effects. Generally, our results
show that single models tend to over estimate the presence of cognitive
behaviors in neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08043">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), such as ChatGPT, are becoming increasingly
sophisticated, demonstrating capabilities that closely resemble those of
humans. These AI models are playing an essential role in assisting humans with
a wide array of tasks in daily life. A significant application of AI is its use
as a chat agent, responding to human inquiries across various domains. Current
LLMs have shown proficiency in answering general questions. However, basic
question-answering dialogue often falls short in complex diagnostic scenarios,
such as legal or medical consultations. These scenarios typically necessitate
Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively
pose questions and guide users towards specific task completion. Previous
fine-tuning models have underperformed in TOD, and current LLMs do not
inherently possess this capability. In this paper, we introduce DiagGPT
(Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD
scenarios. Our experiments reveal that DiagGPT exhibits outstanding performance
in conducting TOD with users, demonstrating its potential for practical
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models. (arXiv:2308.08061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08061">
<div class="article-summary-box-inner">
<span><p>When deploying machine learning models in production for any
product/application, there are three properties that are commonly desired.
First, the models should be generalizable, in that we can extend it to further
use cases as our knowledge of the domain area develops. Second they should be
evaluable, so that there are clear metrics for performance and the calculation
of those metrics in production settings are feasible. Finally, the deployment
should be cost-optimal as far as possible. In this paper we propose that these
three objectives (i.e. generalization, evaluation and cost-optimality) can
often be relatively orthogonal and that for large language models, despite
their performance over conventional NLP models, enterprises need to carefully
assess all the three factors before making substantial investments in this
technology. We propose a framework for generalization, evaluation and
cost-modeling specifically tailored to large language models, offering insights
into the intricacies of development, deployment and management for these large
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation. (arXiv:2308.08090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08090">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have been widely used in various applications
but are known to suffer from issues related to untruthfulness and toxicity.
While parameter-efficient modules (PEMs) have demonstrated their effectiveness
in equipping models with new skills, leveraging PEMs for deficiency unlearning
remains underexplored. In this work, we propose a PEMs operation approach,
namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and
detoxification of LLMs through the integration of ``expert'' PEM and
``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable
capabilities due to their proficiency in generating fabricated content, which
necessitates language modeling and logical narrative competence. Rather than
merely negating the parameters, our approach involves extracting and
eliminating solely the deficiency capability within anti-expert PEM while
preserving the general capabilities. To evaluate the effectiveness of our
approach in terms of truthfulness and detoxification, we conduct extensive
experiments on LLMs, encompassing additional abilities such as language
modeling and mathematical reasoning. Our empirical results demonstrate that our
approach effectively improves truthfulness and detoxification, while largely
preserving the fundamental abilities of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals. (arXiv:2308.08125v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08125">
<div class="article-summary-box-inner">
<span><p>Millimeter wave (mmWave) based speech recognition provides more possibility
for audio-related applications, such as conference speech transcription and
eavesdropping. However, considering the practicality in real scenarios, latency
and recognizable vocabulary size are two critical factors that cannot be
overlooked. In this paper, we propose Radio2Text, the first mmWave-based system
for streaming automatic speech recognition (ASR) with a vocabulary size
exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer
that is capable of effectively learning representations of speech-related
features, paving the way for streaming ASR with a large vocabulary. To
alleviate the deficiency of streaming networks unable to access entire future
inputs, we propose the Guidance Initialization that facilitates the transfer of
feature knowledge related to the global context from the non-streaming
Transformer to the tailored streaming Transformer through weight inheritance.
Further, we propose a cross-modal structure based on knowledge distillation
(KD), named cross-modal KD, to mitigate the negative effect of low quality
mmWave signals on recognition performance. In the cross-modal KD, the audio
streaming Transformer provides feature and response guidance that inherit
fruitful and accurate speech information to supervise the training of the
tailored radio streaming Transformer. The experimental results show that our
Radio2Text can achieve a character error rate of 5.7% and a word error rate of
9.4% for the recognition of a vocabulary consisting of over 13,000 words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation. (arXiv:2308.08147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08147">
<div class="article-summary-box-inner">
<span><p>Dialogue systems for Automatic Differential Diagnosis (ADD) have a wide range
of real-life applications. These dialogue systems are promising for providing
easy access and reducing medical costs. Building end-to-end ADD dialogue
systems requires dialogue training datasets. However, to the best of our
knowledge, there is no publicly available ADD dialogue dataset in English
(although non-English datasets exist). Driven by this, we introduce MDDial, the
first differential diagnosis dialogue dataset in English which can aid to build
and evaluate end-to-end ADD dialogue systems. Additionally, earlier studies
present the accuracy of diagnosis and symptoms either individually or as a
combined weighted score. This method overlooks the connection between the
symptoms and the diagnosis. We introduce a unified score for the ADD system
that takes into account the interplay between symptoms and diagnosis. This
score also indicates the system's reliability. To the end, we train two
moderate-size of language models on MDDial. Our experiments suggest that while
these language models can perform well on many natural language understanding
tasks, including dialogue tasks in the general domain, they struggle to relate
relevant symptoms and disease and thus have poor performance on MDDial. MDDial
will be released publicly to aid the study of ADD dialogue research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Training of NMT Model with Data Sorting. (arXiv:2308.08153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08153">
<div class="article-summary-box-inner">
<span><p>The Transformer model has revolutionized Natural Language Processing tasks
such as Neural Machine Translation, and many efforts have been made to study
the Transformer architecture, which increased its efficiency and accuracy. One
potential area for improvement is to address the computation of empty tokens
that the Transformer computes only to discard them later, leading to an
unnecessary computational burden. To tackle this, we propose an algorithm that
sorts translation sentence pairs based on their length before batching,
minimizing the waste of computing power. Since the amount of sorting could
violate the independent and identically distributed (i.i.d) data assumption, we
sort the data partially. In experiments, we apply the proposed method to
English-Korean and English-Luganda language pairs for machine translation and
show that there are gains in computational time while maintaining the
performance. Our method is independent of architectures, so that it can be
easily integrated into any training process with flexible data lengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08155">
<div class="article-summary-box-inner">
<span><p>This technical report presents AutoGen, a new framework that enables
development of LLM applications using multiple agents that can converse with
each other to solve tasks. AutoGen agents are customizable, conversable, and
seamlessly allow human participation. They can operate in various modes that
employ combinations of LLMs, human inputs, and tools. AutoGen's design offers
multiple advantages: a) it gracefully navigates the strong but imperfect
generation and reasoning abilities of these LLMs; b) it leverages human
understanding and intelligence, while providing valuable automation through
conversations between agents; c) it simplifies and unifies the implementation
of complex LLM workflows as automated agent chats. We provide many diverse
examples of how developers can easily use AutoGen to effectively solve tasks or
build applications, ranging from coding, mathematics, operations research,
entertainment, online decision-making, question answering, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sarcasm Detection in a Disaster Context. (arXiv:2308.08156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08156">
<div class="article-summary-box-inner">
<span><p>During natural disasters, people often use social media platforms such as
Twitter to ask for help, to provide information about the disaster situation,
or to express contempt about the unfolding event or public policies and
guidelines. This contempt is in some cases expressed as sarcasm or irony.
Understanding this form of speech in a disaster-centric context is essential to
improving natural language understanding of disaster-related tweets. In this
paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for
intended sarcasm, and provide a comprehensive investigation of sarcasm
detection using pre-trained language models. Our best model is able to obtain
as much as 0.70 F1 on our dataset. We also demonstrate that the performance on
HurricaneSARC can be improved by leveraging intermediate task transfer
learning. We release our data and code at
https://github.com/tsosea2/HurricaneSarc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System. (arXiv:2308.08169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08169">
<div class="article-summary-box-inner">
<span><p>End-to-end task-oriented dialogue (TOD) systems have achieved promising
performance by leveraging sophisticated natural language understanding and
natural language generation capabilities of pre-trained models. This work
enables the TOD systems with more flexibility through a simple cache. The cache
provides the flexibility to dynamically update the TOD systems and handle both
existing and unseen dialogue scenarios. Towards this end, we first fine-tune a
retrieval module to effectively retrieve the most relevant information entries
from the cache. We then train end-to-end TOD models that can refer to and
ground on both dialogue history and retrieved information during TOD
generation. The cache is straightforward to construct, and the backbone models
of TOD systems are compatible with existing pre-trained generative models.
Extensive experiments demonstrate the superior performance of our framework,
with a notable improvement in non-empty joint goal accuracy by 6.7% compared to
strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling Check. (arXiv:2308.08176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08176">
<div class="article-summary-box-inner">
<span><p>Chinese Spelling Check (CSC) refers to the detection and correction of
spelling errors in Chinese texts. In practical application scenarios, it is
important to make CSC models have the ability to correct errors across
different domains. In this paper, we propose a retrieval-augmented spelling
check framework called RSpell, which searches corresponding domain terms and
incorporates them into CSC models. Specifically, we employ pinyin fuzzy
matching to search for terms, which are combined with the input and fed into
the CSC model. Then, we introduce an adaptive process control mechanism to
dynamically adjust the impact of external knowledge on the model. Additionally,
we develop an iterative strategy for the RSpell framework to enhance reasoning
capabilities. We conducted experiments on CSC datasets in three domains: law,
medicine, and official document writing. The results demonstrate that RSpell
achieves state-of-the-art performance in both zero-shot and fine-tuning
scenarios, demonstrating the effectiveness of the retrieval-augmented CSC
framework. Our code is available at https://github.<a href="/abs/com/4777777">com/4777777</a>7/Rspell.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023. (arXiv:2308.08181v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08181">
<div class="article-summary-box-inner">
<span><p>This technical report describes ChinaTelecom system for Track 1 (closed) of
the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system
consists of several ResNet variants trained only on VoxCeleb2, which were fused
for better performance later. Score calibration was also applied for each
variant and the fused system. The final submission achieved minDCF of 0.1066
and EER of 1.980%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models. (arXiv:2308.08204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08204">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts
within knowledge graphs and automatically infer missing links. Existing methods
can mainly be categorized into structure-based or description-based. On the one
hand, structure-based methods effectively represent relational facts in
knowledge graphs using entity embeddings. However, they struggle with
semantically rich real-world entities due to limited structural information and
fail to generalize to unseen entities. On the other hand, description-based
methods leverage pre-trained language models (PLMs) to understand textual
information. They exhibit strong robustness towards unseen entities. However,
they have difficulty with larger negative sampling and often lag behind
structure-based methods. To address these issues, in this paper, we propose
Momentum Contrast for knowledge graph completion with Structure-Augmented
pre-trained language models (MoCoSA), which allows the PLM to perceive the
structural information by the adaptable structure encoder. To improve learning
efficiency, we proposed momentum hard negative and intra-relation negative
sampling. Experimental results demonstrate that our approach achieves
state-of-the-art performance in terms of mean reciprocal rank (MRR), with
improvements of 2.5% on WN18RR and 21% on OpenBG500.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08234">
<div class="article-summary-box-inner">
<span><p>The increasing adoption of natural language processing (NLP) models across
industries has led to practitioners' need for machine learning systems to
handle these models efficiently, from training to serving them in production.
However, training, deploying, and updating multiple models can be complex,
costly, and time-consuming, mainly when using transformer-based pre-trained
language models. Multi-Task Learning (MTL) has emerged as a promising approach
to improve efficiency and performance through joint training, rather than
training separate models. Motivated by this, we first provide an overview of
transformer-based MTL approaches in NLP. Then, we discuss the challenges and
opportunities of using MTL approaches throughout typical ML lifecycle phases,
specifically focusing on the challenges related to data engineering, model
development, deployment, and monitoring phases. This survey focuses on
transformer-based MTL architectures and, to the best of our knowledge, is novel
in that it systematically analyses how transformer-based MTL in NLP fits into
ML lifecycle phases. Furthermore, we motivate research on the connection
between MTL and continual learning (CL), as this area remains unexplored. We
believe it would be practical to have a model that can handle both MTL and CL,
as this would make it easier to periodically re-train the model, update it due
to distribution shifts, and add new capabilities to meet real-world
requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08239">
<div class="article-summary-box-inner">
<span><p>We propose MemoChat, a pipeline for refining instructions that enables large
language models (LLMs) to effectively employ self-composed memos for
maintaining consistent long-range open-domain conversations. We demonstrate a
long-range open-domain conversation through iterative
"memorization-retrieval-response" cycles. This requires us to carefully design
tailored tuning instructions for each distinct stage. The instructions are
reconstructed from a collection of public datasets to teach the LLMs to
memorize and retrieve past dialogues with structured memos, leading to enhanced
consistency when participating in future conversations. We invite experts to
manually annotate a test set designed to evaluate the consistency of long-range
conversations questions. Experiments on three testing scenarios involving both
open-source and API-accessible chatbots at scale verify the efficacy of
MemoChat, which outperforms strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08241">
<div class="article-summary-box-inner">
<span><p>This work summarizes two strategies for completing time-series (TS) tasks
using today's language model (LLM): LLM-for-TS, design and train a fundamental
large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS
data. Considering the insufficient data accumulation, limited resources, and
semantic context requirements, this work focuses on TS-for-LLM methods, where
we aim to activate LLM's ability for TS data by designing a TS embedding method
suitable for LLM. The proposed method is named TEST. It first tokenizes TS,
builds an encoder to embed them by instance-wise, feature-wise, and
text-prototype-aligned contrast, and then creates prompts to make LLM more open
to embeddings, and finally implements TS tasks. Experiments are carried out on
TS classification and forecasting tasks using 8 LLMs with different structures
and sizes. Although its results cannot significantly outperform the current
SOTA models customized for TS tasks, by treating LLM as the pattern machine, it
can endow LLM's ability to process TS data without compromising the language
ability. This paper is intended to serve as a foundational work that will
inspire further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Neural Network Generalization for Grammar Induction. (arXiv:2308.08253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08253">
<div class="article-summary-box-inner">
<span><p>How well do neural networks generalize? Even for grammar induction tasks,
where the target generalization is fully known, previous works have left the
question open, testing very limited ranges beyond the training set and using
different success criteria. We provide a measure of neural network
generalization based on fully specified formal languages. Given a model and a
formal grammar, the method assigns a generalization score representing how well
a model generalizes to unseen samples in inverse relation to the amount of data
it was trained on. The benchmark includes languages such as $a^nb^n$,
$a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selected
architectures using the benchmark and find that networks trained with a Minimum
Description Length objective (MDL) generalize better and using less data than
networks trained using standard loss functions. The benchmark is available at
https://github.com/taucompling/bliss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval. (arXiv:2308.08285v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08285">
<div class="article-summary-box-inner">
<span><p>In this paper, we systematically study the potential of pre-training with
Large Language Model(LLM)-based document expansion for dense passage retrieval.
Concretely, we leverage the capabilities of LLMs for document expansion, i.e.
query generation, and effectively transfer expanded knowledge to retrievers
using pre-training strategies tailored for passage retrieval. These strategies
include contrastive learning and bottlenecked query generation. Furthermore, we
incorporate a curriculum learning strategy to reduce the reliance on LLM
inferences. Experimental results demonstrate that pre-training with LLM-based
document expansion significantly boosts the retrieval performance on
large-scale web-search tasks. Our work shows strong zero-shot and out-of-domain
retrieval abilities, making it more widely applicable for retrieval when
initializing with no human-labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detoxify Language Model Step-by-Step. (arXiv:2308.08295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08295">
<div class="article-summary-box-inner">
<span><p>Detoxification for LLMs is challenging since it requires models to avoid
generating harmful content while maintaining the generation capability. To
ensure the safety of generations, previous detoxification methods detoxify the
models by changing the data distributions or constraining the generations from
different aspects in a single-step manner. However, these approaches will
dramatically affect the generation quality of LLMs, e.g., discourse coherence
and semantic consistency, since language models tend to generate along the
toxic prompt while detoxification methods work in the opposite direction. To
handle such a conflict, we decompose the detoxification process into different
sub-steps, where the detoxification is concentrated in the input stage and the
subsequent continual generation is based on the non-toxic prompt. Besides, we
also calibrate the strong reasoning ability of LLMs by designing a Detox-Chain
to connect the above sub-steps in an orderly manner, which allows LLMs to
detoxify the text step-by-step. Automatic and human evaluation on two
benchmarks reveals that by training with Detox-Chain, six LLMs scaling from 1B
to 33B can obtain significant detoxification and generation improvement. Our
code and data are available at https://github.com/CODINNLG/Detox-CoT. Warning:
examples in the paper may contain uncensored offensive content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SummHelper: Collaborative Human-Computer Summarization. (arXiv:2308.08363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08363">
<div class="article-summary-box-inner">
<span><p>Current approaches for text summarization are predominantly automatic, with
rather limited space for human intervention and control over the process. In
this paper, we introduce SummHelper, a 2-phase summarization assistant designed
to foster human-machine collaboration. The initial phase involves content
selection, where the system recommends potential content, allowing users to
accept, modify, or introduce additional selections. The subsequent phase,
content consolidation, involves SummHelper generating a coherent summary from
these selections, which users can then refine using visual mappings between the
summary and the source text. Small-scale user studies reveal the effectiveness
of our application, with participants being especially appreciative of the
balance between automated guidance and opportunities for personal input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation. (arXiv:2308.08378v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08378">
<div class="article-summary-box-inner">
<span><p>Continual learning refers to the capability of a machine learning model to
learn and adapt to new information, without compromising its performance on
previously learned tasks. Although several studies have investigated continual
learning methods for information retrieval tasks, a well-defined task
formulation is still lacking, and it is unclear how typical learning strategies
perform in this context. To address this challenge, a systematic task
formulation of continual neural information retrieval is presented, along with
a multiple-topic dataset that simulates continuous information retrieval. A
comprehensive continual neural information retrieval framework consisting of
typical retrieval models and continual learning strategies is then proposed.
Empirical evaluations illustrate that the proposed framework can successfully
prevent catastrophic forgetting in neural information retrieval and enhance
performance on previously learned tasks. The results indicate that
embedding-based retrieval models experience a decline in their continual
learning performance as the topic shift distance and dataset volume of new
tasks increase. In contrast, pretraining-based models do not show any such
correlation. Adopting suitable learning strategies can mitigate the effects of
topic shift and data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction. (arXiv:2308.08413v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08413">
<div class="article-summary-box-inner">
<span><p>Existing attribute-value extraction (AVE) models require large quantities of
labeled data for training. However, new products with new attribute-value pairs
enter the market every day in real-world e-Commerce. Thus, we formulate AVE in
multi-label few-shot learning (FSL), aiming to extract unseen attribute value
pairs based on a small number of training examples. We propose a
Knowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks,
leveraging the generated label description and category information to learn
more discriminative prototypes. Besides, KEAF integrates with hybrid attention
to reduce noise and capture more informative semantics for each class by
calculating the label-relevant and query-related weights. To achieve
multi-label inference, KEAF further learns a dynamic threshold by integrating
the semantic information from both the support set and the query set. Extensive
experiments with ablation studies conducted on two datasets demonstrate that
KEAF outperforms other SOTA models for information extraction in FSL. The code
can be found at: https://github.com/gjiaying/KEAF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction. (arXiv:2308.08442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08442">
<div class="article-summary-box-inner">
<span><p>Text-to-Text Transfer Transformer (T5) has recently been considered for the
Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free
byte-level model based on T5 referred to as ByT5, recently gave promising
results on word-level G2P conversion by representing each input character with
its corresponding UTF-8 encoding. Although it is generally understood that
sentence-level or paragraph-level G2P can improve usability in real-world
applications as it is better suited to perform on heteronyms and linking sounds
between words, we find that using ByT5 for these scenarios is nontrivial. Since
ByT5 operates on the character level, it requires longer decoding steps, which
deteriorates the performance due to the exposure bias commonly observed in
auto-regressive generation models. This paper shows that the performance of
sentence-level and paragraph-level G2P can be improved by mitigating such
exposure bias using our proposed loss-based sampling method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving CTC-AED model with integrated-CTC and auxiliary loss regularization. (arXiv:2308.08449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08449">
<div class="article-summary-box-inner">
<span><p>Connectionist temporal classification (CTC) and attention-based encoder
decoder (AED) joint training has been widely applied in automatic speech
recognition (ASR). Unlike most hybrid models that separately calculate the CTC
and AED losses, our proposed integrated-CTC utilizes the attention mechanism of
AED to guide the output of CTC. In this paper, we employ two fusion methods,
namely direct addition of logits (DAL) and preserving the maximum probability
(PMP). We achieve dimensional consistency by adaptively affine transforming the
attention results to match the dimensions of CTC. To accelerate model
convergence and improve accuracy, we introduce auxiliary loss regularization
for accelerated convergence. Experimental results demonstrate that the DAL
method performs better in attention rescoring, while the PMP method excels in
CTC prefix beam search and greedy search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TBIN: Modeling Long Textual Behavior Data for CTR Prediction. (arXiv:2308.08483v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08483">
<div class="article-summary-box-inner">
<span><p>Click-through rate (CTR) prediction plays a pivotal role in the success of
recommendations. Inspired by the recent thriving of language models (LMs), a
surge of works improve prediction by organizing user behavior data in a
\textbf{textual} format and using LMs to understand user interest at a semantic
level. While promising, these works have to truncate the textual data to reduce
the quadratic computational overhead of self-attention in LMs. However, it has
been studied that long user behavior data can significantly benefit CTR
prediction. In addition, these works typically condense user diverse interests
into a single feature vector, which hinders the expressive capability of the
model. In this paper, we propose a \textbf{T}extual \textbf{B}ehavior-based
\textbf{I}nterest Chunking \textbf{N}etwork (TBIN), which tackles the above
limitations by combining an efficient locality-sensitive hashing algorithm and
a shifted chunk-based self-attention. The resulting user diverse interests are
dynamically activated, producing user interest representation towards the
target item. Finally, the results of both offline and online experiments on
real-world food recommendation platform demonstrate the effectiveness of TBIN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder. (arXiv:2308.08488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08488">
<div class="article-summary-box-inner">
<span><p>In recent research, slight performance improvement is observed from automatic
speech recognition systems to audio-visual speech recognition systems in the
end-to-end framework with low-quality videos. Unmatching convergence rates and
specialized input representations between audio and visual modalities are
considered to cause the problem. In this paper, we propose two novel techniques
to improve audio-visual speech recognition (AVSR) under a pre-training and
fine-tuning training framework. First, we explore the correlation between lip
shapes and syllable-level subword units in Mandarin to establish good
frame-level syllable boundaries from lip shapes. This enables accurate
alignment of video and audio streams during visual model pre-training and
cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder
(CMFE) neural network to utilize main training parameters for multiple
cross-modal attention layers to make full use of modality complementarity.
Experiments on the MISP2021-AVSR data set show the effectiveness of the two
proposed techniques. Together, using only a relatively small amount of training
data, the final system achieves better performances than state-of-the-art
systems with more complex front-ends and back-ends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08493">
<div class="article-summary-box-inner">
<span><p>Data contamination, i.e., the presence of test data from downstream tasks in
the training data of large language models (LLMs), is a potential major issue
in understanding LLMs' effectiveness on other tasks. We propose a
straightforward yet effective method for identifying data contamination within
LLMs. At its core, our approach starts by identifying potential contamination
in individual instances that are drawn from a small random sample; using this
information, our approach then assesses if an entire dataset partition is
contaminated. To estimate contamination of individual instances, we employ
"guided instruction:" a prompt consisting of the dataset name, partition type,
and the initial segment of a reference instance, asking the LLM to complete it.
An instance is flagged as contaminated if the LLM's output either exactly or
closely matches the latter segment of the reference. To understand if an entire
partition is contaminated, we propose two ideas. The first idea marks a dataset
partition as contaminated if the average overlap score with the reference
instances (as measured by ROUGE or BLEURT) is statistically significantly
better with the guided instruction vs. a general instruction that does not
include the dataset and partition name. The second idea marks a dataset as
contaminated if a classifier based on GPT-4 with in-context learning prompting
marks multiple instances as contaminated. Our best method achieves an accuracy
between 92% and 100% in detecting if an LLM is contaminated with seven
datasets, containing train and test/validation partitions, when contrasted with
manual evaluation by human expert. Further, our findings indicate that GPT-4 is
contaminated with AG News, WNLI, and XSum datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes. (arXiv:2308.08494v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08494">
<div class="article-summary-box-inner">
<span><p>The large amount of time clinicians spend sifting through patient notes and
documenting in electronic health records (EHRs) is a leading cause of clinician
burnout. By proactively and dynamically retrieving relevant notes during the
documentation process, we can reduce the effort required to find relevant
patient history. In this work, we conceptualize the use of EHR audit logs for
machine learning as a source of supervision of note relevance in a specific
clinical context, at a particular point in time. Our evaluation focuses on the
dynamic retrieval in the emergency department, a high acuity setting with
unique patterns of information retrieval and note writing. We show that our
methods can achieve an AUC of 0.963 for predicting which notes will be read in
an individual note writing session. We additionally conduct a user study with
several clinicians and find that our framework can help clinicians retrieve
relevant information more efficiently. Demonstrating that our framework and
methods can perform well in this demanding setting is a promising proof of
concept that they will translate to other clinical settings and data modalities
(e.g., labs, medications, imaging).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Limits of Self-Supervision in Handling Bias in Language. (arXiv:2112.08637v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08637">
<div class="article-summary-box-inner">
<span><p>Prompting inputs with natural language task descriptions has emerged as a
popular mechanism to elicit reasonably accurate outputs from large-scale
generative language models with little to no in-context supervision. This also
helps gain insight into how well language models capture the semantics of a
wide range of downstream tasks purely from self-supervised pre-training on
massive corpora of unlabeled text. Such models have naturally also been exposed
to a lot of undesirable content like racist and sexist language and there is
limited work on awareness of models along these dimensions. In this paper, we
define and comprehensively evaluate how well such language models capture the
semantics of four tasks for bias: diagnosis, identification, extraction and
rephrasing. We define three broad classes of task descriptions for these tasks:
statement, question, and completion, with numerous lexical variants within each
class. We study the efficacy of prompting for each task using these classes and
the null task description across several decoding methods and few-shot
examples. Our analyses indicate that language models are capable of performing
these tasks to widely varying degrees across different bias dimensions, such as
gender and political affiliation. We believe our work is an important step
towards unbiased language models by quantifying the limits of current
self-supervision objectives at accomplishing such sociologically challenging
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event and Entity Extraction from Generated Video Captions. (arXiv:2211.02982v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02982">
<div class="article-summary-box-inner">
<span><p>Annotation of multimedia data by humans is time-consuming and costly, while
reliable automatic generation of semantic metadata is a major challenge. We
propose a framework to extract semantic metadata from automatically generated
video captions. As metadata, we consider entities, the entities' properties,
relations between entities, and the video category. We employ two
state-of-the-art dense video captioning models with masked transformer (MT) and
parallel decoding (PVDC) to generate captions for videos of the ActivityNet
Captions dataset. Our experiments show that it is possible to extract entities,
their properties, relations between entities, and the video category from the
generated captions. We observe that the quality of the extracted information is
mainly influenced by the quality of the event localization in the video as well
as the performance of the event caption generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09095">
<div class="article-summary-box-inner">
<span><p>Language models have been shown to perform better with an increase in scale
on a wide variety of tasks via the in-context learning paradigm. In this paper,
we investigate the hypothesis that the ability of a large language model to
in-context learn-perform a task is not uniformly spread across all of its
underlying components. Using a 66 billion parameter language model (OPT-66B)
across a diverse set of 14 downstream tasks, we find this is indeed the case:
$\sim$70% of attention heads and $\sim$20% of feed forward networks can be
removed with minimal decline in task performance. We find substantial overlap
in the set of attention heads (un)important for in-context learning across
tasks and number of in-context examples. We also address our hypothesis through
a task-agnostic lens, finding that a small set of attention heads in OPT-66B
score highly on their ability to perform primitive induction operations
associated with in-context learning, namely, prefix matching and copying. These
induction heads overlap with task-specific important heads, reinforcing
arguments by Olsson et al. (<a href="/abs/2209.11895">arXiv:2209.11895</a>) regarding induction head
generality to more sophisticated behaviors associated with in-context learning.
Overall, our study provides several insights that indicate large language
models may be under-trained for in-context learning and opens up questions on
how to pre-train language models to more effectively perform in-context
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10405">
<div class="article-summary-box-inner">
<span><p>Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hyper network to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator. (arXiv:2302.14036v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.14036">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end Automatic Speech Recognition (ASR) system that can
be trained on transcribed speech data, text-only data, or a mixture of both.
The proposed model uses an integrated auxiliary block for text-based training.
This block combines a non-autoregressive multi-speaker text-to-mel-spectrogram
generator with a GAN-based enhancer to improve the spectrogram quality. The
proposed system can generate a mel-spectrogram dynamically during training. It
can be used to adapt the ASR model to a new domain by using text-only data from
this domain. We demonstrate that the proposed training method significantly
improves ASR accuracy compared to the system trained on transcribed speech
only. It also surpasses cascade TTS systems with the vocoder in the adaptation
quality and training speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos. (arXiv:2303.09713v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09713">
<div class="article-summary-box-inner">
<span><p>Visual information is central to conversation: body gestures and physical
behaviour, for example, contribute to meaning that transcends words alone. To
date, however, most neural conversational models are limited to just text. We
introduce CHAMPAGNE, a generative model of conversations that can account for
visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a
large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from
web videos: crucial to our data collection pipeline is a pretrained language
model that converts error-prone automatic transcripts to a cleaner dialogue
format while maintaining meaning. Human evaluation reveals that YTD-18M is more
sensible and specific than prior resources (MMDialog, 1M dialogues), while
maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE
learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it
achieves state-of-the-art results on four vision-language tasks focused on
real-world conversations. We release data, models, and code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP. (arXiv:2303.16166v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16166">
<div class="article-summary-box-inner">
<span><p>Despite its crucial role in research experiments, code correctness is often
presumed only on the basis of the perceived quality of results. This assumption
comes with the risk of erroneous outcomes and potentially misleading findings.
To address this issue, we posit that the current focus on reproducibility
should go hand in hand with the emphasis on software quality. We present a case
study in which we identify and fix three bugs in widely used implementations of
the state-of-the-art Conformer architecture. Through experiments on speech
recognition and translation in various languages, we demonstrate that the
presence of bugs does not prevent the achievement of good and reproducible
results, which however can lead to incorrect conclusions that potentially
misguide future research. As a countermeasure, we propose a Code-quality
Checklist and release pangoliNN, a library dedicated to testing neural models,
with the goal of promoting coding best practices and improving research
software quality within the NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An interpretability framework for Similar case matching. (arXiv:2304.01622v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01622">
<div class="article-summary-box-inner">
<span><p>Similar Case Matching (SCM) plays a pivotal role in the legal system by
facilitating the efficient identification of similar cases for legal
professionals. While previous research has primarily concentrated on enhancing
the performance of SCM models, the aspect of interpretability has been
neglected. To bridge the gap, this study proposes an integrated pipeline
framework for interpretable SCM. The framework comprises four modules: judicial
feature sentence identification, case matching, feature sentence alignment, and
conflict resolution. In contrast to current SCM methods, our framework first
extracts feature sentences within a legal case that contain essential
information. Then it conducts case matching based on these extracted features.
Subsequently, our framework aligns the corresponding sentences in two legal
cases to provide evidence of similarity. In instances where the results of case
matching and feature sentence alignment exhibit conflicts, the conflict
resolution module resolves these inconsistencies. The experimental results show
the effectiveness of our proposed framework, establishing a new benchmark for
interpretable SCM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01752">
<div class="article-summary-box-inner">
<span><p>Vision-Language (V-L) models trained with contrastive learning to align the
visual and language modalities have been shown to be strong few-shot learners.
Soft prompt learning is the method of choice for few-shot downstream adaption
aiming to bridge the modality gap caused by the distribution shift induced by
the new domain. While parameter-efficient, prompt learning still requires
access to the model weights and can be computationally infeasible for large
models with billions of parameters. To address these shortcomings, in this
work, we describe a black-box method for V-L few-shot adaptation that (a)
operates on pre-computed image and text features and hence works without access
to the model's weights, (b) it is orders of magnitude faster at training time,
(c) it is amenable to both supervised and unsupervised training, and (d) it can
be even used to align image and text features computed from uni-modal models.
To achieve this, we propose Linear Feature Alignment (LFA), a simple linear
approach for V-L re-alignment in the target domain. LFA is initialized from a
closed-form solution to a least-squares problem and then it is iteratively
updated by minimizing a re-ranking loss. Despite its simplicity, our approach
can even surpass soft-prompt learning methods as shown by extensive experiments
on 11 image and 2 video datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08862">
<div class="article-summary-box-inner">
<span><p>This paper presents an extension to train end-to-end Context-Aware
Transformer Transducer ( CATT ) models by using a simple, yet efficient method
of mining hard negative phrases from the latent space of the context encoder.
During training, given a reference query, we mine a number of similar phrases
using approximate nearest neighbour search. These sampled phrases are then used
as negative examples in the context list alongside random and ground truth
contextual information. By including approximate nearest neighbour phrases
(ANN-P) in the context list, we encourage the learned representation to
disambiguate between similar, but not identical, biasing phrases. This improves
biasing accuracy when there are several similar phrases in the biasing
inventory. We carry out experiments in a large-scale data regime obtaining up
to 7% relative word error rate reductions for the contextual portion of test
data. We also extend and evaluate CATT approach in streaming applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03453">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have recently demonstrated exceptional
performance in various Natural Language Processing (NLP) tasks. They have also
shown the ability to perform chain-of-thought (CoT) reasoning to solve complex
problems. Recent studies have explored CoT reasoning in complex multimodal
scenarios, such as the science question answering task, by fine-tuning
multimodal models with high-quality human-annotated CoT rationales. However,
collecting high-quality COT rationales is usually time-consuming and costly.
Besides, the annotated rationales are hardly accurate due to the external
essential information missed. To address these issues, we propose a novel
method termed \emph{T-SciQ} that aims at teaching science question answering
with LLM signals. The T-SciQ approach generates high-quality CoT rationales as
teaching signals and is advanced to train much smaller models to perform CoT
reasoning in complex modalities. Additionally, we introduce a novel data mixing
strategy to produce more effective teaching data samples by policy for simple
and complex science question answer problems. Extensive experimental results
show that our T-SciQ method achieves a new state-of-the-art performance on the
ScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approach
outperforms the most powerful fine-tuned baseline by 4.5\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09781">
<div class="article-summary-box-inner">
<span><p>The high computational and memory requirements of generative large language
models (LLMs) make it challenging to serve them quickly and cheaply. This paper
introduces SpecInfer, an LLM serving system that accelerates generative LLM
inference with speculative inference and token tree verification. A key insight
behind Specinfer is to combine various collectively boost-tuned small language
models to jointly predict the LLM's outputs; the predictions are organized as a
token tree, whose nodes each represent a candidate token sequence. The
correctness of all candidate token sequences represented by a token tree is
verified against the LLM in parallel using a novel tree-based parallel decoding
mechanism. SpecInfer uses an LLM as a token tree verifier instead of an
incremental decoder, which significantly reduces the end-to-end latency and
computational requirement for serving generative LLMs while provably preserving
model quality. Our evaluation shows that SpecInfer outperforms existing LLM
serving systems by 1.3-2.4x for distributed LLM inference and by 2.6-3.5x for
offloading-based LLM inference, while preserving the same generative
performance. SpecInfer is publicly available at
https://github.com/flexflow/FlexFlow/tree/inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11095">
<div class="article-summary-box-inner">
<span><p>We investigate the emergent abilities of the recently proposed web-scale
speech model Whisper, by adapting it to unseen tasks with prompt engineering.
We selected three tasks: audio-visual speech recognition (AVSR), code-switched
speech recognition (CS-ASR), and speech translation (ST) on unseen language
pairs. We design task-specific prompts, by either leveraging another
large-scale model, or simply manipulating the special tokens in the default
prompts. Experiments show that compared to the default prompts, our proposed
prompts improve performance by 10% to 45% on the three zero-shot tasks, and
even outperform SotA supervised models on some datasets. In addition, our
experiments reveal many interesting properties of Whisper, including its
robustness to prompts, bias on accents, and the multilingual understanding in
its latent space. Code is available at
https://github.com/jasonppy/PromptingWhisper
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01102">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have emerged as powerful tools capable of
accomplishing a broad spectrum of tasks. Their abilities span numerous areas,
and one area where they have made a significant impact is in the domain of code
generation. In this context, we view LLMs as mutation and crossover tools.
Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and
robust solutions. By merging the code-generating abilities of LLMs with the
diversity and robustness of QD solutions, we introduce LLMatic, a Neural
Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS
directly through prompts, LLMatic uses a procedural approach, leveraging QD for
prompts and network architecture to create diverse and highly performant
networks. We test LLMatic on the CIFAR-10 image classification benchmark,
demonstrating that it can produce competitive networks with just $2,000$
searches, even without prior knowledge of the benchmark domain or exposure to
any previous top-performing models for the benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes. (arXiv:2306.04306v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04306">
<div class="article-summary-box-inner">
<span><p>This paper proposes Allophant, a multilingual phoneme recognizer. It requires
only a phoneme inventory for cross-lingual transfer to a target language,
allowing for low-resource recognition. The architecture combines a
compositional phone embedding approach with individually supervised phonetic
attribute classifiers in a multi-task architecture. We also introduce
Allophoible, an extension of the PHOIBLE database. When combined with a
distance based mapping approach for grapheme-to-phoneme outputs, it allows us
to train on PHOIBLE inventories directly. By training and evaluating on 34
languages, we found that the addition of multi-task learning improves the
model's capability of being applied to unseen phonemes and phoneme inventories.
On supervised languages we achieve phoneme error rate improvements of 11
percentage points (pp.) compared to a baseline without multi-task learning.
Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of
2.63 pp. over the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling. (arXiv:2306.07384v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07384">
<div class="article-summary-box-inner">
<span><p>With their increasing size, large language models (LLMs) are becoming
increasingly good at language understanding tasks. But even with high
performance on specific downstream task, LLMs fail at simple linguistic tests
for negation or quantifier understanding. Previous work on quantifier
understanding in LLMs show inverse scaling in understanding few-type
quantifiers. In this paper, we question the claims of of previous work and show
that it is a result of inappropriate testing methodology. We also present
alternate methods to measure quantifier comprehension in LLMs and show that
LLMs are able to better understand the difference between the meaning of
few-type and most-type quantifiers as their size increases, although they are
not particularly good at it. We also observe inverse scaling for most-type
quantifier understanding, which is contrary to human psycho-linguistic
experiments and previous work, where the model's understanding of most-type
quantifier gets worse as the model size increases. We do this evaluation on
models ranging from 125M-175B parameters, which suggests that LLMs do not do as
well as expected with quantifiers. We also discuss the possible reasons for
this and the relevance of quantifier understanding in evaluating language
understanding in LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the extraction of robust sign embeddings for low resource sign language recognition. (arXiv:2306.17558v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17558">
<div class="article-summary-box-inner">
<span><p>Isolated Sign Language Recognition (SLR) has mostly been applied on datasets
containing signs executed slowly and clearly by a limited group of signers. In
real-world scenarios, however, we are met with challenging visual conditions,
coarticulated signing, small datasets, and the need for signer independent
models. To tackle this difficult problem, we require a robust feature extractor
to process the sign language videos. One could expect human pose estimators to
be ideal candidates. However, due to a domain mismatch with their training sets
and challenging poses in sign language, they lack robustness on sign language
data and image-based models often still outperform keypoint-based models.
Furthermore, whereas the common practice of transfer learning with image-based
models yields even higher accuracy, keypoint-based models are typically trained
from scratch on every SLR dataset. These factors limit their usefulness for
SLR. From the existing literature, it is also not clear which, if any, pose
estimator performs best for SLR. We compare the three most popular pose
estimators for SLR: OpenPose, MMPose and MediaPipe. We show that through
keypoint normalization, missing keypoint imputation, and learning a pose
embedding, we can obtain significantly better results and enable transfer
learning. We show that keypoint-based embeddings contain cross-lingual
features: they can transfer between sign languages and achieve competitive
performance even when fine-tuning only the classifier layer of an SLR model on
a target sign language. We furthermore achieve better performance using
fine-tuned transferred embeddings than models trained only on the target sign
language. The embeddings can also be learned in a multilingual fashion. The
application of these embeddings could prove particularly useful for low
resource sign languages in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models. (arXiv:2307.07889v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07889">
<div class="article-summary-box-inner">
<span><p>Current developments in large language models (LLMs) have enabled impressive
zero-shot capabilities across various natural language tasks. An interesting
application of these systems is in the automated assessment of natural language
generation (NLG), a highly challenging area with great practical benefit. In
this paper, we explore two options for exploiting the emergent abilities of
LLMs for zero-shot NLG assessment: absolute score prediction, and comparative
assessment which uses relative comparisons between pairs of candidates. Though
comparative assessment has not been extensively studied in NLG assessment, we
note that humans often find it more intuitive to compare two options rather
than scoring each one independently. This work examines comparative assessment
from multiple perspectives: performance compared to absolute grading;
positional biases in the prompt; and efficient ranking in terms of the number
of comparisons. We illustrate that LLM comparative assessment is a simple,
general and effective approach for NLG assessment. For moderate-sized
open-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is
superior to prompt scoring, and in many cases can achieve performance
competitive with state-of-the-art methods. Additionally, we demonstrate that
LLMs often exhibit strong positional biases when making pairwise comparisons,
and we propose debiasing methods that can further improve performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11787">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have lately been on the spotlight of
researchers, businesses, and consumers alike. While the linguistic capabilities
of such models have been studied extensively, there is growing interest in
investigating them as cognitive subjects. In the present work I examine GPT-3
and ChatGPT capabilities on an limited-data inductive reasoning task from the
cognitive science literature. The results suggest that these models' cognitive
judgements are not human-like.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.12267">
<div class="article-summary-box-inner">
<span><p>The recent large language models (LLMs), e.g., ChatGPT, have been able to
generate human-like and fluent responses when provided with specific
instructions. While admitting the convenience brought by technological
advancement, educators also have concerns that students might leverage LLMs to
complete their writing assignments and pass them off as their original work.
Although many AI content detection studies have been conducted as a result of
such concerns, most of these prior studies modeled AI content detection as a
classification problem, assuming that a text is either entirely human-written
or entirely AI-generated. In this study, we investigated AI content detection
in a rarely explored yet realistic setting where the text to be detected is
collaboratively written by human and generative LLMs (i.e., hybrid text). We
first formalized the detection task as identifying the transition points
between human-written content and AI-generated content from a given hybrid text
(boundary detection). Then we proposed a two-step approach where we (1)
separated AI-generated content from human-written content during the encoder
training process; and (2) calculated the distances between every two adjacent
prototypes and assumed that the boundaries exist between the two adjacent
prototypes that have the furthest distance from each other. Through extensive
experiments, we observed the following main findings: (1) the proposed approach
consistently outperformed the baseline methods across different experiment
settings; (2) the encoder training process can significantly boost the
performance of the proposed approach; (3) when detecting boundaries for
single-boundary hybrid essays, the proposed approach could be enhanced by
adopting a relatively large prototype size, leading to a 22% improvement in the
In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.14385">
<div class="article-summary-box-inner">
<span><p>Advances in large language models (LLMs) have empowered a variety of
applications. However, there is still a significant gap in research when it
comes to understanding and enhancing the capabilities of LLMs in the field of
mental health. In this work, we present the first comprehensive evaluation of
multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on
various mental health prediction tasks via online text data. We conduct a broad
range of experiments, covering zero-shot prompting, few-shot prompting, and
instruction fine-tuning. The results indicate a promising yet limited
performance of LLMs with zero-shot and few-shot prompt designs for the mental
health tasks. More importantly, our experiments show that instruction
finetuning can significantly boost the performance of LLMs for all tasks
simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5,
outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9%
on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%.
They further perform on par with the state-of-the-art task-specific language
model. We also conduct an exploratory case study on LLMs' capability on the
mental health reasoning tasks, illustrating the promising capability of certain
models such as GPT-4. We summarize our findings into a set of action guidelines
for potential methods to enhance LLMs' capability for mental health tasks.
Meanwhile, we also emphasize the important limitations before achieving
deployability in real-world mental health settings, such as known racial and
gender bias. We highlight the important ethical risks accompanying this line of
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15780">
<div class="article-summary-box-inner">
<span><p>We investigate various prompting strategies for enhancing personalized
recommendation performance with large language models (LLMs) through input
augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct
prompting strategies: (1) basic prompting, (2) recommendation-driven prompting,
(3) engagement-guided prompting, and (4) recommendation-driven +
engagement-guided prompting. Our empirical experiments show that incorporating
the augmented input text generated by LLM leads to improved recommendation
performance. Recommendation-driven and engagement-guided prompting strategies
are found to elicit LLM's understanding of global and local item
characteristics. This finding highlights the importance of leveraging diverse
prompts and input augmentation techniques to enhance the recommendation
capabilities with LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16082">
<div class="article-summary-box-inner">
<span><p>Social platforms have emerged as crucial platforms for disseminating
information and discussing real-life social events, which offers an excellent
opportunity for researchers to design and implement novel event detection
frameworks. However, most existing approaches merely exploit keyword burstiness
or network structures to detect unspecified events. Thus, they often fail to
identify unspecified events regarding the challenging nature of events and
social data. Social data, e.g., tweets, is characterized by misspellings,
incompleteness, word sense ambiguation, and irregular language, as well as
variation in aspects of opinions. Moreover, extracting discriminative features
and patterns for evolving events by exploiting the limited structural knowledge
is almost infeasible. To address these challenges, in this thesis, we propose a
novel framework, namely EnrichEvent, that leverages the lexical and contextual
representations of streaming social data. In particular, we leverage contextual
knowledge, as well as lexical knowledge, to detect semantically related tweets
and enhance the effectiveness of the event detection approaches. Eventually,
our proposed framework produces cluster chains for each event to show the
evolving variation of the event through time. We conducted extensive
experiments to evaluate our framework, validating its high performance and
effectiveness in detecting and distinguishing unspecified social events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability. (arXiv:2308.03266v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03266">
<div class="article-summary-box-inner">
<span><p>Hotword customization is one of the important issues remained in ASR field -
it is of value to enable users of ASR systems to customize names of entities,
persons and other phrases. The past few years have seen both implicit and
explicit modeling strategies for ASR contextualization developed. While these
approaches have performed adequately, they still exhibit certain shortcomings
such as instability in effectiveness. In this paper we propose
Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based
ASR system with flexible and effective hotword customization ability. It
combines the accuracy of the AED-based model, the efficiency of the NAR model,
and the excellent performance in contextualization. In 50,000 hours industrial
big data experiments, our proposed model outperforms strong baselines in
customization and general ASR tasks. Besides, we explore an efficient way to
filter large scale incoming hotwords for further improvement. The source codes
and industrial models proposed and compared are all opened as well as two
hotword test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.05342">
<div class="article-summary-box-inner">
<span><p>In Large Language Models (LLMs), there have been consistent advancements in
task-specific performance, largely influenced by effective prompt design. While
recent research on prompting has enhanced the reasoning capabilities of LLMs, a
gap remains in further improving their understanding abilities. In this study,
we introduce Metacognitive Prompting (MP), a strategy inspired by human
introspective reasoning processes. Using MP, LLMs undergo a systematic series
of structured, self-aware evaluations, drawing on both their vast inherent
knowledge and new insights. Our experiments involve five prevalent LLMs:
Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general
natural language understanding (NLU) tasks from the GLUE and SuperGLUE
benchmarks. Results indicate that, although GPT-4 consistently excels in most
tasks, PaLM, when equipped with MP, approaches its performance level.
Furthermore, across models and datasets, MP consistently outperforms existing
prompting methods, including standard and chain-of-thought prompting. This
study underscores the potential to amplify the understanding abilities of LLMs
and highlights the benefits of mirroring human introspective reasoning in NLU
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search. (arXiv:2308.07711v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07711">
<div class="article-summary-box-inner">
<span><p>In e-commerce search, relevance between query and documents is an essential
requirement for satisfying user experience. Different from traditional
e-commerce platforms that offer products, users search on life service
platforms such as Meituan mainly for product providers, which usually have
abundant structured information, e.g. name, address, category, thousands of
products. Modeling search relevance with these rich structured contents is
challenging due to the following issues: (1) there is language distribution
discrepancy among different fields of structured document, making it difficult
to directly adopt off-the-shelf pretrained language model based methods like
BERT. (2) different fields usually have different importance and their length
vary greatly, making it difficult to extract document information helpful for
relevance matching.
</p>
<p>To tackle these issues, in this paper we propose a novel two-stage
pretraining and matching architecture for relevance matching with rich
structured documents. At pretraining stage, we propose an effective pretraining
method that employs both query and multiple fields of document as inputs,
including an effective information compression method for lengthy fields. At
relevance matching stage, a novel matching method is proposed by leveraging
domain knowledge in search query to generate more effective document
representations for relevance scoring. Extensive offline experiments and online
A/B tests on millions of users verify that the proposed architectures
effectively improve the performance of relevance modeling. The model has
already been deployed online, serving the search traffic of Meituan for over a
year.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-17 23:10:36.381339234 UTC">2023-08-17 23:10:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>