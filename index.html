<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-08T01:30:00Z">11-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">1Cademy @ Causal News Corpus 2022: Leveraging Self-Training in Causality Classification of Socio-Political Event Data. (arXiv:2211.02729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02729">
<div class="article-summary-box-inner">
<span><p>This paper details our participation in the Challenges and Applications of
Automated Extraction of Socio-political Events from Text (CASE) workshop @
EMNLP 2022, where we take part in Subtask 1 of Shared Task 3. We approach the
given task of event causality detection by proposing a self-training pipeline
that follows a teacher-student classifier method. More specifically, we
initially train a teacher model on the true, original task data, and use that
teacher model to self-label data to be used in the training of a separate
student model for the final task prediction. We test how restricting the number
of positive or negative self-labeled examples in the self-training process
affects classification performance. Our final results show that using
self-training produces a comprehensive performance improvement across all
models and self-labeled training sets tested within the task of event causality
sequence classification. On top of that, we find that self-training performance
did not diminish even when restricting either positive/negative examples used
in training. Our code is be publicly available at
https://github.com/Gzhang-umich/1CademyTeamOfCASE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intriguing Properties of Compression on Multilingual Models. (arXiv:2211.02738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02738">
<div class="article-summary-box-inner">
<span><p>Multilingual models are often particularly dependent on scaling to generalize
to a growing number of languages. Compression techniques are widely relied upon
to reconcile the growth in model size with real world resource constraints, but
compression can have a disparate effect on model performance for low-resource
languages. It is thus crucial to understand the trade-offs between scale,
multilingualism, and compression. In this work, we propose an experimental
framework to characterize the impact of sparsifying multilingual pre-trained
language models during fine-tuning. Applying this framework to mBERT named
entity recognition models across 40 languages, we find that compression confers
several intriguing and previously unknown generalization properties. In
contrast to prior findings, we find that compression may improve model
robustness over dense models. We additionally observe that under certain
sparsification regimes compression may aid, rather than disproportionately
impact the performance of low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction. (arXiv:2211.02744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02744">
<div class="article-summary-box-inner">
<span><p>The ability of knowledge graphs to represent complex relationships at scale
has led to their adoption for various needs including knowledge representation,
question-answering, fraud detection, and recommendation systems. Knowledge
graphs are often incomplete in the information they represent, necessitating
the need for knowledge graph completion tasks, such as link and relation
prediction. Pre-trained and fine-tuned language models have shown promise in
these tasks although these models ignore the intrinsic information encoded in
the knowledge graph, namely the entity and relation types. In this work, we
propose the Knowledge Graph Language Model (KGLM) architecture, where we
introduce a new entity/relation embedding layer that learns to differentiate
distinctive entity and relation types, therefore allowing the model to learn
the structure of the knowledge graph. In this work, we show that further
pre-training the language models with this additional embedding layer using the
triples extracted from the knowledge graph, followed by the standard
fine-tuning phase sets a new state-of-the-art performance for the link
prediction task on the benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and Translation Using Neural Transducers. (arXiv:2211.02809v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02809">
<div class="article-summary-box-inner">
<span><p>End-to-end formulation of automatic speech recognition (ASR) and speech
translation (ST) makes it easy to use a single model for both multilingual ASR
and many-to-many ST. In this paper, we propose streaming language-agnostic
multilingual speech recognition and translation using neural transducers
(LAMASSU). To enable multilingual text generation in LAMASSU, we conduct a
systematic comparison between specified and unified prediction and joint
networks. We leverage a language-agnostic multilingual encoder that
substantially outperforms shared encoders. To enhance LAMASSU, we propose to
feed target LID to encoders. We also apply connectionist temporal
classification regularization to transducer training. Experimental results show
that LAMASSU not only drastically reduces the model size but also outperforms
monolingual ASR and bilingual ST models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Multi-Label Classification of Scientific Documents. (arXiv:2211.02810v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02810">
<div class="article-summary-box-inner">
<span><p>Automatic topic classification has been studied extensively to assist
managing and indexing scientific documents in a digital collection. With the
large number of topics being available in recent years, it has become necessary
to arrange them in a hierarchy. Therefore, the automatic classification systems
need to be able to classify the documents hierarchically. In addition, each
paper is often assigned to more than one relevant topic. For example, a paper
can be assigned to several topics in a hierarchy tree. In this paper, we
introduce a new dataset for hierarchical multi-label text classification
(HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and
1,233 categories from the ACM CCS tree. We establish strong baselines for HMLTC
and propose a multi-task learning approach for topic classification with
keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score
of 34.57% which shows that this dataset provides significant research
opportunities on hierarchical scientific topic classification. We make our
dataset and code available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Automated Speech Recognition Systems for Conversational Speech: A Linguistic Perspective. (arXiv:2211.02812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02812">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) meets more informal and free-form input
data as voice user interfaces and conversational agents such as the voice
assistants such as Alexa, Google Home, etc., gain popularity. Conversational
speech is both the most difficult and environmentally relevant sort of data for
speech recognition. In this paper, we take a linguistic perspective, and take
the French language as a case study toward disambiguation of the French
homophones. Our contribution aims to provide more insight into human speech
transcription accuracy in conditions to reproduce those of state-of-the-art ASR
systems, although in a much focused situation. We investigate a case study
involving the most common errors encountered in the automatic transcription of
French language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training. (arXiv:2211.02816v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02816">
<div class="article-summary-box-inner">
<span><p>Fact verification has attracted a lot of research attention recently, e.g.,
in journalism, marketing, and policymaking, as misinformation and
disinformation online can sway one's opinion and affect one's actions. While
fact-checking is a hard task in general, in many cases, false statements can be
easily debunked based on analytics over tables with reliable information.
Hence, table-based fact verification has recently emerged as an important and
growing research area. Yet, progress has been limited due to the lack of
datasets that can be used to pre-train language models (LMs) to be aware of
common table operations, such as aggregating a column or comparing tuples. To
bridge this gap, in this paper we introduce PASTA, a novel state-of-the-art
framework for table-based fact verification via pre-training with synthesized
sentence-table cloze questions. In particular, we design six types of common
sentence-table cloze tasks, including Filter, Aggregation, Superlative,
Comparative, Ordinal, and Unique, based on which we synthesize a large corpus
consisting of 1.2 million sentence-table pairs from WikiTables. PASTA uses a
recent pre-trained LM, DeBERTaV3, and further pretrains it on our corpus. Our
experimental results show that PASTA achieves new state-of-the-art performance
on two table-based fact verification benchmarks: TabFact and SEM-TAB-FACTS. In
particular, on the complex set of TabFact, which contains multiple operations,
PASTA largely outperforms the previous state of the art by 4.7 points (85.6%
vs. 80.9%), and the gap between PASTA and human performance on the small
TabFact test set is narrowed to just 1.5 points (90.6% vs. 92.1%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventEA: Benchmarking Entity Alignment for Event-centric Knowledge Graphs. (arXiv:2211.02817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02817">
<div class="article-summary-box-inner">
<span><p>Entity alignment is to find identical entities in different knowledge graphs
(KGs) that refer to the same real-world object. Embedding-based entity
alignment techniques have been drawing a lot of attention recently because they
can help solve the issue of symbolic heterogeneity in different KGs. However,
in this paper, we show that the progress made in the past was due to biased and
unchallenging evaluation. We highlight two major flaws in existing datasets
that favor embedding-based entity alignment techniques, i.e., the isomorphic
graph structures in relation triples and the weak heterogeneity in attribute
triples. Towards a critical evaluation of embedding-based entity alignment
methods, we construct a new dataset with heterogeneous relations and attributes
based on event-centric KGs. We conduct extensive experiments to evaluate
existing popular methods, and find that they fail to achieve promising
performance. As a new approach to this difficult problem, we propose a
time-aware literal encoder for entity alignment. The dataset and source code
are publicly available to foster future research. Our work calls for more
effective and practical embedding-based solutions to entity alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Recommendation and Conversation via Dual Imitation. (arXiv:2211.02848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02848">
<div class="article-summary-box-inner">
<span><p>Human conversations of recommendation naturally involve the shift of
interests which can align the recommendation actions and conversation process
to make accurate recommendations with rich explanations. However, existing
conversational recommendation systems (CRS) ignore the advantage of user
interest shift in connecting recommendation and conversation, which leads to an
ineffective loose coupling structure of CRS. To address this issue, by modeling
the recommendation actions as recommendation paths in a knowledge graph (KG),
we propose DICR (Dual Imitation for Conversational Recommendation), which
designs a dual imitation to explicitly align the recommendation paths and user
interest shift paths in a recommendation module and a conversation module,
respectively. By exchanging alignment signals, DICR achieves bidirectional
promotion between recommendation and conversation modules and generates
high-quality responses with accurate recommendations and coherent explanations.
Experiments demonstrate that DICR outperforms the state-of-the-art models on
recommendation and conversation performance with automatic, human, and novel
explainability metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEKG: A Built Environment Knowledge Graph. (arXiv:2211.02864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02864">
<div class="article-summary-box-inner">
<span><p>Practices in the built environment have become more digitalized with the
rapid development of modern design and construction technologies. However, the
requirement of practitioners or scholars to gather complicated professional
knowledge in the built environment has not been satisfied yet. In this paper,
more than 80,000 paper abstracts in the built environment field were obtained
to build a knowledge graph, a knowledge base storing entities and their
connective relations in a graph-structured data model. To ensure the retrieval
accuracy of the entities and relations in the knowledge graph, two
well-annotated datasets have been created, containing 2,000 instances and 1,450
instances each in 29 relations for the named entity recognition task and
relation extraction task respectively. These two tasks were solved by two
BERT-based models trained on the proposed dataset. Both models attained an
accuracy above 85% on these two tasks. More than 200,000 high-quality relations
and entities were obtained using these models to extract all abstract data.
Finally, this knowledge graph is presented as a self-developed visualization
system to reveal relations between various entities in the domain. Both the
source code and the annotated dataset can be found here:
https://github.com/HKUST-KnowComp/BEKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Manifold-based Defense Against Natural Language Adversarial Examples. (arXiv:2211.02878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02878">
<div class="article-summary-box-inner">
<span><p>Recent studies on adversarial images have shown that they tend to leave the
underlying low-dimensional data manifold, making them significantly more
challenging for current models to make correct predictions. This so-called
off-manifold conjecture has inspired a novel line of defenses against
adversarial attacks on images. In this study, we find a similar phenomenon
occurs in the contextualized embedding space induced by pretrained language
models, in which adversarial texts tend to have their embeddings diverge from
the manifold of natural ones. Based on this finding, we propose Textual
Manifold-based Defense (TMD), a defense mechanism that projects text embeddings
onto an approximated embedding manifold before classification. It reduces the
complexity of potential adversarial examples, which ultimately enhances the
robustness of the protected model. Through extensive experiments, our method
consistently and significantly outperforms previous defenses under various
attack settings without trading off clean accuracy. To the best of our
knowledge, this is the first NLP defense that leverages the manifold structure
against adversarial attacks. Our code is available at
\url{https://github.com/dangne/tmd}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models. (arXiv:2211.02882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02882">
<div class="article-summary-box-inner">
<span><p>Fairness has become a trending topic in natural language processing (NLP),
which addresses biases targeting certain social groups such as genders and
religions. However, regional bias in language models (LMs), a long-standing
global discrimination problem, still remains unexplored. This paper bridges the
gap by analysing the regional bias learned by the pre-trained language models
that are broadly used in NLP tasks. In addition to verifying the existence of
regional bias in LMs, we find that the biases on regional groups can be
strongly influenced by the geographical clustering of the groups. We
accordingly propose a HiErarchical Regional Bias evaluation method (HERB)
utilising the information from the sub-region clusters to quantify the bias in
pre-trained LMs. Experiments show that our hierarchical metric can effectively
evaluate the regional bias with respect to comprehensive topics and measure the
potential regional bias that can be propagated to downstream tasks. Our codes
are available at https://github.com/Bernard-Yang/HERB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural Language Processing. (arXiv:2211.02899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02899">
<div class="article-summary-box-inner">
<span><p>In natural language processing (NLP), the context of a word or sentence plays
an essential role. Contextual information such as the semantic representation
of a passage or historical dialogue forms an essential part of a conversation
and a precise understanding of the present phrase or sentence. However, the
standard attention mechanisms typically generate weights using query and key
but ignore context, forming a Bi-Attention framework, despite their great
success in modeling sequence alignment. This Bi-Attention mechanism does not
explicitly model the interactions between the contexts, queries and keys of
target sequences, missing important contextual information and resulting in
poor attention performance. Accordingly, a novel and general triple-attention
(Tri-Attention) framework expands the standard Bi-Attention mechanism and
explicitly interacts query, key, and context by incorporating context as the
third dimension in calculating relevance scores. Four variants of Tri-Attention
are generated by expanding the two-dimensional vector-based additive,
dot-product, scaled dot-product, and bilinear operations in Bi-Attention to the
tensor operations for Tri-Attention. Extensive experiments on three NLP tasks
demonstrate that Tri-Attention outperforms about 30 state-of-the-art
non-attention, standard Bi-Attention, contextual Bi-Attention approaches and
pretrained neural language models1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Legal Argument Reasoning Task in Civil Procedure. (arXiv:2211.02950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02950">
<div class="article-summary-box-inner">
<span><p>We present a new NLP task and dataset from the domain of the U.S. civil
procedure. Each instance of the dataset consists of a general introduction to
the case, a particular question, and a possible solution argument, accompanied
by a detailed analysis of why the argument applies in that case. Since the
dataset is based on a book aimed at law students, we believe that it represents
a truly complex task for benchmarking modern legal language models. Our
baseline evaluation shows that fine-tuning a legal transformer provides some
advantage over random baseline models, but our analysis reveals that the actual
ability to infer legal arguments remains a challenging open research question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Models for Legal Natural Language Processing. (arXiv:2211.02956v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02956">
<div class="article-summary-box-inner">
<span><p>Pre-training large transformer models with in-domain data improves domain
adaptation and helps gain performance on the domain-specific downstream tasks.
However, sharing models pre-trained on potentially sensitive data is prone to
adversarial privacy attacks. In this paper, we asked to which extent we can
guarantee privacy of pre-training data and, at the same time, achieve better
downstream performance on legal tasks without the need of additional labeled
data. We extensively experiment with scalable self-supervised learning of
transformer models under the formal paradigm of differential privacy and show
that under specific training configurations we can improve downstream
performance without sacrifying privacy protection for the in-domain data. Our
main contribution is utilizing differential privacy for large-scale
pre-training of transformer language models in the legal NLP domain, which, to
the best of our knowledge, has not been addressed before.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference. (arXiv:2211.02971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02971">
<div class="article-summary-box-inner">
<span><p>Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims
at predicting the relation between a pair of sentences (premise and hypothesis)
as entailment, contradiction or semantic independence. Although deep learning
models have shown promising performance for NLI in recent years, they rely on
large scale expensive human-annotated datasets. Semi-supervised learning (SSL)
is a popular technique for reducing the reliance on human annotation by
leveraging unlabeled data for training. However, despite its substantial
success on single sentence classification tasks where the challenge in making
use of unlabeled data is to assign "good enough" pseudo-labels, for NLI tasks,
the nature of unlabeled data is more complex: one of the sentences in the pair
(usually the hypothesis) along with the class label are missing from the data
and require human annotations, which makes SSL for NLI more challenging. In
this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI
where we use a conditional language model, BART to generate the hypotheses for
the unlabeled sentences (used as premises). Our experiments show that our SSL
framework successfully exploits unlabeled data and substantially improves the
performance of four NLI datasets in low-resource settings. We release our code
at: https://github.com/msadat3/SSL_for_NLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparison of Automatic Labelling Approaches for Sentiment Analysis. (arXiv:2211.02976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02976">
<div class="article-summary-box-inner">
<span><p>Labelling a large quantity of social media data for the task of supervised
machine learning is not only time-consuming but also difficult and expensive.
On the other hand, the accuracy of supervised machine learning models is
strongly related to the quality of the labelled data on which they train, and
automatic sentiment labelling techniques could reduce the time and cost of
human labelling. We have compared three automatic sentiment labelling
techniques: TextBlob, Vader, and Afinn to assign sentiments to tweets without
any human assistance. We compare three scenarios: one uses training and testing
datasets with existing ground truth labels; the second experiment uses
automatic labels as training and testing datasets; and the third experiment
uses three automatic labelling techniques to label the training dataset and
uses the ground truth labels for testing. The experiments were evaluated on two
Twitter datasets: SemEval-2013 (DS-1) and SemEval-2016 (DS-2). Results show
that the Afinn labelling technique obtains the highest accuracy of 80.17%
(DS-1) and 80.05% (DS-2) using a BiLSTM deep learning model. These findings
imply that automatic text labelling could provide significant benefits, and
suggest a feasible alternative to the time and cost of human labelling efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Metadata Extraction from Dense Video Captioning. (arXiv:2211.02982v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02982">
<div class="article-summary-box-inner">
<span><p>Annotation of multimedia data by humans is time-consuming and costly, while
reliable automatic generation of semantic metadata is a major challenge. We
propose a framework to extract semantic metadata from automatically generated
video captions. As metadata, we consider entities, the entities' properties,
relations between entities, and the video category. We employ two
state-of-the-art dense video captioning models with masked transformer (MT) and
parallel decoding (PVDC) to generate captions for videos of the ActivityNet
Captions dataset. Our experiments show that it is possible to extract entities,
their properties, relations between entities, and the video category from the
generated captions. We observe that the quality of the extracted information is
mainly influenced by the quality of the event localization in the video as well
as the performance of the event caption generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Lottery Tickets for Pre-trained Language Models. (arXiv:2211.03013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03013">
<div class="article-summary-box-inner">
<span><p>Recent works on Lottery Ticket Hypothesis have shown that pre-trained
language models (PLMs) contain smaller matching subnetworks(winning tickets)
which are capable of reaching accuracy comparable to the original models.
However, these tickets are proved to be notrobust to adversarial examples, and
even worse than their PLM counterparts. To address this problem, we propose a
novel method based on learning binary weight masks to identify robust tickets
hidden in the original PLMs. Since the loss is not differentiable for the
binary mask, we assign the hard concrete distribution to the masks and
encourage their sparsity using a smoothing approximation of L0
regularization.Furthermore, we design an adversarial loss objective to guide
the search for robust tickets and ensure that the tickets perform well bothin
accuracy and robustness. Experimental results show the significant improvement
of the proposed method over previous work on adversarial robustness evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Speech and Textual Pre-trained Models with Unsupervised ASR. (arXiv:2211.03025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03025">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) is a task aiming to extract high-level
semantics from spoken utterances. Previous works have investigated the use of
speech self-supervised models and textual pre-trained models, which have shown
reasonable improvements to various SLU tasks. However, because of the
mismatched modalities between speech signals and text tokens, previous methods
usually need complex designs of the frameworks. This work proposes a simple yet
efficient unsupervised paradigm that connects speech and textual pre-trained
models, resulting in an unsupervised speech-to-semantic pre-trained model for
various tasks in SLU. To be specific, we propose to use unsupervised automatic
speech recognition (ASR) as a connector that bridges different modalities used
in speech and textual pre-trained models. Our experiments show that
unsupervised ASR itself can improve the representations from speech
self-supervised models. More importantly, it is shown as an efficient connector
between speech and textual pre-trained models, improving the performances of
five different SLU tasks. Notably, on spoken question answering, we reach the
state-of-the-art result over the challenging NMSQA benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Text Entailment for Low-Resource Named Entity Recognition. (arXiv:2211.03039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03039">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PLMs) have been applied in NLP tasks and achieve
promising results. Nevertheless, the fine-tuning procedure needs labeled data
of the target domain, making it difficult to learn in low-resource and
non-trivial labeled scenarios. To address these challenges, we propose
Prompt-based Text Entailment (PTE) for low-resource named entity recognition,
which better leverages knowledge in the PLMs. We first reformulate named entity
recognition as the text entailment task. The original sentence with entity
type-specific prompts is fed into PLMs to get entailment scores for each
candidate. The entity type with the top score is then selected as final label.
Then, we inject tagging labels into prompts and treat words as basic units
instead of n-gram spans to reduce time complexity in generating candidates by
n-grams enumeration. Experimental results demonstrate that the proposed method
PTE achieves competitive performance on the CoNLL03 dataset, and better than
fine-tuned counterparts on the MIT Movie and Few-NERD dataset in low-resource
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates. (arXiv:2211.03041v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03041">
<div class="article-summary-box-inner">
<span><p>Calibration strengthens the trustworthiness of black-box models by producing
better accurate confidence estimates on given examples. However, little is
known about if model explanations can help confidence calibration. Intuitively,
humans look at important features attributions and decide whether the model is
trustworthy. Similarly, the explanations can tell us when the model may or may
not know. Inspired by this, we propose a method named CME that leverages model
explanations to make the model less confident with non-inductive attributions.
The idea is that when the model is not highly confident, it is difficult to
identify strong indications of any class, and the tokens accordingly do not
have high attribution scores for any class and vice versa. We conduct extensive
experiments on six datasets with two popular pre-trained language models in the
in-domain and out-of-domain settings. The results show that CME improves
calibration performance in all settings. The expected calibration errors are
further reduced when combined with temperature scaling. Our findings highlight
that model explanations can help calibrate posterior estimates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning. (arXiv:2211.03044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03044">
<div class="article-summary-box-inner">
<span><p>Recent studies have revealed the intriguing few-shot learning ability of
pretrained language models (PLMs): They can quickly adapt to a new task when
fine-tuned on a small amount of labeled data formulated as prompts, without
requiring abundant task-specific annotations. Despite their promising
performance, most existing few-shot approaches that only learn from the small
training set still underperform fully supervised training by nontrivial
margins. In this work, we study few-shot learning with PLMs from a different
perspective: We first tune an autoregressive PLM on the few-shot samples and
then use it as a generator to synthesize a large amount of novel training
samples which augment the original training set. To encourage the generator to
produce label-discriminative samples, we train it via weighted maximum
likelihood where the weight of each token is automatically adjusted based on a
discriminative meta-learning objective. A classification PLM can then be
fine-tuned on both the few-shot and the synthetic samples with regularization
for better generalization and stability. Our approach FewGen achieves an
overall better result across seven classification tasks of the GLUE benchmark
than existing few-shot learning methods, improving no-augmentation methods by
5+ average points, and outperforming augmentation methods by 3+ average points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03046">
<div class="article-summary-box-inner">
<span><p>Legal judgment Prediction (LJP), aiming to predict a judgment based on fact
descriptions, serves as legal assistance to mitigate the great work burden of
limited legal practitioners. Most existing methods apply various large-scale
pre-trained language models (PLMs) finetuned in LJP tasks to obtain consistent
improvements. However, we discover the fact that the state-of-the-art (SOTA)
model makes judgment predictions according to wrong (or non-casual)
information, which not only weakens the model's generalization capability but
also results in severe social problems like discrimination. Here, we analyze
the causal mechanism misleading the LJP model to learn the spurious
correlations, and then propose a framework to guide the model to learn the
underlying causality knowledge in the legal texts. Specifically, we first
perform open information extraction (OIE) to refine the text having a high
proportion of causal information, according to which we generate a new set of
data. Then, we design a model learning the weights of the refined data and the
raw data for LJP model training. The extensive experimental results show that
our model is more generalizable and robust than the baselines and achieves a
new SOTA performance on two commonly used legal-specific datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suffix Retrieval-Augmented Language Modeling. (arXiv:2211.03053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03053">
<div class="article-summary-box-inner">
<span><p>Causal language modeling (LM) uses word history to predict the next word.
BERT, on the other hand, makes use of bi-directional word information in a
sentence to predict words at masked positions. While BERT is effective in
sequence encoding, it is non-causal by nature and is not designed for sequence
generation. In this paper, we propose a novel language model, SUffix
REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual
effect in an autoregressive manner. SUREALM employs an embedding retriever to
search for training sentences in a data store that share similar word history
during sequence generation. In particular, the suffix portions of the retrieved
sentences mimick the "future" context. We evaluated our proposed model on the
DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on
the validation and test set compared to competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Target-specific Stance Detection on Social Media Platforms by Delving into Conversation Threads. (arXiv:2211.03061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03061">
<div class="article-summary-box-inner">
<span><p>Target-specific stance detection on social media, which aims at classifying a
textual data instance such as a post or a comment into a stance class of a
target issue, has become an emerging opinion mining paradigm of importance. An
example application would be to overcome vaccine hesitancy in combating the
coronavirus pandemic. However, existing stance detection strategies rely merely
on the individual instances which cannot always capture the expressed stance of
a given target. In response, we address a new task called conversational stance
detection which is to infer the stance towards a given target (e.g., COVID-19
vaccination) when given a data instance and its corresponding conversation
thread. To tackle the task, we first propose a benchmarking conversational
stance detection (CSD) dataset with annotations of stances and the structures
of conversation threads among the instances based on six major social media
platforms in Hong Kong. To infer the desired stances from both data instances
and conversation threads, we propose a model called Branch-BERT that
incorporates contextual information in conversation threads. Extensive
experiments on our CSD dataset show that our proposed model outperforms all the
baseline models that do not make use of contextual information. Specifically,
it improves the F1 score by 10.3% compared with the state-of-the-art method in
the SemEval-2016 Task 6 competition. This shows the potential of incorporating
rich contextual information on detecting target-specific stances on social
media platforms and implies a more practical way to construct future stance
detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAIL: Malware Analysis Intermediate Language. (arXiv:2211.03068v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03068">
<div class="article-summary-box-inner">
<span><p>This paper introduces and presents a new language named MAIL (Malware
Analysis Intermediate Language). MAIL is basically used for building malware
analysis and detection tools. MAIL provides an abstract representation of an
assembly program and hence the ability of a tool to automate malware analysis
and detection. By translating binaries compiled for different platforms to
MAIL, a tool can achieve platform independence. Each MAIL statement is
annotated with patterns that can be used by a tool to optimize malware analysis
and detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Channel for Automatic Text Simplification. (arXiv:2211.03152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03152">
<div class="article-summary-box-inner">
<span><p>In this paper we present a simple re-ranking method for Automatic Sentence
Simplification based on the noisy channel scheme. Instead of directly computing
the best simplification given a complex text, the re-ranking method also
considers the probability of the simple sentence to produce the complex
counterpart, as well as the probability of the simple text itself, according to
a language model. Our experiments show that combining these scores outperform
the original system in three different English datasets, yielding the best
known result in one of them. Adopting the noisy channel scheme opens new ways
to infuse additional information into ATS systems, and thus to control
important aspects of them, a known limitation of end-to-end neural seq2seq
generative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Domain Adaptation and Generalization of Pretrained Language Models: A Survey. (arXiv:2211.03154v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03154">
<div class="article-summary-box-inner">
<span><p>Recent advances in NLP are brought by a range of large-scale pretrained
language models (PLMs). These PLMs have brought significant performance gains
for a range of NLP tasks, circumventing the need to customize complex designs
for specific tasks. However, most current work focus on finetuning PLMs on a
domain-specific datasets, ignoring the fact that the domain gap can lead to
overfitting and even performance drop. Therefore, it is practically important
to find an appropriate method to effectively adapt PLMs to a target domain of
interest. Recently, a range of methods have been proposed to achieve this
purpose. Early surveys on domain adaptation are not suitable for PLMs due to
the sophisticated behavior exhibited by PLMs from traditional models trained
from scratch and that domain adaptation of PLMs need to be redesigned to take
effect. This paper aims to provide a survey on these newly proposed methods and
shed light in how to apply traditional machine learning methods to newly
evolved and future technologies. By examining the issues of deploying PLMs for
downstream tasks, we propose a taxonomy of domain adaptation approaches from a
machine learning system view, covering methods for input augmentation, model
optimization and personalization. We discuss and compare those methods and
suggest promising future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deliberation Networks and How to Train Them. (arXiv:2211.03217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03217">
<div class="article-summary-box-inner">
<span><p>Deliberation networks are a family of sequence-to-sequence models, which have
achieved state-of-the-art performance in a wide range of tasks such as machine
translation and speech synthesis. A deliberation network consists of multiple
standard sequence-to-sequence models, each one conditioned on the initial input
and the output of the previous model. During training, there are several key
questions: whether to apply Monte Carlo approximation to the gradients or the
loss, whether to train the standard models jointly or separately, whether to
run an intermediate model in teacher forcing or free running mode, whether to
apply task-specific techniques. Previous work on deliberation networks
typically explores one or two training options for a specific task. This work
introduces a unifying framework, covering various training options, and
addresses the above questions. In general, it is simpler to approximate the
gradients. When parallel training is essential, separate training should be
adopted. Regardless of the task, the intermediate model should be in free
running mode. For tasks where the output is continuous, a guided attention loss
can be used to prevent degradation into a standard model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MyProfessors: Mining Turkish Student Reviews. (arXiv:2109.02325v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02325">
<div class="article-summary-box-inner">
<span><p>We introduce Hocalarim (MyProfessors), the largest student review dataset
available for the Turkish language. It consists of over 5000 professor reviews
left online by students, with different aspects of education rated on a scale
of 1 to 5 stars. We investigate the properties of the dataset and present its
statistics. We examine the impact of students' institution type on their
ratings and the correlation of students' bias to give positive or negative
feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISAPERE: A Dataset for Discourse Structure in Peer Review Discussions. (arXiv:2110.08520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08520">
<div class="article-summary-box-inner">
<span><p>At the foundation of scientific evaluation is the labor-intensive process of
peer review. This critical task requires participants to consume vast amounts
of highly technical text. Prior work has annotated different aspects of review
argumentation, but discourse relations between reviews and rebuttals have yet
to be examined. We present DISAPERE, a labeled dataset of 20k sentences
contained in 506 review-rebuttal pairs in English, annotated by experts.
DISAPERE synthesizes label sets from prior work and extends them to include
fine-grained annotation of the rebuttal sentences, characterizing their context
in the review and the authors' stance towards review arguments. Further, we
annotate every review and rebuttal sentence. We show that discourse cues from
rebuttals can shed light on the quality and interpretation of reviews. Further,
an understanding of the argumentative strategies employed by the reviewers and
authors provides useful signal for area chairs and other decision makers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Segmentation of Legal Documents via Rhetorical Roles. (arXiv:2112.01836v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01836">
<div class="article-summary-box-inner">
<span><p>Legal documents are unstructured, use legal jargon, and have considerable
length, making them difficult to process automatically via conventional text
processing techniques. A legal document processing system would benefit
substantially if the documents could be segmented into coherent information
units. This paper proposes a new corpus of legal documents annotated (with the
help of legal experts) with a set of 13 semantically coherent units labels
(referred to as Rhetorical Roles), e.g., facts, arguments, statute, issue,
precedent, ruling, and ratio. We perform a thorough analysis of the corpus and
the annotations. For automatically segmenting the legal documents, we
experiment with the task of rhetorical role prediction: given a document,
predict the text segments corresponding to various roles. Using the created
corpus, we experiment extensively with various deep learning-based baseline
models for the task. Further, we develop a multitask learning (MTL) based deep
model with document rhetorical role label shift as an auxiliary task for
segmenting a legal document. The proposed model shows superior performance over
the existing models. We also experiment with model performance in the case of
domain transfer and model distillation techniques to see the model performance
in limited data conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shapes of Emotions: Multimodal Emotion Recognition in Conversations via Emotion Shifts. (arXiv:2112.01938v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01938">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversations (ERC) is an important and active
research area. Recent work has shown the benefits of using multiple modalities
(e.g., text, audio, and video) for the ERC task. In a conversation,
participants tend to maintain a particular emotional state unless some stimuli
evokes a change. There is a continuous ebb and flow of emotions in a
conversation. Inspired by this observation, we propose a multimodal ERC model
and augment it with an emotion-shift component that improves performance. The
proposed emotion-shift component is modular and can be added to any existing
multimodal ERC model (with a few modifications). We experiment with different
variants of the model, and results show that the inclusion of emotion shift
signal helps the model to outperform existing models for ERC on MOSEI and
IEMOCAP datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Questions Generate Named Entity Recognition Datasets. (arXiv:2112.08808v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08808">
<div class="article-summary-box-inner">
<span><p>Recent named entity recognition (NER) models often rely on human-annotated
datasets, requiring the significant engagement of professional knowledge on the
target domain and entities. This research introduces an ask-to-generate
approach that automatically generates NER datasets by asking questions in
simple natural language to an open-domain question answering system (e.g.,
"Which disease?"). Despite using fewer in-domain resources, our models, solely
trained on the generated datasets, largely outperform strong low-resource
models by an average F1 score of 19.4 for six popular NER benchmarks.
Furthermore, our models provide competitive performance with rich-resource
models that additionally leverage in-domain dictionaries provided by domain
experts. In few-shot NER, we outperform the previous best model by an F1 score
of 5.2 on three benchmarks and achieve new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Matters: Radiology Report Generation with General and Specific Knowledge. (arXiv:2112.15009v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15009">
<div class="article-summary-box-inner">
<span><p>Automatic radiology report generation is critical in clinics which can
relieve experienced radiologists from the heavy workload and remind
inexperienced radiologists of misdiagnosis or missed diagnose. Existing
approaches mainly formulate radiology report generation as an image captioning
task and adopt the encoder-decoder framework. However, in the medical domain,
such pure data-driven approaches suffer from the following problems: 1) visual
and textual bias problem; 2) lack of expert knowledge. In this paper, we
propose a knowledge-enhanced radiology report generation approach introduces
two types of medical knowledge: 1) General knowledge, which is input
independent and provides the broad knowledge for report generation; 2) Specific
knowledge, which is input dependent and provides the fine-grained knowledge for
report generation. To fully utilize both the general and specific knowledge, we
also propose a knowledge-enhanced multi-head attention mechanism. By merging
the visual features of the radiology image with general knowledge and specific
knowledge, the proposed model can improve the quality of generated reports.
Experimental results on two publicly available datasets IU-Xray and MIMIC-CXR
show that the proposed knowledge enhanced approach outperforms state-of-the-art
image captioning based methods. Ablation studies also demonstrate that both
general and specific knowledge can help to improve the performance of radiology
report generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Hallucination in Natural Language Generation. (arXiv:2202.03629v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03629">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) has improved exponentially in recent years
thanks to the development of sequence-to-sequence deep learning technologies
such as Transformer-based language models. This advancement has led to more
fluent and coherent NLG, leading to improved development in downstream tasks
such as abstractive summarization, dialogue generation and data-to-text
generation. However, it is also apparent that deep learning based generation is
prone to hallucinate unintended text, which degrades the system performance and
fails to meet user expectations in many real-world scenarios. To address this
issue, many studies have been presented in measuring and mitigating
hallucinated texts, but these have never been reviewed in a comprehensive
manner before. In this survey, we thus provide a broad overview of the research
progress and challenges in the hallucination problem in NLG. The survey is
organized into two parts: (1) a general overview of metrics, mitigation
methods, and future directions; and (2) an overview of task-specific research
progress on hallucinations in the following downstream tasks, namely
abstractive summarization, dialogue generation, generative question answering,
data-to-text generation, machine translation, and visual-language generation.
This survey serves to facilitate collaborative efforts among researchers in
tackling the challenge of hallucinated texts in NLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges. (arXiv:2203.01054v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01054">
<div class="article-summary-box-inner">
<span><p>As an important fine-grained sentiment analysis problem, aspect-based
sentiment analysis (ABSA), aiming to analyze and understand people's opinions
at the aspect level, has been attracting considerable interest in the last
decade. To handle ABSA in different scenarios, various tasks are introduced for
analyzing different sentiment elements and their relations, including the
aspect term, aspect category, opinion term, and sentiment polarity. Unlike
early ABSA works focusing on a single sentiment element, many compound ABSA
tasks involving multiple elements have been studied in recent years for
capturing more complete aspect-level sentiment information. However, a
systematic review of various ABSA tasks and their corresponding solutions is
still lacking, which we aim to fill in this survey. More specifically, we
provide a new taxonomy for ABSA which organizes existing studies from the axes
of concerned sentiment elements, with an emphasis on recent advances of
compound ABSA tasks. From the perspective of solutions, we summarize the
utilization of pre-trained language models for ABSA, which improved the
performance of ABSA to a new stage. Besides, techniques for building more
practical ABSA systems in cross-domain/lingual scenarios are discussed.
Finally, we review some emerging topics and discuss some open challenges to
outlook potential future directions of ABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again. (arXiv:2203.08410v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08410">
<div class="article-summary-box-inner">
<span><p>The strong few-shot in-context learning capability of large pre-trained
language models (PLMs) such as GPT-3 is highly appealing for application
domains such as biomedicine, which feature high and diverse demands of language
technologies but also high data annotation costs. In this paper, we present the
first systematic and comprehensive study to compare the few-shot performance of
GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on
two highly representative biomedical information extraction tasks, named entity
recognition and relation extraction. We follow the true few-shot setting to
avoid overestimating models' few-shot performance by model selection over a
large validation set. We also optimize GPT-3's performance with known
techniques such as contextual calibration and dynamic in-context example
retrieval. However, our results show that GPT-3 still significantly
underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3
in-context learning also yields smaller gains in accuracy when more training
data becomes available. Our in-depth analyses further reveal issues of the
in-context learning setting that may be detrimental to information extraction
tasks in general. Given the high cost of experimenting with GPT-3, we hope our
study provides guidance for biomedical researchers and practitioners towards
more promising directions such as fine-tuning small PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nix-TTS: Lightweight and End-to-End Text-to-Speech via Module-wise Distillation. (arXiv:2203.15643v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15643">
<div class="article-summary-box-inner">
<span><p>Several solutions for lightweight TTS have shown promising results. Still,
they either rely on a hand-crafted design that reaches non-optimum size or use
a neural architecture search but often suffer training costs. We present
Nix-TTS, a lightweight TTS achieved via knowledge distillation to a
high-quality yet large-sized, non-autoregressive, and end-to-end (vocoder-free)
TTS teacher model. Specifically, we offer module-wise distillation, enabling
flexible and independent distillation to the encoder and decoder module. The
resulting Nix-TTS inherited the advantageous properties of being
non-autoregressive and end-to-end from the teacher, yet significantly smaller
in size, with only 5.23M parameters or up to 89.34% reduction of the teacher
model; it also achieves over 3.04x and 8.36x inference speedup on Intel-i7 CPU
and Raspberry Pi 3B respectively and still retains a fair voice naturalness and
intelligibility compared to the teacher model. We provide pretrained models and
audio samples of Nix-TTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KOLD: Korean Offensive Language Dataset. (arXiv:2205.11315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11315">
<div class="article-summary-box-inner">
<span><p>Recent directions for offensive language detection are hierarchical modeling,
identifying the type and the target of offensive language, and interpretability
with offensive span annotation and prediction. These improvements are focused
on English and do not transfer well to other languages because of cultural and
linguistic differences. In this paper, we present the Korean Offensive Language
Dataset (KOLD) comprising 40,429 comments, which are annotated hierarchically
with the type and the target of offensive language, accompanied by annotations
of the corresponding text spans. We collect the comments from NAVER news and
YouTube platform and provide the titles of the articles and videos as the
context information for the annotation process. We use these annotated comments
as training data for Korean BERT and RoBERTa models and find that they are
effective at offensiveness detection, target classification, and target span
detection while having room for improvement for target group classification and
offensive span detection. We discover that the target group distribution
differs drastically from the existing English datasets, and observe that
providing the context information improves the model performance in
offensiveness detection (+0.3), target classification (+1.5), and target group
classification (+13.1). We publicly release the dataset and baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment. (arXiv:2205.11616v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11616">
<div class="article-summary-box-inner">
<span><p>Word translation without parallel corpora has become feasible, rivaling the
performance of supervised methods. Recent findings have shown that the accuracy
and robustness of unsupervised word translation (UWT) can be improved by making
use of visual observations, which are universal representations across
languages. In this work, we investigate the potential of using not only visual
observations but also pretrained language-image models for enabling a more
efficient and robust UWT. Specifically, we develop a novel UWT method dubbed
Word Alignment using Language-Image Pretraining (WALIP), which leverages visual
observations via the shared embedding space of images and texts provided by
CLIP models (Radford et al., 2021). WALIP has a two-step procedure. First, we
retrieve word pairs with high confidences of similarity, computed using our
proposed image-based fingerprints, which define the initial pivot for the word
alignment. Second, we apply our robust Procrustes algorithm to estimate the
linear mapping between two embedding spaces, which iteratively corrects and
refines the estimated alignment. Our extensive experiments show that WALIP
improves upon the state-of-the-art performance of bilingual word alignment for
a few language pairs across different word embeddings and displays great
robustness to the dissimilarity of language pairs or training corpora for two
word embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer. (arXiv:2205.11631v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11631">
<div class="article-summary-box-inner">
<span><p>In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
mainly focused solely on source sentence tokens' attributions. Therefore, we
lack a full understanding of the influences of every input token (source
sentence and target prefix) in the model predictions. In this work, we propose
an interpretability method that tracks input tokens' attributions for both
contexts. Our method, which can be extended to any encoder-decoder
Transformer-based model, allows us to better comprehend the inner workings of
current NMT models. We apply the proposed method to both bilingual and
multilingual Transformers and present insights into their behaviour.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation. (arXiv:2205.12078v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12078">
<div class="article-summary-box-inner">
<span><p>Subject to the huge semantic gap between natural and formal languages, neural
semantic parsing is typically bottlenecked by its complexity of dealing with
both input semantics and output syntax. Recent works have proposed several
forms of supplementary supervision but none is generalized across multiple
formal languages. This paper proposes a unified intermediate representation
(IR) for graph query languages, named GraphQ IR. It has a natural-language-like
expression that bridges the semantic gap and formally defined syntax that
maintains the graph structure. Therefore, a neural semantic parser can more
precisely convert user queries into GraphQ IR, which can be later losslessly
compiled into various downstream graph query languages. Extensive experiments
on several benchmarks including KQA Pro, Overnight, GrailQA, and MetaQA-Cypher
under standard i.i.d., out-of-distribution, and low-resource settings validate
GraphQ IR's superiority over the previous state-of-the-arts with a maximum 11%
accuracy improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chunk-based Nearest Neighbor Machine Translation. (arXiv:2205.12230v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12230">
<div class="article-summary-box-inner">
<span><p>Semi-parametric models, which augment generation with retrieval, have led to
impressive results in language modeling and machine translation, due to their
ability to retrieve fine-grained information from a datastore of examples. One
of the most prominent approaches, $k$NN-MT, exhibits strong domain adaptation
capabilities by retrieving tokens from domain-specific datastores
\citep{khandelwal2020nearest}. However, $k$NN-MT requires an expensive
retrieval operation for every single generated token, leading to a very low
decoding speed (around 8 times slower than a parametric model). In this paper,
we introduce a \textit{chunk-based} $k$NN-MT model which retrieves chunks of
tokens from the datastore, instead of a single token. We propose several
strategies for incorporating the retrieved chunks into the generation process,
and for selecting the steps at which the model needs to search for neighbors in
the datastore. Experiments on machine translation in two settings, static and
``on-the-fly'' domain adaptation, show that the chunk-based $k$NN-MT model
leads to significant speed-ups (up to 4 times) with only a small drop in
translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Heuristics for AI-Generated Language Are Flawed. (arXiv:2206.07271v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07271">
<div class="article-summary-box-inner">
<span><p>Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems produce smart replies,
autocompletes, and translations. AI-generated language is often not identified
as such but presented as language written by humans, raising concerns about
novel forms of deception and manipulation. Here, we study how humans discern
whether verbal self-presentations, one of the most personal and consequential
forms of language, were generated by AI. In six experiments, participants (N =
4,600) were unable to detect self-presentations generated by state-of-the-art
AI language models in professional, hospitality, and dating contexts. A
computational analysis of language features shows that human judgments of
AI-generated language are handicapped by intuitive but flawed heuristics such
as associating first-person pronouns, spontaneous wording, or family topics
with human-written language. We experimentally demonstrate that these
heuristics make human judgment of AI-generated language predictable and
manipulable, allowing AI systems to produce language perceived as more human
than human. We discuss solutions, such as AI accents, to reduce the deceptive
potential of language generated by AI, limiting the subversion of human
intuition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation. (arXiv:2207.14467v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14467">
<div class="article-summary-box-inner">
<span><p>Transformer structure, stacked by a sequence of encoder and decoder network
layers, achieves significant development in neural machine translation.
However, vanilla Transformer mainly exploits the top-layer representation,
assuming the lower layers provide trivial or redundant information and thus
ignoring the bottom-layer feature that is potentially valuable. In this work,
we propose the Group-Transformer model (GTrans) that flexibly divides
multi-layer representations of both encoder and decoder into different groups
and then fuses these group features to generate target words. To corroborate
the effectiveness of the proposed method, extensive experiments and analytic
experiments are conducted on three bilingual translation benchmarks and two
multilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14
and OPUS-100 benchmark. Experimental and analytical results demonstrate that
our model outperforms its Transformer counterparts by a consistent gain.
Furthermore, it can be successfully scaled up to 60 encoder layers and 36
decoder layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deriving dynamical systems for language based on the Tolerance Principle. (arXiv:2209.04261v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.04261">
<div class="article-summary-box-inner">
<span><p>In this research note, I derive explicit dynamical systems for language
within an acquisition-driven framework (Niyogi \&amp; Berwick, 1997; Niyogi, 2006)
assuming that children/learners follow the Tolerance Principle (Yang, 2016) to
determine whether a rule is productive during the process of language
acquisition. I consider different theoretical parameters such as population
size (finite vs. infinite) and the number of previous generations that provide
learners with data. Multiple simulations of the dynamics obtained here and
applications to diacrhonic language data are in preparation, so they are not
included in this first note.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The (In)Effectiveness of Intermediate Task Training For Domain Adaptation and Cross-Lingual Transfer Learning. (arXiv:2210.01091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01091">
<div class="article-summary-box-inner">
<span><p>Transfer learning from large language models (LLMs) has emerged as a powerful
technique to enable knowledge-based fine-tuning for a number of tasks,
adaptation of models for different domains and even languages. However, it
remains an open question, if and when transfer learning will work, i.e. leading
to positive or negative transfer. In this paper, we analyze the knowledge
transfer across three natural language processing (NLP) tasks - text
classification, sentimental analysis, and sentence similarity, using three LLMs
- BERT, RoBERTa, and XLNet - and analyzing their performance, by fine-tuning on
target datasets for domain and cross-lingual adaptation tasks, with and without
an intermediate task training on a larger dataset. Our experiments showed that
fine-tuning without an intermediate task training can lead to a better
performance for most tasks, while more generalized tasks might necessitate a
preceding intermediate task training step. We hope that this work will act as a
guide on transfer learning to NLP practitioners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino. (arXiv:2210.02675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.02675">
<div class="article-summary-box-inner">
<span><p>With 84.75 million Filipinos online, the ability for models to process online
text is crucial for developing Filipino NLP applications. To this end, spelling
correction is a crucial preprocessing step for downstream processing. However,
the lack of data prevents the use of language models for this task. In this
paper, we propose an N-Gram + Damerau Levenshtein distance model with automatic
rule extraction. We train the model on 300 samples, and show that despite
limited training data, it achieves good performance and outperforms other deep
learning approaches in terms of accuracy and edit distance. Moreover, the model
(1) requires little compute power, (2) trains in little time, thus allowing for
retraining, and (3) is easily interpretable, allowing for direct
troubleshooting, highlighting the success of traditional approaches over more
complex deep learning models in settings where data is unavailable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crisis Response. (arXiv:2210.04573v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04573">
<div class="article-summary-box-inner">
<span><p>Timely and effective response to humanitarian crises requires quick and
accurate analysis of large amounts of text data - a process that can highly
benefit from expert-assisted NLP systems trained on validated and annotated
data in the humanitarian response domain. To enable creation of such NLP
systems, we introduce and release HumSet, a novel and rich multilingual dataset
of humanitarian response documents annotated by experts in the humanitarian
response community. The dataset provides documents in three languages (English,
French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021
across the globe. For each document, HUMSET provides selected snippets
(entries) as well as assigned classes to each entry annotated using common
humanitarian information analysis frameworks. HUMSET also provides novel and
challenging entry extraction and multi-label entry classification tasks. In
this paper, we take a first step towards approaching these tasks and conduct a
set of experiments on Pre-trained Language Models (PLM) to establish strong
baselines for future research in this domain. The dataset is available at
https://blog.thedeep.io/humset/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker- and Age-Invariant Training for Child Acoustic Modeling Using Adversarial Multi-Task Learning. (arXiv:2210.10231v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10231">
<div class="article-summary-box-inner">
<span><p>One of the major challenges in acoustic modelling of child speech is the
rapid changes that occur in the children's articulators as they grow up, their
differing growth rates and the subsequent high variability in the same age
group. These high acoustic variations along with the scarcity of child speech
corpora have impeded the development of a reliable speech recognition system
for children. In this paper, a speaker- and age-invariant training approach
based on adversarial multi-task learning is proposed. The system consists of
one generator shared network that learns to generate speaker- and age-invariant
features connected to three discrimination networks, for phoneme, age, and
speaker. The generator network is trained to minimize the
phoneme-discrimination loss and maximize the speaker- and age-discrimination
losses in an adversarial multi-task learning fashion. The generator network is
a Time Delay Neural Network (TDNN) architecture while the three discriminators
are feed-forward networks. The system was applied to the OGI speech corpora and
achieved a 13% reduction in the WER of the ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling. (arXiv:2210.10369v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10369">
<div class="article-summary-box-inner">
<span><p>Recent joint multiple intent detection and slot filling models employ label
embeddings to achieve the semantics-label interactions. However, they treat all
labels and label embeddings as uncorrelated individuals, ignoring the
dependencies among them. Besides, they conduct the decoding for the two tasks
independently, without leveraging the correlations between them. Therefore, in
this paper, we first construct a Heterogeneous Label Graph (HLG) containing two
kinds of topologies: (1) statistical dependencies based on labels'
co-occurrence patterns and hierarchies in slot labels; (2) rich relations among
the label nodes. Then we propose a novel model termed ReLa-Net. It can capture
beneficial correlations among the labels from HLG. The label correlations are
leveraged to enhance semantic-label interactions. Moreover, we also propose the
label-aware inter-dependent decoding mechanism to further exploit the label
correlations for decoding. Experiment results show that our ReLa-Net
significantly outperforms previous models. Remarkably, ReLa-Net surpasses the
previous best model by over 20\% in terms of overall accuracy on MixATIS
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs. (arXiv:2210.10375v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10375">
<div class="article-summary-box-inner">
<span><p>Recent graph-based models for joint multiple intent detection and slot
filling have obtained promising results through modeling the guidance from the
prediction of intents to the decoding of slot filling. However, existing
methods (1) only model the \textit{unidirectional guidance} from intent to
slot; (2) adopt \textit{homogeneous graphs} to model the interactions between
the slot semantics nodes and intent label nodes, which limit the performance.
In this paper, we propose a novel model termed Co-guiding Net, which implements
a two-stage framework achieving the \textit{mutual guidances} between the two
tasks. In the first stage, the initial estimated labels of both tasks are
produced, and then they are leveraged in the second stage to model the mutual
guidances. Specifically, we propose two \textit{heterogeneous graph attention
networks} working on the proposed two \textit{heterogeneous semantics-label
graphs}, which effectively represent the relations among the semantics nodes
and label nodes. Experiment results show that our model outperforms existing
models by a large margin, obtaining a relative improvement of 19.3\% over the
previous best model on MixATIS dataset in overall accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving abstractive summarization with energy-based re-ranking. (arXiv:2210.15553v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15553">
<div class="article-summary-box-inner">
<span><p>Current abstractive summarization systems present important weaknesses which
prevent their deployment in real-world applications, such as the omission of
relevant information and the generation of factual inconsistencies (also known
as hallucinations). At the same time, automatic evaluation metrics such as CTC
scores have been recently proposed that exhibit a higher correlation with human
judgments than traditional lexical-overlap metrics such as ROUGE. In this work,
we intend to close the loop by leveraging the recent advances in summarization
metrics to create quality-aware abstractive summarizers. Namely, we propose an
energy-based model that learns to re-rank summaries according to one or a
combination of these metrics. We experiment using several metrics to train our
energy-based re-ranker and show that it consistently improves the scores
achieved by the predicted summaries. Nonetheless, human evaluation results show
that the re-ranking approach should be used with care for highly abstractive
summaries, as the available metrics are not yet sufficiently reliable for this
purpose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking. (arXiv:2210.17168v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17168">
<div class="article-summary-box-inner">
<span><p>Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has
widespread applications. Existing systems typically utilize BERT for text
encoding. However, CSC requires the model to account for both phonetic and
graphemic information. To adapt BERT to the CSC task, we propose a token-level
self-distillation contrastive learning method. We employ BERT to encode both
the corrupted and corresponding correct sentence. Then, we use contrastive
learning loss to regularize corrupted tokens' hidden states to be closer to
counterparts in the correct sentence. On three CSC datasets, we confirmed our
method provides a significant improvement above baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Event Extraction via Tracking Visual States of Arguments. (arXiv:2211.01781v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01781">
<div class="article-summary-box-inner">
<span><p>Video event extraction aims to detect salient events from a video and
identify the arguments for each event as well as their semantic roles. Existing
methods focus on capturing the overall visual scene of each frame, ignoring
fine-grained argument-level information. Inspired by the definition of events
as changes of states, we propose a novel framework to detect video events by
tracking the changes in the visual states of all involved arguments, which are
expected to provide the most informative evidence for the extraction of video
events. In order to capture the visual state changes of arguments, we decompose
them into changes in pixels within objects, displacements of objects, and
interactions among multiple arguments. We further propose Object State
Embedding, Object Motion-aware Embedding and Argument Interaction Embedding to
encode and track these changes respectively. Experiments on various video event
extraction tasks demonstrate significant improvements compared to
state-of-the-art models. In particular, on verb classification, we achieve
3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation
Recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Path to Autonomous Learners. (arXiv:2211.02403v1 [stat.ML] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02403">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new theoretical approach for enabling domain
knowledge acquisition by intelligent systems. We introduce a hybrid model that
starts with minimal input knowledge in the form of an upper ontology of
concepts, stores and reasons over this knowledge through a knowledge graph
database and learns new information through a Logic Neural Network. We study
the behavior of this architecture when handling new data and show that the
final system is capable of enriching its current knowledge as well as extending
it to new domains.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-08 23:17:07.165875454 UTC">2022-11-08 23:17:07 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>