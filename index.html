<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-12T01:30:00Z">04-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Expectations over Unspoken Alternatives Predict Pragmatic Inferences. (arXiv:2304.04758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04758">
<div class="article-summary-box-inner">
<span><p>Scalar inferences (SI) are a signature example of how humans interpret
language based on unspoken alternatives. While empirical studies have
demonstrated that human SI rates are highly variable -- both within instances
of a single scale, and across different scales -- there have been few proposals
that quantitatively explain both cross- and within-scale variation.
Furthermore, while it is generally assumed that SIs arise through reasoning
about unspoken alternatives, it remains debated whether humans reason about
alternatives as linguistic forms, or at the level of concepts. Here, we test a
shared mechanism explaining SI rates within and across scales: context-driven
expectations about the unspoken alternatives. Using neural language models to
approximate human predictive distributions, we find that SI rates are captured
by the expectedness of the strong scalemate as an alternative. Crucially,
however, expectedness robustly predicts cross-scale variation only under a
meaning-based view of alternatives. Our results suggest that pragmatic
inferences arise from context-driven expectations over alternatives, and these
expectations operate at the level of concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Temporalities on Stance Detection Towards COVID-19 Vaccination. (arXiv:2304.04806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04806">
<div class="article-summary-box-inner">
<span><p>Previous studies have highlighted the importance of vaccination as an
effective strategy to control the transmission of the COVID-19 virus. It is
crucial for policymakers to have a comprehensive understanding of the public's
stance towards vaccination on a large scale. However, attitudes towards
COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved
over time on social media. Thus, it is necessary to account for possible
temporal shifts when analysing these stances. This study aims to examine the
impact of temporal concept drift on stance detection towards COVID-19
vaccination on Twitter. To this end, we evaluate a range of transformer-based
models using chronological and random splits of social media data. Our findings
demonstrate significant discrepancies in model performance when comparing
random and chronological splits across all monolingual and multilingual
datasets. Chronological splits significantly reduce the accuracy of stance
classification. Therefore, real-world stance detection approaches need to be
further refined to incorporate temporal factors as a key consideration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-Scale Comparative Study of Accurate COVID-19 Information versus Misinformation. (arXiv:2304.04811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04811">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic led to an infodemic where an overwhelming amount of
COVID-19 related content was being disseminated at high velocity through social
media. This made it challenging for citizens to differentiate between accurate
and inaccurate information about COVID-19. This motivated us to carry out a
comparative study of the characteristics of COVID-19 misinformation versus
those of accurate COVID-19 information through a large-scale computational
analysis of over 242 million tweets. The study makes comparisons alongside four
key aspects: 1) the distribution of topics, 2) the live status of tweets, 3)
language analysis and 4) the spreading power over time. An added contribution
of this study is the creation of a COVID-19 misinformation classification
dataset. Finally, we demonstrate that this new dataset helps improve
misinformation classification by more than 9% based on average F1 measure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Knowledge Selection for Knowledge-Grounded Dialogues. (arXiv:2304.04836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04836">
<div class="article-summary-box-inner">
<span><p>Knowledge selection is the key in knowledge-grounded dialogues (KGD), which
aims to select an appropriate knowledge snippet to be used in the utterance
based on dialogue history. Previous studies mainly employ the classification
approach to classify each candidate snippet as "relevant" or "irrelevant"
independently. However, such approaches neglect the interactions between
snippets, leading to difficulties in inferring the meaning of snippets.
Moreover, they lack modeling of the discourse structure of dialogue-knowledge
interactions. We propose a simple yet effective generative approach for
knowledge selection, called GenKS. GenKS learns to select snippets by
generating their identifiers with a sequence-to-sequence model. GenKS therefore
captures intra-knowledge interaction inherently through attention mechanisms.
Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge
interactions explicitly. We conduct experiments on three benchmark datasets,
and verify GenKS achieves the best results on both knowledge selection and
response generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach. (arXiv:2304.04881v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04881">
<div class="article-summary-box-inner">
<span><p>Multiple choice questions (MCQs) are an efficient and common way to assess
reading comprehension (RC). Every MCQ needs a set of distractor answers that
are incorrect, but plausible enough to test student knowledge. Distractor
generation (DG) models have been proposed, and their performance is typically
evaluated using machine translation (MT) metrics. However, MT metrics often
misjudge the suitability of generated distractors. We propose DISTO: the first
learned evaluation metric for generated distractors. We validate DISTO by
showing its scores correlate highly with human ratings of distractor quality.
At the same time, DISTO ranks the performance of state-of-the-art DG models
very differently from MT-based metrics, showing that MT metrics should not be
used for distractor evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Vision-and-Language Navigation by Generating Future-View Image Semantics. (arXiv:2304.04907v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04907">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) is the task that requires an agent to
navigate through the environment based on natural language instructions. At
each step, the agent takes the next action by selecting from a set of navigable
locations. In this paper, we aim to take one step further and explore whether
the agent can benefit from generating the potential future view during
navigation. Intuitively, humans will have an expectation of how the future
environment will look like, based on the natural language instructions and
surrounding views, which will aid correct navigation. Hence, to equip the agent
with this ability to generate the semantics of future navigation views, we
first propose three proxy tasks during the agent's in-domain pre-training:
Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action
Prediction with Image Generation (APIG). These three objectives teach the model
to predict missing views in a panorama (MPM), predict missing steps in the full
trajectory (MTM), and generate the next view based on the full instruction and
navigation history (APIG), respectively. We then fine-tune the agent on the VLN
task with an auxiliary loss that minimizes the difference between the view
semantics generated by the agent and the ground truth view semantics of the
next step. Empirically, our VLN-SIG achieves the new state-of-the-art on both
the Room-to-Room dataset and the CVDN dataset. We further show that our agent
learns to fill in missing patches in future views qualitatively, which brings
more interpretability over agents' predicted actions. Lastly, we demonstrate
that learning to predict future view semantics also enables the agent to have
better performance on longer paths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04933">
<div class="article-summary-box-inner">
<span><p>Resource limitations make it hard to provide all students with one of the
most effective educational interventions: personalized instruction.
Reinforcement learning could be a key tool to reduce the development cost and
improve the effectiveness of, intelligent tutoring software that aims to
provide the right support, at the right time, to a student. Here we illustrate
that deep reinforcement learning can be used to provide adaptive pedagogical
support to students learning about the concept of volume in a narrative
storyline software. Using explainable artificial intelligence tools, we also
extracted interpretable insights about the pedagogical policy learned, and we
demonstrate that the resulting policy had similar performance in a different
student population. Most importantly, in both studies the
reinforcement-learning narrative system had the largest benefit for those
students with the lowest initial pretest scores, suggesting the opportunity for
AI to adapt and provide support for those most in need.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-Level Relation Extraction via Contrastive Learning with Descriptive Relation Prompts. (arXiv:2304.04935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04935">
<div class="article-summary-box-inner">
<span><p>Sentence-level relation extraction aims to identify the relation between two
entities for a given sentence. The existing works mostly focus on obtaining a
better entity representation and adopting a multi-label classifier for relation
extraction. A major limitation of these works is that they ignore background
relational knowledge and the interrelation between entity types and candidate
relations. In this work, we propose a new paradigm, Contrastive Learning with
Descriptive Relation Prompts(CTL-DRP), to jointly consider entity information,
relational knowledge and entity type restrictions. In particular, we introduce
an improved entity marker and descriptive relation prompts when generating
contextual embedding, and utilize contrastive learning to rank the restricted
candidate relations. The CTL-DRP obtains a competitive F1-score of 76.7% on
TACRED. Furthermore, the new presented paradigm achieves F1-scores of 85.8% and
91.6% on TACREV and Re-TACRED respectively, which are both the state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference. (arXiv:2304.04947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04947">
<div class="article-summary-box-inner">
<span><p>We propose Conditional Adapter (CoDA), a parameter-efficient transfer
learning method that also improves inference efficiency. CoDA generalizes
beyond standard adapter approaches to enable a new way of balancing speed and
accuracy using conditional computation. Starting with an existing dense
pretrained model, CoDA adds sparse activation together with a small number of
new parameters and a light-weight training phase. Our experiments demonstrate
that the CoDA approach provides an unexpectedly efficient way to transfer
knowledge. Across a variety of language, vision, and speech tasks, CoDA
achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter
approach with moderate to no accuracy loss and the same parameter efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition. (arXiv:2304.04991v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04991">
<div class="article-summary-box-inner">
<span><p>In recent years, a great deal of attention has been paid to the Transformer
network for speech recognition tasks due to its excellent model performance.
However, the Transformer network always involves heavy computation and large
number of parameters, causing serious deployment problems in devices with
limited computation sources or storage memory. In this paper, a new lightweight
model called Sim-T has been proposed to expand the generality of the
Transformer model. Under the help of the newly developed multiplexing
technique, the Sim-T can efficiently compress the model with negligible
sacrifice on its performance. To be more precise, the proposed technique
includes two parts, that are, module weight multiplexing and attention score
multiplexing. Moreover, a novel decoder structure has been proposed to
facilitate the attention score multiplexing. Extensive experiments have been
conducted to validate the effectiveness of Sim-T. In Aishell-1 dataset, when
the proposed Sim-T is 48% parameter less than the baseline Transformer, 0.4%
CER improvement can be obtained. Alternatively, 69% parameter reduction can be
achieved if the Sim-T gives the same performance as the baseline Transformer.
With regard to the HKUST and WSJ eval92 datasets, CER and WER will be improved
by 0.3% and 0.2%, respectively, when parameters in Sim-T are 40% less than the
baseline Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-machine cooperation for semantic feature listing. (arXiv:2304.05012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05012">
<div class="article-summary-box-inner">
<span><p>Semantic feature norms, lists of features that concepts do and do not
possess, have played a central role in characterizing human conceptual
knowledge, but require extensive human labor. Large language models (LLMs)
offer a novel avenue for the automatic generation of such feature lists, but
are prone to significant error. Here, we present a new method for combining a
learned model of human lexical-semantics from limited data with LLM-generated
data to efficiently generate high-quality feature norms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Food Do We Tweet about on a Rainy Day?. (arXiv:2304.05041v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05041">
<div class="article-summary-box-inner">
<span><p>Food choice is a complex phenomenon shaped by factors such as taste,
ambience, culture or weather. In this paper, we explore food-related tweeting
in different weather conditions. We inspect a Latvian food tweet dataset
spanning the past decade in conjunction with a weather observation dataset
consisting of average temperature, precipitation, and other phenomena. We find
which weather conditions lead to specific food information sharing;
automatically classify tweet sentiment and discuss how it changes depending on
the weather. This research contributes to the growing area of large-scale
social network data understanding of food consumers' choices and perceptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training. (arXiv:2304.05051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05051">
<div class="article-summary-box-inner">
<span><p>Fashion vision-language pre-training models have shown efficacy for a wide
range of downstream tasks. However, general vision-language pre-training models
pay less attention to fine-grained domain features, while these features are
important in distinguishing the specific domain tasks from general tasks. We
propose a method for fine-grained fashion vision-language pre-training based on
fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained
multi-modalities fashion attributes and characteristics. Firstly, we propose
the fashion symbols, a novel abstract fashion concept layer, to represent
different fashion items and to generalize various kinds of fine-grained fashion
features, making modelling fine-grained attributes more effective. Secondly,
the attributes prompt method is proposed to make the model learn specific
attributes of fashion items explicitly. We design proper prompt templates
according to the format of fashion data. Comprehensive experiments are
conducted on two public fashion benchmarks, i.e., FashionGen and FashionIQ, and
FashionSAP gets SOTA performances for four popular fashion tasks. The ablation
study also shows the proposed abstract fashion symbols, and the attribute
prompt method enables the model to acquire fine-grained semantics in the
fashion domain effectively. The obvious performance gains from FashionSAP
provide a new baseline for future fashion task research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05128">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have achieved impressive performance on code
generation. However, for complex programming tasks, generating the correct
solution in one go becomes challenging, thus some prior works have designed
program repair approaches to improve code generation performance. In this work,
we propose Self-Debugging, which teaches a large language model to debug its
predicted program via few-shot demonstrations. In particular, we demonstrate
that Self-Debugging can teach the large language model to perform rubber duck
debugging; i.e., without any feedback on the code correctness or error
messages, the model is able to identify its mistakes by explaining the
generated code in natural language. Self-Debugging achieves the
state-of-the-art performance on several code generation benchmarks, including
the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python
translation, and MBPP for text-to-Python generation. On the Spider benchmark
where there are no unit tests to verify the correctness of predictions,
Self-Debugging with code explanation consistently improves the baseline by
2-3%, and improves the prediction accuracy on problems of the hardest label by
9%. On TransCoder and MBPP where unit tests are available, Self-Debugging
improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback
messages and reusing failed predictions, Self-Debugging notably improves sample
efficiency, and can match or outperform baseline models that generate more than
10x candidate programs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05197">
<div class="article-summary-box-inner">
<span><p>With the rapid progress of large language models (LLMs), many downstream NLP
tasks can be well solved given good prompts. Though model developers and
researchers work hard on dialog safety to avoid generating harmful content from
LLMs, it is still challenging to steer AI-generated content (AIGC) for the
human good. As powerful LLMs are devouring existing text data from various
domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether
the private information is included in the training data and what privacy
threats can these LLMs and their downstream applications bring. In this paper,
we study the privacy threats from OpenAI's model APIs and New Bing enhanced by
ChatGPT and show that application-integrated LLMs may cause more severe privacy
threats ever than before. To this end, we conduct extensive experiments to
support our claims and discuss LLMs' privacy implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and generative models for Vietnamese multi-document summarization. (arXiv:2304.05205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05205">
<div class="article-summary-box-inner">
<span><p>Multi-document summarization is challenging because the summaries should not
only describe the most important information from all documents but also
provide a coherent interpretation of the documents. This paper proposes a
method for multi-document summarization based on cluster similarity. In the
extractive method we use hybrid model based on a modified version of the
PageRank algorithm and a text correlation considerations mechanism. After
generating summaries by selecting the most important sentences from each
cluster, we apply BARTpho and ViT5 to construct the abstractive models. Both
extractive and abstractive approaches were considered in this study. The
proposed method achieves competitive results in VLSP 2022 competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond. (arXiv:2304.05216v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05216">
<div class="article-summary-box-inner">
<span><p>Recently, fine-tuning pre-trained code models such as CodeBERT on downstream
tasks has achieved great success in many software testing and analysis tasks.
While effective and prevalent, fine-tuning the pre-trained parameters incurs a
large computational cost. In this paper, we conduct an extensive experimental
study to explore what happens to layer-wise pre-trained representations and
their encoded code knowledge during fine-tuning. We then propose efficient
alternatives to fine-tune the large pre-trained code model based on the above
findings. Our experimental study shows that (1) lexical, syntactic and
structural properties of source code are encoded in the lower, intermediate,
and higher layers, respectively, while the semantic property spans across the
entire model. (2) The process of fine-tuning preserves most of the code
properties. Specifically, the basic code properties captured by lower and
intermediate layers are still preserved during fine-tuning. Furthermore, we
find that only the representations of the top two layers change most during
fine-tuning for various downstream tasks. (3) Based on the above findings, we
propose Telly to efficiently fine-tune pre-trained code models via layer
freezing. The extensive experimental results on five various downstream tasks
demonstrate that training parameters and the corresponding time cost are
greatly reduced, while performances are similar or better. Replication package
including source code, datasets, and online Appendix is available at:
\url{https://github.com/DeepSoftwareAnalytics/Telly}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards preserving word order importance through Forced Invalidation. (arXiv:2304.05221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05221">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models such as BERT have been widely used as a
framework for natural language understanding (NLU) tasks. However, recent
findings have revealed that pre-trained language models are insensitive to word
order. The performance on NLU tasks remains unchanged even after randomly
permuting the word of a sentence, where crucial syntactic information is
destroyed. To help preserve the importance of word order, we propose a simple
approach called Forced Invalidation (FI): forcing the model to identify
permuted sequences as invalid samples. We perform an extensive evaluation of
our approach on various English NLU and QA based tasks over BERT-based and
attention-based models over word embeddings. Our experiments demonstrate that
Forced Invalidation significantly improves the sensitivity of the models to
word order.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximating Human Evaluation of Social Chatbots with Prompting. (arXiv:2304.05253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05253">
<div class="article-summary-box-inner">
<span><p>Once powerful conversational models have become available for a wide
audience, users started actively engaging in social interactions with this
technology. Such unprecedented interaction experiences may pose considerable
social and psychological risks to the users unless the technology is properly
controlled. This creates an urgent need for scalable and robust evaluation
metrics for conversational chatbots. Existing automatic evaluation metrics
usually focus on objective quality measures and disregard subjective
perceptions of social dimensions. Moreover, most of these approaches operate on
pre-produced dialogs from available benchmark corpora, which implies human
involvement for preparing the material for evaluation and, thus, impeded
scalability of the metrics. To address this limitation, we propose to make use
of the emerging large language models (LLMs) from the GPT-family and describe a
new framework allowing to conduct dialog system evaluation with prompting. With
this framework, we are able to achieve full automation of the evaluation
pipeline and reach impressive correlation with the human judgement (up to
Pearson r=0.95 on system level). The underlying concept is to collect synthetic
chat logs of evaluated bots with a LLM in the other-play setting, where LLM is
carefully conditioned to follow a specific scenario. We further explore
different prompting approaches to produce evaluation scores with the same LLM.
The best-performing prompts, containing few-show demonstrations and
instructions, show outstanding performance on the tested dataset and
demonstrate the ability to generalize to other dialog corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05265">
<div class="article-summary-box-inner">
<span><p>The recent large-scale generative modeling has attained unprecedented
performance especially in producing high-fidelity images driven by text
prompts. Text inversion (TI), alongside the text-to-image model backbones, is
proposed as an effective technique in personalizing the generation when the
prompts contain user-defined, unseen or long-tail concept tokens. Despite that,
we find and show that the deployment of TI remains full of "dark-magics" -- to
name a few, the harsh requirement of additional datasets, arduous human efforts
in the loop and lack of robustness. In this work, we propose a much-enhanced
version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all
the aforementioned problems and in turn delivering a robust, data-efficient and
easy-to-use framework. The core to COTI is a theoretically-guided loss
objective instantiated with a comprehensive and novel weighted scoring
mechanism, encapsulated by an active-learning paradigm. The extensive results
show that COTI significantly outperforms the prior TI-related approaches with a
26.05 decrease in the FID score and a 23.00% boost in the R-precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Entity-based Claim Extraction Pipeline for Real-world Biomedical Fact-checking. (arXiv:2304.05268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05268">
<div class="article-summary-box-inner">
<span><p>Existing fact-checking models for biomedical claims are typically trained on
synthetic or well-worded data and hardly transfer to social media content. This
mismatch can be mitigated by adapting the social media input to mimic the
focused nature of common training claims. To do so, Wuehrl &amp; Klinger (2022)
propose to extract concise claims based on medical entities in the text.
However, their study has two limitations: First, it relies on gold-annotated
entities. Therefore, its feasibility for a real-world application cannot be
assessed since this requires detecting relevant entities automatically. Second,
they represent claim entities with the original tokens. This constitutes a
terminology mismatch which potentially limits the fact-checking performance. To
understand both challenges, we propose a claim extraction pipeline for medical
tweets that incorporates named entity recognition and terminology normalization
via entity linking. We show that automatic NER does lead to a performance drop
in comparison to using gold annotations but the fact-checking performance still
improves considerably over inputting the unchanged tweets. Normalizing entities
to their canonical forms does, however, not improve the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05302">
<div class="article-summary-box-inner">
<span><p>Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment
of large language models with human preferences, significantly enhancing the
quality of interactions between humans and these models. InstructGPT implements
RLHF through several stages, including Supervised Fine-Tuning (SFT), reward
model training, and Proximal Policy Optimization (PPO). PPO, however, is
sensitive to hyperparameters and requires a minimum of four models in its
standard implementation, which makes it hard to train. In contrast, we propose
a novel learning paradigm called RRHF, which scores responses generated by
different sampling policies and learns to align them with human preferences
through ranking loss. RRHF can efficiently align language model output
probabilities with human preferences as robust as fine-tuning and it only needs
1 to 2 models during tuning. In addition, RRHF can be considered an extension
of SFT and reward models while being simpler than PPO in terms of coding, model
counts, and hyperparameters. The entire alignment process can be accomplished
within a single RRHF training session. We evaluate RRHF using LLaMA and Alpaca
on Helpful and Harmless data, demonstrating performance comparable to PPO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELVIS: Empowering Locality of Vision Language Pre-training with Intra-modal Similarity. (arXiv:2304.05303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05303">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown great potential in assisting radiologists in reading
chest X-ray (CXR) images, but its need for expensive annotations for improving
performance prevents widespread clinical application. Visual language
pre-training (VLP) can alleviate the burden and cost of annotation by
leveraging routinely generated reports for radiographs, which exist in large
quantities as well as in paired form (imagetext pairs). Additionally,
extensions to localization-aware VLPs are being proposed to address the needs
of accurate localization of abnormalities for CAD in CXR. However, we find that
the formulation proposed by locality-aware VLP literatures actually leads to
loss in spatial relationships required for downstream localization tasks.
Therefore, we propose Empowering Locality of VLP with Intra-modal Similarity,
ELVIS, a VLP aware of intra-modal locality, to better preserve the locality
within radiographs or reports, which enhances the ability to comprehend
location references in text reports. Our locality-aware VLP method
significantly outperforms state-of-the art baselines in multiple segmentation
tasks and the MS-CXR phrase grounding task. Qualitatively, ELVIS is able to
focus well on regions of interest described in the report text compared to
prior approaches, allowing for enhanced interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent autonomous scientific research capabilities of large language models. (arXiv:2304.05332v1 [physics.chem-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05332">
<div class="article-summary-box-inner">
<span><p>Transformer-based large language models are rapidly advancing in the field of
machine learning research, with applications spanning natural language,
biology, chemistry, and computer programming. Extreme scaling and reinforcement
learning from human feedback have significantly improved the quality of
generated text, enabling these models to perform various tasks and reason about
their choices. In this paper, we present an Intelligent Agent system that
combines multiple large language models for autonomous design, planning, and
execution of scientific experiments. We showcase the Agent's scientific
research capabilities with three distinct examples, with the most complex being
the successful performance of catalyzed cross-coupling reactions. Finally, we
discuss the safety implications of such systems and propose measures to prevent
their misuse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. (arXiv:2304.05335v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05335">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown incredible capabilities and
transcended the natural language processing (NLP) community, with adoption
throughout many services like healthcare, therapy, education, and customer
service. Since users include people with critical information needs like
students or patients engaging with chatbots, the safety of these systems is of
prime importance. Therefore, a clear understanding of the capabilities and
limitations of LLMs is necessary. To this end, we systematically evaluate
toxicity in over half a million generations of ChatGPT, a popular
dialogue-based LLM. We find that setting the system parameter of ChatGPT by
assigning it a persona, say that of the boxer Muhammad Ali, significantly
increases the toxicity of generations. Depending on the persona assigned to
ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect
stereotypes, harmful dialogue, and hurtful opinions. This may be potentially
defamatory to the persona and harmful to an unsuspecting user. Furthermore, we
find concerning patterns where specific entities (e.g., certain races) are
targeted more than others (3x more) irrespective of the assigned persona, that
reflect inherent discriminatory biases in the model. We hope that our findings
inspire the broader AI community to rethink the efficacy of current safety
guardrails and develop better techniques that lead to robust, safe, and
trustworthy AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Use of Foundation Models for Named Entity Recognition and Lemmatization Tasks in Slavic Languages. (arXiv:2304.05336v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05336">
<div class="article-summary-box-inner">
<span><p>This paper describes Adam Mickiewicz University's (AMU) solution for the 4th
Shared Task on SlavNER. The task involves the identification, categorization,
and lemmatization of named entities in Slavic languages. Our approach involved
exploring the use of foundation models for these tasks. In particular, we used
models based on the popular BERT and T5 model architectures. Additionally, we
used external datasets to further improve the quality of our models. Our
solution obtained promising results, achieving high metrics scores in both
tasks. We describe our approach and the results of our experiments in detail,
showing that the method is effective for NER and lemmatization in Slavic
languages. Additionally, our models for lemmatization will be available at:
https://huggingface.co/amu-cai.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05351">
<div class="article-summary-box-inner">
<span><p>Recently, large language models (LLMs) like ChatGPT have demonstrated
remarkable performance across a variety of natural language processing tasks.
However, their effectiveness in the financial domain, specifically in
predicting stock market movements, remains to be explored. In this paper, we
conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal
stock movement prediction, on three tweets and historical stock price datasets.
Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited
success in predicting stock movements, as it underperforms not only
state-of-the-art methods but also traditional methods like linear regression
using price features. Despite the potential of Chain-of-Thought prompting
strategies and the inclusion of tweets, ChatGPT's performance remains subpar.
Furthermore, we observe limitations in its explainability and stability,
suggesting the need for more specialized training or fine-tuning. This research
provides insights into ChatGPT's capabilities and serves as a foundation for
future work aimed at improving financial market analysis and prediction by
leveraging social media sentiment and historical stock data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05368">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have made significant progress in various
domains, including healthcare. However, the specialized nature of clinical
language understanding tasks presents unique challenges and limitations that
warrant further investigation. In this study, we conduct a comprehensive
evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within
the realm of clinical language understanding tasks. These tasks span a diverse
range, including named entity recognition, relation extraction, natural
language inference, semantic textual similarity, document classification, and
question-answering. We also introduce a novel prompting strategy,
self-questioning prompting (SQP), tailored to enhance LLMs' performance by
eliciting informative questions and answers pertinent to the clinical scenarios
at hand. Our evaluation underscores the significance of task-specific learning
strategies and prompting techniques for improving LLMs' effectiveness in
healthcare-related tasks. Additionally, our in-depth error analysis on the
challenging relation extraction task offers valuable insights into error
distribution and potential avenues for improvement using SQP. Our study sheds
light on the practical implications of employing LLMs in the specialized domain
of healthcare, serving as a foundation for future research and the development
of potential applications in healthcare settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories. (arXiv:2304.05371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05371">
<div class="article-summary-box-inner">
<span><p>One of the new developments in chit-chat bots is a long-term memory mechanism
that remembers information from past conversations for increasing engagement
and consistency of responses. The bot is designed to extract knowledge of
personal nature from their conversation partner, e.g., stating preference for a
particular color. In this paper, we show that this memory mechanism can result
in unintended behavior. In particular, we found that one can combine a personal
statement with an informative statement that would lead the bot to remember the
informative statement alongside personal knowledge in its long term memory.
This means that the bot can be tricked into remembering misinformation which it
would regurgitate as statements of fact when recalling information relevant to
the topic of conversation. We demonstrate this vulnerability on the BlenderBot
2 framework implemented on the ParlAI platform and provide examples on the more
recent and significantly larger BlenderBot 3 model. We generate 150 examples of
misinformation, of which 114 (76%) were remembered by BlenderBot 2 when
combined with a personal statement. We further assessed the risk of this
misinformation being recalled after intervening innocuous conversation and in
response to multiple questions relevant to the injected memory. Our evaluation
was performed on both the memory-only and the combination of memory and
internet search modes of BlenderBot 2. From the combinations of these
variables, we generated 12,890 conversations and analyzed recalled
misinformation in the responses. We found that when the chat bot is questioned
on the misinformation topic, it was 328% more likely to respond with the
misinformation as fact when the misinformation was in the long-term memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance. (arXiv:2304.05372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05372">
<div class="article-summary-box-inner">
<span><p>ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that
are slated to promise different applications in diverse areas. In education,
these AI technologies have been tested for applications in assessment and
teaching. In assessment, AI has long been used in automated essay scoring and
automated item generation. One psychometric property that these tools must have
to assist or replace humans in assessment is high reliability in terms of
agreement between AI scores and human raters. In this paper, we measure the
reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and
trained humans in perceiving and rating the complexity of writing prompts.
Intraclass correlation (ICC) as a performance metric showed that the
inter-reliability of both the OpenAI ChatGPT and the Google Bard were low
against the gold standard of human ratings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06965">
<div class="article-summary-box-inner">
<span><p>Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05232">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is a natural language processing
problem that requires analyzing user-generated reviews to determine: a) The
target entity being reviewed, b) The high-level aspect to which it belongs, and
c) The sentiment expressed toward the targets and the aspects. Numerous yet
scattered corpora for ABSA make it difficult for researchers to identify
corpora best suited for a specific ABSA subtask quickly. This study aims to
present a database of corpora that can be used to train and assess autonomous
ABSA systems. Additionally, we provide an overview of the major corpora for
ABSA and its subtasks and highlight several features that researchers should
consider when selecting a corpus. Finally, we discuss the advantages and
disadvantages of current collection approaches and make recommendations for
future corpora creation. This survey examines 65 publicly available ABSA
datasets covering over 25 domains, including 45 English and 20 other languages
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14579">
<div class="article-summary-box-inner">
<span><p>Medical report generation task, which targets to produce long and coherent
descriptions of medical images, has attracted growing research interests
recently. Different from the general image captioning tasks, medical report
generation is more challenging for data-driven neural models. This is mainly
due to 1) the serious data bias and 2) the limited medical data. To alleviate
the data bias and make best use of available data, we propose a
Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically,
CMCL simulates the learning process of radiologists and optimizes the model in
a step by step manner. Firstly, CMCL estimates the difficulty of each training
instance and evaluates the competence of current model; Secondly, CMCL selects
the most suitable batch of training instances considering current model
competence. By iterating above two steps, CMCL can gradually improve the
model's performance. The experiments on the public IU-Xray and MIMIC-CXR
datasets show that CMCL can be incorporated into existing models to improve
their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.07316">
<div class="article-summary-box-inner">
<span><p>Recently proposed BERT-based evaluation metrics for text generation perform
well on standard benchmarks but are vulnerable to adversarial attacks, e.g.,
relating to information correctness. We argue that this stems (in part) from
the fact that they are models of semantic similarity. In contrast, we develop
evaluation metrics based on Natural Language Inference (NLI), which we deem a
more appropriate modeling. We design a preference-based adversarial attack
framework and show that our NLI based metrics are much more robust to the
attacks than the recent BERT-based metrics. On standard benchmarks, our NLI
based metrics outperform existing summarization metrics, but perform below SOTA
MT metrics. However, when combining existing metrics with our NLI metrics, we
obtain both higher adversarial robustness (15%-30%) and higher quality metrics
as measured on standard benchmarks (+5% to 30%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prophet Attention: Predicting Attention with Future Attention for Image Captioning. (arXiv:2210.10914v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10914">
<div class="article-summary-box-inner">
<span><p>Recently, attention based models have been used extensively in many
sequence-to-sequence learning systems. Especially for image captioning, the
attention based models are expected to ground correct image regions with proper
generated words. However, for each time step in the decoding process, the
attention based models usually use the hidden state of the current input to
attend to the image regions. Under this setting, these attention models have a
"deviated focus" problem that they calculate the attention weights based on
previous words instead of the one to be generated, impairing the performance of
both grounding and captioning. In this paper, we propose the Prophet Attention,
similar to the form of self-supervision. In the training stage, this module
utilizes the future information to calculate the "ideal" attention weights
towards image regions. These calculated "ideal" weights are further used to
regularize the "deviated" attention. In this manner, image regions are grounded
with the correct words. The proposed Prophet Attention can be easily
incorporated into existing image captioning models to improve their performance
of both grounding and captioning. The experiments on the Flickr30k Entities and
the MSCOCO datasets show that the proposed Prophet Attention consistently
outperforms baselines in both automatic metrics and human evaluations. It is
worth noticing that we set new state-of-the-arts on the two benchmark datasets
and achieve the 1st place on the leaderboard of the online MSCOCO benchmark in
terms of the default ranking score, i.e., CIDEr-c40.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15629">
<div class="article-summary-box-inner">
<span><p>Training generalist agents is difficult across several axes, requiring us to
deal with high-dimensional inputs (space), long horizons (time), and multiple
and new tasks. Recent advances with architectures have allowed for improved
scaling along one or two of these dimensions, but are still prohibitive
computationally. In this paper, we propose to address all three axes by
leveraging Language to Control Diffusion models as a hierarchical planner
conditioned on language (LCD). We effectively and efficiently scale diffusion
models for planning in extended temporal, state, and task dimensions to tackle
long horizon control problems conditioned on natural language instructions. We
compare LCD with other state-of-the-art models on the CALVIN language robotics
benchmark and find that LCD outperforms other SOTA methods in multi task
success rates while dramatically improving computational efficiency with a
single task success rate (SR) of 88.7% against the previous best of 82.6%. We
show that LCD can successfully leverage the unique strength of diffusion models
to produce coherent long range plans while addressing their weakness at
generating low-level details and control. We release our code and models at
https://github.com/ezhang7423/language-control-diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VaxxHesitancy: A Dataset for Studying Hesitancy Towards COVID-19 Vaccination on Twitter. (arXiv:2301.06660v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.06660">
<div class="article-summary-box-inner">
<span><p>Vaccine hesitancy has been a common concern, probably since vaccines were
created and, with the popularisation of social media, people started to express
their concerns about vaccines online alongside those posting pro- and
anti-vaccine content. Predictably, since the first mentions of a COVID-19
vaccine, social media users posted about their fears and concerns or about
their support and belief into the effectiveness of these rapidly developing
vaccines. Identifying and understanding the reasons behind public hesitancy
towards COVID-19 vaccines is important for policy markers that need to develop
actions to better inform the population with the aim of increasing vaccine
take-up. In the case of COVID-19, where the fast development of the vaccines
was mirrored closely by growth in anti-vaxx disinformation, automatic means of
detecting citizen attitudes towards vaccination became necessary. This is an
important computational social sciences task that requires data analysis in
order to gain in-depth understanding of the phenomena at hand. Annotated data
is also necessary for training data-driven models for more nuanced analysis of
attitudes towards vaccination. To this end, we created a new collection of over
3,101 tweets annotated with users' attitudes towards COVID-19 vaccination
(stance). Besides, we also develop a domain-specific language model (VaxxBERT)
that achieves the best predictive performance (73.0 accuracy and 69.3 F1-score)
as compared to a robust set of baselines. To the best of our knowledge, these
are the first dataset and model that model vaccine hesitancy as a category
distinct from pro- and anti-vaccine stance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.07695">
<div class="article-summary-box-inner">
<span><p>We present a new text-to-SQL dataset for electronic health records (EHRs).
The utterances were collected from 222 hospital staff, including physicians,
nurses, insurance review and health records teams, and more. To construct the
QA dataset on structured EHR data, we conducted a poll at a university hospital
and templatized the responses to create seed questions. Then, we manually
linked them to two open-source EHR databases, MIMIC-III and eICU, and included
them with various time expressions and held-out unanswerable questions in the
dataset, which were all collected from the poll. Our dataset poses a unique set
of challenges: the model needs to 1) generate SQL queries that reflect a wide
range of needs in the hospital, including simple retrieval and complex
operations such as calculating survival rate, 2) understand various time
expressions to answer time-sensitive questions in healthcare, and 3)
distinguish whether a given question is answerable or unanswerable based on the
prediction confidence. We believe our dataset, EHRSQL, could serve as a
practical benchmark to develop and assess QA models on structured EHR data and
take one step further towards bridging the gap between text-to-SQL research and
its real-life deployment in healthcare. EHRSQL is available at
https://github.com/glee4810/EHRSQL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?. (arXiv:2301.11219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11219">
<div class="article-summary-box-inner">
<span><p>Memes can sway people's opinions over social media as they combine visual and
textual information in an easy-to-consume manner. Since memes instantly turn
viral, it becomes crucial to infer their intent and potentially associated
harmfulness to take timely measures as needed. A common problem associated with
meme comprehension lies in detecting the entities referenced and characterizing
the role of each of these entities. Here, we aim to understand whether the meme
glorifies, vilifies, or victimizes each entity it refers to. To this end, we
address the task of role identification of entities in harmful memes, i.e.,
detecting who is the 'hero', the 'villain', and the 'victim' in the meme, if
any. We utilize HVVMemes - a memes dataset on US Politics and Covid-19 memes,
released recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains
memes, entities referenced, and their associated roles: hero, villain, victim,
and other. We further design VECTOR (Visual-semantic role dEteCToR), a robust
multi-modal framework for the task, which integrates entity-based contextual
information in the multi-modal representation and compare it to several
standard unimodal (text-only or image-only) or multi-modal (image+text) models.
Our experimental results show that our proposed model achieves an improvement
of 4% over the best baseline and 1% over the best competing stand-alone
submission from the shared-task. Besides divulging an extensive experimental
setup with comparative analyses, we finally highlight the challenges
encountered in addressing the complex task of semantic role labeling within
memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04360">
<div class="article-summary-box-inner">
<span><p>Recent advancements in large language models (LLMs) have led to the
development of highly potent models like OpenAI's ChatGPT. These models have
exhibited exceptional performance in a variety of tasks, such as question
answering, essay composition, and code generation. However, their effectiveness
in the healthcare sector remains uncertain. In this study, we seek to
investigate the potential of ChatGPT to aid in clinical text mining by
examining its ability to extract structured information from unstructured
healthcare texts, with a focus on biological named entity recognition and
relation extraction. However, our preliminary results indicate that employing
ChatGPT directly for these tasks resulted in poor performance and raised
privacy concerns associated with uploading patients' information to the ChatGPT
API. To overcome these limitations, we propose a new training paradigm that
involves generating a vast quantity of high-quality synthetic data with labels
utilizing ChatGPT and fine-tuning a local model for the downstream task. Our
method has resulted in significant improvements in the performance of
downstream tasks, improving the F1-score from 23.37% to 63.99% for the named
entity recognition task and from 75.86% to 83.59% for the relation extraction
task. Furthermore, generating data using ChatGPT can significantly reduce the
time and effort required for data collection and labeling, as well as mitigate
data privacy concerns. In summary, the proposed framework presents a promising
solution to enhance the applicability of LLM models to clinical text mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Classification with Decoupled Representation. (arXiv:2303.13065v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13065">
<div class="article-summary-box-inner">
<span><p>Retrieval augmented methods have shown promising results in various
classification tasks. However, existing methods focus on retrieving extra
context to enrich the input, which is noise sensitive and non-expandable. In
this paper, following this line, we propose a $k$-nearest-neighbor (KNN) -based
method for retrieval augmented classifications, which interpolates the
predicted label distribution with retrieved instances' label distributions.
Different from the standard KNN process, we propose a decoupling mechanism as
we find that shared representation for classification and retrieval hurts
performance and leads to training instability. We evaluate our method on a wide
range of classification datasets. Experimental results demonstrate the
effectiveness and robustness of our proposed method. We also conduct extra
experiments to analyze the contributions of different components in our
model.\footnote{\url{https://github.com/xnliang98/knn-cls-w-decoupling}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13988">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called "machine psychology". The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.16753">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a highly parameter-efficient approach to scaling
pre-trained language models (PLMs) to a deeper model depth. Unlike prior work
that shares all parameters or uses extra blocks, we design a more capable
parameter-sharing architecture based on matrix product operator (MPO). MPO
decomposition can reorganize and factorize the information of a parameter
matrix into two parts: the major part that contains the major information
(central tensor) and the supplementary part that only has a small proportion of
parameters (auxiliary tensors). Based on such a decomposition, our architecture
shares the central tensor across all layers for reducing the model size and
meanwhile keeps layer-specific auxiliary tensors (also using adapters) for
enhancing the adaptation flexibility. To improve the model training, we further
propose a stable initialization algorithm tailored for the MPO-based
architecture. Extensive experiments have demonstrated the effectiveness of our
proposed model in reducing the model size and achieving highly competitive
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Large Language Models. (arXiv:2303.18223v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.18223">
<div class="article-summary-box-inner">
<span><p>Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02017">
<div class="article-summary-box-inner">
<span><p>Large language models have revolutionized the field of artificial
intelligence and have been used in various applications. Among these models,
ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,
it stands out as a powerful tool that has been widely adopted. ChatGPT has been
successfully applied in numerous areas, including chatbots, content generation,
language translation, personalized recommendations, and even medical diagnosis
and treatment. Its success in these applications can be attributed to its
ability to generate human-like responses, understand natural language, and
adapt to different contexts. Its versatility and accuracy make it a powerful
tool for natural language processing (NLP). However, there are also limitations
to ChatGPT, such as its tendency to produce biased responses and its potential
to perpetuate harmful language patterns. This article provides a comprehensive
overview of ChatGPT, its applications, advantages, and limitations.
Additionally, the paper emphasizes the importance of ethical considerations
when using this robust tool in real-world scenarios. Finally, This paper
contributes to ongoing discussions surrounding artificial intelligence and its
impact on vision and NLP domains by providing insights into prompt engineering
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02213">
<div class="article-summary-box-inner">
<span><p>The amount of data has growing significance in exploring cutting-edge
materials and a number of datasets have been generated either by hand or
automated approaches. However, the materials science field struggles to
effectively utilize the abundance of data, especially in applied disciplines
where materials are evaluated based on device performance rather than their
properties. This article presents a new natural language processing (NLP) task
called structured information inference (SII) to address the complexities of
information extraction at the device level in materials science. We
accomplished this task by tuning GPT-3 on an existing perovskite solar cell
FAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8%
F1-score and extended the dataset with data published since its release. The
produced data is formatted and normalized, enabling its direct utilization as
input in subsequent data analysis. This feature empowers materials scientists
to develop models by selecting high-quality review articles within their
domain. Additionally, we designed experiments to predict the electrical
performance of solar cells and design materials or devices with targeted
parameters using large language models (LLMs). Our results demonstrate
comparable performance to traditional machine learning methods without feature
selection, highlighting the potential of LLMs to acquire scientific knowledge
and design new materials akin to materials scientists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linking Representations with Multimodal Contrastive Learning. (arXiv:2304.03464v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03464">
<div class="article-summary-box-inner">
<span><p>Many applications require grouping instances contained in diverse document
datasets into classes. Most widely used methods do not employ deep learning and
do not exploit the inherently multimodal nature of documents. Notably, record
linkage is typically conceptualized as a string-matching problem. This study
develops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a
multimodal framework for record linkage. CLIPPINGS employs end-to-end training
of symmetric vision and language bi-encoders, aligned through contrastive
language-image pre-training, to learn a metric space where the pooled
image-text representation for a given instance is close to representations in
the same class and distant from representations in different classes. At
inference time, instances can be linked by retrieving their nearest neighbor
from an offline exemplar embedding index or by clustering their
representations. The study examines two challenging applications: constructing
comprehensive supply chains for mid-20th century Japan through linking firm
level financial records - with each firm name represented by its crop in the
document image and the corresponding OCR - and detecting which image-caption
pairs in a massive corpus of historical U.S. newspapers came from the same
underlying photo wire source. CLIPPINGS outperforms widely used string matching
methods by a wide margin and also outperforms unimodal methods. Moreover, a
purely self-supervised model trained on only image-OCR pairs also outperforms
popular string-matching methods without requiring any labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit. (arXiv:2304.04596v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04596">
<div class="article-summary-box-inner">
<span><p>ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by
the broadening interests of the spoken language translation community.
ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2)
simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech
translation (S2ST) -- each task is supported with a wide variety of approaches,
differentiating ESPnet-ST-v2 from other open source spoken language translation
toolkits. This toolkit offers state-of-the-art architectures such as
transducers, hybrid CTC/attention, multi-decoders with searchable
intermediates, time-synchronous blockwise CTC/attention, Translatotron models,
and direct discrete unit models. In this paper, we describe the overall design,
example models for each task, and performance benchmarking behind ESPnet-ST-v2,
which is publicly available at https://github.com/espnet/espnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approach Intelligent Writing Assistants Usability with Seven Stages of Action. (arXiv:2304.02822v1 [cs.HC] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02822">
<div class="article-summary-box-inner">
<span><p>Despite the potential of Large Language Models (LLMs) as writing assistants,
they are plagued by issues like coherence and fluency of the model output,
trustworthiness, ownership of the generated content, and predictability of
model performance, thereby limiting their usability. In this position paper, we
propose to adopt Norman's seven stages of action as a framework to approach the
interaction design of intelligent writing assistants. We illustrate the
framework's applicability to writing tasks by providing an example of software
tutorial authoring. The paper also discusses the framework as a tool to
synthesize research on the interaction design of LLM-based tools and presents
examples of tools that support the stages of action. Finally, we briefly
outline the potential of a framework for human-LLM interaction research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-12 23:11:16.873956229 UTC">2023-04-12 23:11:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>