<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-24T01:30:00Z">03-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12795">
<div class="article-summary-box-inner">
<span><p>A scientific paper is traditionally prefaced by an abstract that summarizes
the paper. Recently, research highlights that focus on the main findings of the
paper have emerged as a complementary summary in addition to an abstract.
However, highlights are not yet as common as abstracts, and are absent in many
papers. In this paper, we aim to automatically generate research highlights
using different sections of a research paper as input. We investigate whether
the use of named entity recognition on the input improves the quality of the
generated highlights. In particular, we have used two deep learning-based
models: the first is a pointer-generator network, and the second augments the
first model with coverage mechanism. We then augment each of the above models
with named entity recognition features. The proposed method can be used to
produce highlights for papers with missing highlights. Our experiments show
that adding named entity information improves the performance of the deep
learning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12796">
<div class="article-summary-box-inner">
<span><p>People nowadays use search engines like Google, Yahoo, and Bing to find
information on the Internet. Due to explosion in data, it is helpful for users
if they are provided relevant summaries of the search results rather than just
links to webpages. Text summarization has become a vital approach to help
consumers swiftly grasp vast amounts of information.In this paper, different
pre-trained models for text summarization are evaluated on different datasets.
Specifically, we have used three different pre-trained models, namely,
google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have
considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum
to get the output from the above three models. The pre-trained models are
compared over these different datasets, each of 2000 examples, through ROUGH
and BLEU metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Features matching using natural language processing. (arXiv:2303.12804v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12804">
<div class="article-summary-box-inner">
<span><p>The feature matching is a basic step in matching different datasets. This
article proposes shows a new hybrid model of a pretrained Natural Language
Processing (NLP) based model called BERT used in parallel with a statistical
model based on Jaccard similarity to measure the similarity between list of
features from two different datasets. This reduces the time required to search
for correlations or manually match each feature from one dataset to another.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PACO: Provocation Involving Action, Culture, and Oppression. (arXiv:2303.12808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12808">
<div class="article-summary-box-inner">
<span><p>In India, people identify with a particular group based on certain attributes
such as religion. The same religious groups are often provoked against each
other. Previous studies show the role of provocation in increasing tensions
between India's two prominent religious groups: Hindus and Muslims. With the
advent of the Internet, such provocation also surfaced on social media
platforms such as WhatsApp.
</p>
<p>By leveraging an existing dataset of Indian WhatsApp posts, we identified
three categories of provoking sentences against Indian Muslims. Further, we
labeled 7,000 sentences for three provocation categories and called this
dataset PACO. We leveraged PACO to train a model that can identify provoking
sentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and
achieved a 0.851 average AUC score over five-fold cross-validation.
Automatically identifying provoking sentences could stop provoking text from
reaching out to the masses, and can prevent possible discrimination or violence
against the target religious group.
</p>
<p>Further, we studied the provocative speech through a pragmatic lens, by
identifying the dialog acts and impoliteness super-strategies used against the
religious group.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12810">
<div class="article-summary-box-inner">
<span><p>The potential of large language models (LLMs) to reason like humans has been
a highly contested topic in Machine Learning communities. However, the
reasoning abilities of humans are multifaceted and can be seen in various
forms, including analogical, spatial and moral reasoning, among others. This
fact raises the question whether LLMs can perform equally well across all these
different domains. This research work aims to investigate the performance of
LLMs on different reasoning tasks by conducting experiments that directly use
or draw inspirations from existing datasets on analogical and spatial
reasoning. Additionally, to evaluate the ability of LLMs to reason like human,
their performance is evaluted on more open-ended, natural language questions.
My findings indicate that LLMs excel at analogical and moral reasoning, yet
struggle to perform as proficiently on spatial reasoning tasks. I believe these
experiments are crucial for informing the future development of LLMs,
particularly in contexts that require diverse reasoning proficiencies. By
shedding light on the reasoning abilities of LLMs, this study aims to push
forward our understanding of how they can better emulate the cognitive
abilities of humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12816">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream tasks. Conventional KGE methods
require relatively high-dimensional entity representations to preserve the
structural information of knowledge graph, but lead to oversized model
parameters. Recent methods reduce model parameters by adopting low-dimensional
entity representations, while developing techniques (e.g., knowledge
distillation) to compensate for the reduced dimension. However, such operations
produce degraded model accuracy and limited reduction of model parameters.
Specifically, we view the concatenation of all entity representations as an
embedding layer, and then conventional KGE methods that adopt high-dimensional
entity representations equal to enlarging the width of the embedding layer to
gain expressiveness. To achieve parameter efficiency without sacrificing
accuracy, we instead increase the depth and propose a deeper embedding network
for entity representations, i.e., a narrow embedding layer and a multi-layer
dimension lifting network (LiftNet). Experiments on three public datasets show
that the proposed method (implemented based on TransE and DistMult) with
4-dimensional entity representations achieves more accurate link prediction
results than counterpart parameter-efficient KGE methods and strong KGE
baselines, including TransE and DistMult with 512-dimensional entity
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Span Masking for Temporal Understanding. (arXiv:2303.12860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12860">
<div class="article-summary-box-inner">
<span><p>Salient Span Masking (SSM) has shown itself to be an effective strategy to
improve closed-book question answering performance. SSM extends general masked
language model pretraining by creating additional unsupervised training
sentences that mask a single entity or date span, thus oversampling factual
information. Despite the success of this paradigm, the span types and sampling
strategies are relatively arbitrary and not widely studied for other tasks.
Thus, we investigate SSM from the perspective of temporal tasks, where learning
a good representation of various temporal expressions is important. To that
end, we introduce Temporal Span Masking (TSM) intermediate training. First, we
find that SSM alone improves the downstream performance on three temporal tasks
by an avg. +5.8 points. Further, we are able to achieve additional improvements
(avg. +0.29 points) by adding the TSM task. These comprise the new best
reported results on the targeted tasks. Our analysis suggests that the
effectiveness of SSM stems from the sentences chosen in the training data
rather than the mask choice: sentences with entities frequently also contain
temporal expressions. Nonetheless, the additional targeted spans of TSM can
still improve performance, especially in a zero-shot context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JaCoText: A Pretrained Model for Java Code-Text Generation. (arXiv:2303.12869v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12869">
<div class="article-summary-box-inner">
<span><p>Pretrained transformer-based models have shown high performance in natural
language generation task. However, a new wave of interest has surged: automatic
programming language generation. This task consists of translating natural
language instructions to a programming code. Despite the fact that well-known
pretrained models on language generation have achieved good performance in
learning programming languages, effort is still needed in automatic code
generation. In this paper, we introduce JaCoText, a model based on Transformers
neural network. It aims to generate java source code from natural language
text. JaCoText leverages advantages of both natural language and code
generation models. More specifically, we study some findings from the state of
the art and use them to (1) initialize our model from powerful pretrained
models, (2) explore additional pretraining on our java dataset, (3) carry out
experiments combining the unimodal and bimodal data in the training, and (4)
scale the input and output length during the fine-tuning of the model.
Conducted experiments on CONCODE dataset show that JaCoText achieves new
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12892">
<div class="article-summary-box-inner">
<span><p>In recent years, Transformer-based models such as the Switch Transformer have
achieved remarkable results in natural language processing tasks. However,
these models are often too complex and require extensive pre-training, which
limits their effectiveness for small clinical text classification tasks with
limited data. In this study, we propose a simplified Switch Transformer
framework and train it from scratch on a small French clinical text
classification dataset at CHU Sainte-Justine hospital. Our results demonstrate
that the simplified small-scale Transformer models outperform pre-trained
BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.
Additionally, using a mixture of expert mechanisms from the Switch Transformer
helps capture diverse patterns; hence, the proposed approach achieves better
results than a conventional Transformer with the self-attention mechanism.
Finally, our proposed framework achieves an accuracy of 87\%, precision at
87\%, and recall at 85\%, compared to the third-best pre-trained BERT-based
model, FlauBERT, which achieved an accuracy of 84\%, precision at 84\%, and
recall at 84\%. However, Switch Transformers have limitations, including a
generalization gap and sharp minima. We compare it with a multi-layer
perceptron neural network for small French clinical narratives classification
and show that the latter outperforms all other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets. (arXiv:2303.12898v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12898">
<div class="article-summary-box-inner">
<span><p>Electronic medical records (EMRs) are stored in relational databases. It can
be challenging to access the required information if the user is unfamiliar
with the database schema or general database fundamentals. Hence, researchers
have explored text-to-SQL generation methods that provide healthcare
professionals direct access to EMR data without needing a database expert.
However, currently available datasets have been essentially "solved" with
state-of-the-art models achieving accuracy greater than or near 90%. In this
paper, we show that there is still a long way to go before solving text-to-SQL
generation in the medical domain. To show this, we create new splits of the
existing medical text-to-SQL dataset MIMICSQL that better measure the
generalizability of the resulting models. We evaluate state-of-the-art language
models on our new split showing substantial drops in performance with accuracy
dropping from up to 92% to 28%, thus showing substantial room for improvement.
Moreover, we introduce a novel data augmentation approach to improve the
generalizability of the language models. Overall, this paper is the first step
towards developing more robust text-to-SQL models in the medical
domain.\footnote{The dataset and code will be released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification. (arXiv:2303.12936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12936">
<div class="article-summary-box-inner">
<span><p>This study evaluates the robustness of two state-of-the-art deep contextual
language representations, ELMo and DistilBERT, on supervised learning of binary
protest news classification and sentiment analysis of product reviews. A
"cross-context" setting is enabled using test sets that are distinct from the
training data. Specifically, in the news classification task, the models are
developed on local news from India and tested on the local news from China. In
the sentiment analysis task, the models are trained on movie reviews and tested
on customer reviews. This comparison is aimed at exploring the limits of the
representative power of today's Natural Language Processing systems on the path
to the systems that are generalizable to real-life scenarios. The models are
fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long
Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector
Machine are used as traditional baselines. The results show that, in binary
text classification, DistilBERT is significantly better than ELMo on
generalizing to the cross-context setting. ELMo is observed to be significantly
more robust to the cross-context test data than both baselines. On the other
hand, the baselines performed comparably well to ELMo when the training and
test data are subsets of the same corpus (no cross-context). DistilBERT is also
found to be 30% smaller and 83% faster than ELMo. The results suggest that
DistilBERT can transfer generic semantic knowledge to other domains better than
ELMo. DistilBERT is also favorable in incorporating into real-life systems for
it requires a smaller computational training budget. When generalization is not
the utmost preference and test domain is similar to the training domain, the
traditional ML algorithms can still be considered as more economic alternatives
to deep language representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13001">
<div class="article-summary-box-inner">
<span><p>The emergence of ChatGPT has recently garnered significant attention from the
computational linguistics community. To demonstrate its capabilities as a
keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the
keyphrase generation task. We evaluate its performance in various aspects,
including keyphrase generation prompts, keyphrase generation diversity,
multi-domain keyphrase generation, and long document understanding. Our
evaluation is based on six benchmark datasets, and we adopt the prompt
suggested by OpenAI while extending it to six candidate prompts. We find that
ChatGPT performs exceptionally well on all six candidate prompts, with minor
performance differences observed across the datasets. Based on our findings, we
conclude that ChatGPT has great potential for keyphrase generation. Moreover,
we discover that ChatGPT still faces challenges when it comes to generating
absent keyphrases. Meanwhile, in the final section, we also present some
limitations and future expansions of this report.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13013">
<div class="article-summary-box-inner">
<span><p>Gesture synthesis has gained significant attention as a critical research
area, focusing on producing contextually appropriate and natural gestures
corresponding to speech or textual input. Although deep learning-based
approaches have achieved remarkable progress, they often overlook the rich
semantic information present in the text, leading to less expressive and
meaningful gestures. We propose GesGPT, a novel approach to gesture generation
that leverages the semantic analysis capabilities of Large Language Models
(LLMs), such as GPT. By capitalizing on the strengths of LLMs for text
analysis, we design prompts to extract gesture-related information from textual
input. Our method entails developing prompt principles that transform gesture
generation into an intention classification problem based on GPT, and utilizing
a curated gesture library and integration module to produce semantically rich
co-speech gestures. Experimental results demonstrate that GesGPT effectively
generates contextually appropriate and expressive gestures, offering a new
perspective on semantic co-speech gesture generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13035">
<div class="article-summary-box-inner">
<span><p>Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Classification with Decoupled Representation. (arXiv:2303.13065v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13065">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have shown marvelous improvements across
various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence
of characters, and completely ignore word information. Although Whole Word
Masking can alleviate this, the semantics in words is still not well
represented. In this paper, we revisit the segmentation granularity of Chinese
PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both
characters and words. To achieve this, we design objective functions for
learning both character and word-level representations. We conduct extensive
experiments on various Chinese NLP tasks to evaluate existing PLMs as well as
the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA
performance on all these tasks. Further analysis demonstrates that words are
semantically richer than characters. More interestingly, we show that MigBERT
also works with Japanese. Our code has been released
here~\footnote{\url{https://github.com/xnliang98/MigBERT}} and you can download
our model here~\footnote{\url{https://huggingface.co/xnliang/MigBERT-large/}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit. (arXiv:2303.13072v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13072">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have recently made significant achievements in the
application of end-to-end (E2E) automatic speech recognition (ASR). It is
possible to deploy the E2E ASR system on smart devices with the help of
Transformer-based models. While these models still have the disadvantage of
requiring a large number of model parameters. To overcome the drawback of
universal Transformer models for the application of ASR on edge devices, we
propose a solution that can reuse the block in Transformer models for the
occasion of the small footprint ASR system, which meets the objective of
accommodating resource limitations without compromising recognition accuracy.
Specifically, we design a novel block-reusing strategy for speech Transformer
(BRST) to enhance the effectiveness of parameters and propose an adapter module
(ADM) that can produce a compact and adaptable model with only a few additional
trainable parameters accompanying each reusing block. We conducted an
experiment with the proposed method on the public AISHELL-1 corpus, and the
results show that the proposed approach achieves the character error rate (CER)
of 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,
respectively. In addition, we also make a deeper analysis to show the effect of
ADM in the general block-reusing method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13099">
<div class="article-summary-box-inner">
<span><p>In Task Oriented Dialogue (TOD) system, detecting and inducing new intents
are two main challenges to apply the system in the real world. In this paper,
we suggest the semantic multi-view model to resolve these two challenges: (1)
SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue
domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized
semantic. MDB feeds diverse dialogue datasets to the model at once to tackle
the multi-domain problem by learning the multiple domain knowledge. We
introduce a novel method PGT, which employs the Siamese network to fine-tune
the model with a clustering method directly.Our model can learn how to cluster
dialogue utterances by using PGT. Experimental results demonstrate that our
multi-view model with MDB and PGT significantly improves the Open Intent
Induction performance compared to baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13112">
<div class="article-summary-box-inner">
<span><p>Various recent experimental results show that large language models (LLM)
exhibit emergent abilities that are not present in small models. System
performance is greatly improved after passing a certain critical threshold of
scale. In this letter, we provide a simple explanation for such a phase
transition phenomenon. For this, we model an LLM as a sequence-to-sequence
random function. Instead of using instant generation at each step, we use a
list decoder that keeps a list of candidate sequences at each step and defers
the generation of the output sequence at the end. We show that there is a
critical threshold such that the expected number of erroneous candidate
sequences remains bounded when an LLM is below the threshold, and it grows
exponentially when an LLM is above the threshold. Such a threshold is related
to the basic reproduction number in a contagious disease.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13217">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated surprising ability to perform
in-context learning, i.e., these models can be directly applied to solve
numerous downstream tasks by conditioning on a prompt constructed by a few
input-output examples. However, prior research has shown that in-context
learning can suffer from high instability due to variations in training
examples, example order, and prompt formats. Therefore, the construction of an
appropriate prompt is essential for improving the performance of in-context
learning. In this paper, we revisit this problem from the view of predictive
bias. Specifically, we introduce a metric to evaluate the predictive bias of a
fixed prompt against labels or a given attributes. Then we empirically show
that prompts with higher bias always lead to unsatisfactory predictive quality.
Based on this observation, we propose a novel search strategy based on the
greedy search to identify the near-optimal prompt for improving the performance
of in-context learning. We perform comprehensive experiments with
state-of-the-art mainstream models such as GPT-3 on various downstream tasks.
Our results indicate that our method can enhance the model's in-context
learning performance in an effective and interpretable manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Sparse Retrievers and Rerankers using Adapters. (arXiv:2303.13220v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13220">
<div class="article-summary-box-inner">
<span><p>Parameter-Efficient transfer learning with Adapters have been studied in
Natural Language Processing (NLP) as an alternative to full fine-tuning.
Adapters are memory-efficient and scale well with downstream tasks by training
small bottle-neck layers added between transformer layers while keeping the
large pretrained language model (PLMs) frozen. In spite of showing promising
results in NLP, these methods are under-explored in Information Retrieval.
While previous studies have only experimented with dense retriever or in a
cross lingual retrieval scenario, in this paper we aim to complete the picture
on the use of adapters in IR. First, we study adapters for SPLADE, a sparse
retriever, for which adapters not only retain the efficiency and effectiveness
otherwise achieved by finetuning, but are memory-efficient and orders of
magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes
just 2\% of training parameters, but outperforms fully fine-tuned counterpart
and existing parameter-efficient dense IR models on IR benchmark datasets.
Secondly, we address domain adaptation of neural retrieval thanks to adapters
on cross-domain BEIR datasets and TripClick. Finally, we also consider
knowledge sharing between rerankers and first stage rankers. Overall, our study
complete the examination of adapters for neural IR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. (arXiv:2303.13283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13283">
<div class="article-summary-box-inner">
<span><p>Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13284">
<div class="article-summary-box-inner">
<span><p>In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwissBERT: The Multilingual Language Model for Switzerland. (arXiv:2303.13310v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13310">
<div class="article-summary-box-inner">
<span><p>We present SwissBERT, a masked language model created specifically for
processing Switzerland-related text. SwissBERT is a pre-trained model that we
adapted to news articles written in the national languages of Switzerland --
German, French, Italian, and Romansh. We evaluate SwissBERT on natural language
understanding tasks related to Switzerland and find that it tends to outperform
previous models on these tasks, especially when processing contemporary news
and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be
extended to Swiss German dialects in future work. The model and our open-source
code are publicly released at https://github.com/ZurichNLP/swissbert.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Foundation Models for Clinical Text Analysis. (arXiv:2303.13314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13314">
<div class="article-summary-box-inner">
<span><p>Infectious diseases are a significant public health concern globally, and
extracting relevant information from scientific literature can facilitate the
development of effective prevention and treatment strategies. However, the
large amount of clinical data available presents a challenge for information
extraction. To address this challenge, this study proposes a natural language
processing (NLP) framework that uses a pre-trained transformer model fine-tuned
on task-specific data to extract key information related to infectious diseases
from free-text clinical data. The proposed framework includes three components:
a data layer for preparing datasets from clinical texts, a foundation model
layer for entity extraction, and an assessment layer for performance analysis.
The results of the evaluation indicate that the proposed method outperforms
standard methods, and leveraging prior knowledge through the pre-trained
transformer model makes it useful for investigating other infectious diseases
in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13351">
<div class="article-summary-box-inner">
<span><p>In this work we create a question answering dataset over the DBLP scholarly
knowledge graph (KG). DBLP is an on-line reference for bibliographic
information on major computer science publications that indexes over 4.4
million publications published by more than 2.2 million authors. Our dataset
consists of 10,000 question answer pairs with the corresponding SPARQL queries
which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD
is the largest scholarly question answering dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension. (arXiv:2303.13355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13355">
<div class="article-summary-box-inner">
<span><p>Although the curse of multilinguality significantly restricts the language
abilities of multilingual models in monolingual settings, researchers now still
have to rely on multilingual models to develop state-of-the-art systems in
Vietnamese Machine Reading Comprehension. This difficulty in researching is
because of the limited number of high-quality works in developing Vietnamese
language models. In order to encourage more work in this research field, we
present a comprehensive analysis of language weaknesses and strengths of
current Vietnamese monolingual models using the downstream task of Machine
Reading Comprehension. From the analysis results, we suggest new directions for
developing Vietnamese language models. Besides this main contribution, we also
successfully reveal the existence of artifacts in Vietnamese Machine Reading
Comprehension benchmarks and suggest an urgent need for new high-quality
benchmarks to track the progress of Vietnamese Machine Reading Comprehension.
Moreover, we also introduced a minor but valuable modification to the process
of annotating unanswerable questions for Machine Reading Comprehension from
previous work. Our proposed modification helps improve the quality of
unanswerable questions to a higher level of difficulty for Machine Reading
Comprehension systems to solve.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Scalable Evaluation of Cooperativeness in Language Models. (arXiv:2303.13360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13360">
<div class="article-summary-box-inner">
<span><p>It is likely that AI systems driven by pre-trained language models (PLMs)
will increasingly be used to assist humans in high-stakes interactions with
other agents, such as negotiation or conflict resolution. Consistent with the
goals of Cooperative AI \citep{dafoe_open_2020}, we wish to understand and
shape the multi-agent behaviors of PLMs in a pro-social manner. An important
first step is the evaluation of model behaviour across diverse cooperation
problems. Since desired behaviour in an interaction depends upon precise
game-theoretic structure, we focus on generating scenarios with particular
structures with both crowdworkers and a language model. Our work proceeds as
follows. First, we discuss key methodological issues in the generation of
scenarios corresponding to particular game-theoretic structures. Second, we
employ both crowdworkers and a language model to generate such scenarios. We
find that the quality of generations tends to be mediocre in both cases. We
additionally get both crowdworkers and a language model to judge whether given
scenarios align with their intended game-theoretic structure, finding mixed
results depending on the game. Third, we provide a dataset of scenario based on
our data generated. We provide both quantitative and qualitative evaluations of
UnifiedQA and GPT-3 on this dataset. We find that instruct-tuned models tend to
act in a way that could be perceived as cooperative when scaled up, while other
models seemed to have flat scaling trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reevaluating Data Partitioning for Emotion Detection in EmoWOZ. (arXiv:2303.13364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13364">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that
provides emotion labels for the dialogues. MultiWOZ was partitioned initially
for another purpose, resulting in a distributional shift when considering the
new purpose of emotion recognition. The emotion tags in EmoWoz are highly
imbalanced and unevenly distributed across the partitions, which causes
sub-optimal performance and poor comparison of models. We propose a stratified
sampling scheme based on emotion tags to address this issue, improve the
dataset's distribution, and reduce dataset shift. We also introduce a special
technique to handle conversation (sequential) data with many emotional tags.
Using our proposed sampling method, models built upon EmoWoz can perform
better, making it a more reliable resource for training conversational agents
with emotional intelligence. We recommend that future researchers use this new
partitioning to ensure consistent and accurate performance evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13365">
<div class="article-summary-box-inner">
<span><p>Improvement of software development methodologies attracts developers to
automatic Requirement Formalisation (RF) in the Requirement Engineering (RE)
field. The potential advantages by applying Natural Language Processing (NLP)
and Machine Learning (ML) in reducing the ambiguity and incompleteness of
requirement written in natural languages is reported in different studies. The
goal of this paper is to survey and classify existing work on NLP and ML for
RF, identifying challenges in this domain and providing promising future
research directions. To achieve this, we conducted a systematic literature
review to outline the current state-of-the-art of NLP and ML techniques in RF
by selecting 257 papers from common used libraries. The search result is
filtered by defining inclusion and exclusion criteria and 47 relevant studies
between 2012 and 2022 are selected. We found that heuristic NLP approaches are
the most common NLP techniques used for automatic RF, primary operating on
structured and semi-structured data. This study also revealed that Deep
Learning (DL) technique are not widely used, instead classical ML techniques
are predominant in the surveyed studies. More importantly, we identified the
difficulty of comparing the performance of different approaches due to the lack
of standard benchmark cases for RF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT and a New Academic Reality: AI-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing. (arXiv:2303.13367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13367">
<div class="article-summary-box-inner">
<span><p>This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks. (arXiv:2303.13373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13373">
<div class="article-summary-box-inner">
<span><p>In recent years there has been a growing demand from financial agents,
especially from particular and institutional investors, for companies to report
on climate-related financial risks. A vast amount of information, in text
format, can be expected to be disclosed in the short term by firms in order to
identify these types of risks in their financial and non financial reports,
particularly in response to the growing regulation that is being passed on the
matter. To this end, this paper applies state-of-the-art NLP techniques to
achieve the detection of climate change in text corpora. We use transfer
learning to fine-tune two transformer models, BERT and ClimateBert -a recently
published DistillRoBERTa-based model that has been specifically tailored for
climate text classification-. These two algorithms are based on the transformer
architecture which enables learning the contextual relationships between words
in a text. We carry out the fine-tuning process of both models on the novel
Clima-Text database, consisting of data collected from Wikipedia, 10K Files
Reports and web-based claims. Our text classification model obtained from the
ClimateBert fine-tuning process on ClimaText, outperforms the models created
with BERT and the current state-of-the-art transformer in this particular
problem. Our study is the first one to implement on the ClimaText database the
recently published ClimateBert algorithm. Based on our results, it can be said
that ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP
pre-trained transformer models that may and should be used by investors,
institutional agents and companies themselves to monitor the disclosure of
climate risk in financial reports. In addition, our transfer learning
methodology is cheap in computational terms, thus allowing any organization to
perform it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13375">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13379">
<div class="article-summary-box-inner">
<span><p>Educational technology innovations that have been developed based on large
language models (LLMs) have shown the potential to automate the laborious
process of generating and analysing textual content. While various innovations
have been developed to automate a range of educational tasks (e.g., question
generation, feedback provision, and essay grading), there are concerns
regarding the practicality and ethicality of these innovations. Such concerns
may hinder future research and the adoption of LLMs-based innovations in
authentic educational contexts. To address this, we conducted a systematic
literature review of 118 peer-reviewed papers published since 2017 to pinpoint
the current state of research on using LLMs to automate and support educational
tasks. The practical and ethical challenges of LLMs-based innovations were also
identified by assessing their technological readiness, model performance,
replicability, system transparency, privacy, equality, and beneficence. The
findings were summarised into three recommendations for future studies,
including updating existing innovations with state-of-the-art models (e.g.,
GPT-3), embracing the initiative of open-sourcing models/systems, and adopting
a human-centred approach throughout the developmental process. These
recommendations could support future research to develop practical and ethical
innovations for supporting diverse educational tasks and benefiting students,
teachers, and institutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13386">
<div class="article-summary-box-inner">
<span><p>Label scarcity is a bottleneck for improving task performance in specialised
domains. We propose a novel compositional transfer learning framework (DoT5 -
domain compositional zero-shot T5) for zero-shot domain transfer. Without
access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of
unlabelled in-domain free text) and task knowledge (from task training on more
readily available general-domain data) in a multi-task manner. To improve the
transferability of task training, we design a strategy named NLGU: we
simultaneously train NLG for in-domain label-to-data generation which enables
data augmentation for self-finetuning and NLU for label prediction. We evaluate
DoT5 on the biomedical domain and the resource-lean subdomain of radiology,
focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates
the effectiveness of compositional transfer learning through multi-task
learning. In particular, DoT5 outperforms the current SOTA in zero-shot
transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with
ablations and a case study demonstrating its ability to solve challenging NLI
examples requiring in-domain expertise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13408">
<div class="article-summary-box-inner">
<span><p>To detect the deployment of large language models for malicious use cases
(e.g., fake content creation or academic plagiarism), several approaches have
recently been proposed for identifying AI-generated text via watermarks or
statistical irregularities. How robust are these detection algorithms to
paraphrases of AI-generated text? To stress test these detectors, we first
train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase
paragraphs, optionally leveraging surrounding text (e.g., user-written prompts)
as context. DIPPER also uses scalar knobs to control the amount of lexical
diversity and reordering in the paraphrases. Paraphrasing text generated by
three large language models (including GPT3.5-davinci-003) with DIPPER
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the
detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false
positive rate of 1%), without appreciably modifying the input semantics. To
increase the robustness of AI-generated text detection to paraphrase attacks,
we introduce a simple defense that relies on retrieving semantically-similar
generations and must be maintained by a language model API provider. Given a
candidate text, our algorithm searches a database of sequences previously
generated by the API, looking for sequences that match the candidate text
within a certain threshold. We empirically verify our defense using a database
of 15M generations from a fine-tuned T5-XXL model and find that it can detect
80% to 97% of paraphrased generations across different settings, while only
classifying 1% of human-written sequences as AI-generated. We will open source
our code, model and data for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development and validation of a natural language processing algorithm to pseudonymize documents in the context of a clinical data warehouse. (arXiv:2303.13451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13451">
<div class="article-summary-box-inner">
<span><p>The objective of this study is to address the critical issue of
de-identification of clinical reports in order to allow access to data for
research purposes, while ensuring patient privacy. The study highlights the
difficulties faced in sharing tools and resources in this domain and presents
the experience of the Greater Paris University Hospitals (AP-HP) in
implementing a systematic pseudonymization of text documents from its Clinical
Data Warehouse. We annotated a corpus of clinical documents according to 12
types of identifying entities, and built a hybrid system, merging the results
of a deep learning model as well as manual rules. Our results show an overall
performance of 0.99 of F1-score. We discuss implementation choices and present
experiments to better understand the effort involved in such a task, including
dataset size, document types, language models, or rule addition. We share
guidelines and code under a 3-Clause BSD license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoBIT: A Contrastive Bi-directional Image-Text Generation Model. (arXiv:2303.13455v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13455">
<div class="article-summary-box-inner">
<span><p>The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">W2KPE: Keyphrase Extraction with Word-Word Relation. (arXiv:2303.13463v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13463">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to ICASSP 2023 MUG Challenge Track 4,
Keyphrase Extraction, which aims to extract keyphrases most relevant to the
conference theme from conference materials. We model the challenge as a
single-class Named Entity Recognition task and developed techniques for better
performance on the challenge: For the data preprocessing, we encode the split
keyphrases after word segmentation. In addition, we increase the amount of
input information that the model can accept at one time by fusing multiple
preprocessed sentences into one segment. We replace the loss function with the
multi-class focal loss to address the sparseness of keyphrases. Besides, we
score each appearance of keyphrases and add an extra output layer to fit the
score to rank keyphrases. Exhaustive evaluations are performed to find the best
combination of the word segmentation tool, the pre-trained embedding model, and
the corresponding hyperparameters. With these proposals, we scored 45.04 on the
final test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13465">
<div class="article-summary-box-inner">
<span><p>Conventionally, since the natural language action space is astronomical,
approximate dynamic programming applied to dialogue generation involves policy
improvement with action sampling. However, such a practice is inefficient for
reinforcement learning (RL) because the eligible (high action value) responses
are very sparse, and the greedy policy sustained by the random sampling is
flabby. This paper shows that the performance of dialogue policy positively
correlated with sampling size by theoretical and experimental. We introduce a
novel dual-granularity Q-function to alleviate this limitation by exploring the
most promising response category to intervene in the sampling. It extracts the
actions following the grained hierarchy, which can achieve the optimum with
fewer policy iterations. Our approach learns in the way of offline RL from
multiple reward functions designed to recognize human emotional details.
Empirical studies demonstrate that our algorithm outperforms the baseline
methods. Further verification presents that ours can generate responses with
higher expected rewards and controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13466">
<div class="article-summary-box-inner">
<span><p>Physical rehabilitation plays a crucial role in the recovery process of
post-stroke patients. By personalizing therapies for patients leveraging
predictive modeling and electronic health records (EHRs), healthcare providers
can make the rehabilitation process more efficient. Before predictive modeling
can provide decision support for the assignment of treatment plans, automated
methods are necessary to extract physical rehabilitation exercise information
from unstructured EHRs. We introduce a rule-based natural language processing
algorithm to annotate therapeutic procedures for stroke patients and compare it
to several small machine learning models. We find that our algorithm
outperforms these models in extracting half of the concepts where sufficient
data is available, and individual exercise descriptions can be assigned binary
labels with an f-score of no less than 0.75 per concept. More research needs to
be done before these algorithms can be deployed on unlabeled documents, but
current progress gives promise to the potential of precision rehabilitation
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Semantic Text Similarity to rank Hypernyms of Financial Terms. (arXiv:2303.13475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13475">
<div class="article-summary-box-inner">
<span><p>Over the years, there has been a paradigm shift in how users access financial
services. With the advancement of digitalization more users have been
preferring the online mode of performing financial activities. This has led to
the generation of a huge volume of financial content. Most investors prefer to
go through these contents before making decisions. Every industry has terms
that are specific to the domain it operates in. Banking and Financial Services
are not an exception to this. In order to fully comprehend these contents, one
needs to have a thorough understanding of the financial terms. Getting a basic
idea about a term becomes easy when it is explained with the help of the broad
category to which it belongs. This broad category is referred to as hypernym.
For example, "bond" is a hypernym of the financial term "alternative
debenture". In this paper, we propose a system capable of extracting and
ranking hypernyms for a given financial term. The system has been trained with
financial text corpora obtained from various sources like DBpedia [4],
Investopedia, Financial Industry Business Ontology (FIBO), prospectus and so
on. Embeddings of these terms have been extracted using FinBERT [3], FinISH [1]
and fine-tuned using SentenceBERT [54]. A novel approach has been used to
augment the training set with negative samples. It uses the hierarchy present
in FIBO. Finally, we benchmark the system performance with that of the existing
ones. We establish that it performs better than the existing ones and is also
scalable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13519">
<div class="article-summary-box-inner">
<span><p>Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offensive Language and Hate Speech Detection for Danish. (arXiv:1908.04531v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.04531">
<div class="article-summary-box-inner">
<span><p>The presence of offensive language on social media platforms and the
implications this poses is becoming a major concern in modern society. Given
the enormous amount of content created every day, automatic methods are
required to detect and deal with this type of content. Until now, most of the
research has focused on solving the problem for the English language, while the
problem is multilingual.
</p>
<p>We construct a Danish dataset containing user-generated comments from
\textit{Reddit} and \textit{Facebook}. It contains user generated comments from
various social media platforms, and to our knowledge, it is the first of its
kind. Our dataset is annotated to capture various types and target of offensive
language. We develop four automatic classification systems, each designed to
work for both the English and the Danish language. In the detection of
offensive language in English, the best performing system achieves a macro
averaged F1-score of $0.74$, and the best performing system for Danish achieves
a macro averaged F1-score of $0.70$. In the detection of whether or not an
offensive post is targeted, the best performing system for English achieves a
macro averaged F1-score of $0.62$, while the best performing system for Danish
achieves a macro averaged F1-score of $0.73$. Finally, in the detection of the
target type in a targeted offensive post, the best performing system for
English achieves a macro averaged F1-score of $0.56$, and the best performing
system for Danish achieves a macro averaged F1-score of $0.63$.
</p>
<p>Our work for both the English and the Danish language captures the type and
targets of offensive language, and present automatic methods for detecting
different kinds of offensive language such as hate speech and cyberbullying.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminating Between Similar Nordic Languages. (arXiv:2012.06431v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06431">
<div class="article-summary-box-inner">
<span><p>Automatic language identification is a challenging problem. Discriminating
between closely related languages is especially difficult. This paper presents
a machine learning approach for automatic language identification for the
Nordic languages, which often suffer miscategorisation by existing
state-of-the-art tools. Concretely we will focus on discrimination between six
Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\aa}l),
Faroese and Icelandic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03837">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) aims to identify mentions of named entities in
an unstructured text and classify them into predefined named entity classes.
While deep learning-based pre-trained language models help to achieve good
predictive performances in NER, many domain-specific NER applications still
call for a substantial amount of labeled data. Active learning (AL), a general
framework for the label acquisition problem, has been used for NER tasks to
minimize the annotation cost without sacrificing model performance. However,
the heavily imbalanced class distribution of tokens introduces challenges in
designing effective AL querying methods for NER. We propose several AL sentence
query evaluation functions that pay more attention to potential positive
tokens, and evaluate these proposed functions with both sentence-based and
token-based cost evaluation strategies. We also propose a better data-driven
normalization approach to penalize sentences that are too long or too short.
Our experiments on three datasets from different domains reveal that the
proposed approach reduces the number of annotated tokens while achieving better
or comparable prediction performance with conventional methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot Learners. (arXiv:2205.09229v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09229">
<div class="article-summary-box-inner">
<span><p>Recent advances in large pre-trained language models (PLMs) lead to
impressive gains in natural language understanding (NLU) tasks with
task-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on
sufficient labeled training instances, which are usually hard to obtain.
Prompt-based tuning on PLMs has shown to be powerful for various downstream
few-shot tasks. Existing works studying prompt-based tuning for few-shot NLU
tasks mainly focus on deriving proper label words with a verbalizer or
generating prompt templates to elicit semantics from PLMs. In addition,
conventional data augmentation strategies such as synonym substitution, though
widely adopted in low-resource scenarios, only bring marginal improvements for
prompt-based few-shot learning. Thus, an important research question arises:
how to design effective data augmentation methods for prompt-based few-shot
tuning? To this end, considering the label semantics are essential in
prompt-based tuning, we propose a novel label-guided data augmentation
framework PromptDA, which exploits the enriched label semantic information for
data augmentation. Extensive experiment results on few-shot text classification
tasks demonstrate the superior performance of the proposed framework by
effectively leveraging label semantics and data augmentation for natural
language understanding. Our code is available at
https://github.com/canyuchen/PromptDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.14868">
<div class="article-summary-box-inner">
<span><p>We present new benchmarks on evaluation code generation models: MBXP and
Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming
languages and are generated using a scalable conversion framework that
transpiles prompts and test cases from the original Python datasets into the
corresponding data in the target language. Using these benchmarks, we are able
to assess the performance of code generation models in a multi-lingual fashion,
and discovered generalization ability of language models on out-of-domain
languages, advantages of multi-lingual models over mono-lingual, the ability of
few-shot prompting to teach the model new languages, and zero-shot translation
abilities even on mono-lingual settings. Furthermore, we use our code
generation model to perform large-scale bootstrapping to obtain synthetic
canonical solutions in several languages, which can be used for other
code-related evaluations such as code insertion, robustness, or summarization
tasks. Overall, our benchmarks represents a significant step towards a deeper
understanding of language models' code generation abilities. We publicly
release our code and datasets at https://github.com/amazon-research/mxeval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15387">
<div class="article-summary-box-inner">
<span><p>Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16848">
<div class="article-summary-box-inner">
<span><p>Although contextualized embeddings generated from large-scale pre-trained
models perform well in many tasks, traditional static embeddings (e.g.,
Skip-gram, Word2Vec) still play an important role in low-resource and
lightweight settings due to their low computational cost, ease of deployment,
and stability. In this paper, we aim to improve word embeddings by 1)
incorporating more contextual information from existing pre-trained models into
the Skip-gram framework, which we call Context-to-Vec; 2) proposing a
post-processing retrofitting method for static embeddings independent of
training by employing priori synonym knowledge and weighted vector
distribution. Through extrinsic and intrinsic tasks, our methods are well
proven to outperform the baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction. (arXiv:2301.09209v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09209">
<div class="article-summary-box-inner">
<span><p>We study object interaction anticipation in egocentric videos. This task
requires an understanding of the spatiotemporal context formed by past actions
on objects, coined action context. We propose TransFusion, a multimodal
transformer-based architecture. It exploits the representational power of
language by summarising the action context. TransFusion leverages pre-trained
image captioning and vision-language models to extract the action context from
past video frames. This action context together with the next video frame is
processed by the multimodal fusion module to forecast the next object
interaction. Our model enables more efficient end-to-end learning. The large
pre-trained language models add common sense and a generalisation capability.
Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our
multimodal fusion model. They also highlight the benefits of using
language-based context summaries in a task where vision seems to suffice. Our
method outperforms state-of-the-art approaches by 40.4% in relative terms in
overall mAP on the Ego4D test set. We validate the effectiveness of TransFusion
via experiments on EPIC-KITCHENS-100. Video and code are available at:
https://eth-ait.github.io/transfusion-proj/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes. (arXiv:2303.09892v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09892">
<div class="article-summary-box-inner">
<span><p>Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12024">
<div class="article-summary-box-inner">
<span><p>An open challenge in multimodal conversational AI requires augmenting large
language models with information from textual and non-textual sources for
multi-turn dialogue. To address this problem, this paper introduces
Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve
tabular information and generate dialogue responses grounded on the retrieved
information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval
and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over
sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs
tabular knowledge retrieval using both encoder and decoder models, resulting in
up to 46% relative improvement in ROUGE scores and better human evaluation for
response generation on HyrbiDialogue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04587">
<div class="article-summary-box-inner">
<span><p>The dominant paradigm for semantic parsing in recent years is to formulate
parsing as a sequence-to-sequence task, generating predictions with
auto-regressive sequence decoders. In this work, we explore an alternative
paradigm. We formulate semantic parsing as a dependency parsing task, applying
graph-based decoding techniques developed for syntactic parsing. We compare
various decoding techniques given the same pre-trained Transformer encoder on
the TOP dataset, including settings where training data is limited or contains
only partially-annotated examples. We find that our graph-based approach is
competitive with sequence decoders on the standard setting, and offers
significant improvements in data efficiency and settings where
partially-annotated data is available.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-24 23:11:40.260085012 UTC">2023-03-24 23:11:40 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>