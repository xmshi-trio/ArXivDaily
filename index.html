<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-14T01:30:00Z">09-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06453">
<div class="article-summary-box-inner">
<span><p>Sentence Representation Learning (SRL) is a fundamental task in Natural
Language Processing (NLP), with Contrastive learning of Sentence Embeddings
(CSE) as the mainstream technique due to its superior performance. An
intriguing phenomenon in CSE is the significant performance gap between
supervised and unsupervised methods, even when their sentence encoder and loss
function are the same. Previous works attribute this performance gap to
differences in two representation properties (alignment and uniformity).
However, alignment and uniformity only measure the results, which means they
cannot answer "What happens during the training process that leads to the
performance gap?" and "How can the performance gap be narrowed?". In this
paper, we conduct empirical experiments to answer these "What" and "How"
questions. We first answer the "What" question by thoroughly comparing the
behavior of supervised and unsupervised CSE during their respective training
processes. From the comparison, We observe a significant difference in fitting
difficulty. Thus, we introduce a metric, called Fitting Difficulty Increment
(FDI), to measure the fitting difficulty gap between the evaluation dataset and
the held-out training dataset, and use the metric to answer the "What"
question. Then, based on the insights gained from the "What" question, we
tackle the "How" question by increasing the fitting difficulty of the training
dataset. We achieve this by leveraging the In-Context Learning (ICL) capability
of the Large Language Model (LLM) to generate data that simulates complex
patterns. By utilizing the hierarchical patterns in the LLM-generated data, we
effectively narrow the gap between supervised and unsupervised CSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Widely Interpretable Semantic Representation: Frameless Meaning Representation for Broader Applicability. (arXiv:2309.06460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06460">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel semantic representation, WISeR, that overcomes
challenges for Abstract Meaning Representation (AMR). Despite its strengths,
AMR is not easily applied to languages or domains without predefined semantic
frames, and its use of numbered arguments results in semantic role labels,
which are not directly interpretable and are semantically overloaded for
parsers. We examine the numbered arguments of predicates in AMR and convert
them to thematic roles that do not require reference to semantic frames. We
create a new corpus of 1K English dialogue sentences annotated in both WISeR
and AMR. WISeR shows stronger inter-annotator agreement for beginner and
experienced annotators, with beginners becoming proficient in WISeR annotation
more quickly. Finally, we train a state-of-the-art parser on the AMR 3.0 corpus
and a WISeR corpus converted from AMR 3.0. The parser is evaluated on these
corpora and our dialogue corpus. The WISeR model exhibits higher accuracy than
its AMR counterpart across the board, demonstrating that WISeR is easier for
parsers to learn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models for Automated Dialogue Analysis. (arXiv:2309.06490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06490">
<div class="article-summary-box-inner">
<span><p>Developing high-performing dialogue systems benefits from the automatic
identification of undesirable behaviors in system responses. However, detecting
such behaviors remains challenging, as it draws on a breadth of general
knowledge and understanding of conversational practices. Although recent
research has focused on building specialized classifiers for detecting specific
dialogue behaviors, the behavior coverage is still incomplete and there is a
lack of testing on real-world human-bot interactions. This paper investigates
the ability of a state-of-the-art large language model (LLM), ChatGPT-3.5, to
perform dialogue behavior detection for nine categories in real human-bot
dialogues. We aim to assess whether ChatGPT can match specialized models and
approximate human performance, thereby reducing the cost of behavior detection
tasks. Our findings reveal that neither specialized models nor ChatGPT have yet
achieved satisfactory results for this task, falling short of human
performance. Nevertheless, ChatGPT shows promising potential and often
outperforms specialized detection models. We conclude with an in-depth
examination of the prevalent shortcomings of ChatGPT, offering guidance for
future research to enhance LLM capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models. (arXiv:2309.06495v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06495">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) like ChatGPT have revealed amazing intelligence.
How to evaluate the question-solving abilities of LLMs and their degrees of
intelligence is a hot-spot but challenging issue. First, the question-solving
abilities are interlaced with different ability branches like understanding and
massive knowledge categories like mathematics. Second, the inputs of questions
are multimodal that may involve text and images. Third, the response format of
LLMs is diverse and thus poses great challenges for result extraction and
evaluation. In this paper, we propose AGIBench -- a multi-granularity,
multimodal, human-referenced, and auto-scoring benchmarking methodology for
LLMs. Instead of a collection of blended questions, AGIBench focuses on three
typical ability branches and adopts a four-tuple &lt;ability branch, knowledge,
difficulty, modal&gt; to label the attributes of each question. First, it supports
multi-granularity benchmarking, e.g., per-question, per-ability branch,
per-knowledge, per-modal, per-dataset, and per-difficulty level granularities.
Second, it contains multimodal input, including text and images. Third, it
classifies all the questions into five degrees of difficulty according to the
average accuracy rate of abundant educated humans (human-referenced). Fourth,
it adopts zero-shot learning to avoid introducing additional unpredictability
and provides an auto-scoring method to extract and judge the result. Finally,
it defines multi-dimensional metrics, including accuracy under the average,
worst, best, and majority voting cases, and repeatability. AGIBench is
publically available from \url{https://www.benchcouncil.org/agibench}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets. (arXiv:2309.06503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06503">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has presented significant challenges to the healthcare
industry and society as a whole. With the rapid development of COVID-19
vaccines, social media platforms have become a popular medium for discussions
on vaccine-related topics. Identifying vaccine-related tweets and analyzing
them can provide valuable insights for public health research-ers and
policymakers. However, manual annotation of a large number of tweets is
time-consuming and expensive. In this study, we evaluate the usage of Large
Language Models, in this case GPT-4 (March 23 version), and weak supervision,
to identify COVID-19 vaccine-related tweets, with the purpose of comparing
performance against human annotators. We leveraged a manu-ally curated
gold-standard dataset and used GPT-4 to provide labels without any additional
fine-tuning or instructing, in a single-shot mode (no additional prompting).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish Memes. (arXiv:2309.06517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06517">
<div class="article-summary-box-inner">
<span><p>Analyzing memes on the internet has emerged as a crucial endeavor due to the
impact this multi-modal form of content wields in shaping online discourse.
Memes have become a powerful tool for expressing emotions and sentiments,
possibly even spreading hate and misinformation, through humor and sarcasm. In
this paper, we present the overview of the Memotion 3 shared task, as part of
the DeFactify 2 workshop at AAAI-23. The task released an annotated dataset of
Hindi-English code-mixed memes based on their Sentiment (Task A), Emotion (Task
B), and Emotion intensity (Task C). Each of these is defined as an individual
task and the participants are ranked separately for each task. Over 50 teams
registered for the shared task and 5 made final submissions to the test set of
the Memotion 3 dataset. CLIP, BERT modifications, ViT etc. were the most
popular models among the participants along with approaches such as
Student-Teacher model, Fusion, and Ensembling. The best final F1 score for Task
A is 34.41, Task B is 79.77 and Task C is 59.82.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems. (arXiv:2309.06520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06520">
<div class="article-summary-box-inner">
<span><p>For sequence-to-sequence tasks it is challenging to combine individual system
outputs. Further, there is also often a mismatch between the decoding criterion
and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used
to combine system outputs in a manner that encourages better alignment with the
final assessment criterion. This paper examines MBR decoding for Grammatical
Error Correction (GEC) systems, where performance is usually evaluated in terms
of edits and an associated F-score. Hence, we propose a novel MBR loss function
directly linked to this form of criterion. Furthermore, an approach to expand
the possible set of candidate sentences is described. This builds on a current
max-voting combination scheme, as well as individual edit-level selection.
Experiments on three popular GEC datasets and with state-of-the-art GEC systems
demonstrate the efficacy of the proposed MBR approach. Additionally, the paper
highlights how varying reward metrics within the MBR decoding framework can
provide control over precision, recall, and the F-score in combined GEC
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation Models Stand Strong in the Face of Adversarial Attacks. (arXiv:2309.06527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06527">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks expose vulnerabilities of deep learning models by
introducing minor perturbations to the input, which lead to substantial
alterations in the output. Our research focuses on the impact of such
adversarial attacks on sequence-to-sequence (seq2seq) models, specifically
machine translation models. We introduce algorithms that incorporate basic text
perturbation heuristics and more advanced strategies, such as the
gradient-based attack, which utilizes a differentiable approximation of the
inherently non-differentiable translation metric. Through our investigation, we
provide evidence that machine translation models display robustness displayed
robustness against best performed known adversarial attacks, as the degree of
perturbation in the output is directly proportional to the perturbation in the
input. However, among underdogs, our attacks outperform alternatives, providing
the best relative performance. Another strong candidate is an attack based on
mixing of individual characters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity. (arXiv:2309.06541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06541">
<div class="article-summary-box-inner">
<span><p>Amidst the sharp rise in the evaluation of large language models (LLMs) on
various tasks, we find that semantic textual similarity (STS) has been
under-explored. In this study, we show that STS can be cast as a text
generation problem while maintaining strong performance on multiple STS
benchmarks. Additionally, we show generative LLMs significantly outperform
existing encoder-based STS models when characterizing the semantic similarity
between two texts with complex semantic relationships dependent on world
knowledge. We validate this claim by evaluating both generative LLMs and
existing encoder-based STS models on three newly collected STS challenge sets
which require world knowledge in the domains of Health, Politics, and Sports.
All newly collected data is sourced from social media content posted after May
2023 to ensure the performance of closed-source models like ChatGPT cannot be
credited to memorization. Our results show that, on average, generative LLMs
outperform the best encoder-only baselines by an average of 22.3% on STS tasks
requiring world knowledge. Our results suggest generative language models with
STS-specific prompting strategies achieve state-of-the-art performance in
complex, domain-specific STS tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Text Generation using Hypergraph Representations. (arXiv:2309.06550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06550">
<div class="article-summary-box-inner">
<span><p>Generating synthetic variants of a document is often posed as text-to-text
transformation. We propose an alternate LLM based method that first decomposes
a document into semantic frames and then generates text using this interim
sparse format. The frames are modeled using a hypergraph, which allows
perturbing the frame contents in a principled manner. Specifically, new
hyperedges are mined through topological analysis and complex polyadic
relationships including hierarchy and temporal dynamics are accommodated. We
show that our solution generates documents that are diverse, coherent and vary
in style, sentiment, format, composition and facts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06553">
<div class="article-summary-box-inner">
<span><p>The recent advances in the development of Large Language Models (LLMs) like
ChatGPT have achieved remarkable performance by leveraging human expertise.
Yet, fully eliciting LLMs' potential for complex tasks requires navigating the
vast search space of natural language prompts. While prompt engineering has
shown promise, the requisite human-crafted prompts in trial-and-error attempts
and the associated costs pose significant challenges. Crucially, the efficiency
of prompt optimization hinges on the costly procedure of prompt evaluation.
This work introduces Prompt-OIRL, an approach rooted in offline inverse
reinforcement learning that seeks to bridge the gap between effective prompt
evaluation and affordability. Our method draws on offline datasets from expert
evaluations, employing Inverse-RL to derive a reward model for offline,
query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold:
it predicts prompt performance, is cost-efficient, produces human-readable
results, and efficiently navigates the prompt space. We validate our method
across four LLMs and three arithmetic datasets, highlighting its potential as a
robust and effective tool for offline prompt evaluation and optimization. Our
code as well as the offline datasets are released, and we highlight the
Prompt-OIRL can be reproduced within a few hours using a single laptop using
CPU
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Bias Detection in College Student Newspapers. (arXiv:2309.06557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06557">
<div class="article-summary-box-inner">
<span><p>This paper presents a pipeline with minimal human influence for scraping and
detecting bias on college newspaper archives. This paper introduces a framework
for scraping complex archive sites that automated tools fail to grab data from,
and subsequently generates a dataset of 14 student papers with 23,154 entries.
This data can also then be queried by keyword to calculate bias by comparing
the sentiment of a large language model summary to the original article. The
advantages of this approach are that it is less comparative than reconstruction
bias and requires less labelled data than generating keyword sentiment. Results
are calculated on politically charged words as well as control words to show
how conclusions can be drawn. The complete method facilitates the extraction of
nuanced insights with minimal assumptions and categorizations, paving the way
for a more objective understanding of bias within student newspaper sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Addressing the Blind Spots in Spoken Language Processing. (arXiv:2309.06572v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06572">
<div class="article-summary-box-inner">
<span><p>This paper explores the critical but often overlooked role of non-verbal
cues, including co-speech gestures and facial expressions, in human
communication and their implications for Natural Language Processing (NLP). We
argue that understanding human communication requires a more holistic approach
that goes beyond textual or spoken words to include non-verbal elements.
Borrowing from advances in sign language processing, we propose the development
of universal automatic gesture segmentation and transcription models to
transcribe these non-verbal cues into textual form. Such a methodology aims to
bridge the blind spots in spoken language understanding, enhancing the scope
and applicability of NLP models. Through motivating examples, we demonstrate
the limitations of relying solely on text-based models. We propose a
computationally efficient and flexible approach for incorporating non-verbal
cues, which can seamlessly integrate with existing NLP pipelines. We conclude
by calling upon the research community to contribute to the development of
universal transcription methods and to validate their effectiveness in
capturing the complexities of real-world, multi-modal interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06578">
<div class="article-summary-box-inner">
<span><p>Hypothesis formulation and testing are central to empirical research. A
strong hypothesis is a best guess based on existing evidence and informed by a
comprehensive view of relevant literature. However, with exponential increase
in the number of scientific articles published annually, manual aggregation and
synthesis of evidence related to a given hypothesis is a challenge. Our work
explores the ability of current large language models (LLMs) to discern
evidence in support or refute of specific hypotheses based on the text of
scientific abstracts. We share a novel dataset for the task of scientific
hypothesis evidencing using community-driven annotations of studies in the
social sciences. We compare the performance of LLMs to several state-of-the-art
benchmarks and highlight opportunities for future research in this area. The
dataset is available at
https://github.com/Sai90000/ScientificHypothesisEvidencing.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can humans help BERT gain "confidence"?. (arXiv:2309.06580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06580">
<div class="article-summary-box-inner">
<span><p>The advancements in artificial intelligence over the last decade have opened
a multitude of avenues for interdisciplinary research. Since the idea of
artificial intelligence was inspired by the working of neurons in the brain, it
seems pretty practical to combine the two fields and take the help of cognitive
data to train AI models. Not only it will help to get a deeper understanding of
the technology, but of the brain as well. In this thesis, I conduct novel
experiments to integrate cognitive features from the Zurich Cognitive Corpus
(ZuCo) (Hollenstein et al., 2018) with a transformer-based encoder model called
BERT. I show how EEG and eye-tracking features from ZuCo can help to increase
the performance of the NLP model. I confirm the performance increase with the
help of a robustness-checking pipeline and derive a word-EEG lexicon to use in
benchmarking on an external dataset that does not have any cognitive features
associated with it. Further, I analyze the internal working mechanism of BERT
and explore a potential method for model explainability by correlating it with
a popular model-agnostic explainability framework called LIME (Ribeiro et al.,
2016). Finally, I discuss the possible directions to take this research
forward.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Generative Large Language Models need billions of parameters?. (arXiv:2309.06589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06589">
<div class="article-summary-box-inner">
<span><p>This paper presents novel systems and methodologies for the development of
efficient large language models (LLMs). It explores the trade-offs between
model size, performance, and computational resources, with the aim of
maximizing the efficiency of these AI systems. The research explores novel
methods that allow different parts of the model to share parameters, reducing
the total number of unique parameters required. This approach ensures that the
model remains compact without sacrificing its ability to learn and represent
complex language structures. This study provides valuable insights and tools
for creating more efficient and effective LLMs, contributing to a more
sustainable and accessible future for AI language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Narrative as a Dynamical System. (arXiv:2309.06600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06600">
<div class="article-summary-box-inner">
<span><p>There is increasing evidence that human activity in general, and narrative in
particular, can be treated as a dynamical system in the physics sense; a system
whose evolution is described by an action integral, such that the average of
all possible paths from point A to point B is given by the extremum of the
action. We create by construction three such paths by averaging about 500
different narratives, and we show that the average path is consistent with an
action principle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models. (arXiv:2309.06619v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06619">
<div class="article-summary-box-inner">
<span><p>Recent advancements in language models (LMs) have gained substantial
attentions on their capability to generate human-like responses. Though
exhibiting a promising future for various applications such as conversation AI,
these LMs face deployment challenges on various devices due to their extreme
computational cost and unpredictable inference latency. Such varied inference
latency, identified as a consequence of uncertainty intrinsic to the nature of
language, can lead to computational inefficiency and degrade the overall
performance of LMs, especially under high-traffic workloads. Unfortunately, the
bandwidth of these uncertainty sources is extensive, complicating the
prediction of latency and the effects emanating from such uncertainties. To
understand and mitigate the impact of uncertainty on real-time
response-demanding systems, we take the first step to comprehend, quantify and
optimize these uncertainty-induced latency performance variations in LMs.
Specifically, we present RT-LM, an uncertainty-aware resource management
ecosystem for real-time inference of LMs. RT-LM innovatively quantifies how
specific input uncertainties, adversely affect latency, often leading to an
increased output length. Exploiting these insights, we devise a lightweight yet
effective method to dynamically correlate input text uncertainties with output
length at runtime. Utilizing this quantification as a latency heuristic, we
integrate the uncertainty information into a system-level scheduler which
explores several uncertainty-induced optimization opportunities, including
uncertainty-aware prioritization, dynamic consolidation, and strategic CPU
offloading. Quantitative experiments across five state-of-the-art LMs on two
hardware platforms demonstrates that RT-LM can significantly reduce the average
response time and improve throughput while incurring a rather small runtime
overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Rejection Sampling Improves Preference Optimization. (arXiv:2309.06657v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06657">
<div class="article-summary-box-inner">
<span><p>Improving the alignment of language models with human preferences remains an
active research challenge. Previous approaches have primarily utilized
Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as
Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence
Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have
emerged as attractive alternatives, offering improvements in stability and
scalability while maintaining competitive performance. SLiC refines its loss
function using sequence pairs sampled from a supervised fine-tuned (SFT)
policy, while DPO directly optimizes language models based on preference data,
foregoing the need for a separate reward model. However, the maximum likelihood
estimator (MLE) of the target optimal policy requires labeled preference pairs
sampled from that policy. DPO's lack of a reward model constrains its ability
to sample preference pairs from the optimal policy, and SLiC is restricted to
sampling preference pairs only from the SFT policy. To address these
limitations, we introduce a novel approach called Statistical Rejection
Sampling Optimization (RSO) that aims to source preference data from the target
optimal policy using rejection sampling, enabling a more accurate estimation of
the optimal policy. We also propose a unified framework that enhances the loss
functions used in both SLiC and DPO from a preference modeling standpoint.
Through extensive experiments across three diverse tasks, we demonstrate that
RSO consistently outperforms both SLiC and DPO on evaluations from both Large
Language Model (LLM) and human raters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish. (arXiv:2309.06698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06698">
<div class="article-summary-box-inner">
<span><p>Understanding procedural natural language (e.g., step-by-step instructions)
is a crucial step to execution and planning. However, while there are ample
corpora and downstream tasks available in English, the field lacks such
resources for most languages. To address this gap, we conduct a case study on
Turkish procedural texts. We first expand the number of tutorials in Turkish
wikiHow from 2,000 to 52,000 using automated translation tools, where the
translation quality and loyalty to the original meaning are validated by a team
of experts on a random set. Then, we generate several downstream tasks on the
corpus, such as linking actions, goal inference, and summarization. To tackle
these tasks, we implement strong baseline models via fine-tuning large
language-specific models such as TR-BART and BERTurk, as well as multilingual
models such as mBART, mT5, and XLM. We find that language-specific models
consistently outperform their multilingual models by a significant margin
across most procedural language understanding (PLU) tasks. We release our
corpus, downstream tasks and the baseline models with https://github.com/
GGLAB-KU/turkish-plu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSlice: Interactive Vision-and-Language Slice Discovery. (arXiv:2309.06703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06703">
<div class="article-summary-box-inner">
<span><p>Recent work in vision-and-language demonstrates that large-scale pretraining
can learn generalizable models that are efficiently transferable to downstream
tasks. While this may improve dataset-scale aggregate metrics, analyzing
performance around hand-crafted subgroups targeting specific bias dimensions
reveals systemic undesirable behaviors. However, this subgroup analysis is
frequently stalled by annotation efforts, which require extensive time and
resources to collect the necessary data. Prior art attempts to automatically
discover subgroups to circumvent these constraints but typically leverages
model behavior on existing task-specific annotations and rapidly degrades on
more complex inputs beyond "tabular" data, none of which study
vision-and-language models. This paper presents VLSlice, an interactive system
enabling user-guided discovery of coherent representation-level subgroups with
consistent visiolinguistic behavior, denoted as vision-and-language slices,
from unlabeled image sets. We show that VLSlice enables users to quickly
generate diverse high-coherency slices in a user study (n=22) and release the
tool publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Machine Translation with Large Language Models. (arXiv:2309.06706v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06706">
<div class="article-summary-box-inner">
<span><p>Large language models (LLM) have demonstrated their abilities to solve
various natural language processing tasks through dialogue-based interactions.
For instance, research indicates that LLMs can achieve competitive performance
in offline machine translation tasks for high-resource languages. However,
applying LLMs to simultaneous machine translation (SimulMT) poses many
challenges, including issues related to the training-inference mismatch arising
from different decoding patterns. In this paper, we explore the feasibility of
utilizing LLMs for SimulMT. Building upon conventional approaches, we introduce
a simple yet effective mixture policy that enables LLMs to engage in SimulMT
without requiring additional training. Furthermore, after Supervised
Fine-Tuning (SFT) on a mixture of full and prefix sentences, the model exhibits
significant performance improvements. Our experiments, conducted with
Llama2-7B-chat on nine language pairs from the MUST-C dataset, demonstrate that
LLM can achieve translation quality and latency comparable to dedicated SimulMT
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling. (arXiv:2309.06726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06726">
<div class="article-summary-box-inner">
<span><p>Keyphrase generation is a task of identifying a set of phrases that best
repre-sent the main topics or themes of a given text. Keyphrases are dividend
int pre-sent and absent keyphrases. Recent approaches utilizing
sequence-to-sequence models show effectiveness on absent keyphrase generation.
However, the per-formance is still limited due to the hardness of finding
absent keyphrases. In this paper, we propose Keyphrase-Focused BART, which
exploits the differ-ences between present and absent keyphrase generations, and
performs fine-tuning of two separate BART models for present and absent
keyphrases. We further show effective approaches of shuffling keyphrases and
candidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART
achieved new state-of-the-art score on F1@5 in two out of five keyphrase
gen-eration benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation. (arXiv:2309.06748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06748">
<div class="article-summary-box-inner">
<span><p>Conversational search provides a natural interface for information retrieval
(IR). Recent approaches have demonstrated promising results in applying dense
retrieval to conversational IR. However, training dense retrievers requires
large amounts of in-domain paired data. This hinders the development of
conversational dense retrievers, as abundant in-domain conversations are
expensive to collect. In this paper, we propose CONVERSER, a framework for
training conversational dense retrievers with at most 6 examples of in-domain
dialogues. Specifically, we utilize the in-context learning capability of large
language models to generate conversational queries given a passage in the
retrieval corpus. Experimental results on conversational retrieval benchmarks
OR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable
performance to fully-supervised models, demonstrating the effectiveness of our
proposed framework in few-shot conversational dense retrieval. All source code
and generated datasets are available at https://github.com/MiuLab/CONVERSER
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaled Prompt-Tuning for Few-Shot Natural Language Generation. (arXiv:2309.06759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06759">
<div class="article-summary-box-inner">
<span><p>The increasingly Large Language Models (LLMs) demonstrate stronger language
understanding and generation capabilities, while the memory demand and
computation cost of fine-tuning LLMs on downstream tasks are non-negligible.
Besides, fine-tuning generally requires a certain amount of data from
individual tasks whilst data collection cost is another issue to consider in
real-world applications. In this work, we focus on Parameter-Efficient
Fine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG),
which freeze most parameters in LLMs and tune a small subset of parameters in
few-shot cases so that memory footprint, training cost, and labeling cost are
reduced while maintaining or even improving the performance. We propose a
Scaled Prompt-Tuning (SPT) method which surpasses conventional PT with better
performance and generalization ability but without an obvious increase in
training cost. Further study on intermediate SPT suggests the superior
transferability of SPT in few-shot scenarios, providing a recipe for
data-deficient and computation-limited circumstances. Moreover, a comprehensive
comparison of existing PEFT methods reveals that certain approaches exhibiting
decent performance with modest training cost such as Prefix-Tuning in prior
study could struggle in few-shot NLG tasks, especially on challenging datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cognitive Mirage: A Review of Hallucinations in Large Language Models. (arXiv:2309.06794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06794">
<div class="article-summary-box-inner">
<span><p>As large language models continue to develop in the field of AI, text
generation systems are susceptible to a worrisome phenomenon known as
hallucination. In this study, we summarize recent compelling insights into
hallucinations in LLMs. We present a novel taxonomy of hallucinations from
various text generation tasks, thus provide theoretical insights, detection
methods and improvement approaches. Based on this, future research directions
are proposed. Our contribution are threefold: (1) We provide a detailed and
complete taxonomy for hallucinations appearing in text generation tasks; (2) We
provide theoretical analyses of hallucinations in LLMs and provide existing
detection and improvement methods; (3) We propose several research directions
that can be developed in the future. As hallucinations garner significant
attention from the community, we will maintain updates on relevant research
progress.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models. (arXiv:2309.06814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06814">
<div class="article-summary-box-inner">
<span><p>Contextual Relation Extraction (CRE) is mainly used for constructing a
knowledge graph with a help of ontology. It performs various tasks such as
semantic search, query answering, and textual entailment. Relation extraction
identifies the entities from raw texts and the relations among them. An
efficient and accurate CRE system is essential for creating domain knowledge in
the biomedical industry. Existing Machine Learning and Natural Language
Processing (NLP) techniques are not suitable to predict complex relations from
sentences that consist of more than two relations and unspecified entities
efficiently. In this work, deep learning techniques have been used to identify
the appropriate semantic relation based on the context from multiple sentences.
Even though various machine learning models have been used for relation
extraction, they provide better results only for binary relations, i.e.,
relations occurred exactly between the two entities in a sentence. Machine
learning models are not suited for complex sentences that consist of the words
that have various meanings. To address these issues, hybrid deep learning
models have been used to extract the relations from complex sentence
effectively. This paper explores the analysis of various deep learning models
that are used for relation extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for Subjectivity Detection in News Articles. (arXiv:2309.06844v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06844">
<div class="article-summary-box-inner">
<span><p>The wide-spread use of social networks has given rise to subjective,
misleading, and even false information on the Internet. Thus, subjectivity
detection can play an important role in ensuring the objectiveness and the
quality of a piece of information. This paper presents the solution built by
the Gpachov team for the CLEF-2023 CheckThat! lab Task~2 on subjectivity
detection. Three different research directions are explored. The first one is
based on fine-tuning a sentence embeddings encoder model and dimensionality
reduction. The second one explores a sample-efficient few-shot learning model.
The third one evaluates fine-tuning a multilingual transformer on an altered
dataset, using data from multiple languages. Finally, the three approaches are
combined in a simple majority voting ensemble, resulting in 0.77 macro F1 on
the test set and achieving 2nd place on the English subtask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06908">
<div class="article-summary-box-inner">
<span><p>Topic models have been proposed for decades with various applications and
recently refreshed by the neural variational inference. However, these topic
models adopt totally distinct dataset, implementation, and evaluation settings,
which hinders their quick utilization and fair comparisons. This greatly
hinders the research progress of topic models. To address these issues, in this
paper we propose a Topic Modeling System Toolkit (TopMost). Compared to
existing toolkits, TopMost stands out by covering a wider range of topic
modeling scenarios including complete lifecycles with dataset pre-processing,
model training, testing, and evaluations. The highly cohesive and decoupled
modular design of TopMost enables quick utilization, fair comparisons, and
flexible extensions of different topic models. This can facilitate the research
and applications of topic models. Our code, tutorials, and documentation are
available at https://github.com/bobxwu/topmost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06917">
<div class="article-summary-box-inner">
<span><p>Recent advancements in data-driven task-oriented dialogue systems (ToDs)
struggle with incremental learning due to computational constraints and
time-consuming issues. Continual Learning (CL) attempts to solve this by
avoiding intensive pre-training, but it faces the problem of catastrophic
forgetting (CF). While generative-based rehearsal CL methods have made
significant strides, generating pseudo samples that accurately reflect the
underlying task-specific distribution is still a challenge. In this paper, we
present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal
strategy for CL. Unlike the traditionally used Gaussian latent variable in the
Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and
versatility of the Dirichlet distribution to model the latent prior variable.
This enables it to efficiently capture sentence-level features of previous
tasks and effectively guide the generation of pseudo samples. In addition, we
introduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based
knowledge distillation method that enhances knowledge transfer during pseudo
sample generation. Our experiments confirm the efficacy of our approach in both
intent detection and slot-filling tasks, outperforming state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Native Language Identification with Big Bird Embeddings. (arXiv:2309.06923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06923">
<div class="article-summary-box-inner">
<span><p>Native Language Identification (NLI) intends to classify an author's native
language based on their writing in another language. Historically, the task has
heavily relied on time-consuming linguistic feature engineering, and
transformer-based NLI models have thus far failed to offer effective, practical
alternatives. The current work investigates if input size is a limiting factor,
and shows that classifiers trained using Big Bird embeddings outperform
linguistic feature engineering models by a large margin on the Reddit-L2
dataset. Additionally, we provide further insight into input length
dependencies, show consistent out-of-sample performance, and qualitatively
analyze the embedding space. Given the effectiveness and computational
efficiency of this method, we believe it offers a promising avenue for future
NLI work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Causal Disentanglement Model for Dialogue Emotion Detection. (arXiv:2309.06928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06928">
<div class="article-summary-box-inner">
<span><p>Emotion detection is a critical technology extensively employed in diverse
fields. While the incorporation of commonsense knowledge has proven beneficial
for existing emotion detection methods, dialogue-based emotion detection
encounters numerous difficulties and challenges due to human agency and the
variability of dialogue content.In dialogues, human emotions tend to accumulate
in bursts. However, they are often implicitly expressed. This implies that many
genuine emotions remain concealed within a plethora of unrelated words and
dialogues.In this paper, we propose a Dynamic Causal Disentanglement Model
based on hidden variable separation, which is founded on the separation of
hidden variables. This model effectively decomposes the content of dialogues
and investigates the temporal accumulation of emotions, thereby enabling more
precise emotion recognition. First, we introduce a novel Causal Directed
Acyclic Graph (DAG) to establish the correlation between hidden emotional
information and other observed elements. Subsequently, our approach utilizes
pre-extracted personal attributes and utterance topics as guiding factors for
the distribution of hidden variables, aiming to separate irrelevant ones.
Specifically, we propose a dynamic temporal disentanglement model to infer the
propagation of utterances and hidden variables, enabling the accumulation of
emotion-related information throughout the conversation. To guide this
disentanglement process, we leverage the ChatGPT-4.0 and LSTM networks to
extract utterance topics and personal attributes as observed
information.Finally, we test our approach on two popular datasets in dialogue
emotion detection and relevant experimental results verified the model's
superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06979">
<div class="article-summary-box-inner">
<span><p>Large language models display remarkable capabilities in logical and
mathematical reasoning, allowing them to solve complex tasks. Interestingly,
these abilities emerge in networks trained on the simple task of next-token
prediction. In this work, we present a theoretical framework for studying
auto-regressive next-token predictors. We demonstrate that even simple models
such as linear next-token predictors, trained on Chain-of-Thought (CoT) data,
can approximate any function efficiently computed by a Turing machine. We
introduce a new complexity measure -- length complexity -- which measures the
number of intermediate tokens in a CoT sequence required to approximate some
target function, and analyze the interplay between length complexity and other
notions of complexity. Finally, we show experimentally that simple next-token
predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs),
display non-trivial performance on text generation and arithmetic tasks. Our
results demonstrate that the power of language models can be attributed, to a
great extent, to the auto-regressive next-token training scheme, and not
necessarily to a particular choice of architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Remote Inference of Cognitive Scores in ALS Patients Using a Picture Description. (arXiv:2309.06989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06989">
<div class="article-summary-box-inner">
<span><p>Amyotrophic lateral sclerosis is a fatal disease that not only affects
movement, speech, and breath but also cognition. Recent studies have focused on
the use of language analysis techniques to detect ALS and infer scales for
monitoring functional progression. In this paper, we focused on another
important aspect, cognitive impairment, which affects 35-50% of the ALS
population. In an effort to reach the ALS population, which frequently exhibits
mobility limitations, we implemented the digital version of the Edinburgh
Cognitive and Behavioral ALS Screen (ECAS) test for the first time. This test
which is designed to measure cognitive impairment was remotely performed by 56
participants from the EverythingALS Speech Study. As part of the study,
participants (ALS and non-ALS) were asked to describe weekly one picture from a
pool of many pictures with complex scenes displayed on their computer at home.
We analyze the descriptions performed within +/- 60 days from the day the ECAS
test was administered and extract different types of linguistic and acoustic
features. We input those features into linear regression models to infer 5 ECAS
sub-scores and the total score. Speech samples from the picture description are
reliable enough to predict the ECAS subs-scores, achieving statistically
significant Spearman correlation values between 0.32 and 0.51 for the model's
performance using 10-fold cross-validation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06991">
<div class="article-summary-box-inner">
<span><p>Language models contain ranking-based knowledge and are powerful solvers of
in-context ranking tasks. For instance, they may have parametric knowledge
about the ordering of countries by size or may be able to rank reviews by
sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting
techniques to elicit a language model's ranking knowledge. However, we find
that even with careful calibration and constrained decoding, prompting-based
techniques may not always be self-consistent in the rankings they produce. This
motivates us to explore an alternative approach that is inspired by an
unsupervised probing method called Contrast-Consistent Search (CCS). The idea
is to train a probing model guided by a logical constraint: a model's
representation of a statement and its negation must be mapped to contrastive
true-false poles consistently across multiple statements. We hypothesize that
similar constraints apply to ranking tasks where all items are related via
consistent pairwise or listwise comparisons. To this end, we extend the binary
CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking
methods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regression
objective. Our results confirm that, for the same language model, CCR probing
outperforms prompting and even performs on a par with prompting much larger
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OYXOY: A Modern NLP Test Suite for Modern Greek. (arXiv:2309.07009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07009">
<div class="article-summary-box-inner">
<span><p>This paper serves as a foundational step towards the development of a
linguistically motivated and technically relevant evaluation suite for Greek
NLP. We initiate this endeavor by introducing four expert-verified evaluation
tasks, specifically targeted at natural language inference, word sense
disambiguation (through example comparison or sense selection) and metaphor
detection. More than language-adapted replicas of existing tasks, we contribute
two innovations which will resonate with the broader resource and evaluation
community. Firstly, our inference dataset is the first of its kind, marking not
just \textit{one}, but rather \textit{all} possible inference labels,
accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we
demonstrate a cost-efficient method to obtain datasets for under-resourced
languages. Using ChatGPT as a language-neutral parser, we transform the
Dictionary of Standard Modern Greek into a structured format, from which we
derive the other three tasks through simple projections. Alongside each task,
we conduct experiments using currently available state of the art machinery.
Our experimental baselines affirm the challenging nature of our tasks and
highlight the need for expedited progress in order for the Greek NLP ecosystem
to keep pace with contemporary mainstream research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R\'esum\'e Parsing as Hierarchical Sequence Labeling: An Empirical Study. (arXiv:2309.07015v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07015">
<div class="article-summary-box-inner">
<span><p>Extracting information from r\'esum\'es is typically formulated as a
two-stage problem, where the document is first segmented into sections and then
each section is processed individually to extract the target entities. Instead,
we cast the whole problem as sequence labeling in two levels -- lines and
tokens -- and study model architectures for solving both tasks simultaneously.
We build high-quality r\'esum\'e parsing corpora in English, French, Chinese,
Spanish, German, Portuguese, and Swedish. Based on these corpora, we present
experimental results that demonstrate the effectiveness of the proposed models
for the information extraction task, outperforming approaches introduced in
previous work. We conduct an ablation study of the proposed architectures. We
also analyze both model performance and resource efficiency, and describe the
trade-offs for model deployment in the context of a production environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond original Research Articles Categorization via NLP. (arXiv:2309.07020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07020">
<div class="article-summary-box-inner">
<span><p>This work proposes a novel approach to text categorization -- for unknown
categories -- in the context of scientific literature, using Natural Language
Processing techniques. The study leverages the power of pre-trained language
models, specifically SciBERT, to extract meaningful representations of
abstracts from the ArXiv dataset. Text categorization is performed using the
K-Means algorithm, and the optimal number of clusters is determined based on
the Silhouette score. The results demonstrate that the proposed approach
captures subject information more effectively than the traditional arXiv
labeling system, leading to improved text categorization. The approach offers
potential for better navigation and recommendation systems in the rapidly
growing landscape of scientific research literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How (Not) to Use Sociodemographic Information for Subjective NLP Tasks. (arXiv:2309.07034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07034">
<div class="article-summary-box-inner">
<span><p>Annotators' sociodemographic backgrounds (i.e., the individual compositions
of their gender, age, educational background, etc.) have a strong impact on
their decisions when working on subjective NLP tasks, such as hate speech
detection. Often, heterogeneous backgrounds result in high disagreements. To
model this variation, recent work has explored sociodemographic prompting, a
technique, which steers the output of prompt-based models towards answers that
humans with specific sociodemographic profiles would give. However, the
available NLP literature disagrees on the efficacy of this technique -- it
remains unclear, for which tasks and scenarios it can help and evaluations are
limited to specific tasks only. We address this research gap by presenting the
largest and most comprehensive study of sociodemographic prompting today.
Concretely, we evaluate several prompt formulations across seven datasets and
six instruction-tuned model families. We find that (1) while sociodemographic
prompting can be beneficial for improving zero-shot learning in subjective NLP
tasks, (2) its outcomes largely vary for different model types, sizes, and
datasets, (3) are subject to large variance with regards to prompt
formulations. Thus, sociodemographic prompting is not a reliable proxy for
traditional data annotation with a sociodemographically heterogeneous group of
annotators. Instead, we propose (4) to use it for identifying ambiguous
instances resulting in more informed annotation efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions. (arXiv:2309.07045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07045">
<div class="article-summary-box-inner">
<span><p>With the rapid development of Large Language Models (LLMs), increasing
attention has been paid to their safety concerns. Consequently, evaluating the
safety of LLMs has become an essential task for facilitating the broad
applications of LLMs. Nevertheless, the absence of comprehensive safety
evaluation benchmarks poses a significant impediment to effectively assess and
enhance the safety of LLMs. In this work, we present SafetyBench, a
comprehensive benchmark for evaluating the safety of LLMs, which comprises
11,435 diverse multiple choice questions spanning across 7 distinct categories
of safety concerns. Notably, SafetyBench also incorporates both Chinese and
English data, facilitating the evaluation in both languages. Our extensive
tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot
settings reveal a substantial performance advantage for GPT-4 over its
counterparts, and there is still significant room for improving the safety of
current LLMs. We believe SafetyBench will enable fast and comprehensive
evaluation of LLMs' safety, and foster the development of safer LLMs. Data and
evaluation guidelines are available at https://github.com/thu-coai/SafetyBench.
Submission entrance and leaderboard are available at
https://llmbench.ai/safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Compiler Optimization. (arXiv:2309.07062v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07062">
<div class="article-summary-box-inner">
<span><p>We explore the novel application of Large Language Models to code
optimization. We present a 7B-parameter transformer model trained from scratch
to optimize LLVM assembly for code size. The model takes as input unoptimized
assembly and outputs a list of compiler options to best optimize the program.
Crucially, during training, we ask the model to predict the instruction counts
before and after optimization, and the optimized code itself. These auxiliary
learning tasks significantly improve the optimization performance of the model
and improve the model's depth of understanding.
</p>
<p>We evaluate on a large suite of test programs. Our approach achieves a 3.0%
improvement in reducing instruction counts over the compiler, outperforming two
state-of-the-art baselines that require thousands of compilations. Furthermore,
the model shows surprisingly strong code reasoning abilities, generating
compilable code 91% of the time and perfectly emulating the output of the
compiler 70% of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Whisper perform speech-based in-context learning. (arXiv:2309.07081v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07081">
<div class="article-summary-box-inner">
<span><p>This paper investigates the in-context learning abilities of the Whisper
automatic speech recognition (ASR) models released by OpenAI. A novel
speech-based in-context learning (SICL) approach is proposed for test-time
adaptation, which can reduce the word error rates (WERs) with only a small
number of labelled speech samples without gradient descent. Language-level
adaptation experiments using Chinese dialects showed that when applying SICL to
isolated word ASR, consistent and considerable relative WER reductions can be
achieved using Whisper models of any size on two dialects, which is on average
32.3%. A k-nearest-neighbours-based in-context example selection technique can
be applied to further improve the efficiency of SICL, which can increase the
average relative WER reduction to 36.4%. The findings are verified using
speaker adaptation or continuous speech recognition tasks, and both achieved
considerable relative WER reductions. Detailed quantitative analyses are also
provided to shed light on SICL's adaptability to phonological variances and
dialect-specific lexical nuances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding. (arXiv:2309.07098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07098">
<div class="article-summary-box-inner">
<span><p>Hallucinations and off-target translation remain unsolved problems in machine
translation, especially for low-resource languages and massively multilingual
models. In this paper, we introduce methods to mitigate both failure cases with
a modified decoding objective, without requiring retraining or external models.
In source-contrastive decoding, we search for a translation that is probable
given the correct input, but improbable given a random input segment,
hypothesising that hallucinations will be similarly probable given either. In
language-contrastive decoding, we search for a translation that is probable,
but improbable given the wrong language indicator token. In experiments on
M2M-100 (418M) and SMaLL-100, we find that these methods effectively suppress
hallucinations and off-target translations, improving chrF2 by 1.7 and 1.4
points on average across 57 tested translation directions. In a proof of
concept on English--German, we also show that we can suppress off-target
translations with the Llama 2 chat models, demonstrating the applicability of
the method to machine translation with LLMs. We release our source code at
https://github.com/ZurichNLP/ContraDecode.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics. (arXiv:2309.07120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07120">
<div class="article-summary-box-inner">
<span><p>Multi-modal large language models (MLLMs) are trained based on large language
models (LLM), with an enhanced capability to comprehend multi-modal inputs and
generate textual responses. While they excel in multi-modal tasks, the pure NLP
abilities of MLLMs are often underestimated and left untested. In this study,
we get out of the box and unveil an intriguing characteristic of MLLMs -- our
preliminary results suggest that visual instruction tuning, a prevailing
strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly
helps models attain both improved truthfulness and ethical alignment in the
pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model
surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one
million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further
analysis reveals that the improved alignment can be attributed to the superior
instruction quality inherent to visual-text data. In releasing our code at
github.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further exploration
into the intrinsic value of visual-text synergies and, in a broader scope,
multi-modal interactions in alignment research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07124">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) often demonstrate inconsistencies with human
preferences. Previous research gathered human preference data and then aligned
the pre-trained models using reinforcement learning or instruction tuning, the
so-called finetuning step. In contrast, aligning frozen LLMs without any extra
data is more appealing. This work explores the potential of the latter setting.
We discover that by integrating self-evaluation and rewind mechanisms,
unaligned LLMs can directly produce responses consistent with human preferences
via self-boosting. We introduce a novel inference method, Rewindable
Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate
their own generation and use the evaluation results to guide backward rewind
and forward generation for AI safety. Notably, RAIN operates without the need
of extra data for model alignment and abstains from any training, gradient
computation, or parameter updates; during the self-evaluation phase, the model
receives guidance on which human preference to align with through a
fixed-template prompt, eliminating the need to modify the initial prompt.
Experimental results evaluated by GPT-4 and humans demonstrate the
effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate
of LLaMA 30B over vanilla inference from 82% to 97%, while maintaining the
helpfulness rate. Under the leading adversarial attack llm-attacks on Vicuna
33B, RAIN establishes a new defense baseline by reducing the attack success
rate from 94% to 19%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event and Entity Extraction from Generated Video Captions. (arXiv:2211.02982v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.02982">
<div class="article-summary-box-inner">
<span><p>Annotation of multimedia data by humans is time-consuming and costly, while
reliable automatic generation of semantic metadata is a major challenge. We
propose a framework to extract semantic metadata from automatically generated
video captions. As metadata, we consider entities, the entities' properties,
relations between entities, and the video category. We employ two
state-of-the-art dense video captioning models with masked transformer (MT) and
parallel decoding (PVDC) to generate captions for videos of the ActivityNet
Captions dataset. Our experiments show that it is possible to extract entities,
their properties, relations between entities, and the video category from the
generated captions. We observe that the quality of the extracted information is
mainly influenced by the quality of the event localization in the video as well
as the performance of the event caption generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01378">
<div class="article-summary-box-inner">
<span><p>We propose a new paradigm to continually evolve pretrained models, denoted
ColD Fusion. It provides the benefits of multitask learning but leverages
distributed computation with limited communication and eliminates the need for
shared data. Consequentially, ColD Fusion can give rise to a synergistic loop,
where finetuned models can be recycled to continually improve the pretrained
model they are based upon. We show that ColD Fusion yields comparable benefits
to multitask training by producing a model that (a) attains strong performance
on all of the datasets it was trained on; and (b) is a better starting point
for finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTa
and even previous multitask models. Specifically, when training and testing on
35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 points
on average without any changes to the architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diminished Diversity-of-Thought in a Standard Large Language Model. (arXiv:2302.07267v6 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07267">
<div class="article-summary-box-inner">
<span><p>We test whether Large Language Models (LLMs) can be used to simulate human
participants in social-science studies. To do this, we run replications of 14
studies from the Many Labs 2 replication project with OpenAI's text-davinci-003
model, colloquially known as GPT3.5. Based on our pre-registered analyses, we
find that among the eight studies we could analyse, our GPT sample replicated
37.5% of the original results and 37.5% of the Many Labs 2 results. However, we
were unable to analyse the remaining six studies due to an unexpected
phenomenon we call the "correct answer" effect. Different runs of GPT3.5
answered nuanced questions probing political orientation, economic preference,
judgement, and moral philosophy with zero or near-zero variation in responses:
with the supposedly "correct answer." In one exploratory follow-up study, we
found that a "correct answer" was robust to changing the demographic details
that precede the prompt. In another, we found that most but not all "correct
answers" were robust to changing the order of answer choices. One of our most
striking findings occurred in our replication of the Moral Foundations Theory
survey results, where we found GPT3.5 identifying as a political conservative
in 99.6% of the cases, and as a liberal in 99.3% of the cases in the
reverse-order condition. However, both self-reported 'GPT conservatives' and
'GPT liberals' showed right-leaning moral foundations. Our results cast doubts
on the validity of using LLMs as a general replacement for human participants
in the social sciences. Our results also raise concerns that a hypothetical
AI-led future may be subject to a diminished diversity-of-thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects. (arXiv:2304.11075v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11075">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in NLP largely increased the presence of ASR systems in
our daily lives. However, for many low-resource languages, ASR models still
need to be improved due in part to the difficulty of acquiring pertinent data.
This project aims to help advance research in ASR models for Swiss German
dialects, by providing insights about the performance of state-of-the-art ASR
models on recently published Swiss German speech datasets. We propose a novel
loss that takes into account the semantic distance between the predicted and
the ground-truth labels. We outperform current state-of-the-art results by
fine-tuning OpenAI's Whisper model on Swiss-German datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Processing Natural Language on Embedded Devices: How Well Do Modern Models Perform?. (arXiv:2304.11520v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11520">
<div class="article-summary-box-inner">
<span><p>Voice-controlled systems are becoming ubiquitous in many IoT-specific
applications such as home/industrial automation, automotive infotainment, and
healthcare. While cloud-based voice services (\eg Alexa, Siri) can leverage
high-performance computing servers, some use cases (\eg robotics, automotive
infotainment) may require to execute the natural language processing (NLP)
tasks offline, often on resource-constrained embedded devices. Large language
models such as BERT and its variants are primarily developed with compute-heavy
servers in mind. Despite the great performance of BERT models across various
NLP tasks, their large size and numerous parameters pose substantial obstacles
to offline computation on embedded systems. Lighter replacement of such
language models (\eg DistilBERT and TinyBERT) often sacrifice accuracy,
particularly for complex NLP tasks. Until now, it is still unclear \ca whether
the state-of-the-art language models, \viz BERT and its variants are deployable
on embedded systems with a limited processor, memory, and battery power and \cb
if they do, what are the ``right'' set of configurations and parameters to
choose for a given NLP task. This paper presents an \textit{exploratory study
of modern language models} under different resource constraints and accuracy
budgets to derive empirical observations about these resource/accuracy
trade-offs. In particular, we study how the four most commonly used BERT-based
language models (\eg BERT, RoBERTa, DistilBERT, and TinyBERT) perform on
embedded systems. We tested them on a Raspberry Pi-based robotic platform with
three hardware configurations and four datasets running various NLP tasks. Our
findings can help designers to understand the deployability and performance of
modern language models, especially those based on BERT architectures, thus
saving a lot of time wasted in trial-and-error efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06908">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models (DDPMs) have shown promising
performance for speech synthesis. However, a large number of iterative steps
are required to achieve high sample quality, which restricts the inference
speed. Maintaining sample quality while increasing sampling speed has become a
challenging task. In this paper, we propose a "Co"nsistency "Mo"del-based
"Speech" synthesis method, CoMoSpeech, which achieve speech synthesis through a
single diffusion sampling step while achieving high audio quality. The
consistency constraint is applied to distill a consistency model from a
well-designed diffusion-based teacher model, which ultimately yields superior
performances in the distilled CoMoSpeech. Our experiments show that by
generating audio recordings by a single sampling step, the CoMoSpeech achieves
an inference speed more than 150 times faster than real-time on a single NVIDIA
A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based
speech synthesis truly practical. Meanwhile, objective and subjective
evaluations on text-to-speech and singing voice synthesis show that the
proposed teacher models yield the best audio quality, and the one-step sampling
based CoMoSpeech achieves the best inference speed with better or comparable
audio quality to other conventional multi-step diffusion model baselines. Audio
samples are available at https://comospeech.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does ChatGPT have Theory of Mind?. (arXiv:2305.14020v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14020">
<div class="article-summary-box-inner">
<span><p>Theory of Mind (ToM) is the ability to understand human thinking and
decision-making, an ability that plays a crucial role in social interaction
between people, including linguistic communication. This paper investigates to
what extent recent Large Language Models in the ChatGPT tradition possess ToM.
We posed six well-known problems that address biases in human reasoning and
decision making to two versions of ChatGPT and we compared the results under a
range of prompting strategies. While the results concerning ChatGPT-3 were
somewhat inconclusive, ChatGPT-4 was shown to arrive at the correct answers
more often than would be expected based on chance, although correct answers
were often arrived at on the basis of false assumptions or invalid reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Language Models Know When They're Hallucinating References?. (arXiv:2305.18248v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18248">
<div class="article-summary-box-inner">
<span><p>State-of-the-art language models (LMs) are famous for "hallucinating"
references. These fabricated article and book titles lead to harms, obstacles
to their use, and public backlash. While other types of LM hallucinations are
also important, we propose hallucinated references as the "drosophila" of
research on hallucination in large language models (LLMs), as they are
particularly easy to study. We show that simple search engine queries reliably
identify such hallucinations, which facilitates evaluation. To begin to dissect
the nature of hallucinated LM references, we attempt to classify them using
black-box queries to the same LM, without consulting any external resources.
Consistency checks done with "direct" queries about whether the generated
reference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022,
Manakul et al. 2023) are compared to consistency checks with "indirect" queries
which ask for ancillary details such as the authors of the work. These
consistency checks are found to be partially reliable indicators of whether or
not the reference is a hallucination. In particular, we find that LMs often
hallucinate differing authors of hallucinated references when queried in
independent sessions, while consistently identify authors of real references.
This suggests that the hallucination may be more a generation issue than
inherent to current training techniques or representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Accurate Speech Emotion Recognition. (arXiv:2306.07848v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07848">
<div class="article-summary-box-inner">
<span><p>Contrastive cross-modality pretraining has recently exhibited impressive
success in diverse fields, whereas there is limited research on their merits in
speech emotion recognition (SER). In this paper, we propose GEmo-CLAP, a kind
of gender-attribute-enhanced contrastive language-audio pretraining (CLAP)
method for SER. Specifically, we first construct an effective emotion CLAP
(Emo-CLAP) for SER, using pre-trained text and audio encoders. Second, given
the significance of gender information in SER, two novel multi-task learning
based GEmo-CLAP (ML-GEmo-CLAP) and soft label based GEmo-CLAP (SL-GEmo-CLAP)
models are further proposed to incorporate gender information of speech
signals, forming more reasonable objectives. Experiments on IEMOCAP indicate
that our proposed two GEmo-CLAPs consistently outperform Emo-CLAP with
different pre-trained models. Remarkably, the proposed WavLM-based SL-GEmo-CLAP
obtains the best UAR of 81.43% and WAR of 83.16%, which performs better than
state-of-the-art SER methods by at least 3%. Our system is open-sourced on
Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-corpus Readability Compatibility Assessment for English Texts. (arXiv:2306.09704v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09704">
<div class="article-summary-box-inner">
<span><p>Text readability assessment has gained significant attention from researchers
in various domains. However, the lack of exploration into corpus compatibility
poses a challenge as different research groups utilize different corpora. In
this study, we propose a novel evaluation framework, Cross-corpus text
Readability Compatibility Assessment (CRCA), to address this issue. The
framework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES,
OSP, and RACE. Linguistic features, GloVe word vector representations, and
their fusion features were extracted. (2) Classification models: Machine
learning methods (XGBoost, SVM) and deep learning methods (BiLSTM,
Attention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and
NDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with
OSP standing out as significantly different from other datasets. (2) An
adaptation effect among corpora, feature representations, and classification
methods. (3) Consistent outcomes across the three metrics, validating the
robustness of the compatibility assessment framework. The outcomes of this
study offer valuable insights into corpus selection, feature representation,
and classification methods, and it can also serve as a beginning effort for
cross-corpus transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06435">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have recently demonstrated remarkable
capabilities in natural language processing tasks and beyond. This success of
LLMs has led to a large influx of research contributions in this direction.
These works encompass diverse topics such as architectural innovations of the
underlying neural networks, context length improvements, model alignment,
training datasets, benchmarking, efficiency and more. With the rapid
development of techniques and regular breakthroughs in LLM research, it has
become considerably challenging to perceive the bigger picture of the advances
in this direction. Considering the rapidly emerging plethora of literature on
LLMs, it is imperative that the research community is able to benefit from a
concise yet comprehensive overview of the recent developments in this field.
This article provides that overview to the research community. It not only
focuses on a systematic treatment of the existing literature on a broad range
of LLM related concept, but also pays special attention to providing
comprehensive summaries with extensive details about the individual existing
models, datasets and major insights. We also pay heed to aligning our overview
with the emerging outlook of this research direction by accounting for the
other recently materializing reviews of the broader research direction of LLMs.
Our self-contained comprehensive overview of LLMs discusses relevant background
concepts along with covering the advanced topics at the frontier of this
research direction. This review article is intended to not only provide a
systematic survey, but also a quick comprehensive reference for the researchers
and practitioners to draw insights from extensive informative summaries of the
existing works to advance the LLM research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Findings of Factify 2: Multimodal Fake News Detection. (arXiv:2307.10475v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10475">
<div class="article-summary-box-inner">
<span><p>With social media usage growing exponentially in the past few years, fake
news has also become extremely prevalent. The detrimental impact of fake news
emphasizes the need for research focused on automating the detection of false
information and verifying its accuracy. In this work, we present the outcome of
the Factify 2 shared task, which provides a multi-modal fact verification and
satire news dataset, as part of the DeFactify 2 workshop at AAAI'23. The data
calls for a comparison based approach to the task by pairing social media
claims with supporting documents, with both text and image, divided into 5
classes based on multi-modal relations. In the second iteration of this task we
had over 60 participants and 9 final test-set submissions. The best
performances came from the use of DeBERTa for text and Swinv2 and CLIP for
image. The highest F1 score averaged for all five classes was 81.82%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRDD: A Dataset for Greek Dialectal NLP. (arXiv:2308.00802v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00802">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a dataset for the computational study of a number
of Modern Greek dialects. It consists of raw text data from four dialects of
Modern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is
of considerable size, albeit imbalanced, and presents the first attempt to
create large scale dialectal resources of this type for Modern Greek dialects.
We then use the dataset to perform dialect idefntification. We experiment with
traditional ML algorithms, as well as simple DL architectures. The results show
very good performance on the task, potentially revealing that the dialects in
question have distinct enough characteristics allowing even simple ML models to
perform well on the task. Error analysis is performed for the top performing
algorithms showing that in a number of cases the errors are due to insufficient
dataset cleaning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. (arXiv:2308.01825v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01825">
<div class="article-summary-box-inner">
<span><p>Mathematical reasoning is a challenging task for large language models
(LLMs), while the scaling relationship of it with respect to LLM capacity is
under-explored. In this paper, we investigate how the pre-training loss,
supervised data amount, and augmented data amount influence the reasoning
performances of a supervised LLM. We find that pre-training loss is a better
indicator of the model's performance than the model's parameter count. We apply
supervised fine-tuning (SFT) with different amounts of supervised data and
empirically find a log-linear relation between data amount and model
performance, and we find better models improve less with enlarged supervised
datasets. To augment more data samples for improving model performances without
any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT
uses supervised models to generate and collect correct reasoning paths as
augmented fine-tuning datasets. We find with augmented samples containing more
distinct reasoning paths, RFT improves mathematical reasoning performance more
for LLMs. We also find RFT brings more improvement for less performant LLMs.
Furthermore, we combine rejection samples from multiple models which push
LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised
fine-tuning (SFT) accuracy of 35.9\% significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability. (arXiv:2308.03266v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03266">
<div class="article-summary-box-inner">
<span><p>Hotword customization is one of the concerned issues remained in ASR field -
it is of value to enable users of ASR systems to customize names of entities,
persons and other phrases to obtain better experience. The past few years have
seen effective modeling strategies for ASR contextualization developed, but
they still exhibit space for improvement about training stability and the
invisible activation process. In this paper we propose Semantic-Augmented
Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with
flexible and effective hotword customization ability. It possesses the
advantages of AED-based model's accuracy, NAR model's efficiency, and explicit
customization capacity of superior performance. Through extensive experiments
with 50,000 hours of industrial big data, our proposed model outperforms strong
baselines in customization. Besides, we explore an efficient way to filter
large-scale incoming hotwords for further improvement. The industrial models
compared, source codes and two hotword test sets are all open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages. (arXiv:2308.09435v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09435">
<div class="article-summary-box-inner">
<span><p>Modern large language models demonstrate impressive capabilities in text
generation and generalization. However, they often struggle with solving text
editing tasks, particularly when it comes to correcting spelling errors and
mistypings. In this paper, we present a methodology for generative spelling
correction (SC), which was tested on English and Russian languages and
potentially can be extended to any language with minor changes. Our research
mainly focuses on exploring natural spelling errors and mistypings in texts and
studying the ways those errors can be emulated in correct sentences to
effectively enrich generative models' pre-train procedure. We investigate the
impact of such emulations and the models' abilities across different text
domains. In this work, we investigate two spelling corruption techniques: 1)
first one mimics human behavior when making a mistake through leveraging
statistics of errors from particular dataset and 2) second adds the most common
spelling errors, keyboard miss clicks, and some heuristics within the texts. We
conducted experiments employing various corruption strategies, models'
architectures and sizes on the pre-training and fine-tuning stages and
evaluated the models using single-domain and multi-domain test sets. As a
practical outcome of our work, we introduce SAGE(Spell checking via
Augmentation and Generative distribution Emulation). It is a library for
automatic generative SC that includes a family of pre-trained generative models
and built-in augmentation algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning. (arXiv:2309.01538v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01538">
<div class="article-summary-box-inner">
<span><p>Logical rules are essential for uncovering the logical connections between
relations, which could improve the reasoning performance and provide
interpretable results on knowledge graphs (KGs). Although there have been many
efforts to mine meaningful logical rules over KGs, existing methods suffer from
the computationally intensive searches over the rule space and a lack of
scalability for large-scale KGs. Besides, they often ignore the semantics of
relations which is crucial for uncovering logical connections. Recently, large
language models (LLMs) have shown impressive performance in the field of
natural language processing and various applications, owing to their emergent
ability and generalizability. In this paper, we propose a novel framework,
ChatRule, unleashing the power of large language models for mining logical
rules over knowledge graphs. Specifically, the framework is initiated with an
LLM-based rule generator, leveraging both the semantic and structural
information of KGs to prompt LLMs to generate logical rules. To refine the
generated rules, a rule ranking module estimates the rule quality by
incorporating facts from existing KGs. Last, a rule validator harnesses the
reasoning ability of LLMs to validate the logical correctness of ranked rules
through chain-of-thought reasoning. ChatRule is evaluated on four large-scale
KGs, w.r.t. different rule quality metrics and downstream tasks, showing the
effectiveness and scalability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Impact of Post-Training Quantization on Large Language Models. (arXiv:2309.05210v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05210">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are rapidly increasing in size, with the number
of parameters becoming a key factor in the success of many commercial models,
such as ChatGPT, Claude, and Bard. Even the recently released publicly
accessible models for commercial usage, such as Falcon and Llama2, come
equipped with billions of parameters. This significant increase in the number
of parameters makes deployment and operation very costly. The remarkable
progress in the field of quantization for large neural networks in general and
LLMs in particular, has made these models more accessible by enabling them to
be deployed on consumer-grade GPUs. Quantized models generally demonstrate
comparable performance levels to their unquantized base counterparts.
Nonetheless, there exists a notable gap in our comprehensive understanding of
how these quantized models respond to hyperparameters, such as temperature, max
new tokens, and topk, particularly for next word prediction. The present
analysis reveals that nf4 and fp4 are equally proficient 4-bit quantization
techniques, characterized by similar attributes such as inference speed, memory
consumption, and the quality of generated content. Nevertheless, these
quantization methods exhibit distinct behaviors at varying temperature
settings, both in the context of smaller and larger models. It is noteworthy
that, in general, 4-bit quantized models of varying sizes exhibit heightened
sensitivity to lower temperature settings, unlike their unquantized
counterparts. Additionally, int8 quantization is associated with significantly
slower inference speeds, whereas unquantized fp16 models consistently yield the
fastest inference speeds across models of all sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NExT-GPT: Any-to-Any Multimodal LLM. (arXiv:2309.05519v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05519">
<div class="article-summary-box-inner">
<span><p>While recently Multimodal Large Language Models (MM-LLMs) have made exciting
strides, they mostly fall prey to the limitation of only input-side multimodal
understanding, without the ability to produce content in multiple modalities.
As we humans always perceive the world and communicate with people through
various modalities, developing any-to-any MM-LLMs capable of accepting and
delivering content in any modality becomes essential to human-level AI. To fill
the gap, we present an end-to-end general-purpose any-to-any MM-LLM system,
NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion
decoders, enabling NExT-GPT to perceive inputs and generate outputs in
arbitrary combinations of text, images, videos, and audio. By leveraging the
existing well-trained highly-performing encoders and decoders, NExT-GPT is
tuned with only a small amount of parameter (1%) of certain projection layers,
which not only benefits low-cost training and also facilitates convenient
expansion to more potential modalities. Moreover, we introduce a
modality-switching instruction tuning (MosIT) and manually curate a
high-quality dataset for MosIT, based on which NExT-GPT is empowered with
complex cross-modal semantic understanding and content generation. Overall, our
research showcases the promising possibility of building an AI agent capable of
modeling universal modalities, paving the way for more human-like AI research
in the community. Project page: https://next-gpt.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05605">
<div class="article-summary-box-inner">
<span><p>Answering multi-hop reasoning questions requires retrieving and synthesizing
information from diverse sources. Large Language Models (LLMs) struggle to
perform such reasoning consistently. Here we propose an approach to pinpoint
and rectify multi-hop reasoning failures through targeted memory injections on
LLM attention heads. First, we analyze the per-layer activations of GPT-2
models in response to single and multi-hop prompts. We then propose a mechanism
that allows users to inject pertinent prompt-specific information, which we
refer to as "memories," at critical LLM locations during inference. By thus
enabling the LLM to incorporate additional relevant information during
inference, we enhance the quality of multi-hop prompt completions. We show
empirically that a simple, efficient, and targeted memory injection into a key
attention layer can often increase the probability of the desired next token in
multi-hop tasks, by up to 424%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05918">
<div class="article-summary-box-inner">
<span><p>In our opinion the exuberance surrounding the relative success of data-driven
large language models (LLMs) is slightly misguided and for several reasons (i)
LLMs cannot be relied upon for factual information since for LLMs all ingested
text (factual or non-factual) was created equal; (ii) due to their subsymbolic
na-ture, whatever 'knowledge' these models acquire about language will always
be buried in billions of microfeatures (weights), none of which is meaningful
on its own; and (iii) LLMs will often fail to make the correct inferences in
several linguistic contexts (e.g., nominal compounds, copredication, quantifier
scope ambi-guities, intensional contexts. Since we believe the relative success
of data-driven large language models (LLMs) is not a reflection on the symbolic
vs. subsymbol-ic debate but a reflection on applying the successful strategy of
a bottom-up reverse engineering of language at scale, we suggest in this paper
applying the effective bottom-up strategy in a symbolic setting resulting in
symbolic, explainable, and ontologically grounded language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking. (arXiv:2309.06175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06175">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach to address the Entity Recognition and
Linking Challenge at NLPCC 2015. The task involves extracting named entity
mentions from short search queries and linking them to entities within a
reference Chinese knowledge base. To tackle this problem, we first expand the
existing knowledge base and utilize external knowledge to identify candidate
entities, thereby improving the recall rate. Next, we extract features from the
candidate entities and utilize Support Vector Regression and Multiple Additive
Regression Tree as scoring functions to filter the results. Additionally, we
apply rules to further refine the results and enhance precision. Our method is
computationally efficient and achieves an F1 score of 0.535.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-14 23:11:04.965218496 UTC">2023-09-14 23:11:04 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>