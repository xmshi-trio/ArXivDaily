<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-05T01:30:00Z">01-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">An ensemble-based framework for mispronunciation detection of Arabic phonemes. (arXiv:2301.01378v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01378">
<div class="article-summary-box-inner">
<span><p>Determination of mispronunciations and ensuring feedback to users are
maintained by computer-assisted language learning (CALL) systems. In this work,
we introduce an ensemble model that defines the mispronunciation of Arabic
phonemes and assists learning of Arabic, effectively. To the best of our
knowledge, this is the very first attempt to determine the mispronunciations of
Arabic phonemes employing ensemble learning techniques and conventional machine
learning models, comprehensively. In order to observe the effect of feature
extraction techniques, mel-frequency cepstrum coefficients (MFCC), and Mel
spectrogram are blended with each learning algorithm. To show the success of
proposed model, 29 letters in the Arabic phonemes, 8 of which are hafiz, are
voiced by a total of 11 different person. The amount of data set has been
enhanced employing the methods of adding noise, time shifting, time stretching,
pitch shifting. Extensive experiment results demonstrate that the utilization
of voting classifier as an ensemble algorithm with Mel spectrogram feature
extraction technique exhibits remarkable classification result with 95.9% of
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Efficient Conformer for Robust Speech Recognition. (arXiv:2301.01456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01456">
<div class="article-summary-box-inner">
<span><p>End-to-end Automatic Speech Recognition (ASR) systems based on neural
networks have seen large improvements in recent years. The availability of
large scale hand-labeled datasets and sufficient computing resources made it
possible to train powerful deep neural networks, reaching very low Word Error
Rate (WER) on academic benchmarks. However, despite impressive performance on
clean audio samples, a drop of performance is often observed on noisy speech.
In this work, we propose to improve the noise robustness of the recently
proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based
architecture by processing both audio and visual modalities. We improve
previous lip reading methods using an Efficient Conformer back-end on top of a
ResNet-18 visual front-end and by adding intermediate CTC losses between
blocks. We condition intermediate block features on early predictions using
Inter CTC residual modules to relax the conditional independence assumption of
CTC-based models. We also replace the Efficient Conformer grouped attention by
a more efficient and simpler attention mechanism that we call patch attention.
We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip
Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and
visual modalities allows to better recognize speech in the presence of
environmental noise and significantly accelerate training, reaching lower WER
with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)
model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on
LRS2 and LRS3 test sets. Code and pretrained models are available at
https://github.com/burchim/AVEC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Ambiguity from Crowd Sequential Annotations. (arXiv:2301.01579v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01579">
<div class="article-summary-box-inner">
<span><p>Most crowdsourcing learning methods treat disagreement between annotators as
noisy labelings while inter-disagreement among experts is often a good
indicator for the ambiguity and uncertainty that is inherent in natural
language. In this paper, we propose a framework called Learning Ambiguity from
Crowd Sequential Annotations (LA-SCA) to explore the inter-disagreement between
reliable annotators and effectively preserve confusing label information.
First, a hierarchical Bayesian model is developed to infer ground-truth from
crowds and group the annotators with similar reliability together. By modeling
the relationship between the size of group the annotator involved in, the
annotator's reliability and element's unambiguity in each sequence,
inter-disagreement between reliable annotators on ambiguous elements is
computed to obtain label confusing information that is incorporated to
cost-sensitive sequence labeling. Experimental results on POS tagging and NER
tasks show that our proposed framework achieves competitive performance in
inferring ground-truth from crowds and predicting unknown sequences, and
interpreting hierarchical clustering results helps discover labeling patterns
of annotators with similar reliability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar construction methods for extended deterministic expressions. (arXiv:2301.01621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01621">
<div class="article-summary-box-inner">
<span><p>Extended regular expressions with counting and interleaving are widely used
in practice. However the related theoretical studies for this kind of
expressions currently cannot meet the need of practical work. This paper
develops syntax definitions for extended deterministic expressions and their
subclasses, hope to completely solve the long-standing problem that there are
no syntax definitions for this kind of expressions, which has become an
important reason for restricting the use of extended expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01664">
<div class="article-summary-box-inner">
<span><p>Recent studies on knowledge graphs (KGs) show that path-based methods
empowered by pre-trained language models perform well in the provision of
inductive and explainable relation predictions. In this paper, we introduce the
concepts of relation path coverage and relation path confidence to filter out
unreliable paths prior to model training to elevate the model performance.
Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict
inductive relations in KGs. KRST is designed to encode the extracted reliable
paths in KGs, allowing us to properly cluster paths and provide multi-aspect
explanations. We conduct extensive experiments on three real-world datasets.
The experimental results show that compared to SOTA models, KRST achieves the
best performance in most transductive and inductive test cases (4 of 6), and in
11 of 12 few-shot test cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chatbots as Problem Solvers: Playing Twenty Questions with Role Reversals. (arXiv:2301.01743v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01743">
<div class="article-summary-box-inner">
<span><p>New chat AI applications like ChatGPT offer an advanced understanding of
question context and memory across multi-step tasks, such that experiments can
test its deductive reasoning. This paper proposes a multi-role and multi-step
challenge, where ChatGPT plays the classic twenty-questions game but
innovatively switches roles from the questioner to the answerer. The main
empirical result establishes that this generation of chat applications can
guess random object names in fewer than twenty questions (average, 12) and
correctly guess 94% of the time across sixteen different experimental setups.
The research introduces four novel cases where the chatbot fields the
questions, asks the questions, both question-answer roles, and finally tries to
guess appropriate contextual emotions. One task that humans typically fail but
trained chat applications complete involves playing bilingual games of twenty
questions (English answers to Spanish questions). Future variations address
direct problem-solving using a similar inquisitive format to arrive at novel
outcomes deductively, such as patentable inventions or combination thinking.
Featured applications of this dialogue format include complex protein designs,
neuroscience metadata, and child development educational materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes. (arXiv:2301.01751v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01751">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) can perform complex reasoning either end-to-end, with
hidden latent state, or compositionally, with transparent intermediate state.
Composition offers benefits for interpretability and safety, but may need
workflow support and infrastructure to remain competitive. We describe iterated
decomposition, a human-in-the-loop workflow for developing and refining
compositional LM programs. We improve the performance of compositions by
zooming in on failing components and refining them through decomposition,
additional context, chain of thought, etc. To support this workflow, we develop
ICE, an open-source tool for visualizing the execution traces of LM programs.
We apply iterated decomposition to three real-world tasks and improve the
accuracy of LM programs over less compositional baselines: describing the
placebo used in a randomized controlled trial (25% to 65%), evaluating
participant adherence to a medical intervention (53% to 70%), and answering NLP
questions on the Qasper dataset (38% to 69%). These applications serve as case
studies for a workflow that, if automated, could keep ML systems interpretable
and safe even as they scale to increasingly complex tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification. (arXiv:2301.01764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01764">
<div class="article-summary-box-inner">
<span><p>Previous state-of-the-art models for lexical simplification consist of
complex pipelines with several components, each of which requires deep
technical knowledge and fine-tuned interaction to achieve its full potential.
As an alternative, we describe a frustratingly simple pipeline based on
prompted GPT-3 responses, beating competing approaches by a wide margin in
settings with few training instances. Our best-performing submission to the
English language track of the TSAR-2022 shared task consists of an ``ensemble''
of six different prompt templates with varying context levels. As a
late-breaking result, we further detail a language transfer technique that
allows simplification in languages other than English. Applied to the Spanish
and Portuguese subset, we achieve state-of-the-art results with only minor
modification to the original prompts. Aside from detailing the implementation
and setup, we spend the remainder of this work discussing the particularities
of prompting and implications for future work. Code for the experiments is
available online \url{https://github.com/dennlinger/TSAR-2022-Shared-Task}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMC-Patients: A Large-scale Dataset of Patient Notes and Relations Extracted from Case Reports in PubMed Central. (arXiv:2202.13876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13876">
<div class="article-summary-box-inner">
<span><p>Objective: Data unavailability has been one of the biggest barriers in
clinical natural language processing. This paper is aimed at providing a
large-scale and publicly available patient note dataset, named PMC-Patients,
with relevant articles and similar patients annotations. The ultimate goal of
PMC-Patients is to facilitate the development of retrieval-based clinical
decision support systems. Materials and Methods: To collect PMC-Patients, we
extract patient notes from case reports in PubMed Central by recognizing
certain section patterns. Patient-article relevance and patient-patient
similarity are annotated by citation relationships in PubMed. In addition, we
perform three tasks with PMC-Patients to demonstrate its utility in providing
clinical decision support for a given patient, including (1) classifying
whether another patient is similar, (2) retrieving similar patients in
PMC-Patients, and (3) retrieving relevant articles in PubMed. Results: We
collect and release PMC-Patients under the CC BY-NC-SA license, which becomes
the largest publicly available patient note dataset so far. PMC-Patients
contains 167k patient notes that are annotated with 3.1M relevant articles and
293k similar patients. Qualitative and quantitative analyses reveal the high
quality and richness of our dataset. Experiments show that classifying the
similarity of patient pairs is relatively easy, but it is hard to retrieve
similar patients or relevant articles for a given patient from a large set of
candidates. Conclusion: We present PMC-Patients, a large-scale dataset of
patient notes with high quality, easy access, diverse conditions, and rich
annotations. The proposed dataset can also serve as a hard benchmark for
evaluating retrieval-based clinical decision support systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation. (arXiv:2203.02177v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02177">
<div class="article-summary-box-inner">
<span><p>Conversations have become a critical data format on social media platforms.
Understanding conversation from emotion, content and other aspects also
attracts increasing attention from researchers due to its widespread
application in human-computer interaction. In real-world environments, we often
encounter the problem of incomplete modalities, which has become a core issue
of conversation understanding. To address this problem, researchers propose
various methods. However, existing approaches are mainly designed for
individual utterances rather than conversational data, which cannot fully
exploit temporal and speaker information in conversations. To this end, we
propose a novel framework for incomplete multimodal learning in conversations,
called "Graph Complete Network (GCNet)", filling the gap of existing works. Our
GCNet contains two well-designed graph neural network-based modules, "Speaker
GNN" and "Temporal GNN", to capture temporal and speaker dependencies. To make
full use of complete and incomplete data, we jointly optimize classification
and reconstruction tasks in an end-to-end manner. To verify the effectiveness
of our method, we conduct experiments on three benchmark conversational
datasets. Experimental results demonstrate that our GCNet is superior to
existing state-of-the-art approaches in incomplete multimodal learning. Code is
available at https://github.com/zeroQiaoba/GCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaVocoder: Adaptive Vocoder for Custom Voice. (arXiv:2203.09825v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09825">
<div class="article-summary-box-inner">
<span><p>Custom voice is to construct a personal speech synthesis system by adapting
the source speech synthesis model to the target model through the target few
recordings. The solution to constructing a custom voice is to combine an
adaptive acoustic model with a robust vocoder. However, training a robust
vocoder usually requires a multi-speaker dataset, which should include various
age groups and various timbres, so that the trained vocoder can be used for
unseen speakers. Collecting such a multi-speaker dataset is difficult, and the
dataset distribution always has a mismatch with the distribution of the target
speaker dataset. This paper proposes an adaptive vocoder for custom voice from
another novel perspective to solve the above problems. The adaptive vocoder
mainly uses a cross-domain consistency loss to solve the overfitting problem
encountered by the GAN-based neural vocoder in the transfer learning of
few-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.
First, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,
respectively. Then, fine-tune it on the internal dataset VXI-children with few
adaptation data. The empirical results show that a high-quality custom voice
system can be built by combining a adaptive acoustic model with a adaptive
vocoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10438">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) show excellent performance but are compute- and
memory-intensive. Quantization can reduce memory and accelerate inference.
However, for LLMs beyond 100 billion parameters, existing methods cannot
maintain accuracy or do not run efficiently on hardware. We propose
SmoothQuant, a training-free, accuracy-preserving, and general-purpose
post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit
activation (W8A8) quantization for LLMs that can be implemented efficiently. We
observe that systematic outliers appear at fixed activation channels. Based on
the fact that weights are easy to quantize while activations are not,
SmoothQuant smooths the activation outliers by offline migrating the
quantization difficulty from activations to weights with a mathematically
equivalent transformation. SmoothQuant enables an INT8 quantization of both
weights and activations for all the GEMMs in LLMs, including OPT-175B,
BLOOM-176B, and GLM-130B. SmoothQuant has better hardware efficiency than
existing techniques using mixed-precision activation quantization or
weight-only quantization. We demonstrate up to 1.56x speedup and 2x memory
reduction for LLMs with negligible loss in accuracy. Thanks to the
hardware-friendly design, we integrate SmoothQuant into FasterTransformer, a
state-of-the-art LLM serving framework, and achieve faster inference speed with
half the number of GPUs compared to FP16. Our work offers a turn-key solution
that reduces hardware costs and democratizes LLMs. Code is available at:
https://github.com/mit-han-lab/smoothquant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task. (arXiv:2211.11216v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11216">
<div class="article-summary-box-inner">
<span><p>Benefiting from large-scale datasets and pre-trained models, the field of
generative models has recently gained significant momentum. However, most
datasets for symbolic music are very small, which potentially limits the
performance of data-driven multimodal models. An intuitive solution to this
problem is to leverage pre-trained models from other modalities (e.g., natural
language) to improve the performance of symbolic music-related multimodal
tasks. In this paper, we carry out the first study of generating complete and
semantically consistent symbolic music scores from text descriptions, and
explore the efficacy of using publicly available checkpoints (i.e., BERT,
GPT-2, and BART) for natural language processing in the task of text-to-music
generation. Our experimental results show that the improvement from using
pre-trained checkpoints is statistically significant in terms of BLEU score and
edit distance similarity. We analyse the capabilities and limitations of our
model to better understand the potential of language-music models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement. (arXiv:2212.04523v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04523">
<div class="article-summary-box-inner">
<span><p>The long-distance agreement, evidence for syntactic structure, is
increasingly used to assess the syntactic generalization of Neural Language
Models. Much work has shown that transformers are capable of high accuracy in
varied agreement tasks, but the mechanisms by which the models accomplish this
behavior are still not well understood. To better understand transformers'
internal working, this work contrasts how they handle two superficially similar
but theoretically distinct agreement phenomena: subject-verb and object-past
participle agreement in French. Using probing and counterfactual analysis
methods, our experiments show that i) the agreement task suffers from several
confounders which partially question the conclusions drawn so far and ii)
transformers handle subject-verb and object-past participle agreements in a way
that is consistent with their modeling in theoretical linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-Level Debiased Natural Language Understanding. (arXiv:2212.05421v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05421">
<div class="article-summary-box-inner">
<span><p>Natural language understanding (NLU) models often rely on dataset biases
rather than intended task-relevant features to achieve high performance on
specific datasets. As a result, these models perform poorly on datasets outside
the training distribution. Some recent studies address this issue by reducing
the weights of biased samples during the training process. However, these
methods still encode biased latent features in representations and neglect the
dynamic nature of bias, which hinders model prediction. We propose an NLU
debiasing method, named debiasing contrastive learning (DCT), to simultaneously
alleviate the above problems based on contrastive learning. We devise a
debiasing, positive sampling strategy to mitigate biased latent features by
selecting the least similar biased positive samples. We also propose a dynamic
negative sampling strategy to capture the dynamic influence of biases by
employing a bias-only model to dynamically select the most similar biased
negative samples. We conduct experiments on three NLU benchmark datasets.
Experimental results show that DCT outperforms state-of-the-art baselines on
out-of-distribution datasets while maintaining in-distribution performance. We
also verify that DCT can reduce biased latent features from the model's
representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01181">
<div class="article-summary-box-inner">
<span><p>We demonstrate a proof-of-concept of a large language model conducting
corporate lobbying related activities. An autoregressive large language model
(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are
relevant to specific public companies and provides explanations and confidence
levels. For the bills the model deems as relevant, the model drafts a letter to
the sponsor of the bill in an attempt to persuade the congressperson to make
changes to the proposed legislation. We use hundreds of ground-truth labels of
the relevance of a bill to a company to benchmark the performance of the model,
which outperforms the baseline of predicting the most common outcome of
irrelevance. We also benchmark the performance of the previous OpenAI GPT-3
model (text-davinci-002), which was state-of-the-art on many language tasks
until text-davinci-003 was released on November 28, 2022. The performance of
text-davinci-002 is worse than simply always predicting that a bill is
irrelevant to a company. These results suggest that, as large language models
continue to exhibit improved core natural language understanding capabilities,
performance on corporate lobbying related tasks will continue to improve. We
then discuss why this could be problematic for societal-AI alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StarGraph: Knowledge Representation Learning based on Incomplete Two-hop Subgraph. (arXiv:2205.14209v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14209">
<div class="article-summary-box-inner">
<span><p>Conventional representation learning algorithms for knowledge graphs (KG) map
each entity to a unique embedding vector, ignoring the rich information
contained in the neighborhood. We propose a method named StarGraph, which gives
a novel way to utilize the neighborhood information for large-scale knowledge
graphs to obtain entity representations. An incomplete two-hop neighborhood
subgraph for each target node is at first generated, then processed by a
modified self-attention network to obtain the entity representation, which is
used to replace the entity embedding in conventional methods. We achieved SOTA
performance on ogbl-wikikg2 and got competitive results on fb15k-237. The
experimental results proves that StarGraph is efficient in parameters, and the
improvement made on ogbl-wikikg2 demonstrates its great effectiveness of
representation learning on large-scale knowledge graphs. The code is now
available at \url{https://github.com/hzli-ucas/StarGraph}.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-05 23:14:05.554511403 UTC">2023-01-05 23:14:05 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>