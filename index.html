<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-22T01:30:00Z">08-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09720">
<div class="article-summary-box-inner">
<span><p>Large language models are capable of displaying a wide range of abilities
that are not directly connected with the task for which they are trained:
predicting the next words of human-written texts. In this article, I discuss
the nature of this indirect acquisition process and its relation to other known
indirect processes. I argue that an important side effect of such indirect
acquisition is the development of integrated abilities. I discuss the extent to
which the abilities developed by large language models are predictable.
Finally, I briefly discuss the relation between the cognitive skills acquired
by these systems and human cognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data. (arXiv:2308.09722v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09722">
<div class="article-summary-box-inner">
<span><p>Social media cyberbullying has a detrimental effect on human life. As online
social networking grows daily, the amount of hate speech also increases. Such
terrible content can cause depression and actions related to suicide. This
paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection
on social media using synthetic data. We have demonstrated a cutting-edge
method to address data availability difficulties by producing
machine-translated data. However, several languages such as Hindi and Bangla
still lack adequate investigations due to a lack of datasets. We carried out
experimental identification of aggressive comments on Hindi, Bangla, and
English datasets using the proposed model and traditional models, including
Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM),
LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from
Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.
We employed evaluation metrics such as f1-score, accuracy, precision, and
recall to assess the models performance. Our proposed model outperformed all
the models on all datasets, achieving the highest accuracy of 95%. Our model
achieves state-of-the-art results among all the previous works on the dataset
we used in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs. (arXiv:2308.09723v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09723">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved state-of-the-art performance
across various language tasks but pose challenges for practical deployment due
to their substantial memory requirements. Furthermore, the latest generative
models suffer from high inference costs caused by the memory bandwidth
bottleneck in the auto-regressive decoding process. To address these issues, we
propose an efficient weight-only quantization method that reduces memory
consumption and accelerates inference for LLMs. To ensure minimal quality
degradation, we introduce a simple and effective heuristic approach that
utilizes only the model weights of a pre-trained model. This approach is
applicable to both Mixture-of-Experts (MoE) and dense models without requiring
additional fine-tuning. To demonstrate the effectiveness of our proposed
method, we first analyze the challenges and issues associated with LLM
quantization. Subsequently, we present our heuristic approach, which adaptively
finds the granularity of quantization, effectively addressing these problems.
Furthermore, we implement highly efficient GPU GEMMs that perform on-the-fly
matrix multiplication and dequantization, supporting the multiplication of fp16
or bf16 activations with int8 or int4 weights. We evaluate our approach on
large-scale open source models such as OPT-175B and internal MoE models,
showcasing minimal accuracy loss while achieving up to 3.65 times higher
throughput on the same number of GPUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09729">
<div class="article-summary-box-inner">
<span><p>LLMs usually exhibit limitations in their ability to incorporate new
knowledge, the generation of hallucinations, and the transparency of their
decision-making process. In this paper, we explore how to prompt LLMs with
knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date
knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a
prompting pipeline that endows LLMs with the capability of comprehending KG
inputs and inferring with a combined implicit knowledge and the retrieved
external knowledge. In addition, we investigate eliciting the mind map on which
LLMs perform the reasoning and generate the answers. It is identified that the
produced mind map exhibits the reasoning pathways of LLMs grounded on the
ontology of knowledge, hence bringing the prospects of probing and gauging LLM
inference in production. The experiments on three question &amp; answering datasets
also show that MindMap prompting leads to a striking empirical gain. For
instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance
over GPT-4 consistently. We also demonstrate that with structured facts
retrieved from KG, MindMap can outperform a series of
prompting-with-document-retrieval methods, benefiting from more accurate,
concise, and comprehensive knowledge from KGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT. (arXiv:2308.09731v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09731">
<div class="article-summary-box-inner">
<span><p>This study presents an innovative approach to the application of large
language models (LLMs) in clinical decision-making, focusing on OpenAI's
ChatGPT. Our approach introduces the use of contextual prompts-strategically
designed to include task description, feature description, and crucially,
integration of domain knowledge-for high-quality binary classification tasks
even in data-scarce scenarios. The novelty of our work lies in the utilization
of domain knowledge, obtained from high-performing interpretable ML models, and
its seamless incorporation into prompt design. By viewing these ML models as
medical experts, we extract key insights on feature importance to aid in
decision-making processes. This interplay of domain knowledge and AI holds
significant promise in creating a more insightful diagnostic tool.
</p>
<p>Additionally, our research explores the dynamics of zero-shot and few-shot
prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT
with traditional supervised ML models in different data conditions, we aim to
provide insights into the effectiveness of prompt engineering strategies under
varied data availability. In essence, this paper bridges the gap between AI and
healthcare, proposing a novel methodology for LLMs application in clinical
decision support systems. It highlights the transformative potential of
effective prompt design, domain knowledge integration, and flexible learning
approaches in enhancing automated decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09765">
<div class="article-summary-box-inner">
<span><p>Accurately evaluating the similarity of object vector embeddings is of
critical importance for natural language processing, information retrieval and
classification tasks. Popular similarity scores (e.g cosine similarity) are
based on pairs of embedding vectors and disregard the distribution of the
ensemble from which objects are drawn. Human perception of object similarity
significantly depends on the context in which the objects appear. In this work
we propose the \emph{surprise score}, an ensemble-normalized similarity metric
that encapsulates the contrast effect of human perception and significantly
improves the classification performance on zero- and few-shot document
classification tasks. This score quantifies the surprise to find a given
similarity between two elements relative to the pairwise ensemble similarities.
We evaluate this metric on zero/few shot classification and clustering tasks
and typically find 10-15\% better performance compared to raw cosine
similarity. Our code is available at
https://github.com/MeetElise/surprise-similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YORC: Yoruba Reading Comprehension dataset. (arXiv:2308.09768v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09768">
<div class="article-summary-box-inner">
<span><p>In this paper, we create YORC: a new multi-choice Yoruba Reading
Comprehension dataset that is based on Yoruba high-school reading comprehension
examination. We provide baseline results by performing cross-lingual transfer
using existing English RACE dataset based on a pre-trained encoder-only model.
Additionally, we provide results by prompting large language models (LLMs) like
GPT-4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09778">
<div class="article-summary-box-inner">
<span><p>With the advances in large scale vision-and-language models (VLMs) it is of
interest to assess their performance on various visual reasoning tasks such as
counting, referring expressions and general visual question answering. The
focus of this work is to study the ability of these models to understanding
spatial relations. Previously, this has been tackled using image-text matching
(Liu, Emerson, and Collier 2022) or visual question answering task, both
showing poor performance and a large gap compared to human performance. To
better understand the gap, we present fine-grained compositional grounding of
spatial relationships and propose a bottom up approach for ranking spatial
clauses and evaluating the performance of spatial relationship reasoning task.
We propose to combine the evidence from grounding noun phrases corresponding to
objects and their locations to compute the final rank of the spatial clause. We
demonstrate the approach on representative vision-language models (Tan and
Bansal 2019; Gupta et al. 2022; Kamath et al. 2021) and compare and highlight
their abilities to reason about spatial relationships.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control. (arXiv:2308.09804v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09804">
<div class="article-summary-box-inner">
<span><p>As the model size of pre-trained language models (PLMs) grows rapidly, full
fine-tuning becomes prohibitively expensive for model training and storage. In
vision-and-language (VL), parameter-efficient tuning (PET) techniques are
proposed to integrate modular modifications (e.g., Adapter and LoRA) into
encoder-decoder PLMs. By tuning a small set of trainable parameters, these
techniques perform on par with full fine-tuning. However, excessive modular
modifications and neglecting the functionality gap between the encoders and
decoders can lead to performance degradation, while existing PET techniques
(e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a
Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose
effective control over modular modifications via a novel granularity-controlled
mechanism. Considering different granularity-controlled matrices generated by
this mechanism, a variety of model-agnostic VL-PET modules can be instantiated
from our framework for better efficiency and effectiveness trade-offs. We
further propose lightweight PET module designs to enhance VL alignment and
modeling for the encoders and maintain text generation for the decoders.
Extensive experiments conducted on four image-text tasks and four video-text
tasks demonstrate the efficiency, effectiveness and transferability of our
VL-PET framework. In particular, our VL-PET-large with lightweight PET module
designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37%
(7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate
the enhanced effect of employing our VL-PET designs on existing PET techniques,
enabling them to achieve significant performance improvements. Our code is
available at https://github.com/HenryHZY/VL-PET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Image is Worth a Thousand Toxic Words: A Metamorphic Testing Framework for Content Moderation Software. (arXiv:2308.09810v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09810">
<div class="article-summary-box-inner">
<span><p>The exponential growth of social media platforms has brought about a
revolution in communication and content dissemination in human society.
Nevertheless, these platforms are being increasingly misused to spread toxic
content, including hate speech, malicious advertising, and pornography, leading
to severe negative consequences such as harm to teenagers' mental health.
Despite tremendous efforts in developing and deploying textual and image
content moderation methods, malicious users can evade moderation by embedding
texts into images, such as screenshots of the text, usually with some
interference. We find that modern content moderation software's performance
against such malicious inputs remains underexplored. In this work, we propose
OASIS, a metamorphic testing framework for content moderation software. OASIS
employs 21 transform rules summarized from our pilot study on 5,000 real-world
toxic contents collected from 4 popular social media applications, including
Twitter, Instagram, Sina Weibo, and Baidu Tieba. Given toxic textual contents,
OASIS can generate image test cases, which preserve the toxicity yet are likely
to bypass moderation. In the evaluation, we employ OASIS to test five
commercial textual content moderation software from famous companies (i.e.,
Google Cloud, Microsoft Azure, Baidu Cloud, Alibaba Cloud and Tencent Cloud),
as well as a state-of-the-art moderation research model. The results show that
OASIS achieves up to 100% error finding rates. Moreover, through retraining the
models with the test cases generated by OASIS, the robustness of the moderation
model can be improved without performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How susceptible are LLMs to Logical Fallacies?. (arXiv:2308.09853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09853">
<div class="article-summary-box-inner">
<span><p>This paper investigates the rational thinking capability of Large Language
Models (LLMs) in multi-round argumentative debates by exploring the impact of
fallacious arguments on their logical reasoning performance. More specifically,
we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic
benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM
involves two agents: a persuader and a debater engaging in a multi-round debate
on a controversial topic, where the persuader tries to convince the debater of
the correctness of its claim. First, LOGICOM assesses the potential of LLMs to
change their opinions through reasoning. Then, it evaluates the debater's
performance in logical reasoning by contrasting the scenario where the
persuader employs logical fallacies against one where logical reasoning is
used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4
using a dataset containing controversial topics, claims, and reasons supporting
them. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their
opinion through reasoning. However, when presented with logical fallacies,
GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often,
respectively, compared to when logical reasoning is used. Finally, we introduce
a new dataset containing over 5k pairs of logical vs. fallacious arguments. The
source code and dataset of this work are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view Contrastive Learning Method. (arXiv:2308.09861v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09861">
<div class="article-summary-box-inner">
<span><p>Neural ranking models (NRMs) and dense retrieval (DR) models have given rise
to substantial improvements in overall retrieval performance. In addition to
their effectiveness, and motivated by the proven lack of robustness of deep
learning-based approaches in other areas, there is growing interest in the
robustness of deep learning-based approaches to the core retrieval problem.
Adversarial attack methods that have so far been developed mainly focus on
attacking NRMs, with very little attention being paid to the robustness of DR
models. In this paper, we introduce the adversarial retrieval attack (AREA)
task. The AREA task is meant to trick DR models into retrieving a target
document that is outside the initial set of candidate documents retrieved by
the DR model in response to a query. We consider the decision-based black-box
adversarial setting, which is realistic in real-world search engines. To
address the AREA task, we first employ existing adversarial attack methods
designed for NRMs. We find that the promising results that have previously been
reported on attacking NRMs, do not generalize to DR models: these methods
underperform a simple term spamming method. We attribute the observed lack of
generalizability to the interaction-focused architecture of NRMs, which
emphasizes fine-grained relevance matching. DR models follow a different
representation-focused architecture that prioritizes coarse-grained
representations. We propose to formalize attacks on DR models as a contrastive
learning problem in a multi-view representation space. The core idea is to
encourage the consistency between each view representation of the target
document and its corresponding viewer via view-wise supervision signals.
Experimental results demonstrate that the proposed method can significantly
outperform existing attack strategies in misleading the DR model with small
indiscernible text perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi. (arXiv:2308.09862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09862">
<div class="article-summary-box-inner">
<span><p>The recent advances in deep-learning have led to the development of highly
sophisticated systems with an unquenchable appetite for data. On the other
hand, building good deep-learning models for low-resource languages remains a
challenging task. This paper focuses on developing a Question Answering dataset
for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most
spoken language worldwide, with 345 million speakers, and Marathi being the
11th most spoken language globally, with 83.2 million speakers, both languages
face limited resources for building efficient Question Answering systems. To
tackle the challenge of data scarcity, we have developed a novel approach for
translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the
largest Question-Answering dataset available for these languages, with each
dataset containing 28,000 samples. We evaluate the dataset on various
architectures and release the best-performing models for both Hindi and
Marathi, which will facilitate further research in these languages. Leveraging
similarity tools, our method holds the potential to create datasets in diverse
languages, thereby enhancing the understanding of natural language across
varied linguistic contexts. Our fine-tuned models, code, and dataset will be
made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09890">
<div class="article-summary-box-inner">
<span><p>Large Language Models(LLMs) have been attracting attention due to a ability
called in-context learning(ICL). ICL, without updating the parameters of a LLM,
it is possible to achieve highly accurate inference based on rules ``in the
context'' by merely inputting a training data into the prompt. Although ICL is
a developing field with many unanswered questions, LLMs themselves serves as a
inference model, seemingly realizing inference without explicitly indicate
``inductive bias''. On the other hand, a code generation is also a highlighted
application of LLMs. The accuracy of code generation has dramatically improved,
enabling even non-engineers to generate code to perform the desired tasks by
crafting appropriate prompts. In this paper, we propose a novel ``learning''
method called an ``Inductive-Bias Learning (IBL)'', which combines the
techniques of ICL and code generation. An idea of IBL is straightforward. Like
ICL, IBL inputs a training data into the prompt and outputs a code with a
necessary structure for inference (we referred to as ``Code Model'') from a
``contextual understanding''. Despite being a seemingly simple approach, IBL
encompasses both a ``property of inference without explicit inductive bias''
inherent in ICL and a ``readability and explainability'' of the code
generation. Surprisingly, generated Code Models have been found to achieve
predictive accuracy comparable to, and in some cases surpassing, ICL and
representative machine learning models. Our IBL code is open source:
https://github.com/fuyu-quant/IBLM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection. (arXiv:2308.09892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09892">
<div class="article-summary-box-inner">
<span><p>Survey data can contain a high number of features while having a
comparatively low quantity of examples. Machine learning models that attempt to
predict outcomes from survey data under these conditions can overfit and result
in poor generalizability. One remedy to this issue is feature selection, which
attempts to select an optimal subset of features to learn upon. A relatively
unexplored source of information in the feature selection process is the usage
of textual names of features, which may be semantically indicative of which
features are relevant to a target outcome. The relationships between feature
names and target names can be evaluated using language models (LMs) to produce
semantic textual similarity (STS) scores, which can then be used to select
features. We examine the performance using STS to select features directly and
in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance
of STS as a feature selection metric is evaluated against preliminary survey
data collected as a part of a clinical study on persistent post-surgical pain
(PPSP). The results suggest that features selected with STS can result in
higher performance models compared to traditional feature selection algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs. (arXiv:2308.09954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09954">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) possess a wealth of knowledge encoded in their
parameters. However, this knowledge may become outdated or unsuitable over
time. As a result, there has been a growing interest in knowledge editing for
LLMs and evaluating its effectiveness. Existing studies primarily focus on
knowledge editing using factual triplets, which not only incur high costs for
collection but also struggle to express complex facts. Furthermore, these
studies are often limited in their evaluation perspectives. In this paper, we
propose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs.
This benchmark includes an evaluation framework and a corresponding dataset.
Under our framework, we first ask the LLM to perform knowledge editing using
raw documents, which provides a more convenient and universal approach compared
to using factual triplets. We then evaluate the updated LLM from multiple
perspectives. In addition to assessing the effectiveness of knowledge editing
and the retention of unrelated knowledge from conventional studies, we further
test the LLM's ability in two aspects: 1) Reasoning with the altered knowledge,
aiming for the LLM to genuinely learn the altered knowledge instead of simply
memorizing it. 2) Cross-lingual knowledge transfer, where the LLM updated with
raw documents in one language should be capable of handling queries from
another language. To facilitate further research, we construct and release the
corresponding dataset. Using this benchmark, we investigate the effectiveness
of several commonly-used knowledge editing methods. Experimental results
indicate that the current methods for knowledge editing using raw documents are
not effective in yielding satisfactory results, particularly when it comes to
reasoning with altered knowledge and cross-lingual knowledge transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate. (arXiv:2308.09957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09957">
<div class="article-summary-box-inner">
<span><p>LLMs like GPT are great at tasks involving English which dominates in their
training data. In this paper, we look at how they cope with tasks involving
languages that are severely under-represented in their training data, in the
context of data-to-text generation for Irish, Maltese, Welsh and Breton. During
the prompt-engineering phase we tested a range of prompt types and formats on
GPT-3.5 and~4 with a small sample of example input/output pairs. We then fully
evaluated the two most promising prompts in two scenarios: (i) direct
generation into the under-resourced language, and (ii) generation into English
followed by translation into the under-resourced language. We find that
few-shot prompting works better for direct generation into under-resourced
languages, but that the difference disappears when pivoting via English. The
few-shot + translation system variants were submitted to the WebNLG 2023 shared
task where they outperformed competitor systems by substantial margins in all
languages on all metrics. We conclude that good performance on under-resourced
languages can be achieved out-of-the box with state-of-the-art LLMs. However,
our best results (for Welsh) remain well below the lowest ranked English system
at WebNLG'20.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Vision Language Tasks Through Learning Inner Monologues. (arXiv:2308.09970v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09970">
<div class="article-summary-box-inner">
<span><p>Visual language tasks require AI models to comprehend and reason with both
visual and textual content. Driven by the power of Large Language Models
(LLMs), two prominent methods have emerged: (1) the hybrid integration between
LLMs and Vision-Language Models (VLMs), where visual inputs are firstly
converted into language descriptions by VLMs, serving as inputs for LLMs to
generate final answer(s); (2) visual feature alignment in language space, where
visual inputs are encoded as embeddings and projected to LLMs' language space
via further supervised fine-tuning. The first approach provides light training
costs and interpretability but is hard to be optimized in an end-to-end
fashion. The second approach presents decent performance, but feature alignment
usually requires large amounts of training data and lacks interpretability. To
tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal
Optimization (IMMO), to solve complex vision language problems by simulating
inner monologue processes, a cognitive process in which an individual engages
in silent verbal communication with themselves. We enable LLMs and VLMs to
interact through natural language conversation and propose to use a two-stage
training process to learn how to do the inner monologue (self-asking questions
and answering questions). IMMO is evaluated on two popular tasks and the
results suggest by emulating the cognitive phenomenon of internal dialogue, our
approach can enhance reasoning and explanation abilities, contributing to the
more effective fusion of vision and language models. More importantly, instead
of using predefined human-crafted monologues, IMMO learns this process within
the deep learning models, promising wider applicability to many different AI
problems beyond vision language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models. (arXiv:2308.09975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09975">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated exceptional performance in
various natural language processing tasks, yet their efficacy in more
challenging and domain-specific tasks remains largely unexplored. This paper
presents FinEval, a benchmark specifically designed for the financial domain
knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice
questions covering Finance, Economy, Accounting, and Certificate. It includes
4,661 questions spanning 34 different academic subjects. To ensure a
comprehensive model performance evaluation, FinEval employs a range of prompt
types, including zero-shot and few-shot prompts, as well as answer-only and
chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs
on FinEval, the results show that only GPT-4 achieved an accuracy close to 70%
in different prompt settings, indicating significant growth potential for LLMs
in the financial domain knowledge. Our work offers a more comprehensive
financial knowledge evaluation benchmark, utilizing data of mock exams and
covering a wide range of evaluated LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HICL: Hashtag-Driven In-Context Learning for Social Media Natural Language Understanding. (arXiv:2308.09985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09985">
<div class="article-summary-box-inner">
<span><p>Natural language understanding (NLU) is integral to various social media
applications. However, existing NLU models rely heavily on context for semantic
learning, resulting in compromised performance when faced with short and noisy
social media content. To address this issue, we leverage in-context learning
(ICL), wherein language models learn to make inferences by conditioning on a
handful of demonstrations to enrich the context and propose a novel
hashtag-driven in-context learning (HICL) framework. Concretely, we pre-train a
model #Encoder, which employs #hashtags (user-annotated topic labels) to drive
BERT-based pre-training through contrastive learning. Our objective here is to
enable #Encoder to gain the ability to incorporate topic-related semantic
information, which allows it to retrieve topic-related posts to enrich contexts
and enhance social media NLU with noisy contexts. To further integrate the
retrieved context with the source text, we employ a gradient-based method to
identify trigger terms useful in fusing information from both sources. For
empirical studies, we collected 45M tweets to set up an in-context NLU
benchmark, and the experimental results on seven downstream tasks show that
HICL substantially advances the previous state-of-the-art results. Furthermore,
we conducted extensive analyzes and found that: (1) combining source input with
a top-retrieved post from #Encoder is more effective than using semantically
similar posts; (2) trigger words can largely benefit in merging context from
the source and retrieved posts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval. (arXiv:2308.10025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10025">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that dense retrieval models, lacking dedicated
training data, struggle to perform well across diverse retrieval tasks, as
different retrieval tasks often entail distinct search intents. To address this
challenge, in this work we introduce ControlRetriever, a generic and efficient
approach with a parameter isolated architecture, capable of controlling dense
retrieval models to directly perform varied retrieval tasks, harnessing the
power of instructions that explicitly describe retrieval intents in natural
language. Leveraging the foundation of ControlNet, which has proven powerful in
text-to-image generation, ControlRetriever imbues different retrieval models
with the new capacity of controllable retrieval, all while being guided by
task-specific instructions. Furthermore, we propose a novel LLM guided
Instruction Synthesizing and Iterative Training strategy, which iteratively
tunes ControlRetriever based on extensive automatically-generated retrieval
data with diverse instructions by capitalizing the advancement of large
language models. Extensive experiments show that in the BEIR benchmark, with
only natural language descriptions of specific retrieval intent for each task,
ControlRetriever, as a unified multi-task retrieval system without
task-specific tuning, significantly outperforms baseline methods designed with
task-specific retrievers and also achieves state-of-the-art zero-shot
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GameEval: Evaluating LLMs on Conversational Games. (arXiv:2308.10032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10032">
<div class="article-summary-box-inner">
<span><p>The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v20 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multimodal Word Discovery based on Double Articulation Analysis with Co-occurrence cues. (arXiv:2201.06786v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06786">
<div class="article-summary-box-inner">
<span><p>Human infants acquire their verbal lexicon with minimal prior knowledge of
language based on the statistical properties of phonological distributions and
the co-occurrence of other sensory stimuli. This study proposes a novel fully
unsupervised learning method for discovering speech units using phonological
information as a distributional cue and object information as a co-occurrence
cue. The proposed method can acquire words and phonemes from speech signals
using unsupervised learning and utilize object information based on multiple
modalities-vision, tactile, and auditory-simultaneously. The proposed method is
based on the nonparametric Bayesian double articulation analyzer (NPB-DAA)
discovering phonemes and words from phonological features, and multimodal
latent Dirichlet allocation (MLDA) categorizing multimodal information obtained
from objects. In an experiment, the proposed method showed higher word
discovery performance than baseline methods. Words that expressed the
characteristics of objects (i.e., words corresponding to nouns and adjectives)
were segmented accurately. Furthermore, we examined how learning performance is
affected by differences in the importance of linguistic information. Increasing
the weight of the word modality further improved performance relative to that
of the fixed condition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13243">
<div class="article-summary-box-inner">
<span><p>The last decade of machine learning has seen drastic increases in scale and
capabilities. Deep neural networks (DNNs) are increasingly being deployed in
the real world. However, they are difficult to analyze, raising concerns about
using them without a rigorous understanding of how they function. Effective
tools for interpreting them will be important for building more trustworthy AI
by helping to identify problems, fix bugs, and improve basic understanding. In
particular, "inner" interpretability techniques, which focus on explaining the
internal components of DNNs, are well-suited for developing a mechanistic
understanding, guiding manual modifications, and reverse engineering solutions.
</p>
<p>Much recent work has focused on DNN interpretability, and rapid progress has
thus far made a thorough systematization of methods difficult. In this survey,
we review over 300 works with a focus on inner interpretability tools. We
introduce a taxonomy that classifies methods by what part of the network they
help to explain (weights, neurons, subnetworks, or latent representations) and
whether they are implemented during (intrinsic) or after (post hoc) training.
To our knowledge, we are also the first to survey a number of connections
between interpretability research and work in adversarial robustness, continual
learning, modularity, network compression, and studying the human visual
system. We discuss key challenges and argue that the status quo in
interpretability research is largely unproductive. Finally, we highlight the
importance of future work that emphasizes diagnostics, debugging, adversaries,
and benchmarking in order to make interpretability tools more useful to
engineers in practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PreSTU: Pre-Training for Scene-Text Understanding. (arXiv:2209.05534v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.05534">
<div class="article-summary-box-inner">
<span><p>The ability to recognize and reason about text embedded in visual inputs is
often lacking in vision-and-language (V&amp;L) models, perhaps because V&amp;L
pre-training methods have often failed to include such an ability in their
training objective. In this paper, we propose PreSTU, a novel pre-training
recipe dedicated to scene-text understanding (STU). PreSTU introduces OCR-aware
pre-training objectives that encourage the model to recognize text from an
image and connect it to the rest of the image content. We implement PreSTU
using a simple transformer-based encoder-decoder architecture, combined with
large-scale image-text datasets with scene text obtained from an off-the-shelf
OCR system. We empirically demonstrate the effectiveness of this pre-training
approach on eight visual question answering and four image captioning
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YATO: Yet Another deep learning based Text analysis Open toolkit. (arXiv:2209.13877v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.13877">
<div class="article-summary-box-inner">
<span><p>We introduce YATO, an open-source, easy-to-use toolkit for text analysis with
deep learning. Different from existing heavily engineered toolkits and
platforms, YATO is lightweight and user-friendly for researchers from
cross-disciplinary areas. Designed in a hierarchical structure, YATO supports
free combinations of three types of widely used features including 1)
traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models
(BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a
simple configurable file. Benefiting from the advantages of flexibility and
ease of use, YATO can facilitate fast reproduction and refinement of
state-of-the-art NLP models, and promote the cross-disciplinary applications of
NLP techniques. The code, examples, and documentation are publicly available at
https://github.com/jiesutd/YATO. A demo video is also available at
https://youtu.be/tSjjf5BzfQg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision. (arXiv:2211.09778v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.09778">
<div class="article-summary-box-inner">
<span><p>Many high-level skills that are required for computer vision tasks, such as
parsing questions, comparing and contrasting semantics, and writing
descriptions, are also required in other domains such as natural language
processing. In this paper, we ask whether it is possible to learn those skills
from text data and then transfer them to vision tasks without ever training on
visual training data. Key to our approach is exploiting the joint embedding
space of contrastively trained vision and language encoders. In practice, there
can be systematic differences between embedding spaces for different modalities
in contrastive models, and we analyze how these differences affect our approach
and study strategies to mitigate this concern. We produce models using only
text training data on four representative tasks: image captioning, visual
entailment, visual question answering and visual news captioning, and evaluate
them on standard benchmarks using images. We find these models perform close to
models trained on images, while surpassing prior work for captioning and visual
entailment in this text-only setting by over 9 points, and outperforming all
prior work on visual news by over 30 points. We also showcase a variety of
stylistic image captioning models that are trained using no image data and no
human-curated language data, but instead using readily-available text data from
books, the web, or language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Practical Few-shot Federated NLP. (arXiv:2212.00192v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00192">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models have emerged as the predominant solution
for natural language processing (NLP). Fine-tuning such pre-trained models for
downstream tasks often requires a considerable amount of labeled private data.
In practice, private data is often distributed across heterogeneous mobile
devices and may be prohibited from being uploaded. Moreover, well-curated
labeled data is often scarce, presenting an additional challenge. To address
these challenges, we first introduce a data generator for federated few-shot
learning tasks, which encompasses the quantity and skewness of scarce labeled
data in a realistic setting. Subsequently, we propose AUG-FedPrompt, a
prompt-based federated learning system that exploits abundant unlabeled data
for data augmentation. Our experiments indicate that AUG-FedPrompt can perform
on par with full-set fine-tuning with a limited amount of labeled data.
However, such competitive performance comes at a significant system cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Few-Shot Learning for Mobile NLP. (arXiv:2212.05974v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05974">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) sees rich mobile applications. To support
various language understanding tasks, a foundation NLP model is often
fine-tuned in a federated, privacy-preserving setting (FL). This process
currently relies on at least hundreds of thousands of labeled training samples
from mobile clients; yet mobile users often lack willingness or knowledge to
label their data. Such an inadequacy of data labels is known as a few-shot
scenario; it becomes the key blocker for mobile NLP applications.
</p>
<p>For the first time, this work investigates federated NLP in the few-shot
scenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and
prompt learning, we first establish a training pipeline that delivers
competitive accuracy when only 0.05% (fewer than 100) of the training data is
labeled and the remaining is unlabeled. To instantiate the workflow, we further
present a system FeS, addressing the high execution cost with novel designs.
(1) Curriculum pacing, which injects pseudo labels to the training workflow at
a rate commensurate to the learning progress; (2) Representational diversity, a
mechanism for selecting the most learnable data, only for which pseudo labels
will be generated; (3) Co-planning of a model's training depth and layer
capacity. Together, these designs reduce the training delay, client energy, and
network traffic by up to 46.0$\times$, 41.2$\times$ and 3000.0$\times$,
respectively. Through algorithm/system co-design, FFNLP demonstrates that FL
can apply to challenging settings where most training samples are unlabeled.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.13939">
<div class="article-summary-box-inner">
<span><p>The performance of learning models heavily relies on the availability and
adequacy of training data. To address the dataset adequacy issue, researchers
have extensively explored data augmentation (DA) as a promising approach. DA
generates new data instances through transformations applied to the available
data, thereby increasing dataset size and variability. This approach has
enhanced model performance and accuracy, particularly in addressing class
imbalance problems in classification tasks. However, few studies have explored
DA for the Arabic language, relying on traditional approaches such as
paraphrasing or noising-based techniques. In this paper, we propose a new
Arabic DA method that employs the recent powerful modeling technique, namely
the AraGPT-2, for the augmentation process. The generated sentences are
evaluated in terms of context, semantics, diversity, and novelty using the
Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT
transformer is used on sentiment classification tasks to evaluate the
classification performance of the augmented Arabic dataset. The experiments
were conducted on four sentiment Arabic datasets: AraSarcasm, ASTD, ATT, and
MOVIE. The selected datasets vary in size, label number, and unbalanced
classes. The results show that the proposed methodology enhanced the Arabic
sentiment text classification on all datasets with an increase in F1 score by
4% in AraSarcasm, 6% in ASTD, 9% in ATT, and 13% in MOVIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09582">
<div class="article-summary-box-inner">
<span><p>Understanding how language supports emotion inference remains a topic of
debate in emotion science. The present study investigated whether
language-derived emotion-concept knowledge would causally support emotion
inference by manipulating the language-specific knowledge representations in
large language models. Using the prompt technique, 14 attributes of emotion
concepts were found to be represented by distinct artificial neuron
populations. By manipulating these attribute-related neurons, the majority of
the emotion inference tasks showed performance deterioration compared to random
manipulations. The attribute-specific performance deterioration was related to
the importance of different attributes in human mental space. Our findings
provide causal evidence in support of a language-based mechanism for emotion
inference and highlight the contributions of emotion-concept knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.05063">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated
remarkable results in various natural language processing (NLP) tasks with
in-context learning, which involves inference based on a few demonstration
examples. Despite their successes in NLP tasks, no investigation has been
conducted to assess the ability of LLMs to perform document information
extraction (DIE) using in-context learning. Applying LLMs to DIE poses two
challenges: the modality and task gap. To this end, we propose a simple but
effective in-context learning framework called ICL-D3IE, which enables LLMs to
perform DIE with different types of demonstration examples. Specifically, we
extract the most difficult and distinct segments from hard training documents
as hard demonstrations for benefiting all test instances. We design
demonstrations describing relationships that enable LLMs to understand
positional relationships. We introduce formatting demonstrations for easy
answer extraction. Additionally, the framework improves diverse demonstrations
by updating them iteratively. Our experiments on three widely used benchmark
datasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to
achieve superior performance when compared to previous pre-trained methods
fine-tuned with full training in both the in-distribution (ID) setting and in
the out-of-distribution (OOD) setting. Code is available at
https://github.com/MAEHCM/ICL-D3IE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Composed Image Retrieval with Textual Inversion. (arXiv:2303.15247v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15247">
<div class="article-summary-box-inner">
<span><p>Composed Image Retrieval (CIR) aims to retrieve a target image based on a
query composed of a reference image and a relative caption that describes the
difference between the two images. The high effort and cost required for
labeling datasets for CIR hamper the widespread usage of existing methods, as
they rely on supervised learning. In this work, we propose a new task,
Zero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled
training dataset. Our approach, named zero-Shot composEd imAge Retrieval with
textuaL invErsion (SEARLE), maps the visual features of the reference image
into a pseudo-word token in CLIP token embedding space and integrates it with
the relative caption. To support research on ZS-CIR, we introduce an
open-domain benchmarking dataset named Composed Image Retrieval on Common
Objects in context (CIRCO), which is the first dataset for CIR containing
multiple ground truths for each query. The experiments show that SEARLE
exhibits better performance than the baselines on the two main datasets for CIR
tasks, FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and
the model are publicly available at https://github.com/miccunifi/SEARLE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09991">
<div class="article-summary-box-inner">
<span><p>Large language models are becoming increasingly pervasive and ubiquitous in
society via deployment in sociotechnical systems. Yet these language models, be
it for classification or generation, have been shown to be biased and behave
irresponsibly, causing harm to people at scale. It is crucial to audit these
language models rigorously. Existing auditing tools leverage either or both
humans and AI to find failures. In this work, we draw upon literature in
human-AI collaboration and sensemaking, and conduct interviews with research
experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro
and Lundberg, 2022), which is powered by a generative large language model
(LLM). Through the design process we highlight the importance of sensemaking
and human-AI communication to leverage complementary strengths of humans and
generative models in collaborative auditing. To evaluate the effectiveness of
the augmented tool, AdaTest++, we conduct user studies with participants
auditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment
analysis model. Qualitative analysis shows that AdaTest++ effectively leverages
human strengths such as schematization, hypothesis formation and testing.
Further, with our tool, participants identified a variety of failures modes,
covering 26 different topics over 2 tasks, that have been shown before in
formal audits and also those previously under-reported.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology. (arXiv:2304.11957v4 [physics.med-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.11957">
<div class="article-summary-box-inner">
<span><p>The potential of large language models in medicine for education and decision
making purposes has been demonstrated as they achieve decent scores on medical
exams such as the United States Medical Licensing Exam (USMLE) and the MedQA
exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized
field of radiation oncology using the 38th American College of Radiology (ACR)
radiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone
cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of
63.65% and 74.57%, respectively, highlighting the advantage of the latest
ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in
radiation oncology are identified to some extent. Specifically, ChatGPT-4
demonstrates better knowledge of statistics, CNS &amp; eye, pediatrics, biology,
and physics than knowledge of bone &amp; soft tissue and gynecology, as per the ACR
knowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in
diagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks
proficiency in in-depth details of clinical trials. For the Gray Zone cases,
ChatGPT-4 is able to suggest a personalized treatment approach to each case
with high correctness and comprehensiveness. Importantly, it provides novel
treatment aspects for many cases, which are not suggested by any human experts.
Both evaluations demonstrate the potential of ChatGPT-4 in medical education
for the general public and cancer patients, as well as the potential to aid
clinical decision-making, while acknowledging its limitations in certain
domains. Because of the risk of hallucination, facts provided by ChatGPT always
need to be verified.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.00948">
<div class="article-summary-box-inner">
<span><p>The performance of large language models (LLMs) has recently improved to the
point where the models can perform well on many language tasks. We show here
that for the first time, the models can also generate coherent and valid formal
analyses of linguistic data and illustrate the vast potential of large language
models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their
metalinguistic abilities improves our understanding of their general
capabilities and sheds new light on theoretical models in linguistics. In this
paper, we probe into GPT-4's metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a
research program for metalinguistic analyses of large language models, propose
experimental designs, provide general guidelines, discuss limitations, and
offer future directions for this line of research. This line of inquiry also
exemplifies behavioral interpretability of deep learning, where models'
representations are accessed by explicit prompting rather than internal
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v2 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09438">
<div class="article-summary-box-inner">
<span><p>Automatic source-to-source parallelization of serial code for shared and
distributed memory systems is a challenging task in high-performance computing.
While many attempts were made to translate serial code into parallel code for a
shared memory environment (usually using OpenMP), none has managed to do so for
a distributed memory environment. In this paper, we propose a novel approach,
called MPI-rical, for automated MPI code generation using a transformer-based
model trained on approximately 25,000 serial code snippets and their
corresponding parallelized MPI code out of more than 50,000 code snippets in
our corpus (MPICodeCorpus). To evaluate the performance of the model, we first
break down the serial code to MPI-based parallel code translation problem into
two sub-problems and develop two research objectives: code completion defined
as given a location in the source code, predict the MPI function for that
location, and code translation defined as predicting an MPI function as well as
its location in the source code. We evaluate MPI-rical on MPICodeCorpus dataset
and on real-world scientific code benchmarks and compare its performance
between the code completion and translation tasks. Our experimental results
show that while MPI-rical performs better on the code completion task than the
code translation task, the latter is better suited for real-world programming
assistance, in which the tool suggests the need for an MPI function regardless
of prior knowledge. Overall, our approach represents a significant step forward
in automating the parallelization of serial code for distributed memory
systems, which can save valuable time and resources for software developers and
researchers. The source code used in this work, as well as other relevant
sources, are available at:
https://github.com/Scientific-Computing-Lab-NRCN/MPI-rical
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Sensitivity on Speaker Names for Text Generation from Dialogues. (arXiv:2305.13833v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13833">
<div class="article-summary-box-inner">
<span><p>Changing speaker names consistently throughout a dialogue should not affect
its meaning and corresponding outputs for text generation from dialogues.
However, pre-trained language models, serving as the backbone for
dialogue-processing tasks, have shown to be sensitive to nuances. This may
result in unfairness in real-world applications. No comprehensive analysis of
this problem has been done in the past. In this work, we propose to
quantitatively measure a model's sensitivity on speaker names, and
comprehensively evaluate a number of known methods for reducing speaker name
sensitivity, including a novel approach of our own. Extensive experiments on
multiple datasets provide a benchmark for this problem and show the favorable
performance of our approach in sensitivity reduction and quality of generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Detection and Quantification of Selective Forces in Language Change. (arXiv:2305.15914v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15914">
<div class="article-summary-box-inner">
<span><p>Language change is a cultural evolutionary process in which variants of
linguistic variables change in frequency through processes analogous to
mutation, selection and genetic drift. In this work, we apply a
recently-introduced method to corpus data to quantify the strength of selection
in specific instances of historical language change. We first demonstrate, in
the context of English irregular verbs, that this method is more reliable and
interpretable than similar methods that have previously been applied. We
further extend this study to demonstrate that a bias towards phonological
simplicity overrides that favouring grammatical simplicity when these are in
conflict. Finally, with reference to Spanish spelling reforms, we show that the
method can also detect points in time at which selection strengths change, a
feature that is generically expected for socially-motivated language change.
Together, these results indicate how hypotheses for mechanisms of language
change can be tested quantitatively using historical corpus data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08107">
<div class="article-summary-box-inner">
<span><p>The fields of both Natural Language Processing (NLP) and Automated Machine
Learning (AutoML) have achieved remarkable results over the past years. In NLP,
especially Large Language Models (LLMs) have experienced a rapid series of
breakthroughs very recently. We envision that the two fields can radically push
the boundaries of each other through tight integration. To showcase this
vision, we explore the potential of a symbiotic relationship between AutoML and
LLMs, shedding light on how they can benefit each other. In particular, we
investigate both the opportunities to enhance AutoML approaches with LLMs from
different perspectives and the challenges of leveraging AutoML to further
improve LLMs. To this end, we survey existing work, and we critically assess
risks. We strongly believe that the integration of the two fields has the
potential to disrupt both fields, NLP and AutoML. By highlighting conceivable
synergies, but also risks, we aim to foster further exploration at the
intersection of AutoML and LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Domain Adaptation of Sentence Embeddings Using Adapters. (arXiv:2307.03104v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03104">
<div class="article-summary-box-inner">
<span><p>Sentence embeddings enable us to capture the semantic similarity of short
texts. Most sentence embedding models are trained for general semantic textual
similarity tasks. Therefore, to use sentence embeddings in a particular domain,
the model must be adapted to it in order to achieve good results. Usually, this
is done by fine-tuning the entire sentence embedding model for the domain of
interest. While this approach yields state-of-the-art results, all of the
model's weights are updated during fine-tuning, making this method
resource-intensive. Therefore, instead of fine-tuning entire sentence embedding
models for each target domain individually, we propose to train lightweight
adapters. These domain-specific adapters do not require fine-tuning all
underlying sentence embedding model parameters. Instead, we only train a small
number of additional parameters while keeping the weights of the underlying
sentence embedding model fixed. Training domain-specific adapters allows always
using the same base model and only exchanging the domain-specific adapters to
adapt sentence embeddings to a specific domain. We show that using adapters for
parameter-efficient domain adaptation of sentence embeddings yields competitive
performance within 1% of a domain-adapted, entirely fine-tuned sentence
embedding model while only training approximately 3.6% of the parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07870">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are often misleadingly recognized as having a
personality or a set of values. We argue that an LLM can be seen as a
superposition of perspectives with different values and personality traits.
LLMs exhibit context-dependent values and personality traits that change based
on the induced perspective (as opposed to humans, who tend to have more
coherent values and personality traits across contexts). We introduce the
concept of perspective controllability, which refers to a model's affordance to
adopt various perspectives with differing values and personality traits. In our
experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study
how exhibited values and personality traits change based on different
perspectives. Through qualitative experiments, we show that LLMs express
different values when those are (implicitly or explicitly) implied in the
prompt, and that LLMs express different values even when those are not
obviously implied (demonstrating their context-dependent nature). We then
conduct quantitative experiments to study the controllability of different
models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the
effectiveness of various methods for inducing perspectives, and the smoothness
of the models' drivability. We conclude by examining the broader implications
of our work and outline a variety of associated scientific questions. The
project website is available at
https://sites.google.com/view/llm-superpositions .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Guided Generation for Large Language Models. (arXiv:2307.09702v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.09702">
<div class="article-summary-box-inner">
<span><p>In this article we show how the problem of neural text generation can be
constructively reformulated in terms of transitions between the states of a
finite-state machine. This framework leads to an efficient approach to guiding
text generation with regular expressions and context-free grammars by allowing
the construction of an index over a language model's vocabulary. The approach
is model agnostic, allows one to enforce domain-specific knowledge and
constraints, and enables the construction of reliable interfaces by
guaranteeing the structure of the generated text. It adds little overhead to
the token sequence generation process and significantly outperforms existing
solutions. An implementation is provided in the open source Python library
Outlines
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10652">
<div class="article-summary-box-inner">
<span><p>As an efficient approach to understand, generate, and process natural
language texts, research in natural language processing (NLP) has exhibited a
rapid spread and wide adoption in recent years. Given the increasing research
work in this area, several NLP-related approaches have been surveyed in the
research community. However, a comprehensive study that categorizes established
topics, identifies trends, and outlines areas for future research remains
absent. Contributing to closing this gap, we have systematically classified and
analyzed research papers in the ACL Anthology. As a result, we present a
structured overview of the research landscape, provide a taxonomy of fields of
study in NLP, analyze recent developments in NLP, summarize our findings, and
highlight directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.12950">
<div class="article-summary-box-inner">
<span><p>We propose Reinforcement Learning from Contrast Distillation (RLCD), a method
for aligning language models to follow natural language principles without
using human feedback. RLCD trains a preference model using simulated preference
pairs that contain both a high-quality and low-quality example, generated using
contrasting positive and negative prompts. The preference model is then used to
improve a base unaligned language model via reinforcement learning.
Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context
distillation (Huang et al., 2022) baselines across three diverse alignment
tasks--harmlessness, helpfulness, and story outline generation--and on both 7B
and 30B model scales for preference data simulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoDo Learning: DOmain-DemOgraphic Transfer in Language Models for Detecting Abuse Targeted at Public Figures. (arXiv:2307.16811v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16811">
<div class="article-summary-box-inner">
<span><p>Public figures receive a disproportionate amount of abuse on social media,
impacting their active participation in public life. Automated systems can
identify abuse at scale but labelling training data is expensive, complex and
potentially harmful. So, it is desirable that systems are efficient and
generalisable, handling both shared and specific aspects of online abuse. We
explore the dynamics of cross-group text classification in order to understand
how well classifiers trained on one domain or demographic can transfer to
others, with a view to building more generalisable abuse classifiers. We
fine-tune language models to classify tweets targeted at public figures across
DOmains (sport and politics) and DemOgraphics (women and men) using our novel
DODO dataset, containing 28,000 labelled entries, split equally across four
domain-demographic pairs. We find that (i) small amounts of diverse data are
hugely beneficial to generalisation and model adaptation; (ii) models transfer
more easily across demographics but models trained on cross-domain data are
more generalisable; (iii) some groups contribute more to generalisability than
others; and (iv) dataset similarity is a signal of transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00158">
<div class="article-summary-box-inner">
<span><p>Translation Quality Estimation (TQE) is an essential step before deploying
the output translation into usage. TQE is also critical in assessing machine
translation (MT) and human translation (HT) quality without seeing the
reference translations. This work examines whether the state-of-the-art large
language models (LLMs) can be fine-tuned for the TQE task and their capability.
We take ChatGPT as one example and approach TQE as a binary classification
task. Using \textbf{eight language pairs} including English to Italian, German,
French, Japanese, Dutch, Portuguese, Turkish, and Chinese training corpora, our
experimental results show that fine-tuned ChatGPT via its API can achieve a
relatively high score on predicting translation quality, i.e. \textit{if the
translation needs to be edited}. However, there is definitely much space to
improve the model accuracy, e.g. they are 82.42\% and 83.69\% for
English-Italian and English-German respectively using our experimental
settings. English-Italiano bilingual Abstract is available in the paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. (arXiv:2308.00946v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00946">
<div class="article-summary-box-inner">
<span><p>We equip a smaller Language Model to generalise to answering challenging
compositional questions that have not been seen in training. To do so we
propose a combination of multitask supervised pretraining on up to 93 tasks
designed to instill diverse reasoning abilities, and a dense retrieval system
that aims to retrieve a set of evidential paragraph fragments. Recent progress
in question-answering has been achieved either through prompting methods
against very large pretrained Language Models in zero or few-shot fashion, or
by fine-tuning smaller models, sometimes in conjunction with information
retrieval. We focus on the less explored question of the extent to which
zero-shot generalisation can be enabled in smaller models with retrieval
against a corpus within which sufficient information to answer a particular
question may not exist. We establish strong baselines in this setting for
diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and
ARC-DA), and show that performance can be significantly improved by adding
retrieval-augmented training datasets which are designed to expose our models
to a variety of heuristic reasoning strategies such as weighing partial
evidence or ignoring an irrelevant context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02180">
<div class="article-summary-box-inner">
<span><p>Clinical trial matching is a key process in health delivery and discovery. In
practice, it is plagued by overwhelming unstructured data and unscalable manual
processing. In this paper, we conduct a systematic study on scaling clinical
trial matching using large language models (LLMs), with oncology as the focus
area. Our study is grounded in a clinical trial matching system currently in
test deployment at a large U.S. health network. Initial findings are promising:
out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate
eligibility criteria of clinical trials and extract complex matching logic
(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially
outperform prior strong baselines and may serve as a preliminary solution to
help triage patient-trial candidates with humans in the loop. Our study also
reveals a few significant growth areas for applying LLMs to end-to-end clinical
trial matching, such as context limitation and accuracy, especially in
structuring patient information from longitudinal medical records.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaniniQA: Enhancing Patient Education Through Interactive Question Answering. (arXiv:2308.03253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03253">
<div class="article-summary-box-inner">
<span><p>Patient portal allows discharged patients to access their personalized
discharge instructions in electronic health records (EHRs). However, many
patients have difficulty understanding or memorizing their discharge
instructions. In this paper, we present PaniniQA, a patient-centric interactive
question answering system designed to help patients understand their discharge
instructions. PaniniQA first identifies important clinical content from
patients' discharge instructions and then formulates patient-specific
educational questions. In addition, PaniniQA is also equipped with answer
verification functionality to provide timely feedback to correct patients'
misunderstandings. Our comprehensive automatic and human evaluation results
demonstrate our PaniniQA is capable of improving patients' mastery of their
medical instructions through effective interactions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04566">
<div class="article-summary-box-inner">
<span><p>Machine Reading Comprehension (MRC) models tend to take advantage of spurious
correlations (also known as dataset bias or annotation artifacts in the
research community). Consequently, these models may perform the MRC task
without fully comprehending the given context and question, which is
undesirable since it may result in low robustness against distribution shift.
This paper delves into the concept of answer-position bias, where a significant
percentage of training questions have answers located solely in the first
sentence of the context. We propose a Single-Sentence Reader as a new approach
for addressing answer position bias in MRC. We implement this approach using
six different models and thoroughly analyze their performance. Remarkably, our
proposed Single-Sentence Readers achieve results that nearly match those of
models trained on conventional training sets, proving their effectiveness. Our
study also discusses several challenges our Single-Sentence Readers encounter
and proposes a potential solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07134">
<div class="article-summary-box-inner">
<span><p>The emergence of large-scale pre-trained language models, such as ChatGPT,
has revolutionized various research fields in artificial intelligence.
Transformers-based large language models (LLMs) have gradually replaced CNNs
and RNNs to unify fields of computer vision and natural language processing.
Compared with the data that exists relatively independently such as images,
videos or texts, graph is a type of data that contains rich structural and
relational information. Meanwhile, natural language, as one of the most
expressive mediums, excels in describing complex structures. However, existing
work on incorporating graph learning problems into the generative language
modeling framework remains very limited. As the importance of large language
models continues to grow, it becomes essential to explore whether LLMs can also
replace GNNs as the foundation model for graphs. In this paper, we propose
InstructGLM (Instruction-finetuned Graph Language Model), systematically design
highly scalable prompts based on natural language instructions, and use natural
language to describe the geometric structure and node features of the graph for
instruction tuning an LLM to perform learning and inference on graphs in a
generative manner. Our method exceeds all competitive GNN baselines on
ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of
our method and sheds light on generative large language models as the
foundation model for graph machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08747">
<div class="article-summary-box-inner">
<span><p>Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning
when a model forgets previously learned information as it learns new
information. As large language models (LLMs) have shown excellent performance,
it is interesting to uncover whether CF exists in the continual fine-tuning of
LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs'
knowledge, from the perspectives of domain knowledge, reasoning, and reading
comprehension. The experiments demonstrate that catastrophic forgetting is
generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale
increases, the severity of forgetting also intensifies. Comparing the
decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers
less forgetting and maintains more knowledge. We also observe that LLMs can
mitigate language bias (e.g. gender bias) during continual fine-tuning.
Moreover, we find that ALPACA can maintain more knowledge and capacity compared
with LLAMA during the continual fine-tuning, which implies that general
instruction tuning can help mitigate the forgetting phenomenon of LLMs in the
further fine-tuning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08780">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) operates by showing language models (LMs) examples
of input-output pairs for a given task, i.e., demonstrations. The standard
approach for ICL is to prompt the LM with concatenated demonstrations followed
by the test input. This approach suffers from some issues. First, concatenation
offers almost no control over the contribution of each demo to the model
prediction. This can be sub-optimal when some demonstrations are irrelevant to
the test example. Second, due to the input length limit of some transformer
models, it might be infeasible to fit many examples into the context,
especially when dealing with long-input tasks. In this work, we explore
Demonstration Ensembling (DENSE) as an alternative to simple concatenation.
DENSE predicts outputs using subsets (i.e., buckets) of the demonstrations and
then combines the output probabilities resulting from each subset to produce
the final prediction. We study different ensembling methods using GPT-j and
experiment on 12 language tasks. Our experiments show weighted max ensembling
to outperform vanilla concatenation by as large as 2.4 average points. Code
available at https://github.com/mukhal/icl-ensembling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08998">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning from human feedback (RLHF) can improve the quality of
large language model's (LLM) outputs by aligning them with human preferences.
We propose a simple algorithm for aligning LLMs with human preferences inspired
by growing batch reinforcement learning (RL), which we call Reinforced
Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by
generating samples from the policy, which are then used to improve the LLM
policy using offline RL algorithms. ReST is more efficient than typical online
RLHF methods because the training dataset is produced offline, which allows
data reuse. While ReST is a general approach applicable to all generative
learning settings, we focus on its application to machine translation. Our
results show that ReST can substantially improve translation quality, as
measured by automated metrics and human evaluation on machine translation
benchmarks in a compute and sample-efficient manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification. (arXiv:2308.09308v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09308">
<div class="article-summary-box-inner">
<span><p>Retrieval augmentation, which enhances downstream models by a knowledge
retriever and an external corpus instead of by merely increasing the number of
model parameters, has been successfully applied to many natural language
processing (NLP) tasks such as text classification, question answering and so
on. However, existing methods that separately or asynchronously train the
retriever and downstream model mainly due to the non-differentiability between
the two parts, usually lead to degraded performance compared to end-to-end
joint training. In this paper, we propose Differentiable Retrieval Augmentation
via Generative lANguage modeling(Dragan), to address this problem by a novel
differentiable reformulation. We demonstrate the effectiveness of our proposed
method on a challenging NLP task in e-commerce search, namely query intent
classification. Both the experimental results and ablation study show that the
proposed method significantly and reasonably improves the state-of-the-art
baselines on both offline evaluation and online A/B test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning. (arXiv:2308.09658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09658">
<div class="article-summary-box-inner">
<span><p>There emerges a promising trend of using large language models (LLMs) to
generate code-like plans for complex inference tasks such as visual reasoning.
This paradigm, known as LLM-based planning, provides flexibility in problem
solving and endows better interpretability. However, current research is mostly
limited to basic scenarios of simple questions that can be straightforward
answered in a few inference steps. Planning for the more challenging multi-hop
visual reasoning tasks remains under-explored. Specifically, under multi-hop
reasoning situations, the trade-off between accuracy and the complexity of
plan-searching becomes prominent. The prevailing algorithms either address the
efficiency issue by employing the fast one-stop generation or adopt a complex
iterative generation method to improve accuracy. Both fail to balance the need
for efficiency and performance. Drawing inspiration from the dual system of
cognition in the human brain, the fast and the slow think processes, we propose
a hierarchical plan-searching algorithm that integrates the one-stop reasoning
(fast) and the Tree-of-thought (slow). Our approach succeeds in performance
while significantly saving inference steps. Moreover, we repurpose the PTR and
the CLEVER datasets, developing a systematic framework for evaluating the
performance and efficiency of LLMs-based plan-search algorithms under reasoning
tasks at different levels of difficulty. Extensive experiments demonstrate the
superiority of our proposed algorithm in terms of performance and efficiency.
The dataset and code will be release soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09687">
<div class="article-summary-box-inner">
<span><p>We introduce Graph of Thoughts (GoT): a framework that advances prompting
capabilities in large language models (LLMs) beyond those offered by paradigms
such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary
advantage of GoT is the ability to model the information generated by an LLM as
an arbitrary graph, where units of information ("LLM thoughts") are vertices,
and edges correspond to dependencies between these vertices. This approach
enables combining arbitrary LLM thoughts into synergistic outcomes, distilling
the essence of whole networks of thoughts, or enhancing thoughts using feedback
loops. We illustrate that GoT offers advantages over state of the art on
different tasks, for example increasing the quality of sorting by 62% over ToT,
while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible
with new thought transformations and thus can be used to spearhead new
prompting schemes. This work brings the LLM reasoning closer to human thinking
or brain mechanisms such as recurrence, both of which form complex networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v3 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00121">
<div class="article-summary-box-inner">
<span><p>The field of software security testing, more specifically penetration
testing, is an activity that requires high levels of expertise and involves
many manual testing and analysis steps. This paper explores the potential usage
of large-language models, such as GPT3.5, to augment penetration testers with
AI sparring partners. We explore the feasibility of supplementing penetration
testers with AI models for two distinct use cases: high-level task planning for
security testing assignments and low-level vulnerability hunting within a
vulnerable virtual machine. For the latter, we implemented a closed-feedback
loop between LLM-generated low-level actions with a vulnerable virtual machine
(connected through SSH) and allowed the LLM to analyze the machine state for
vulnerabilities and suggest concrete attack vectors which were automatically
executed within the virtual machine. We discuss promising initial results,
detail avenues for improvement, and close deliberating on the ethics of
providing AI-based sparring partners.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-22 23:10:38.097896497 UTC">2023-08-22 23:10:38 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>