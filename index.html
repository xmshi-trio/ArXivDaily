<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-13T01:30:00Z">09-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks. (arXiv:2309.05668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05668">
<div class="article-summary-box-inner">
<span><p>In recent times, significant advancements have been witnessed in the field of
language models, particularly with the emergence of Large Language Models
(LLMs) that are trained on vast amounts of data extracted from internet
archives. These LLMs, such as ChatGPT, have become widely accessible, allowing
users to generate text for various purposes including articles, essays, jokes,
and poetry. Given that LLMs are trained on a diverse range of text sources,
encompassing platforms like Reddit and Twitter, it is foreseeable that future
training datasets will also incorporate text generated by previous iterations
of the models themselves. In light of this development, our research aims to
investigate the influence of artificial text in the pre-training phase of
language models. Specifically, we conducted a comparative analysis between a
language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and
ChatGPT, which employed the same articles for its training and evaluated their
performance on three downstream tasks as well as their potential gender bias,
using sentiment analysis as a metric. Through a series of experiments, we
demonstrate that the utilization of artificial text during pre-training does
not have a significant impact on either the performance of the models in
downstream tasks or their gender bias. In conclusion, our findings suggest that
the inclusion of text generated by LLMs in their own pre-training process does
not yield substantial effects on the subsequent performance of the models in
downstream tasks or their potential gender bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model for Science: A Study on P vs. NP. (arXiv:2309.05689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05689">
<div class="article-summary-box-inner">
<span><p>In this work, we use large language models (LLMs) to augment and accelerate
research on the P versus NP problem, one of the most important open problems in
theoretical computer science and mathematics. Specifically, we propose Socratic
reasoning, a general framework that promotes in-depth thinking with LLMs for
complex problem-solving. Socratic reasoning encourages LLMs to recursively
discover, solve, and integrate problems while facilitating self-evaluation and
refinement. Our pilot study on the P vs. NP problem shows that GPT-4
successfully produces a proof schema and engages in rigorous reasoning
throughout 97 dialogue turns, concluding "P $\neq$ NP", which is in alignment
with (Xu and Zhou, 2023). The investigation uncovers novel insights within the
extensive solution space of LLMs, shedding light on LLM for Science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05804">
<div class="article-summary-box-inner">
<span><p>Over the past two decades, dialogue modeling has made significant strides,
moving from simple rule-based responses to personalized and persuasive response
generation. However, despite these advancements, the objective functions and
evaluation metrics for dialogue generation have remained stagnant, i.e.,
cross-entropy and BLEU, respectively. These lexical-based metrics have the
following key limitations: (a) word-to-word matching without semantic
consideration: It assigns the same credit for failure to generate 'nice' and
'rice' for 'good'. (b) missing context attribute for evaluating the generated
response: Even if a generated response is relevant to the ongoing dialogue
context, it may still be penalized for not matching the gold utterance provided
in the corpus. In this paper, we first investigate these limitations
comprehensively and propose a new loss function called Semantic Infused
Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we
formulate a new evaluation metric called Dialuation, which incorporates both
context relevance and semantic appropriateness while evaluating a generated
response. We conducted experiments on two benchmark dialogue corpora,
encompassing both task-oriented and open-domain scenarios. We found that the
dialogue generation model trained with SemTextualLogue loss attained superior
performance (in both quantitative and qualitative evaluation) compared to the
traditional cross-entropy loss function across the datasets and evaluation
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05833">
<div class="article-summary-box-inner">
<span><p>In recent years, the transition to cloud-based platforms in the IT sector has
emphasized the significance of cloud incident root cause analysis to ensure
service reliability and maintain customer trust. Central to this process is the
efficient determination of root causes, a task made challenging due to the
complex nature of contemporary cloud infrastructures. Despite the proliferation
of AI-driven tools for root cause identification, their applicability remains
limited by the inconsistent quality of their outputs. This paper introduces a
method for enhancing confidence estimation in root cause analysis tools by
prompting retrieval-augmented large language models (LLMs). This approach
operates in two phases. Initially, the model evaluates its confidence based on
historical incident data, considering its assessment of the evidence strength.
Subsequently, the model reviews the root cause generated by the predictor. An
optimization step then combines these evaluations to determine the final
confidence assignment. Experimental results illustrate that our method enables
the model to articulate its confidence effectively, providing a more calibrated
score. We address research questions evaluating the ability of our method to
produce calibrated confidence scores using LLMs, the impact of domain-specific
retrieved examples on confidence estimates, and its potential generalizability
across various root cause analysis models. Through this, we aim to bridge the
confidence estimation gap, aiding on-call engineers in decision-making and
bolstering the efficiency of cloud incident management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05918">
<div class="article-summary-box-inner">
<span><p>In our opinion the exuberance surrounding the relative success of data-driven
large language models (LLMs) is slightly misguided and for several reasons (i)
LLMs cannot be relied upon for factual information since for LLMs all ingested
text (factual or non-factual) was created equal; (ii) due to their subsymbolic
na-ture, whatever 'knowledge' these models acquire about language will always
be buried in billions of microfeatures (weights), none of which is meaningful
on its own; and (iii) LLMs will often fail to make the correct inferences in
several linguistic contexts (e.g., nominal compounds, copredication, quantifier
scope ambi-guities, intensional contexts. Since we believe the relative success
of data-driven large language models (LLMs) is not a reflection on the symbolic
vs. subsymbol-ic debate but a reflection on applying the successful strategy of
a bottom-up reverse engineering of language at scale, we suggest in this paper
applying the effective bottom-up strategy in a symbolic setting resulting in
symbolic, explainable, and ontologically grounded language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs. (arXiv:2309.05920v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05920">
<div class="article-summary-box-inner">
<span><p>We introduce SAGE; a Generative LLM for inferring attribute values for
products across world-wide e-Commerce catalogs. We introduce a novel
formulation of the attribute-value prediction problem as a Seq2Seq
summarization task, across languages, product types and target attributes. Our
novel modeling approach lifts the restriction of predicting attribute values
within a pre-specified set of choices, as well as, the requirement that the
sought attribute values need to be explicitly mentioned in the text. SAGE can
infer attribute values even when such values are mentioned implicitly using
periphrastic language, or not-at-all-as is the case for common-sense defaults.
Additionally, SAGE is capable of predicting whether an attribute is
inapplicable for the product at hand, or non-obtainable from the available
information. SAGE is the first method able to tackle all aspects of the
attribute-value-prediction task as they arise in practical settings in
e-Commerce catalogs. A comprehensive set of experiments demonstrates the
effectiveness of the proposed approach, as well as, its superiority against
state-of-the-art competing alternatives. Moreover, our experiments highlight
SAGE's ability to tackle the task of predicting attribute values in zero-shot
setting; thereby, opening up opportunities for significantly reducing the
overall number of labeled examples required for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Hallucination in Large Foundation Models. (arXiv:2309.05922v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05922">
<div class="article-summary-box-inner">
<span><p>Hallucination in a foundation model (FM) refers to the generation of content
that strays from factual reality or includes fabricated information. This
survey paper provides an extensive overview of recent efforts that aim to
identify, elucidate, and tackle the problem of hallucination, with a particular
focus on ``Large'' Foundation Models (LFMs). The paper classifies various types
of hallucination phenomena that are specific to LFMs and establishes evaluation
criteria for assessing the extent of hallucination. It also examines existing
strategies for mitigating hallucination in LFMs and discusses potential
directions for future research in this area. Essentially, the paper offers a
comprehensive examination of the challenges and solutions related to
hallucination in LFMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do PLMs Know and Understand Ontological Knowledge?. (arXiv:2309.05936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05936">
<div class="article-summary-box-inner">
<span><p>Ontological knowledge, which comprises classes and properties and their
relationships, is integral to world knowledge. It is significant to explore
whether Pretrained Language Models (PLMs) know and understand such knowledge.
However, existing PLM-probing studies focus mainly on factual knowledge,
lacking a systematic probing of ontological knowledge. In this paper, we focus
on probing whether PLMs store ontological knowledge and have a semantic
understanding of the knowledge rather than rote memorization of the surface
form. To probe whether PLMs know ontological knowledge, we investigate how well
PLMs memorize: (1) types of entities; (2) hierarchical relationships among
classes and properties, e.g., Person is a subclass of Animal and Member of
Sports Team is a subproperty of Member of ; (3) domain and range constraints of
properties, e.g., the subject of Member of Sports Team should be a Person and
the object should be a Sports Team. To further probe whether PLMs truly
understand ontological knowledge beyond memorization, we comprehensively study
whether they can reliably perform logical reasoning with given knowledge
according to ontological entailment rules. Our probing results show that PLMs
can memorize certain ontological knowledge and utilize implicit knowledge in
reasoning. However, both the memorizing and reasoning performances are less
than perfect, indicating incomplete knowledge and understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05938">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new task in the field of Answering Subjective Induction
Question on Products (SUBJPQA). The answer to this kind of question is
non-unique, but can be interpreted from many perspectives. For example, the
answer to 'whether the phone is heavy' has a variety of different viewpoints. A
satisfied answer should be able to summarize these subjective opinions from
multiple sources and provide objective knowledge, such as the weight of a
phone. That is quite different from the traditional QA task, in which the
answer to a factoid question is unique and can be found from a single data
source. To address this new task, we propose a three-steps method. We first
retrieve all answer-related clues from multiple knowledge sources on facts and
opinions. The implicit commonsense facts are also collected to supplement the
necessary but missing contexts. We then capture their relevance with the
questions by interactive attention. Next, we design a reinforcement-based
summarizer to aggregate all these knowledgeable clues. Based on a
template-controlled decoder, we can output a comprehensive and
multi-perspective answer. Due to the lack of a relevant evaluated benchmark set
for the new task, we construct a large-scale dataset, named SupQA, consisting
of 48,352 samples across 15 product domains. Evaluation results show the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05950">
<div class="article-summary-box-inner">
<span><p>Vision-language models (VLMs) pre-trained on web-scale datasets have
demonstrated remarkable capabilities across a variety of vision and multimodal
tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box
setting, requiring access to model parameters for backpropagation. However,
many VLMs rely on proprietary data and are not open-source, which restricts the
use of white-box approaches for fine-tuning. Given that popular private large
language models (LLMs) like ChatGPT still offer a language-based user
interface, we aim to develop a novel fine-tuning approach for VLMs through
natural language prompts, thereby avoiding the need to access model parameters,
feature embeddings, or output logits. In this setup, we propose employing
chat-based LLMs as black-box optimizers to search for the best text prompt on
the illustrative task of few-shot image classification using CLIP.
Specifically, we adopt an automatic "hill-climbing" procedure that converges on
an effective prompt by evaluating the accuracy of current prompts and asking
LLMs to refine them based on textual feedback, all within a conversational
process without human-in-the-loop. In a challenging 1-shot learning setup, our
simple approach surpasses the white-box continuous prompting method CoOp by an
average of 1.5% across 11 datasets including ImageNet. Our approach also
outperforms OpenAI's manually crafted prompts and is more efficient than other
black-box methods like iterative APE. Additionally, we highlight the advantage
of conversational feedback incorporating both positive and negative prompts,
suggesting that LLMs can utilize the implicit "gradient" direction in textual
feedback for a more efficient search. Lastly, we find that the text prompts
generated through our strategy are not only more interpretable but also
transfer well across different CLIP architectures in a black-box manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced and Explainable Social Media Analysis for Public Health with Large Language Models. (arXiv:2309.05951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05951">
<div class="article-summary-box-inner">
<span><p>As social media becomes increasingly popular, more and more public health
activities emerge, which is worth noting for pandemic monitoring and government
decision-making. Current techniques for public health analysis involve popular
models such as BERT and large language models (LLMs). Although recent progress
in LLMs has shown a strong ability to comprehend knowledge by being fine-tuned
on specific domain datasets, the costs of training an in-domain LLM for every
specific public health task are especially expensive. Furthermore, such kinds
of in-domain datasets from social media are generally highly imbalanced, which
will hinder the efficiency of LLMs tuning. To tackle these challenges, the data
imbalance issue can be overcome by sophisticated data augmentation methods for
social media datasets. In addition, the ability of the LLMs can be effectively
utilised by prompting the model properly. In light of the above discussion, in
this paper, a novel ALEX framework is proposed for social media analysis on
public health. Specifically, an augmentation pipeline is developed to resolve
the data imbalance issue. Furthermore, an LLMs explanation mechanism is
proposed by prompting an LLM with the predicted results from BERT models.
Extensive experiments conducted on three tasks at the Social Media Mining for
Health 2023 (SMM4H) competition with the first ranking in two tasks demonstrate
the superior performance of the proposed ALEX method. Our code has been
released in https://github.com/YanJiangJerry/ALEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Moral Machine Experiment on Large Language Models. (arXiv:2309.05958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05958">
<div class="article-summary-box-inner">
<span><p>As large language models (LLMs) become more deeply integrated into various
sectors, understanding how they make moral judgments has become crucial,
particularly in the realm of autonomous driving. This study utilized the Moral
Machine framework to investigate the ethical decision-making tendencies of
prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their
responses to human preferences. While LLMs' and humans' preferences such as
prioritizing humans over pets and favoring saving more lives are broadly
aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.
Additionally, despite the qualitative similarities between the LLM and human
preferences, there are significant quantitative disparities, suggesting that
LLMs might lean toward more uncompromising decisions, compared to the milder
inclinations of humans. These insights elucidate the ethical frameworks of LLMs
and their potential implications for autonomous driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05961">
<div class="article-summary-box-inner">
<span><p>Community Question Answering (CQA) platforms steadily gain popularity as they
provide users with fast responses to their queries. The swiftness of these
responses is contingent on a mixture of query-specific and user-related
elements. This paper scrutinizes these contributing factors within the context
of six highly popular CQA platforms, identified through their standout
answering speed. Our investigation reveals a correlation between the time taken
to yield the first response to a question and several variables: the metadata,
the formulation of the questions, and the level of interaction among users.
Additionally, by employing conventional machine learning models to analyze
these metadata and patterns of user interaction, we endeavor to predict which
queries will receive their initial responses promptly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05973">
<div class="article-summary-box-inner">
<span><p>Language models often exhibit behaviors that improve performance on a
pre-training objective but harm performance on downstream tasks. We propose a
novel approach to removing undesirable behaviors by ablating a small number of
causal pathways between model components, with the intention of disabling the
computational circuit responsible for the bad behavior. Given a small dataset
of inputs where the model behaves poorly, we learn to ablate a small number of
important causal pathways. In the setting of reducing GPT-2 toxic language
generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic
generation with minimal degradation of performance on other inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content Reduction, Surprisal and Information Density Estimation for Long Documents. (arXiv:2309.06009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06009">
<div class="article-summary-box-inner">
<span><p>Many computational linguistic methods have been proposed to study the
information content of languages. We consider two interesting research
questions: 1) how is information distributed over long documents, and 2) how
does content reduction, such as token selection and text summarization, affect
the information density in long documents. We present four criteria for
information density estimation for long documents, including surprisal,
entropy, uniform information density, and lexical density. Among those
criteria, the first three adopt the measures from information theory. We
propose an attention-based word selection method for clinical notes and study
machine summarization for multiple-domain documents. Our findings reveal the
systematic difference in information density of long text in various domains.
Empirical results on automated medical coding from long clinical notes show the
effectiveness of the attention-based word selection method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06054">
<div class="article-summary-box-inner">
<span><p>In-context learning, i.e., learning from in-context samples, is an impressive
ability of Transformer. However, the mechanism driving the in-context learning
is not yet fully understood. In this study, we aim to investigate from an
underexplored perspective of representation learning. The representation is
more complex for in-context learning senario, where the representation can be
impacted by both model weights and in-context samples. We refer the above two
conceptually aspects of representation as in-weight component and in-context
component, respectively. To study how the two components affect in-context
learning capabilities, we construct a novel synthetic task, making it possible
to device two probes, in-weights probe and in-context probe, to evaluate the
two components, respectively. We demonstrate that the goodness of in-context
component is highly related to the in-context learning performance, which
indicates the entanglement between in-context learning and representation
learning. Furthermore, we find that a good in-weights component can actually
benefit the learning of the in-context component, indicating that in-weights
learning should be the foundation of in-context learning. To further understand
the the in-context learning mechanism and importance of the in-weights
component, we proof by construction that a simple Transformer, which uses
pattern matching and copy-past mechanism to perform in-context learning, can
match the in-context learning performance with more complex, best tuned
Transformer under the perfect in-weights component assumption. In short, those
discoveries from representation learning perspective shed light on new
approaches to improve the in-context capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair. (arXiv:2309.06057v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06057">
<div class="article-summary-box-inner">
<span><p>Automatic program repair (APR) is crucial to reduce manual debugging efforts
for developers and improve software reliability. While conventional
search-based techniques typically rely on heuristic rules or a redundancy
assumption to mine fix patterns, recent years have witnessed the surge of deep
learning (DL) based approaches to automate the program repair process in a
data-driven manner. However, their performance is often limited by a fixed set
of parameters to model the highly complex search space of APR. To ease such
burden on the parametric models, in this work, we propose a novel
Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly
leveraging relevant fix patterns retrieved from a codebase of previous bug-fix
pairs. Specifically, we build a hybrid patch retriever to account for both
lexical and semantic matching based on the raw source code in a
language-agnostic manner, which does not rely on any code-specific features. In
addition, we adapt a code-aware language model CodeT5 as our foundation model
to facilitate both patch retrieval and generation tasks in a unified manner. We
adopt a stage-wise approach where the patch retriever first retrieves a
relevant external bug-fix pair to augment the buggy input for the CodeT5 patch
generator, which synthesizes a ranked list of repair patch candidates. Notably,
RAP-Gen is a generic APR framework that can flexibly integrate different patch
retrievers and generators to repair various types of bugs. We thoroughly
evaluate RAP-Gen on three benchmarks in two programming languages, including
the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks
in Java, where the bug localization information may or may not be provided.
Experimental results show that RAP-Gen significantly outperforms previous
state-of-the-art approaches on all benchmarks, e.g., repairing 15 more bugs on
818 Defects4J bugs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models. (arXiv:2309.06085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06085">
<div class="article-summary-box-inner">
<span><p>The rapid development of Large Language Models (LLMs) and the emergence of
novel abilities with scale have necessitated the construction of holistic,
diverse and challenging benchmarks such as HELM and BIG-bench. However, at the
moment, most of these benchmarks focus only on performance in English and
evaluations that include Southeast Asian (SEA) languages are few in number. We
therefore propose BHASA, a holistic linguistic and cultural evaluation suite
for LLMs in SEA languages. It comprises three components: (1) a NLP benchmark
covering eight tasks across Natural Language Understanding (NLU), Generation
(NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit
that spans the gamut of linguistic phenomena including syntax, semantics and
pragmatics, and (3) a cultural diagnostics dataset that probes for both
cultural representation and sensitivity. For this preliminary effort, we
implement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil,
and we only include Indonesian and Tamil for LINDSEA and the cultural
diagnostics dataset. As GPT-4 is purportedly one of the best-performing
multilingual LLMs at the moment, we use it as a yardstick to gauge the
capabilities of LLMs in the context of SEA languages. Our initial experiments
on GPT-4 with BHASA find it lacking in various aspects of linguistic
capabilities, cultural representation and sensitivity in the targeted SEA
languages. BHASA is a work in progress and will continue to be improved and
expanded in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06089">
<div class="article-summary-box-inner">
<span><p>The cross-lingual transfer is a promising technique to solve tasks in
less-resourced languages. In this empirical study, we compare two fine-tuning
approaches combined with zero-shot and full-shot learning approaches for large
language models in a cross-lingual setting. As fine-tuning strategies, we
compare parameter-efficient adapter methods with fine-tuning of all parameters.
As cross-lingual transfer strategies, we compare the intermediate-training
(\textit{IT}) that uses each language sequentially and cross-lingual validation
(\textit{CLV}) that uses a target language already in the validation phase of
fine-tuning. We assess the success of transfer and the extent of catastrophic
forgetting in a source language due to cross-lingual transfer, i.e., how much
previously acquired knowledge is lost when we learn new information in a
different language. The results on two different classification problems, hate
speech detection and product reviews, each containing datasets in several
languages, show that the \textit{IT} cross-lingual strategy outperforms
\textit{CLV} for the target language. Our findings indicate that, in the
majority of cases, the \textit{CLV} strategy demonstrates superior retention of
knowledge in the base language (English) compared to the \textit{IT} strategy,
when evaluating catastrophic forgetting in multiple cross-lingual transfers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual Taxonomy Expansion. (arXiv:2309.06105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06105">
<div class="article-summary-box-inner">
<span><p>Taxonomy expansion task is essential in organizing the ever-increasing volume
of new concepts into existing taxonomies. Most existing methods focus
exclusively on using textual semantics, leading to an inability to generalize
to unseen terms and the "Prototypical Hypernym Problem." In this paper, we
propose Visual Taxonomy Expansion (VTE), introducing visual features into the
taxonomy expansion task. We propose a textual hypernymy learning task and a
visual prototype learning task to cluster textual and visual semantics. In
addition to the tasks on respective modalities, we introduce a hyper-proto
constraint that integrates textual and visual semantics to produce fine-grained
visual semantics. Our method is evaluated on two datasets, where we obtain
compelling results. Specifically, on the Chinese taxonomy dataset, our method
significantly improves accuracy by 8.75 %. Additionally, our approach performs
better than ChatGPT on the Chinese taxonomy dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Latent Perspectives of Media Houses Towards Public Figures. (arXiv:2309.06112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06112">
<div class="article-summary-box-inner">
<span><p>Media houses reporting on public figures, often come with their own biases
stemming from their respective worldviews. A characterization of these
underlying patterns helps us in better understanding and interpreting news
stories. For this, we need diverse or subjective summarizations, which may not
be amenable for classifying into predefined class labels. This work proposes a
zero-shot approach for non-extractive or generative characterizations of person
entities from a corpus using GPT-2. We use well-articulated articles from
several well-known news media houses as a corpus to build a sound argument for
this approach. First, we fine-tune a GPT-2 pre-trained language model with a
corpus where specific person entities are characterized. Second, we further
fine-tune this with demonstrations of person entity characterizations, created
from a corpus of programmatically constructed characterizations. This twice
fine-tuned model is primed with manual prompts consisting of entity names that
were not previously encountered in the second fine-tuning, to generate a simple
sentence about the entity. The results were encouraging, when compared against
actual characterizations from the corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06126">
<div class="article-summary-box-inner">
<span><p>Large language models excel in many human-language tasks but often falter in
highly specialized domains like scholarly astronomy. To bridge this gap, we
introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using
over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal
language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2,
showing marked domain adaptation. Our model generates more insightful and
scientifically relevant text completions and embedding extraction than
state-of-the-arts foundation models despite having significantly fewer
parameters. AstroLLaMA serves as a robust, domain-specific model with broad
fine-tuning potential. Its public release aims to spur astronomy-focused
research, including automatic paper summarization and conversational agent
development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection. (arXiv:2309.06131v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06131">
<div class="article-summary-box-inner">
<span><p>Search methods based on Pretrained Language Models (PLM) have demonstrated
great effectiveness gains compared to statistical and early neural ranking
models. However, fine-tuning PLM-based rankers requires a great amount of
annotated training data. Annotating data involves a large manual effort and
thus is expensive, especially in domain specific tasks. In this paper we
investigate fine-tuning PLM-based rankers under limited training data and
budget. We investigate two scenarios: fine-tuning a ranker from scratch, and
domain adaptation starting with a ranker already fine-tuned on general data,
and continuing fine-tuning on a target dataset. We observe a great variability
in effectiveness when fine-tuning on different randomly selected subsets of
training data. This suggests that it is possible to achieve effectiveness gains
by actively selecting a subset of the training data that has the most positive
effect on the rankers. This way, it would be possible to fine-tune effective
PLM rankers at a reduced annotation budget. To investigate this, we adapt
existing Active Learning (AL) strategies to the task of fine-tuning PLM rankers
and investigate their effectiveness, also considering annotation and
computational costs. Our extensive analysis shows that AL strategies do not
significantly outperform random selection of training subsets in terms of
effectiveness. We further find that gains provided by AL strategies come at the
expense of more assessments (thus higher annotation costs) and AL strategies
underperform random selection when comparing effectiveness given a fixed
annotation cost. Our results highlight that ``optimal'' subsets of training
data that provide high effectiveness at low annotation cost do exist, but
current mainstream AL strategies applied to PLM rankers are not capable of
identifying them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06132">
<div class="article-summary-box-inner">
<span><p>We present a hybrid approach to the automated measurement of vagueness and
subjectivity in texts. We first introduce the expert system VAGO, we illustrate
it on a small benchmark of fact vs. opinion sentences, and then test it on the
larger French press corpus FreSaDa to confirm the higher prevalence of
subjective markers in satirical vs. regular texts. We then build a neural clone
of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores
obtained on FreSaDa. Using explainability tools (LIME), we show the interest of
this neural version for the enrichment of the lexicons of the symbolic version,
and for the production of versions in other languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. (arXiv:2309.06135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06135">
<div class="article-summary-box-inner">
<span><p>Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown
remarkable ability in high-quality content generation, and become one of the
representatives for the recent wave of transformative AI. Nevertheless, such
advance comes with an intensifying concern about the misuse of this generative
technology, especially for producing copyrighted or NSFW (i.e. not safe for
work) images. Although efforts have been made to filter inappropriate
images/prompts or remove undesirable concepts/styles via model fine-tuning, the
reliability of these safety mechanisms against diversified problematic prompts
remains largely unexplored. In this work, we propose Prompting4Debugging (P4D)
as a debugging and red-teaming tool that automatically finds problematic
prompts for diffusion models to test the reliability of a deployed safety
mechanism. We demonstrate the efficacy of our P4D tool in uncovering new
vulnerabilities of SD models with safety mechanisms. Particularly, our result
shows that around half of prompts in existing safe prompting benchmarks which
were originally considered "safe" can actually be manipulated to bypass many
deployed safety mechanisms, including concept removal, negative prompt, and
safety guidance. Our findings suggest that, without comprehensive testing, the
evaluations on limited safe prompting benchmarks can lead to a false sense of
safety for text-to-image models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis. (arXiv:2309.06163v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06163">
<div class="article-summary-box-inner">
<span><p>We present the first shared task for detecting and analyzing code-switching
in Guarani and Spanish, GUA-SPA at IberLEF 2023. The challenge consisted of
three tasks: identifying the language of a token, NER, and a novel task of
classifying the way a Spanish span is used in the code-switched context. We
annotated a corpus of 1500 texts extracted from news articles and tweets,
around 25 thousand tokens, with the information for the tasks. Three teams took
part in the evaluation phase, obtaining in general good results for Task 1, and
more mixed results for Tasks 2 and 3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking. (arXiv:2309.06175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06175">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach to address the Entity Recognition and
Linking Challenge at NLPCC 2015. The task involves extracting named entity
mentions from short search queries and linking them to entities within a
reference Chinese knowledge base. To tackle this problem, we first expand the
existing knowledge base and utilize external knowledge to identify candidate
entities, thereby improving the recall rate. Next, we extract features from the
candidate entities and utilize Support Vector Regression and Multiple Additive
Regression Tree as scoring functions to filter the results. Additionally, we
apply rules to further refine the results and enhance precision. Our method is
computationally efficient and achieves an F1 score of 0.535.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Glancing Future for Simultaneous Machine Translation. (arXiv:2309.06179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06179">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) outputs translation while reading the
source sentence. Unlike conventional sequence-to-sequence (seq2seq) training,
existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training,
where the model predicts target tokens based on partial source tokens. However,
the prefix2prefix training diminishes the ability of the model to capture
global information and introduces forced predictions due to the absence of
essential source information. Consequently, it is crucial to bridge the gap
between the prefix2prefix training and seq2seq training to enhance the
translation capability of the SiMT model. In this paper, we propose a novel
method that glances future in curriculum learning to achieve the transition
from the seq2seq training to prefix2prefix training. Specifically, we gradually
reduce the available source information from the whole sentence to the prefix
corresponding to that latency. Our method is applicable to a wide range of SiMT
methods and experiments demonstrate that our method outperforms strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains. (arXiv:2309.06192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06192">
<div class="article-summary-box-inner">
<span><p>News recommender systems play an increasingly influential role in shaping
information access within democratic societies. However, tailoring
recommendations to users' specific interests can result in the divergence of
information streams. Fragmented access to information poses challenges to the
integrity of the public sphere, thereby influencing democracy and public
discourse. The Fragmentation metric quantifies the degree of fragmentation of
information streams in news recommendations. Accurate measurement of this
metric requires the application of Natural Language Processing (NLP) to
identify distinct news events, stories, or timelines. This paper presents an
extensive investigation of various approaches for quantifying Fragmentation in
news recommendations. These approaches are evaluated both intrinsically, by
measuring performance on news story clustering, and extrinsically, by assessing
the Fragmentation scores of different simulated news recommender scenarios. Our
findings demonstrate that agglomerative hierarchical clustering coupled with
SentenceBERT text representation is substantially better at detecting
Fragmentation than earlier implementations. Additionally, the analysis of
simulated scenarios yields valuable insights and recommendations for
stakeholders concerning the measurement and interpretation of Fragmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06219">
<div class="article-summary-box-inner">
<span><p>We introduce the task of automatic human action co-occurrence identification,
i.e., determine whether two human actions can co-occur in the same interval of
time. We create and make publicly available the ACE (Action Co-occurrencE)
dataset, consisting of a large graph of ~12k co-occurring pairs of visual
actions and their corresponding video clips. We describe graph link prediction
models that leverage visual and textual information to automatically infer if
two actions are co-occurring. We show that graphs are particularly well suited
to capture relations between human actions, and the learned graph
representations are effective for our task and capture novel and relevant
information across different data domains. The ACE dataset and the code
introduced in this paper are publicly available at
https://github.com/MichiganNLP/vlog_action_co-occurrence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. (arXiv:2309.06236v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06236">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable generalization
across diverse tasks, leading individuals to increasingly use them as personal
assistants and universal computing engines. Nevertheless, a notable obstacle
emerges when feeding numerical/temporal data into these models, such as data
sourced from wearables or electronic health records. LLMs employ tokenizers in
their input that break down text into smaller units. However, tokenizers are
not designed to represent numerical values and might struggle to understand
repetitive patterns and context, treating consecutive values as separate tokens
and disregarding their temporal relationships. Here, we discuss recent works
that employ LLMs for human-centric tasks such as in mobile health sensing and
present a case study showing that popular LLMs tokenize temporal data
incorrectly. To address that, we highlight potential solutions such as prompt
tuning with lightweight embedding layers as well as multimodal adapters, that
can help bridge this "modality gap". While the capability of language models to
generalize to other modalities with minimal or no finetuning is exciting, this
paper underscores the fact that their outputs cannot be meaningful if they
stumble over input nuances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06275">
<div class="article-summary-box-inner">
<span><p>Reasoning presents a significant and challenging issue for Large Language
Models (LLMs). The predominant focus of research has revolved around developing
diverse prompting strategies to guide and structure the reasoning processes of
LLMs. However, these approaches based on decoder-only causal language models
often operate the input question in a single forward pass, potentially missing
the rich, back-and-forth interactions inherent in human reasoning. Scant
attention has been paid to a critical dimension, i.e., the input question
itself embedded within the prompts. In response, we introduce a deceptively
simple yet highly effective prompting strategy, termed question "re-reading".
Drawing inspiration from human learning and problem-solving, re-reading entails
revisiting the question information embedded within input prompts. This
approach aligns seamlessly with the cognitive principle of reinforcement,
enabling LLMs to extract deeper insights, identify intricate patterns,
establish more nuanced connections, and ultimately enhance their reasoning
capabilities across various tasks. Experiments conducted on a series of
reasoning benchmarks serve to underscore the effectiveness and generality of
our method. Moreover, our findings demonstrate that our approach seamlessly
integrates with various language models, though-eliciting prompting methods,
and ensemble techniques, further underscoring its versatility and compatibility
in the realm of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06358">
<div class="article-summary-box-inner">
<span><p>Robustness in Natural Language Processing continues to be a pertinent issue,
where state of the art models under-perform under naturally shifted
distributions. In the context of Question Answering, work on domain adaptation
methods continues to be a growing body of research. However, very little
attention has been given to the notion of domain generalization under natural
distribution shifts, where the target domain is unknown. With drastic
improvements in the quality and access to generative models, we answer the
question: How do generated datasets influence the performance of QA models
under natural distribution shifts? We perform experiments on 4 different
datasets under varying amounts of distribution shift, and analyze how
"in-the-wild" generation can help achieve domain generalization. We take a
two-step generation approach, generating both contexts and QA pairs to augment
existing datasets. Through our experiments, we demonstrate how augmenting
reading comprehension datasets with generated data leads to better robustness
towards natural distribution shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Predict Concept Ordering for Common Sense Generation. (arXiv:2309.06363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06363">
<div class="article-summary-box-inner">
<span><p>Prior work has shown that the ordering in which concepts are shown to a
commonsense generator plays an important role, affecting the quality of the
generated sentence. However, it remains a challenge to determine the optimal
ordering of a given set of concepts such that a natural sentence covering all
the concepts could be generated from a pretrained generator. To understand the
relationship between the ordering of the input concepts and the quality of the
generated sentences, we conduct a systematic study considering multiple
language models (LMs) and concept ordering strategies. We find that BART-large
model consistently outperforms all other LMs considered in this study when
fine-tuned using the ordering of concepts as they appear in CommonGen training
data as measured using multiple evaluation metrics. Moreover, the larger
GPT3-based large language models (LLMs) variants do not necessarily outperform
much smaller LMs on this task, even when fine-tuned on task-specific training
data. Interestingly, human annotators significantly reorder input concept sets
when manually writing sentences covering those concepts, and this ordering
provides the best sentence generations independently of the LM used for the
generation, outperforming a probabilistic concept ordering baseline
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06364">
<div class="article-summary-box-inner">
<span><p>Today, using Large-scale generative Language Models (LLMs) it is possible to
simulate free responses to interview questions like those traditionally
analyzed using qualitative research methods. Qualitative methodology
encompasses a broad family of techniques involving manual analysis of
open-ended interviews or conversations conducted freely in natural language.
Here we consider whether artificial "silicon participants" generated by LLMs
may be productively studied using qualitative methods aiming to produce
insights that could generalize to real human populations. The key concept in
our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023)
capturing the degree to which LLM-generated outputs mirror human
sub-populations' beliefs and attitudes. By definition, high algorithmic
fidelity suggests latent beliefs elicited from LLMs may generalize to real
humans, whereas low algorithmic fidelity renders such research invalid. Here we
used an LLM to generate interviews with silicon participants matching specific
demographic characteristics one-for-one with a set of human participants. Using
framework-based qualitative analysis, we showed the key themes obtained from
both human and silicon participants were strikingly similar. However, when we
analyzed the structure and tone of the interviews we found even more striking
differences. We also found evidence of the hyper-accuracy distortion described
by Aher et al. (2023). We conclude that the LLM we tested (GPT-3.5) does not
have sufficient algorithmic fidelity to expect research on it to generalize to
human populations. However, the rapid pace of LLM research makes it plausible
this could change in the future. Thus we stress the need to establish epistemic
norms now around how to assess validity of LLM-based qualitative research,
especially concerning the need to ensure representation of heterogeneous lived
experiences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cited Text Spans for Citation Text Generation. (arXiv:2309.06365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06365">
<div class="article-summary-box-inner">
<span><p>Automatic related work generation must ground their outputs to the content of
the cited papers to avoid non-factual hallucinations, but due to the length of
scientific documents, existing abstractive approaches have conditioned only on
the cited paper \textit{abstracts}. We demonstrate that the abstract is not
always the most appropriate input for citation generation and that models
trained in this way learn to hallucinate. We propose to condition instead on
the \textit{cited text span} (CTS) as an alternative to the abstract. Because
manual CTS annotation is extremely time- and labor-intensive, we experiment
with automatic, ROUGE-based labeling of candidate CTS sentences, achieving
sufficiently strong performance to substitute for expensive human annotations,
and we propose a human-in-the-loop, keyword-based CTS retrieval approach that
makes generating citation texts grounded in the full text of cited papers both
promising and practical.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems. (arXiv:2309.06384v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06384">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have emerged as versatile tools in various daily
applications. However, they are fraught with issues that undermine their
utility and trustworthiness. These include the incorporation of erroneous
references (citation), the generation of hallucinated information
(correctness), and the inclusion of superfluous or omission of crucial details
(fluency). To ameliorate these concerns, this study makes several key
contributions. First, we build a dataset to train a critic model capable of
evaluating the citation, correctness, and fluency of responses generated by
LLMs in QA systems. Second, we propose an automated feedback mechanism that
leverages the critic model to offer real-time feedback on heterogeneous aspects
of generated text. Third, we introduce a feedback learning loop that uses this
critic model to iteratively improve the performance of the LLM responsible for
response generation. Experimental results demonstrate the efficacy of our
approach, showing substantial improvements in citation and fluency metrics for
ChatGPT, including a 4% precision increase in citation and an approximately 8%
enhancement in the MAUVE metric for fluency, while maintaining high levels of
correctness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06415">
<div class="article-summary-box-inner">
<span><p>This paper conducts a robustness audit of the safety feedback of PaLM 2
through a novel toxicity rabbit hole framework introduced here. Starting with a
stereotype, the framework instructs PaLM 2 to generate more toxic content than
the stereotype. Every subsequent iteration it continues instructing PaLM 2 to
generate more toxic content than the previous iteration until PaLM 2 safety
guardrails throw a safety violation. Our experiments uncover highly disturbing
antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few)
generated content that PaLM 2 safety guardrails do not evaluate as highly
unsafe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiology-Llama2: Best-in-Class Large Language Model for Radiology. (arXiv:2309.06419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.06419">
<div class="article-summary-box-inner">
<span><p>This paper introduces Radiology-Llama2, a large language model specialized
for radiology through a process known as instruction tuning. Radiology-Llama2
is based on the Llama2 architecture and further trained on a large dataset of
radiology reports to generate coherent and clinically useful impressions from
radiological findings. Quantitative evaluations using ROUGE metrics on the
MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves
state-of-the-art performance compared to other generative language models, with
a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional
assessments by radiology experts highlight the model's strengths in
understandability, coherence, relevance, conciseness, and clinical utility. The
work illustrates the potential of localized language models designed and tuned
for specialized domains like radiology. When properly evaluated and deployed,
such models can transform fields like radiology by automating rote tasks and
enhancing human expertise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08501">
<div class="article-summary-box-inner">
<span><p>Implicit knowledge, such as common sense, is key to fluid human
conversations. Current neural response generation (RG) models are trained to
generate responses directly, omitting unstated implicit knowledge. In this
paper, we present Think-Before-Speaking (TBS), a generative approach to first
externalize implicit commonsense knowledge (think) and use this knowledge to
generate responses (speak). We expect that externalizing implicit knowledge
allows more efficient learning, produces more informative responses, and
enables more explainable models. We analyze different choices to collect
knowledge-aligned dialogues, represent implicit knowledge, and transition
between knowledge and dialogues. Empirical results show TBS models outperform
end-to-end and knowledge-augmented RG baselines on most automatic metrics and
generate more informative, specific, and commonsense-following responses, as
evaluated by human annotators. TBS also generates knowledge that makes sense
and is relevant to the dialogue around 85\% of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03592">
<div class="article-summary-box-inner">
<span><p>Neural network language models can serve as computational hypotheses about
how humans process language. We compared the model-human consistency of diverse
language models using a novel experimental approach: controversial sentence
pairs. For each controversial sentence pair, two language models disagree about
which sentence is more likely to occur in natural text. Considering nine
language models (including n-gram, recurrent neural networks, and transformer
models), we created hundreds of such controversial sentence pairs by either
selecting sentences from a corpus or synthetically optimizing sentence pairs to
be highly controversial. Human subjects then provided judgments indicating for
each pair which of the two sentences is more likely. Controversial sentence
pairs proved highly effective at revealing model failures and identifying
models that aligned most closely with human judgments. The most
human-consistent model tested was GPT-2, although experiments also revealed
significant shortcomings of its alignment with human perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Select from Multiple Options. (arXiv:2212.00301v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00301">
<div class="article-summary-box-inner">
<span><p>Many NLP tasks can be regarded as a selection problem from a set of options,
such as classification tasks, multi-choice question answering, etc. Textual
entailment (TE) has been shown as the state-of-the-art (SOTA) approach to
dealing with those selection problems. TE treats input texts as premises (P),
options as hypotheses (H), then handles the selection problem by modeling (P,
H) pairwise. Two limitations: first, the pairwise modeling is unaware of other
options, which is less intuitive since humans often determine the best options
by comparing competing candidates; second, the inference process of pairwise TE
is time-consuming, especially when the option space is large. To deal with the
two issues, this work first proposes a contextualized TE model (Context-TE) by
appending other k options as the context of the current (P, H) modeling.
Context-TE is able to learn more reliable decision for the H since it considers
various context. Second, we speed up Context-TE by coming up with Parallel-TE,
which learns the decisions of multiple options simultaneously. Parallel-TE
significantly improves the inference speed while keeping comparable performance
with Context-TE. Our methods are evaluated on three tasks (ultra-fine entity
typing, intent detection and multi-choice QA) that are typical selection
problems with different sizes of options. Experiments show our models set new
SOTA performance; particularly, Parallel-TE is faster than the pairwise TE by k
times in inference. Our code is publicly available at
https://github.com/jiangshdd/LearningToSelect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04634">
<div class="article-summary-box-inner">
<span><p>Storytelling and narrative are fundamental to human experience, intertwined
with our social and cultural engagement. As such, researchers have long
attempted to create systems that can generate stories automatically. In recent
years, powered by deep learning and massive data resources, automatic story
generation has shown significant advances. However, considerable challenges,
like the need for global coherence in generated stories, still hamper
generative models from reaching the same storytelling ability as human
narrators. To tackle these challenges, many studies seek to inject structured
knowledge into the generation process, which is referred to as structured
knowledge-enhanced story generation. Incorporating external knowledge can
enhance the logical coherence among story events, achieve better knowledge
grounding, and alleviate over-generalization and repetition problems in
stories. This survey provides the latest and comprehensive review of this
research field: (i) we present a systematic taxonomy regarding how existing
methods integrate structured knowledge into story generation; (ii) we summarize
involved story corpora, structured knowledge datasets, and evaluation metrics;
(iii) we give multidimensional insights into the challenges of
knowledge-enhanced story generation and cast light on promising directions for
future study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07919">
<div class="article-summary-box-inner">
<span><p>Large language models show improved downstream task performance when prompted
to generate step-by-step reasoning to justify their final answers. These
reasoning steps greatly improve model interpretability and verification, but
objectively studying their correctness (independent of the final answer) is
difficult without reliable methods for automatic evaluation. We simply do not
know how often the stated reasoning steps actually support the final end task
predictions. In this work, we present ROSCOE, a suite of interpretable,
unsupervised automatic scores that improve and extend previous text generation
evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a
typology of reasoning errors and collect synthetic and human evaluation scores
on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE
can measure semantic consistency, logicality, informativeness, fluency, and
factuality - among other traits - by leveraging properties of step-by-step
rationales. We empirically verify the strength of our metrics on five human
annotated and six programmatically perturbed diagnostics datasets - covering a
diverse set of tasks that require reasoning skills and show that ROSCOE can
consistently outperform baseline metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13592">
<div class="article-summary-box-inner">
<span><p>While code-mixing is a common linguistic practice in many parts of the world,
collecting high-quality and low-cost code-mixed data remains a challenge for
natural language processing (NLP) research. The recent proliferation of Large
Language Models (LLMs) compels one to ask: how capable are these systems in
generating code-mixed data? In this paper, we explore prompting multilingual
LLMs in a zero-shot manner to generate code-mixed data for seven languages in
South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,
Tamil, and Singlish. We find that publicly available multilingual
instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of
producing texts with phrases or clauses from different languages. ChatGPT
exhibits inconsistent capabilities in generating code-mixed texts, wherein its
performance varies depending on the prompt template and language pairing. For
instance, ChatGPT generates fluent and natural Singlish texts (an English-based
creole spoken in Singapore), but for English-Tamil language pair, the system
mostly produces grammatically incorrect or semantically meaningless utterances.
Furthermore, it may erroneously introduce languages not specified in the
prompt. Based on our investigation, existing multilingual LLMs exhibit a wide
range of proficiency in code-mixed data generation for SEA languages. As such,
we advise against using LLMs in this context without extensive human checks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. (arXiv:2304.05406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05406">
<div class="article-summary-box-inner">
<span><p>We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large
language model to engage in meaningful interactions with Astronomy papers using
in-context prompting. To optimize for efficiency, we employ a distillation
technique that effectively reduces the size of the original input paper by
50\%, while maintaining the paragraph structure and overall semantic integrity.
We then explore the model's responses using a multi-document context (ten
distilled documents). Our findings indicate that GPT-4 excels in the
multi-document domain, providing detailed answers contextualized within the
framework of related research findings. Our results showcase the potential of
large language models for the astronomical community, offering a promising
avenue for further exploration, particularly the possibility of utilizing the
models for hypothesis generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaLM 2 Technical Report. (arXiv:2305.10403v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10403">
<div class="article-summary-box-inner">
<span><p>We introduce PaLM 2, a new state-of-the-art language model that has better
multilingual and reasoning capabilities and is more compute-efficient than its
predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture
of objectives. Through extensive evaluations on English and multilingual
language, and reasoning tasks, we demonstrate that PaLM 2 has significantly
improved quality on downstream tasks across different model sizes, while
simultaneously exhibiting faster and more efficient inference compared to PaLM.
This improved efficiency enables broader deployment while also allowing the
model to respond faster, for a more natural pace of interaction. PaLM 2
demonstrates robust reasoning capabilities exemplified by large improvements
over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable
performance on a suite of responsible AI evaluations, and enables
inference-time control over toxicity without additional overhead or impact on
other capabilities. Overall, PaLM 2 achieves state-of-the-art performance
across a diverse set of tasks and capabilities.
</p>
<p>When discussing the PaLM 2 family, it is important to distinguish between
pre-trained models (of various sizes), fine-tuned variants of these models, and
the user-facing products that use these models. In particular, user-facing
products typically include additional pre- and post-processing steps.
Additionally, the underlying models may evolve over time. Therefore, one should
not expect the performance of user-facing products to exactly match the results
reported in this report.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10652">
<div class="article-summary-box-inner">
<span><p>The current monaural state of the art tools for speech separation relies on
supervised learning. This means that they must deal with permutation problem,
they are impacted by the mismatch on the number of speakers used in training
and inference. Moreover, their performance heavily relies on the presence of
high-quality labelled data. These problems can be effectively addressed by
employing a fully unsupervised technique for speech separation. In this paper,
we use contrastive learning to establish the representations of frames then use
the learned representations in the downstream deep modularization task.
Concretely, we demonstrate experimentally that in speech separation, different
frames of a speaker can be viewed as augmentations of a given hidden standard
frame of that speaker. The frames of a speaker contain enough prosodic
information overlap which is key in speech separation. Based on this, we
implement a self-supervised learning to learn to minimize the distance between
frames belonging to a given speaker. The learned representations are used in a
downstream deep modularization task to cluster frames based on speaker
identity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix
shows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively
in WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7
respectively in WSJ0-2mix. Its greatest strength being that as the number of
speakers increase, its performance does not degrade significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ModuleFormer: Modularity Emerges from Mixture-of-Experts. (arXiv:2306.04640v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04640">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved remarkable results. However,
existing models are expensive to train and deploy, and it is also difficult to
expand their knowledge beyond pre-training data without forgetting previous
knowledge. This paper proposes a new neural network architecture, ModuleFormer,
that leverages modularity to improve the efficiency and flexibility of large
language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).
Unlike the previous SMoE-based modular language model, which requires
domain-labeled data to learn domain-specific experts, ModuleFormer can induce
modularity from uncurated data with its new load balancing and concentration
losses. ModuleFormer is a modular architecture that includes two different
types of modules: new stick-breaking attention heads and feedforward experts.
Different modules are sparsely activated conditions on the input token during
training and inference. In our experiment, we found that the modular
architecture enables three important abilities for large pre-trained language
models: 1) Efficiency, since ModuleFormer only activates a subset of its
modules for each input token, thus it could achieve the same performance as
dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer
is more immune to catastrophic forgetting than dense LLMs and can be easily
extended with new modules to learn new knowledge that is not included in the
training data; 3) Specialisation, finetuning ModuleFormer could specialize a
subset of modules to the finetuning task and the task-unrelated modules could
be easily pruned for a lightweight deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources. (arXiv:2306.08753v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08753">
<div class="article-summary-box-inner">
<span><p>Multilingual Automatic Speech Recognition (ASR) models are capable of
transcribing audios across multiple languages, eliminating the need for
separate models. In addition, they can perform Language Identification (LID)
and handle code-switched speech. However, training these models requires
special code-switch and multilingual speech corpora which are sparsely
available. In this paper, we evaluate different approaches towards training of
bilingual as well as code-switched ASR models using purely monolingual data
sources. We introduce the concept of aggregate tokenizers that differs from the
current prevalent technique of generating LIDs at the boundaries of monolingual
samples and produces LID for each emitted token instead. We compare bilingual
and monolingual model performance, showcase the efficacy of aggregate
tokenizers, present a synthetic code-switched ASR data generation technique and
demonstrate the effectiveness of the proposed code-switched ASR models for the
tasks of speech recognition and spoken language identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00264">
<div class="article-summary-box-inner">
<span><p>In this work we investigate the optimal selection and fusion of features
across multiple modalities and combine these in a neural network to improve
emotion detection. We compare different fusion methods and examine the impact
of multi-loss training within the multi-modality fusion network, identifying
useful findings relating to subnet performance. Our best model achieves
state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and
CH-SIMS), and outperforms the other methods in most metrics. We have found that
training on multimodal features improves single modality testing and designing
fusion methods based on dataset annotation schema enhances model performance.
These results suggest a roadmap towards an optimized feature selection and
fusion approach for enhancing emotion detection in neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01936">
<div class="article-summary-box-inner">
<span><p>A hallmark of intelligence is the ability to use a familiar domain to make
inferences about a less familiar domain, known as analogical reasoning. In this
article, we delve into the performance of Large Language Models (LLMs) in
dealing with progressively complex analogies expressed in unstructured text. We
discuss analogies at four distinct levels of complexity: lexical analogies,
syntactic analogies, semantic analogies, and pragmatic analogies. As the
analogies become more complex, they require increasingly extensive, diverse
knowledge beyond the textual content, unlikely to be found in the lexical
co-occurrence statistics that power LLMs. To address this, we discuss the
necessity of employing Neuro-symbolic AI techniques that combine statistical
and symbolic AI, informing the representation of unstructured text to highlight
and augment relevant content, provide abstraction and guide the mapping
process. Our knowledge-informed approach maintains the efficiency of LLMs while
preserving the ability to explain analogies for pedagogical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FonMTL: Towards Multitask Learning for the Fon Language. (arXiv:2308.14280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.14280">
<div class="article-summary-box-inner">
<span><p>The Fon language, spoken by an average 2 million of people, is a truly
low-resourced African language, with a limited online presence, and existing
datasets (just to name but a few). Multitask learning is a learning paradigm
that aims to improve the generalization capacity of a model by sharing
knowledge across different but related tasks: this could be prevalent in very
data-scarce scenarios. In this paper, we present the first explorative approach
to multitask learning, for model capabilities enhancement in Natural Language
Processing for the Fon language. Specifically, we explore the tasks of Named
Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage
two language model heads as encoders to build shared representations for the
inputs, and we use linear layers blocks for classification relative to each
task. Our results on the NER and POS tasks for Fon, show competitive (or
better) performances compared to several multilingual pretrained language
models finetuned on single tasks. Additionally, we perform a few ablation
studies to leverage the efficiency of two different loss combination strategies
and find out that the equal loss weighting approach works best in our case. Our
code is open-sourced at https://github.com/bonaventuredossou/multitask_fon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaSM: Large Language and Speech Model. (arXiv:2308.15930v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15930">
<div class="article-summary-box-inner">
<span><p>Multi-modal large language models have garnered significant interest
recently. Though, most of the works focus on vision-language multi-modal models
providing strong capabilities in following vision-and-language instructions.
However, we claim that speech is also an important modality through which
humans interact with the world. Hence, it is crucial for a general-purpose
assistant to be able to follow multi-modal speech-and-language instructions. In
this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an
end-to-end trained large multi-modal speech-language model with cross-modal
conversational abilities, capable of following speech-and-language
instructions. Our early experiments show that LLaSM demonstrates a more
convenient and natural way for humans to interact with artificial intelligence.
Specifically, we also release a large Speech Instruction Following dataset
LLaSM-Audio-Instructions. Code and demo are available at
https://github.com/LinkSoul-AI/LLaSM and
https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions
dataset is available at
https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations. (arXiv:2308.16349v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16349">
<div class="article-summary-box-inner">
<span><p>We introduce Affective Visual Dialog, an emotion explanation and reasoning
task as a testbed for research on understanding the formation of emotions in
visually grounded conversations. The task involves three skills: (1)
Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3)
Affective emotion explanation generation based on the dialog. Our key
contribution is the collection of a large-scale dataset, dubbed AffectVisDial,
consisting of 50K 10-turn visually grounded dialogs as well as concluding
emotion attributions and dialog-informed textual emotion explanations,
resulting in a total of 27,180 working hours. We explain our design decisions
in collecting the dataset and introduce the questioner and answerer tasks that
are associated with the participants in the conversation. We train and
demonstrate solid Affective Visual Dialog baselines adapted from
state-of-the-art models. Remarkably, the responses generated by our models show
promising emotional reasoning abilities in response to visually grounded
conversations. Our project page is available at
https://affective-visual-dialog.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02654">
<div class="article-summary-box-inner">
<span><p>The prevalent use of large language models (LLMs) in various domains has
drawn attention to the issue of "hallucination," which refers to instances
where LLMs generate factually inaccurate or ungrounded information. Existing
techniques for hallucination detection in language assistants rely on intricate
fuzzy, specific free-language-based chain of thought (CoT) techniques or
parameter-based methods that suffer from interpretability issues. Additionally,
the methods that identify hallucinations post-generation could not prevent
their occurrence and suffer from inconsistent performance due to the influence
of the instruction format and model style. In this paper, we introduce a novel
pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which
focuses on evaluating the model's familiarity with the concepts present in the
input instruction and withholding the generation of response in case of
unfamiliar concepts. This approach emulates the human ability to refrain from
responding to unfamiliar topics, thus reducing hallucinations. We validate
SELF-FAMILIARITY across four different large language models, demonstrating
consistently superior performance compared to existing techniques. Our findings
propose a significant shift towards preemptive strategies for hallucination
mitigation in LLM assistants, promising improvements in reliability,
applicability, and interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03241">
<div class="article-summary-box-inner">
<span><p>Previous studies have typically assumed that large language models are unable
to accurately perform arithmetic operations, particularly multiplication of &gt;8
digits, and operations involving decimals and fractions, without the use of
calculator tools. This paper aims to challenge this misconception. With
sufficient training data, a 2 billion-parameter language model can accurately
perform multi-digit arithmetic operations with almost 100% accuracy without
data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication
accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from
GLM-10B on a dataset with additional multi-step arithmetic operations and math
problems described in text, achieves similar performance to GPT-4 on a
5,000-samples Chinese math problem test set. Our code and data are public at
https://github.com/THUDM/MathGLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoDia: A New Dataset for Romanian Dialect Identification from Speech. (arXiv:2309.03378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03378">
<div class="article-summary-box-inner">
<span><p>Dialect identification is a critical task in speech processing and language
technology, enhancing various applications such as speech recognition, speaker
verification, and many others. While most research studies have been dedicated
to dialect identification in widely spoken languages, limited attention has
been given to dialect identification in low-resource languages, such as
Romanian. To address this research gap, we introduce RoDia, the first dataset
for Romanian dialect identification from speech. The RoDia dataset includes a
varied compilation of speech samples from five distinct regions of Romania,
covering both urban and rural environments, totaling 2 hours of manually
annotated speech data. Along with our dataset, we introduce a set of
competitive models to be used as baselines for future research. The top scoring
model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%,
indicating that the task is challenging. We thus believe that RoDia is a
valuable resource that will stimulate research aiming to address the challenges
of Romanian dialect identification. We publicly release our dataset and code at
https://github.com/codrut2/RoDia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.03905">
<div class="article-summary-box-inner">
<span><p>We present ImageBind-LLM, a multi-modality instruction tuning method of large
language models (LLMs) via ImageBind. Existing works mainly focus on language
and image instruction tuning, different from which, our ImageBind-LLM can
respond to multi-modality conditions, including audio, 3D point clouds, video,
and their embedding-space arithmetic by only image-text alignment training.
During training, we adopt a learnable bind network to align the embedding space
between LLaMA and ImageBind's image encoder. Then, the image features
transformed by the bind network are added to word tokens of all layers in
LLaMA, which progressively injects visual instructions via an attention-free
and zero-initialized gating mechanism. Aided by the joint embedding of
ImageBind, the simple image-text training enables our model to exhibit superior
multi-modality instruction-following capabilities. During inference, the
multi-modality inputs are fed into the corresponding ImageBind encoders, and
processed by a proposed visual cache model for further cross-modal embedding
enhancement. The training-free cache model retrieves from three million image
features extracted by ImageBind, which effectively mitigates the
training-inference modality discrepancy. Notably, with our approach,
ImageBind-LLM can respond to instructions of diverse modalities and demonstrate
significant language generation quality. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04198">
<div class="article-summary-box-inner">
<span><p>The application of Large Language Models (LLMs) to the medical domain has
stimulated the interest of researchers. Recent studies have focused on
constructing Instruction Fine-Tuning (IFT) data through medical knowledge
graphs to enrich the interactive medical knowledge of LLMs. However, the
medical literature serving as a rich source of medical knowledge remains
unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive
knowledge acquisition from Chinese medical literature. It assesses the
proficiency of LLMs in mastering medical knowledge through a free-dialogue
fact-checking task. We identify a phenomenon called the ``fact-following
response``, where LLMs tend to affirm facts mentioned in questions and display
a reluctance to challenge them. To eliminate the inaccurate evaluation caused
by this phenomenon, for the golden fact, we artificially construct test data
from two perspectives: one consistent with the fact and one inconsistent with
the fact. Drawing from the probing experiment on the CALLA dataset, we conclude
that IFT data highly correlated with the medical literature corpus serves as a
potent catalyst for LLMs, enabling themselves to skillfully employ the medical
knowledge acquired during the pre-training phase within interactive scenarios,
enhancing accuracy. Furthermore, we design a framework for automatically
constructing IFT data based on medical literature and discuss some real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04213">
<div class="article-summary-box-inner">
<span><p>As social media becomes increasingly popular, more and more activities
related to public health emerge. Current techniques for public health analysis
involve popular models such as BERT and large language models (LLMs). However,
the costs of training in-domain LLMs for public health are especially
expensive. Furthermore, such kinds of in-domain datasets from social media are
generally imbalanced. To tackle these challenges, the data imbalance issue can
be overcome by data augmentation and balanced training. Moreover, the ability
of the LLMs can be effectively utilized by prompting the model properly. In
this paper, a novel ALEX framework is proposed to improve the performance of
public health analysis on social media by adopting an LLMs explanation
mechanism. Results show that our ALEX model got the best performance among all
submissions in both Task 2 and Task 4 with a high score in Task 1 in Social
Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://
github.com/YanJiangJerry/ALEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04663">
<div class="article-summary-box-inner">
<span><p>Learning paradigms for large language models (LLMs) currently tend to fall
within either in-context learning (ICL) or full fine-tuning. Each of these
comes with their own trade-offs based on available data, model size, compute
cost, ease-of-use, and final quality with neither solution performing well
across-the-board. In this article, we first describe ICL and fine-tuning
paradigms in a way that highlights their natural connections. Based on these
connections, we propose a new learning paradigm called FIAT that fuses the best
of these paradigms together, enabling prompt-engineered instructions and
chain-of-thought reasoning with the very largest models while also using
similar methods to perform parameter updates on a modestly-sized LLM with
parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of
multilingual tasks and observe that FIAT performs better than both ICL and
fine-tuning at scales ranging from 100-10,000 training examples. We hope that
FIAT provides a practical way of harnessing the full potential of LLMs without
needing to make a hard choice between learning paradigms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04842">
<div class="article-summary-box-inner">
<span><p>While large language models excel in a variety of natural language processing
(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they
must either rely on off-the-shelf automatic speech recognition (ASR) systems
for transcription, or be equipped with an in-built speech modality. This work
focuses on the former scenario, where LLM's accuracy on SLU tasks is
constrained by the accuracy of a fixed ASR system on the spoken input.
Specifically, we tackle speech-intent classification task, where a high
word-error-rate can limit the LLM's ability to understand the spoken intent.
Instead of chasing a high accuracy by designing complex or specialized
architectures regardless of deployment costs, we seek to answer how far we can
go without substantially changing the underlying ASR and LLM, which can
potentially be shared by multiple unrelated tasks. To this end, we propose
prompting the LLM with an n-best list of ASR hypotheses instead of only the
error-prone 1-best hypothesis. We explore prompt-engineering to explain the
concept of n-best lists to the LLM; followed by the finetuning of Low-Rank
Adapters on the downstream tasks. Our approach using n-best lists proves to be
effective on a device-directed speech detection task as well as on a keyword
spotting task, where systems using n-best list prompts outperform those using
1-best ASR hypothesis; thus paving the way for an efficient method to exploit
ASR uncertainty via LLMs for speech-based applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04951">
<div class="article-summary-box-inner">
<span><p>This paper is aimed at evaluating state-of-the-art models for Multi-document
Summarization (MDS) on different types of datasets in various domains and
investigating the limitations of existing models to determine future research
directions. To address this gap, we conducted an extensive literature review to
identify state-of-the-art models and datasets. We analyzed the performance of
PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed
unique challenges due to their varied domains. Our findings show that the
General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the
MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the
identified models on different datasets. Our study provides valuable insights
into the models' strengths and weaknesses, as well as their applicability in
different domains. This work serves as a reference for future MDS research and
contributes to the development of accurate and robust models which can be
utilized on demanding datasets with academically and/or scientifically complex
data as well as generalized, relatively simple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.05557">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) can respond to human language queries and have
shown powerful potential applications in network operations (NetOps). Thanks to
the large amount of commonsense knowledge inherent, LLMs achieve much better
inference accuracy than traditional models and emerge with strong abilities in
generalization, reasoning, and code generation. These abilities may have a
crucial boost to automated and intelligent NetOps. However, it remains
under-explored how well LLMs perform in various NetOps tasks. In this work, we
make a systematic assessment of the capabilities, strengths, and limitations of
selected LLMs in the field of NetOps. The evaluation is conducted on a
collection of 5,732 questions about NetOps, encompassing 26 publicly available
general-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune
some of these LLMs with our collected NetOps corpus and evaluate the resulting
models. The evaluation method follows the widely adopted benchmarks for
general-domain LLMs, combined with Chain-of-Thought Prompts and
Retrieval-Augmented Generation. The results show that only GPT-4 achieves high
accuracy equivalent to passing the NetOps certification exam for humans, while
all the other LLMs have much lower accuracy. However, some open models like
LLaMA 2 still demonstrate significant potential. Furthermore, we evaluate the
impact of factors such as model parameters, prompt engineering, instruction
fine-tuning etc. This work shall be treated as the initial effort to systematic
evaluation of LLMs in NetOps, and a more rigorous study is required for
production use. The evaluation code and dataset will be released to benefit
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rewriting the Script: Adapting Text Instructions for Voice Interaction. (arXiv:2306.09992v1 [cs.HC] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09992">
<div class="article-summary-box-inner">
<span><p>Voice assistants have sharply risen in popularity in recent years, but their
use has been limited mostly to simple applications like music, hands-free
search, or control of internet-of-things devices. What would it take for voice
assistants to guide people through more complex tasks? In our work, we study
the limitations of the dominant approach voice assistants take to complex task
guidance: reading aloud written instructions. Using recipes as an example, we
observe twelve participants cook at home with a state-of-the-art voice
assistant. We learn that the current approach leads to nine challenges,
including obscuring the bigger picture, overwhelming users with too much
information, and failing to communicate affordances. Instructions delivered by
a voice assistant are especially difficult because they cannot be skimmed as
easily as written instructions. Alexa in particular did not surface crucial
details to the user or answer questions well. We draw on our observations to
propose eight ways in which voice assistants can ``rewrite the script'' --
summarizing, signposting, splitting, elaborating, volunteering, reordering,
redistributing, and visualizing -- to transform written sources into forms that
are readily communicated through spoken conversation. We conclude with a vision
of how modern advancements in natural language processing can be leveraged for
intelligent agents to guide users effectively through complex tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-13 23:10:57.540434126 UTC">2023-09-13 23:10:57 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>