<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-23T01:30:00Z">08-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10959">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Docprompt for document question answering tasks
with powerful zero-shot and few-shot performance. We proposed a novel weakly
supervised data generation method, a novel multl-stage training method and a
novel understanding model &amp; generation model ensemble method. Experiment
results show that the Docprompt model after continue pretrain significantly
outperforms the existing strong baseline models on document question answering
tasks. This method greatly improves the delivery efficiency and model
performance of document question answering customer projects, reducing
annotation costs and labor costs. Our demo can be found at
https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT. (arXiv:2308.11001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11001">
<div class="article-summary-box-inner">
<span><p>The groundbreaking invention of ChatGPT has triggered enormous discussion
among users across all fields and domains. Among celebration around its various
advantages, questions have been raised with regards to its correctness and
ethics of its use. Efforts are already underway towards capturing user
sentiments around it. But it begs the question as to how the research community
is analyzing ChatGPT with regards to various aspects of its usage. It is this
sentiment of the researchers that we analyze in our work. Since Aspect-Based
Sentiment Analysis has usually only been applied on a few datasets, it gives
limited success and that too only on short text data. We propose a methodology
that uses Explainable AI to facilitate such analysis on research data. Our
technique presents valuable insights into extending the state of the art of
Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not
hampered by the length of the text data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using language models in the implicit automated assessment of mathematical short answer items. (arXiv:2308.11006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11006">
<div class="article-summary-box-inner">
<span><p>We propose a new way to assess certain short constructed responses to
mathematics items. Our approach uses a pipeline that identifies the key values
specified by the student in their response. This allows us to determine the
correctness of the response, as well as identify any misconceptions. The
information from the value identification pipeline can then be used to provide
feedback to the teacher and student. The value identification pipeline consists
of two fine-tuned language models. The first model determines if a value is
implicit in the student response. The second model identifies where in the
response the key value is specified. We consider both a generic model that can
be used for any prompt and value, as well as models that are specific to each
prompt and value. The value identification pipeline is a more accurate and
informative way to assess short constructed responses than traditional
rubric-based scoring. It can be used to provide more targeted feedback to
students, which can help them improve their understanding of mathematics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors. (arXiv:2308.11020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11020">
<div class="article-summary-box-inner">
<span><p>This paper tackles the challenging task of evaluating socially situated
conversational robots and presents a novel objective evaluation approach that
relies on multimodal user behaviors. In this study, our main focus is on
assessing the human-likeness of the robot as the primary evaluation metric.
While previous research often relied on subjective evaluations from users, our
approach aims to evaluate the robot's human-likeness based on observable user
behaviors indirectly, thus enhancing objectivity and reproducibility. To begin,
we created an annotated dataset of human-likeness scores, utilizing user
behaviors found in an attentive listening dialogue corpus. We then conducted an
analysis to determine the correlation between multimodal user behaviors and
human-likeness scores, demonstrating the feasibility of our proposed
behavior-based evaluation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11103">
<div class="article-summary-box-inner">
<span><p>Anonymity of both natural and legal persons in court rulings is a critical
aspect of privacy protection in the European Union and Switzerland. With the
advent of LLMs, concerns about large-scale re-identification of anonymized
persons are growing. In accordance with the Federal Supreme Court of
Switzerland, we explore the potential of LLMs to re-identify individuals in
court rulings by constructing a proof-of-concept using actual legal data from
the Swiss federal supreme court. Following the initial experiment, we
constructed an anonymized Wikipedia dataset as a more rigorous testing ground
to further investigate the findings. With the introduction and application of
the new task of re-identifying people in texts, we also introduce new metrics
to measure performance. We systematically analyze the factors that influence
successful re-identifications, identifying model size, input length, and
instruction tuning among the most critical determinants. Despite high
re-identification rates on Wikipedia, even the best LLMs struggled with court
decisions. The complexity is attributed to the lack of test datasets, the
necessity for substantial training resources, and data sparsity in the
information used for re-identification. In conclusion, this study demonstrates
that re-identification using LLMs may not be feasible for now, but as the
proof-of-concept on Wikipedia showed, it might become possible in the future.
We hope that our system can help enhance the confidence in the security of
anonymized decisions, thus leading to the courts being more confident to
publish decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v1 [stat.ME])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11138">
<div class="article-summary-box-inner">
<span><p>We develop an NLP-based procedure for detecting systematic nonmeritorious
consumer complaints, simply called systematic anomalies, among complaint
narratives. While classification algorithms are used to detect pronounced
anomalies, in the case of smaller and frequent systematic anomalies, the
algorithms may falter due to a variety of reasons, including technical ones as
well as natural limitations of human analysts. Therefore, as the next step
after classification, we convert the complaint narratives into quantitative
data, which are then analyzed using an algorithm for detecting systematic
anomalies. We illustrate the entire procedure using complaint narratives from
the Consumer Complaint Database of the Consumer Financial Protection Bureau.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11148">
<div class="article-summary-box-inner">
<span><p>The automation of code review activities, a long-standing pursuit in software
engineering, has been primarily addressed by numerous domain-specific
pre-trained models. Despite their success, these models frequently demand
extensive resources for pre-training from scratch. In contrast, Large Language
Models (LLMs) provide an intriguing alternative, given their remarkable
capabilities when supplemented with domain-specific knowledge. However, their
potential for automating code review tasks remains largely unexplored.
</p>
<p>In response to this research gap, we present LLaMA-Reviewer, an innovative
framework that leverages the capabilities of LLaMA, a popular LLM, in the realm
of code review. Mindful of resource constraints, this framework employs
parameter-efficient fine-tuning (PEFT) methods, delivering high performance
while using less than 1% of trainable parameters.
</p>
<p>An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,
publicly available datasets. Notably, even with the smallest LLaMA base model
consisting of 6.7B parameters and a limited number of tuning epochs,
LLaMA-Reviewer equals the performance of existing code-review-focused models.
</p>
<p>The ablation experiments provide insights into the influence of various
fine-tuning process components, including input representation, instruction
tuning, and different PEFT methods. To foster continuous progress in this
field, the code and all PEFT-weight plugins have been made open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViCo: Engaging Video Comment Generation with Human Preference Rewards. (arXiv:2308.11171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11171">
<div class="article-summary-box-inner">
<span><p>Engaging video comments play an important role in video social media, as they
are the carrier of feelings, thoughts, or humor of the audience. Preliminary
works have made initial exploration for video comment generation by adopting
caption-style encoder-decoder models. However, comment generation presents some
unique challenges distinct from caption generation, which makes these methods
somewhat less effective at generating engaging comments. In contrast to the
objective and descriptive nature of captions, comments tend to be inherently
subjective, making it hard to quantify and evaluate the engagement of comments.
Furthermore, the scarcity of truly engaging comments brings difficulty to
collecting enough high-quality training examples. In this paper, we propose
ViCo with three novel designs to tackle the above challenges for generating
engaging Video Comments. Firstly, to quantify the engagement of comments, we
utilize the number of "likes" each comment receives as a proxy of human
preference after an appropriate debiasing procedure. Secondly, to automatically
evaluate the engagement of comments, we train a reward model to align its
judgment to the above proxy. Our user studies indicate that this reward model
effectively aligns with human judgments. Lastly, to alleviate the scarcity of
high-quality comments, an initial generator is trained on readily available but
noisy data to generate comments. Then the reward model is employed to offer
feedback on the generated comments, thus optimizing the initial generator. To
facilitate the research of video commenting, we collect a large video
comment-dataset (ViCo-20k) with rich metadata from a popular video website.
Experiments on ViCo-20k show that the comments generated by our ViCo model
exhibit the best performance in terms of both quantitative and qualitative
results, particularly when engagement is considered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11189">
<div class="article-summary-box-inner">
<span><p>Error prediction in large language models often relies on domain-specific
information. In this paper, we present measures for quantification of error in
the response of a large language model based on the diversity of responses to a
given prompt - hence independent of the underlying application. We describe how
three such measures - based on entropy, Gini impurity, and centroid distance -
can be employed. We perform a suite of experiments on multiple datasets and
temperature settings to demonstrate that these measures strongly correlate with
the probability of failure. Additionally, we present empirical results
demonstrating how these measures can be applied to few-shot prompting,
chain-of-thought reasoning, and error detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11224">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have garnered considerable interest within both
academic and industrial. Yet, the application of LLMs to graph data remains
under-explored. In this study, we evaluate the capabilities of four LLMs in
addressing several analytical problems with graph data. We employ four distinct
evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.
Our results show that: 1) LLMs effectively comprehend graph data in natural
language and reason with graph topology. 2) GPT models can generate logical and
coherent results, outperforming alternatives in correctness. 3) All examined
LLMs face challenges in structural reasoning, with techniques like zero-shot
chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT
models often produce erroneous answers in multi-answer tasks, raising concerns
in fidelity. 5) GPT models exhibit elevated confidence in their outputs,
potentially hindering their rectification capacities. Notably, GPT-4 has
demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own
previous iterations. The code is available at:
https://github.com/Ayame1006/LLMtoGraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11257">
<div class="article-summary-box-inner">
<span><p>The semantic parsing-based method is an important research branch for
knowledge-based question answering. It usually generates executable programs
lean upon the question and then conduct them to reason answers over a knowledge
base. Benefit from this inherent mechanism, it has advantages in the
performance and the interpretability. However,traditional semantic parsing
methods usually generate a complete program before executing it, which
struggles with multi-hop question answering over heterogeneous knowledge.
Firstly,a complete multi-hop program relies on multiple heterogeneous
supporting facts, and it is difficult for models to receive these facts
simultaneously. Secondly,these methods ignore the interaction information
between the previous-hop execution result and the current-hop program
generation. To alleviate these challenges, we propose a self-iterative
framework for multi-hop program generation (HopPG) over heterogeneous
knowledge, which leverages the previous-hop execution results to retrieve
supporting facts and generate subsequent programs iteratively. We evaluate our
model on MMQA-T^2. The experimental results show that HopPG outperforms
existing semantic-parsing-based baselines, especially on the multi-hop
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. (arXiv:2308.11276v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11276">
<div class="article-summary-box-inner">
<span><p>Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity
of large-scale publicly available music datasets with natural language
captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),
capable of answering music-related questions and generating captions for music
files. Our model utilizes audio representations from a pretrained MERT model to
extract music features. However, obtaining a suitable dataset for training the
MU-LLaMA model remains challenging, as existing publicly accessible audio
question answering datasets lack the necessary depth for open-ended music
question answering. To fill this gap, we present a methodology for generating
question-answer pairs from existing audio captioning datasets and introduce the
MusicQA Dataset designed for answering open-ended music-related questions. The
experiments demonstrate that the proposed MU-LLaMA model, trained on our
designed MusicQA dataset, achieves outstanding performance in both music
question answering and music caption generation across various metrics,
outperforming current state-of-the-art (SOTA) models in both fields and
offering a promising advancement in the T2M-Gen research field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEAP: Efficient and Automated Test Method for NLP Software. (arXiv:2308.11284v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11284">
<div class="article-summary-box-inner">
<span><p>The widespread adoption of DNNs in NLP software has highlighted the need for
robustness. Researchers proposed various automatic testing techniques for
adversarial test cases. However, existing methods suffer from two limitations:
weak error-discovering capabilities, with success rates ranging from 0% to
24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to
205.28s per test case, making them challenging for time-constrained scenarios.
To address these issues, this paper proposes LEAP, an automated test method
that uses LEvy flight-based Adaptive Particle swarm optimization integrated
with textual features to generate adversarial test cases. Specifically, we
adopt Levy flight for population initialization to increase the diversity of
generated test cases. We also design an inertial weight adaptive update
operator to improve the efficiency of LEAP's global optimization of
high-dimensional text examples and a mutation operator based on the greedy
strategy to reduce the search time. We conducted a series of experiments to
validate LEAP's ability to test NLP software and found that the average success
rate of LEAP in generating adversarial test cases is 79.1%, which is 6.1%
higher than the next best approach (PSOattack). While ensuring high success
rates, LEAP significantly reduces time overhead by up to 147.6s compared to
other heuristic-based methods. Additionally, the experimental results
demonstrate that LEAP can generate more transferable test cases and
significantly enhance the robustness of DNN-based systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce. (arXiv:2308.11351v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11351">
<div class="article-summary-box-inner">
<span><p>Given the long textual product information and the product image, Multi-Modal
Product Summarization (MMPS) aims to attract customers' interest and increase
their desire to purchase by highlighting product characteristics with a short
textual summary. Existing MMPS methods have achieved promising performance.
Nevertheless, there still exist several problems: 1) lack end-to-end product
summarization, 2) lack multi-grained multi-modal modeling, and 3) lack
multi-modal attribute modeling. To address these issues, we propose an
end-to-end multi-grained multi-modal attribute-aware product summarization
method (M3PS) for generating high-quality product summaries in e-commerce. M3PS
jointly models product attributes and generates product summaries. Meanwhile,
we design several multi-grained multi-modal tasks to better guide the
multi-modal learning of M3PS. Furthermore, we model product attributes based on
both text and image modalities so that multi-modal product characteristics can
be manifested in the generated summaries. Extensive experiments on a real
large-scale Chinese e-commence dataset demonstrate that our model outperforms
state-of-the-art product summarization methods w.r.t. several summarization
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11380">
<div class="article-summary-box-inner">
<span><p>This paper presents an end-to-end model designed to improve automatic speech
recognition (ASR) for a particular speaker in a crowded, noisy environment. The
model utilizes a single-channel speech enhancement module that isolates the
speaker's voice from background noise, along with an ASR module. Through this
approach, the model is able to decrease the word error rate (WER) of ASR from
80% to 26.4%. Typically, these two components are adjusted independently due to
variations in data requirements. However, speech enhancement can create
anomalies that decrease ASR efficiency. By implementing a joint fine-tuning
strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5%
in joint tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm. (arXiv:2308.11411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11411">
<div class="article-summary-box-inner">
<span><p>Extracting relational triples (subject, predicate, object) from text enables
the transformation of unstructured text data into structured knowledge. The
named entity recognition (NER) and the relation extraction (RE) are two
foundational subtasks in this knowledge generation pipeline. The integration of
subtasks poses a considerable challenge due to their disparate nature. This
paper presents a novel approach that converts the triple extraction task into a
graph labeling problem, capitalizing on the structural information of
dependency parsing and graph recursive neural networks (GRNNs). To integrate
subtasks, this paper proposes a dynamic feedback forest algorithm that connects
the representations of subtasks by inference operations during model training.
Experimental results demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11432">
<div class="article-summary-box-inner">
<span><p>Autonomous agents have long been a prominent research topic in the academic
community. Previous research in this field often focuses on training agents
with limited knowledge within isolated environments, which diverges
significantly from the human learning processes, and thus makes the agents hard
to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating autonomous agents based on LLMs. To harness
the full potential of LLMs, researchers have devised diverse agent
architectures tailored to different applications. In this paper, we present a
comprehensive survey of these studies, delivering a systematic review of the
field of autonomous agents from a holistic perspective. More specifically, our
focus lies in the construction of LLM-based agents, for which we propose a
unified framework that encompasses a majority of the previous work.
Additionally, we provide a summary of the various applications of LLM-based AI
agents in the domains of social science, natural science, and engineering.
Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI
agents. Based on the previous studies, we also present several challenges and
future directions in this field. To keep track of this field and continuously
update our survey, we maintain a repository for the related references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification. (arXiv:2308.11447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11447">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment classification is a crucial problem in fine-grained
sentiment analysis, which aims to predict the sentiment polarity of the given
aspect according to its context. Previous works have made remarkable progress
in leveraging attention mechanism to extract opinion words for different
aspects. However, a persistent challenge is the effective management of
semantic mismatches, which stem from attention mechanisms that fall short in
adequately aligning opinions words with their corresponding aspect in
multi-aspect sentences. To address this issue, we propose a novel
Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual
association between opinion words and the corresponding aspect. Specifically,
we first introduce a neighboring span enhanced module which highlights various
compositions of neighboring words and given aspects. In addition, we design a
multi-perspective attention mechanism that align relevant opinion information
with respect to the given aspect. Extensive experiments on three benchmark
datasets demonstrate that our model achieves state-of-the-art results. The
source code is available at https://github.com/AONE-NLP/ABSA-AOAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11462">
<div class="article-summary-box-inner">
<span><p>The advent of large language models (LLMs) and their adoption by the legal
community has given rise to the question: what types of legal reasoning can
LLMs perform? To enable greater study of this question, we present LegalBench:
a collaboratively constructed legal reasoning benchmark consisting of 162 tasks
covering six different types of legal reasoning. LegalBench was built through
an interdisciplinary process, in which we collected tasks designed and
hand-crafted by legal professionals. Because these subject matter experts took
a leading role in construction, tasks either measure legal reasoning
capabilities that are practically useful, or measure reasoning skills that
lawyers find interesting. To enable cross-disciplinary conversations about LLMs
in the law, we additionally show how popular legal frameworks for describing
legal reasoning -- which distinguish between its many forms -- correspond to
LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary.
This paper describes LegalBench, presents an empirical evaluation of 20
open-source and commercial LLMs, and illustrates the types of research
explorations LegalBench enables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-Level Multimodal and Language-Agnostic Representations. (arXiv:2308.11466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11466">
<div class="article-summary-box-inner">
<span><p>We introduce SONAR, a new multilingual and multimodal fixed-size sentence
embedding space. Our single text encoder, covering 200 languages, substantially
outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim
and xsim++ multilingual similarity search tasks. Speech segments can be
embedded in the same SONAR embedding space using language-specific speech
encoders trained in a teacher-student setting on speech transcription data. Our
encoders outperform existing speech encoders on similarity search tasks. We
also provide a text decoder for 200 languages, which allows us to perform
text-to-text and speech-to-text machine translation, including for zero-shot
language and modality combinations. Our text-to-text results are competitive
compared to the state-of-the-art NLLB~1B model, despite the fixed-size
bottleneck representation. Our zero-shot speech-to-text translation results
compare favorably with strong supervised baselines such as Whisper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11483">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
various NLP tasks. However, previous works have shown these models are
sensitive towards prompt wording, and few-shot demonstrations and their order,
posing challenges to fair assessment of these models. As these models become
more powerful, it becomes imperative to understand and address these
limitations. In this paper, we focus on LLMs robustness on the task of
multiple-choice questions -- commonly adopted task to study reasoning and
fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs
towards the order of options in multiple-choice questions, we demonstrate a
considerable performance gap of approximately 13% to 75% in LLMs on different
benchmarks, when answer options are reordered, even when using demonstrations
in a few-shot setting. Through a detailed analysis, we conjecture that this
sensitivity arises when LLMs are uncertain about the prediction between the
top-2/3 choices, and specific options placements may favor certain prediction
between those top choices depending on the question caused by positional bias.
We also identify patterns in top-2 choices that amplify or mitigate the model's
bias toward option placement. We found that for amplifying bias, the optimal
strategy involves positioning the top two choices as the first and last
options. Conversely, to mitigate bias, we recommend placing these choices among
the adjacent options. To validate our conjecture, we conduct various
experiments and adopt two approaches to calibrate LLMs' predictions, leading to
up to 8 percentage points improvement across different models and benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11490">
<div class="article-summary-box-inner">
<span><p>Automatically disentangling an author's style from the content of their
writing is a longstanding and possibly insurmountable problem in computational
linguistics. At the same time, the availability of large text corpora furnished
with author labels has recently enabled learning authorship representations in
a purely data-driven manner for authorship attribution, a task that ostensibly
depends to a greater extent on encoding writing style than encoding content.
However, success on this surrogate task does not ensure that such
representations capture writing style since authorship could also be correlated
with other latent variables, such as topic. In an effort to better understand
the nature of the information these representations convey, and specifically to
validate the hypothesis that they chiefly encode writing style, we
systematically probe these representations through a series of targeted
experiments. The results of these experiments suggest that representations
learned for the surrogate authorship prediction task are indeed sensitive to
writing style. As a consequence, authorship representations may be expected to
be robust to certain kinds of data shift, such as topic drift over time.
Additionally, our findings may open the door to downstream applications that
require stylistic representations, such as style transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Prototype Adapter for Vision-Language Models. (arXiv:2308.11507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11507">
<div class="article-summary-box-inner">
<span><p>Recently, large-scale pre-trained vision-language models (e.g. CLIP and
ALIGN) have demonstrated remarkable effectiveness in acquiring transferable
visual representations. To leverage the valuable knowledge encoded within these
models for downstream tasks, several fine-tuning approaches, including prompt
tuning methods and adapter-based methods, have been developed to adapt
vision-language models effectively with supervision. However, these methods
rely on the availability of annotated samples, which can be labor-intensive and
time-consuming to acquire, thus limiting scalability. To address this issue, in
this work, we design an unsupervised fine-tuning approach for vision-language
models called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for
the unannotated target datasets, we leverage the text-image aligning capability
of CLIP to automatically select the most confident samples for each class.
Utilizing these selected samples, we generate class prototypes, which serve as
the initialization for the learnable prototype model. After fine-tuning, the
prototype model prediction is combined with the original CLIP's prediction by a
residual connection to perform downstream recognition tasks. Our extensive
experimental results on image recognition and domain generalization show that
the proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter,
and also the state-of-the-art UPL method by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers. (arXiv:2308.11519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11519">
<div class="article-summary-box-inner">
<span><p>Customer reviews play a crucial role in assessing customer satisfaction,
gathering feedback, and driving improvements for businesses. Analyzing these
reviews provides valuable insights into customer sentiments, including
compliments, comments, and suggestions. Text classification techniques enable
businesses to categorize customer reviews into distinct categories,
facilitating a better understanding of customer feedback. However, challenges
such as overfitting and bias limit the effectiveness of a single classifier in
ensuring optimal prediction. This study proposes a novel approach to address
these challenges by introducing a stacking ensemble-based multi-text
classification method that leverages transformer models. By combining multiple
single transformers, including BERT, ELECTRA, and DistilBERT, as base-level
classifiers, and a meta-level classifier based on RoBERTa, an optimal
predictive model is generated. The proposed stacking ensemble-based multi-text
classification method aims to enhance the accuracy and robustness of customer
review analysis. Experimental evaluations conducted on a real-world customer
review dataset demonstrate the effectiveness and superiority of the proposed
approach over traditional single classifier models. The stacking ensemble-based
multi-text classification method using transformers proves to be a promising
solution for businesses seeking to extract valuable insights from customer
reviews and make data-driven decisions to enhance customer satisfaction and
drive continuous improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Power of Topic Modeling Techniques in Analyzing Customer Reviews: A Comparative Analysis. (arXiv:2308.11520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11520">
<div class="article-summary-box-inner">
<span><p>The exponential growth of online social network platforms and applications
has led to a staggering volume of user-generated textual content, including
comments and reviews. Consequently, users often face difficulties in extracting
valuable insights or relevant information from such content. To address this
challenge, machine learning and natural language processing algorithms have
been deployed to analyze the vast amount of textual data available online. In
recent years, topic modeling techniques have gained significant popularity in
this domain. In this study, we comprehensively examine and compare five
frequently used topic modeling methods specifically applied to customer
reviews. The methods under investigation are latent semantic analysis (LSA),
latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF),
pachinko allocation model (PAM), Top2Vec, and BERTopic. By practically
demonstrating their benefits in detecting important topics, we aim to highlight
their efficacy in real-world scenarios. To evaluate the performance of these
topic modeling methods, we carefully select two textual datasets. The
evaluation is based on standard statistical evaluation metrics such as topic
coherence score. Our findings reveal that BERTopic consistently yield more
meaningful extracted topics and achieve favorable results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11521">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as ChatGPT, have emerged with astonishing
capabilities approaching artificial general intelligence. While providing
convenience for various societal needs, LLMs have also lowered the cost of
generating harmful content. Consequently, LLM developers have deployed
semantic-level defenses to recognize and reject prompts that may lead to
inappropriate content. Unfortunately, these defenses are not foolproof, and
some attackers have crafted "jailbreak" prompts that temporarily hypnotize the
LLM into forgetting content defense rules and answering any improper questions.
To date, there is no clear explanation of the principles behind these
semantic-level attacks and defenses in both industry and academia.
</p>
<p>This paper investigates the LLM jailbreak problem and proposes an automatic
jailbreak method for the first time. We propose the concept of a semantic
firewall and provide three technical implementation approaches. Inspired by the
attack that penetrates traditional firewalls through reverse tunnels, we
introduce a "self-deception" attack that can bypass the semantic firewall by
inducing LLM to generate prompts that facilitate jailbreak. We generated a
total of 2,520 attack payloads in six languages (English, Russian, French,
Spanish, Chinese, and Arabic) across seven virtual scenarios, targeting the
three most common types of violations: violence, hate, and pornography. The
experiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The
success rates on the two models were 86.2% and 67%, while the failure rates
were 4.7% and 2.2%, respectively. This highlighted the effectiveness of the
proposed attack method. All experimental code and raw data will be released as
open-source to inspire future research. We believe that manipulating AI
behavior through carefully crafted prompts will become an important research
direction in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Representations on Logs for AIOps. (arXiv:2308.11526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11526">
<div class="article-summary-box-inner">
<span><p>AI for IT Operations (AIOps) is a powerful platform that Site Reliability
Engineers (SREs) use to automate and streamline operational workflows with
minimal human intervention. Automated log analysis is a critical task in AIOps
as it provides key insights for SREs to identify and address ongoing faults.
Tasks such as log format detection, log classification, and log parsing are key
components of automated log analysis. Most of these tasks require supervised
learning; however, there are multiple challenges due to limited labelled log
data and the diverse nature of log data. Large Language Models (LLMs) such as
BERT and GPT3 are trained using self-supervision on a vast amount of unlabeled
data. These models provide generalized representations that can be effectively
used for various downstream tasks with limited labelled data. Motivated by the
success of LLMs in specific domains like science and biology, this paper
introduces a LLM for log data which is trained on public and proprietary log
data. The results of our experiments demonstrate that the proposed LLM
outperforms existing models on multiple downstream tasks. In summary, AIOps
powered by LLMs offers an efficient and effective solution for automating log
analysis tasks and enabling SREs to focus on higher-level tasks. Our proposed
LLM, trained on public and proprietary log data, offers superior performance on
multiple downstream tasks, making it a valuable addition to the AIOps platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. (arXiv:2308.11527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11527">
<div class="article-summary-box-inner">
<span><p>Although deep pre-trained language models have shown promising benefit in a
large set of industrial scenarios, including Click-Through-Rate (CTR)
prediction, how to integrate pre-trained language models that handle only
textual signals into a prediction pipeline with non-textual features is
challenging.
</p>
<p>Up to now two directions have been explored to integrate multi-modal inputs
in fine-tuning of pre-trained language models. One consists of fusing the
outcome of language models and non-textual features through an aggregation
layer, resulting into ensemble framework, where the cross-information between
textual and non-textual inputs are only learned in the aggregation layer. The
second one consists of splitting non-textual features into fine-grained
fragments and transforming the fragments to new tokens combined with textual
ones, so that they can be fed directly to transformer layers in language
models. However, this approach increases the complexity of the learning and
inference because of the numerous additional tokens.
</p>
<p>To address these limitations, we propose in this work a novel framework
BERT4CTR, with the Uni-Attention mechanism that can benefit from the
interactions between non-textual and textual features while maintaining low
time-costs in training and inference through a dimensionality reduction.
Comprehensive experiments on both public and commercial data demonstrate that
BERT4CTR can outperform significantly the state-of-the-art frameworks to handle
multi-modal inputs and be applicable to CTR prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11531">
<div class="article-summary-box-inner">
<span><p>Our project aims at helping and supporting stakeholders in refugee status
adjudications, such as lawyers, judges, governing bodies, and claimants, in
order to make better decisions through data-driven intelligence and increase
the understanding and transparency of the refugee application process for all
involved parties. This PhD project has two primary objectives: (1) to retrieve
past cases, and (2) to analyze legal decision-making processes on a dataset of
Canadian cases. In this paper, we present the current state of our work, which
includes a completed experiment on part (1) and ongoing efforts related to part
(2). We believe that NLP-based solutions are well-suited to address these
challenges, and we investigate the feasibility of automating all steps
involved. In addition, we introduce a novel benchmark for future NLP research
in refugee law. Our methodology aims to be inclusive to all end-users and
stakeholders, with expected benefits including reduced time-to-decision, fairer
and more transparent outcomes, and improved decision quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11534">
<div class="article-summary-box-inner">
<span><p>The unparalleled performance of closed-sourced ChatGPT has sparked efforts
towards its democratization, with notable strides made by leveraging real user
and ChatGPT conversations, as evidenced by Vicuna. However, while current
endeavors like Baize and UltraChat aim to auto-generate conversational data due
to challenges in gathering human participation, they primarily rely on ChatGPT
to simulate human behaviors based on directives rather than genuine human
learning. This results in a limited scope, diminished diversity, and an absence
of genuine multi-round conversational dynamics. To address the above issues, we
innovatively target human questions extracted from genuine human-machine
conversations as a learning goal and train a user simulator, UserGPT, to
produce a high-quality human-centric synthetic conversation dataset, RealChat.
Subsequently, this dataset trains our assistant model, ReaLM. Experimentally,
ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
comparison when considering equivalent training set sizes, and manual
evaluation also shows that our model is highly competitive. Impressively, when
fine-tuned with the latest LLaMA 2 model, ReaLM secured a leading score of 6.33
in the MT-Bench, outshining the contemporary same-scale models, including the
LLaMA-2-7B-chat model. Further in-depth analysis demonstrates the scalability
and transferability of our approach. A preliminary exploration into the
interplay between training set data quality and resultant model performance is
also undertaken, laying a robust groundwork for future investigations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BELB: a Biomedical Entity Linking Benchmark. (arXiv:2308.11537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11537">
<div class="article-summary-box-inner">
<span><p>Biomedical entity linking (BEL) is the task of grounding entity mentions to a
knowledge base. It plays a vital role in information extraction pipelines for
the life sciences literature. We review recent work in the field and find that,
as the task is absent from existing benchmarks for biomedical text mining,
different studies adopt different experimental setups making comparisons based
on published numbers problematic. Furthermore, neural systems are tested
primarily on instances linked to the broad coverage knowledge base UMLS,
leaving their performance to more specialized ones, e.g. genes or variants,
understudied. We therefore developed BELB, a Biomedical Entity Linking
Benchmark, providing access in a unified format to 11 corpora linked to 7
knowledge bases and spanning six entity types: gene, disease, chemical,
species, cell line and variant. BELB greatly reduces preprocessing overhead in
testing BEL systems on multiple corpora offering a standardized testbed for
reproducible experiments. Using BELB we perform an extensive evaluation of six
rule-based entity-specific systems and three recent neural approaches
leveraging pre-trained language models. Our results reveal a mixed picture
showing that neural approaches fail to perform consistently across entity
types, highlighting the need of further studies towards entity-agnostic models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using ChatGPT as a CAT tool in Easy Language translation. (arXiv:2308.11563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11563">
<div class="article-summary-box-inner">
<span><p>This study sets out to investigate the feasibility of using ChatGPT to
translate citizen-oriented administrative texts into German Easy Language, a
simplified, controlled language variety that is adapted to the needs of people
with reading impairments. We use ChatGPT to translate selected texts from
websites of German public authorities using two strategies, i.e. linguistic and
holistic. We analyse the quality of the generated texts based on different
criteria, such as correctness, readability, and syntactic complexity. The
results indicated that the generated texts are easier than the standard texts,
but that they still do not fully meet the established Easy Language standards.
Additionally, the content is not always rendered correctly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11578">
<div class="article-summary-box-inner">
<span><p>After the inception of emotion recognition or affective computing, it has
increasingly become an active research topic due to its broad applications.
Over the past couple of decades, emotion recognition models have gradually
migrated from statistically shallow models to neural network-based deep models,
which can significantly boost the performance of emotion recognition models and
consistently achieve the best results on different benchmarks. Therefore, in
recent years, deep models have always been considered the first option for
emotion recognition. However, the debut of large language models (LLMs), such
as ChatGPT, has remarkably astonished the world due to their emerged
capabilities of zero/few-shot learning, in-context learning, chain-of-thought,
and others that are never shown in previous deep models. In the present paper,
we comprehensively investigate how the LLMs perform in emotion recognition in
terms of diverse aspects, including in-context learning, few-short learning,
accuracy, generalisation, and explanation. Moreover, we offer some insights and
pose other potential challenges, hoping to ignite broader discussions about
enhancing emotion recognition in the new era of advanced and generalised large
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Emotional Support Chatbots in the Era of LLMs. (arXiv:2308.11584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11584">
<div class="article-summary-box-inner">
<span><p>The integration of emotional support into various conversational scenarios
presents profound societal benefits, such as social interactions, mental health
counseling, and customer service. However, there are unsolved challenges that
hinder real-world applications in this field, including limited data
availability and the absence of well-accepted model training paradigms. This
work endeavors to navigate these challenges by harnessing the capabilities of
Large Language Models (LLMs). We introduce an innovative methodology that
synthesizes human insights with the computational prowess of LLMs to curate an
extensive emotional support dialogue dataset. Our approach is initiated with a
meticulously designed set of dialogues spanning diverse scenarios as generative
seeds. By utilizing the in-context learning potential of ChatGPT, we
recursively generate an ExTensible Emotional Support dialogue dataset, named
ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,
examining the impact of diverse training strategies, ultimately yielding an LLM
meticulously optimized for emotional support interactions. An exhaustive
assessment of the resultant model showcases its proficiency in offering
emotional support, marking a pivotal step in the realm of emotional support
bots and paving the way for subsequent research and implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes. (arXiv:2308.11585v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11585">
<div class="article-summary-box-inner">
<span><p>In the wake of the explosive growth of machine learning (ML) usage,
particularly within the context of emerging Large Language Models (LLMs),
comprehending the semantic significance rooted in their internal workings is
crucial. While causal analyses focus on defining semantics and its
quantification, the gradient-based approach is central to explainable AI (XAI),
tackling the interpretation of the black box. By synergizing these approaches,
the exploration of how a model's internal mechanisms illuminate its causal
effect has become integral for evidence-based decision-making. A parallel line
of research has revealed that intersectionality - the combinatory impact of
multiple demographics of an individual - can be structured in the form of an
Averaged Treatment Effect (ATE). Initially, this study illustrates that the
hateful memes detection problem can be formulated as an ATE, assisted by the
principles of intersectionality, and that a modality-wise summarization of
gradient-based attention attribution scores can delineate the distinct
behaviors of three Transformerbased models concerning ATE. Subsequently, we
show that the latest LLM LLaMA2 has the ability to disentangle the
intersectional nature of memes detection in an in-context learning setting,
with their mechanistic properties elucidated via meta-gradient, a secondary
form of gradient. In conclusion, this research contributes to the ongoing
dialogue surrounding XAI and the multifaceted nature of ML models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indonesian Automatic Speech Recognition with XLSR-53. (arXiv:2308.11589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11589">
<div class="article-summary-box-inner">
<span><p>This study focuses on the development of Indonesian Automatic Speech
Recognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for
cross-lingual speech representations. The use of this XLSR-53 pre-trained model
is to significantly reduce the amount of training data in non-English languages
required to achieve a competitive Word Error Rate (WER). The total amount of
data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14
hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common
Voice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in
this study can compete with similar models using the Common Voice dataset split
test. WER can be decreased by around 8% using a language model, resulted in WER
from 20% to 12%. Thus, the results of this study have succeeded in perfecting
previous research in contributing to the creation of a better Indonesian ASR
with a smaller amount of data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11592">
<div class="article-summary-box-inner">
<span><p>In the era of Large Language Models (LLMs), tremendous strides have been made
in the field of multimodal understanding. However, existing advanced algorithms
are limited to effectively utilizing the immense representation capabilities
and rich world knowledge inherent to these large pre-trained models, and the
beneficial connections among tasks within the context of text-rich scenarios
have not been sufficiently explored. In this work, we introduce UniDoc, a novel
multimodal model equipped with text detection and recognition capabilities,
which are deficient in existing approaches. Moreover, UniDoc capitalizes on the
beneficial interactions among tasks to enhance the performance of each
individual task. To implement UniDoc, we perform unified multimodal instruct
tuning on the contributed large-scale instruction following datasets.
Quantitative and qualitative experimental results show that UniDoc sets
state-of-the-art scores across multiple challenging benchmarks. To the best of
our knowledge, this is the first large multimodal model capable of simultaneous
text detection, recognition, spotting, and understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeamlessM4T-Massively Multilingual & Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11596">
<div class="article-summary-box-inner">
<span><p>What does it take to create the Babel Fish, a tool that can help individuals
translate speech between any two languages? While recent breakthroughs in
text-based models have pushed machine translation coverage beyond 200
languages, unified speech-to-speech translation models have yet to achieve
similar strides. More specifically, conventional speech-to-speech translation
systems rely on cascaded systems that perform translation progressively,
putting high-performing unified systems out of reach. To address these gaps, we
introduce SeamlessM4T, a single model that supports speech-to-speech
translation, speech-to-text translation, text-to-speech translation,
text-to-text translation, and automatic speech recognition for up to 100
languages. To build this, we used 1 million hours of open speech audio data to
learn self-supervised speech representations with w2v-BERT 2.0. Subsequently,
we created a multimodal corpus of automatically aligned speech translations.
Filtered and combined with human-labeled and pseudo-labeled data, we developed
the first multilingual system capable of translating from and into English for
both speech and text. On FLEURS, SeamlessM4T sets a new standard for
translations into multiple target languages, achieving an improvement of 20%
BLEU over the previous SOTA in direct speech-to-text translation. Compared to
strong cascaded models, SeamlessM4T improves the quality of into-English
translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in
speech-to-speech. Tested for robustness, our system performs better against
background noises and speaker variations in speech-to-text tasks compared to
the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and
added toxicity to assess translation safety. Finally, all contributions in this
work are open-sourced at this https
https://github.com/facebookresearch/seamless_communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11601">
<div class="article-summary-box-inner">
<span><p>The introduction of the transformer architecture and the self-attention
mechanism has led to an explosive production of language models trained on
specific downstream tasks and data domains. With over 200, 000 models in the
Hugging Face ecosystem, users grapple with selecting and optimizing models to
suit multifaceted workflows and data domains while addressing computational,
security, and recency concerns. There is an urgent need for machine learning
frameworks that can eliminate the burden of model selection and customization
and unleash the incredible power of the vast emerging model library for end
users. Here, we propose a context-aware routing system, Tryage, that leverages
a language model router for optimal selection of expert models from a model
library based on analysis of individual input prompts. Inspired by the thalamic
router in the brain, Tryage employs a perceptive router to predict down-stream
model performance on prompts and, then, makes a routing decision using an
objective function that integrates performance predictions with user goals and
constraints that are incorporated through flags (e.g., model size, model
recency). Tryage allows users to explore a Pareto front and automatically
trade-off between task accuracy and secondary goals including minimization of
model size, recency, security, verbosity, and readability. Across heterogeneous
data sets that include code, text, clinical data, and patents, the Tryage
framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection
identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by
GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how
routing models can be applied to program and control the behavior of
multi-model LLM systems to maximize efficient use of the expanding and evolving
language model ecosystem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. (arXiv:2308.11606v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11606">
<div class="article-summary-box-inner">
<span><p>Generating video stories from text prompts is a complex task. In addition to
having high visual quality, videos need to realistically adhere to a sequence
of text prompts whilst being consistent throughout the frames. Creating a
benchmark for video generation requires data annotated over time, which
contrasts with the single caption used often in video datasets. To fill this
gap, we collect comprehensive human annotations on three existing datasets, and
introduce StoryBench: a new, challenging multi-task benchmark to reliably
evaluate forthcoming text-to-video models. Our benchmark includes three video
generation tasks of increasing difficulty: action execution, where the next
action must be generated starting from a conditioning video; story
continuation, where a sequence of actions must be executed starting from a
conditioning video; and story generation, where a video must be generated from
only text prompts. We evaluate small yet strong text-to-video baselines, and
show the benefits of training on story-like data algorithmically generated from
existing video captions. Finally, we establish guidelines for human evaluation
of video stories, and reaffirm the need of better automatic metrics for video
generation. StoryBench aims at encouraging future research efforts in this
exciting new area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Social Biases in Grounded Vision and Language Embeddings. (arXiv:2002.08911v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.08911">
<div class="article-summary-box-inner">
<span><p>We generalize the notion of social biases from language embeddings to
grounded vision and language embeddings. Biases are present in grounded
embeddings, and indeed seem to be equally or more significant than for
ungrounded embeddings. This is despite the fact that vision and language can
suffer from different biases, which one might hope could attenuate the biases
in both. Multiple ways exist to generalize metrics measuring bias in word
embeddings to this new setting. We introduce the space of generalizations
(Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations
answer different yet important questions about how biases, language, and vision
interact. These metrics are used on a new dataset, the first for grounded bias,
created by augmenting extending standard linguistic bias benchmarks with 10,228
images from COCO, Conceptual Captions, and Google Images. Dataset construction
is challenging because vision datasets are themselves very biased. The presence
of these biases in systems will begin to have real-world consequences as they
are deployed, making carefully measuring bias and then mitigating it critical
to building a fair society.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Measure-Theoretic Characterization of Tight Language Models. (arXiv:2212.10502v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10502">
<div class="article-summary-box-inner">
<span><p>Language modeling, a central task in natural language processing, involves
estimating a probability distribution over strings. In most cases, the
estimated distribution sums to 1 over all finite strings. However, in some
pathological cases, probability mass can ``leak'' onto the set of infinite
sequences. In order to characterize the notion of leakage more precisely, this
paper offers a measure-theoretic treatment of language modeling. We prove that
many popular language model families are in fact tight, meaning that they will
not leak in this sense. We also generalize characterizations of tightness
proposed in previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09767">
<div class="article-summary-box-inner">
<span><p>In this paper, a new perspective is suggested for unsupervised Ontology
Matching (OM) or Ontology Alignment (OA) by treating it as a translation task.
Ontologies are represented as graphs, and the translation is performed from a
node in the source ontology graph to a path in the target ontology graph. The
proposed framework, Truveta Mapper (TM), leverages a multi-task
sequence-to-sequence transformer model to perform alignment across multiple
ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables
the model to implicitly learn the relationship between different ontologies via
transfer-learning without requiring any explicit cross-ontology manually
labeled data. This also enables the formulated framework to outperform existing
solutions for both runtime latency and alignment quality. The model is
pre-trained and fine-tuned only on publicly available text corpus and
inner-ontologies data. The proposed solution outperforms state-of-the-art
approaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented
new OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers
log-linear complexity, and overall makes the OM task efficient and more
straightforward without much post-processing involving mapping extension or
mapping repair. We are open sourcing our solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrojText: Test-time Invisible Textual Trojan Insertion. (arXiv:2303.02242v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.02242">
<div class="article-summary-box-inner">
<span><p>In Natural Language Processing (NLP), intelligent neuron models can be
susceptible to textual Trojan attacks. Such attacks occur when Trojan models
behave normally for standard inputs but generate malicious output for inputs
that contain a specific trigger. Syntactic-structure triggers, which are
invisible, are becoming more popular for Trojan attacks because they are
difficult to detect and defend against. However, these types of attacks require
a large corpus of training data to generate poisoned samples with the necessary
syntactic structures for Trojan insertion. Obtaining such data can be difficult
for attackers, and the process of generating syntactic poisoned triggers and
inserting Trojans can be time-consuming. This paper proposes a solution called
TrojText, which aims to determine whether invisible textual Trojan attacks can
be performed more efficiently and cost-effectively without training data. The
proposed approach, called the Representation-Logit Trojan Insertion (RLI)
algorithm, uses smaller sampled test data instead of large training data to
achieve the desired attack. The paper also introduces two additional
techniques, namely the accumulated gradient ranking (AGR) and Trojan Weights
Pruning (TWP), to reduce the number of tuned parameters and the attack
overhead. The TrojText approach was evaluated on three datasets (AG's News,
SST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The
experiments demonstrated that the TrojText approach achieved a 98.35\%
classification accuracy for test sentences in the target class on the BERT
model for the AG's News dataset. The source code for TrojText is available at
https://github.com/UCF-ML-Research/TrojText.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01852">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and
GPT-4) research, state-of-the-art large language models (LLM) from the GPT
series, and their prospective applications across diverse domains. Indeed, key
innovations such as large-scale pre-training that captures knowledge across the
entire world wide web, instruction fine-tuning and Reinforcement Learning from
Human Feedback (RLHF) have played significant roles in enhancing LLMs'
adaptability and performance. We performed an in-depth analysis of 194 relevant
papers on arXiv, encompassing trend analysis, word cloud representation, and
distribution analysis across various application domains. The findings reveal a
significant and increasing interest in ChatGPT-related research, predominantly
centered on direct natural language processing applications, while also
demonstrating considerable potential in areas ranging from education and
history to mathematics, medicine, and physics. This study endeavors to furnish
insights into ChatGPT's capabilities, potential implications, ethical concerns,
and offer direction for future advancements in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing. (arXiv:2305.03881v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03881">
<div class="article-summary-box-inner">
<span><p>Multi-modal search engines have experienced significant growth and widespread
use in recent years, making them the second most common internet use. While
search engine systems offer a range of services, the image search field has
recently become a focal point in the information retrieval community, as the
adage goes, "a picture is worth a thousand words". Although popular search
engines like Google excel at image search accuracy and agility, there is an
ongoing debate over whether their search results can be biased in terms of
gender, language, demographics, socio-cultural aspects, and stereotypes. This
potential for bias can have a significant impact on individuals' perceptions
and influence their perspectives.
</p>
<p>In this paper, we present our study on bias and fairness in web search, with
a focus on keyword-based image search. We first discuss several kinds of biases
that exist in search systems and why it is important to mitigate them. We
narrow down our study to assessing and mitigating occupational stereotypes in
image search, which is a prevalent fairness issue in image retrieval. For the
assessment of stereotypes, we take gender as an indicator. We explore various
open-source and proprietary APIs for gender identification from images. With
these, we examine the extent of gender bias in top-tanked image search results
obtained for several occupational keywords. To mitigate the bias, we then
propose a fairness-aware re-ranking algorithm that optimizes (a) relevance of
the search result with the keyword and (b) fairness w.r.t genders identified.
We experiment on 100 top-ranked images obtained for 10 occupational keywords
and consider random re-ranking and re-ranking based on relevance as baselines.
Our experimental results show that the fairness-aware re-ranking algorithm
produces rankings with better fairness scores and competitive relevance scores
than the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification. (arXiv:2305.10971v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10971">
<div class="article-summary-box-inner">
<span><p>Africa has over 2000 indigenous languages but they are under-represented in
NLP research due to lack of datasets. In recent years, there have been progress
in developing labeled corpora for African languages. However, they are often
available in a single domain and may not generalize to other domains. In this
paper, we focus on the task of sentiment classification for cross domain
adaptation. We create a new dataset, NollySenti - based on the Nollywood movie
reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo,
Nigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using
classical machine learning methods and pre-trained language models. Leveraging
transfer learning, we compare the performance of cross-domain adaptation from
Twitter domain, and cross-lingual adaptation from English language. Our
evaluation shows that transfer from English in the same target domain leads to
more than 5% improvement in accuracy compared to transfer from Twitter in the
same language. To further mitigate the domain difference, we leverage machine
translation (MT) from English to other Nigerian languages, which leads to a
further improvement of 7% over cross-lingual evaluation. While MT to
low-resource languages are often of low quality, through human evaluation, we
show that most of the translated sentences preserve the sentiment of the
original English reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19370">
<div class="article-summary-box-inner">
<span><p>Transformers have emerged as the cornerstone of state-of-the-art natural
language processing models, showcasing exceptional performance across a wide
range of AI applications. However, the memory demands posed by the
self-attention mechanism and the large feedforward network in Transformers
limit their ability to handle long sequences, thereby creating challenges for
tasks involving multiple long sequences or long-term dependencies. We present a
distinct approach, Blockwise Parallel Transformer (BPT), that leverages
blockwise computation of self-attention and feedforward network fusion to
minimize memory costs. By processing longer input sequences while maintaining
memory efficiency, BPT enables training sequences up to 32 times longer than
vanilla Transformers and 2 to 4 times longer than previous memory-efficient
methods. Extensive experiments on language modeling and reinforcement learning
tasks demonstrate the effectiveness of BPT in reducing memory requirements and
improving performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08158">
<div class="article-summary-box-inner">
<span><p>Deep neural networks often learn unintended bias during training, which might
have harmful effects when deployed in real-world settings. This work surveys
214 papers related to sociodemographic bias in natural language processing
(NLP). In this study, we aim to provide a more comprehensive understanding of
the similarities and differences among approaches to sociodemographic bias in
NLP. To better understand the distinction between bias and real-world harm, we
turn to ideas from psychology and behavioral economics to propose a definition
for sociodemographic bias. We identify three main categories of NLP bias
research: types of bias, quantifying bias, and debiasing techniques. We
highlight the current trends in quantifying bias and debiasing techniques,
offering insights into their strengths and weaknesses. We conclude that current
approaches on quantifying bias face reliability issues, that many of the bias
metrics do not relate to real-world bias, and that debiasing techniques need to
focus more on training methods. Finally, we provide recommendations for future
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07312">
<div class="article-summary-box-inner">
<span><p>In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10652">
<div class="article-summary-box-inner">
<span><p>As an efficient approach to understand, generate, and process natural
language texts, research in natural language processing (NLP) has exhibited a
rapid spread and wide adoption in recent years. Given the increasing research
work in this area, several NLP-related approaches have been surveyed in the
research community. However, a comprehensive study that categorizes established
topics, identifies trends, and outlines areas for future research remains
absent. Contributing to closing this gap, we have systematically classified and
analyzed research papers in the ACL Anthology. As a result, we present a
structured overview of the research landscape, provide a taxonomy of fields of
study in NLP, analyze recent developments in NLP, summarize our findings, and
highlight directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LARCH: Large Language Model-based Automatic Readme Creation with Heuristics. (arXiv:2308.03099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03099">
<div class="article-summary-box-inner">
<span><p>Writing a readme is a crucial aspect of software development as it plays a
vital role in managing and reusing program code. Though it is a pain point for
many developers, automatically creating one remains a challenge even with the
recent advancements in large language models (LLMs), because it requires
generating an abstract description from thousands of lines of code. In this
demo paper, we show that LLMs are capable of generating a coherent and
factually correct readmes if we can identify a code fragment that is
representative of the repository. Building upon this finding, we developed
LARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages
representative code identification with heuristics and weak supervision.
Through human and automated evaluations, we illustrate that LARCH can generate
coherent and factually correct readmes in the majority of cases, outperforming
a baseline that does not rely on representative code identification. We have
made LARCH open-source and provided a cross-platform Visual Studio Code
interface and command-line interface, accessible at
https://github.com/hitachi-nlp/larch. A demo video showcasing LARCH's
capabilities is available at https://youtu.be/ZUKkh5ED-O4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.03212">
<div class="article-summary-box-inner">
<span><p>Transformers have emerged as a widely used neural network model for various
natural language processing tasks. Previous research explored their
relationship with constant-depth threshold circuits, making two assumptions:
average-hard attention and logarithmic precision for internal computations
relative to input length. Merrill et al. (2022) prove that average-hard
attention transformers recognize languages that fall within the complexity
class TC0, denoting the set of languages that can be recognized by
constant-depth polynomial-size threshold circuits. Likewise, Merrill and
Sabharwal (2023) show that log-precision transformers recognize languages
within the class of uniform TC0. This shows that both transformer models can be
simulated by constant-depth threshold circuits, with the latter being more
robust due to generating a uniform circuit family. Our paper shows that the
first result can be extended to yield uniform circuits as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.08742">
<div class="article-summary-box-inner">
<span><p>Model editing techniques modify a minor proportion of knowledge in Large
Language Models (LLMs) at a relatively low cost, which have demonstrated
notable success. Existing methods assume Transformer Layer (TL) hidden states
are values of key-value memories of the Feed-Forward Network (FFN). They
usually optimize the TL hidden states to memorize target knowledge and use it
to update the weights of the FFN in LLMs. However, the information flow of TL
hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,
and residual connections. Existing methods neglect the fact that the TL hidden
states contains information not specifically required for FFN. Consequently,
the performance of model editing decreases. To achieve more precise model
editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes
certain general knowledge extraction patterns. This implies that MHSA weights
do not require updating when new knowledge is introduced. Based on above
findings, we introduce PMET, which simultaneously optimizes Transformer
Component (TC, namely MHSA and FFN) hidden states, while only using the
optimized TC hidden states of FFN to precisely update FFN weights. Our
experiments demonstrate that PMET exhibits state-of-the-art performance on both
the COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the
effectiveness of our enhancements, further reinforcing the finding that the
MHSA encodes certain general knowledge extraction patterns and indicating its
storage of a small amount of factual knowledge. Our code is available at
https://github.com/xpq-tech/PMET.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09729">
<div class="article-summary-box-inner">
<span><p>LLMs usually exhibit limitations in their ability to incorporate new
knowledge, the generation of hallucinations, and the transparency of their
decision-making process. In this paper, we explore how to prompt LLMs with
knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date
knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a
prompting pipeline that endows LLMs with the capability of comprehending KG
inputs and inferring with a combined implicit knowledge and the retrieved
external knowledge. In addition, we investigate eliciting the mind map on which
LLMs perform the reasoning and generate the answers. It is identified that the
produced mind map exhibits the reasoning pathways of LLMs grounded on the
ontology of knowledge, hence bringing the prospects of probing and gauging LLM
inference in production. The experiments on three question &amp; answering datasets
also show that MindMap prompting leads to a striking empirical gain. For
instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance
over GPT-4 consistently. We also demonstrate that with structured facts
retrieved from KG, MindMap can outperform a series of
prompting-with-document-retrieval methods, benefiting from more accurate,
concise, and comprehensive knowledge from KGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.09765">
<div class="article-summary-box-inner">
<span><p>Accurately evaluating the similarity of object vector embeddings is of
critical importance for natural language processing, information retrieval and
classification tasks. Popular similarity scores (e.g cosine similarity) are
based on pairs of embedding vectors and disregard the distribution of the
ensemble from which objects are drawn. Human perception of object similarity
significantly depends on the context in which the objects appear. In this work
we propose the $\textit{surprise score}$, an ensemble-normalized similarity
metric that encapsulates the contrast effect of human perception and
significantly improves the classification performance on zero- and few-shot
document classification tasks. This score quantifies the surprise to find a
given similarity between two elements relative to the pairwise ensemble
similarities. We evaluate this metric on zero/few shot classification and
clustering tasks and typically find 10-15 % better performance compared to raw
cosine similarity. Our code is available at
https://github.com/MeetElise/surprise-similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning. (arXiv:2308.10195v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10195">
<div class="article-summary-box-inner">
<span><p>Watermarking serves as a widely adopted approach to safeguard media
copyright. In parallel, the research focus has extended to watermark removal
techniques, offering an adversarial means to enhance watermark robustness and
foster advancements in the watermarking field. Existing watermark removal
methods mainly rely on UNet with task-specific decoder branches--one for
watermark localization and the other for background image restoration. However,
watermark localization and background restoration are not isolated tasks;
precise watermark localization inherently implies regions necessitating
restoration, and the background restoration process contributes to more
accurate watermark localization. To holistically integrate information from
both branches, we introduce an implicit joint learning paradigm. This empowers
the network to autonomously navigate the flow of information between implicit
branches through a gate mechanism. Furthermore, we employ cross-channel
attention to facilitate local detail restoration and holistic structural
comprehension, while harnessing nested structures to integrate multi-scale
information. Extensive experiments are conducted on various challenging
benchmarks to validate the effectiveness of our proposed method. The results
demonstrate our approach's remarkable superiority, surpassing existing
state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10390">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) have demonstrated commendable performance
across a myriad of domains and tasks, existing LLMs still exhibit a palpable
deficit in handling multimodal functionalities, especially for the Spoken
Question Answering (SQA) task which necessitates precise alignment and deep
interaction between speech and text features. To address the SQA challenge on
LLMs, we initially curated the free-form and open-ended LibriSQA dataset from
Librispeech, comprising Part I with natural conversational formats and Part II
encompassing multiple-choice questions followed by answers and analytical
segments. Both parts collectively include 107k SQA pairs that cover various
topics. Given the evident paucity of existing speech-text LLMs, we propose a
lightweight, end-to-end framework to execute the SQA task on the LibriSQA,
witnessing significant results. By reforming ASR into the SQA format, we
further substantiate our framework's capability in handling ASR tasks. Our
empirical findings bolster the LLMs' aptitude for aligning and comprehending
multimodal information, paving the way for the development of universal
multimodal LLMs. The dataset and demo can be found at
https://github.com/ZihanZhaoSJTU/LibriSQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Effective Method using Phrase Mechanism in Neural Machine Translation. (arXiv:2308.10482v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10482">
<div class="article-summary-box-inner">
<span><p>Machine Translation is one of the essential tasks in Natural Language
Processing (NLP), which has massive applications in real life as well as
contributing to other tasks in the NLP research community. Recently,
Transformer -based methods have attracted numerous researchers in this domain
and achieved state-of-the-art results in most of the pair languages. In this
paper, we report an effective method using a phrase mechanism,
PhraseTransformer, to improve the strong baseline model Transformer in
constructing a Neural Machine Translation (NMT) system for parallel corpora
Vietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022
competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2
BLEU scores on Chinese to Vietnamese data. Our code is available at
https://github.com/phuongnm94/PhraseTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models. (arXiv:2308.10755v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10755">
<div class="article-summary-box-inner">
<span><p>The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the
development of large models, leading to the creation of numerous impressive
large language models(LLMs) and multimodal large language models (MLLMs). These
cutting-edge models owe their remarkable performance to high-quality data.
However, the details of the training data used in leading paradigms are often
kept confidential. This lack of transparency, coupled with the scarcity of
open-source data, impedes further developments within the community. As a
response, this paper presents "Wan Juan", a large-scale multimodal dataset
composed of both Chinese and English data, collected from a wide range of web
sources. The dataset incorporates text, image-text, and video modalities, with
a total volume exceeding 2TB. It was utilized in the training of InternLM, a
model that demonstrated significant advantages in multi-dimensional evaluations
when compared to models of a similar scale. All data can be accessed at
https://opendatalab.org.cn/WanJuan1.0.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-23 23:10:58.144940462 UTC">2023-08-23 23:10:58 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>