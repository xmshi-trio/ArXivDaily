<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-20T01:30:00Z">03-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics. (arXiv:2303.09601v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09601">
<div class="article-summary-box-inner">
<span><p>We introduce a Reinforcement Learning Psychotherapy AI Companion that
generates topic recommendations for therapists based on patient responses. The
system uses Deep Reinforcement Learning (DRL) to generate multi-objective
policies for four different psychiatric conditions: anxiety, depression,
schizophrenia, and suicidal cases. We present our experimental results on the
accuracy of recommended topics using three different scales of working alliance
ratings: task, bond, and goal. We show that the system is able to capture the
real data (historical topics discussed by the therapists) relatively well, and
that the best performing models vary by disorder and rating scale. To gain
interpretable insights into the learned policies, we visualize policy
trajectories in a 2D principal component analysis space and transition
matrices. These visualizations reveal distinct patterns in the policies trained
with different reward signals and trained on different clinical diagnoses. Our
system's success in generating DIsorder-Specific Multi-Objective Policies
(DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in
providing personalized and efficient therapeutic recommendations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09618">
<div class="article-summary-box-inner">
<span><p>Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models. (arXiv:2303.09639v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09639">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models have achieved state-of-the-art results on a
variety of downstream tasks. Knowledge Distillation (KD) of a smaller student
model addresses their inefficiency, allowing for deployment in
resource-constraint environments. KD however remains ineffective, as the
student is manually selected from a set of existing options already pre-trained
on large corpora, a sub-optimal choice within the space of all possible student
architectures. This paper proposes KD-NAS, the use of Neural Architecture
Search (NAS) guided by the Knowledge Distillation process to find the optimal
student model for distillation from a teacher, for a given natural language
task. In each episode of the search process, a NAS controller predicts a reward
based on a combination of accuracy on the downstream task and latency of
inference. The top candidate architectures are then distilled from the teacher
on a small proxy set. Finally the architecture(s) with the highest reward is
selected, and distilled on the full downstream task training set. When
distilling on the MNLI task, our KD-NAS model produces a 2 point improvement in
accuracy on GLUE tasks with equivalent GPU latency with respect to a
hand-crafted student architecture available in the literature. Using Knowledge
Distillation, this model also achieves a 1.4x speedup in GPU Latency (3.2x
speedup on CPU) with respect to a BERT-Base Teacher, while maintaining 97%
performance on GLUE Tasks (without CoLA). We also obtain an architecture with
equivalent performance as the hand-crafted student model on the GLUE benchmark,
but with a 15% speedup in GPU latency (20% speedup in CPU latency) and 0.8
times the number of parameters
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos. (arXiv:2303.09713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09713">
<div class="article-summary-box-inner">
<span><p>Visual information is central to conversation: body gestures and facial
expressions, for example, contribute to meaning that transcends words alone. To
date, however, most neural conversational models are limited to just text. We
introduce CHAMPAGNE, a generative model of conversations that can account for
visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a
large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from
web videos: crucial to our data collection pipeline is a pretrained language
model that converts error-prone automatic transcripts to a cleaner dialogue
format while maintaining meaning. Human evaluation reveals that YTD-18M is more
sensible and specific than prior resources (MMDialog, 1M dialogues), while
maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE
learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it
achieves state-of-the-art results on four vision-language tasks focused on
real-world conversations. We release data, models, and code at
https://seungjuhan.me/champagne.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning towards Selective Data Augmentation for Dialogue Generation. (arXiv:2303.09719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09719">
<div class="article-summary-box-inner">
<span><p>As it is cumbersome and expensive to acquire a huge amount of data for
training neural dialog models, data augmentation is proposed to effectively
utilize existing training samples. However, current data augmentation
techniques on the dialog generation task mostly augment all cases in the
training dataset without considering the intrinsic attributes between different
cases. We argue that not all cases are beneficial for augmentation task, and
the cases suitable for augmentation should obey the following two attributes:
(1) low-quality (the dialog model cannot generate a high-quality response for
the case), (2) representative (the case should represent the property of the
whole dataset). Herein, we explore this idea by proposing a Selective Data
Augmentation framework (SDA) for the response generation task. SDA employs a
dual adversarial network to select the lowest quality and most representative
data points for augmentation in one stage. Extensive experiments conducted on
two publicly available datasets, i.e., DailyDialog and OpenSubtitles, show that
our framework can improve the response generation performance with respect to
various metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09752">
<div class="article-summary-box-inner">
<span><p>Many natural language processing tasks benefit from long inputs, but
processing long documents with Transformers is expensive -- not only due to
quadratic attention complexity but also from applying feedforward and
projection layers to every token. However, not all tokens are equally
important, especially for longer documents. We propose CoLT5, a long-input
Transformer model that builds on this intuition by employing conditional
computation, devoting more resources to important tokens in both feedforward
and attention layers. We show that CoLT5 achieves stronger performance than
LongT5 with much faster training and inference, achieving SOTA on the
long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably
make use of extremely long inputs, showing strong gains up to 64k input length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09823">
<div class="article-summary-box-inner">
<span><p>This paper describes our participation in the shared task of hate speech
detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our
experiments evaluate the performance of six transformer models and their
combination using 2 ensemble approaches. The best results on the training set,
in a five-fold cross validation scenario, were obtained by using the ensemble
approach based on the majority vote. The evaluation of this approach on the
test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing. (arXiv:2303.09827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09827">
<div class="article-summary-box-inner">
<span><p>We present our work on Track 2 in the Dialog System Technology Challenges 11
(DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot,
cross-domain, intent-set induction. In the absence of in-domain training
dataset, robust utterance representation that can be used across domains is
necessary to induce users' intentions. To achieve this, we leveraged a
multi-domain dialogue dataset to fine-tune the language model and proposed
extracting Verb-Object pairs to remove the artifacts of unnecessary
information. Furthermore, we devised the method that generates each cluster's
name for the explainability of clustered results. Our approach achieved 3rd
place in the precision score and showed superior accuracy and normalized mutual
information (NMI) score than the baseline model on various domain datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09859">
<div class="article-summary-box-inner">
<span><p>While modern masked language models (LMs) are trained on ever larger corpora,
we here explore the effects of down-scaling training to a modestly-sized but
representative, well-balanced, and publicly available English text source --
the British National Corpus. We show that pre-training on this carefully
curated corpus can reach better performance than the original BERT model. We
argue that this type of corpora has great potential as a language modeling
benchmark. To showcase this potential, we present fair, reproducible and
data-efficient comparative studies of LMs, in which we evaluate several
training objectives and model architectures and replicate previous empirical
results in a systematic way. We propose an optimized LM architecture called
LTG-BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes. (arXiv:2303.09892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09892">
<div class="article-summary-box-inner">
<span><p>Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09901">
<div class="article-summary-box-inner">
<span><p>This paper presents the winning system for the zero-shot Spanish framing
detection task, which also achieves competitive places in eight additional
languages. The challenge of the framing detection task lies in identifying a
set of 14 frames when only a few or zero samples are available, i.e., a
multilingual multi-label few- or zero-shot setting. Our developed solution
employs a pre-training procedure based on multilingual Transformers using a
label-aware contrastive loss function. In addition to describing the system, we
perform an embedding space analysis and ablation study to demonstrate how our
pre-training procedure supports framing detection to advance computational
framing analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking. (arXiv:2303.09905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09905">
<div class="article-summary-box-inner">
<span><p>The schema-guided paradigm overcomes scalability issues inherent in building
task-oriented dialogue (TOD) agents with static ontologies. Instead of
operating on dialogue context alone, agents have access to hierarchical schemas
containing task-relevant natural language descriptions. Fine-tuned language
models excel at schema-guided dialogue state tracking (DST) but are sensitive
to the writing style of the schemas. We explore methods for improving the
robustness of DST models. We propose a framework for generating synthetic
schemas which uses tree-based ranking to jointly optimise lexical diversity and
semantic faithfulness. The generalisation of strong baselines is improved when
augmenting their training data with prompts generated by our framework, as
demonstrated by marked improvements in average joint goal accuracy (JGA) and
schema sensitivity (SS) on the SGD-X benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10093">
<div class="article-summary-box-inner">
<span><p>Vision-language pretraining to learn a fine-grained, region-word alignment
between image-caption pairs has propelled progress in open-vocabulary object
detection. We observe that region-word alignment methods are typically used in
detection with respect to only object nouns, and the impact of other rich
context in captions, such as attributes, is unclear. In this study, we explore
how language context affects downstream object detection and propose to enhance
the role of context. In particular, we show how to strategically contextualize
the grounding pretraining objective for improved alignment. We further hone in
on attributes as especially useful object context and propose a novel adjective
and noun-based negative sampling strategy for increasing their focus in
contrastive learning. Overall, our methods enhance object detection when
compared to the state-of-the-art in region-word pretraining. We also highlight
the fine-grained utility of an attribute-sensitive model through text-region
retrieval and phrase grounding analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct and indirect evidence of compression of word lengths. Zipf's law of abbreviation revisited. (arXiv:2303.10128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10128">
<div class="article-summary-box-inner">
<span><p>Zipf's law of abbreviation, the tendency of more frequent words to be
shorter, is one of the most solid candidates for a linguistic universal, in the
sense that it has the potential for being exceptionless or with a number of
exceptions that is vanishingly small compared to the number of languages on
Earth. Since Zipf's pioneering research, this law has been viewed as a
manifestation of a universal principle of communication, i.e. the minimization
of word lengths, to reduce the effort of communication. Here we revisit the
concordance of written language with the law of abbreviation. Crucially, we
provide wider evidence that the law holds also in speech (when word length is
measured in time), in particular in 46 languages from 14 linguistic families.
Agreement with the law of abbreviation provides indirect evidence of
compression of languages via the theoretical argument that the law of
abbreviation is a prediction of optimal coding. Motivated by the need of direct
evidence of compression, we derive a simple formula for a random baseline
indicating that word lengths are systematically below chance, across linguistic
families and writing systems, and independently of the unit of measurement
(length in characters or duration in time). Our work paves the way to measure
and compare the degree of optimality of word lengths in languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01636">
<div class="article-summary-box-inner">
<span><p>With the fast development of Deep Learning techniques, Named Entity
Recognition (NER) is becoming more and more important in the information
extraction task. The greatest difficulty that the NER task faces is to keep the
detectability even when types of NE and documents are unfamiliar. Realizing
that the specificity information may contain potential meanings of a word and
generate semantic-related features for word embedding, we develop a
distribution-aware word embedding and implement three different methods to make
use of the distribution information in a NER framework. And the result shows
that the performance of NER will be improved if the word specificity is
incorporated into existing NER methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v3 [cs.DS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06924">
<div class="article-summary-box-inner">
<span><p>The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping
$\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers
that maximizes $D(G)=\sum_{uv\in E(G)}|\pi(u) - \pi(v)|$. In this setting,
vertices are considered to lie on a horizontal line and edges are drawn as
semicircles above the line. There exist variants of MaxLA in which the
arrangements are constrained. In the planar variant, edge crossings are
forbidden. In the projective variant for rooted trees, arrangements are planar
and the root cannot be covered by any edge. Here we present $O(n)$-time and
$O(n)$-space algorithms that solve planar and projective MaxLA for trees. We
also prove several properties of maximum projective and planar arrangements,
and show that caterpillar trees maximize planar MaxLA over all trees of a fixed
size thereby generalizing a previous extremal result on trees.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.09015">
<div class="article-summary-box-inner">
<span><p>Standard inference and training with transformer based architectures scale
quadratically with input sequence length. This is prohibitively large for a
variety of applications especially in web-page translation, query-answering
etc. Consequently, several approaches have been developed recently to speedup
attention computation by enforcing different attention structures such as
sparsity, low-rank, approximating attention using kernels. In this work, we
view attention computation as that of nearest neighbor retrieval, and use
decision tree based hierarchical navigation to reduce the retrieval cost per
query token from linear in sequence length to nearly logarithmic. Based on such
hierarchical navigation, we design Treeformer which can use one of two
efficient attention layers -- TF-Attention and TC-Attention. TF-Attention
computes the attention in a fine-grained style, while TC-Attention is a coarse
attention layer which also ensures that the gradients are "dense". To optimize
such challenging discrete layers, we propose a two-level bootstrapped training
method. Using extensive experiments on standard NLP benchmarks, especially for
long-sequences, we demonstrate that our Treeformer architecture can be almost
as accurate as baseline Transformer while using 30x lesser FLOPs in the
attention layer. Compared to Linformer, the accuracy can be as much as 12%
higher while using similar FLOPs in the attention layer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10723">
<div class="article-summary-box-inner">
<span><p>We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12353">
<div class="article-summary-box-inner">
<span><p>While large language models (LLMs) like GPT-3 have achieved impressive
results on multiple choice question answering (MCQA) tasks in the zero, one,
and few-shot settings, they generally lag behind the MCQA state of the art
(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.
An LLM is conditioned on a question (without the associated answer options) and
its chosen option is the one assigned the highest probability after
normalization (for length, etc.). A more natural prompting approach is to
present the question and answer options to the LLM jointly and have it output
the symbol (e.g., "A") associated with its chosen answer option. This approach
allows the model to explicitly compare answer options, reduces computational
costs, and mitigates the effects of tokenization scheme and answer option
representations on answer selection. For the natural approach to be effective,
the LLM it is used with must be able to associate answer options with the
symbols that represent them. The LLM needs what we term multiple choice symbol
binding (MCSB) ability. This ability varies greatly by model. We show that a
model with high MCSB ability performs much better with the natural approach
than with the traditional approach across 20 diverse datasets and largely
closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been
previously underestimated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EW-Tune: A Framework for Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15042">
<div class="article-summary-box-inner">
<span><p>Pre-trained Large Language Models (LLMs) are an integral part of modern AI
that have led to breakthrough performances in complex AI tasks. Major AI
companies with expensive infrastructures are able to develop and train these
large models with billions and millions of parameters from scratch. Third
parties, researchers, and practitioners are increasingly adopting these
pre-trained models and fine-tuning them on their private data to accomplish
their downstream AI tasks. However, it has been shown that an adversary can
extract/reconstruct the exact training samples from these LLMs, which can lead
to revealing personally identifiable information. The issue has raised deep
concerns about the privacy of LLMs. Differential privacy (DP) provides a
rigorous framework that allows adding noise in the process of training or
fine-tuning LLMs such that extracting the training data becomes infeasible
(i.e., with a cryptographically small success probability). While the
theoretical privacy guarantees offered in most extant studies assume learning
models from scratch through many training iterations in an asymptotic setting,
this assumption does not hold in fine-tuning scenarios in which the number of
training iterations is significantly smaller. To address the gap, we present
\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with
finite-sample privacy guarantees. Our results across four well-established
natural language understanding (NLU) tasks show that while \ewtune~adds privacy
guarantees to LLM fine-tuning process, it directly contributes to decreasing
the induced noise to up to 5.6\% and improves the state-of-the-art LLMs
performance by up to 1.1\% across all NLU tasks. We have open-sourced our
implementations for wide adoption and public testing purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder. (arXiv:2211.00792v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00792">
<div class="article-summary-box-inner">
<span><p>We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech
recognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced
encoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR
has been actively studied, aiming to utilize versatile linguistic knowledge for
generating accurate text. One crucial factor that makes this integration
challenging lies in the vocabulary mismatch; the vocabulary constructed for a
pre-trained LM is generally too large for E2E-ASR training and is likely to
have a mismatch against a target ASR domain. To overcome such an issue, we
propose BECTRA, an extended version of our previous BERT-CTC, that realizes
BERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based
model, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder
using a vocabulary suitable for a target task. With the combination of the
transducer and BERT-CTC, we also propose a novel inference algorithm for taking
advantage of both autoregressive and non-autoregressive decoding. Experimental
results on several ASR tasks, varying in amounts of data, speaking styles, and
languages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing
with the vocabulary mismatch while exploiting BERT knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss. (arXiv:2211.00795v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.00795">
<div class="article-summary-box-inner">
<span><p>This paper presents InterMPL, a semi-supervised learning method of end-to-end
automatic speech recognition (ASR) that performs pseudo-labeling (PL) with
intermediate supervision. Momentum PL (MPL) trains a connectionist temporal
classification (CTC)-based model on unlabeled data by continuously generating
pseudo-labels on the fly and improving their quality. In contrast to
autoregressive formulations, such as the attention-based encoder-decoder and
transducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in
general, owing to its simple/fast inference algorithm and robustness against
generating collapsed labels. However, CTC generally yields inferior performance
than the autoregressive models due to the conditional independence assumption,
thereby limiting the performance of MPL. We propose to enhance MPL by
introducing intermediate loss, inspired by the recent advances in CTC-based
modeling. Specifically, we focus on self-conditional and hierarchical
conditional CTC, that apply auxiliary CTC losses to intermediate layers such
that the conditional independence assumption is explicitly relaxed. We also
explore how pseudo-labels should be generated and used as supervision for
intermediate losses. Experimental results in different semi-supervised settings
demonstrate that the proposed approach outperforms MPL and improves an ASR
model by up to a 12.1% absolute performance gain. In addition, our detailed
analysis validates the importance of the intermediate loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03929">
<div class="article-summary-box-inner">
<span><p>Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive successes on downstream tasks, we still have a
limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-3-driven pedagogical agents for training children's curious question-asking skills. (arXiv:2211.14228v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14228">
<div class="article-summary-box-inner">
<span><p>In order to train children's ability to ask curiosity-driven questions,
previous research has explored designing specific exercises relying on
providing semantic and linguistic cues to help formulate such questions. But
despite showing pedagogical efficiency, this method is still limited as it
relies on generating the said cues by hand, which can be a very costly process.
In this context, we propose to leverage advances in the natural language
processing field (NLP) and investigate the efficiency of using a large language
model (LLM) for automating the production of the pedagogical content of a
curious question-asking (QA) training. We study generating the said content
using the "prompt-based" method that consists of explaining the task to the LLM
in natural text. We evaluate the output using human experts annotations and
comparisons with hand-generated content. Results suggested indeed the relevance
and usefulness of this content. We also conduct a field study in primary school
(75 children aged 9-10), where we evaluate children's QA performance when
having this training. We compare 3 types of content : 1) hand-generated content
that proposes "closed" cues leading to predefined questions; 2) GPT-3-generated
content that proposes the same type of cues; 3) GPT-3-generated content that
proposes "open" cues leading to several possible questions. We see a similar QA
performance between the two "closed" trainings (showing the scalability of the
approach using GPT-3), and a better one for participants with the "open"
training. These results suggest the efficiency of using LLMs to support
children in generating more curious questions, using a natural language
prompting approach that affords usability by teachers and other users not
specialists of AI techniques. Furthermore, results also show that open-ended
content may be more suitable for training curious question-asking skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who are you referring to? Coreference resolution in image narrations. (arXiv:2211.14563v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.14563">
<div class="article-summary-box-inner">
<span><p>Coreference resolution aims to identify words and phrases which refer to same
entity in a text, a core task in natural language processing. In this paper, we
extend this task to resolving coreferences in long-form narrations of visual
scenes. First we introduce a new dataset with annotated coreference chains and
their bounding boxes, as most existing image-text datasets only contain short
sentences without coreferring expressions or labeled chains. We propose a new
technique that learns to identify coreference chains using weak supervision,
only from image-text pairs and a regularization using prior linguistic
knowledge. Our model yields large performance gains over several strong
baselines in resolving coreferences. We also show that coreference resolution
helps improving grounding narratives in images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02021">
<div class="article-summary-box-inner">
<span><p>The focus of this work is to investigate unsupervised approaches to overcome
quintessential challenges in designing task-oriented dialog schema: assigning
intent labels to each dialog turn (intent clustering) and generating a set of
intents based on the intent clustering methods (intent induction). We postulate
there are two salient factors for automatic induction of intents: (1)
clustering algorithm for intent labeling and (2) user utterance embedding
space. We compare existing off-the-shelf clustering models and embeddings based
on DSTC11 evaluation. Our extensive experiments demonstrate that the combined
selection of utterance embedding and clustering method in the intent induction
task should be carefully considered. We also present that pretrained MiniLM
with Agglomerative clustering shows significant improvement in NMI, ARI, F1,
accuracy and example coverage in intent induction tasks. The source codes are
available at https://github.com/Jeiyoon/dstc11-track2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks. (arXiv:2212.08489v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08489">
<div class="article-summary-box-inner">
<span><p>In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs, namely
word-consensus-networks, allows the SLU system to improve in comparison to the
1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,
learning from acoustic and text embeddings, obtains performance similar to the
oracle setup, a relative improvement of 17.8% over the 1-best configuration,
being a recommended alternative to overcome the limitations of working with
automatically generated transcripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08756">
<div class="article-summary-box-inner">
<span><p>Machine learning models can reach high performance on benchmark natural
language processing (NLP) datasets but fail in more challenging settings. We
study this issue when a pre-trained model learns dataset artifacts in natural
language inference (NLI), the topic of studying the logical relationship
between a pair of text sequences. We provide a variety of techniques for
analyzing and locating dataset artifacts inside the crowdsourced Stanford
Natural Language Inference (SNLI) corpus. We study the stylistic pattern of
dataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a
unique multi-scale data augmentation technique with two distinct frameworks: a
behavioral testing checklist at the sentence level and lexical synonym criteria
at the word level. Specifically, our combination method enhances our model's
resistance to perturbation testing, enabling it to continuously outperform the
pre-trained baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.08771">
<div class="article-summary-box-inner">
<span><p>Developing models to automatically score students' written responses to
science problems is critical for science education. However, collecting and
labeling sufficient student responses for training models is time and
cost-consuming. Recent studies suggest that pre-trained language models (PLMs)
can be adapted to downstream tasks without fine-tuning with prompts. However,
no research has employed such a prompt approach in science education. As
student responses are presented with natural language, aligning the scoring
procedure as the next sentence prediction task using prompts can skip the
costly fine-tuning stage. In this study, we developed a zero-shot approach to
automatically score student responses via Matching Exemplars as Next Sentence
Prediction (MeNSP). This approach employs no training samples. We first apply
MeNSP in scoring three assessment tasks of scientific argumentation and found
machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and
F1 score ranges from 0.54 to 0.81. To improve the performance, we extend our
research to the few-shots setting, either randomly selecting labeled student
responses or manually constructing responses to fine-tune the models. We find
that one task's performance is improved with more samples, Cohen's Kappa from
0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring
performance is not improved. We also find that randomly selected few-shots
perform better than the human expert-crafted approach. This study suggests that
MeNSP can yield referable automatic scoring for student responses while
significantly reducing the cost of model training. This method can benefit
low-stakes classroom assessment practices in science education. Future research
should further explore the applicability of the MeNSP in different types of
assessment tasks in science education and improve the model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02123">
<div class="article-summary-box-inner">
<span><p>Prior work has attempted to understand the internal structures and
functionalities of Transformer-based encoder-decoder architectures on the level
of multi-head attention and feed-forward sublayers. Interpretations have
focused on the encoder and decoder, along with the combinatorial possibilities
of the self-attention, cross-attention, and feed-forward sublayers. However,
without examining the low-level structures, one gains limited understanding of
the motivation behind sublayer reordering. Could we dive into the sublayer
abstraction and permute layer weight matrices to improve the quality of
translation? We propose AEIUOrder to greedily reorder layer weight matrices in
the encoder by their well-trainedness, as measured by Heavy-Tailed
Self-Regularization (HT-SR) metrics, and order the decoder matrices
correspondingly. Our results suggest that greedily reordering layer weight
matrices to maximize Total well-trainedness facilitates the model to learn
representations and generate translations more effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10186">
<div class="article-summary-box-inner">
<span><p>This paper reimagines some aspects of speech processing using speech
encoders, specifically about extracting entities directly from speech, with no
intermediate textual representation. In human-computer conversations,
extracting entities such as names, postal addresses and email addresses from
speech is a challenging task. In this paper, we study the impact of fine-tuning
pre-trained speech encoders on extracting spoken entities in human-readable
form directly from speech without the need for text transcription. We
illustrate that such a direct approach optimizes the encoder to transcribe only
the entity relevant portions of speech, ignoring the superfluous portions such
as carrier phrases and spellings of entities. In the context of dialogs from an
enterprise virtual agent, we demonstrate that the 1-step approach outperforms
the typical 2-step cascade of first generating lexical transcriptions followed
by text-based entity extraction for identifying spoken entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs. (arXiv:2303.08954v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08954">
<div class="article-summary-box-inner">
<span><p>Research interest in task-oriented dialogs has increased as systems such as
Google Assistant, Alexa and Siri have become ubiquitous in everyday life.
However, the impact of academic research in this area has been limited by the
lack of datasets that realistically capture the wide array of user pain points.
To enable research on some of the more challenging aspects of parsing realistic
conversations, we introduce PRESTO, a public dataset of over 550K contextual
multilingual conversations between humans and virtual assistants. PRESTO
contains a diverse array of challenges that occur in real-world NLU tasks such
as disfluencies, code-switching, and revisions. It is the only large scale
human generated conversational parsing dataset that provides structured context
such as a user's contacts and lists for each example. Our mT5 model based
baselines demonstrate that the conversational phenomenon present in PRESTO are
challenging to model, which is further pronounced in a low-resource setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.09306">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a fundamental task in natural language
processing that involves identifying and classifying named entities in text.
But much work hasn't been done for complex named entity recognition in Bangla,
despite being the seventh most spoken language globally. CNER is a more
challenging task than traditional NER as it involves identifying and
classifying complex and compound entities, which are not common in Bangla
language. In this paper, we present the winning solution of Bangla Complex
Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER
dataset using two different approaches, namely Conditional Random Fields (CRF)
and finetuning transformer based Deep Learning models such as BanglaBERT.
</p>
<p>The dataset consisted of 15300 sentences for training and 800 sentences for
validation, in the .conll format. Exploratory Data Analysis (EDA) on the
dataset revealed that the dataset had 7 different NER tags, with notable
presence of English words, suggesting that the dataset is synthetic and likely
a product of translation.
</p>
<p>We experimented with a variety of feature combinations including Part of
Speech (POS) tags, word suffixes, Gazetteers, and cluster information from
embeddings, while also finetuning the BanglaBERT (large) model for NER. We
found that not all linguistic patterns are immediately apparent or even
intuitive to humans, which is why Deep Learning based models has proved to be
the more effective model in NLP, including CNER task. Our fine tuned BanglaBERT
(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our
study highlights the importance of Bangla Complex Named Entity Recognition,
particularly in the context of synthetic datasets. Our findings also
demonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in
Bangla language.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-20 23:11:57.895200032 UTC">2023-03-20 23:11:57 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>