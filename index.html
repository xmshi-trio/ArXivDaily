<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-10-10T01:30:00Z">10-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Brief History of Prompt: Leveraging Language Models. (arXiv:2310.04438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04438">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive exploration of the evolution of prompt
engineering and generation in the field of natural language processing (NLP).
Starting from the early language models and information retrieval systems, we
trace the key developments that have shaped prompt engineering over the years.
The introduction of attention mechanisms in 2015 revolutionized language
understanding, leading to advancements in controllability and
context-awareness. Subsequent breakthroughs in reinforcement learning
techniques further enhanced prompt engineering, addressing issues like exposure
bias and biases in generated text. We examine the significant contributions in
2018 and 2019, focusing on fine-tuning strategies, control codes, and
template-based generation. The paper also discusses the growing importance of
fairness, human-AI collaboration, and low-resource adaptation. In 2020 and
2021, contextual prompting and transfer learning gained prominence, while 2022
and 2023 witnessed the emergence of advanced techniques like unsupervised
pre-training and novel reward shaping. Throughout the paper, we reference
specific research studies that exemplify the impact of various developments on
prompt engineering. The journey of prompt engineering continues, with ethical
considerations being paramount for the responsible and inclusive future of AI
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04443">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) systems have attracted much attention from the
artificial intelligence community as they can learn to answer questions based
on the given knowledge source (e.g., images in visual question answering).
However, the research into question answering systems with human mobility data
remains unexplored. Mining human mobility data is crucial for various
applications such as smart city planning, pandemic management, and personalised
recommendation system. In this paper, we aim to tackle this gap and introduce a
novel task, that is, human mobility question answering (MobQA). The aim of the
task is to let the intelligent system learn from mobility data and answer
related questions. This task presents a new paradigm change in mobility
prediction research and further facilitates the research of human mobility
recommendation systems. To better support this novel research topic, this
vision paper also proposes an initial design of the dataset and a potential
deep learning model framework for the introduced MobQA task. We hope that this
paper will provide novel insights and open new directions in human mobility
research and question answering research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04444">
<div class="article-summary-box-inner">
<span><p>Prompt engineering is effective and important in the deployment of LLMs but
is poorly understood mathematically. Here, we formalize prompt engineering as
an optimal control problem on LLMs -- where the prompt is considered a control
variable for modulating the output distribution of the LLM. Within this
framework, we ask a simple question: given a sequence of tokens, does there
always exist a prompt we can prepend that will steer the LLM toward accurately
predicting the final token? We call such an optimal prompt the magic word since
prepending the prompt causes the LLM to output the correct answer. If magic
words exist, can we find them? If so, what are their properties? We offer
analytic analysis on the controllability of the self-attention head where we
prove a bound on controllability as a function of the singular values of its
weight matrices. We take inspiration from control theory to propose a metric
called $k-\epsilon$ controllability to characterize LLM steerability. We
compute the $k-\epsilon$ controllability of a panel of large language models,
including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language
modeling tasks. Remarkably, we find that magic words of 10 tokens or less exist
for over 97% of WikiText instances surveyed for each model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04445">
<div class="article-summary-box-inner">
<span><p>It has been shown that Large Language Model (LLM) alignments can be
circumvented by appending specially crafted attack suffixes with harmful
queries to elicit harmful responses. To conduct attacks against private target
models whose characterization is unknown, public models can be used as proxies
to fashion the attack, with successful attacks being transferred from public
proxies to private target models. The success rate of attack depends on how
closely the proxy model approximates the private model. We hypothesize that for
attacks to be transferrable, it is sufficient if the proxy can approximate the
target model in the neighborhood of the harmful query. Therefore, in this
paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning
proxy models on similar queries that lie in the lexico-semantic neighborhood of
harmful queries to decrease the divergence between the proxy and target models.
First, we demonstrate three approaches to prompt private target models to
obtain similar queries given harmful queries. Next, we obtain data for local
fine-tuning by eliciting responses from target models for the generated similar
queries. Then, we optimize attack suffixes to generate attack prompts and
evaluate the impact of our local fine-tuning on the attack's success rate.
Experiments show that local fine-tuning of proxy models improves attack
transferability and increases attack success rate by $39\%$, $7\%$, and $0.5\%$
(absolute) on target models ChatGPT, GPT-4, and Claude respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Large Language Models' Perception of Emotion Using Appraisal Theory. (arXiv:2310.04450v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04450">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLM) like ChatGPT have significantly advanced in
recent years and are now being used by the general public. As more people
interact with these systems, improving our understanding of these black box
models is crucial, especially regarding their understanding of human
psychological aspects. In this work, we investigate their emotion perception
through the lens of appraisal and coping theory using the Stress and Coping
Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting
of multiple stories that evolve over time and differ in key appraisal variables
such as controllability and changeability. We applied SCPQ to three recent LLMs
from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with
predictions from the appraisal theory and human data. The results show that
LLMs' responses are similar to humans in terms of dynamics of appraisal and
coping, but their responses did not differ along key appraisal dimensions as
predicted by the theory and data. The magnitude of their responses is also
quite different from humans in several variables. We also found that GPTs can
be quite sensitive to instruction and how questions are asked. This work adds
to the growing literature evaluating the psychological aspects of LLMs and
helps enrich our understanding of the current models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. (arXiv:2310.04451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04451">
<div class="article-summary-box-inner">
<span><p>The aligned Large Language Models (LLMs) are powerful language understanding
and decision-making tools that are created through extensive alignment with
human feedback. However, these large models remain susceptible to jailbreak
attacks, where adversaries manipulate prompts to elicit malicious outputs that
should not be given by aligned LLMs. Investigating jailbreak prompts can lead
us to delve into the limitations of LLMs and further guide us to secure them.
Unfortunately, existing jailbreak techniques suffer from either (1) scalability
issues, where attacks heavily rely on manual crafting of prompts, or (2)
stealthiness problems, as attacks depend on token-based algorithms to generate
prompts that are often semantically meaningless, making them susceptible to
detection through basic perplexity testing. In light of these challenges, we
intend to answer this question: Can we develop an approach that can
automatically generate stealthy jailbreak prompts? In this paper, we introduce
AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can
automatically generate stealthy jailbreak prompts by the carefully designed
hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN
not only automates the process while preserving semantic meaningfulness, but
also demonstrates superior attack strength in cross-model transferability, and
cross-sample universality compared with the baseline. Moreover, we also compare
AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass
them effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short text classification with machine learning in the social sciences: The case of climate change on Twitter. (arXiv:2310.04452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04452">
<div class="article-summary-box-inner">
<span><p>To analyse large numbers of texts, social science researchers are
increasingly confronting the challenge of text classification. When manual
labeling is not possible and researchers have to find automatized ways to
classify texts, computer science provides a useful toolbox of machine-learning
methods whose performance remains understudied in the social sciences. In this
article, we compare the performance of the most widely used text classifiers by
applying them to a typical research scenario in social science research: a
relatively small labeled dataset with infrequent occurrence of categories of
interest, which is a part of a large unlabeled dataset. As an example case, we
look at Twitter communication regarding climate change, a topic of increasing
scholarly interest in interdisciplinary social science research. Using a novel
dataset including 5,750 tweets from various international organizations
regarding the highly ambiguous concept of climate change, we evaluate the
performance of methods in automatically classifying tweets based on whether
they are about climate change or not. In this context, we highlight two main
findings. First, supervised machine-learning methods perform better than
state-of-the-art lexicons, in particular as class balance increases. Second,
traditional machine-learning methods, such as logistic regression and random
forest, perform similarly to sophisticated deep-learning methods, whilst
requiring much less training time and computational resources. The results have
important implications for the analysis of short texts in social science
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets. (arXiv:2310.04453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04453">
<div class="article-summary-box-inner">
<span><p>Very large numbers of M-pox cases have, since the start of May 2022, been
reported in non-endemic countries leading many to fear that the M-pox Outbreak
would rapidly transition into another pandemic, while the COVID-19 pandemic
ravages on. Given the similarities of M-pox with COVID-19, we chose to test the
performance of COVID-19 models trained on South African twitter data on a
hand-labelled M-pox dataset before and after fine-tuning. More than 20k
M-pox-related tweets from South Africa were hand-labelled as being either
positive, negative or neutral. After fine-tuning these COVID-19 models on the
M-pox dataset, the F1-scores increased by more than 8% falling just short of
70%, but still outperforming state-of-the-art models and well-known
classification algorithms. An LDA-based topic modelling procedure was used to
compare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model
with its fine-tuned version, and from this analysis, we were able to draw
conclusions on how to build more sophisticated models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On migration to Perpetual Enterprise System. (arXiv:2104.04844v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04844">
<div class="article-summary-box-inner">
<span><p>This document describes a pragmatic approach on how to migrate an
organisation computer system towards a new system that could evolve forever,
addresses the whole organisation and it is integrated.
</p>
<p>Governance aspects are as important, if not more, than purely technical IT
aspects: human resources, call for tenders, and similar. Migration implies that
one is not starting from a green field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02605">
<div class="article-summary-box-inner">
<span><p>The representation learning on textual graph is to generate low-dimensional
embeddings for the nodes based on the individual textual features and the
neighbourhood information. Recent breakthroughs on pretrained language models
and graph neural networks push forward the development of corresponding
techniques. The existing works mainly rely on the cascaded model architecture:
the textual features of nodes are independently encoded by language models at
first; the textual embeddings are aggregated by graph neural networks
afterwards. However, the above architecture is limited due to the independent
modeling of textual features. In this work, we propose GraphFormers, where
layerwise GNN components are nested alongside the transformer blocks of
language models. With the proposed architecture, the text encoding and the
graph aggregation are fused into an iterative workflow, {making} each node's
semantic accurately comprehended from the global perspective. In addition, a
{progressive} learning strategy is introduced, where the model is successively
trained on manipulated data and original data to reinforce its capability of
integrating information on graph. Extensive evaluations are conducted on three
large-scale benchmark datasets, where GraphFormers outperform the SOTA
baselines with comparable running efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01810">
<div class="article-summary-box-inner">
<span><p>The frustratingly fragile nature of neural network models make current
natural language generation (NLG) systems prone to backdoor attacks and
generate malicious sequences that could be sexist or offensive. Unfortunately,
little effort has been invested to how backdoor attacks can affect current NLG
models and how to defend against these attacks. In this work, by giving a
formal definition of backdoor attack and defense, we investigate this problem
on two important NLG tasks, machine translation and dialog generation. Tailored
to the inherent nature of NLG models (e.g., producing a sequence of coherent
words given contexts), we design defending strategies against attacks. We find
that testing the backward probability of generating sources given targets
yields effective defense performance against all different types of attacks,
and is able to handle the {\it one-to-many} issue in many NLG tasks such as
dialog generation. We hope that this work can raise the awareness of backdoor
risks concealed in deep NLG systems and inspire more future work (both attack
and defense) towards this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10314">
<div class="article-summary-box-inner">
<span><p>We introduce small-text, an easy-to-use active learning library, which offers
pool-based active learning for single- and multi-label text classification in
Python. It features numerous pre-implemented state-of-the-art query strategies,
including some that leverage the GPU. Standardized interfaces allow the
combination of a variety of classifiers, query strategies, and stopping
criteria, facilitating a quick mix and match, and enabling a rapid and
convenient development of both active learning experiments and applications.
With the objective of making various classifiers and query strategies
accessible for active learning, small-text integrates several well-known
machine learning libraries, namely scikit-learn, PyTorch, and Hugging Face
transformers. The latter integrations are optionally installable extensions, so
GPUs can be used but are not required. Using this new library, we investigate
the performance of the recently published SetFit training paradigm, which we
compare to vanilla transformer fine-tuning, finding that it matches the latter
in classification accuracy while outperforming it in area under the curve. The
library is available under the MIT License at
https://github.com/webis-de/small-text, in version 1.3.0 at the time of
writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters. (arXiv:2211.06869v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06869">
<div class="article-summary-box-inner">
<span><p>In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT
and GPT4 have demonstrated immense potential in constructing open-domain
dialogue agents. However, aligning these agents with specific characters or
individuals remains a considerable challenge due to the complexities of
character representation and the lack of comprehensive annotations. In this
paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to
advance the study of dialogue agents and character alignment. The dataset
encompasses all dialogue sessions (in both English and Chinese) from the Harry
Potter series and is annotated with vital background information, including
dialogue scenes, speakers, character relationships, and attributes. These
extensive annotations may empower LLMs to unlock character-driven dialogue
capabilities. Furthermore, it can serve as a universal benchmark for evaluating
how well can a LLM aligning with a specific character. We benchmark LLMs on HPD
using both fine-tuning and in-context learning settings. Evaluation results
reveal that although there is substantial room for improvement in generating
high-quality, character-aligned responses, the proposed dataset is valuable in
guiding models toward responses that better align with the character of Harry
Potter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Similarity Models for Depression Severity Estimation. (arXiv:2211.07624v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.07624">
<div class="article-summary-box-inner">
<span><p>Depressive disorders constitute a severe public health issue worldwide.
However, public health systems have limited capacity for case detection and
diagnosis. In this regard, the widespread use of social media has opened up a
way to access public information on a large scale. Computational methods can
serve as support tools for rapid screening by exploiting this user-generated
social media content. This paper presents an efficient semantic pipeline to
study depression severity in individuals based on their social media writings.
We select test user sentences for producing semantic rankings over an index of
representative training sentences corresponding to depressive symptoms and
severity levels. Then, we use the sentences from those results as evidence for
predicting users' symptom severity. For that, we explore different aggregation
methods to answer one of four Beck Depression Inventory (BDI) options per
symptom. We evaluate our methods on two Reddit-based benchmarks, achieving 30\%
improvement over state of the art in terms of measuring depression severity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Extraction Attack against Self-supervised Speech Models. (arXiv:2211.16044v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16044">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) speech models generate meaningful
representations of given clips and achieve incredible performance across
various downstream tasks. Model extraction attack (MEA) often refers to an
adversary stealing the functionality of the victim model with only query
access. In this work, we study the MEA problem against SSL speech model with a
small number of queries. We propose a two-stage framework to extract the model.
In the first stage, SSL is conducted on the large-scale unlabeled corpus to
pre-train a small speech model. Secondly, we actively sample a small portion of
clips from the unlabeled corpus and query the target model with these clips to
acquire their representations as labels for the small model's second-stage
training. Experiment results show that our sampling methods can effectively
extract the target model without knowing any information about its model
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00068">
<div class="article-summary-box-inner">
<span><p>Learning to predict masked tokens in a sequence has been shown to be a
powerful pretraining objective for large language models. After training, such
masked language models can provide distributions of tokens conditioned on
bidirectional context.
</p>
<p>In this paper, we show that contrary to popular assumptions, such
bidirectional conditionals often demonstrate considerable inconsistencies,
i.e., they cannot be derived from a coherent joint distribution when considered
together. We empirically quantify such inconsistencies in the simple scenario
of bigram comparison for two common styles of masked language models: T5-style
and BERT-style. For example, we show that T5 models often confuse their own
preference regarding two similar bigrams. We show that inconsistencies exist
ubiquitously in masked language models of diverse sizes and configurations,
from RoBERTa-base to GLM-130B.
</p>
<p>As an initial attempt to address this issue during the inference phase, we
propose Ensemble of Conditionals, a self-ensemble algorithm that jointly
considers many inconsistent conditionals directly produced by the MLM to
synthesize a distribution that is used as the model's final output. Such
ensembling improves open-source SOTA results on LAMBADA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04054">
<div class="article-summary-box-inner">
<span><p>Reliability of machine learning evaluation -- the consistency of observed
evaluation scores across replicated model training runs -- is affected by
several sources of nondeterminism which can be regarded as measurement noise.
Current tendencies to remove noise in order to enforce reproducibility of
research results neglect inherent nondeterminism at the implementation level
and disregard crucial interaction effects between algorithmic noise factors and
data properties. This limits the scope of conclusions that can be drawn from
such experiments. Instead of removing noise, we propose to incorporate several
sources of variance, including their interaction with data properties, into an
analysis of significance and reliability of machine learning evaluation, with
the aim to draw inferences beyond particular instances of trained models. We
show how to use linear mixed effects models (LMEMs) to analyze performance
evaluation scores, and to conduct statistical inference with a generalized
likelihood ratio test (GLRT). This allows us to incorporate arbitrary sources
of noise like meta-parameter variations into statistical significance testing,
and to assess performance differences conditional on data properties.
Furthermore, a variance component analysis (VCA) enables the analysis of the
contribution of noise sources to overall variance and the computation of a
reliability coefficient by the ratio of substantial to total variance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.06674">
<div class="article-summary-box-inner">
<span><p>Identifying relevant persona or knowledge for conversational systems is
critical to grounded dialogue response generation. However, each grounding has
been mostly researched in isolation with more practical multi-context dialogue
tasks introduced in recent works. We define Persona and Knowledge Dual Context
Identification as the task to identify persona and knowledge jointly for a
given dialogue, which could be of elevated importance in complex multi-context
dialogue settings. We develop a novel grounding retrieval method that utilizes
all contexts of dialogue simultaneously. Our method requires less computational
power via utilizing neural QA retrieval models. We further introduce our novel
null-positive rank test which measures ranking performance on semantically
dissimilar samples (i.e. hard negatives) in relation to data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Response Generation for Chinese Reading Comprehension. (arXiv:2302.08817v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.08817">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension (MRC) is an important area of conversation
agents and draws a lot of attention. However, there is a notable limitation to
current MRC benchmarks: The labeled answers are mostly either spans extracted
from the target corpus or the choices of the given candidates, ignoring the
natural aspect of high-quality responses. As a result, MRC models trained on
these datasets can not generate human-like responses in real QA scenarios. To
this end, we construct a new dataset called Penguin to promote the research of
MRC, providing a training and test bed for natural response generation to real
scenarios. Concretely, Penguin consists of 200k training data with high-quality
fluent, and well-informed responses. Penguin is the first benchmark towards
natural response generation in Chinese MRC on a relatively large scale. To
address the challenges in Penguin, we develop two strong baselines: end-to-end
and two-stage frameworks. Following that, we further design Prompt-BART:
fine-tuning the pre-trained generative language models with a mixture of prefix
prompts in Penguin. Extensive experiments validated the effectiveness of this
design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RETVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.09207">
<div class="article-summary-box-inner">
<span><p>This paper describes RETVec, an efficient, resilient, and multilingual text
vectorizer designed for neural-based text processing. RETVec combines a novel
character encoding with an optional small embedding model to embed words into a
256-dimensional vector space. The RETVec embedding model is pre-trained using
pair-wise metric learning to be robust against typos and character-level
adversarial attacks. In this paper, we evaluate and compare RETVec to
state-of-the-art vectorizers and word embeddings on popular model architectures
and datasets. These comparisons demonstrate that RETVec leads to competitive,
multilingual models that are significantly more resilient to typos and
adversarial text attacks. RETVec is available under the Apache 2 license at
https://github.com/google-research/retvec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11713">
<div class="article-summary-box-inner">
<span><p>Pre-trained vision and language models have demonstrated state-of-the-art
capabilities over existing tasks involving images and texts, including visual
question answering. However, it remains unclear whether these models possess
the capability to answer questions that are not only querying visual content
but knowledge-intensive and information-seeking. In this study, we introduce
InfoSeek, a visual question answering dataset tailored for information-seeking
questions that cannot be answered with only common sense knowledge. Using
InfoSeek, we analyze various pre-trained visual question answering models and
gain insights into their characteristics. Our findings reveal that
state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)
face challenges in answering visual information-seeking questions, but
fine-tuning on the InfoSeek dataset elicits models to use fine-grained
knowledge that was learned during their pre-training. Furthermore, we show that
accurate visual entity recognition can be used to improve performance on
InfoSeek by retrieving relevant documents, showing a significant space for
improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Support Examples for In-Context Learning. (arXiv:2302.13539v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13539">
<div class="article-summary-box-inner">
<span><p>Additionally, the strong dependency among in-context examples makes it an
NP-hard combinatorial optimization problem and enumerating all permutations is
infeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this
challenge in two stages: First we filter the dataset to obtain informative
in-context examples individually. Specifically, we propose a novel metric,
InfoScore, to evaluate the example's in-context informativeness based on the
language model's feedback, and further propose a progressive filtering process
to filter out uninformative examples. Then we propose diversity-guided example
search which iteratively refines and evaluates the selected example
permutations, to find examples that fully depict the task. The experimental
results show that LENS significantly outperforms a wide range of baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OptBA: Optimizing Hyperparameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08021">
<div class="article-summary-box-inner">
<span><p>One of the challenges that artificial intelligence engineers face,
specifically in the field of deep learning is obtaining the optimal model
hyperparameters. The search for optimal hyperparameters usually hinders the
progress of solutions to real-world problems such as healthcare. To overcome
this hurdle, the proposed work introduces a novel mechanism called ``OptBA" to
automatically fine-tune the hyperparameters of deep learning models by
leveraging the Bees Algorithm, which is a recent promising swarm intelligence
algorithm. In this paper, the optimization problem of OptBA is to maximize the
accuracy in classifying ailments using medical text, where initial
hyperparameters are iteratively adjusted by specific criteria. Experimental
results demonstrate a noteworthy enhancement in accuracy with approximately
1.4%. This outcome highlights the effectiveness of the proposed mechanism in
addressing the critical issue of hyperparameter optimization and its potential
impact on advancing solutions for healthcare and other societal challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaScore: Story Evaluation with Perturbations. (arXiv:2303.08991v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08991">
<div class="article-summary-box-inner">
<span><p>Numerous evaluation metrics have been developed for natural language
generation tasks, but their effectiveness in evaluating stories is limited as
they are not specifically tailored to assess intricate aspects of storytelling,
such as fluency and interestingness. In this paper, we introduce DELTASCORE, a
novel methodology that employs perturbation techniques for the evaluation of
nuanced story aspects. Our central proposition posits that the extent to which
a story excels in a specific aspect (e.g., fluency) correlates with the
magnitude of its susceptibility to particular perturbations (e.g., the
introduction of typos). Given this, we measure the quality of an aspect by
calculating the likelihood difference between pre- and post-perturbation states
using pre-trained language models. We compare DELTASCORE with existing metrics
on storytelling datasets from two domains in five fine-grained story aspects:
fluency, coherence, relatedness, logicality, and interestingness. DELTASCORE
demonstrates remarkable performance, revealing a surprising finding that a
specific perturbation proves highly effective in capturing multiple aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieving Multimodal Information for Augmented Generation: A Survey. (arXiv:2303.10868v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10868">
<div class="article-summary-box-inner">
<span><p>As Large Language Models (LLMs) become popular, there emerged an important
trend of using multimodality to augment the LLMs' generation ability, which
enables LLMs to better interact with the world. However, there lacks a unified
perception of at which stage and how to incorporate different modalities. In
this survey, we review methods that assist and augment generative models by
retrieving multimodal knowledge, whose formats range from images, codes,
tables, graphs, to audio. Such methods offer a promising solution to important
concerns such as factuality, reasoning, interpretability, and robustness. By
providing an in-depth review, this survey is expected to provide scholars with
a deeper understanding of the methods' applications and encourage them to adapt
existing techniques to the fast-growing field of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does Transformer model evolve to learn diverse chemical structures?. (arXiv:2303.11593v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.11593">
<div class="article-summary-box-inner">
<span><p>Recent years have seen rapid development of descriptor generation based on
representation learning of extremely diverse molecules, especially those that
apply natural language processing (NLP) models to SMILES, a literal
representation of molecular structure. However, little research has been done
on how these models understand chemical structure. To address this black box,
we investigated the relationship between the learning progress of SMILES and
chemical structure using a representative NLP model, the Transformer. The
results suggest that while the Transformer learns partial structures of
molecules quickly, it requires extended training to understand overall
structures. Consistently, the accuracy of molecular property predictions using
descriptors generated from models at different learning steps was similar from
the beginning to the end of training. Furthermore, we found that the
Transformer requires particularly long training to learn chirality and
sometimes stagnates with low translation accuracy due to misunderstanding of
enantiomers. These findings are expected to deepen the understanding of NLP
models in chemistry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13809">
<div class="article-summary-box-inner">
<span><p>Generative large language models (LLMs), e.g., ChatGPT, have demonstrated
remarkable proficiency across several NLP tasks, such as machine translation,
text summarization. Recent research (Kocmi and Federmann, 2023) has shown that
utilizing ChatGPT for assessing the quality of machine translation (MT)
achieves state-of-the-art performance at the system level but performs poorly
at the segment level. To further improve the performance of LLMs on MT quality
assessment, we conduct an investigation into several prompting methods, and
propose a new prompting method called Error Analysis Prompting (EAPrompt) by
combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al.,
2022). Our results on WMT22 indicate that prompting LLMs like ChatGPT with
error analysis can generate human-like MT evaluations at both the system and
segment level. Additionally, we first discover some limitations of ChatGPT as
an MT evaluator, such as changing the order of input may significantly
influence the judgment when providing multiple translations in a single query.
This work provides a preliminary experience of prompting LLMs as an evaluator
to improve the reliability of translation evaluation metrics under the error
analysis paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation. (arXiv:2303.15413v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15413">
<div class="article-summary-box-inner">
<span><p>Existing score-distilling text-to-3D generation techniques, despite their
considerable promise, often encounter the view inconsistency problem. One of
the most notable issues is the Janus problem, where the most canonical view of
an object (\textit{e.g}., face or head) appears in other views. In this work,
we explore existing frameworks for score-distilling text-to-3D generation and
identify the main causes of the view inconsistency problem -- the embedded bias
of 2D diffusion models. Based on these findings, we propose two approaches to
debias the score-distillation frameworks for view-consistent text-to-3D
generation. Our first approach, called score debiasing, involves cutting off
the score estimated by 2D diffusion models and gradually increasing the
truncation value throughout the optimization process. Our second approach,
called prompt debiasing, identifies conflicting words between user prompts and
view prompts using a language model, and adjusts the discrepancy between view
prompts and the viewing direction of an object. Our experimental results show
that our methods improve the realism of the generated 3D objects by
significantly reducing artifacts and achieve a good trade-off between
faithfulness to the 2D diffusion models and 3D consistency with little
overhead. Our project page is available
at~\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15714">
<div class="article-summary-box-inner">
<span><p>Language models have been shown to perform remarkably well on a wide range of
natural language processing tasks. In this paper, we propose LEAP, a novel
system that uses language models to perform multi-step logical reasoning and
incorporates explicit planning into the inference procedure. Explicit planning
enables the system to make more informed reasoning decisions at each step by
looking ahead into their future effects. Moreover, we propose a training
strategy that safeguards the planning process from being led astray by spurious
features. Our full system significantly outperforms other competing methods on
multiple standard datasets. When using small T5 models as its core selection
and deduction components, our system performs competitively compared to GPT-3
despite having only about 1B parameters (i.e., 175 times smaller than GPT-3).
When using GPT-3.5, it significantly outperforms chain-of-thought prompting on
the challenging PrOntoQA dataset. We have conducted extensive empirical studies
to demonstrate that explicit planning plays a crucial role in the system's
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01933">
<div class="article-summary-box-inner">
<span><p>The success of large language models (LLMs), like GPT-4 and ChatGPT, has led
to the development of numerous cost-effective and accessible alternatives that
are created by finetuning open-access LLMs with task-specific data (e.g.,
ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning
methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly
one of the most attractive topics, as it only requires fine-tuning a few
external parameters instead of the entire LLMs while achieving comparable or
even better performance. To enable further research on PEFT methods of LLMs,
this paper presents LLM-Adapters, an easy-to-use framework that integrates
various adapters into LLMs and can execute these adapter-based PEFT methods of
LLMs for different tasks. The framework includes state-of-the-art open-access
LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as
Series adapters, Parallel adapter, Prompt-based learning and
Reparametrization-based methods. Moreover, we conduct extensive empirical
studies on the impact of adapter types, placement locations, and
hyper-parameters to the best design for each adapter-based methods. We evaluate
the effectiveness of the adapters on fourteen datasets from two different
reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results
demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few
extra trainable parameters yields comparable, and in some cases superior,
performance to powerful LLMs (175B) in zero-shot inference on both reasoning
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03548">
<div class="article-summary-box-inner">
<span><p>Human experts write summaries using different techniques, including
extracting a sentence from the document and rewriting it, or fusing various
information from the document to abstract it. These techniques are flexible and
thus difficult to be imitated by any single method. To address this issue, we
propose an adaptive model, GEMINI, that integrates a rewriter and a generator
to mimic the sentence rewriting and abstracting techniques, respectively.
GEMINI adaptively chooses to rewrite a specific document sentence or generate a
summary sentence from scratch. Experiments demonstrate that our adaptive
approach outperforms the pure abstractive and rewriting baselines on three
benchmark datasets, achieving the best results on WikiHow. Interestingly,
empirical results show that the human summary styles of summary sentences are
consistently predictable given their context. We release our code and model at
\url{https://github.com/baoguangsheng/gemini}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition. (arXiv:2304.04704v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04704">
<div class="article-summary-box-inner">
<span><p>This work proposes POMP, a prompt pre-training method for vision-language
models. Being memory and computation efficient, POMP enables the learned prompt
to condense semantic information for a rich set of visual concepts with over
twenty-thousand classes. Once pre-trained, the prompt with a strong
transferable ability can be directly plugged into a variety of visual
recognition tasks including image classification, semantic segmentation, and
object detection, to boost recognition performances in a zero-shot manner.
Empirical evaluation shows that POMP achieves state-of-the-art performances on
21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1%
compared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation
(+6.9 compared to ZSSeg). Our code is available at
https://github.com/amazon-science/prompt-pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05302">
<div class="article-summary-box-inner">
<span><p>Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment
of large language models with human preferences, significantly enhancing the
quality of interactions between humans and models. InstructGPT implements RLHF
through several stages, including Supervised Fine-Tuning (SFT), reward model
training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to
hyperparameters and requires multiple models in its standard implementation,
making it hard to train and scale up to larger parameter counts. In contrast,
we propose a novel learning paradigm called RRHF, which scores sampled
responses from different sources via a logarithm of conditional probabilities
and learns to align these probabilities with human preferences through ranking
loss. RRHF can leverage sampled responses from various sources including the
model responses from itself, other large language model responses, and human
expert responses to learn to rank them. RRHF only needs 1 to 2 models during
tuning and can efficiently align language models with human preferences
robustly without complex hyperparameter tuning. Additionally, RRHF can be
considered an extension of SFT and reward model training while being simpler
than PPO in terms of coding, model counts, and hyperparameters. We evaluate
RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment
performance with PPO by reward model score and human labeling. Extensive
experiments show that the performance of RRHF is highly related to sampling
quality which suggests RRHF is a best-of-n learner. Codes available at
https://github.com/GanjinZero/RRHF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10428">
<div class="article-summary-box-inner">
<span><p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA
performances on a variety of NLP tasks, its performance on NER is still
significantly below supervised baselines. This is due to the gap between the
two tasks the NER and LLMs: the former is a sequence labeling task in nature
while the latter is a text-generation model.
</p>
<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the
gap by transforming the sequence labeling task to a generation task that can be
easily adapted by LLMs e.g., the task of finding location entities in the input
text "Columbus is a city" is transformed to generate the text sequence
"@@Columbus## is a city", where special tokens @@## marks the entity to
extract. To efficiently address the "hallucination" issue of LLMs, where LLMs
have a strong inclination to over-confidently label NULL inputs as entities, we
propose a self-verification strategy by prompting LLMs to ask itself whether
the extracted entities belong to a labeled entity tag.
</p>
<p>We conduct experiments on five widely adopted NER datasets, and GPT-NER
achieves comparable performances to fully supervised baselines, which is the
first time as far as we are concerned. More importantly, we find that GPT-NER
exhibits a greater ability in the low-resource and few-shot setups, when the
amount of training data is extremely scarce, GPT-NER performs significantly
better than supervised models. This demonstrates the capabilities of GPT-NER in
real-world NER applications where the number of labeled examples is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation. (arXiv:2305.01498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01498">
<div class="article-summary-box-inner">
<span><p>We present PeerSum, a novel dataset for generating meta-reviews of scientific
papers. The meta-reviews can be interpreted as abstractive summaries of
reviews, multi-turn discussions and the paper abstract. These source documents
have rich inter-document relationships with an explicit hierarchical
conversational structure, cross-references and (occasionally) conflicting
information. To introduce the structural inductive bias into pre-trained
language models, we introduce Rammer ( Relationship-aware Multi-task
Meta-review Generator), a model that uses sparse attention based on the
conversational structure and a multi-task training objective that predicts
metadata features (e.g., review ratings). Our experimental results show that
Rammer outperforms other strong baseline models in terms of a suite of
automatic evaluation metrics. Further analyses, however, reveal that RAMMER and
other models struggle to handle conflicts in source documents of PeerSum,
suggesting meta-review generation is a challenging task and a promising avenue
for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoT: Memory-of-Thought Enables ChatGPT to Self-Improve. (arXiv:2305.05181v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05181">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown impressive abilities in various
tasks. However, fundamentally improving them depends on high-quality datasets
or computationally expensive fine-tuning. On the contrary, humans can easily
improve themselves by self-thinking and memory, without external resources. In
this paper, we propose a framework, MoT, to let the LLM self-improve through
Memory-of-Thought, without annotated datasets and parameter updates.
Specifically, MoT is divided into two stages: 1. before the test stage, the LLM
pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as
external memory; 2. During the test stage, given a test question, the LLM
recalls relevant memory to help itself reason and answer it. Experimental
results show that MoT can help ChatGPT significantly improve its abilities in
arithmetic reasoning, commonsense reasoning, factual reasoning, and natural
language inference. Further analyses show that each component contributes
critically to the improvements and MoT can lead to consistent improvements
across various CoT methods and LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Person or Entity-centric Knowledge Graphs: An Application in Healthcare. (arXiv:2305.05640v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05640">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs (KGs) are a popular way to organise information based on
ontologies or schemas and have been used across a variety of scenarios from
search to recommendation. Despite advances in KGs, representing knowledge
remains a non-trivial task across industries and it is especially challenging
in the biomedical and healthcare domains due to complex interdependent
relations between entities, heterogeneity, lack of standardization, and
sparseness of data. KGs are used to discover diagnoses or prioritize genes
relevant to disease, but they often rely on schemas that are not centred around
a node or entity of interest, such as a person. Entity-centric KGs are
relatively unexplored but hold promise in representing important facets
connected to a central node and unlocking downstream tasks beyond graph
traversal and reasoning, such as generating graph embeddings and training graph
neural networks for a wide range of predictive tasks. This paper presents an
end-to-end representation learning framework to extract entity-centric KGs from
structured and unstructured data. We introduce a star-shaped ontology to
represent the multiple facets of a person and use it to guide KG creation.
Compact representations of the graphs are created leveraging graph neural
networks and experiments are conducted using different levels of heterogeneity
or explicitness. A readmission prediction task is used to evaluate the results
of the proposed framework, showing a stable system, robust to missing data,
that outperforms a range of baseline machine learning classifiers. We highlight
that this approach has several potential applications across domains and is
open-sourced. Lastly, we discuss lessons learned, challenges, and next steps
for the adoption of the framework in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06176">
<div class="article-summary-box-inner">
<span><p>Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to
significantly enhance the performance of large language models (LLMs) by
aligning their outputs with desired human values through instruction tuning.
However, RLHF is constrained by the expertise and productivity limitations of
human evaluators. A response to this downside is to fall back to supervised
fine-tuning (SFT) with additional carefully selected expert demonstrations.
However, while this method has been proven to be effective, it invariably also
leads to increased human-in-the-loop overhead. In this study, we propose
another alternative approach: Reinforcement Learning with Generative
Adversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative
adversarial training style to enable the LLMs to learn useful human expert
demonstrations without being directly exposed to the training examples, thus
enabling good generalization capabilities while preserving sample efficiency.
Our preliminary findings indicate that RLGAF can help align LLMs outputs with
competitive performance against RLHF and SFT, while not suffering from their
respective inherent restrictions, suggesting promising avenues for further
research on automating AI alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06311">
<div class="article-summary-box-inner">
<span><p>A recent focus of large language model (LLM) development, as exemplified by
generative search engines, is to incorporate external references to generate
and support its claims. However, evaluating the attribution, i.e., verifying
whether the generated statement is fully supported by the cited reference,
remains an open problem. Although human evaluation is common practice, it is
costly and time-consuming. In this paper, we investigate the automatic
evaluation of attribution given by LLMs. We begin by defining different types
of attribution errors, and then explore two approaches for automatic
evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is
repurposed from related tasks such as question answering, fact-checking,
natural language inference, and summarization. We manually curate a set of test
examples covering 12 domains from a generative search engine, New Bing. Our
results on this curated test set and simulated examples from existing
benchmarks highlight both promising signals and challenges. We hope our problem
formulation, testbeds, and findings will help lay the foundation for future
studies on this important problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Classification via Large Language Models. (arXiv:2305.08377v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08377">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable success of large-scale Language Models (LLMs) such as
GPT-3, their performances still significantly underperform fine-tuned models in
the task of text classification. This is due to (1) the lack of reasoning
ability in addressing complex linguistic phenomena (e.g., intensification,
contrast, irony etc); (2) limited number of tokens allowed in in-context
learning.
</p>
<p>In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts
a progressive reasoning strategy tailored to addressing the complex linguistic
phenomena involved in text classification: CARP first prompts LLMs to find
superficial clues (e.g., keywords, tones, semantic relations, references, etc),
based on which a diagnostic reasoning process is induced for final decisions.
To further address the limited-token issue, CARP uses a fine-tuned model on the
supervised dataset for $k$NN demonstration search in the in-context learning,
allowing the model to take the advantage of both LLM's generalization ability
and the task-specific evidence provided by the full labeled dataset.
Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used
text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on
AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance
comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP
delivers impressive abilities on low-resource and domain-adaptation setups.
Specifically, using 16 examples per class, CARP achieves comparable
performances to supervised models with 1,024 examples per class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08732">
<div class="article-summary-box-inner">
<span><p>Previous studies have revealed that vanilla pre-trained language models
(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,
several works have attempted to integrate external knowledge into PLMs.
However, despite the promising outcome, we empirically observe that PLMs may
have already encoded rich knowledge in their pre-trained parameters but fail to
fully utilize them when applying them to knowledge-intensive tasks. In this
paper, we propose a new paradigm dubbed Knowledge Rumination to help the
pre-trained language model utilize that related latent knowledge without
retrieving it from the external corpus. By simply adding a prompt like "As far
as I know" to the PLMs, we try to review related latent knowledge and inject
them back into the model for knowledge consolidation. We apply the proposed
knowledge rumination to various language models, including RoBERTa, DeBERTa,
and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE
benchmarks demonstrate the effectiveness of our proposed approach, which proves
that the knowledge stored in PLMs can be better exploited to enhance
performance. Code is available in
https://github.com/zjunlp/knowledge-rumination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Knowledge Graph Forecasting Using In-Context Learning. (arXiv:2305.10613v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10613">
<div class="article-summary-box-inner">
<span><p>Temporal knowledge graph (TKG) forecasting benchmarks challenge models to
predict future facts using knowledge of past facts. In this paper, we apply
large language models (LLMs) to these benchmarks using in-context learning
(ICL). We investigate whether and to what extent LLMs can be used for TKG
forecasting, especially without any fine-tuning or explicit modules for
capturing structural and temporal information. For our experiments, we present
a framework that converts relevant historical facts into prompts and generates
ranked predictions using token probabilities. Surprisingly, we observe that
LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully
designed and trained for TKG forecasting. Our extensive evaluation presents
performances across several models and datasets with different characteristics,
compares alternative heuristics for preparing contextual information, and
contrasts to prominent TKG methods and simple frequency and recency baselines.
We also discover that using numerical indices instead of entity/relation names,
i.e., hiding semantic information, does not significantly affect the
performance ($\pm$0.4\% Hit@1). This shows that prior semantic knowledge is
unnecessary; instead, LLMs can leverage the existing patterns in the context to
achieve such performance. Our analysis also reveals that ICL enables LLMs to
learn irregular patterns from the historical context, going beyond simple
predictions based on common or recent information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10731">
<div class="article-summary-box-inner">
<span><p>Toxic language, such as hate speech, can deter users from participating in
online communities and enjoying popular platforms. Previous approaches to
detecting toxic language and norm violations have been primarily concerned with
conversations from online forums and social media, such as Reddit and Twitter.
These approaches are less effective when applied to conversations on
live-streaming platforms, such as Twitch and YouTube Live, as each comment is
only visible for a limited time and lacks a thread structure that establishes
its relationship with other comments. In this work, we share the first NLP
study dedicated to detecting norm violations in conversations on live-streaming
platforms. We define norm violation categories in live-stream chats and
annotate 4,583 moderated comments from Twitch. We articulate several facets of
live-stream data that differ from other forums, and demonstrate that existing
models perform poorly in this setting. By conducting a user study, we identify
the informational context humans use in live-stream moderation, and train
models leveraging context to identify norm violations. Our results show that
appropriate contextual information can boost moderation performance by 35\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoTrial: Prompting Language Models for Clinical Trial Design. (arXiv:2305.11366v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11366">
<div class="article-summary-box-inner">
<span><p>Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60% winning rate against the GPT-3.5
baselines via human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Autoregressive Document-Level Machine Translation. (arXiv:2305.12878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12878">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive translation (NAT) models achieve comparable performance
and superior speed compared to auto-regressive translation (AT) models in the
context of sentence-level machine translation (MT). However, their abilities
are unexplored in document-level MT, hindering their usage in real scenarios.
In this paper, we conduct a comprehensive examination of typical NAT models in
the context of document-level MT and further propose a simple but effective
design of sentence alignment between source and target. Experiments show that
NAT models achieve high acceleration on documents, and sentence alignment
significantly enhances their performance.
</p>
<p>However, current NAT models still have a significant performance gap compared
to their AT counterparts. Further investigation reveals that NAT models suffer
more from the multi-modality and misalignment issues in the context of
document-level MT, and current NAT models struggle with exploiting document
context and handling discourse phenomena. We delve into these challenges and
provide our code at \url{https://github.com/baoguangsheng/nat-on-doc}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance. (arXiv:2305.13225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13225">
<div class="article-summary-box-inner">
<span><p>Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered
significant attention due to their exceptional capabilities in handling a
diverse range of tasks. Recent studies demonstrate that open-sourced smaller
foundational models, such as 7B-size LLaMA, can also display remarkable
proficiency in tackling diverse tasks when fine-tuned using instruction-driven
data. In this work, we investigate a practical problem setting where the
primary focus is on one or a few particular tasks rather than general-purpose
instruction following, and explore whether LLMs can be beneficial and further
improved for such targeted scenarios. We choose the writing-assistant scenario
as the testbed, which includes seven writing tasks. We collect training data
for these tasks, reframe them in an instruction-following format, and
subsequently refine the LLM, specifically LLaMA, via instruction tuning.
Experimental results show that fine-tuning LLaMA on writing instruction data
significantly improves its ability on writing tasks. We also conduct more
experiments and analyses to offer insights for future work on effectively
fine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion
regarding the necessity of employing LLMs for only one targeted task, taking
into account the efforts required for tuning and the resources consumed during
deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis. (arXiv:2305.13230v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13230">
<div class="article-summary-box-inner">
<span><p>Recent research has highlighted the importance of dataset size in scaling
language models. However, large language models (LLMs) are notoriously
token-hungry during pre-training, and high-quality text data on the web is
approaching its scaling limit for LLMs. To further enhance LLMs, a
straightforward approach is to repeat the pre-training data for additional
epochs. In this study, we empirically investigate three key aspects under this
approach. First, we explore the consequences of repeating pre-training data,
revealing that the model is susceptible to overfitting, leading to multi-epoch
degradation. Second, we examine the key factors contributing to multi-epoch
degradation, finding that significant factors include dataset size, model
parameters, and training objectives, while less influential factors consist of
dataset quality and model FLOPs. Finally, we explore whether widely used
regularization can alleviate multi-epoch degradation. Most regularization
techniques do not yield significant improvements, except for dropout, which
demonstrates remarkable effectiveness but requires careful tuning when scaling
up the model size. Additionally, we discover that leveraging mixture-of-experts
(MoE) enables cost-effective and efficient hyper-parameter tuning for
computationally intensive dense LLMs with comparable trainable parameters,
potentially impacting efficient LLM development on a broader scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Mistakes via Interactive Study Assistant for Large Language Models. (arXiv:2305.13829v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13829">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown promising capabilities to refine
their generation based on feedback. However, LLM refinement based on feedback
is not always robust and may produce incorrect answers. In this paper, we
propose Large LAnguage Model (SALAM) to learn and correct from their mistakes.
Our method introduces a study assistant agent to analyze mistakes and generate
improvement guidelines from the main LLM. During inference, it identifies
common misunderstandings based on the mistake collections and provides
guidelines for LLMs to help them avoid similar mistakes. We further finetune
the study assistant using imitation learning with successful feedback
interaction. Our experiments on two challenging frameworks (BBH and BBQ)
demonstrate that SALAM outperforms baselines by a margin of up to 10.7 in
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Condensing Multilingual Knowledge with Lightweight Language-Specific Modules. (arXiv:2305.13993v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13993">
<div class="article-summary-box-inner">
<span><p>Incorporating language-specific (LS) modules is a proven method to boost
performance in multilingual machine translation. This approach bears similarity
to Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the
scalability of this approach to hundreds of languages (experts) tends to be
unmanageable due to the prohibitive number of parameters introduced by
full-rank matrices in fully-connected layers. In this work, we introduce the
Language-Specific Matrix Synthesis (LMS) method. This approach constructs LS
modules by generating low-rank matrices from two significantly smaller matrices
to approximate the full-rank matrix. Furthermore, we condense multilingual
knowledge from multiple LS modules into a single shared module with the Fuse
Distillation (FD) technique to improve the efficiency of inference and model
serialization. We show that our LMS method significantly outperforms previous
LS methods and MoE methods with the same amount of extra parameters, e.g., 1.73
BLEU points over the Switch Transformer on many-to-many multilingual machine
translation. Importantly, LMS is able to have comparable translation
performance with much fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14259">
<div class="article-summary-box-inner">
<span><p>Literature-Based Discovery (LBD) aims to discover new scientific knowledge by
mining papers and generating hypotheses. Standard LBD is limited to predicting
pairwise relations between discrete concepts (e.g., drug-disease links). LBD
also ignores critical contexts like experimental settings (e.g., a specific
patient population where a drug is evaluated) and background knowledge and
motivations that human scientists consider (e.g., to find a drug candidate
without specific side effects). We address these limitations with a novel
formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in
natural language, while grounding them in a context that controls the
hypothesis search space. We present a modeling framework using retrieval of
``inspirations'' from a heterogeneous network of citations and knowledge graph
relations, and create a new dataset derived from papers. Our evaluations with
powerful large language models (LLMs) reveal that GPT4 tends to generate ideas
with overall low technical depth and novelty, while our inspiration prompting
approaches partially mitigate this issue. However, fundamental challenges
remain on the road to building machines that generate new scientific knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14282">
<div class="article-summary-box-inner">
<span><p>Automatically evaluating the quality of language generation is critical.
Although recent learned metrics show high correlation with human judgement,
these metrics can not explain their verdict or associate the scores with
defects in generated text. To address this limitation, we present
InstructScore, an explainable evaluation metric for text generation. By
harnessing both explicit human instruction and the implicit knowledge of GPT-4,
we fine-tune a text evaluation metric based on LLaMA, producing both a score
for generated text and a human readable diagnostic report. We evaluate
InstructScore on a variety of generation tasks, including translation,
captioning, data-to-text and commonsense generation. Experiments show that our
7B model surpasses all other unsupervised metrics, including those based on
175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct
supervision from human-rated data, achieves performance levels on par with
state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models. (arXiv:2305.14318v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14318">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made significant progress in utilizing
tools, but their ability is limited by API availability and the instability of
implicit reasoning, particularly when both planning and execution are involved.
To overcome these limitations, we propose CREATOR, a novel framework that
enables LLMs to create their own tools using documentation and code
realization. CREATOR disentangles abstract tool creation and concrete decision
execution, resulting in improved performance. We evaluate CREATOR on MATH and
TabMWP benchmarks, respectively consisting of challenging math competition
problems and diverse tabular contents. Remarkably, CREATOR outperforms existing
chain-of-thought, program-of-thought, and tool-using baselines. Additionally,
we introduce the Creation Challenge dataset, featuring 2K diverse questions, to
emphasize the necessity and benefits of LLMs' tool creation ability. Further
research demonstrates that leveraging LLMs as tool creators facilitates
knowledge transfer, and LLMs exhibit varying levels of tool creation abilities,
enabling them to adapt to diverse situations. The tool creation ability
revolutionizes the LLM's problem-solving paradigm, driving us closer to the
next frontier of artificial intelligence. All the codes and data are released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14493">
<div class="article-summary-box-inner">
<span><p>Prompt-based models have made remarkable advancements in the fields of
zero-shot and few-shot learning, attracting a lot of attention from
researchers. Developing an effective prompt template plays a critical role.
However, prior studies have mainly focused on prompt vocabulary selection or
embedding initialization with the reserved prompt position fixed. In this
empirical study, we conduct the most comprehensive analysis to date of prompt
position option for natural language understanding tasks. Our findings quantify
the substantial impact prompt position has on model performance. We observe
that the prompt position used in prior studies is often sub-optimal for both
zero-shot and few-shot settings. These findings suggest prompt position
optimisation as an interesting research direction alongside the existing focus
on prompt engineering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14761">
<div class="article-summary-box-inner">
<span><p>Charts are very popular for analyzing data, visualizing key insights and
answering complex reasoning questions about data. To facilitate chart-based
data analysis using natural language, several downstream tasks have been
introduced recently such as chart question answering and chart summarization.
However, most of the methods that solve these tasks use pretraining on language
or vision-language tasks that do not attempt to explicitly model the structure
of the charts (e.g., how data is visually encoded and how chart elements are
related to each other). To address this, we first build a large corpus of
charts covering a wide variety of topics and visual styles. We then present
UniChart, a pretrained model for chart comprehension and reasoning. UniChart
encodes the relevant text, data, and visual elements of charts and then uses a
chart-grounded text decoder to generate the expected output in natural
language. We propose several chart-specific pretraining tasks that include: (i)
low-level tasks to extract the visual elements (e.g., bars, lines) and data
from charts, and (ii) high-level tasks to acquire chart understanding and
reasoning skills. We find that pretraining the model on a large corpus with
chart-specific low- and high-level tasks followed by finetuning on three
down-streaming tasks results in state-of-the-art performance on three
downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SummIt: Iterative Text Summarization via ChatGPT. (arXiv:2305.14835v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14835">
<div class="article-summary-box-inner">
<span><p>Text summarization systems have made significant progress in recent years,
but typically generate summaries in one single step. However, the one-shot
summarization setting is sometimes inadequate, as the generated summary may
contain hallucinations or overlook essential details related to the reader's
interests. This paper addresses this limitation by proposing SummIt, an
iterative text summarization framework based on large language models like
ChatGPT. Our framework enables the model to refine the generated summary
iteratively through self-evaluation and feedback, resembling humans' iterative
process when drafting and revising summaries. Furthermore, we explore the
potential benefits of integrating knowledge and topic extractors into the
framework to enhance summary faithfulness and controllability. We automatically
evaluate the performance of our framework on three benchmark summarization
datasets. We also conduct a human evaluation to validate the effectiveness of
the iterative refinements and identify a potential issue of over-correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16646">
<div class="article-summary-box-inner">
<span><p>Large language models have shown astonishing performance on a wide range of
reasoning tasks. In this paper, we investigate whether they could reason about
real-world events and help improve the prediction performance of event sequence
models. We design LAMP, a framework that integrates a large language model in
event prediction. Particularly, the language model performs abductive reasoning
to assist an event sequence model: the event model proposes predictions on
future events given the past; instructed by a few expert-annotated
demonstrations, the language model learns to suggest possible causes for each
proposal; a search module finds out the previous events that match the causes;
a scoring function learns to examine whether the retrieved events could
actually cause the proposal. Through extensive experiments on several
challenging real-world datasets, we demonstrate that our framework -- thanks to
the reasoning capabilities of large language models -- could significantly
outperform the state-of-the-art event sequence models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18500">
<div class="article-summary-box-inner">
<span><p>Vision and text have been fully explored in contemporary video-text
foundational models, while other modalities such as audio and subtitles in
videos have not received sufficient attention. In this paper, we resort to
establish connections between multi-modality video tracks, including Vision,
Audio, and Subtitle, and Text by exploring an automatically generated
large-scale omni-modality video caption dataset called VAST-27M. Specifically,
we first collect 27 million open-domain video clips and separately train a
vision and an audio captioner to generate vision and audio captions. Then, we
employ an off-the-shelf Large Language Model (LLM) to integrate the generated
captions, together with subtitles and instructional prompts into omni-modality
captions. Based on the proposed VAST-27M dataset, we train an omni-modality
video-text foundational model named VAST, which can perceive and process
vision, audio, and subtitle modalities from video, and better support various
tasks including vision-text, audio-text, and multi-modal video-text tasks
(retrieval, captioning and QA). Extensive experiments have been conducted to
demonstrate the effectiveness of our proposed VAST-27M corpus and VAST
foundation model. VAST achieves 22 new state-of-the-art results on various
cross-modality benchmarks. Code, model and dataset will be released at
https://github.com/TXH-mercury/VAST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18507">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have scaled up to unlock a wide range of complex
reasoning tasks with the aid of various prompting methods. However, current
prompting methods generate natural language intermediate steps to help
reasoning, which can cause imperfect task reduction and confusion. To mitigate
such limitations, we explore code prompting, a neural symbolic prompting method
with both zero-shot and few-shot versions which triggers code as intermediate
steps. We conduct experiments on 7 widely-used benchmarks involving symbolic
reasoning and arithmetic reasoning. Code prompting generally outperforms
chain-of-thought (CoT) prompting. To further understand the performance and
limitations of code prompting, we perform extensive ablation studies and error
analyses, and identify several exclusive advantages of using symbolic
promptings compared to natural language. We also consider the ensemble of code
prompting and CoT prompting to combine the strengths of both. Finally, we show
through experiments how code annotations and their locations affect code
prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask learning for recognizing stress and depression in social media. (arXiv:2305.18907v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18907">
<div class="article-summary-box-inner">
<span><p>Stress and depression are prevalent nowadays across people of all ages due to
the quick paces of life. People use social media to express their feelings.
Thus, social media constitute a valuable form of information for the early
detection of stress and depression. Although many research works have been
introduced targeting the early recognition of stress and depression, there are
still limitations. There have been proposed multi-task learning settings, which
use depression and emotion (or figurative language) as the primary and
auxiliary tasks respectively. However, although stress is inextricably linked
with depression, researchers face these two tasks as two separate tasks. To
address these limitations, we present the first study, which exploits two
different datasets collected under different conditions, and introduce two
multitask learning frameworks, which use depression and stress as the main and
auxiliary tasks respectively. Specifically, we use a depression dataset and a
stressful dataset including stressful posts from ten subreddits of five
domains. In terms of the first approach, each post passes through a shared BERT
layer, which is updated by both tasks. Next, two separate BERT encoder layers
are exploited, which are updated by each task separately. Regarding the second
approach, it consists of shared and task-specific layers weighted by attention
fusion networks. We conduct a series of experiments and compare our approaches
with existing research initiatives, single-task learning, and transfer
learning. Experiments show multiple advantages of our approaches over
state-of-the-art ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.19187">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) specializing in natural language generation
(NLG) have recently started exhibiting promising capabilities across a variety
of domains. However, gauging the trustworthiness of responses generated by LLMs
remains an open challenge, with limited research on uncertainty quantification
(UQ) for NLG. Furthermore, existing literature typically assumes white-box
access to language models, which is becoming unrealistic either due to the
closed-source nature of the latest LLMs or computational constraints. In this
work, we investigate UQ in NLG for black-box LLMs. We first differentiate
uncertainty vs confidence: the former refers to the "dispersion" of the
potential predictions for a fixed input, and the latter refers to the
confidence on a particular prediction/generation. We then propose and compare
several confidence/uncertainty metrics, applying them to selective NLG where
unreliable results could either be ignored or yielded for further assessment.
Experiments were carried out with several popular LLMs on question-answering
datasets (for evaluation purposes). Results reveal that a simple metric for the
semantic dispersion can be a reliable predictor of the quality of LLM
responses, providing valuable insights for practitioners on uncertainty
management when adopting LLMs. The code to replicate our experiments is
available at https://github.com/zlin7/UQ-NLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02105">
<div class="article-summary-box-inner">
<span><p>While there has been significant progress in ASR, African-accented clinical
ASR has been understudied due to a lack of training datasets. Building robust
ASR systems in this domain requires large amounts of annotated or labeled data,
for a wide variety of linguistically and morphologically rich accents, which
are expensive to create. Our study aims to address this problem by reducing
annotation expenses through informative uncertainty-based data selection. We
show that incorporating epistemic uncertainty into our adaptation rounds
outperforms several baseline results, established using state-of-the-art (SOTA)
ASR models, while reducing the required amount of labeled data, and hence
reducing annotation costs. Our approach also improves out-of-distribution
generalization for very low-resource accents, demonstrating the viability of
our approach for building generalizable ASR models in the context of accented
African clinical ASR, where training datasets are predominantly scarce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07207">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), with their remarkable conversational
capabilities, have demonstrated impressive performance across various
applications and have emerged as formidable AI assistants. In view of this, it
raises an intuitive question: Can we harness the power of LLMs to build
multimodal AI assistants for visual applications? Recently, several multi-modal
models have been developed for this purpose. They typically pre-train an
adaptation module to align the semantics of the vision encoder and language
model, followed by fine-tuning on instruction-following data. However, despite
the success of this pipeline in image and language understanding, its
effectiveness in joint video and language understanding has not been widely
explored. In this paper, we aim to develop a novel multi-modal foundation model
capable of comprehending video, image, and language within a general framework.
To achieve this goal, we introduce Valley, a Video Assistant with Large
Language model Enhanced abilitY. The Valley consists of a LLM, a temporal
modeling module, a visual encoder, and a simple projection module designed to
bridge visual and textual modes. To empower Valley with video comprehension and
instruction-following capabilities, we construct a video instruction dataset
and adopt a two-stage tuning procedure to train it. Specifically, we employ
ChatGPT to facilitate the construction of task-oriented conversation data
encompassing various tasks, including multi-shot captions, long video
descriptions, action recognition, causal relationship inference, etc.
Subsequently, we adopt a pre-training-then-instructions-tuned pipeline to align
visual and textual modalities and improve the instruction-following capability
of Valley. Qualitative experiments demonstrate that Valley has the potential to
function as a highly effective video assistant that can make complex video
understanding scenarios easy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08193">
<div class="article-summary-box-inner">
<span><p>Despite its centrality in the philosophy of cognitive science, there has been
little prior philosophical work engaging with the notion of representation in
contemporary NLP practice. This paper attempts to fill that lacuna: drawing on
ideas from cognitive science, I introduce a framework for evaluating the
representational claims made about components of neural NLP models, proposing
three criteria with which to evaluate whether a component of a model represents
a property and operationalising these criteria using probing classifiers, a
popular analysis technique in NLP (and deep learning more broadly).
</p>
<p>The project of operationalising a philosophically-informed notion of
representation should be of interest to both philosophers of science and NLP
practitioners. It affords philosophers a novel testing-ground for claims about
the nature of representation, and helps NLPers organise the large literature on
probing experiments, suggesting novel avenues for empirical research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PEACE: Cross-Platform Hate Speech Detection- A Causality-guided Framework. (arXiv:2306.08804v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.08804">
<div class="article-summary-box-inner">
<span><p>Hate speech detection refers to the task of detecting hateful content that
aims at denigrating an individual or a group based on their religion, gender,
sexual orientation, or other characteristics. Due to the different policies of
the platforms, different groups of people express hate in different ways.
Furthermore, due to the lack of labeled data in some platforms it becomes
challenging to build hate speech detection models. To this end, we revisit if
we can learn a generalizable hate speech detection model for the cross platform
setting, where we train the model on the data from one (source) platform and
generalize the model across multiple (target) platforms. Existing
generalization models rely on linguistic cues or auxiliary information, making
them biased towards certain tags or certain kinds of words (e.g., abusive
words) on the source platform and thus not applicable to the target platforms.
Inspired by social and psychological theories, we endeavor to explore if there
exist inherent causal cues that can be leveraged to learn generalizable
representations for detecting hate speech across these distribution shifts. To
this end, we propose a causality-guided framework, PEACE, that identifies and
leverages two intrinsic causal cues omnipresent in hateful content: the overall
sentiment and the aggression in the text. We conduct extensive experiments
across multiple platforms (representing the distribution shift) showing if
causal cues can help cross-platform generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09719">
<div class="article-summary-box-inner">
<span><p>Despite the success of ChatGPT, its performances on most NLP tasks are still
well below the supervised baselines. In this work, we looked into the causes,
and discovered that its subpar performance was caused by the following factors:
(1) token limit in the prompt does not allow for the full utilization of the
supervised datasets; (2) mismatch between the generation nature of ChatGPT and
NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly
focus on certain keywords, etc.
</p>
<p>In this work, we propose a collection of general modules to address these
issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed
modules include (1) a one-input-multiple-prompts strategy that employs multiple
prompts for one input to accommodate more demonstrations; (2) using fine-tuned
models for better demonstration retrieval; (3) transforming tasks to formats
that are more tailored to the generation nature; (4) employing reasoning
strategies that are tailored to addressing the task-specific complexity; (5)
the self-verification strategy to address the hallucination issue of LLMs; (6)
the paraphrase strategy to improve the robustness of model predictions.
</p>
<p>We conduct experiments on 21 datasets of 10 representative NLP tasks,
including question answering, commonsense reasoning, natural language
inference, sentiment analysis, named entity recognition, entity-relation
extraction, event extraction, dependency parsing, semantic role labeling, and
part-of-speech tagging. Using the proposed assemble of techniques, we are able
to significantly boost the performance of ChatGPT on the selected NLP tasks,
achieving performances comparable to or better than supervised baselines, or
even existing SOTA performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis. (arXiv:2306.11260v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11260">
<div class="article-summary-box-inner">
<span><p>Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation
task, which analyzes the emotional polarity of the evaluation aspects.
Generally, the emotional polarity of an aspect exists in the corresponding
opinion expression, whose diversity has great impact on model's performance. To
mitigate this problem, we propose a novel and simple counterfactual data
augmentation method to generate opinion expressions with reversed sentiment
polarity. In particular, the integrated gradients are calculated to locate and
mask the opinion expression. Then, a prompt combined with the reverse
expression polarity is added to the original text, and a Pre-trained language
model (PLM), T5, is finally was employed to predict the masks. The experimental
results shows the proposed counterfactual data augmentation method performs
better than current augmentation methods on three ABSA datasets, i.e. Laptop,
Restaurant, and MAMS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14939">
<div class="article-summary-box-inner">
<span><p>Hate speech detection is a challenging natural language processing task that
requires capturing linguistic and contextual nuances. Pre-trained language
models (PLMs) offer rich semantic representations of text that can improve this
task. However there is still limited knowledge about ways to effectively
combine representations across PLMs and leverage their complementary strengths.
In this work, we shed light on various combination techniques for several PLMs
and comprehensively analyze their effectiveness. Our findings show that
combining embeddings leads to slight improvements but at a high computational
cost and the choice of combination has marginal effect on the final outcome. We
also make our codebase public at
https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.17175">
<div class="article-summary-box-inner">
<span><p>Clinical decision-making is a fundamental stage in delivering appropriate
care to patients. In recent years several decision-making systems designed to
aid the clinician in this process have been developed. However, technical
solutions currently in use are based on simple regression models and are only
able to take into account simple pre-defined multiple-choice features, such as
patient age, pre-existing conditions, smoker status, etc. One particular source
of patient data, that available decision-making systems are incapable of
processing is the collection of patient consultation GP notes. These contain
crucial signs and symptoms - the information used by clinicians in order to
make a final decision and direct the patient to the appropriate care.
Extracting information from GP notes is a technically challenging problem, as
they tend to include abbreviations, typos, and incomplete sentences.
</p>
<p>This paper addresses this open challenge. We present a framework that
performs knowledge graph construction from raw GP medical notes written during
or after patient consultations. By relying on support phrases mined from the
SNOMED ontology, as well as predefined supported facts from values used in the
RECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction
tool, our graph generative framework is able to extract structured knowledge
graphs from the highly unstructured and inconsistent format that consultation
notes are written in. Our knowledge graphs include information about existing
patient symptoms, their duration, and their severity.
</p>
<p>We apply our framework to consultation notes of COVID-19 patients in the UK
COVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a
quantitative evaluation of the performance of our framework, demonstrating that
our approach has better accuracy than traditional NLP methods when answering
questions about patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading. (arXiv:2307.00782v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00782">
<div class="article-summary-box-inner">
<span><p>While state-of-the-art Text-to-Speech systems can generate natural speech of
very high quality at sentence level, they still meet great challenges in speech
generation for paragraph / long-form reading. Such deficiencies are due to i)
ignorance of cross-sentence contextual information, and ii) high computation
and memory cost for long-form synthesis. To address these issues, this work
develops a lightweight yet effective TTS system, ContextSpeech. Specifically,
we first design a memory-cached recurrence mechanism to incorporate global text
and speech context into sentence encoding. Then we construct
hierarchically-structured textual semantics to broaden the scope for global
context enhancement. Additionally, we integrate linearized self-attention to
improve model efficiency. Experiments show that ContextSpeech significantly
improves the voice quality and prosody expressiveness in paragraph reading with
competitive model efficiency. Audio samples are available at:
https://contextspeech.github.io/demo/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.01379">
<div class="article-summary-box-inner">
<span><p>While Large Language Models (LLMs) have demonstrated remarkable potential in
natural language generation and instruction following, a persistent challenge
lies in their susceptibility to "hallucinations", which erodes trust in their
outputs. Although Uncertainty Quantification (UQ) presents a promising
solution, its accurate implementation within the context of LLMs remains a
significant hurdle. To address this critical roadblock, our research originates
from a fundamental heuristic insight: tokens within auto-regressive
LLM-generated text do not equally reflect the underlying meaning. Some tokens
carry greater relevance and representativeness than others, owing to the
phenomenon of "linguistic redundancy", wherein a select few keywords suffice to
convey the essence of lengthy sentences. Regrettably, existing methodologies
treat all tokens with equal importance when estimating uncertainty,
disregarding these inherent generative inequalities. Our analysis reveals a
significant issue with state-of-the-art: numerous tokens (and sentences) of
limited semantic significance receive equal or even excessive weighting during
uncertainty estimation. To rectify this bias, we propose to jointly Shifting
Attention to more Relevant (SAR) components, at both the token- and the
sentence-levels for accurate uncertainty estimation. We conduct extensive
experiments involving a range of popular "off-the-shelf" LLMs, including
instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as
pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B
parameters. We carry out evaluation across various free-form question-answering
tasks, encompassing domains such as reading comprehension, science Q&amp;A, and
medical Q&amp;A. Our experimental results demonstrate the superior performance of
SAR in addressing the challenges of uncertainty estimation within the realm of
LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divide & Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.10864">
<div class="article-summary-box-inner">
<span><p>Emerging large-scale text-to-image generative models, e.g., Stable Diffusion
(SD), have exhibited overwhelming results with high fidelity. Despite the
magnificent progress, current state-of-the-art models still struggle to
generate images fully adhering to the input prompt. Prior work, Attend &amp;
Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming
to optimize cross-attention during inference time to better incorporate the
semantics. It demonstrates promising results in generating simple prompts,
e.g., ``a cat and a dog''. However, its efficacy declines when dealing with
more complex prompts, and it does not explicitly address the problem of
improper attribute binding. To address the challenges posed by complex prompts
or scenarios involving multiple entities and to achieve improved attribute
binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for
GSN: a novel attendance loss and a binding loss. Our approach stands out in its
ability to faithfully synthesize desired objects with improved attribute
alignment from complex prompts and exhibits superior performance across
multiple evaluation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15337">
<div class="article-summary-box-inner">
<span><p>This work aims at decreasing the end-to-end generation latency of large
language models (LLMs). One of the major causes of the high generation latency
is the sequential decoding approach adopted by almost all state-of-the-art
LLMs. In this work, motivated by the thinking and writing process of humans, we
propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the
skeleton of the answer, and then conducts parallel API calls or batched
decoding to complete the contents of each skeleton point in parallel. Not only
does SoT provide considerable speed-ups across 12 LLMs, but it can also
potentially improve the answer quality on several question categories. SoT is
an initial attempt at data-centric optimization for inference efficiency, and
further underscores the potential of pushing LLMs to think more like a human
for answer quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Private Watermark for Large Language Models. (arXiv:2307.16230v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16230">
<div class="article-summary-box-inner">
<span><p>Recently, text watermarking algorithms for large language models (LLMs) have
been mitigating the potential harms of text generated by the LLMs, including
fake news and copyright issues. However, the watermark detection of current
text algorithms requires the key from the generation process, making them
susceptible to breaches and counterfeiting. In this work, we propose the first
private watermarking algorithm, which extends the current text watermarking
algorithms by using two different neural networks respectively for watermark
generation and detection, rather than using the same key at both stages.
Meanwhile, part of the parameters of the watermark generation and detection
networks are shared, which makes the detection network achieve a high accuracy
very efficiently. Experiments show that our algorithm ensures high detection
accuracy with minimal impact on generation and detection speed, due to the
small parameter size of both networks. Additionally, our subsequent analysis
demonstrates the difficulty of reverting the watermark generation rules from
the detection network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.01313">
<div class="article-summary-box-inner">
<span><p>Vision-language models like CLIP are widely used in zero-shot image
classification due to their ability to understand various visual concepts and
natural language descriptions. However, how to fully leverage CLIP's
unprecedented human-like understanding capabilities to achieve better
performance is still an open question. This paper draws inspiration from the
human visual perception process: when classifying an object, humans first infer
contextual attributes (e.g., background and orientation) which help separate
the foreground object from the background, and then classify the object based
on this information. Inspired by it, we observe that providing CLIP with
contextual attributes improves zero-shot image classification and mitigates
reliance on spurious features. We also observe that CLIP itself can reasonably
infer the attributes from an image. With these observations, we propose a
training-free, two-step zero-shot classification method PerceptionCLIP. Given
an image, it first infers contextual attributes (e.g., background) and then
performs object classification conditioning on them. Our experiments show that
PerceptionCLIP achieves better generalization, group robustness, and
interpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst
group accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extrapolating Large Language Models to Non-English by Aligning Languages. (arXiv:2308.04948v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.04948">
<div class="article-summary-box-inner">
<span><p>Existing large language models show disparate capability across different
languages, due to the imbalance in the training data. Their performances on
English tasks are often stronger than on tasks of other languages. In this
paper, we empower pre-trained LLMs on non-English languages by building
semantic alignment across languages. We start from targeting individual
languages by performing cross-lingual instruction-tuning (CoIT) on LLaMA, i.e.
tuning it with translation task data and cross-lingual general task data to
obtain cross-lingual models (x-LLaMAs), and formulate underlying scaling laws
to investigate the advantages of using scalable translation data. Then we
perform multilingual instruction-tuning (MuIT) with mixed resources to build
multilingual m-LLaMA. We also illustrate how we leverage the scaling laws to
optimize data allocation in a resource-constrained setting. Experiment results
on cross-lingual benchmarks XQUAD and MLQA show that x-LLaMAs surpass the
English instruction-tuned counterpart (Alpaca) by an average of 27.83% across
six non-English languages. Evaluation results on translation dataset Flores-101
show that x-LLaMAs outperform previous LLaMA-based models by an average of
18.89%. Encouragingly, m-LLaMA achieves comparable performance to x-LLaMAs on
individual languages and demonstrates the ability to follow multilingual
instructions. Further analysis on response content and representation space
reveals the alignment of the multilingual semantic space within the middle
layers of m-LLaMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10792">
<div class="article-summary-box-inner">
<span><p>This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. (arXiv:2308.10848v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.10848">
<div class="article-summary-box-inner">
<span><p>Autonomous agents empowered by Large Language Models (LLMs) have undergone
significant improvements, enabling them to generalize across a broad spectrum
of tasks. However, in real-world scenarios, cooperation among individuals is
often required to enhance the efficiency and effectiveness of task
accomplishment. Hence, inspired by human group dynamics, we propose a
multi-agent framework \framework that can collaboratively and dynamically
adjust its composition as a greater-than-the-sum-of-its-parts system. Our
experiments demonstrate that \framework framework can effectively deploy
multi-agent groups that outperform a single agent. Furthermore, we delve into
the emergence of social behaviors among individual agents within a group during
collaborative task accomplishment. In view of these behaviors, we discuss some
possible strategies to leverage positive ones and mitigate negative ones for
improving the collaborative potential of multi-agent groups. Our codes for
\framework will soon be released at
\url{https://github.com/OpenBMB/AgentVerse}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator. (arXiv:2308.11534v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11534">
<div class="article-summary-box-inner">
<span><p>The unparalleled performance of closed-sourced ChatGPT has sparked efforts
towards its democratization, with notable strides made by leveraging real user
and ChatGPT conversations, as evidenced by Vicuna. However, due to challenges
in gathering conversations involving human participation, current endeavors
like Baize and UltraChat aim to automatically generate conversational data.
They primarily rely on ChatGPT conducting roleplay to simulate human behaviors
based on instructions rather than genuine learning from humans, resulting in
limited scope, diminished diversity, and an absence of genuine multi-round
conversational dynamics. To address the above issues, we target human questions
extracted from genuine human-machine conversations as a learning goal and train
a user simulator called `Socratic' to produce a high-quality human-centric
synthetic conversation dataset. Subsequently, this dataset was used to train
our assistant model, named `PlatoLM'. Experimentally, PlatoLM outpaces baseline
models in both Vicuna-Bench and MT-Bench by pairwise comparison when
considering equivalent training set sizes, and manual evaluation also shows
that our model is highly competitive. Impressively, when fine-tuned with the
latest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models
(including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and in
Alpaca-Eval benchmark, it ranks second among 7B models, even beating some
larger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depth
analysis demonstrates the scalability and transferability of our approach. The
code is available at https://github.com/FreedomIntelligence/PlatoLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation. (arXiv:2308.15122v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15122">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks (SNNs) offer a promising avenue to implement deep
neural networks in a more energy-efficient way. However, the network
architectures of existing SNNs for language tasks are still simplistic and
relatively shallow, and deep architectures have not been fully explored,
resulting in a significant performance gap compared to mainstream
transformer-based networks such as BERT. To this end, we improve a
recently-proposed spiking Transformer (i.e., Spikformer) to make it possible to
process language tasks and propose a two-stage knowledge distillation method
for training it, which combines pre-training by distilling knowledge from BERT
with a large collection of unlabelled texts and fine-tuning with task-specific
instances via knowledge distillation again from the BERT fine-tuned on the same
training examples. Through extensive experimentation, we show that the models
trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and
even achieve comparable results to BERTs on text classification tasks for both
English and Chinese with much less energy consumption. Our code is available at
https://github.com/Lvchangze/SpikeBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge. (arXiv:2309.01437v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.01437">
<div class="article-summary-box-inner">
<span><p>Recently, excellent progress has been made in speech recognition. However,
pure data-driven approaches have struggled to solve the problem in
domain-mismatch and long-tailed data. Considering that knowledge-driven
approaches can help data-driven approaches alleviate their flaws, we introduce
sememe-based semantic knowledge information to speech recognition (SememeASR).
Sememe, according to the linguistic definition, is the minimum semantic unit in
a language and is able to represent the implicit semantic information behind
each word very well. Our experiments show that the introduction of sememe
information can improve the effectiveness of speech recognition. In addition,
our further experiments show that sememe knowledge can improve the model's
recognition of long-tailed data and enhance the model's domain generalization
ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!. (arXiv:2309.02110v3 [math.HO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02110">
<div class="article-summary-box-inner">
<span><p>Wordle is a popular, online word game offered by the New York Times
(nytimes.com). Currently there are some 2 million players of the English
version worldwide. Players have 6 attempts to guess the daily word (target
word) and after each attempt, the player receives color-coded information about
the correctness and position of each letter in the guess. After either a
successful completion of the puzzle or the final unsuccessful attempt, software
can assess the player's luck and skill using Information Theory and can display
data for the first, second, ..., sixth guesses of a random sample of all
players. Recently, I discovered that the latter data is presented in a format
that can easily be copied and pasted into a spreadsheet. I compiled data on
Wordle players' first guesses from May 2023 - August 2023 and inferred some
interesting information about Wordle players. A) Every day, about 0.2-0.5% of
players solve the puzzle in one attempt. Because the odds of guessing the one
of 2,315 possible target words at random is 0.043%, this implies that 4,000 -
10,000 players cheat by obtaining the target word outside of playing the game!
B) At least 1/3 of the players have a favorite starting word, or cycle through
several. And even though players should be aware that target words are never
repeated, most players appear to remain loyal to their starting word even after
its appearance as a target word. C) On August 15, 2023, about 30,000 players
abruptly changed their starting word, presumably based on a crossword puzzle
clue! Wordle players can be influenced! This study goes beyond social media
postings, surveys, and Google Trends to provide solid, quantitative evidence
about cheating in Wordle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation. (arXiv:2309.02459v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02459">
<div class="article-summary-box-inner">
<span><p>Mapping two modalities, speech and text, into a shared representation space,
is a research topic of using text-only data to improve end-to-end automatic
speech recognition (ASR) performance in new domains. However, the length of
speech representation and text representation is inconsistent. Although the
previous method up-samples the text representation to align with acoustic
modality, it may not match the expected actual duration. In this paper, we
proposed novel representations match strategy through down-sampling acoustic
representation to align with text modality. By introducing a continuous
integrate-and-fire (CIF) module generating acoustic representations consistent
with token length, our ASR model can learn unified representations from both
modalities better, allowing for domain adaptation using text-only data of the
target domain. Experiment results of new domain data demonstrate the
effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.02654">
<div class="article-summary-box-inner">
<span><p>The prevalent use of large language models (LLMs) in various domains has
drawn attention to the issue of "hallucination," which refers to instances
where LLMs generate factually inaccurate or ungrounded information. Existing
techniques for hallucination detection in language assistants rely on intricate
fuzzy, specific free-language-based chain of thought (CoT) techniques or
parameter-based methods that suffer from interpretability issues. Additionally,
the methods that identify hallucinations post-generation could not prevent
their occurrence and suffer from inconsistent performance due to the influence
of the instruction format and model style. In this paper, we introduce a novel
pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which
focuses on evaluating the model's familiarity with the concepts present in the
input instruction and withholding the generation of response in case of
unfamiliar concepts. This approach emulates the human ability to refrain from
responding to unfamiliar topics, thus reducing hallucinations. We validate
SELF-FAMILIARITY across four different large language models, demonstrating
consistently superior performance compared to existing techniques. Our findings
propose a significant shift towards preemptive strategies for hallucination
mitigation in LLM assistants, promising improvements in reliability,
applicability, and interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta predictive learning model of languages in neural circuits. (arXiv:2309.04106v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04106">
<div class="article-summary-box-inner">
<span><p>Large language models based on self-attention mechanisms have achieved
astonishing performances not only in natural language itself, but also in a
variety of tasks of different nature. However, regarding processing language,
our human brain may not operate using the same principle. Then, a debate is
established on the connection between brain computation and artificial
self-supervision adopted in large language models. One of most influential
hypothesis in brain computation is the predictive coding framework, which
proposes to minimize the prediction error by local learning. However, the role
of predictive coding and the associated credit assignment in language
processing remains unknown. Here, we propose a mean-field learning model within
the predictive coding framework, assuming that the synaptic weight of each
connection follows a spike and slab distribution, and only the distribution,
rather than specific weights, is trained. This meta predictive learning is
successfully validated on classifying handwritten digits where pixels are input
to the network in sequence, and moreover on the toy and real language corpus.
Our model reveals that most of the connections become deterministic after
learning, while the output connections have a higher level of variability. The
performance of the resulting network ensemble changes continuously with data
load, further improving with more training data, in analogy with the emergent
behavior of large language models. Therefore, our model provides a starting
point to investigate the connection among brain computation, next-token
prediction and general intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07124">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) often demonstrate inconsistencies with human
preferences. Previous research typically gathered human preference data and
then aligned the pre-trained models using reinforcement learning or instruction
tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without
requiring alignment data is more appealing. This work explores the potential of
the latter setting. We discover that by integrating self-evaluation and rewind
mechanisms, unaligned LLMs can directly produce responses consistent with human
preferences via self-boosting. We introduce a novel inference method,
Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to
evaluate their own generation and use the evaluation results to guide rewind
and generation for AI safety. Notably, RAIN operates without the need of extra
data for model alignment and abstains from any training, gradient computation,
or parameter updates. Experimental results evaluated by GPT-4 and humans
demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the
harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while
maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the
truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.07870">
<div class="article-summary-box-inner">
<span><p>Recent advances on large language models (LLMs) enable researchers and
developers to build autonomous language agents that can automatically solve
various tasks and interact with environments, humans, and other agents using
natural language interfaces. We consider language agents as a promising
direction towards artificial general intelligence and release Agents, an
open-source library with the goal of opening up these advances to a wider
non-specialist audience. Agents is carefully engineered to support important
features including planning, memory, tool usage, multi-agent communication, and
fine-grained symbolic control. Agents is user-friendly as it enables
non-specialists to build, customize, test, tune, and deploy state-of-the-art
autonomous language agents without much coding. The library is also
research-friendly as its modularized design makes it easily extensible for
researchers. Agents is available at https://github.com/aiwaves-cn/agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.12284">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (e.g., LLaMA-2) are still far
away from satisfactory for solving mathematical problem due to the complex
reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%
on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same
size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of
82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the
MetaMathQA dataset, the MetaMath models with different model sizes and the
training code for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing the Moral Development of Large Language Models through Defining Issues Test. (arXiv:2309.13356v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.13356">
<div class="article-summary-box-inner">
<span><p>In this study, we measure the moral reasoning ability of LLMs using the
Defining Issues Test - a psychometric instrument developed for measuring the
moral development stage of a person according to the Kohlberg's Cognitive Moral
Development Model. DIT uses moral dilemmas followed by a set of ethical
considerations that the respondent has to judge for importance in resolving the
dilemma, and then rank-order them by importance. A moral development stage
score of the respondent is then computed based on the relevance rating and
ranking.
</p>
<p>Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning
ability no better than that of a random baseline, while ChatGPT, Llama2-Chat,
PaLM-2 and GPT-4 show significantly better performance on this task, comparable
to adult humans. GPT-4, in fact, has the highest post-conventional moral
reasoning score, equivalent to that of typical graduate school students.
However, we also observe that the models do not perform consistently across all
dilemmas, pointing to important gaps in their understanding and reasoning
abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.14717">
<div class="article-summary-box-inner">
<span><p>Recently years have witnessed a rapid development of large language models
(LLMs). Despite the strong ability in many language-understanding tasks, the
heavy computational burden largely restricts the application of LLMs especially
when one needs to deploy them onto edge devices. In this paper, we propose a
quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies
in the imbalanced degrees of freedom of quantization and adaptation, and the
solution is to use group-wise operators which increase the degree of freedom of
quantization meanwhile decreasing that of adaptation. QA-LoRA is easily
implemented with a few lines of code, and it equips the original LoRA with
two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized
(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the
LLM and auxiliary weights are naturally integrated into a quantized model
without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model
families and validate its effectiveness in different fine-tuning datasets and
downstream scenarios. Code will be made available at
https://github.com/yuhuixu1993/qa-lora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.15630">
<div class="article-summary-box-inner">
<span><p>Recent developments in large language models (LLMs) have shown promise in
enhancing the capabilities of natural language processing (NLP). Despite these
successes, there remains a dearth of research dedicated to the NLP
problem-solving abilities of LLMs. To fill the gap in this area, we present a
unique benchmarking dataset, NLPBench, comprising 378 college-level NLP
questions spanning various NLP topics sourced from Yale University's prior
final exams. NLPBench includes questions with context, in which multiple
sub-questions share the same public information, and diverse question types,
including multiple choice, short answer, and math. Our evaluation, centered on
LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting
strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study
reveals that the effectiveness of the advanced prompting strategies can be
inconsistent, occasionally damaging LLM performance, especially in smaller
models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated
specific shortcomings in LLMs' scientific problem-solving skills, with
weaknesses in logical decomposition and reasoning notably affecting results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.15806">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) present an intriguing avenue for exploration in
the field of formal theorem proving. Nevertheless, their full potential,
particularly concerning the mitigation of hallucinations and refinement through
prover error messages, remains an area that has yet to be thoroughly
investigated. To enhance the effectiveness of LLMs in the field, we introduce
the Lyra, a new framework that employs two distinct correction mechanisms: Tool
Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in
the post-processing of formal proofs, we leverage prior knowledge to utilize
predefined prover tools (e.g., Sledgehammer) for guiding the replacement of
incorrect tools. Tool Correction significantly contributes to mitigating
hallucinations, thereby improving the overall accuracy of the proof. In
addition, we introduce Conjecture Correction, an error feedback mechanism
designed to interact with prover to refine formal proof conjectures with prover
error messages. Compared to the previous refinement framework, the proposed
Conjecture Correction refines generation with instruction but does not collect
paired (generation, error &amp; refinement) prompts. Our method has achieved
state-of-the-art (SOTA) performance on both miniF2F validation (48.0% -&gt; 55.3%)
and test (45.5% -&gt; 51.2%). We also present 3 IMO problems solved by Lyra. We
believe Tool Correction (post-process for hallucination mitigation) and
Conjecture Correction (subgoal adjustment from interaction with environment)
could provide a promising avenue for future research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Image-text Multimodal Models. (arXiv:2309.15857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.15857">
<div class="article-summary-box-inner">
<span><p>Amidst the evolving landscape of artificial intelligence, the convergence of
visual and textual information has surfaced as a crucial frontier, leading to
the advent of image-text multimodal models. This paper provides a comprehensive
review of the evolution and current state of image-text multimodal models,
exploring their application value, challenges, and potential research
trajectories. Initially, we revisit the basic concepts and developmental
milestones of these models, introducing a novel classification that segments
their evolution into three distinct phases, based on their time of introduction
and subsequent impact on the discipline. Furthermore, based on the tasks'
significance and prevalence in the academic landscape, we propose a
categorization of the tasks associated with image-text multimodal models into
five major types, elucidating the recent progress and key technologies within
each category. Despite the remarkable accomplishments of these models, numerous
challenges and issues persist. This paper delves into the inherent challenges
and limitations of image-text multimodal models, fostering the exploration of
prospective research directions. Our objective is to offer an exhaustive
overview of the present research landscape of image-text multimodal models and
to serve as a valuable reference for future scholarly endeavors. We extend an
invitation to the broader community to collaborate in enhancing the image-text
multimodal model community, accessible at:
\href{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00212">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves
Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy
Optimization (PPO) serving as the default RL optimizer. Despite its
effectiveness, PPO has limitations when optimizing rewards trained from
comparison-based loss. Primarily, PPO is not invariant to equivalent reward
functions containing identical preference information due to the need to
calibrate the reward scale. Additionally, PPO's necessity for token-wise
updates introduces complexity in both function approximation and algorithm
design compared to trajectory-wise optimization. This paper proposes a new
framework, reinforcement learning with relative feedback, and a novel
trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show
theoretically that P3O is invariant to equivalent rewards and avoids the
complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO
in the KL-Reward trade-off and can align with human preferences as well as or
better than prior methods. In summary, this work introduces a simpler yet
effective approach for aligning LLMs to human preferences through relative
feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RelBERT: Embedding Relations with Language Models. (arXiv:2310.00299v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00299">
<div class="article-summary-box-inner">
<span><p>Many applications need access to background knowledge about how different
concepts and entities are related. Although Knowledge Graphs (KG) and Large
Language Models (LLM) can address this need to some extent, KGs are inevitably
incomplete and their relational schema is often too coarse-grained, while LLMs
are inefficient and difficult to control. As an alternative, we propose to
extract relation embeddings from relatively small language models. In
particular, we show that masked language models such as RoBERTa can be
straightforwardly fine-tuned for this purpose, using only a small amount of
training data. The resulting model, which we call RelBERT, captures relational
similarity in a surprisingly fine-grained way, allowing us to set a new
state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of
modelling relations that go well beyond what the model has seen during
training. For instance, we obtained strong results on relations between named
entities with a model that was only trained on lexical relations between
concepts, and we observed that RelBERT can recognise morphological analogies
despite not being trained on such examples. Overall, we find that RelBERT
significantly outperforms strategies based on prompting language models that
are several orders of magnitude larger, including recent GPT-based models and
open source models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00378">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models (LLMs) have heightened concerns
about their potential misalignment with human values. However, evaluating their
grasp of these values is complex due to their intricate and adaptable nature.
We argue that truly understanding values in LLMs requires considering both
"know what" and "know why". To this end, we present the Value Understanding
Measurement (VUM) framework that quantitatively assess both "know what" and
"know why" by measuring the discriminator-critique gap related to human values.
Using the Schwartz Value Survey, we specify our evaluation values and develop a
thousand-level dialogue dataset with GPT-4. Our assessment looks at both the
value alignment of LLM's outputs compared to baseline answers and how LLM
responses align with reasons for value recognition versus GPT-4's annotations.
We evaluate five representative LLMs and provide strong evidence that the
scaling law significantly impacts "know what" but not much on "know why", which
has consistently maintained a high level. This may further suggest that LLMs
might craft plausible explanations based on the provided context without truly
understanding their inherent value, indicating potential risks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00533">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have showcased remarkable versatility across
diverse domains. However, the pathway toward autonomous model development, a
cornerstone for achieving human-level learning and advancing autonomous AI,
remains largely uncharted. We introduce an innovative approach, termed "SELF"
(Self-Evolution with Language Feedback). This methodology empowers LLMs to
undergo continual self-evolution. Furthermore, SELF employs language-based
feedback as a versatile and comprehensive evaluative tool, pinpointing areas
for response refinement and bolstering the stability of self-evolutionary
training. Initiating with meta-skill learning, SELF acquires foundational
meta-skills with a focus on self-feedback and self-refinement. These
meta-skills are critical, guiding the model's subsequent self-evolution through
a cycle of perpetual training with self-curated data, thereby enhancing its
intrinsic abilities. Given unlabeled instructions, SELF equips the model with
the capability to autonomously generate and interactively refine responses.
This synthesized training data is subsequently filtered and utilized for
iterative fine-tuning, enhancing the model's capabilities. Experimental results
on representative benchmarks substantiate that SELF can progressively advance
its inherent abilities without the requirement of human intervention, thereby
indicating a viable pathway for autonomous model evolution. Additionally, SELF
can employ online self-refinement strategy to produce responses of superior
quality. In essence, the SELF framework signifies a progressive step towards
autonomous LLM development, transforming the LLM from a mere passive recipient
of information into an active participant in its own evolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeRA: Label-Efficient Geometrically Regularized Alignment. (arXiv:2310.00672v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00672">
<div class="article-summary-box-inner">
<span><p>Pretrained unimodal encoders incorporate rich semantic information into
embedding space structures. To be similarly informative, multi-modal encoders
typically require massive amounts of paired data for alignment and training. We
introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method
to align the embedding spaces of pretrained unimodal encoders in a
label-efficient way. Our method leverages the manifold geometry of unpaired
(unlabeled) data to improve alignment performance. To prevent distortions to
local geometry during the alignment process, potentially disrupting semantic
neighborhood structures and causing misalignment of unobserved pairs, we
introduce a geometric loss term. This term is built upon a diffusion operator
that captures the local manifold geometry of the unimodal pretrained encoders.
GeRA is modality-agnostic and thus can be used to align pretrained encoders
from any data modalities. We provide empirical evidence to the effectiveness of
our method in the domains of speech-text and image-text alignment. Our
experiments demonstrate significant improvement in alignment quality compared
to a variaty of leading baselines, especially with a small amount of paired
data, using our proposed geometric regularization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models. (arXiv:2310.01074v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01074">
<div class="article-summary-box-inner">
<span><p>Temporal reasoning is a crucial NLP task, providing a nuanced understanding
of time-sensitive contexts within textual data. Although recent advancements in
LLMs have demonstrated their potential in temporal reasoning, the predominant
focus has been on tasks such as temporal expression and temporal relation
extraction. These tasks are primarily designed for the extraction of direct and
past temporal cues and to engage in simple reasoning processes. A significant
gap remains when considering complex reasoning tasks such as event forecasting,
which requires multi-step temporal reasoning on events and prediction on the
future timestamp. Another notable limitation of existing methods is their
incapability to provide an illustration of their reasoning process, hindering
explainability. In this paper, we introduce the first task of explainable
temporal reasoning, to predict an event's occurrence at a future timestamp
based on context which requires multiple reasoning over multiple events, and
subsequently provide a clear explanation for their prediction. Our task offers
a comprehensive evaluation of both the LLMs' complex temporal reasoning
ability, the future event prediction ability, and explainability-a critical
attribute for AI applications. To support this task, we present the first
multi-source instruction-tuning dataset of explainable temporal reasoning
(ExpTime) with 26k derived from the temporal knowledge graph datasets and their
temporal reasoning paths, using a novel knowledge-graph-instructed-generation
strategy. Based on the dataset, we propose the first open-source LLM series
TimeLlaMA based on the foundation LlaMA2, with the ability of instruction
following for explainable temporal reasoning. We compare the performance of our
method and a variety of LLMs, where our method achieves the state-of-the-art
performance of temporal prediction and explanation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01352">
<div class="article-summary-box-inner">
<span><p>Retrieval-augmented language models (RALMs) improve performance by accessing
long-tail and up-to-date knowledge from external data stores, but are
challenging to build. Existing approaches require either expensive
retrieval-specific modifications to LM pre-training or use post-hoc integration
of the data store that leads to suboptimal performance. We introduce
Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning
methodology that provides a third option by retrofitting any LLM with retrieval
capabilities. Our approach operates in two distinct fine-tuning steps: (1) one
updates a pre-trained LM to better use retrieved information, while (2) the
other updates the retriever to return more relevant results, as preferred by
the LM. By fine-tuning over tasks that require both knowledge utilization and
contextual awareness, we demonstrate that each stage yields significant
performance improvements, and using both leads to additional gains. Our best
model, RA-DIT 65B, achieves state-of-the-art performance across a range of
knowledge-intensive zero- and few-shot learning benchmarks, significantly
outperforming existing in-context RALM approaches by up to +8.9% in 0-shot
setting and +1.4% in 5-shot setting on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01432">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown promise as automated evaluators for
assessing the quality of answers generated by AI systems. However, these
LLM-based evaluators exhibit position bias, or inconsistency, when used to
evaluate candidate answers in pairwise comparisons, favoring either the first
or second answer regardless of content. To address this limitation, we propose
PORTIA, an alignment-based system designed to mimic human comparison strategies
to calibrate position bias in a lightweight yet effective manner. Specifically,
PORTIA splits the answers into multiple segments, aligns similar content across
candidate answers, and then merges them back into a single prompt for
evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to
evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances
the consistency rates for all the models and comparison forms tested, achieving
an average relative improvement of 47.46%. Remarkably, PORTIA enables less
advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4
model at just 10% of the cost. Furthermore, it rectifies around 80% of the
position bias instances within the GPT-4 model, elevating its consistency rate
up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced
GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with
human evaluators. These findings highlight PORTIA's ability to correct position
bias, improve LLM consistency, and boost performance while keeping
cost-efficiency. This represents a valuable step toward a more reliable and
scalable use of LLMs for automated evaluations across diverse applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.01801">
<div class="article-summary-box-inner">
<span><p>In this study, we introduce adaptive KV cache compression, a plug-and-play
method that reduces the memory footprint of generative inference for Large
Language Models (LLMs). Different from the conventional KV cache that retains
key and value vectors for all context tokens, we conduct targeted profiling to
discern the intrinsic structure of attention modules. Based on the recognized
structure, we then construct the KV cache in an adaptive manner: evicting
long-range contexts on attention heads emphasizing local contexts, discarding
non-special tokens on attention heads centered on special tokens, and only
employing the standard KV cache for attention heads that broadly attend to all
tokens. Moreover, with the lightweight attention profiling used to guide the
construction of the adaptive KV cache, FastGen can be deployed without
resource-intensive fine-tuning or re-training. In our experiments across
various asks, FastGen demonstrates substantial reduction on GPU memory
consumption with negligible generation quality loss. We will release our code
and the compatible CUDA kernel for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models. (arXiv:2310.02229v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02229">
<div class="article-summary-box-inner">
<span><p>Clinical texts, represented in electronic medical records (EMRs), contain
rich medical information and are essential for disease prediction, personalised
information recommendation, clinical decision support, and medication pattern
mining and measurement. Relation extractions between medication mentions and
temporal information can further help clinicians better understand the
patients' treatment history. To evaluate the performances of deep learning (DL)
and large language models (LLMs) in medication extraction and temporal
relations classification, we carry out an empirical investigation of
\textbf{MedTem} project using several advanced learning structures including
BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER),
and BERT-CNN for temporal relation extraction (RE), in addition to the
exploration of different word embedding techniques. Furthermore, we also
designed a set of post-processing roles to generate structured output on
medications and the temporal relation. Our experiments show that CNN-BiLSTM
slightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding
75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro
Average. BERT-CNN model also produced reasonable evaluation scores 64.48,
67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction
test set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted
at \url{https://github.com/HECTA-UoM/MedTem}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03094">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the "answer consistency" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03639">
<div class="article-summary-box-inner">
<span><p>The application of self-supervision to speech representation learning has
garnered significant interest in recent years, due to its scalability to large
amounts of unlabeled data. However, much progress, both in terms of
pre-training and downstream evaluation, has remained concentrated in
monolingual models that only consider English. Few models consider other
languages, and even fewer consider indigenous ones. In our submission to the
New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR
corpus for Quechua, an indigenous South American Language. We benchmark the
efficacy of large SSL models on Quechua, along with 6 other indigenous
languages such as Guarani and Bribri, on low-resource ASR. Our results show
surprisingly strong performance by state-of-the-art SSL models, showing the
potential generalizability of large-scale models to real-world data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03965">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved remarkable success in reasoning
tasks with the development of prompting methods. However, existing prompting
approaches cannot reuse insights of solving similar problems and suffer from
accumulated errors in multi-step reasoning, since they prompt LLMs to reason
\textit{from scratch}. To address these issues, we propose
\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous
problems and leverages their solutions to enhance the complex reasoning ability
of LLMs. These analogous problems are related to the input one, with reusable
solutions and problem-solving strategies. Thus, it is promising to propagate
insights of solving previous analogous problems to inspire new problem-solving.
To achieve this, TP first prompts LLMs to propose and solve a set of analogous
problems that are related to the input one. Then, TP reuses the results of
analogous problems to directly yield a new solution or derive a
knowledge-intensive plan for execution to amend the initial solution obtained
from scratch. TP is compatible with existing prompting approaches, allowing
plug-and-play generalization and enhancement in a wide range of tasks without
much labor in task-specific prompt engineering. Experiments across three
challenging tasks demonstrate TP enjoys a substantial improvement over the
baselines by an average of 12\% absolute increase in finding the optimal
solutions in Shortest-path Reasoning, 13\% improvement of human preference in
Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent
Planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.02469">
<div class="article-summary-box-inner">
<span><p>The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-10-10 23:11:19.058959998 UTC">2023-10-10 23:11:19 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>