<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-14T01:30:00Z">04-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Biases through the Text-to-Image Generation Lens. (arXiv:2304.06034v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06034">
<div class="article-summary-box-inner">
<span><p>Text-to-Image (T2I) generation is enabling new applications that support
creators, designers, and general end users of productivity software by
generating illustrative content with high photorealism starting from a given
descriptive text as a prompt. Such models are however trained on massive
amounts of web data, which surfaces the peril of potential harmful biases that
may leak in the generation process itself. In this paper, we take a
multi-dimensional approach to studying and quantifying common social biases as
reflected in the generated images, by focusing on how occupations, personality
traits, and everyday situations are depicted across representations of
(perceived) gender, age, race, and geographical location. Through an extensive
set of both automated and human evaluation experiments we present findings for
two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that
there exist severe occupational biases of neutral prompts majorly excluding
groups of people from results for both models. Such biases can get mitigated by
increasing the amount of specification in the prompt itself, although the
prompting mitigation will not address discrepancies in image quality or other
usages of the model or its representations in other scenarios. Further, we
observe personality traits being associated with only a limited set of people
at the intersection of race, gender, and age. Finally, an analysis of
geographical location representations on everyday situations (e.g., park, food,
weddings) shows that for most situations, images generated through default
location-neutral prompts are closer and more similar to images generated for
locations of United States and Germany.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Fake Generated Scientific Abstracts. (arXiv:2304.06148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06148">
<div class="article-summary-box-inner">
<span><p>The widespread adoption of Large Language Models and publicly available
ChatGPT has marked a significant turning point in the integration of Artificial
Intelligence into people's everyday lives. The academic community has taken
notice of these technological advancements and has expressed concerns regarding
the difficulty of discriminating between what is real and what is artificially
generated. Thus, researchers have been working on developing effective systems
to identify machine-generated text. In this study, we utilize the GPT-3 model
to generate scientific paper abstracts through Artificial Intelligence and
explore various text representation methods when combined with Machine Learning
models with the aim of identifying machine-written text. We analyze the models'
performance and address several research questions that rise during the
analysis of the results. By conducting this research, we shed light on the
capabilities and limitations of Artificial Intelligence generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LINGO : Visually Debiasing Natural Language Instructions to Support Task Diversity. (arXiv:2304.06184v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06184">
<div class="article-summary-box-inner">
<span><p>Cross-task generalization is a significant outcome that defines mastery in
natural language understanding. Humans show a remarkable aptitude for this, and
can solve many different types of tasks, given definitions in the form of
textual instructions and a small set of examples. Recent work with pre-trained
language models mimics this learning style: users can define and exemplify a
task for the model to attempt as a series of natural language prompts or
instructions. While prompting approaches have led to higher cross-task
generalization compared to traditional supervised learning, analyzing 'bias' in
the task instructions given to the model is a difficult problem, and has thus
been relatively unexplored. For instance, are we truly modeling a task, or are
we modeling a user's instructions? To help investigate this, we develop LINGO,
a novel visual analytics interface that supports an effective, task-driven
workflow to (1) help identify bias in natural language task instructions, (2)
alter (or create) task instructions to reduce bias, and (3) evaluate
pre-trained model performance on debiased task instructions. To robustly
evaluate LINGO, we conduct a user study with both novice and expert instruction
creators, over a dataset of 1,616 linguistic tasks and their natural language
instructions, spanning 55 different languages. For both user groups, LINGO
promotes the creation of more difficult tasks for pre-trained models, that
contain higher linguistic diversity and lower instruction bias. We additionally
discuss how the insights learned in developing and evaluating LINGO can aid in
the design of future dashboards that aim to minimize the effort involved in
prompt creation across multiple domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using large language models for (de-)formalization and natural argumentation exercises for beginner's students. (arXiv:2304.06186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06186">
<div class="article-summary-box-inner">
<span><p>We describe two systems that use text-davinci-003, a large language model,
for the automatized correction of (i) exercises in translating back and forth
between natural language and the languages of propositional logic and
first-order predicate logic and (ii) exercises in writing simple arguments in
natural language in non-mathematical scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LeafAI: query generator for clinical cohort discovery rivaling a human programmer. (arXiv:2304.06203v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06203">
<div class="article-summary-box-inner">
<span><p>Objective: Identifying study-eligible patients within clinical databases is a
critical step in clinical research. However, accurate query design typically
requires extensive technical and biomedical expertise. We sought to create a
system capable of generating data model-agnostic queries while also providing
novel logical reasoning capabilities for complex clinical trial eligibility
criteria.
</p>
<p>Materials and Methods: The task of query creation from eligibility criteria
requires solving several text-processing problems, including named entity
recognition and relation extraction, sequence-to-sequence transformation,
normalization, and reasoning. We incorporated hybrid deep learning and
rule-based modules for these, as well as a knowledge base of the Unified
Medical Language System (UMLS) and linked ontologies. To enable data-model
agnostic query creation, we introduce a novel method for tagging database
schema elements using UMLS concepts. To evaluate our system, called LeafAI, we
compared the capability of LeafAI to a human database programmer to identify
patients who had been enrolled in 8 clinical trials conducted at our
institution. We measured performance by the number of actual enrolled patients
matched by generated queries.
</p>
<p>Results: LeafAI matched a mean 43% of enrolled patients with 27,225 eligible
across 8 clinical trials, compared to 27% matched and 14,587 eligible in
queries by a human database programmer. The human programmer spent 26 total
hours crafting queries compared to several minutes by LeafAI.
</p>
<p>Conclusions: Our work contributes a state-of-the-art data model-agnostic
query generation system capable of conditional reasoning using a knowledge
base. We demonstrate that LeafAI can rival a human programmer in finding
patients eligible for clinical trials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model. (arXiv:2304.06248v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06248">
<div class="article-summary-box-inner">
<span><p>Universally modeling all typical information extraction tasks (UIE) with one
generative language model (GLM) has revealed great potential by the latest
study, where various IE predictions are unified into a linearized hierarchical
expression under a GLM. Syntactic structure information, a type of effective
feature which has been extensively utilized in IE community, should also be
beneficial to UIE. In this work, we propose a novel structure-aware GLM, fully
unleashing the power of syntactic knowledge for UIE. A heterogeneous structure
inductor is explored to unsupervisedly induce rich heterogeneous structural
representations by post-training an existing GLM. In particular, a structural
broadcaster is devised to compact various latent trees into explicit high-order
forests, helping to guide a better generation during decoding. We finally
introduce a task-oriented structure fine-tuning mechanism, further adjusting
the learned structures to most coincide with the end-task's need. Over 12 IE
benchmarks across 7 tasks our system shows significant improvements over the
baseline UIE system. Further in-depth analyses show that our GLM learns rich
task-adaptive structural bias that greatly resolves the UIE crux, the
long-range dependence issue and boundary identifying. Source codes are open at
https://github.com/ChocoWu/LasUIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rule-based detection of access to education and training in Germany. (arXiv:2304.06307v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06307">
<div class="article-summary-box-inner">
<span><p>As a result of transformation processes, the German labor market is highly
dependent on vocational training, retraining and continuing education. To match
training seekers and offers, we present a novel approach towards the automated
detection of access to education and training in German training offers and
advertisements. We will in particular focus on (a) general school and education
degrees and schoolleaving certificates, (b) professional experience, (c) a
previous apprenticeship and (d) a list of skills provided by the German Federal
Employment Agency. This novel approach combines several methods: First, we
provide a mapping of synonyms in education combining different qualifications
and adding deprecated terms. Second, we provide a rule-based matching to
identify the need for professional experience or apprenticeship. However, not
all access requirements can be matched due to incompatible data schemata or
non-standardizes requirements, e.g initial tests or interviews. While we can
identify several shortcomings, the presented approach offers promising results
for two data sets: training and re-training advertisements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational modeling of semantic change. (arXiv:2304.06337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06337">
<div class="article-summary-box-inner">
<span><p>In this chapter we provide an overview of computational modeling for semantic
change using large and semi-large textual corpora. We aim to provide a key for
the interpretation of relevant methods and evaluation techniques, and also
provide insights into important aspects of the computational study of semantic
change. We discuss the pros and cons of different classes of models with
respect to the properties of the data from which one wishes to model semantic
change, and which avenues are available to evaluate the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06364">
<div class="article-summary-box-inner">
<span><p>Evaluating the general abilities of foundation models to tackle human-level
tasks is a vital aspect of their development and application in the pursuit of
Artificial General Intelligence (AGI). Traditional benchmarks, which rely on
artificial datasets, may not accurately represent human-level capabilities. In
this paper, we introduce AGIEval, a novel benchmark specifically designed to
assess foundation model in the context of human-centric standardized exams,
such as college entrance exams, law school admission tests, math competitions,
and lawyer qualification tests. We evaluate several state-of-the-art foundation
models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.
Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math
competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%
accuracy on the English test of the Chinese national college entrance exam.
This demonstrates the extraordinary performance of contemporary foundation
models. In contrast, we also find that GPT-4 is less proficient in tasks that
require complex reasoning or specific domain knowledge. Our comprehensive
analyses of model capabilities (understanding, knowledge, reasoning, and
calculation) reveal these models' strengths and limitations, providing valuable
insights into future directions for enhancing their general capabilities. By
concentrating on tasks pertinent to human cognition and decision-making, our
benchmark delivers a more meaningful and robust evaluation of foundation
models' performance in real-world scenarios. The data, code, and all model
outputs are released in https://github.com/microsoft/AGIEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign Language Translation from Instructional Videos. (arXiv:2304.06371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06371">
<div class="article-summary-box-inner">
<span><p>The advances in automatic sign language translation (SLT) to spoken languages
have been mostly benchmarked with datasets of limited size and restricted
domains. Our work advances the state of the art by providing the first baseline
results on How2Sign, a large and broad dataset.
</p>
<p>We train a Transformer over I3D video features, using the reduced BLEU as a
reference metric for validation, instead of the widely used BLEU score. We
report a result of 8.03 on the BLEU score, and publish the first open-source
implementation of its kind to promote further advances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards hypergraph cognitive networks as feature-rich models of knowledge. (arXiv:2304.06375v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06375">
<div class="article-summary-box-inner">
<span><p>Semantic networks provide a useful tool to understand how related concepts
are retrieved from memory. However, most current network approaches use
pairwise links to represent memory recall patterns. Pairwise connections
neglect higher-order associations, i.e. relationships between more than two
concepts at a time. These higher-order interactions might covariate with (and
thus contain information about) how similar concepts are along psycholinguistic
dimensions like arousal, valence, familiarity, gender and others. We overcome
these limits by introducing feature-rich cognitive hypergraphs as quantitative
models of human memory where: (i) concepts recalled together can all engage in
hyperlinks involving also more than two concepts at once (cognitive hypergraph
aspect), and (ii) each concept is endowed with a vector of psycholinguistic
features (feature-rich aspect). We build hypergraphs from word association data
and use evaluation methods from machine learning features to predict concept
concreteness. Since concepts with similar concreteness tend to cluster together
in human memory, we expect to be able to leverage this structure. Using word
association data from the Small World of Words dataset, we compared a pairwise
network and a hypergraph with N=3586 concepts/nodes. Interpretable artificial
intelligence models trained on (1) psycholinguistic features only, (2)
pairwise-based feature aggregations, and on (3) hypergraph-based aggregations
show significant differences between pairwise and hypergraph links.
Specifically, our results show that higher-order and feature-rich hypergraph
models contain richer information than pairwise networks leading to improved
prediction of word concreteness. The relation with previous studies about
conceptual clustering and compartmentalisation in associative knowledge and
human memory are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06377">
<div class="article-summary-box-inner">
<span><p>Being able to create meaningful symbols and proficiently use them for higher
cognitive functions such as communication, reasoning, planning, etc., is
essential and unique for human intelligence. Current deep neural networks are
still far behind human's ability to create symbols for such higher cognitive
functions. Here we propose a solution, named SEA-net, to endow neural networks
with ability of symbol creation, semantic understanding and communication.
SEA-net generates symbols that dynamically configure the network to perform
specific tasks. These symbols capture compositional semantic information that
enables the system to acquire new functions purely by symbolic manipulation or
communication. In addition, we found that these self-generated symbols exhibit
an intrinsic structure resembling that of natural language, suggesting a common
framework underlying the generation and understanding of symbols in both human
brains and artificial neural networks. We hope that it will be instrumental in
producing more capable systems in the future that can synergize the strengths
of connectionist and symbolic approaches for AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06446">
<div class="article-summary-box-inner">
<span><p>Vision transformers have been applied successfully for image recognition
tasks. There have been either multi-headed self-attention based (ViT
\cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the
original work in textual models or more recently based on spectral layers
(Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global},
AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and
multi-headed attention plays a major role. We investigate this hypothesis
through this work and observe that indeed combining spectral and multi-headed
attention layers provides a better transformer architecture. We thus propose
the novel Spectformer architecture for transformers that combines spectral and
multi-headed attention layers. We believe that the resulting representation
allows the transformer to capture the feature representation appropriately and
it yields improved performance over other transformer representations. For
instance, it improves the top-1 accuracy by 2\% on ImageNet compared to both
GFNet-H and LiT. SpectFormer-S reaches 84.25\% top-1 accuracy on ImageNet-1K
(state of the art for small version). Further, Spectformer-L achieves 85.7\%
that is the state of the art for the comparable base version of the
transformers. We further ensure that we obtain reasonable results in other
scenarios such as transfer learning on standard datasets such as CIFAR-10,
CIFAR-100, Oxford-IIIT-flower, and Standford Car datasets. We then investigate
its use in downstream tasks such of object detection and instance segmentation
on the MS-COCO dataset and observe that Spectformer shows consistent
performance that is comparable to the best backbones and can be further
optimized and improved. Hence, we believe that combined spectral and attention
layers are what are needed for vision transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06447">
<div class="article-summary-box-inner">
<span><p>Document-based Visual Question Answering examines the document understanding
of document images in conditions of natural language questions. We proposed a
new document-based VQA dataset, PDF-VQA, to comprehensively examine the
document understanding from various aspects, including document element
recognition, document layout structural understanding as well as contextual
understanding and key information extraction. Our PDF-VQA dataset extends the
current scale of document understanding that limits on the single document page
to the new scale that asks questions over the full document of multiple pages.
We also propose a new graph-based VQA model that explicitly integrates the
spatial and hierarchically structural relationships between different document
elements to boost the document structural understanding. The performances are
compared with several baselines over different question types and
tasks\footnote{The full dataset will be released after paper acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages. (arXiv:2304.06459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06459">
<div class="article-summary-box-inner">
<span><p>AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform
monolingual sentiment classification (sub-task A) for 12 African languages,
multilingual sentiment classification (sub-task B), and zero-shot sentiment
classification (task C). For sub-task A, we conducted experiments using
classical machine learning classifiers, Afro-centric language models, and
language-specific models. For task B, we fine-tuned multilingual pre-trained
language models that support many of the languages in the task. For task C, we
used we make use of a parameter-efficient Adapter approach that leverages
monolingual texts in the target language for effective zero-shot transfer. Our
findings suggest that using pre-trained Afro-centric language models improves
performance for low-resource African languages. We also ran experiments using
adapters for zero-shot tasks, and the results suggest that we can obtain
promising results by using adapters with a limited amount of resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. (arXiv:2304.06488v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06488">
<div class="article-summary-box-inner">
<span><p>OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is
demonstrated to be one small step for generative AI (GAI), but one giant leap
for artificial general intelligence (AGI). Since its official release in
November 2022, ChatGPT has quickly attracted numerous users with extensive
media coverage. Such unprecedented attention has also motivated numerous
researchers to investigate ChatGPT from various aspects. According to Google
scholar, there are more than 500 articles with ChatGPT in their titles or
mentioning it in their abstracts. Considering this, a review is urgently
needed, and our work fills this gap. Overall, this work is the first to survey
ChatGPT with a comprehensive review of its underlying technology, applications,
and challenges. Moreover, we present an outlook on how ChatGPT might evolve to
realize general-purpose AIGC (a.k.a. AI-generated content), which will be a
significant milestone for the development of AGI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are LLMs All You Need for Task-Oriented Dialogue?. (arXiv:2304.06556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06556">
<div class="article-summary-box-inner">
<span><p>Instructions-tuned Large Language Models (LLMs) gained recently huge
popularity thanks to their ability to interact with users through conversation.
In this work we aim to evaluate their ability to complete multi-turn tasks and
interact with external databases in the context of established task-oriented
dialogue benchmarks. We show that for explicit belief state tracking, LLMs
underperform compared to specialized task-specific models. Nevertheless, they
show ability to guide the dialogue to successful ending if given correct slot
values. Furthermore this ability improves with access to true belief state
distribution or in-domain examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. (arXiv:2304.06588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06588">
<div class="article-summary-box-inner">
<span><p>This paper assesses the accuracy, reliability and bias of the Large Language
Model (LLM) ChatGPT-4 on the text analysis task of classifying the political
affiliation of a Twitter poster based on the content of a tweet. The LLM is
compared to manual annotation by both expert classifiers and crowd workers,
generally considered the gold standard for such tasks. We use Twitter messages
from United States politicians during the 2020 election, providing a ground
truth against which to measure accuracy. The paper finds that ChatGPT-4 has
achieves higher accuracy, higher reliability, and equal or lower bias than the
human classifiers. The LLM is able to correctly annotate messages that require
reasoning on the basis of contextual knowledge, and inferences around the
author's intentions - traditionally seen as uniquely human abilities. These
findings suggest that LLM will have substantial impact on the use of textual
data in the social sciences, by enabling interpretive research at a scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06623">
<div class="article-summary-box-inner">
<span><p>Answering questions related to the legal domain is a complex task, primarily
due to the intricate nature and diverse range of legal document systems.
Providing an accurate answer to a legal query typically necessitates
specialized knowledge in the relevant domain, which makes this task all the
more challenging, even for human experts. QA (Question answering systems) are
designed to generate answers to questions asked in human languages. They use
natural language processing to understand questions and search through
information to find relevant answers. QA has various practical applications,
including customer service, education, research, and cross-lingual
communication. However, they face challenges such as improving natural language
understanding and handling complex and ambiguous questions. Answering questions
related to the legal domain is a complex task, primarily due to the intricate
nature and diverse range of legal document systems. Providing an accurate
answer to a legal query typically necessitates specialized knowledge in the
relevant domain, which makes this task all the more challenging, even for human
experts. At this time, there is a lack of surveys that discuss legal question
answering. To address this problem, we provide a comprehensive survey that
reviews 14 benchmark datasets for question-answering in the legal field as well
as presents a comprehensive review of the state-of-the-art Legal Question
Answering deep learning models. We cover the different architectures and
techniques used in these studies and the performance and limitations of these
models. Moreover, we have established a public GitHub repository where we
regularly upload the most recent articles, open data, and source code. The
repository is available at:
\url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06634">
<div class="article-summary-box-inner">
<span><p>Recent approaches have attempted to personalize dialogue systems by
leveraging profile information into models. However, this knowledge is scarce
and difficult to obtain, which makes the extraction/generation of profile
information from dialogues a fundamental asset. To surpass this limitation, we
introduce the Profile Generation Task (PGTask). We contribute with a new
dataset for this problem, comprising profile sentences aligned with related
utterances, extracted from a corpus of dialogues. Furthermore, using
state-of-the-art methods, we provide a benchmark for profile generation on this
novel dataset. Our experiments disclose the challenges of profile generation,
and we hope that this introduces a new research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Useful are Educational Questions Generated by Large Language Models?. (arXiv:2304.06638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06638">
<div class="article-summary-box-inner">
<span><p>Controllable text generation (CTG) by large language models has a huge
potential to transform education for teachers and students alike. Specifically,
high quality and diverse question generation can dramatically reduce the load
on teachers and improve the quality of their educational content. Recent work
in this domain has made progress with generation, but fails to show that real
teachers judge the generated questions as sufficiently useful for the classroom
setting; or if instead the questions have errors and/or pedagogically unhelpful
content. We conduct a human evaluation with teachers to assess the quality and
usefulness of outputs from combining CTG and question taxonomies (Bloom's and a
difficulty taxonomy). The results demonstrate that the questions generated are
high quality and sufficiently useful, showing their promise for widespread use
in the classroom setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06653">
<div class="article-summary-box-inner">
<span><p>It has been reported that clustering-based topic models, which cluster
high-quality sentence embeddings with an appropriate word selection method, can
generate better topics than generative probabilistic topic models. However,
these approaches suffer from the inability to select appropriate parameters and
incomplete models that overlook the quantitative relation between words with
topics and topics with text. To solve these issues, we propose graph to topic
(G2T), a simple but effective framework for topic modelling. The framework is
composed of four modules. First, document representation is acquired using
pretrained language models. Second, a semantic graph is constructed according
to the similarity between document representations. Third, communities in
document semantic graphs are identified, and the relationship between topics
and documents is quantified accordingly. Fourth, the word--topic distribution
is computed based on a variant of TFIDF. Automatic evaluation suggests that G2T
achieved state-of-the-art performance on both English and Chinese documents
with different lengths. Human judgements demonstrate that G2T can produce
topics with better interpretability and coverage than baselines. In addition,
G2T can not only determine the topic number automatically but also give the
probabilistic distribution of words in topics and topics in documents. Finally,
G2T is publicly available, and the distillation experiments provide instruction
on how it works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06671">
<div class="article-summary-box-inner">
<span><p>Spatial control is a core capability in controllable image generation.
Advancements in layout-guided image generation have shown promising results on
in-distribution (ID) datasets with similar spatial configurations. However, it
is unclear how these models perform when facing out-of-distribution (OOD)
samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,
a diagnostic benchmark for layout-guided image generation that examines four
categories of spatial control skills: number, position, size, and shape. We
benchmark two recent representative layout-guided image generation methods and
observe that the good ID layout control may not generalize well to arbitrary
layouts in the wild (e.g., objects at the boundary). Next, we propose
IterInpaint, a new baseline that generates foreground and background regions in
a step-by-step manner via inpainting, demonstrating stronger generalizability
than existing models on OOD layouts in LayoutBench. We perform quantitative and
qualitative evaluation and fine-grained analysis on the four LayoutBench skills
to pinpoint the weaknesses of existing models. Lastly, we show comprehensive
ablation studies on IterInpaint, including training task ratio, crop&amp;paste vs.
repaint, and generation order. Project website: https://layoutbench.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Verbs in Action: Improving verb understanding in video-language models. (arXiv:2304.06708v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06708">
<div class="article-summary-box-inner">
<span><p>Understanding verbs is crucial to modelling how people and objects interact
with each other and the environment through space and time. Recently,
state-of-the-art video-language models based on CLIP have been shown to have
limited verb understanding and to rely extensively on nouns, restricting their
performance in real-world video applications that require action and temporal
understanding. In this work, we improve verb understanding for CLIP-based
video-language models by proposing a new Verb-Focused Contrastive (VFC)
framework. This consists of two main components: (1) leveraging pretrained
large language models (LLMs) to create hard negatives for cross-modal
contrastive learning, together with a calibration strategy to balance the
occurrence of concepts in positive and negative pairs; and (2) enforcing a
fine-grained, verb phrase alignment loss. Our method achieves state-of-the-art
results for zero-shot performance on three downstream tasks that focus on verb
understanding: video-text matching, video question-answering and video
classification. To the best of our knowledge, this is the first work which
proposes a method to alleviate the verb understanding problem, and does not
simply highlight it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge. (arXiv:2202.07138v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07138">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) aims at investigating the interactions
between agents and humans, processing and analyzing large amounts of natural
language data. Large-scale language models play an important role in current
natural language processing. However, the challenges of explainability and
complexity come along with the developments of language models. One way is to
introduce logical relations and rules into natural language processing models,
such as making use of Automated Planning. Automated planning (AI planning)
focuses on building symbolic domain models and synthesizing plans to transit
initial states to goals based on domain models. Recently, there have been
plenty of works related to these two fields, which have the abilities to
generate explicit knowledge, e.g., preconditions and effects of action models,
and learn from tacit knowledge, e.g., neural models, respectively. Integrating
AI planning and natural language processing effectively improves the
communication between human and intelligent agents. This paper outlines the
commons and relations between AI planning and natural language processing,
argues that each of them can effectively impact on the other one by five areas:
(1) planning-based text understanding, (2) planning-based natural language
processing, (3) planning-based explainability, (4) text-based human-robot
interaction, and (5) applications. We also explore some potential future issues
between AI planning and natural language processing. To the best of our
knowledge, this survey is the first work that addresses the deep connections
between AI planning and Natural language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PePe: Personalized Post-editing Model utilizing User-generated Post-edits. (arXiv:2209.10139v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.10139">
<div class="article-summary-box-inner">
<span><p>Incorporating personal preference is crucial in advanced machine translation
tasks. Despite the recent advancement of machine translation, it remains a
demanding task to properly reflect personal style. In this paper, we introduce
a personalized automatic post-editing framework to address this challenge,
which effectively generates sentences considering distinct personal behaviors.
To build this framework, we first collect post-editing data that connotes the
user preference from a live machine translation system. Specifically,
real-world users enter source sentences for translation and edit the
machine-translated outputs according to the user's preferred style. We then
propose a model that combines a discriminator module and user-specific
parameters on the APE framework. Experimental results show that the proposed
method outperforms other baseline models on four different metrics (i.e., BLEU,
TER, YiSi-1, and human evaluation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models. (arXiv:2210.05619v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05619">
<div class="article-summary-box-inner">
<span><p>While multilingual language models can improve NLP performance on
low-resource languages by leveraging higher-resource languages, they also
reduce average performance on all languages (the 'curse of multilinguality').
Here we show another problem with multilingual models: grammatical structures
in higher-resource languages bleed into lower-resource languages, a phenomenon
we call grammatical structure bias. We show this bias via a novel method for
comparing the fluency of multilingual models to the fluency of monolingual
Spanish and Greek models: testing their preference for two carefully-chosen
variable grammatical structures (optional pronoun-drop in Spanish and optional
Subject-Verb ordering in Greek). We find that multilingual BERT is biased
toward the English-like setting (explicit pronouns and Subject-Verb-Object
ordering) as compared to our monolingual control language model. With our case
studies, we hope to bring to light the fine-grained ways in which multilingual
models can be biased,and encourage more linguistically-aware fluency
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07474">
<div class="article-summary-box-inner">
<span><p>We propose a new task to benchmark scene understanding of embodied agents:
Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,
3D scan), SQA3D requires the tested agent to first understand its situation
(position, orientation, etc.) in the 3D scene as described by text, then reason
about its surrounding environment and answer a question under that situation.
Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k
unique situations, along with 20.4k descriptions and 33.4k diverse reasoning
questions for these situations. These questions examine a wide spectrum of
reasoning capabilities for an intelligent agent, ranging from spatial relation
comprehension to commonsense understanding, navigation, and multi-hop
reasoning. SQA3D imposes a significant challenge to current multi-modal
especially 3D reasoning models. We evaluate various state-of-the-art approaches
and find that the best one only achieves an overall score of 47.20%, while
amateur human participants can reach 90.06%. We believe SQA3D could facilitate
future embodied AI research with stronger situation understanding and reasoning
capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05961">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a widely used technique in machine learning to improve
model performance. However, existing data augmentation techniques in natural
language understanding (NLU) may not fully capture the complexity of natural
language variations, and they can be challenging to apply to large datasets.
This paper proposes the Random Position Noise (RPN) algorithm, a novel data
augmentation technique that operates at the word vector level. RPN modifies the
word embeddings of the original text by introducing noise based on the existing
values of selected word vectors, allowing for more fine-grained modifications
and better capturing natural language variations. Unlike traditional data
augmentation methods, RPN does not require gradients in the computational graph
during virtual sample updates, making it simpler to apply to large datasets.
Experimental results demonstrate that RPN consistently outperforms existing
data augmentation techniques across various NLU tasks, including sentiment
analysis, natural language inference, and paraphrase detection. Moreover, RPN
performs well in low-resource settings and is applicable to any model featuring
a word embeddings layer. The proposed RPN algorithm is a promising approach for
enhancing NLU performance and addressing the challenges associated with
traditional data augmentation techniques in large-scale NLU tasks. Our
experimental results demonstrated that the RPN algorithm achieved
state-of-the-art performance in all seven NLU tasks, thereby highlighting its
effectiveness and potential for real-world NLU applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Multilingual Semantic Parser. (arXiv:2301.12920v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.12920">
<div class="article-summary-box-inner">
<span><p>Current multilingual semantic parsing (MSP) datasets are almost all collected
by translating the utterances in the existing datasets from the resource-rich
language to the target language. However, manual translation is costly. To
reduce the translation effort, this paper proposes the first active learning
procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing
datasets to be translated. We also propose a novel selection method that
prioritizes the examples diversifying the logical form structures with more
lexical choices, and a novel hyperparameter tuning method that needs no extra
annotation cost. Our experiments show that AL-MSP significantly reduces
translation costs with ideal selection methods. Our selection method with
proper hyperparameters yields better parsing performance than the other
baselines on two multilingual datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EvoText: Enhancing Natural Language Generation Models via Self-Escalation Learning for Up-to-Date Knowledge and Improved Performance. (arXiv:2302.03896v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03896">
<div class="article-summary-box-inner">
<span><p>In recent years, pretrained models have been widely used in various fields,
including natural language understanding, computer vision, and natural language
generation. However, the performance of these language generation models is
highly dependent on the model size and the dataset size. While larger models
excel in some aspects, they cannot learn up-to-date knowledge and are
relatively difficult to relearn. In this paper, we introduce EvoText, a novel
training method that enhances the performance of any natural language
generation model without requiring additional datasets during the entire
training process (although a prior dataset is necessary for pretraining).
EvoText employs two models: $G$, a text generation model, and $D$, a model that
can determine whether the data generated by $G$ is legitimate. Initially, the
fine-tuned $D$ model serves as the knowledge base. The text generated by $G$ is
then input to $D$ to determine whether it is legitimate. Finally, $G$ is
fine-tuned based on $D$'s output. EvoText enables the model to learn up-to-date
knowledge through a self-escalation process that builds on a priori knowledge.
When EvoText needs to learn something new, it simply fine-tunes the $D$ model.
Our approach applies to autoregressive language modeling for all Transformer
classes. With EvoText, eight models achieved stable improvements in seven
natural language processing tasks without any changes to the model structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04054">
<div class="article-summary-box-inner">
<span><p>Reliability of machine learning evaluation -- the consistency of observed
evaluation scores across replicated model training runs -- is affected by
several sources of nondeterminism which can be regarded as measurement noise.
Current tendencies to remove noise in order to enforce reproducibility of
research results neglect inherent nondeterminism at the implementation level
and disregard crucial interaction effects between algorithmic noise factors and
data properties. This limits the scope of conclusions that can be drawn from
such experiments. Instead of removing noise, we propose to incorporate several
sources of variance, including their interaction with data properties, into an
analysis of significance and reliability of machine learning evaluation, with
the aim to draw inferences beyond particular instances of trained models. We
show how to use linear mixed effects models (LMEMs) to analyze performance
evaluation scores, and to conduct statistical inference with a generalized
likelihood ratio test (GLRT). This allows us to incorporate arbitrary sources
of noise like meta-parameter variations into statistical significance testing,
and to assess performance differences conditional on data properties.
Furthermore, a variance component analysis (VCA) enables the analysis of the
contribution of noise sources to overall variance and the computation of a
reliability coefficient by the ratio of substantial to total variance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hulk: Graph Neural Networks for Optimizing Regionally Distributed Computing Systems. (arXiv:2302.13741v2 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.13741">
<div class="article-summary-box-inner">
<span><p>Large deep learning models have shown great potential for delivering
exceptional results in various applications. However, the training process can
be incredibly challenging due to the models' vast parameter sizes, often
consisting of hundreds of billions of parameters. Common distributed training
methods, such as data parallelism, tensor parallelism, and pipeline
parallelism, demand significant data communication throughout the process,
leading to prolonged wait times for some machines in physically distant
distributed systems. To address this issue, we propose a novel solution called
Hulk, which utilizes a modified graph neural network to optimize distributed
computing systems. Hulk not only optimizes data communication efficiency
between different countries or even different regions within the same city, but
also provides optimal distributed deployment of models in parallel. For
example, it can place certain layers on a machine in a specific region or pass
specific parameters of a model to a machine in a particular location. By using
Hulk in experiments, we were able to improve the time efficiency of training
large deep learning models on distributed systems by more than 20\%. Our open
source collection of unlabeled data:https://github.com/DLYuanGod/Hulk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT as a Factual Inconsistency Evaluator for Text Summarization. (arXiv:2303.15621v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15621">
<div class="article-summary-box-inner">
<span><p>The performance of text summarization has been greatly boosted by pre-trained
language models. A main concern of existing methods is that most generated
summaries are not factually inconsistent with their source documents. To
alleviate the problem, many efforts have focused on developing effective
factuality evaluation metrics based on natural language inference, question
answering, and syntactic dependency et al. However, these approaches are
limited by either their high computational complexity or the uncertainty
introduced by multi-component pipelines, resulting in only partial agreement
with human judgement. Most recently, large language models(LLMs) have shown
excellent performance in not only text generation but also language
comprehension. In this paper, we particularly explore ChatGPT's ability to
evaluate factual inconsistency under a zero-shot setting by examining it on
both coarse-grained and fine-grained evaluation tasks including binary
entailment inference, summary ranking, and consistency rating. Experimental
results indicate that ChatGPT generally outperforms previous evaluation metrics
across the three tasks, indicating its great potential for factual
inconsistency evaluation. However, a closer inspection of ChatGPT's output
reveals certain limitations including its preference for more lexically similar
candidates, false reasoning, and inadequate understanding of instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02017">
<div class="article-summary-box-inner">
<span><p>Large language models have revolutionized the field of artificial
intelligence and have been used in various applications. Among these models,
ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,
it stands out as a powerful tool that has been widely adopted. ChatGPT has been
successfully applied in numerous areas, including chatbots, content generation,
language translation, personalized recommendations, and even medical diagnosis
and treatment. Its success in these applications can be attributed to its
ability to generate human-like responses, understand natural language, and
adapt to different contexts. Its versatility and accuracy make it a powerful
tool for natural language processing (NLP). However, there are also limitations
to ChatGPT, such as its tendency to produce biased responses and its potential
to perpetuate harmful language patterns. This article provides a comprehensive
overview of ChatGPT, its applications, advantages, and limitations.
Additionally, the paper emphasizes the importance of ethical considerations
when using this robust tool in real-world scenarios. Finally, This paper
contributes to ongoing discussions surrounding artificial intelligence and its
impact on vision and NLP domains by providing insights into prompt engineering
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.04370">
<div class="article-summary-box-inner">
<span><p>Human intelligence has the remarkable ability to assemble basic skills into
complex ones so as to solve complex tasks. This ability is equally important
for Artificial Intelligence (AI), and thus, we assert that in addition to the
development of large, comprehensive intelligent models, it is equally crucial
to equip such models with the capability to harness various domain-specific
expert models for complex task-solving in the pursuit of Artificial General
Intelligence (AGI). Recent developments in Large Language Models (LLMs) have
demonstrated remarkable learning and reasoning abilities, making them promising
as a controller to select, synthesize, and execute external models to solve
complex tasks. In this project, we develop OpenAGI, an open-source AGI research
platform, specifically designed to offer complex, multi-step tasks and
accompanied by task-specific datasets, evaluation metrics, and a diverse range
of extensible models. OpenAGI formulates complex tasks as natural language
queries, serving as input to the LLM. The LLM subsequently selects,
synthesizes, and executes models provided by OpenAGI to address the task.
Furthermore, we propose a Reinforcement Learning from Task Feedback (RLTF)
mechanism, which uses the task-solving result as feedback to improve the LLM's
task-solving ability. Thus, the LLM is responsible for synthesizing various
external models for solving complex tasks, while RLTF provides feedback to
improve its task-solving ability, enabling a feedback loop for self-improving
AI. We believe that the paradigm of LLMs operating various expert models for
complex task-solving is a promising approach towards AGI. To facilitate the
community's long-term improvement and evaluation of AGI's ability, we
open-source the code, benchmark, and evaluation methods of the OpenAGI project
at https://github.com/agiresearch/OpenAGI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05368">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have made significant progress in various
domains, including healthcare. However, the specialized nature of clinical
language understanding tasks presents unique challenges and limitations that
warrant further investigation. In this study, we conduct a comprehensive
evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within
the realm of clinical language understanding tasks. These tasks span a diverse
range, including named entity recognition, relation extraction, natural
language inference, semantic textual similarity, document classification, and
question-answering. We also introduce a novel prompting strategy,
self-questioning prompting (SQP), tailored to enhance LLMs' performance by
eliciting informative questions and answers pertinent to the clinical scenarios
at hand. Our evaluation underscores the significance of task-specific learning
strategies and prompting techniques for improving LLMs' effectiveness in
healthcare-related tasks. Additionally, our in-depth error analysis on the
challenging relation extraction task offers valuable insights into error
distribution and potential avenues for improvement using SQP. Our study sheds
light on the practical implications of employing LLMs in the specialized domain
of healthcare, serving as a foundation for future research and the development
of potential applications in healthcare settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05534">
<div class="article-summary-box-inner">
<span><p>Text-generative artificial intelligence (AI), including ChatGPT, equipped
with GPT-3.5 and GPT-4, from OpenAI, has attracted considerable attention
worldwide. In this study, first, we compared Japanese stylometric features
generated by GPT (-3.5 and -4) and those written by humans. In this work, we
performed multi-dimensional scaling (MDS) to confirm the distributions of 216
texts of three classes (72 academic papers written by 36 single authors, 72
texts generated by GPT-3.5, and 72 texts generated by GPT-4 on the basis of the
titles of the aforementioned papers) focusing on the following stylometric
features: (1) bigrams of parts-of-speech, (2) bigram of postpositional particle
words, (3) positioning of commas, and (4) rate of function words. MDS revealed
distinct distributions at each stylometric feature of GPT (-3.5 and -4) and
human. Although GPT-4 is more powerful than GPT-3.5 because it has more
parameters, both GPT (-3.5 and -4) distributions are likely to overlap. These
results indicate that although the number of parameters may increase in the
future, AI-generated texts may not be close to that written by humans in terms
of stylometric features. Second, we verified the classification performance of
random forest (RF) for two classes (GPT and human) focusing on Japanese
stylometric features. This study revealed the high performance of RF in each
stylometric feature. Furthermore, the RF classifier focusing on the rate of
function words achieved 98.1% accuracy. The RF classifier focusing on all
stylometric features reached 100% in terms of all performance indexes
(accuracy, recall, precision, and F1 score). This study concluded that at this
stage we human discriminate ChatGPT from human limited to Japanese language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05783">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been known to perpetuate biases from the
underlying datasets to downstream tasks. However, these findings are
predominantly based on monolingual language models for English, whereas there
are few investigative studies of biases encoded in language models for
languages beyond English. In this paper, we fill this gap by analysing gender
bias in West Slavic language models. We introduce the first template-based
dataset in Czech, Polish, and Slovak for measuring gender bias towards male,
female and non-binary subjects. We complete the sentences using both mono- and
multilingual language models and assess their suitability for the masked
language modelling objective. Next, we measure gender bias encoded in West
Slavic language models by quantifying the toxicity and genderness of the
generated words. We find that these language models produce hurtful completions
that depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and
Polish language models produce more hurtful completions with men as subjects,
which, upon inspection, we find is due to completions being related to
violence, death, and sickness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05860">
<div class="article-summary-box-inner">
<span><p>Homographs, words with the same spelling but different meanings, remain
challenging in Neural Machine Translation (NMT). While recent works leverage
various word embedding approaches to differentiate word sense in NMT, they do
not focus on the pivotal components in resolving ambiguities of homographs in
NMT: the hidden states of an encoder. In this paper, we propose a novel
approach to tackle homographic issues of NMT in the latent space. We first
train an encoder (aka "HDR-encoder") to learn universal sentence
representations in a natural language inference (NLI) task. We further
fine-tune the encoder using homograph-based synset sentences from WordNet,
enabling it to learn word-level homographic disambiguation representations
(HDR). The pre-trained HDR-encoder is subsequently integrated with a
transformer-based NMT in various schemes to improve translation accuracy.
Experiments on four translation directions demonstrate the effectiveness of the
proposed method in enhancing the performance of NMT systems in the BLEU scores
(up to +2.3 compared to a solid baseline). The effects can be verified by other
metrics (F1, precision, and recall) of translation accuracy in an additional
disambiguation task. Visualization methods like heatmaps, T-SNE and translation
examples are also utilized to demonstrate the effects of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-15 23:10:54.375888796 UTC">2023-04-15 23:10:54 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>