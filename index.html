<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2024-01-08T01:30:00Z">01-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02509">
<div class="article-summary-box-inner">
<span><p>With the development in cognitive science and Large Language Models (LLMs),
increasing connections have come to light between these two distinct fields.
Building upon these connections, we propose a conjecture suggesting the
existence of a duality between LLMs and Tulving's theory of memory. We identify
a potential correspondence between Tulving's synergistic ecphory model (SEM) of
retrieval and the emergent abilities observed in LLMs, serving as supporting
evidence for our conjecture. Furthermore, we speculate that consciousness may
be considered a form of emergent ability based on this duality. We also discuss
how other theories of consciousness intersect with our research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised hard Negative Augmentation for contrastive learning. (arXiv:2401.02594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02594">
<div class="article-summary-box-inner">
<span><p>We present Unsupervised hard Negative Augmentation (UNA), a method that
generates synthetic negative instances based on the term frequency-inverse
document frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to
ascertain the perceived importance of terms in a sentence and then produces
negative samples by replacing terms with respect to that. Our experiments
demonstrate that models trained with UNA improve the overall performance in
semantic textual similarity tasks. Additional performance gains are obtained
when combining UNA with the paraphrasing augmentation. Further results show
that our method is compatible with different backbone models. Ablation studies
also support the choice of having a TF-IDF-driven control on negative
augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02709">
<div class="article-summary-box-inner">
<span><p>This work introduces a benchmark assessing the performance of clustering
German text embeddings in different domains. This benchmark is driven by the
increasing use of clustering neural text embeddings in tasks that require the
grouping of texts (such as topic modeling) and the need for German resources in
existing benchmarks. We provide an initial analysis for a range of pre-trained
mono- and multilingual models evaluated on the outcome of different clustering
algorithms. Results include strong performing mono- and multilingual models.
Reducing the dimensions of embeddings can further improve clustering.
Additionally, we conduct experiments with continued pre-training for German
BERT models to estimate the benefits of this additional training. Our
experiments suggest that significant performance improvements are possible for
short text. All code and datasets are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning. (arXiv:2401.02744v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02744">
<div class="article-summary-box-inner">
<span><p>Neuron labeling is an approach to visualize the behaviour and respond of a
certain neuron to a certain pattern that activates the neuron. Neuron labeling
extract information about the features captured by certain neurons in a deep
neural network, one of which uses the encoder-decoder image captioning
approach. The encoder used can be a pretrained CNN-based model and the decoder
is an RNN-based model for text generation. Previous work, namely MILAN (Mutual
Information-guided Linguistic Annotation of Neuron), has tried to visualize the
neuron behaviour using modified Show, Attend, and Tell (SAT) model in the
encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show
great result on short sequence neuron captioning, but it does not show great
result on long sequence neuron captioning, so in this work, we would like to
improve the performance of MILAN even more by utilizing different kind of
attention mechanism and additionally adding several attention result into one,
in order to combine all the advantages from several attention mechanism. Using
our compound dataset, we obtained higher BLEU and F1-Score on our proposed
model, achieving 17.742 and 0.4811 respectively. At some point where the model
converges at the peak, our model obtained BLEU of 21.2262 and BERTScore
F1-Score of 0.4870.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02749">
<div class="article-summary-box-inner">
<span><p>Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to
beam search decoding for a wide range of text generation tasks. However, MBR
requires a huge amount of time for inference to compute the MBR objective,
which makes the method infeasible in many situations where response time is
critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently
been proposed to reduce the inference time in machine translation tasks.
Although it is shown to significantly reduce the amount of computation, it
requires hyperparameter tuning using a development set to be effective. To this
end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a
hyperparameter-free method to run MBR decoding approximately. AMBR is derived
from the observation that the problem of computing the sample-based MBR
objective is the medoid identification problem. AMBR uses the Correlated
Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best
approximation algorithm to date for the medoid identification problem, to
compute the sample-based MBR objective. We evaluate AMBR on machine
translation, text summarization, and image captioning tasks. The results show
that AMBR achieves on par with CBP, with CBP selecting hyperparameters through
an Oracle for each given computation budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex systems approach to natural language. (arXiv:2401.02772v1 [physics.soc-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02772">
<div class="article-summary-box-inner">
<span><p>The review summarizes the main methodological concepts used in studying
natural language from the perspective of complexity science and documents their
applicability in identifying both universal and system-specific features of
language in its written representation. Three main complexity-related research
trends in quantitative linguistics are covered. The first part addresses the
issue of word frequencies in texts and demonstrates that taking punctuation
into consideration restores scaling whose violation in the Zipf's law is often
observed for the most frequent words. The second part introduces methods
inspired by time series analysis, used in studying various kinds of
correlations in written texts. The related time series are generated on the
basis of text partition into sentences or into phrases between consecutive
punctuation marks. It turns out that these series develop features often found
in signals generated by complex systems, like long-range correlations or
(multi)fractal structures. Moreover, it appears that the distances between
punctuation marks comply with the discrete variant of the Weibull distribution.
In the third part, the application of the network formalism to natural language
is reviewed, particularly in the context of the so-called word-adjacency
networks. Parameters characterizing topology of such networks can be used for
classification of texts, for example, from a stylometric perspective. Network
approach can also be applied to represent the organization of word
associations. Structure of word-association networks turns out to be
significantly different from that observed in random networks, revealing
genuine properties of language. Finally, punctuation seems to have a
significant impact not only on the language's information-carrying ability but
also on its key statistical properties, hence it is recommended to consider
punctuation marks on a par with words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02777">
<div class="article-summary-box-inner">
<span><p>This paper introduces RAISE (Reasoning and Acting through Scratchpad and
Examples), an advanced architecture enhancing the integration of Large Language
Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of
the ReAct framework, incorporates a dual-component memory system, mirroring
human short-term and long-term memory, to maintain context and continuity in
conversations. It entails a comprehensive agent construction scenario,
including phases like Conversation Selection, Scene Extraction, CoT Completion,
and Scene Augmentation, leading to the LLMs Training phase. This approach
appears to enhance agent controllability and adaptability in complex,
multi-turn dialogues. Our preliminary evaluations in a real estate sales
context suggest that RAISE has some advantages over traditional agents,
indicating its potential for broader applications. This work contributes to the
AI field by providing a robust framework for developing more context-aware and
versatile conversational agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models in Plant Biology. (arXiv:2401.02789v1 [q-bio.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02789">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), such as ChatGPT, have taken the world by storm
and have passed certain forms of the Turing test. However, LLMs are not limited
to human language and analyze sequential data, such as DNA, protein, and gene
expression. The resulting foundation models can be repurposed to identify the
complex patterns within the data, resulting in powerful, multi-purpose
prediction tools able to explain cellular systems. This review outlines the
different types of LLMs and showcases their recent uses in biology. Since LLMs
have not yet been embraced by the plant community, we also cover how these
models can be deployed for the plant kingdom.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02797">
<div class="article-summary-box-inner">
<span><p>Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs to
predict free-form answers as a generative task to solve medical visual question
answering (Med-VQA) tasks. In this paper, we propose a parameter efficient
framework for fine-tuning MLLM specifically tailored to Med-VQA applications,
and empirically validate it on a public benchmark dataset. To accurately
measure the performance, we employ human evaluation and the results reveal that
our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v
model by a significant margin of 26% absolute accuracy on closed-ended
questions. The code will be available here: https://github.com/jinlHe/PeFoMed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocGraphLM: Documental Graph Language Model for Information Extraction. (arXiv:2401.02823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02823">
<div class="article-summary-box-inner">
<span><p>Advances in Visually Rich Document Understanding (VrDU) have enabled
information extraction and question answering over documents with complex
layouts. Two tropes of architectures have emerged -- transformer-based models
inspired by LLMs, and Graph Neural Networks. In this paper, we introduce
DocGraphLM, a novel framework that combines pre-trained language models with
graph semantics. To achieve this, we propose 1) a joint encoder architecture to
represent documents, and 2) a novel link prediction approach to reconstruct
document graphs. DocGraphLM predicts both directions and distances between
nodes using a convergent joint loss function that prioritizes neighborhood
restoration and downweighs distant node detection. Our experiments on three
SotA datasets show consistent improvement on IE and QA tasks with the adoption
of graph features. Moreover, we report that adopting the graph features
accelerates convergence in the learning process during training, despite being
solely constructed through link prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pheme: Efficient and Conversational Speech Generation. (arXiv:2401.02839v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02839">
<div class="article-summary-box-inner">
<span><p>In recent years, speech generation has seen remarkable progress, now
achieving one-shot generation capability that is often virtually
indistinguishable from real human voice. Integrating such advancements in
speech generation with large language models might revolutionize a wide range
of applications. However, certain applications, such as assistive
conversational systems, require natural and conversational speech generation
tools that also operate efficiently in real time. Current state-of-the-art
models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,
require large neural components and extensive training data to work well. In
contrast, MQTTS aims to build more compact conversational TTS models while
capitalizing on smaller-scale real-life conversational speech data. However,
its autoregressive nature yields high inference latency and thus limits its
real-time usage. In order to mitigate the current limitations of the
state-of-the-art TTS models while capitalizing on their strengths, in this work
we introduce the Pheme model series that 1) offers compact yet high-performing
models, 2) allows for parallel speech generation of 3) natural conversational
speech, and 4) it can be trained efficiently on smaller-scale conversational
data, cutting data demands by more than 10x but still matching the quality of
the autoregressive TTS models. We also show that through simple teacher-student
distillation we can meet significant improvements in voice quality for
single-speaker setups on top of pretrained Pheme checkpoints, relying solely on
synthetic speech generated by much larger teacher models. Audio samples and
pretrained models are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models. (arXiv:2401.02870v1 [cs.MA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02870">
<div class="article-summary-box-inner">
<span><p>The evolution of Large Language Models (LLMs) has introduced a new paradigm
for investigating human behavior emulation. Recent research has employed
LLM-based Agents to create a sociological research environment, in which agents
exhibit behavior based on the unfiltered characteristics of large language
models. However, these studies overlook the iterative development within a
human-like setting - Human preferences and personalities are complex, shaped by
various factors and subject to ongoing change as a result of environmental and
subjective influences. In light of this observation, we propose Agent Framework
for Shaping Preference and Personality (AFSPP), exploring the multifaceted
impact of social networks and subjective consciousness on LLM-based Agents'
preference and personality formation. With AFSPP, we have, for the first time,
successfully replicated several key findings from human personality
experiments. And other AFSPP-based experimental results indicate that plan
making, sensory perceptions and social networking with subjective information,
wield the most pronounced influence on preference shaping. AFSPP can
significantly enhance the efficiency and scope of psychological experiments,
while yielding valuable insights for Trustworthy Artificial Intelligence
research for strategies to prevent undesirable preference and personality
development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02906">
<div class="article-summary-box-inner">
<span><p>The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a "foreign language" that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector's role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model's overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task. (arXiv:2401.02909v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02909">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are increasingly bringing advances to Natural
Language Processing. However, low-resource languages, those lacking extensive
prominence in datasets for various NLP tasks, or where existing datasets are
not as substantial, such as Portuguese, already obtain several benefits from
LLMs, but not to the same extent. LLMs trained on multilingual datasets
normally struggle to respond to prompts in Portuguese satisfactorily,
presenting, for example, code switching in their responses. This work proposes
a fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two
versions: 7B and 13B. We evaluate the performance of this model in
classification tasks using the zero-shot approach with in-context learning, and
compare it with other LLMs. Our main contribution is to bring an LLM with
satisfactory results in the Portuguese language, as well as to provide a model
that is free for research or commercial purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks. (arXiv:2401.02921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02921">
<div class="article-summary-box-inner">
<span><p>In the realm of spoken language understanding (SLU), numerous natural
language understanding (NLU) methodologies have been adapted by supplying large
language models (LLMs) with transcribed speech instead of conventional written
text. In real-world scenarios, prior to input into an LLM, an automated speech
recognition (ASR) system generates an output transcript hypothesis, where
inherent errors can degrade subsequent SLU tasks. Here we introduce a method
that utilizes the ASR system's lattice output instead of relying solely on the
top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU
outcomes. Our in-context learning experiments, covering spoken question
answering and intent classification, underline the LLM's resilience to noisy
speech transcripts with the help of word confusion networks from lattices,
bridging the SLU performance gap between using the top ASR hypothesis and an
oracle upper bound. Additionally, we delve into the LLM's robustness to varying
ASR performance conditions and scrutinize the aspects of in-context learning
which prove the most influential.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02938">
<div class="article-summary-box-inner">
<span><p>Pruning large language models (LLMs) is a challenging task due to their
enormous size. The primary difficulty is fine-tuning the model after pruning,
which is needed to recover the lost performance caused by dropping weights.
Recent approaches have either ignored fine-tuning entirely, focusing on
efficient pruning criteria, or attempted layer-wise weight updates, preserving
the behavior of each layer. However, even layer-wise weight updates can be
costly for LLMs, and previous works have resorted to various approximations.
</p>
<p>In our paper, we propose a fast and optimal weight update algorithm for
pruned layers based on the Alternating Direction Method of Multipliers (ADMM).
Coupled with a simple iterative pruning mask selection, our algorithm achieves
state-of-the-art pruning performance across a wide range of LLMs. Code is
available at https://github.com/fmfi-compbio/admm-pruning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02954">
<div class="article-summary-box-inner">
<span><p>The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Wrap-Up Effects through an Information-Theoretic Lens. (arXiv:2203.17213v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17213">
<div class="article-summary-box-inner">
<span><p>Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called "wrap-up effects," which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mFACE: Multilingual Summarization with Factual Consistency Evaluation. (arXiv:2212.10622v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10622">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization has enjoyed renewed interest in recent years,
thanks to pre-trained language models and the availability of large-scale
datasets. Despite promising results, current models still suffer from
generating factually inconsistent summaries, reducing their utility for
real-world application. Several recent efforts attempt to address this by
devising models that automatically detect factual inconsistencies in machine
generated summaries. However, they focus exclusively on English, a language
with abundant resources. In this work, we leverage factual consistency
evaluation models to improve multilingual summarization. We explore two
intuitive approaches to mitigate hallucinations based on the signal provided by
a multilingual NLI model, namely data filtering and controlled generation.
Experimental results in the 45 languages from the XLSum dataset show gains over
strong baselines in both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v5 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07372">
<div class="article-summary-box-inner">
<span><p>Relational databases play an important role in business, science, and more.
However, many users cannot fully unleash the analytical power of relational
databases, because they are not familiar with database languages such as SQL.
Many techniques have been proposed to automatically generate SQL from natural
language, but they suffer from two issues: (1) they still make many mistakes,
particularly for complex queries, and (2) they do not provide a flexible way
for non-expert users to validate and refine incorrect queries. To address these
issues, we introduce a new interaction mechanism that allows users to directly
edit a step-by-step explanation of a query to fix errors. Our experiments on
multiple datasets, as well as a user study with 24 participants, demonstrate
that our approach can achieve better performance than multiple SOTA approaches.
Our code and datasets are available at https://github.com/magic-YuanTian/STEPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11698">
<div class="article-summary-box-inner">
<span><p>Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/; our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of
this work is at https://openreview.net/pdf?id=kaHpo8OZw2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Efficacy of Sampling Adapters. (arXiv:2307.03749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03749">
<div class="article-summary-box-inner">
<span><p>Sampling is a common strategy for generating text from probabilistic models,
yet standard ancestral sampling often results in text that is incoherent or
ungrammatical. To alleviate this issue, various modifications to a model's
sampling distribution, such as nucleus or top-k sampling, have been introduced
and are now ubiquitously used in language generation systems. We propose a
unified framework for understanding these techniques, which we term sampling
adapters. Sampling adapters often lead to qualitatively better text, which
raises the question: From a formal perspective, how are they changing the
(sub)word-level distributions of language generation models? And why do these
local changes lead to higher-quality text? We argue that the shift they enforce
can be viewed as a trade-off between precision and recall: while the model
loses its ability to produce certain strings, its precision rate on desirable
text increases. While this trade-off is not reflected in standard metrics of
distribution quality (such as perplexity), we find that several
precision-emphasizing measures indeed indicate that sampling adapters can lead
to probability distributions more aligned with the true distribution. Further,
these measures correlate with higher sequence-level quality scores,
specifically, Mauve.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.04695">
<div class="article-summary-box-inner">
<span><p>Current methods for Knowledge-Based Question Answering (KBQA) usually rely on
complex training techniques and model frameworks, leading to many limitations
in practical applications. Recently, the emergence of In-Context Learning (ICL)
capabilities in Large Language Models (LLMs) provides a simple and
training-free semantic parsing paradigm for KBQA: Given a small number of
questions and their labeled logical forms as demo examples, LLMs can understand
the task intent and generate the logic form for a new question. However,
current powerful LLMs have little exposure to logic forms during pre-training,
resulting in a high format error rate. To solve this problem, we propose a
code-style in-context learning method for KBQA, which converts the generation
process of unfamiliar logical form into the more familiar code generation
process for LLMs. Experimental results on three mainstream datasets show that
our method dramatically mitigated the formatting error problem in generating
logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the
few-shot setting. The code and supplementary files are released at
https://github.com/Arthurizijar/KB-Coder .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.01282">
<div class="article-summary-box-inner">
<span><p>As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and &gt;50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
</p>
<p>We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.14212">
<div class="article-summary-box-inner">
<span><p>When training data are collected from human annotators, the design of the
annotation instrument, the instructions given to annotators, the
characteristics of the annotators, and their interactions can impact training
data. This study demonstrates that design choices made when creating an
annotation instrument also impact the models trained on the resulting
annotations. We introduce the term annotation sensitivity to refer to the
impact of annotation data collection methods on the annotations themselves and
on downstream model performance and predictions. We collect annotations of hate
speech and offensive language in five experimental conditions of an annotation
instrument, randomly assigning annotators to conditions. We then fine-tune BERT
models on each of the five resulting datasets and evaluate model performance on
a holdout portion of each condition. We find considerable differences between
the conditions for 1) the share of hate speech/offensive language annotations,
2) model performance, 3) model predictions, and 4) model learning curves. Our
results emphasize the crucial role played by the annotation instrument which
has received little attention in the machine learning literature. We call for
additional research into how and why the instrument impacts the annotations to
inform the development of best practices in instrument design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.04889">
<div class="article-summary-box-inner">
<span><p>Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user's query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system's performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.07910">
<div class="article-summary-box-inner">
<span><p>The evaluation of large language models (LLMs) is crucial to assess their
performance and mitigate potential security risks. In this paper, we introduce
PromptBench, a unified library to evaluate LLMs. It consists of several key
components that are easily used and extended by researchers: prompt
construction, prompt engineering, dataset and model loading, adversarial prompt
attack, dynamic evaluation protocols, and analysis tools. PromptBench is
designed to be an open, general, and flexible codebase for research purposes
that can facilitate original study in creating new benchmarks, deploying
downstream applications, and designing new evaluation protocols. The code is
available at: https://github.com/microsoft/promptbench and will be continuously
supported.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.10997">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.11514">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
substantial computational and memory requirements present challenges,
especially for devices with limited DRAM capacity. This paper tackles the
challenge of efficiently running LLMs that exceed the available DRAM capacity
by storing the model parameters in flash memory, but bringing them on demand to
DRAM. Our method involves constructing an inference cost model that takes into
account the characteristics of flash memory, guiding us to optimize in two
critical areas: reducing the volume of data transferred from flash and reading
data in larger, more contiguous chunks. Within this hardware-informed
framework, we introduce two principal techniques. First, "windowing"
strategically reduces data transfer by reusing previously activated neurons,
and second, "row-column bundling", tailored to the sequential data access
strengths of flash memory, increases the size of data chunks read from flash
memory. These methods collectively enable running models up to twice the size
of the available DRAM, with a 4-5x and 20-25x increase in inference speed
compared to naive loading approaches in CPU and GPU, respectively. Our
integration of sparsity awareness, context-adaptive loading, and a
hardware-oriented design paves the way for effective inference of LLMs on
devices with limited memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can AI Be as Creative as Humans?. (arXiv:2401.01623v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01623">
<div class="article-summary-box-inner">
<span><p>Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI's creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI's creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI's creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patterns of Persistence and Diffusibility across the World's Languages. (arXiv:2401.01698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01698">
<div class="article-summary-box-inner">
<span><p>Language similarities can be caused by genetic relatedness, areal contact,
universality, or chance. Colexification, i.e. a type of similarity where a
single lexical form is used to convey multiple meanings, is underexplored. In
our work, we shed light on the linguistic causes of cross-lingual similarity in
colexification and phonology, by exploring genealogical stability (persistence)
and contact-induced change (diffusibility). We construct large-scale graphs
incorporating semantic, genealogical, phonological and geographical data for
1,966 languages. We then show the potential of this resource, by investigating
several established hypotheses from previous work in linguistics, while
proposing new ones. Our results strongly support a previously established
hypothesis in the linguistic literature, while offering contradicting evidence
to another. Our large scale resource opens for further research across
disciplines, e.g.~in multilingual NLP and comparative linguistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.01916">
<div class="article-summary-box-inner">
<span><p>We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpora -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 excel in broader question-answering scenarios due to superior
reasoning capabilities, our findings suggest that continual pre-training with
limited resources can still enhance model performance on specialized topics.
Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B
LLaMA model on a domain-specific conversational dataset, culminating in the
release of the chat-enabled AstroLLaMA for community use. Comprehensive
quantitative benchmarking is currently in progress and will be detailed in an
upcoming full paper. The model, AstroLLaMA-Chat, is now available at
https://huggingface.co/universeTBD, providing the first open-source
conversational AI tool tailored for the astronomy community.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2024-01-08 23:12:14.781718891 UTC">2024-01-08 23:12:14 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>