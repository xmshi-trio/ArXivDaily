<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-22T01:30:00Z">06-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11816">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for conditional text generation. In
particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent
conversations with users by incorporating RL and feedback from humans. Inspired
by learning-to-search algorithms and capitalizing on key properties of text
generation, we seek to investigate reinforcement learning algorithms beyond
general purpose algorithms such as Proximal policy optimization (PPO). In
particular, we extend RL algorithms to allow them to interact with a dynamic
black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a
suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive
review and CommonGen text generation task from the GRUE benchmark. We show that
our RL algorithms achieve higher performance than supervised learning (SL) and
default PPO baselines, demonstrating the benefit of interaction with the guide
LLM. On CommonGen, we not only outperform our SL baselines but also improve
beyond PPO across a variety of lexical and semantic metrics beyond the one we
optimized for. Notably, on the IMDB dataset, we show that our GPT-2 based
policy outperforms the zero-shot GPT-3 oracle, indicating that our algorithms
can learn from a powerful, black-box GPT-3 oracle with a simpler, cheaper, and
publicly available GPT-2 model while gaining performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only. (arXiv:2306.11823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11823">
<div class="article-summary-box-inner">
<span><p>This paper presents EvolveMT for efficiently combining multiple machine
translation (MT) engines. The proposed system selects the output from a single
engine for each segment by utilizing online learning techniques to predict the
most suitable system for every translation request. A neural quality estimation
metric supervises the method without requiring reference translations. The
online learning capability of this system allows for dynamic adaptation to
alterations in the domain or machine translation engines, thereby obviating the
necessity for additional training. EvolveMT selects a subset of translation
engines to be called based on the source sentence features. The degree of
exploration is configurable according to the desired quality-cost trade-off.
Results from custom datasets demonstrate that EvolveMT achieves similar
translation accuracy at a lower cost than selecting the best translation of
each segment from all translations using an MT quality estimator. To our
knowledge, EvolveMT is the first meta MT system that adapts itself after
deployment to incoming translation requests from the production environment
without needing costly retraining on human feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Compositionality and Improved Training of NADO. (arXiv:2306.11825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11825">
<div class="article-summary-box-inner">
<span><p>NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable
generation with large language models. Differentiating from finetuning/prompt
tuning, it has the potential to avoid catastrophic forgetting of the large base
model and achieve guaranteed convergence to an entropy-maximized closed-form
solution without significantly limiting the model capacity. Despite its
success, several challenges arise when applying NADO to more complex scenarios.
First, the best practice of using NADO for the composition of multiple control
signals is under-explored. Second, vanilla NADO suffers from gradient vanishing
for low-probability control signals and is highly reliant on the
forward-consistency regularization. In this paper, we study the aforementioned
challenges when using NADO theoretically and empirically. We show we can
achieve guaranteed compositional generalization of NADO with a certain
practice, and propose a novel alternative parameterization of NADO to perfectly
guarantee the forward-consistency. We evaluate the improved training of NADO,
i.e. NADO++, on CommonGen. Results show that NADO++ improves the effectiveness
of the algorithm in multiple aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuOTeS: Query-Oriented Technical Summarization. (arXiv:2306.11832v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11832">
<div class="article-summary-box-inner">
<span><p>Abstract. When writing an academic paper, researchers often spend
considerable time reviewing and summarizing papers to extract relevant
citations and data to compose the Introduction and Related Work sections. To
address this problem, we propose QuOTeS, an interactive system designed to
retrieve sentences related to a summary of the research from a collection of
potential references and hence assist in the composition of new papers. QuOTeS
integrates techniques from Query-Focused Extractive Summarization and
High-Recall Information Retrieval to provide Interactive Query-Focused
Summarization of scientific documents. To measure the performance of our
system, we carried out a comprehensive user study where participants uploaded
papers related to their research and evaluated the system in terms of its
usability and the quality of the summaries it produces. The results show that
QuOTeS provides a positive user experience and consistently provides
query-focused summaries that are relevant, concise, and complete. We share the
code of our system and the novel Query-Focused Summarization dataset collected
during our experiments at https://github.com/jarobyte91/quotes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Machine Translation Corpus Generation. (arXiv:2306.11838v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11838">
<div class="article-summary-box-inner">
<span><p>This paper proposes an efficient and semi-automated method for
human-in-the-loop post-editing for machine translation (MT) corpus generation.
The method is based on online training of a custom MT quality estimation metric
on-the-fly as linguists perform post-edits. The online estimator is used to
prioritize worse hypotheses for post-editing, and auto-close best hypotheses
without post-editing. This way, significant improvements can be achieved in the
resulting quality of post-edits at a lower cost due to reduced human
involvement. The trained estimator can also provide an online sanity check
mechanism for post-edits and remove the need for additional linguists to review
them or work on the same hypotheses. In this paper, the effect of prioritizing
with the proposed method on the resulting MT corpus quality is presented versus
scheduling hypotheses randomly. As demonstrated by experiments, the proposed
method improves the lifecycle of MT models by focusing the linguist effort on
production samples and hypotheses, which matter most for expanding MT corpora
to be used for re-training them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Based Transformer for Table Augmentation. (arXiv:2306.11843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11843">
<div class="article-summary-box-inner">
<span><p>Data preparation, also called data wrangling, is considered one of the most
expensive and time-consuming steps when performing analytics or building
machine learning models. Preparing data typically involves collecting and
merging data from complex heterogeneous, and often large-scale data sources,
such as data lakes. In this paper, we introduce a novel approach toward
automatic data wrangling in an attempt to alleviate the effort of end-users,
e.g. data analysts, in structuring dynamic views from data lakes in the form of
tabular data. We aim to address table augmentation tasks, including row/column
population and data imputation. Given a corpus of tables, we propose a
retrieval augmented self-trained transformer model. Our self-learning strategy
consists in randomly ablating tables from the corpus and training the
retrieval-based model to reconstruct the original values or headers given the
partial tables as input. We adopt this strategy to first train the dense neural
retrieval model encoding table-parts to vectors, and then the end-to-end model
trained to perform table augmentation tasks. We test on EntiTables, the
standard benchmark for table augmentation, as well as introduce a new benchmark
to advance further research: WebTables. Our model consistently and
substantially outperforms both supervised statistical methods and the current
state-of-the-art transformer-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Domain Text Evaluation via Meta Distribution Modeling. (arXiv:2306.11879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11879">
<div class="article-summary-box-inner">
<span><p>Recent advances in open-domain text generation models powered by large
pre-trained language models (LLMs) have achieved remarkable performance.
However, evaluating and controlling these models for desired attributes remains
a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and
METEOR are insufficient for open-ended generation tasks. Similarly, while
trainable discriminator-based evaluation metrics show promise, obtaining
high-quality training data is a non-trivial task. In this paper, we introduce a
novel approach to evaluate open-domain generation - the Meta-Distribution
Methods (MDM). Drawing on the correlation between the rising parameter counts
and the improving performance of LLMs, MDM creates a mapping from the contrast
of two probabilistic distributions -- one known to be superior to the other --
to quality measures, which can be viewed as a distribution of distributions
i.e. Meta-Distribution. We investigate MDM for open-domain text generation
evaluation under two paradigms: 1) \emph{Generative} MDM, which leverages the
Meta-Distribution Methods to generate in-domain negative samples for training
discriminator-based metrics; 2) \emph{Discriminative} MDM, which directly uses
distribution discrepancies between two language models for evaluation. Our
experiments on multi-turn dialogue and factuality in abstractive summarization
demonstrate that MDMs correlate better with human judgment than existing
automatic evaluation metrics on both tasks, highlighting the strong performance
and generalizability of such methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications. (arXiv:2306.11892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11892">
<div class="article-summary-box-inner">
<span><p>This paper explores new frontiers in agricultural natural language processing
by investigating the effectiveness of using food-related text corpora for
pretraining transformer-based language models. In particular, we focus on the
task of semantic matching, which involves establishing mappings between food
descriptions and nutrition data. To accomplish this, we fine-tune a pre-trained
transformer-based language model, AgriBERT, on this task, utilizing an external
source of knowledge, such as the FoodOn ontology. To advance the field of
agricultural NLP, we propose two new avenues of exploration: (1) utilizing
GPT-based models as a baseline and (2) leveraging ChatGPT as an external source
of knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and
we believe it has the potential to improve our model in the task of semantic
matching and enhance our model's understanding of food-related concepts and
relationships. Additionally, we experiment with other applications, such as
cuisine prediction based on food ingredients, and expand the scope of our
research to include other NLP tasks beyond semantic matching. Overall, this
paper provides promising avenues for future research in this field, with
potential implications for improving the performance of agricultural NLP
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Chinese-English Machine Translation of Emotion-Loaded Microblog Texts: A Human Annotated Dataset for the Quality Assessment of Emotion Translation. (arXiv:2306.11900v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11900">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on how current Machine Translation (MT) tools perform
on the translation of emotion-loaded texts by evaluating outputs from Google
Translate according to a framework proposed in this paper. We propose this
evaluation framework based on the Multidimensional Quality Metrics (MQM) and
perform a detailed error analysis of the MT outputs. From our analysis, we
observe that about 50% of the MT outputs fail to preserve the original emotion.
After further analysis of the errors, we find that emotion carrying words and
linguistic phenomena such as polysemous words, negation, abbreviation etc., are
common causes for these translation errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opportunities and Risks of LLMs for Scalable Deliberation with Polis. (arXiv:2306.11932v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11932">
<div class="article-summary-box-inner">
<span><p>Polis is a platform that leverages machine intelligence to scale up
deliberative processes. In this paper, we explore the opportunities and risks
associated with applying Large Language Models (LLMs) towards challenges with
facilitating, moderating and summarizing the results of Polis engagements. In
particular, we demonstrate with pilot experiments using Anthropic's Claude that
LLMs can indeed augment human intelligence to help more efficiently run Polis
conversations. In particular, we find that summarization capabilities enable
categorically new methods with immense promise to empower the public in
collective meaning-making exercises. And notably, LLM context limitations have
a significant impact on insight and quality of these results.
</p>
<p>However, these opportunities come with risks. We discuss some of these risks,
as well as principles and techniques for characterizing and mitigating them,
and the implications for other deliberative or political systems that may
employ LLMs. Finally, we conclude with several open future research directions
for augmenting tools like Polis with LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding What Code Language Models Learned. (arXiv:2306.11943v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11943">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models are effective in a variety of natural language
tasks, but it has been argued their capabilities fall short of fully learning
meaning or understanding language. To understand the extent to which language
models can learn some form of meaning, we investigate their ability to capture
semantics of code beyond superficial frequency and co-occurrence. In contrast
to previous research on probing models for linguistic features, we study
pre-trained models in a setting that allows for objective and straightforward
evaluation of a model's ability to learn semantics. In this paper, we examine
whether such models capture the semantics of code, which is precisely and
formally defined. Through experiments involving the manipulation of code
fragments, we show that code pre-trained models of code learn a robust
representation of the computational semantics of code that goes beyond
superficial features of form alone
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Molecular Discovery with Natural Language. (arXiv:2306.11976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11976">
<div class="article-summary-box-inner">
<span><p>Natural language is expected to be a key medium for various human-machine
interactions in the era of large language models. When it comes to the
biochemistry field, a series of tasks around molecules (e.g., property
prediction, molecule mining, etc.) are of great significance while having a
high technical threshold. Bridging the molecule expressions in natural language
and chemical language can not only hugely improve the interpretability and
reduce the operation difficulty of these tasks, but also fuse the chemical
knowledge scattered in complementary materials for a deeper comprehension of
molecules. Based on these benefits, we propose the conversational molecular
design, a novel task adopting natural language for describing and editing
target molecules. To better accomplish this task, we design ChatMol, a
knowledgeable and versatile generative pre-trained model, enhanced by injecting
experimental property information, molecular spatial knowledge, and the
associations between natural and chemical languages into it. Several typical
solutions including large language models (e.g., ChatGPT) are evaluated,
proving the challenge of conversational molecular design and the effectiveness
of our knowledge enhancement method. Case observations and analysis are
conducted to provide directions for further exploration of natural-language
interaction in molecular discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3HAN: A Deep Neural Network for Fake News Detection. (arXiv:2306.12014v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12014">
<div class="article-summary-box-inner">
<span><p>The rapid spread of fake news is a serious problem calling for AI solutions.
We employ a deep learning based automated detector through a three level
hierarchical attention network (3HAN) for fast, accurate detection of fake
news. 3HAN has three levels, one each for words, sentences, and the headline,
and constructs a news vector: an effective representation of an input news
article, by processing an article in an hierarchical bottom-up manner. The
headline is known to be a distinguishing feature of fake news, and furthermore,
relatively few words and sentences in an article are more important than the
rest. 3HAN gives a differential importance to parts of an article, on account
of its three layers of attention. By experiments on a large real-world data
set, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike
some other deep learning models, 3HAN provides an understandable output through
the attention weights given to different parts of an article, which can be
visualized through a heatmap to enable further manual fact checking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing. (arXiv:2306.12018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12018">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the impressive progress in Neural Dependency
Parsing. According to the different factorization approaches to the graph joint
probabilities, existing parsers can be roughly divided into autoregressive and
non-autoregressive patterns. The former means that the graph should be
factorized into multiple sequentially dependent components, then it can be
built up component by component. And the latter assumes these components to be
independent so that they can be outputted in a one-shot manner. However, when
treating the directed edge as an explicit dependency relationship, we discover
that there is a mixture of independent and interdependent components in the
dependency graph, signifying that both aforementioned models fail to precisely
capture the explicit dependencies among nodes and edges. Based on this
property, we design a Semi-Autoregressive Dependency Parser to generate
dependency graphs via adding node groups and edge groups autoregressively while
pouring out all group elements in parallel. The model gains a trade-off between
non-autoregression and autoregression, which respectively suffer from the lack
of target inter-dependencies and the uncertainty of graph generation orders.
The experiments show the proposed parser outperforms strong baselines on
Enhanced Universal Dependencies of multiple languages, especially achieving
$4\%$ average promotion at graph-level accuracy. Also, the performances of
model variations show the importance of specific parts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-Aware Text-to-Speech. (arXiv:2306.12020v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12020">
<div class="article-summary-box-inner">
<span><p>Dynamically synthesizing talking speech that actively responds to a listening
head is critical during the face-to-face interaction. For example, the speaker
could take advantage of the listener's facial expression to adjust the tones,
stressed syllables, or pauses. In this work, we present a new visual-aware
text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual
inputs and sequential visual feedback (e.g., nod, smile) of the listener in
face-to-face communication. Different from traditional text-to-speech, VA-TTS
highlights the impact of visual modality. On this newly-minted task, we devise
a baseline model to fuse phoneme linguistic information and listener visual
signals for speech synthesis. Extensive experiments on multimodal conversation
dataset ViCo-X verify our proposal for generating more natural audio with
scenario-appropriate rhythm and prosody.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strategies in Transfer Learning for Low-Resource Speech Synthesis: Phone Mapping, Features Input, and Source Language Selection. (arXiv:2306.12040v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12040">
<div class="article-summary-box-inner">
<span><p>We compare using a PHOIBLE-based phone mapping method and using phonological
features input in transfer learning for TTS in low-resource languages. We use
diverse source languages (English, Finnish, Hindi, Japanese, and Russian) and
target languages (Bulgarian, Georgian, Kazakh, Swahili, Urdu, and Uzbek) to
test the language-independence of the methods and enhance the findings'
applicability. We use Character Error Rates from automatic speech recognition
and predicted Mean Opinion Scores for evaluation. Results show that both phone
mapping and features input improve the output quality and the latter performs
better, but these effects also depend on the specific language combination. We
also compare the recently-proposed Angular Similarity of Phone Frequencies
(ASPF) with a family tree-based distance measure as a criterion to select
source languages in transfer learning. ASPF proves effective if label-based
phone input is used, while the language distance does not have expected
effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sample Attackability in Natural Language Adversarial Attacks. (arXiv:2306.12043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12043">
<div class="article-summary-box-inner">
<span><p>Adversarial attack research in natural language processing (NLP) has made
significant progress in designing powerful attack methods and defence
approaches. However, few efforts have sought to identify which source samples
are the most attackable or robust, i.e. can we determine for an unseen target
model, which samples are the most vulnerable to an adversarial attack. This
work formally extends the definition of sample attackability/robustness for NLP
attacks. Experiments on two popular NLP datasets, four state of the art models
and four different NLP adversarial attack methods, demonstrate that sample
uncertainty is insufficient for describing characteristics of attackable/robust
samples and hence a deep learning based detector can perform much better at
identifying the most attackable and robust samples for an unseen target model.
Nevertheless, further analysis finds that there is little agreement in which
samples are considered the most attackable/robust across different NLP attack
methods, explaining a lack of portability of attackability detection methods
across attack methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension. (arXiv:2306.12069v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12069">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension (MRC) poses new challenges over logical
reasoning, which aims to understand the implicit logical relations entailed in
the given contexts and perform inference over them. Due to the complexity of
logic, logical relations exist at different granularity levels. However, most
existing methods of logical reasoning individually focus on either entity-aware
or discourse-based information but ignore the hierarchical relations that may
even have mutual effects. In this paper, we propose a holistic graph network
(HGN) which deals with context at both discourse level and word level, as the
basis for logical reasoning, to provide a more fine-grained relation
extraction. Specifically, node-level and type-level relations, which can be
interpreted as bridges in the reasoning process, are modeled by a hierarchical
interaction mechanism to improve the interpretation of MRC systems.
Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and
natural language inference datasets (SNLI and ANLI) show the effectiveness and
generalization of our method, and in-depth analysis verifies its capability to
understand complex logical relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints. (arXiv:2306.12089v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12089">
<div class="article-summary-box-inner">
<span><p>Lexically-constrained NMT (LNMT) aims to incorporate user-provided
terminology into translations. Despite its practical advantages, existing work
has not evaluated LNMT models under challenging real-world conditions. In this
paper, we focus on two important but under-studied issues that lie in the
current evaluation process of LNMT studies. The model needs to cope with
challenging lexical constraints that are "homographs" or "unseen" during
training. To this end, we first design a homograph disambiguation module to
differentiate the meanings of homographs. Moreover, we propose PLUMCOT, which
integrates contextually rich information about unseen lexical constraints from
pre-trained language models and strengthens a copy mechanism of the pointer
network via direct supervision of a copying score. We also release HOLLY, an
evaluation benchmark for assessing the ability of a model to cope with
"homographic" and "unseen" lexical constraints. Experiments on HOLLY and the
previous test setup show the effectiveness of our method. The effects of
PLUMCOT are shown to be remarkable in "unseen" constraints. Our dataset is
available at https://github.com/papago-lab/HOLLY-benchmark
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12105">
<div class="article-summary-box-inner">
<span><p>Deployed multimodal systems can fail in ways that evaluators did not
anticipate. In order to find these failures before deployment, we introduce
MultiMon, a system that automatically identifies systematic failures --
generalizable, natural-language descriptions of patterns of model failures. To
uncover systematic failures, MultiMon scrapes a corpus for examples of
erroneous agreement: inputs that produce the same output, but should not. It
then prompts a language model (e.g., GPT-4) to find systematic patterns of
failure and describe them in natural language. We use MultiMon to find 14
systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder,
each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many
books"). Because CLIP is the backbone for most state-of-the-art multimodal
systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion,
and others. MultiMon can also steer towards failures relevant to specific use
cases, such as self-driving cars. We see MultiMon as a step towards evaluation
that autonomously explores the long tail of potential system failures. Code for
MULTIMON is available at https://github.com/tsb0601/MultiMon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals. (arXiv:2306.12146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12146">
<div class="article-summary-box-inner">
<span><p>We present a human-in-the-loop dashboard tailored to diagnosing potential
spurious features that NLI models rely on for predictions. The dashboard
enables users to generate diverse and challenging examples by drawing
inspiration from GPT-3 suggestions. Additionally, users can receive feedback
from a trained NLI model on how challenging the newly created example is and
make refinements based on the feedback. Through our investigation, we discover
several categories of spurious correlations that impact the reasoning of NLI
models, which we group into three categories: Semantic Relevance, Logical
Fallacies, and Bias. Based on our findings, we identify and describe various
research opportunities, including diversifying training data and assessing NLI
models' robustness by creating adversarial test suites.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture Encoder for Joint Speech Separation and Recognition. (arXiv:2306.12173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12173">
<div class="article-summary-box-inner">
<span><p>Multi-speaker automatic speech recognition (ASR) is crucial for many
real-world applications, but it requires dedicated modeling techniques.
Existing approaches can be divided into modular and end-to-end methods. Modular
approaches separate speakers and recognize each of them with a single-speaker
ASR system. End-to-end models process overlapped speech directly in a single,
powerful neural network. This work proposes a middle-ground approach that
leverages explicit speech separation similarly to the modular approach but also
incorporates mixture speech information directly into the ASR module in order
to mitigate the propagation of errors made by the speech separator. We also
explore a way to exchange cross-speaker context information through a layer
that combines information of the individual speakers. Our system is optimized
through separate and joint training stages and achieves a relative improvement
of 7% in word error rate over a purely modular setup on the SMS-WSJ task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Interactions Reveal Linguistic Structure in Language Models. (arXiv:2306.12181v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12181">
<div class="article-summary-box-inner">
<span><p>We study feature interactions in the context of feature attribution methods
for post-hoc interpretability. In interpretability research, getting to grips
with feature interactions is increasingly recognised as an important challenge,
because interacting features are key to the success of neural networks. Feature
interactions allow a model to build up hierarchical representations for its
input, and might provide an ideal starting point for the investigation into
linguistic structure in language models. However, uncovering the exact role
that these interactions play is also difficult, and a diverse range of
interaction attribution methods has been proposed. In this paper, we focus on
the question which of these methods most faithfully reflects the inner workings
of the target models. We work out a grey box methodology, in which we train
models to perfection on a formal language classification task, using PCFGs. We
show that under specific configurations, some methods are indeed able to
uncover the grammatical rules acquired by a model. Based on these findings we
extend our evaluation to a case study on language models, providing novel
insights into the linguistic structure that these models have acquired.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12198">
<div class="article-summary-box-inner">
<span><p>Investigating deep learning language models has always been a significant
research area due to the ``black box" nature of most advanced models. With the
recent advancements in pre-trained language models based on transformers and
their increasing integration into daily life, addressing this issue has become
more pressing. In order to achieve an explainable AI model, it is essential to
comprehend the procedural steps involved and compare them with human thought
processes. Thus, in this paper, we use simple, well-understood non-language
tasks to explore these models' inner workings. Specifically, we apply a
pre-trained language model to constrained arithmetic problems with hierarchical
structure, to analyze their attention weight scores and hidden states. The
investigation reveals promising results, with the model addressing hierarchical
problems in a moderately structured manner, similar to human problem-solving
strategies. Additionally, by inspecting the attention weights layer by layer,
we uncover an unconventional finding that layer 10, rather than the model's
final layer, is the optimal layer to unfreeze for the least parameter-intensive
approach to fine-tune the model. We support these findings with entropy
analysis and token embeddings similarity analysis. The attention analysis
allows us to hypothesize that the model can generalize to longer sequences in
ListOps dataset, a conclusion later confirmed through testing on sequences
longer than those in the training set. Lastly, by utilizing a straightforward
task in which the model predicts the winner of a Tic Tac Toe game, we identify
limitations in attention analysis, particularly its inability to capture 2D
patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI. (arXiv:2306.12205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12205">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have recently emerged as a powerful tool for
fine-tuning a variety of language tasks. Ideally, when models are pre-trained
on large amount of data, they are expected to gain implicit knowledge. In this
paper, we investigate the ability of pre-trained language models to generalize
to different non-language tasks. In particular, we test them on tasks from
different domains such as computer vision, reasoning on hierarchical data, and
protein fold prediction. The four pre-trained models that we used, T5, BART,
BERT, and GPT-2 achieve outstanding results. They all have similar performance
and they outperform transformers that are trained from scratch by a large
margin. For instance, pre-trained language models perform better on the Listops
dataset, with an average accuracy of 58.7\%, compared to transformers trained
from scratch, which have an average accuracy of 29.0\%. The significant
improvement demonstrated across three types of datasets suggests that
pre-training on language helps the models to acquire general knowledge,
bringing us a step closer to general AI. We also showed that reducing the
number of parameters in pre-trained language models does not have a great
impact as the performance drops slightly when using T5-Small instead of
T5-Base. In fact, when using only 2\% of the parameters, we achieved a great
improvement compared to training from scratch. Finally, in contrast to prior
work, we find out that using pre-trained embeddings for the input layer is
necessary to achieve the desired results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Limits for Learning with Language Models. (arXiv:2306.12213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12213">
<div class="article-summary-box-inner">
<span><p>With the advent of large language models (LLMs), the trend in NLP has been to
train LLMs on vast amounts of data to solve diverse language understanding and
generation tasks. The list of LLM successes is long and varied. Nevertheless,
several recent papers provide empirical evidence that LLMs fail to capture
important aspects of linguistic meaning. Focusing on universal quantification,
we provide a theoretical foundation for these empirical findings by proving
that LLMs cannot learn certain fundamental semantic properties including
semantic entailment and consistency as they are defined in formal semantics.
More generally, we show that LLMs are unable to learn concepts beyond the first
level of the Borel Hierarchy, which imposes severe limits on the ability of
LMs, both large and small, to capture many aspects of linguistic meaning. This
means that LLMs will continue to operate without formal guarantees on tasks
that require entailments and deep linguistic understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12245">
<div class="article-summary-box-inner">
<span><p>Entity Linking (EL) is a fundamental task for Information Extraction and
Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first
find mentions in the given input document and then link the mentions to
corresponding entities in a specific knowledge base. Recently, the paradigm of
retriever-reader promotes the progress of end-to-end EL, benefiting from the
advantages of dense entity retrieval and machine reading comprehension.
However, the existing study only trains the retriever and the reader separately
in a pipeline manner, which ignores the benefit that the interaction between
the retriever and the reader can bring to the task. To advance the
retriever-reader paradigm to perform more perfectly on end-to-end EL, we
propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever
and Reader. Through our designed bidirectional end-to-end training, BEER$^2$
guides the retriever and the reader to learn from each other, make progress
together, and ultimately improve EL performance. Extensive experiments on
benchmarks of multiple domains demonstrate the effectiveness of our proposed
BEER$^2$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving and Generating NPR Sunday Puzzles with Large Language Models. (arXiv:2306.12255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12255">
<div class="article-summary-box-inner">
<span><p>We explore the ability of large language models to solve and generate puzzles
from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15
years of on-air puzzles. We evaluate four large language models using PUZZLEQA,
in both multiple choice and free response formats, and explore two prompt
engineering techniques to improve free response performance: chain-of-thought
reasoning and prompt summarization. We find that state-of-the-art large
language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,
achieves 50.2% loose accuracy. However, in our few-shot puzzle generation
experiment, we find no evidence that models can generate puzzles: GPT-3.5
generates puzzles with answers that do not conform to the generated rules.
Puzzle generation remains a challenging task for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings. (arXiv:2306.12280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12280">
<div class="article-summary-box-inner">
<span><p>The paradigm of pre-training followed by fine-tuning on downstream tasks has
become the mainstream method in natural language processing tasks. Although
pre-trained models have the advantage of generalization, their performance may
still vary significantly across different domain tasks. This is because the
data distribution in different domains varies. For example, the different parts
of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy
married life' may have different impact for downstream tasks. For similarity
calculations, words such as 'led' and 'life' are more important. On the other
hand, for sentiment analysis, the word 'happy' is crucial. This indicates that
different downstream tasks have different levels of sensitivity to sentence
components. Our starting point is to scale information of the model and data
according to the specifics of downstream tasks, enhancing domain information of
relevant parts for these tasks and reducing irrelevant elements for different
domain tasks, called SIFTER. In the experimental part, we use the SIFTER to
improve SimCSE by constructing positive sample pairs based on enhancing the
sentence stem and reducing the unimportant components in the sentence, and
maximize the similarity between three sentences. Similarly, SIFTER can improve
the gate mechanism of the LSTM model by short-circuiting the input gate of
important words so that the LSTM model remembers the important parts of the
sentence. Our experiments demonstrate that SIFTER outperforms the SimCSE and
LSTM baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical ministrations through web scraping. (arXiv:2306.12310v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12310">
<div class="article-summary-box-inner">
<span><p>Web scraping is a technique that allows us to extract data from websites
automatically. in the field of medicine, web scraping can be used to collect
information about medical procedures, treatments, and healthcare providers.
this information can be used to improve patient care, monitor the quality of
healthcare services, and identify areas for improvement. one area where web
scraping can be particularly useful is in medical ministrations. medical
ministrations are the actions taken to provide medical care to patients, and
web scraping can help healthcare providers identify the most effective
ministrations for their patients. for example, healthcare providers can use web
scraping to collect data about the symptoms and medical histories of their
patients, and then use this information to determine the most appropriate
ministrations. they can also use web scraping to gather information about the
latest medical research and clinical trials, which can help them stay
up-to-date with the latest treatments and procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12317">
<div class="article-summary-box-inner">
<span><p>In this work, we demonstrate the application of a simple first-order Taylor
expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times
m}$ and utilize it in language modeling. To enhance the basic Taylor expansion,
we introduce iteration and piecewise modeling, leading us to name the algorithm
the Iterative Piecewise Affine (IPA) approximation. The final algorithm
exhibits interesting resemblances to the Transformers decoder architecture. By
comparing parameter arrangements in IPA and Transformers, we observe a
strikingly similar performance, with IPA outperforming Transformers by 1.5\% in
the next token prediction task with cross-entropy loss for smaller sequence
lengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Dialogue Grounding Embodied Task in a Simulated Environment using Further Masked Language Modeling. (arXiv:2306.12387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12387">
<div class="article-summary-box-inner">
<span><p>Enhancing AI systems with efficient communication skills that align with
human understanding is crucial for their effective assistance to human users.
Proactive initiatives from the system side are needed to discern specific
circumstances and interact aptly with users to solve these scenarios. In this
research, we opt for a collective building assignment taken from the Minecraft
dataset. Our proposed method employs language modeling to enhance task
understanding through state-of-the-art (SOTA) methods using language models.
These models focus on grounding multi-modal understandinging and task-oriented
dialogue comprehension tasks. This focus aids in gaining insights into how well
these models interpret and respond to a variety of inputs and tasks. Our
experimental results provide compelling evidence of the superiority of our
proposed method. This showcases a substantial improvement and points towards a
promising direction for future research in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. (arXiv:2306.12420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12420">
<div class="article-summary-box-inner">
<span><p>Large foundation models have demonstrated a great ability to achieve general
human-level intelligence far beyond traditional approaches. As the technique
keeps attracting attention from the AI community, more and more large
foundation models have become publically available. However, most of those
models exhibit a major deficiency in specialized-task applications, where the
step of finetuning is still required for obtaining satisfactory performance. As
the number of available models and specialized tasks keeps growing, the job of
general finetuning becomes highly nontrivial. In this paper, we take the first
step to address this issue. We introduce an extensible and lightweight toolkit,
LMFlow, which aims to simplify the finetuning and inference of general large
foundation models. LMFlow offers a complete finetuning workflow for a large
foundation model to support personalized training with limited computing
resources. Furthermore, it supports continuous pretraining, instruction tuning,
parameter-efficient finetuning, alignment tuning, and large model inference,
along with carefully designed and extensible APIs. This toolkit has been
thoroughly tested and is available at https://github.com/OptimalScale/LMFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.12424">
<div class="article-summary-box-inner">
<span><p>We introduce VisoGender, a novel dataset for benchmarking gender bias in
vision-language models. We focus on occupation-related gender biases, inspired
by Winograd and Winogender schemas, where each image is associated with a
caption containing a pronoun relationship of subjects and objects in the scene.
VisoGender is balanced by gender representation in professional roles,
supporting bias evaluation in two ways: i) resolution bias, where we evaluate
the difference between gender resolution accuracies for men and women and ii)
retrieval bias, where we compare ratios of male and female professionals
retrieved for a gender-neutral search query. We benchmark several
state-of-the-art vision-language models and find that they lack the reasoning
abilities to correctly resolve gender in complex scenes. While the direction
and magnitude of gender bias depends on the task and the model being evaluated,
captioning models generally are more accurate and less biased than CLIP-like
models. Dataset and code are available at https://github.com/oxai/visogender
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors. (arXiv:2211.13224v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13224">
<div class="article-summary-box-inner">
<span><p>Recently, text-to-image diffusion models have shown remarkable capabilities
in creating realistic images from natural language prompts. However, few works
have explored using these models for semantic localization or grounding. In
this work, we explore how an off-the-shelf text-to-image diffusion model,
trained without exposure to localization information, can ground various
semantic phrases without segmentation-specific re-training. We introduce an
inference time optimization process capable of generating segmentation masks
conditioned on natural language prompts. Our proposal, Peekaboo, is a
first-of-its-kind zero-shot, open-vocabulary, unsupervised semantic grounding
technique leveraging diffusion models without any training. We evaluate
Peekaboo on the Pascal VOC dataset for unsupervised semantic segmentation and
the RefCOCO dataset for referring segmentation, showing results competitive
with promising results. We also demonstrate how Peekaboo can be used to
generate images with transparency, even though the underlying diffusion model
was only trained on RGB images - which to our knowledge we are the first to
attempt. Please see our project page, including our code:
https://ryanndagreat.github.io/peekaboo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15421">
<div class="article-summary-box-inner">
<span><p>Understanding visually-rich business documents to extract structured data and
automate business workflows has been receiving attention both in academia and
industry. Although recent multi-modal language models have achieved impressive
results, we find that existing benchmarks do not reflect the complexity of real
documents seen in industry. In this work, we identify the desiderata for a more
comprehensive benchmark and propose one we call Visually Rich Document
Understanding (VRDU). VRDU contains two datasets that represent several
challenges: rich schema including diverse data types as well as hierarchical
entities, complex templates including tables and multi-column layouts, and
diversity of different layouts (templates) within a single document type. We
design few-shot and conventional experiment settings along with a carefully
designed matching algorithm to evaluate extraction results. We report the
performance of strong baselines and offer three observations: (1) generalizing
to new document templates is still very challenging, (2) few-shot performance
has a lot of headroom, and (3) models struggle with hierarchical fields such as
line-items in an invoice. We plan to open source the benchmark and the
evaluation toolkit. We hope this helps the community make progress on these
challenging tasks in extracting structured data from visually rich documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design and analysis of tweet-based election models for the 2021 Mexican legislative election. (arXiv:2301.00626v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00626">
<div class="article-summary-box-inner">
<span><p>Modelling and forecasting real-life human behaviour using online social media
is an active endeavour of interest in politics, government, academia, and
industry. Since its creation in 2006, Twitter has been proposed as a potential
laboratory that could be used to gauge and predict social behaviour. During the
last decade, the user base of Twitter has been growing and becoming more
representative of the general population. Here we analyse this user base in the
context of the 2021 Mexican Legislative Election. To do so, we use a dataset of
15 million election-related tweets in the six months preceding election day. We
explore different election models that assign political preference to either
the ruling parties or the opposition. We find that models using data with
geographical attributes determine the results of the election with better
precision and accuracy than conventional polling methods. These results
demonstrate that analysis of public online data can outperform conventional
polling methods, and that political analysis and general forecasting would
likely benefit from incorporating such data in the immediate future. Moreover,
the same Twitter dataset with geographical attributes is positively correlated
with results from official census data on population and internet usage in
Mexico. These findings suggest that we have reached a period in time when
online activity, appropriately curated, can provide an accurate representation
of offline behaviour.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction. (arXiv:2302.05040v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.05040">
<div class="article-summary-box-inner">
<span><p>Speech-to-text errors made by automatic speech recognition (ASR) systems
negatively impact downstream models. Error correction models as a
post-processing text editing method have been recently developed for refining
the ASR outputs. However, efficient models that meet the low latency
requirements of industrial grade production systems have not been well studied.
We propose PATCorrect-a novel non-autoregressive (NAR) approach based on
multi-modal fusion leveraging representations from both text and phoneme
modalities, to reduce word error rate (WER) and perform robustly with varying
input transcription quality. We demonstrate that PATCorrect consistently
outperforms state-of-the-art NAR method on English corpus across different
upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to
9.46% WERR achieved by other methods using text only modality. Besides, its
inference latency is at tens of milliseconds, making it ideal for systems with
low latency requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering. (arXiv:2304.02138v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.02138">
<div class="article-summary-box-inner">
<span><p>The widespread adoption of large language models (LLMs), such as OpenAI's
ChatGPT, could revolutionize various industries, including geotechnical
engineering. However, GPT models can sometimes generate plausible-sounding but
false outputs, leading to hallucinations. In this article, we discuss the
importance of prompt engineering in mitigating these risks and harnessing the
full potential of GPT for geotechnical applications. We explore the challenges
and pitfalls associated with LLMs and highlight the role of context in ensuring
accurate and valuable responses. Furthermore, we examine the development of
context-specific search engines and the potential of LLMs to become a natural
interface for complex tasks, such as data analysis and design. We also develop
a unified interface using natural language to handle complex geotechnical
engineering tasks and data analysis. By integrating GPT into geotechnical
engineering workflows, professionals can streamline their work and develop
sustainable and resilient infrastructure systems for the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07611">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis has become an important research area in the
field of artificial intelligence. With the latest advances in deep learning,
this technology has reached new heights. It has great potential for both
application and research, making it a popular research topic. This review
provides an overview of the definition, background, and development of
multimodal sentiment analysis. It also covers recent datasets and advanced
models, emphasizing the challenges and future prospects of this technology.
Finally, it looks ahead to future research directions. It should be noted that
this review provides constructive suggestions for promising research directions
and building better performing multimodal sentiment analysis models, which can
help researchers in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09770">
<div class="article-summary-box-inner">
<span><p>Despite a surge collection of XAI methods, users still struggle to obtain
required AI explanations. Previous research suggests chatbots as dynamic
solutions, but the effective design of conversational XAI agents for practical
human needs remains under-explored. This paper focuses on Conversational XAI
for AI-assisted scientific writing tasks. Drawing from human linguistic
theories and formative studies, we identify four design rationales:
"multifaceted", "controllability", "mix-initiative", "context-aware
drill-down". We incorporate them into an interactive prototype, ConvXAI, which
facilitates heterogeneous AI explanations for scientific writing through
dialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based
baseline on improving human-perceived understanding and writing improvement.
The paper further discusses the practical human usage patterns in interacting
with ConvXAI for scientific co-writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers. (arXiv:2305.16863v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16863">
<div class="article-summary-box-inner">
<span><p>To address the problem of NLP classifiers learning spurious correlations
between training features and target labels, a common approach is to make the
model's predictions invariant to these features. However, this can be
counter-productive when the features have a non-zero causal effect on the
target label and thus are important for prediction. Therefore, using methods
from the causal inference literature, we propose an algorithm to regularize the
learnt effect of the features on the model's prediction to the estimated effect
of feature on label. This results in an automated augmentation method that
leverages the estimated effect of a feature to appropriately change the labels
for new augmented inputs. On toxicity and IMDB review datasets, the proposed
algorithm minimises spurious correlations and improves the minority group
(i.e., samples breaking spurious correlations) accuracy, while also improving
the total accuracy compared to standard training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking. (arXiv:2306.00887v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00887">
<div class="article-summary-box-inner">
<span><p>Open-vocabulary state tracking is a more practical version of state tracking
that aims to track state changes of entities throughout a process without
restricting the state space and entity space. OpenPI is to date the only
dataset annotated for open-vocabulary state tracking. However, we identify
issues with the dataset quality and evaluation metric. For the dataset, we
categorize 3 types of problems on the procedure level, step level and state
change level respectively, and build a clean dataset OpenPI-C using multiple
rounds of human judgment. For the evaluation metric, we propose a cluster-based
metric to fix the original metric's preference for repetition.
</p>
<p>Model-wise, we enhance the seq2seq generation baseline by reinstating two key
properties for state tracking: temporal dependency and entity awareness. The
state of the world after an action is inherently dependent on the previous
state. We model this dependency through a dynamic memory bank and allow the
model to attend to the memory slots during decoding. On the other hand, the
state of the world is naturally a union of the states of involved entities.
Since the entities are unknown in the open-vocabulary setting, we propose a
two-stage model that refines the state change prediction conditioned on
entities predicted from the first stage. Empirical results show the
effectiveness of our proposed model especially on the cluster-based metric. The
code and data are released at https://github.com/shirley-wu/openpi-c
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03361">
<div class="article-summary-box-inner">
<span><p>This paper presents a method for building a personalized open-domain dialogue
system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and
$\textit{HOW}$) problem for natural response generation in a commercial
setting, where personalized dialogue responses are heavily interleaved with
casual response turns. The proposed approach involves weighted dataset
blending, negative persona information augmentation methods, and the design of
personalized conversation datasets to address the challenges of $\textit{WWH}$
in personalized, open-domain dialogue systems. Our work effectively balances
dialogue fluency and tendency to ground, while also introducing a response-type
label to improve the controllability and explainability of the grounded
responses. The combination of these methods leads to more fluent conversations,
as evidenced by subjective human evaluations as well as objective evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can ChatGPT pass the Vietnamese National High School Graduation Examination?. (arXiv:2306.09170v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09170">
<div class="article-summary-box-inner">
<span><p>This research article highlights the potential of AI-powered chatbots in
education and presents the results of using ChatGPT, a large language model, to
complete the Vietnamese National High School Graduation Examination (VNHSGE).
The study dataset included 30 essays in the literature test case and 1,700
multiple-choice questions designed for other subjects. The results showed that
ChatGPT was able to pass the examination with an average score of 6-7,
demonstrating the technology's potential to revolutionize the educational
landscape. The analysis of ChatGPT performance revealed its proficiency in a
range of subjects, including mathematics, English, physics, chemistry, biology,
history, geography, civic education, and literature, which suggests its
potential to provide effective support for learners. However, further research
is needed to assess ChatGPT performance on more complex exam questions and its
potential to support learners in different contexts. As technology continues to
evolve and improve, we can expect to see the use of AI tools like ChatGPT
become increasingly common in educational settings, ultimately enhancing the
educational experience for both students and educators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models. (arXiv:2306.10968v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.10968">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated remarkable prowess in language
understanding and generation. Advancing from foundation LLMs to
instructionfollowing LLMs, instruction tuning plays a vital role in aligning
LLMs to human preferences. However, the existing LLMs are usually focused on
English, leading to inferior performance in non-English languages. In order to
improve the performance for non-English languages, it is necessary to collect
language-specific training data for foundation LLMs and construct
language-specific instructions for instruction tuning, both of which are heavy
loads. To minimize human workload, we propose to transfer the capabilities of
language generation and instruction following from English to other languages
through an interactive translation task. We have developed BayLing, an
instruction-following LLM by utilizing LLaMA as the foundation LLM and
automatically constructing interactive translation instructions for instructing
tuning. Extensive assessments demonstrate that BayLing achieves comparable
performance to GPT-3.5-turbo, despite utilizing a considerably smaller
parameter size of only 13 billion. Experimental results on translation tasks
show that BayLing achieves 95% of single-turn translation capability compared
to GPT-4 with automatic evaluation and 96% of interactive translation
capability compared to GPT-3.5-turbo with human evaluation. To estimate the
performance on general tasks, we created a multi-turn instruction test set
called BayLing-80. The experimental results on BayLing-80 indicate that BayLing
achieves 89% of performance compared to GPT-3.5-turbo. BayLing also
demonstrates outstanding performance on knowledge assessment of Chinese GaoKao
and English SAT, second only to GPT-3.5-turbo among a multitude of
instruction-following LLMs. Demo, homepage, code and models of BayLing are
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Language Models for Scientific Writing Support. (arXiv:2306.10974v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.10974">
<div class="article-summary-box-inner">
<span><p>We support scientific writers in determining whether a written sentence is
scientific, to which section it belongs, and suggest paraphrasings to improve
the sentence. Firstly, we propose a regression model trained on a corpus of
scientific sentences extracted from peer-reviewed scientific papers and
non-scientific text to assign a score that indicates the scientificness of a
sentence. We investigate the effect of equations and citations on this score to
test the model for potential biases. Secondly, we create a mapping of section
titles to a standard paper layout in AI and machine learning to classify a
sentence to its most likely section. We study the impact of context, i.e.,
surrounding sentences, on the section classification performance. Finally, we
propose a paraphraser, which suggests an alternative for a given sentence that
includes word substitutions, additions to the sentence, and structural changes
to improve the writing style. We train various large language models on
sentences extracted from arXiv papers that were peer reviewed and published at
A*, A, B, and C ranked conferences. On the scientificness task, all models
achieve an MSE smaller than $2\%$. For the section classification, BERT
outperforms WideMLP and SciBERT in most cases. We demonstrate that using
context enhances the classification of a sentence, achieving up to a $90\%$
F1-score. Although the paraphrasing models make comparatively few alterations,
they produce output sentences close to the gold standard. Large fine-tuned
models such as T5 Large perform best in experiments considering various
measures of difference between input sentence and gold standard. Code is
provided under https://github.com/JustinMuecke/SciSen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding. (arXiv:2306.11066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11066">
<div class="article-summary-box-inner">
<span><p>State-of-the-art few-shot learning (FSL) methods leverage prompt-based
fine-tuning to obtain remarkable results for natural language understanding
(NLU) tasks. While much of the prior FSL methods focus on improving downstream
task performance, there is a limited understanding of the adversarial
robustness of such methods. In this work, we conduct an extensive study of
several state-of-the-art FSL methods to assess their robustness to adversarial
perturbations. To better understand the impact of various factors towards
robustness (or the lack of it), we evaluate prompt-based FSL methods against
fully fine-tuned models for aspects such as the use of unlabeled data, multiple
prompts, number of few-shot examples, model size and type. Our results on six
GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL
methods lead to a notable relative drop in task performance (i.e., are less
robust) in the face of adversarial perturbations. However, using (i) unlabeled
data for prompt-based FSL and (ii) multiple prompts flip the trend. We further
demonstrate that increasing the number of few-shot examples and model size lead
to increased adversarial robustness of vanilla FSL methods. Broadly, our work
sheds light on the adversarial robustness evaluation of prompt-based FSL
methods for NLU tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually grounded few-shot word learning in low-resource settings. (arXiv:2306.11371v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.11371">
<div class="article-summary-box-inner">
<span><p>We propose a visually grounded speech model that learns new words and their
visual depictions from just a few word-image example pairs. Given a set of test
images and a spoken query, we ask the model which image depicts the query word.
Previous work has simplified this few-shot learning problem by either using an
artificial setting with digit word-image pairs or by using a large number of
examples per class. Moreover, all previous studies were performed using English
speech-image data. We propose an approach that can work on natural word-image
pairs but with less examples, i.e. fewer shots, and then illustrate how this
approach can be applied for multimodal few-shot learning in a real low-resource
language, Yoruba. Our approach involves using the given word-image example
pairs to mine new unsupervised word-image training pairs from large collections
of unlabelledspeech and images. Additionally, we use a word-to-image attention
mechanism to determine word-image similarity. With this new model, we achieve
better performance with fewer shots than previous approaches on an existing
English benchmark. Many of the model's mistakes are due to confusion between
visual concepts co-occurring in similar contexts. The experiments on Yoruba
show the benefit of transferring knowledge from a multimodal model trained on a
larger set of English speech-image data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-22 23:13:36.969049389 UTC">2023-06-22 23:13:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>