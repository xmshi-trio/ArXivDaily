<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-04-19T01:30:00Z">04-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Researchers eye-view of sarcasm detection in social media textual content. (arXiv:2304.08582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08582">
<div class="article-summary-box-inner">
<span><p>The enormous use of sarcastic text in all forms of communication in social
media will have a physiological effect on target users. Each user has a
different approach to misusing and recognising sarcasm. Sarcasm detection is
difficult even for users, and this will depend on many things such as
perspective, context, special symbols. So, that will be a challenging task for
machines to differentiate sarcastic sentences from non-sarcastic sentences.
There are no exact rules based on which model will accurately detect sarcasm
from many text corpus in the current situation. So, one needs to focus on
optimistic and forthcoming approaches in the sarcasm detection domain. This
paper discusses various sarcasm detection techniques and concludes with some
approaches, related datasets with optimal features, and the researcher's
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Scene Text Recognition for Character-Level Long-Tailed Distribution. (arXiv:2304.08592v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08592">
<div class="article-summary-box-inner">
<span><p>Despite the recent remarkable improvements in scene text recognition (STR),
the majority of the studies focused mainly on the English language, which only
includes few number of characters. However, STR models show a large performance
degradation on languages with a numerous number of characters (e.g., Chinese
and Korean), especially on characters that rarely appear due to the long-tailed
distribution of characters in such languages. To address such an issue, we
conducted an empirical analysis using synthetic datasets with different
character-level distributions (e.g., balanced and long-tailed distributions).
While increasing a substantial number of tail classes without considering the
context helps the model to correctly recognize characters individually,
training with such a synthetic dataset interferes the model with learning the
contextual information (i.e., relation among characters), which is also
important for predicting the whole word. Based on this motivation, we propose a
novel Context-Aware and Free Experts Network (CAFE-Net) using two experts: 1)
context-aware expert learns the contextual representation trained with a
long-tailed dataset composed of common words used in everyday life and 2)
context-free expert focuses on correctly predicting individual characters by
utilizing a dataset with a balanced number of characters. By training two
experts to focus on learning contextual and visual representations,
respectively, we propose a novel confidence ensemble method to compensate the
limitation of each expert. Through the experiments, we demonstrate that
CAFE-Net improves the STR performance on languages containing numerous number
of characters. Moreover, we show that CAFE-Net is easily applicable to various
STR models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08612">
<div class="article-summary-box-inner">
<span><p>Backpropagation, the cornerstone of deep learning, is limited to computing
gradients solely for continuous variables. This limitation hinders various
research on problems involving discrete latent variables. To address this
issue, we propose a novel approach for approximating the gradient of parameters
involved in generating discrete latent variables. First, we examine the widely
used Straight-Through (ST) heuristic and demonstrate that it works as a
first-order approximation of the gradient. Guided by our findings, we propose a
novel method called ReinMax, which integrates Heun's Method, a second-order
numerical method for solving ODEs, to approximate the gradient. Our method
achieves second-order accuracy without requiring Hessian or other second-order
derivatives. We conduct experiments on structured output prediction and
unsupervised generative modeling tasks. Our results show that \ours brings
consistent improvements over the state of the art, including ST and
Straight-Through Gumbel-Softmax. Implementations are released at
https://github.com/microsoft/ReinMax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08637">
<div class="article-summary-box-inner">
<span><p>We present an empirical evaluation of various outputs generated by nine of
the most widely-available large language models (LLMs). Our analysis is done
with off-the-shelf, readily-available tools. We find a correlation between
percentage of memorized text, percentage of unique text, and overall output
quality, when measured with respect to output pathologies such as
counterfactual and logically-flawed statements, and general failures like not
staying on topic. Overall, 80.0% of the outputs evaluated contained memorized
data, but outputs containing the most memorized content were also more likely
to be considered of high quality. We discuss and evaluate mitigation
strategies, showing that, in the models evaluated, the rate of memorized text
being output is reduced. We conclude with a discussion on potential
implications around what it means to learn, to memorize, and to evaluate
quality text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08649">
<div class="article-summary-box-inner">
<span><p>Models based on bidirectional encoder representations from transformers
(BERT) produce state of the art (SOTA) results on many natural language
processing (NLP) tasks such as named entity recognition (NER), part-of-speech
(POS) tagging etc. An interesting phenomenon occurs when classifying long
documents such as those from the US supreme court where BERT-based models can
be considered difficult to use on a first-pass or out-of-the-box basis. In this
paper, we experiment with several BERT-based classification techniques for US
supreme court decisions or supreme court database (SCDB) and compare them with
the previous SOTA results. We then compare our results specifically with SOTA
models for long documents. We compare our results for two classification tasks:
(1) a broad classification task with 15 categories and (2) a fine-grained
classification task with 279 categories. Our best result produces an accuracy
of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories
which marks an improvement of 8\% and 28\% respectively from previously
reported SOTA results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study. (arXiv:2304.08653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08653">
<div class="article-summary-box-inner">
<span><p>Modern deep models for summarization attains impressive benchmark
performance, but they are prone to generating miscalibrated predictive
uncertainty. This means that they assign high confidence to low-quality
predictions, leading to compromised reliability and trustworthiness in
real-world applications. Probabilistic deep learning methods are common
solutions to the miscalibration problem. However, their relative effectiveness
in complex autoregressive summarization tasks are not well-understood. In this
work, we thoroughly investigate different state-of-the-art probabilistic
methods' effectiveness in improving the uncertainty quality of the neural
summarization models, across three large-scale benchmarks with varying
difficulty. We show that the probabilistic methods consistently improve the
model's generation and uncertainty quality, leading to improved selective
generation performance (i.e., abstaining from low-quality summaries) in
practice. We also reveal notable failure patterns of probabilistic methods
widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout),
cautioning the importance of choosing appropriate method for the data setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Biomedical Text Summarization with Pre-trained Language Model. (arXiv:2304.08763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08763">
<div class="article-summary-box-inner">
<span><p>The exponential growth of biomedical texts such as biomedical literature and
electronic health records (EHRs), provides a big challenge for clinicians and
researchers to access clinical information efficiently. To address the problem,
biomedical text summarization has been proposed to support clinical information
retrieval and management, aiming at generating concise summaries that distill
key information from single or multiple biomedical documents. In recent years,
pre-trained language models (PLMs) have been the de facto standard of various
natural language processing tasks in the general domain. Most recently, PLMs
have been further investigated in the biomedical field and brought new insights
into the biomedical text summarization task. In this paper, we systematically
summarize recent advances that explore PLMs for biomedical text summarization,
to help understand recent progress, challenges, and future directions. We
categorize PLMs-based approaches according to how they utilize PLMs and what
PLMs they use. We then review available datasets, recent approaches and
evaluation metrics of the task. We finally discuss existing challenges and
promising future directions. To facilitate the research community, we line up
open resources including available datasets, recent approaches, codes,
evaluation metrics, and the leaderboard in a public project:
https://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08801">
<div class="article-summary-box-inner">
<span><p>In conversational settings, individuals exhibit unique behaviors, rendering a
one-size-fits-all approach insufficient for generating responses by dialogue
agents. Although past studies have aimed to create personalized dialogue agents
using speaker persona information, they have relied on the assumption that the
speaker's persona is already provided. However, this assumption is not always
valid, especially when it comes to chatbots utilized in industries like
banking, hotel reservations, and airline bookings. This research paper aims to
fill this gap by exploring the task of Speaker Profiling in Conversations
(SPC). The primary objective of SPC is to produce a summary of persona
characteristics for each individual speaker present in a dialogue. To
accomplish this, we have divided the task into three subtasks: persona
discovery, persona-type identification, and persona-value extraction. Given a
dialogue, the first subtask aims to identify all utterances that contain
persona information. Subsequently, the second task evaluates these utterances
to identify the type of persona information they contain, while the third
subtask identifies the specific persona values for each identified type. To
address the task of SPC, we have curated a new dataset named SPICE, which comes
with specific labels. We have evaluated various baselines on this dataset and
benchmarked it with a new neural model, SPOT, which we introduce in this paper.
Furthermore, we present a comprehensive analysis of SPOT, examining the
limitations of individual modules both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval. (arXiv:2304.08807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08807">
<div class="article-summary-box-inner">
<span><p>This paper studies the task of best counter-argument retrieval given an input
argument. Following the definition that the best counter-argument addresses the
same aspects as the input argument while having the opposite stance, we aim to
develop an efficient and effective model for scoring counter-arguments based on
similarity and dissimilarity metrics. We first conduct an experimental study on
the effectiveness of available scoring methods, including traditional
Learning-To-Rank (LTR) and recent neural scoring models. We then propose
Bipolar-encoder, a novel BERT-based model to learn an optimal representation
for simultaneous similarity and dissimilarity. Experimental results show that
our proposed method can achieve the accuracy@1 of 88.9\%, which significantly
outperforms other baselines by a large margin. When combined with an
appropriate caching technique, Bipolar-encoder is comparably efficient at
prediction time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08821">
<div class="article-summary-box-inner">
<span><p>Data augmentation has been established as an efficacious approach to
supplement useful information for low-resource datasets. Traditional
augmentation techniques such as noise injection and image transformations have
been widely used. In addition, generative data augmentation (GDA) has been
shown to produce more diverse and flexible data. While generative adversarial
networks (GANs) have been frequently used for GDA, they lack diversity and
controllability compared to text-to-image diffusion models. In this paper, we
propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the
capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image
(T2I) generative models for data augmentation. By conditioning the T2I model on
detailed descriptions produced by T2T models, we are able to generate
photo-realistic labeled images in a flexible and controllable manner.
Experiments on in-domain classification, cross-domain classification, and image
captioning tasks show consistent improvements over other data augmentation
baselines. Analytical studies in varied settings, including few-shot,
long-tail, and adversarial, further reinforce the effectiveness of TTIDA in
enhancing performance and increasing robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese. (arXiv:2304.08823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08823">
<div class="article-summary-box-inner">
<span><p>Multilingual language models have pushed state-of-the-art in cross-lingual
NLP transfer. The majority of zero-shot cross-lingual transfer, however, use
one and the same massively multilingual transformer (e.g., mBERT or XLM-R) to
transfer to all target languages, irrespective of their typological,
etymological, and phylogenetic relations to other languages. In particular,
readily available data and models of resource-rich sibling languages are often
ignored. In this work, we empirically show, in a case study for Faroese -- a
low-resource language from a high-resource language family -- that by
leveraging the phylogenetic information and departing from the
'one-size-fits-all' paradigm, one can improve cross-lingual transfer to
low-resource languages. In particular, we leverage abundant resources of other
Scandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for
the benefit of Faroese. Our evaluation results show that we can substantially
improve the transfer performance to Faroese by exploiting data and models of
closely-related high-resource languages. Further, we release a new web corpus
of Faroese and Faroese datasets for named entity recognition (NER), semantic
text similarity (STS), and new language models trained on all Scandinavian
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08862">
<div class="article-summary-box-inner">
<span><p>This paper presents an extension to train end-to-end Context-Aware
Transformer Transducer ( CATT ) models by using a simple, yet efficient method
of mining hard negative phrases from the latent space of the context encoder.
During training, given a reference query, we mine a number of similar phrases
using approximate nearest neighbour search. These sampled phrases are then used
as negative examples in the context list alongside random and ground truth
contextual information. By including approximate nearest neighbour phrases
(ANN-P) in the context list, we encourage the learned representation to
disambiguate between similar, but not identical, biasing phrases. This improves
biasing accuracy when there are several similar phrases in the biasing
inventory. We carry out experiments in a large-scale data regime obtaining up
to 7% relative word error rate reductions for the contextual portion of test
data. We also extend and evaluate CATT approach in streaming applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08865">
<div class="article-summary-box-inner">
<span><p>Large multilingual pretrained language models (mPLMs) have become the de
facto state of the art for cross-lingual transfer in NLP. However, their
large-scale deployment to many languages, besides pretraining data scarcity, is
also hindered by the increase in vocabulary size and limitations in their
parameter budget. In order to boost the capacity of mPLMs to deal with
low-resource and unseen languages, we explore the potential of leveraging
transliteration on a massive scale. In particular, we explore the UROMAN
transliteration tool, which provides mappings from UTF-8 to Latin characters
for all the writing systems, enabling inexpensive romanization for virtually
any language. We first focus on establishing how UROMAN compares against other
language-specific and manually curated transliterators for adapting
multilingual PLMs. We then study and compare a plethora of data- and
parameter-efficient strategies for adapting the mPLMs to romanized and
non-romanized corpora of 14 diverse low-resource languages. Our results reveal
that UROMAN-based transliteration can offer strong performance for many
languages, with particular gains achieved in the most challenging setups: on
languages with unseen scripts and with limited training data without any
vocabulary augmentation. Further analyses reveal that an improved tokenizer
based on romanized data can even outperform non-transliteration-based methods
in the majority of languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08891">
<div class="article-summary-box-inner">
<span><p>While quality estimation (QE) can play an important role in the translation
process, its effectiveness relies on the availability and quality of training
data. For QE in particular, high-quality labeled data is often lacking due to
the high-cost and effort associated with labeling such data. Aside from the
data scarcity challenge, QE models should also be generalizable, i.e., they
should be able to handle data from different domains, both generic and
specific. To alleviate these two main issues -- data scarcity and domain
mismatch -- this paper combines domain adaptation and data augmentation within
a robust QE system. Our method is to first train a generic QE model and then
fine-tune it on a specific domain while retaining generic knowledge. Our
results show a significant improvement for all the language pairs investigated,
better cross-lingual inference, and a superior performance in zero-shot
learning scenarios as compared to state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-Shot Personalized Table-to-Text Generation with Contrastive Persona Distillation. (arXiv:2304.08911v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08911">
<div class="article-summary-box-inner">
<span><p>Existing neural methods have shown great potentials towards generating
informative text from structured tabular data as well as maintaining high
content fidelity. However, few of them shed light on generating personalized
expressions, which often requires well-aligned persona-table-text datasets that
are difficult to obtain. To overcome these obstacles, we explore personalized
table-to-text generation under a zero-shot setting, by assuming no well-aligned
persona-table-text triples are required during training. To this end, we
firstly collect a set of unpaired persona information and then propose a
semi-supervised approach with contrastive persona distillation (S2P-CPD) to
generate personalized context. Specifically, tabular data and persona
information are firstly represented as latent variables separately. Then, we
devise a latent space fusion technique to distill persona information into the
table representation. Besides, a contrastive-based discriminator is employed to
guarantee the style consistency between the generated context and its
corresponding persona. Experimental results on two benchmarks demonstrate
S2P-CPD's ability on keeping both content fidelity and personalized
expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Textbooks with Visuals from the Web for Improved Learning. (arXiv:2304.08931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08931">
<div class="article-summary-box-inner">
<span><p>Textbooks are the primary vehicle for delivering quality education to
students. It has been shown that explanatory or illustrative visuals play a key
role in the retention, comprehension and the general transfer of knowledge.
However, many textbooks, especially in the developing world, are low quality
and lack interesting visuals to support student learning. In this paper, we
investigate the effectiveness of vision-language models to automatically
enhance textbooks with images from the web. Specifically, we collect a dataset
of e-textbooks from one of the largest free online publishers in the world. We
rigorously analyse the dataset, and use the resulting analysis to motivate a
task that involves retrieving and appropriately assigning web images to
textbooks, which we frame as a novel optimization problem. Through a
crowd-sourced evaluation, we verify that (1) while the original textbook images
are rated higher, automatically assigned ones are not far behind, and (2) the
choice of the optimization problem matters. We release the dataset of textbooks
with an associated image bank to spur further research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08968">
<div class="article-summary-box-inner">
<span><p>The self-attention revolution allowed generative language models to scale and
achieve increasingly impressive abilities. Such models - commonly referred to
as Large Language Models (LLMs) - have recently gained prominence with the
general public, thanks to conversational fine-tuning, putting their behavior in
line with public expectations regarding AI. This prominence amplified prior
concerns regarding the misuse of LLMs and led to the emergence of numerous
tools to detect LLMs in the wild.
</p>
<p>Unfortunately, most such tools are critically flawed. While major
publications in the LLM detectability field suggested that LLMs were easy to
detect with fine-tuned autoencoders, the limitations of their results are easy
to overlook. Specifically, they assumed publicly available generative models
without fine-tunes or non-trivial prompts. While the importance of these
assumptions has been demonstrated, until now, it remained unclear how well such
detection could be countered.
</p>
<p>Here, we show that an attacker with access to such detectors' reference human
texts and output not only evades detection but can fully frustrate the detector
training - with a reasonable budget and all its outputs labeled as such.
Achieving it required combining common "reinforcement from critic" loss
function modification and AdamW optimizer, which led to surprisingly good
fine-tuning generalization. Finally, we warn against the temptation to
transpose the conclusions obtained in RNN-driven text GANs to LLMs due to their
better representative ability.
</p>
<p>These results have critical implications for the detection and prevention of
malicious use of generative language models, and we hope they will aid the
designers of generative models and detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. (arXiv:2304.08981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08981">
<div class="article-summary-box-inner">
<span><p>Over the past few decades, multimodal emotion recognition has made remarkable
progress with the development of deep learning. However, existing technologies
are difficult to meet the demand for practical applications. To improve the
robustness, we launch a Multimodal Emotion Recognition Challenge (MER 2023) to
motivate global researchers to build innovative technologies that can further
accelerate and foster research. For this year's challenge, we present three
distinct sub-challenges: (1) MER-MULTI, in which participants recognize both
discrete and dimensional emotions; (2) MER-NOISE, in which noise is added to
test videos for modality robustness evaluation; (3) MER-SEMI, which provides
large amounts of unlabeled samples for semi-supervised learning. In this paper,
we test a variety of multimodal features and provide a competitive baseline for
each sub-challenge. Our system achieves 77.57% on the F1 score and 0.82 on the
mean squared error (MSE) for MER-MULTI, 69.82% on the F1 score and 1.12 on MSE
for MER-NOISE, and 86.75% on the F1 score for MER-SEMI, respectively. Baseline
code is available at https://github.com/zeroQiaoba/MER2023-Baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08991">
<div class="article-summary-box-inner">
<span><p>This paper describes Difference-aware Deep continuous prompt for Contrastive
Sentence Embeddings (D2CSE) that learns sentence embeddings. Compared to
state-of-the-art approaches, D2CSE computes sentence vectors that are
exceptional to distinguish a subtle difference in similar sentences by
employing a simple neural architecture for continuous prompts. Unlike existing
architectures that require multiple pretrained language models (PLMs) to
process a pair of the original and corrupted (subtly modified) sentences, D2CSE
avoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous
prompts by performing multiple tasks -- i.e., contrastive learning and
conditional replaced token detection all done in a self-guided manner. D2CSE
overloads a single PLM on continuous prompts and greatly saves memory
consumption as a result. The number of training parameters in D2CSE is reduced
to about 1\% of existing approaches while substantially improving the quality
of sentence embeddings. We evaluate D2CSE on seven Semantic Textual Similarity
(STS) benchmarks, using three different metrics, namely, Spearman's rank
correlation, recall@K for a retrieval task, and the anisotropy of an embedding
space measured in alignment and uniformity. Our empirical results suggest that
shallow (not too meticulously devised) continuous prompts can be honed
effectively for multiple NLP tasks and lead to improvements upon existing
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese. (arXiv:2304.08999v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08999">
<div class="article-summary-box-inner">
<span><p>Textual health records of cancer patients are usually protracted and highly
unstructured, making it very time-consuming for health professionals to get a
complete overview of the patient's therapeutic course. As such limitations can
lead to suboptimal and/or inefficient treatment procedures, healthcare
providers would greatly benefit from a system that effectively summarizes the
information of those records. With the advent of deep neural models, this
objective has been partially attained for English clinical texts, however, the
research community still lacks an effective solution for languages with limited
resources. In this paper, we present the approach we developed to extract
procedures, drugs, and diseases from oncology health records written in
European Portuguese. This project was conducted in collaboration with the
Portuguese Institute for Oncology which, besides holding over $10$ years of
duly protected medical records, also provided oncologist expertise throughout
the development of the project. Since there is no annotated corpus for
biomedical entity extraction in Portuguese, we also present the strategy we
followed in annotating the corpus for the development of the models. The final
models, which combined a neural architecture with entity linking, achieved
$F_1$ scores of $88.6$, $95.0$, and $55.8$ per cent in the mention extraction
of procedures, drugs, and diseases, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09048">
<div class="article-summary-box-inner">
<span><p>Current generative knowledge graph construction approaches usually fail to
capture structural knowledge by simply flattening natural language into
serialized texts or a specification language. However, large generative
language model trained on structured data such as code has demonstrated
impressive capability in understanding natural language for structural
prediction and reasoning tasks. Intuitively, we address the task of generative
knowledge graph construction with code language model: given a code-format
natural language input, the target is to generate triples which can be
represented as code completion tasks. Specifically, we develop schema-aware
prompts that effectively utilize the semantic structure within the knowledge
graph. As code inherently possesses structure, such as class and function
definitions, it serves as a useful model for prior semantic structural
knowledge. Furthermore, we employ a rationale-enhanced generation method to
boost the performance. Rationales provide intermediate steps, thereby improving
knowledge extraction abilities. Experimental results indicate that the proposed
approach can obtain better performance on benchmark datasets compared with
baselines. Code and datasets are available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09058">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PLMs), as parametric-based eager learners, have
become the de-facto choice for current paradigms of Natural Language Processing
(NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning
paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we
revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the
methodological level, we propose to adopt k-NN with textual representations of
PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the
training process. (2) Linearly interpolate the probability distribution
predicted by k-NN with that of the PLMs' classifier. At the heart of our
approach is the implementation of k-NN-calibrated training, which treats
predicted results as indicators for easy versus hard examples during the
training process. From the perspective of the diversity of application
scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning
paradigms and zero-shot, few-shot and fully-supervised settings, respectively,
across eight diverse end-tasks. We hope our exploration will encourage the
community to revisit the power of classical methods for efficient
NLP\footnote{Code and datasets are available in
https://github.com/zjunlp/Revisit-KNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised clustering of file dialects according to monotonic decompositions of mixtures. (arXiv:2304.09082v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09082">
<div class="article-summary-box-inner">
<span><p>This paper proposes an unsupervised classification method that partitions a
set of files into non-overlapping dialects based upon their behaviors,
determined by messages produced by a collection of programs that consume them.
The pattern of messages can be used as the signature of a particular kind of
behavior, with the understanding that some messages are likely to co-occur,
while others are not. Patterns of messages can be used to classify files into
dialects. A dialect is defined by a subset of messages, called the required
messages. Once files are conditioned upon dialect and its required messages,
the remaining messages are statistically independent.
</p>
<p>With this definition of dialect in hand, we present a greedy algorithm that
deduces candidate dialects from a dataset consisting of a matrix of
file-message data, demonstrate its performance on several file formats, and
prove conditions under which it is optimal. We show that an analyst needs to
consider fewer dialects than distinct message patterns, which reduces their
cognitive load when studying a complex format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation. (arXiv:2304.09093v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09093">
<div class="article-summary-box-inner">
<span><p>State-of-the-art methods on conversational recommender systems (CRS) leverage
external knowledge to enhance both items' and contextual words' representations
to achieve high quality recommendations and responses generation. However, the
representations of the items and words are usually modeled in two separated
semantic spaces, which leads to misalignment issue between them. Consequently,
this will cause the CRS to only achieve a sub-optimal ranking performance,
especially when there is a lack of sufficient information from the user's
input. To address limitations of previous works, we propose a new CRS framework
KLEVER, which jointly models items and their associated contextual words in the
same semantic space. Particularly, we construct an item descriptive graph from
the rich items' textual features, such as item description and categories.
Based on the constructed descriptive graph, KLEVER jointly learns the
embeddings of the words and items, towards enhancing both recommender and
dialog generation modules. Extensive experiments on benchmarking CRS dataset
demonstrate that KLEVER achieves superior performance, especially when the
information from the users' responses is lacking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Math Word Problems by Combining Language Models With Symbolic Solvers. (arXiv:2304.09102v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09102">
<div class="article-summary-box-inner">
<span><p>Automatically generating high-quality step-by-step solutions to math word
problems has many applications in education. Recently, combining large language
models (LLMs) with external tools to perform complex reasoning and calculation
has emerged as a promising direction for solving math word problems, but prior
approaches such as Program-Aided Language model (PAL) are biased towards simple
procedural problems and less effective for problems that require declarative
reasoning. We propose an approach that combines an LLM that can incrementally
formalize word problems as a set of variables and equations with an external
symbolic solver that can solve the equations. Our approach achieves comparable
accuracy to the original PAL on the GSM8K benchmark of math word problems and
outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more
challenging word problems extracted from Algebra textbooks. Our work highlights
the benefits of using declarative and incremental representations when
interfacing with an external tool for solving complex math word problems. Our
data and prompts are publicly available at
https://github.com/joyheyueya/declarative-math-word-problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT: Applications, Opportunities, and Threats. (arXiv:2304.09103v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09103">
<div class="article-summary-box-inner">
<span><p>Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer)
is an artificial intelligence technology that is fine-tuned using supervised
machine learning and reinforcement learning techniques, allowing a computer to
generate natural language conversation fully autonomously. ChatGPT is built on
the transformer architecture and trained on millions of conversations from
various sources. The system combines the power of pre-trained deep learning
models with a programmability layer to provide a strong base for generating
natural language conversations. In this study, after reviewing the existing
literature, we examine the applications, opportunities, and threats of ChatGPT
in 10 main domains, providing detailed examples for the business and industry
as well as education. We also conducted an experimental study, checking the
effectiveness and comparing the performances of GPT-3.5 and GPT-4, and found
that the latter performs significantly better. Despite its exceptional ability
to generate natural-sounding responses, the authors believe that ChatGPT does
not possess the same level of understanding, empathy, and creativity as a human
and cannot fully replace them in most situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. (arXiv:2304.09116v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09116">
<div class="article-summary-box-inner">
<span><p>Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild
datasets is important to capture the diversity in human speech such as speaker
identities, prosodies, and styles (e.g., singing). Current large TTS systems
usually quantize speech into discrete tokens and use language models to
generate these tokens one by one, which suffer from unstable prosody, word
skipping/repeating issue, and poor voice quality. In this paper, we develop
NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual
vector quantizers to get the quantized latent vectors and uses a diffusion
model to generate these latent vectors conditioned on text input. To enhance
the zero-shot capability that is important to achieve diverse speech synthesis,
we design a speech prompting mechanism to facilitate in-context learning in the
diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to
large-scale datasets with 44K hours of speech and singing data and evaluate its
voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS
systems by a large margin in terms of prosody/timbre similarity, robustness,
and voice quality in a zero-shot setting, and performs novel zero-shot singing
synthesis with only a speech prompt. Audio samples are available at
https://speechresearch.github.io/naturalspeech2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task. (arXiv:2304.09138v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09138">
<div class="article-summary-box-inner">
<span><p>Recently, ChatGPT and GPT-4 have emerged and gained immense global attention
due to their unparalleled performance in language processing. Despite
demonstrating impressive capability in various open-domain tasks, their
adequacy in highly specific fields like radiology remains untested. Radiology
presents unique linguistic phenomena distinct from open-domain data due to its
specificity and complexity. Assessing the performance of large language models
(LLMs) in such specific domains is crucial not only for a thorough evaluation
of their overall performance but also for providing valuable insights into
future model design directions: whether model design should be generic or
domain-specific. To this end, in this study, we evaluate the performance of
ChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned
specifically on task-related data samples. We also conduct a comprehensive
investigation on ChatGPT/GPT-4's reasoning ability by introducing varying
levels of inference difficulty. Our results show that 1) GPT-4 outperforms
ChatGPT in the radiology NLI task; 2) other specifically fine-tuned models
require significant amounts of data samples to achieve comparable performance
to ChatGPT/GPT-4. These findings demonstrate that constructing a generic model
that is capable of solving various tasks across different domains is feasible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09145">
<div class="article-summary-box-inner">
<span><p>Quantization of transformer language models faces significant challenges due
to the existence of detrimental outliers in activations. We observe that these
outliers are asymmetric and concentrated in specific channels. To address this
issue, we propose the Outlier Suppression+ framework. First, we introduce
channel-wise shifting and scaling operations to eliminate asymmetric
presentation and scale down problematic channels. We demonstrate that these
operations can be seamlessly migrated into subsequent modules while maintaining
equivalence. Second, we quantitatively analyze the optimal values for shifting
and scaling, taking into account both the asymmetric property and quantization
errors of weights in the next layer. Our lightweight framework can incur
minimal performance degradation under static and standard post-training
quantization settings. Comprehensive results across various tasks and models
reveal that our approach achieves near-floating-point performance on both small
models, such as BERT, and large language models (LLMs) including OPTs, BLOOM,
and BLOOMZ at 8-bit and 6-bit settings. Furthermore, we establish a new state
of the art for 4-bit BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining. (arXiv:2304.09151v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09151">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual large language models have typically used heuristic
temperature-based sampling to balance between different languages. However
previous work has not systematically evaluated the efficacy of different
pretraining language distributions across model scales. In this paper, we
propose a new sampling method, UniMax, that delivers more uniform coverage of
head languages while mitigating overfitting on tail languages by explicitly
capping the number of repeats over each language's corpus. We perform an
extensive series of ablations testing a range of sampling strategies on a suite
of multilingual benchmarks, while varying model scale. We find that UniMax
outperforms standard temperature-based sampling, and the benefits persist as
scale increases. As part of our contribution, we release: (i) an improved and
refreshed mC4 multilingual corpus consisting of 29 trillion characters across
107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained
with UniMax sampling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Media Slant is Contagious. (arXiv:2202.07269v2 [econ.GN] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07269">
<div class="article-summary-box-inner">
<span><p>We examine the diffusion of media slant, specifically how partisan content
from national cable news affects local newspapers in the U.S., 2005-2008. We
use a text-based measure of cable news slant trained on content from Fox News
Channel (FNC), CNN, and MSNBC to analyze how local newspapers adopt FNC's slant
over CNN/MSNBC's. Our findings show that local news becomes more similar to FNC
content in response to an exogenous increase in local FNC viewership. This
shift is not limited to borrowing from cable news, but rather, local
newspapers' own content changes. Further, cable TV slant polarizes local news
content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08063">
<div class="article-summary-box-inner">
<span><p>Knowledge Extraction (KE), aiming to extract structural information from
unstructured texts, often suffers from data scarcity and emerging unseen types,
i.e., low-resource scenarios. Many neural approaches to low-resource KE have
been widely investigated and achieved impressive performance. In this paper, we
present a literature review towards KE in low-resource scenarios, and
systematically categorize existing works into three paradigms: (1) exploiting
higher-resource data, (2) exploiting stronger models, and (3) exploiting data
and models together. In addition, we highlight promising applications and
outline some potential directions for future research. We hope that our survey
can help both the academic and industrial communities to better understand this
field, inspire more ideas, and boost broader applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems. (arXiv:2202.13876v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13876">
<div class="article-summary-box-inner">
<span><p>Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical
workflow by providing relevant literature and similar patients for a given
patient. However, the development of ReCDS systems has been severely obstructed
by the lack of diverse patient collections and publicly available large-scale
patient-level annotation datasets. In this paper, we aim to define and
benchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and
Patient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called
PMC-Patients.
</p>
<p>Methods: We extract patient summaries from PubMed Central articles using
simple heuristics and utilize the PubMed citation graph to define
patient-article relevance and patient-patient similarity. We also implement and
evaluate several ReCDS systems on the PMC-Patients benchmarks, including sparse
retrievers, dense retrievers, and nearest neighbor retrievers. We conduct
several case studies to show the clinical utility of PMC-Patients.
</p>
<p>Results: PMC-Patients contains 167k patient summaries with 3.1M
patient-article relevance annotations and 293k patient-patient similarity
annotations, which is the largest-scale resource for ReCDS and also one of the
largest patient collections. Human evaluation and analysis show that
PMC-Patients is a diverse dataset with high-quality annotations. The evaluation
of various ReCDS systems shows that the PMC-Patients benchmark is challenging
and calls for further research.
</p>
<p>Conclusion: We present PMC-Patients, a large-scale, diverse, and publicly
available patient summary dataset with the largest-scale patient-level relation
annotations. Based on PMC-Patients, we formally define two benchmark tasks for
ReCDS systems and evaluate various existing retrieval methods. PMC-Patients can
largely facilitate methodology research on ReCDS systems and shows real-world
clinical utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06414">
<div class="article-summary-box-inner">
<span><p>In the past few years, it has become increasingly evident that deep neural
networks are not resilient enough to withstand adversarial perturbations in
input data, leaving them vulnerable to attack. Various authors have proposed
strong adversarial attacks for computer vision and Natural Language Processing
(NLP) tasks. As a response, many defense mechanisms have also been proposed to
prevent these networks from failing. The significance of defending neural
networks against adversarial attacks lies in ensuring that the model's
predictions remain unchanged even if the input data is perturbed. Several
methods for adversarial defense in NLP have been proposed, catering to
different NLP tasks such as text classification, named entity recognition, and
natural language inference. Some of these methods not only defend neural
networks against adversarial attacks but also act as a regularization mechanism
during training, saving the model from overfitting. This survey aims to review
the various methods proposed for adversarial defenses in NLP over the past few
years by introducing a novel taxonomy. The survey also highlights the fragility
of advanced deep neural networks in NLP and the challenges involved in
defending them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">American cultural regions mapped through the lexical analysis of social media. (arXiv:2208.07649v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.07649">
<div class="article-summary-box-inner">
<span><p>Cultural areas represent a useful concept that cross-fertilizes diverse
fields in social sciences. Knowledge of how humans organize and relate their
ideas and behavior within a society helps to understand their actions and
attitudes towards different issues. However, the selection of common traits
that shape a cultural area is somewhat arbitrary. What is needed is a method
that can leverage the massive amounts of data coming online, especially through
social media, to identify cultural regions without ad-hoc assumptions, biases
or prejudices. This work takes a crucial step in this direction by introducing
a method to infer cultural regions based on the automatic analysis of large
datasets from microblogging posts. The approach presented here is based on the
principle that cultural affiliation can be inferred from the topics that people
discuss among themselves. Specifically, regional variations in written
discourse are measured in American social media. From the frequency
distributions of content words in geotagged Tweets, the regional hotspots of
words' usage are found, and from there, principal components of regional
variation are derived. Through a hierarchical clustering of the data in this
lower-dimensional space, this method yields clear cultural areas and the topics
of discussion that define them. It uncovers a manifest North-South separation,
which is primarily influenced by the African American culture, and further
contiguous (East-West) and non-contiguous divisions that provide a
comprehensive picture of today's cultural areas in the US.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models. (arXiv:2209.06506v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.06506">
<div class="article-summary-box-inner">
<span><p>Neural text ranking models have witnessed significant advancement and are
increasingly being deployed in practice. Unfortunately, they also inherit
adversarial vulnerabilities of general neural models, which have been detected
but remain underexplored by prior studies. Moreover, the inherit adversarial
vulnerabilities might be leveraged by blackhat SEO to defeat better-protected
search engines. In this study, we propose an imitation adversarial attack on
black-box neural passage ranking models. We first show that the target passage
ranking model can be transparentized and imitated by enumerating critical
queries/candidates and then train a ranking imitation model. Leveraging the
ranking imitation model, we can elaborately manipulate the ranking results and
transfer the manipulation attack to the target ranking model. For this purpose,
we propose an innovative gradient-based attack method, empowered by the
pairwise objective function, to generate adversarial triggers, which causes
premeditated disorderliness with very few tokens. To equip the trigger
camouflages, we add the next sentence prediction loss and the language model
fluency constraint to the objective function. Experimental results on passage
ranking demonstrate the effectiveness of the ranking imitation attack model and
adversarial triggers against various SOTA neural ranking models. Furthermore,
various mitigation analyses and human evaluation show the effectiveness of
camouflages when facing potential mitigation approaches. To motivate other
scholars to further investigate this novel and important problem, we make the
experiment data and code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variable Attention Masking for Configurable Transformer Transducer Speech Recognition. (arXiv:2211.01438v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01438">
<div class="article-summary-box-inner">
<span><p>This work studies the use of attention masking in transformer transducer
based speech recognition for building a single configurable model for different
deployment scenarios. We present a comprehensive set of experiments comparing
fixed masking, where the same attention mask is applied at every frame, with
chunked masking, where the attention mask for each frame is determined by chunk
boundaries, in terms of recognition accuracy and latency. We then explore the
use of variable masking, where the attention masks are sampled from a target
distribution at training time, to build models that can work in different
configurations. Finally, we investigate how a single configurable model can be
used to perform both first pass streaming recognition and second pass acoustic
rescoring. Experiments show that chunked masking achieves a better accuracy vs
latency trade-off compared to fixed masking, both with and without FastEmit. We
also show that variable masking improves the accuracy by up to 8% relative in
the acoustic re-scoring scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Circling Back to Recurrent Models of Language. (arXiv:2211.01848v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01848">
<div class="article-summary-box-inner">
<span><p>Just because some purely recurrent models suffer from being hard to optimize
and inefficient on today's hardware, they are not necessarily bad models of
language. We demonstrate this by the extent to which these models can still be
improved by a combination of a slightly better recurrent cell, architecture,
objective, as well as optimization. In the process, we establish a new state of
the art for language modelling on small datasets and on Enwik8 with dynamic
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.03046">
<div class="article-summary-box-inner">
<span><p>Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact
descriptions according to rule of law, serves as legal assistance to mitigate
the great work burden of limited legal practitioners. Most existing methods
apply various large-scale pre-trained language models (PLMs) finetuned in LJP
tasks to obtain consistent improvements. However, we discover the fact that the
state-of-the-art (SOTA) model makes judgment predictions according to
irrelevant (or non-casual) information. The violation of rule of law not only
weakens the robustness and generalization ability of models but also results in
severe social problems like discrimination. In this paper, we use causal
structural models (SCMs) to theoretically analyze how LJP models learn to make
decisions and why they can succeed in passing the traditional testing paradigm
without learning causality. According to our analysis, we provide two solutions
intervening on data and model by causality, respectively. In detail, we first
distinguish non-causal information by applying the open information extraction
(OIE) technique. Then, we propose a method named the Causal Information
Enhanced SAmpling Method (CIESAM) to eliminate the non-causal information from
data. To validate our theoretical analysis, we further propose another method
using our proposed Causality-Aware Self-Attention Mechanism (CASAM) to guide
the model to learn the underlying causality knowledge in legal texts. The
confidence of CASAM in learning causal information is higher than that of
CIESAM. The extensive experimental results show that both our proposed methods
achieve state-of-the-art (SOTA) performance on three commonly used
legal-specific datasets. The stronger performance of CASAM further demonstrates
that causality is the key to the robustness and generalization ability of
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06373">
<div class="article-summary-box-inner">
<span><p>Current approaches to empathetic response generation typically encode the
entire dialogue history directly and put the output into a decoder to generate
friendly feedback. These methods focus on modelling contextual information but
neglect capturing the direct intention of the speaker. We argue that the last
utterance in the dialogue empirically conveys the intention of the speaker.
Consequently, we propose a novel model named InferEM for empathetic response
generation. We separately encode the last utterance and fuse it with the entire
dialogue through the multi-head attention based intention fusion module to
capture the speaker's intention. Besides, we utilize previous utterances to
predict the last utterance, which simulates human's psychology to guess what
the interlocutor may speak in advance. To balance the optimizing rates of the
utterance prediction and response generation, a multi-task learning strategy is
designed for InferEM. Experimental results demonstrate the plausibility and
validity of InferEM in improving empathetic expression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method. (arXiv:2301.03029v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03029">
<div class="article-summary-box-inner">
<span><p>Topic Modelling (TM) is from the research branches of natural language
understanding (NLU) and natural language processing (NLP) that is to facilitate
insightful analysis from large documents and datasets, such as a summarisation
of main topics and the topic changes. This kind of discovery is getting more
popular in real-life applications due to its impact on big data analytics. In
this study, from the social-media and healthcare domain, we apply popular
Latent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish
newspaper articles about Coronavirus. We describe the corpus we created
including 6515 articles, methods applied, and statistics on topic changes over
approximately 1 year and two months period of time from 17th January 2020 to
13th March 2021. We hope this work can be an asset for grounding applications
of topic modelling and can be inspiring for similar case studies in an era with
pandemics, to support socio-economic impact research as well as clinical and
healthcare analytics. Our data and source code are openly available at
https://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation
(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;
BERT-topic
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.04647">
<div class="article-summary-box-inner">
<span><p>We learn a visual representation that captures information about the camera
that recorded a given photo. To do this, we train a multimodal embedding
between image patches and the EXIF metadata that cameras automatically insert
into image files. Our model represents this metadata by simply converting it to
text and then processing it with a transformer. The features that we learn
significantly outperform other self-supervised and supervised features on
downstream image forensics and calibration tasks. In particular, we
successfully localize spliced image regions "zero shot" by clustering the
visual embeddings for all of the patches within an image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Erasure of Unaligned Attributes from Neural Representations. (arXiv:2302.02997v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.02997">
<div class="article-summary-box-inner">
<span><p>We present the Assignment-Maximization Spectral Attribute removaL (AMSAL)
algorithm, which erases information from neural representations when the
information to be erased is implicit rather than directly being aligned to each
input example. Our algorithm works by alternating between two steps. In one, it
finds an assignment of the input representations to the information to be
erased, and in the other, it creates projections of both the input
representations and the information to be erased into a joint latent space. We
test our algorithm on an extensive array of datasets, including a Twitter
dataset with multiple guarded attributes, the BiasBios dataset and the
BiasBench benchmark. The last benchmark includes four datasets with various
types of protected attributes. Our results demonstrate that bias can often be
removed in our setup. We also discuss the limitations of our approach when
there is a strong entanglement between the main task and the information to be
erased.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reception Reader: Exploring Text Reuse in Early Modern British Publications. (arXiv:2302.04084v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04084">
<div class="article-summary-box-inner">
<span><p>The Reception Reader is a web tool for studying text reuse in the Early
English Books Online (EEBO-TCP) and Eighteenth Century Collections Online
(ECCO) data. Users can: 1) explore a visual overview of the reception of a
work, or its incoming connections, across time based on shared text segments,
2) interactively survey the details of connected documents, and 3) examine the
context of reused text for "close reading". We show examples of how the tool
streamlines research and exploration tasks, and discuss the utility and
limitations of the user interface along with its current data sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. (arXiv:2303.07142v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.07142">
<div class="article-summary-box-inner">
<span><p>This case study investigates the task of job classification in a real-world
setting, where the goal is to determine whether an English-language job posting
is appropriate for a graduate or entry-level position. We explore multiple
approaches to text classification, including supervised approaches such as
traditional models like Support Vector Machines (SVMs) and state-of-the-art
deep learning methods such as DeBERTa. We compare them with Large Language
Models (LLMs) used in both few-shot and zero-shot classification settings. To
accomplish this task, we employ prompt engineering, a technique that involves
designing prompts to guide the LLMs towards the desired output. Specifically,
we evaluate the performance of two commercially available state-of-the-art
GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also
conduct a detailed analysis of the impact of different aspects of prompt
engineering on the model's performance. Our results show that, with a
well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all
other models, achieving a 6% increase in Precision@95% Recall compared to the
best supervised approach. Furthermore, we observe that the wording of the
prompt is a critical factor in eliciting the appropriate "reasoning" in the
model, and that seemingly minor aspects of the prompt significantly affect the
model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feedback Effect in User Interaction with Intelligent Assistants: Delayed Engagement, Adaption and Drop-out. (arXiv:2303.10255v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10255">
<div class="article-summary-box-inner">
<span><p>With the growing popularity of intelligent assistants (IAs), evaluating IA
quality becomes an increasingly active field of research. This paper identifies
and quantifies the feedback effect, a novel component in IA-user interactions:
how the capabilities and limitations of the IA influence user behavior over
time. First, we demonstrate that unhelpful responses from the IA cause users to
delay or reduce subsequent interactions in the short term via an observational
study. Next, we expand the time horizon to examine behavior changes and show
that as users discover the limitations of the IA's understanding and functional
capabilities, they learn to adjust the scope and wording of their requests to
increase the likelihood of receiving a helpful response from the IA. Our
findings highlight the impact of the feedback effect at both the micro and meso
levels. We further discuss its macro-level consequences: unsatisfactory
interactions continuously reduce the likelihood and diversity of future user
engagements in a feedback loop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages. (arXiv:2303.12308v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12308">
<div class="article-summary-box-inner">
<span><p>Lack of encyclopedic text contributors, especially on Wikipedia, makes
automated text generation for low resource (LR) languages a critical problem.
Existing work on Wikipedia text generation has focused on English only where
English reference articles are summarized to generate English Wikipedia pages.
But, for low-resource languages, the scarcity of reference articles makes
monolingual summarization ineffective in solving this problem. Hence, in this
work, we propose XWikiGen, which is the task of cross-lingual multi-document
summarization of text from multiple reference articles, written in various
languages, to generate Wikipedia-style text. Accordingly, we contribute a
benchmark dataset, XWikiRef, spanning ~69K Wikipedia articles covering five
domains and eight languages. We harness this dataset to train a two-stage
system where the input is a set of citations and a section title and the output
is a section-specific LR summary. The proposed system is based on a novel idea
of neural unsupervised extractive summarization to coarsely identify salient
information followed by a neural abstractive model to generate the
section-specific text. Extensive experiments show that multi-domain training is
better than the multi-lingual setup on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12320">
<div class="article-summary-box-inner">
<span><p>Commonsense question-answering (QA) methods combine the power of pre-trained
Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A
typical approach collects nodes relevant to the QA pair from a KG to form a
Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).
This faces two major challenges: (i) it is difficult to capture all the
information from the QA in the WG, and (ii) the WG contains some irrelevant
nodes from the KG. To address these, we propose GrapeQA with two simple
improvements on the WG: (i) Prominent Entities for Graph Augmentation
identifies relevant text chunks from the QA pair and augments the WG with
corresponding latent representations from the LM, and (ii) Context-Aware Node
Pruning removes nodes that are less relevant to the QA pair. We evaluate our
results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows
consistent improvements over its LM + KG predecessor (QA-GNN in particular) and
large improvements on OpenBookQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.01016">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of improving the inference latency of
language model-based dense retrieval systems by introducing structural
compression and model size asymmetry between the context and query encoders.
First, we investigate the impact of pre and post-training compression on the
MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that
asymmetry in the dual encoders in dense retrieval can lead to improved
inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of
Embeddings (KALE), an efficient and accurate method for increasing the
inference efficiency of dense retrieval methods by pruning and aligning the
query encoder after training. Specifically, KALE extends traditional Knowledge
Distillation after bi-encoder training, allowing for effective query encoder
compression without full retraining or index generation. Using KALE and
asymmetric training, we can generate models which exceed the performance of
DistilBERT despite having 3x faster inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07438">
<div class="article-summary-box-inner">
<span><p>Despite the success of autoregressive large language models in text
generation, it remains a major challenge to generate text that satisfies
complex constraints: sampling from the conditional distribution
$\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical
constraints $\alpha$. To overcome this challenge, we propose to use tractable
probabilistic models to impose lexical constraints in autoregressive text
generation, which we refer to as GeLaTo. To demonstrate the effectiveness of
this framework, we use distilled hidden Markov models to control autoregressive
generation from GPT2. GeLaTo achieves state-of-the-art performance on
CommonGen, a challenging benchmark for constrained text generation, beating a
wide range of strong baselines by a large margin. Our work not only opens up
new avenues for controlling large language models but also motivates the
development of more expressive tractable probabilistic models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Canvas: End-to-End Kernel Architecture Search in Neural Networks. (arXiv:2304.07741v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07741">
<div class="article-summary-box-inner">
<span><p>The demands for higher performance and accuracy in neural networks (NNs)
never end. Existing tensor compilation and Neural Architecture Search (NAS)
techniques orthogonally optimize the two goals but actually share many
similarities in their concrete strategies. We exploit such opportunities by
combining the two into one and make a case for Kernel Architecture Search
(KAS). KAS reviews NAS from a system perspective and zooms into a more
fine-grained level to generate neural kernels with both high performance and
good accuracy. To demonstrate the potential of KAS, we build an end-to-end
framework, Canvas, to find high-quality kernels as convolution replacements.
Canvas samples from a rich set of fine-grained primitives to stochastically and
iteratively construct new kernels and evaluate them according to user-specified
constraints. Canvas supports freely adjustable tensor dimension sizes inside
the kernel and uses two levels of solvers to satisfy structural legality and
fully utilize model budgets. The evaluation shows that by replacing standard
convolutions with generated new kernels in common NNs, Canvas achieves average
1.5x speedups compared to the previous state-of-the-art with acceptable
accuracy loss and search efficiency. Canvas verifies the practicability of KAS
by rediscovering many manually designed kernels in the past and producing new
structures that may inspire future machine learning innovations. For source
code and implementation, we open-sourced Canvas at
https://github.com/tsinghua-ideal/Canvas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07869">
<div class="article-summary-box-inner">
<span><p>Neural Machine translation is a challenging task due to the inherent complex
nature and the fluidity that natural languages bring. Nonetheless, in recent
years, it has achieved state-of-the-art performance in several language pairs.
Although, a lot of traction can be seen in the areas of multilingual neural
machine translation (MNMT) in the recent years, there are no comprehensive
survey done to identify what approaches work well. The goal of this paper is to
investigate the realm of low resource languages and build a Neural Machine
Translation model to achieve state-of-the-art results. The paper looks to build
upon the mBART language model and explore strategies to augment it with various
NLP and Deep Learning techniques like back translation and transfer learning.
This implementation tries to unpack the architecture of the NMT application and
determine the different components which offers us opportunities to amend the
said application within the purview of the low resource languages problem
space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07987">
<div class="article-summary-box-inner">
<span><p>Instruction tuning is widely recognized as a key technique for building
generalist language models, which has attracted the attention of researchers
and the public with the release of InstructGPT~\citep{ouyang2022training} and
ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress
in English-oriented large-scale language models (LLMs), it is still
under-explored whether English-based foundation LLMs can perform similarly on
multilingual tasks compared to English tasks with well-designed instruction
tuning and how we can construct the corpora needed for the tuning.
</p>
<p>To remedy this gap, we propose the project as an attempt to create a Chinese
instruction dataset by various methods adapted to the intrinsic characteristics
of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples,
which have been manually checked to guarantee high quality. We also summarize
the existing English and Chinese instruction corpora and briefly describe some
potential applications of the newly constructed Chinese instruction corpora.
The resulting \textbf{C}hinese \textbf{O}pen \textbf{I}nstruction
\textbf{G}eneralist (\textbf{COIG}) corpora are available in
Huggingface\footnote{\url{https://huggingface.co/datasets/BAAI/COIG}} and
Github\footnote{\url{https://github.com/FlagOpen/FlagInstruct}}, and will be
continuously updated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model. (arXiv:2304.08109v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.08109">
<div class="article-summary-box-inner">
<span><p>Recently, the instruction-tuning of large language models is a crucial area
of research in the field of natural language processing. Due to resource and
cost limitations, several researchers have employed parameter-efficient tuning
techniques, such as LoRA, for instruction tuning, and have obtained encouraging
results In comparison to full-parameter fine-tuning, LoRA-based tuning
demonstrates salient benefits in terms of training costs. In this study, we
undertook experimental comparisons between full-parameter fine-tuning and
LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental
results show that the selection of the foundational model, training dataset
scale, learnable parameter quantity, and model training cost are all important
factors. We hope that the experimental conclusions of this paper can provide
inspiration for training large language models, especially in the field of
Chinese, and help researchers find a better trade-off strategy between training
cost and model performance. To facilitate the reproduction of the paper's
results, the dataset, model and code will be released.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-04-19 23:12:00.059264472 UTC">2023-04-19 23:12:00 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>