<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-02-24T01:30:00Z">02-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Lang2LTL: Translating Natural Language Commands to Temporal Robot Task Specification. (arXiv:2302.11649v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11649">
<div class="article-summary-box-inner">
<span><p>Natural language provides a powerful modality to program robots to perform
temporal tasks. Linear temporal logic (LTL) provides unambiguous semantics for
formal descriptions of temporal tasks. However, existing approaches cannot
accurately and robustly translate English sentences to their equivalent LTL
formulas in unseen environments. To address this problem, we propose Lang2LTL,
a novel modular system that leverages pretrained large language models to first
extract referring expressions from a natural language command, then ground the
expressions to real-world landmarks and objects, and finally translate the
command into an LTL task specification for the robot. It enables any robotic
system to interpret natural language navigation commands without additional
training, provided that it tracks its position and has a semantic map with
landmarks labeled with free-form text. We demonstrate the state-of-the-art
ability to generalize to multi-scale navigation domains such as OpenStreetMap
(OSM) and CleanUp World (a simulated household environment). Lang2LTL achieves
an average accuracy of 88.4% in translating challenging LTL formulas in 22
unseen OSM environments as evaluated on a new corpus of over 10,000 commands,
22 times better than the previous SoTA. Without modification, the best
performing Lang2LTL model on the OSM dataset can translate commands in CleanUp
World with 82.8% accuracy. As a part of our proposed comprehensive evaluation
procedures, we collected a new labeled dataset of English commands representing
2,125 unique LTL formulas, the largest ever dataset of natural language
commands to LTL specifications for robotic tasks with the most diverse LTL
formulas, 40 times more than previous largest dataset. Finally, we integrated
Lang2LTL with a planner to command a quadruped mobile robot to perform
multi-step navigational tasks in an analog real-world environment created in
the lab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11713">
<div class="article-summary-box-inner">
<span><p>Large language models have demonstrated an emergent capability in answering
knowledge intensive questions. With recent progress on web-scale visual and
language pre-training, do these models also understand how to answer visual
information seeking questions? To answer this question, we present InfoSeek, a
Visual Question Answering dataset that focuses on asking information-seeking
questions, where the information can not be answered by common sense knowledge.
We perform a multi-stage human annotation to collect a natural distribution of
high-quality visual information seeking question-answer pairs. We also
construct a large-scale, automatically collected dataset by combining existing
visual entity recognition datasets and Wikidata, which provides over one
million examples for model fine-tuning and validation. Based on InfoSeek, we
analyzed various pre-trained Visual QA systems to gain insights into the
characteristics of different pre-trained models. Our analysis shows that it is
challenging for the state-of-the-art multi-modal pre-trained models to answer
visual information seeking questions, but this capability is improved through
fine-tuning on the automated InfoSeek dataset. We hope our analysis paves the
way to understand and develop the next generation of multi-modal pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSP 2022 -- EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11752">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) is a challenging task of natural language
processing (NLP) and computer vision (CV), attracting significant attention
from researchers. English is a resource-rich language that has witnessed
various developments in datasets and models for visual question answering.
Visual question answering in other languages also would be developed for
resources and models. In addition, there is no multilingual dataset targeting
the visual content of a particular country with its own objects and cultural
characteristics. To address the weakness, we provide the research community
with a benchmark dataset named EVJVQA, including 33,000+ pairs of
question-answer over three languages: Vietnamese, English, and Japanese, on
approximately 5,000 images taken from Vietnam for evaluating multilingual VQA
systems or models. EVJVQA is used as a benchmark dataset for the challenge of
multilingual visual question answering at the 9th Workshop on Vietnamese
Language and Speech Processing (VLSP 2022). This task attracted 62 participant
teams from various universities and organizations. In this article, we present
details of the organization of the challenge, an overview of the methods
employed by shared-task participants, and the results. The highest performances
are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The
multilingual QA systems proposed by the top 2 teams use ViT for the pre-trained
vision model and mT5 for the pre-trained language model, a powerful pre-trained
language model based on the transformer architecture. EVJVQA is a challenging
dataset that motivates NLP and CV researchers to further explore the
multilingual models or systems for visual question answering systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUTANT: A Multi-sentential Code-mixed Hinglish Dataset. (arXiv:2302.11766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11766">
<div class="article-summary-box-inner">
<span><p>The multi-sentential long sequence textual data unfolds several interesting
research directions pertaining to natural language processing and generation.
Though we observe several high-quality long-sequence datasets for English and
other monolingual languages, there is no significant effort in building such
resources for code-mixed languages such as Hinglish (code-mixing of
Hindi-English). In this paper, we propose a novel task of identifying
multi-sentential code-mixed text (MCT) from multilingual articles. As a use
case, we leverage multilingual articles from two different data sources and
build a first-of-its-kind multi-sentential code-mixed Hinglish dataset i.e.,
MUTANT. We propose a token-level language-aware pipeline and extend the
existing metrics measuring the degree of code-mixing to a multi-sentential
framework and automatically identify MCT in the multilingual articles. The
MUTANT dataset comprises 67k articles with 85k identified Hinglish MCTs. To
facilitate future research, we make the publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Response Generation via Emotion Cause Transition Graph. (arXiv:2302.11787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11787">
<div class="article-summary-box-inner">
<span><p>Empathetic dialogue is a human-like behavior that requires the perception of
both affective factors (e.g., emotion status) and cognitive factors (e.g.,
cause of the emotion). Besides concerning emotion status in early work, the
latest approaches study emotion causes in empathetic dialogue. These approaches
focus on understanding and duplicating emotion causes in the context to show
empathy for the speaker. However, instead of only repeating the contextual
causes, the real empathic response often demonstrate a logical and
emotion-centered transition from the causes in the context to those in the
responses. In this work, we propose an emotion cause transition graph to
explicitly model the natural transition of emotion causes between two adjacent
turns in empathetic dialogue. With this graph, the concept words of the emotion
causes in the next turn can be predicted and used by a specifically designed
concept-aware decoder to generate the empathic response. Automatic and human
experimental results on the benchmark dataset demonstrate that our method
produces more empathetic, coherent, informative, and specific responses than
existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11799">
<div class="article-summary-box-inner">
<span><p>Knowledge-aware question answering (KAQA) requires the model to answer
questions over a knowledge base, which is essential for both open-domain QA and
domain-specific QA, especially when language models alone cannot provide all
the knowledge needed. Despite the promising result of recent KAQA systems which
tend to integrate linguistic knowledge from pre-trained language models (PLM)
and factual knowledge from knowledge graphs (KG) to answer complex questions, a
bottleneck exists in effectively fusing the representations from PLMs and KGs
because of (i) the semantic and distributional gaps between them, and (ii) the
difficulties in joint reasoning over the provided knowledge from both
modalities. To address the above two problems, we propose a Fine-grained
Two-stage training framework (FiTs) to boost the KAQA system performance: The
first stage aims at aligning representations from the PLM and the KG, thus
bridging the modality gaps between them, named knowledge adaptive
post-training. The second stage, called knowledge-aware fine-tuning, aims to
improve the model's joint reasoning ability based on the aligned
representations. In detail, we fine-tune the post-trained model via two
auxiliary self-supervised tasks in addition to the QA supervision. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA,
OpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers. (arXiv:2302.11812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11812">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformer models such as BERT have shown great success in a
wide range of applications, but at the cost of substantial increases in model
complexity. Quantization-aware training (QAT) is a promising method to lower
the implementation cost and energy consumption. However, aggressive
quantization below 2-bit causes considerable accuracy degradation due to
unstable convergence, especially when the downstream dataset is not abundant.
This work proposes a proactive knowledge distillation method called Teacher
Intervention (TI) for fast converging QAT of ultra-low precision pre-trained
Transformers. TI intervenes layer-wise signal propagation with the intact
signal from the teacher to remove the interference of propagated quantization
errors, smoothing loss surface of QAT and expediting the convergence.
Furthermore, we propose a gradual intervention mechanism to stabilize the
recovery of subsections of Transformer layers from quantization. The proposed
schemes enable fast convergence of QAT and improve the model accuracy
regardless of the diverse characteristics of downstream fine-tuning tasks. We
demonstrate that TI consistently achieves superior accuracy with significantly
lower fine-tuning iterations on well-known Transformers of natural language
processing as well as computer vision compared to the state-of-the-art QAT
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Knowledge Selection for Document Grounded Dialogs. (arXiv:2302.11849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11849">
<div class="article-summary-box-inner">
<span><p>Multi-document grounded dialogue systems (DGDS) belong to a class of
conversational agents that answer users' requests by finding supporting
knowledge from a collection of documents. Most previous studies aim to improve
the knowledge retrieval model or propose more effective ways to incorporate
external knowledge into a parametric generation model. These methods, however,
focus on retrieving knowledge from mono-granularity language units (e.g.
passages, sentences, or spans in documents), which is not enough to effectively
and efficiently capture precise knowledge in long documents. This paper
proposes Re3G, which aims to optimize both coarse-grained knowledge retrieval
and fine-grained knowledge extraction in a unified framework. Specifically, the
former efficiently finds relevant passages in a retrieval-and-reranking
process, whereas the latter effectively extracts finer-grain spans within those
passages to incorporate into a parametric answer generation model (BART, T5).
Experiments on DialDoc Shared Task demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Training of Mixture-of-Experts Language GANs. (arXiv:2302.11875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11875">
<div class="article-summary-box-inner">
<span><p>Despite the dramatic success in image generation, Generative Adversarial
Networks (GANs) still face great challenges in synthesizing sequences of
discrete elements, in particular human language. The difficulty in generator
training arises from the limited representation capacity and uninformative
learning signals obtained from the discriminator. In this work, we (1) first
empirically show that the mixture-of-experts approach is able to enhance the
representation capacity of the generator for language GANs and (2) harness the
Feature Statistics Alignment (FSA) paradigm to render fine-grained learning
signals to advance the generator training. Specifically, FSA forces the mean
statistics of the distribution of fake data to approach that of real samples as
close as possible in the finite-dimensional feature space. Empirical study on
synthetic and real benchmarks shows the superior performance in quantitative
evaluation and demonstrates the effectiveness of our approach to adversarial
text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Simplification via Large Language Models. (arXiv:2302.11957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11957">
<div class="article-summary-box-inner">
<span><p>Sentence Simplification aims to rephrase complex sentences into simpler
sentences while retaining original meaning. Large Language models (LLMs) have
demonstrated the ability to perform a variety of natural language processing
tasks. However, it is not yet known whether LLMs can be served as a
high-quality sentence simplification system. In this work, we empirically
analyze the zero-/few-shot learning ability of LLMs by evaluating them on a
number of benchmark test sets. Experimental results show LLMs outperform
state-of-the-art sentence simplification methods, and are judged to be on a par
with human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Deep Learning Learn to Abstract? A Systematic Probing Framework. (arXiv:2302.11978v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11978">
<div class="article-summary-box-inner">
<span><p>Abstraction is a desirable capability for deep learning models, which means
to induce abstract concepts from concrete instances and flexibly apply them
beyond the learning context. At the same time, there is a lack of clear
understanding about both the presence and further characteristics of this
capability in deep learning models. In this paper, we introduce a systematic
probing framework to explore the abstraction capability of deep learning models
from a transferability perspective. A set of controlled experiments are
conducted based on this framework, providing strong evidence that two probed
pre-trained language models (PLMs), T5 and GPT2, have the abstraction
capability. We also conduct in-depth analysis, thus shedding further light: (1)
the whole training phase exhibits a "memorize-then-abstract" two-stage process;
(2) the learned abstract concepts are gathered in a few middle-layer attention
heads, rather than being evenly distributed throughout the model; (3) the
probed abstraction capabilities exhibit robustness against concept mutations,
and are more robust to low-level/source-side mutations than
high-level/target-side ones; (4) generic pre-training is critical to the
emergence of abstraction capability, and PLMs exhibit better abstraction with
larger model sizes and data scales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metric-oriented Speech Enhancement using Diffusion Probabilistic Model. (arXiv:2302.11989v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11989">
<div class="article-summary-box-inner">
<span><p>Deep neural network based speech enhancement technique focuses on learning a
noisy-to-clean transformation supervised by paired training data. However, the
task-specific evaluation metric (e.g., PESQ) is usually non-differentiable and
can not be directly constructed in the training criteria. This mismatch between
the training objective and evaluation metric likely results in sub-optimal
performance. To alleviate it, we propose a metric-oriented speech enhancement
method (MOSE), which leverages the recent advances in the diffusion
probabilistic model and integrates a metric-oriented training strategy into its
reverse process. Specifically, we design an actor-critic based framework that
considers the evaluation metric as a posterior reward, thus guiding the reverse
process to the metric-increasing direction. The experimental results
demonstrate that MOSE obviously benefits from metric-oriented training and
surpasses the generative baselines in terms of all evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing in the Legal Domain. (arXiv:2302.12039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12039">
<div class="article-summary-box-inner">
<span><p>In this paper, we summarize the current state of the field of NLP &amp; Law with
a specific focus on recent technical and substantive developments. To support
our analysis, we construct and analyze a nearly complete corpus of more than
six hundred NLP &amp; Law related papers published over the past decade. Our
analysis highlights several major trends. Namely, we document an increasing
number of papers written, tasks undertaken, and languages covered over the
course of the past decade. We observe an increase in the sophistication of the
methods which researchers deployed in this applied context. Slowly but surely,
Legal NLP is beginning to match not only the methodological sophistication of
general NLP but also the professional standards of data availability and code
reproducibility observed within the broader scientific community. We believe
all of these trends bode well for the future of the field, but many questions
in both the academic and commercial sphere still remain open.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Agents and Children: Let Children Learn. (arXiv:2302.12043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12043">
<div class="article-summary-box-inner">
<span><p>Using online information discovery as a case study, in this position paper we
discuss the need to design, develop, and deploy (conversational) agents that
can -- non-intrusively -- guide children in their quest for online resources
rather than simply finding resources for them. We argue that agents should "let
children learn" and should be built to take on a teacher-facilitator function,
allowing children to develop their technical and critical thinking abilities as
they interact with varied technology in a broad range of use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Social Media for Early Detection of Depression in COVID-19 Patients. (arXiv:2302.12044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12044">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has caused substantial damage to global health. Even
though three years have passed, the world continues to struggle with the virus.
Concerns are growing about the impact of COVID-19 on the mental health of
infected individuals, who are more likely to experience depression, which can
have long-lasting consequences for both the affected individuals and the world.
Detection and intervention at an early stage can reduce the risk of depression
in COVID-19 patients. In this paper, we investigated the relationship between
COVID-19 infection and depression through social media analysis. Firstly, we
managed a dataset of COVID-19 patients that contains information about their
social media activity both before and after infection. Secondly,We conducted an
extensive analysis of this dataset to investigate the characteristic of
COVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep
neural network for early prediction of depression risk. This model considers
daily mood swings as a psychiatric signal and incorporates textual and
emotional characteristics via knowledge distillation. Experimental results
demonstrate that our proposed framework outperforms baselines in detecting
depression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has
the potential to enable public health organizations to initiate prompt
intervention with high-risk patients
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Sentiment Transfer via Adaptive Masking. (arXiv:2302.12045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12045">
<div class="article-summary-box-inner">
<span><p>Sentiment transfer aims at revising the input text to satisfy a given
sentiment polarity while retaining the original semantic content. The nucleus
of sentiment transfer lies in precisely separating the sentiment information
from the content information. Existing explicit approaches generally identify
and mask sentiment tokens simply based on prior linguistic knowledge and
manually-defined rules, leading to low generality and undesirable transfer
performance. In this paper, we view the positions to be masked as the learnable
parameters, and further propose a novel AM-ST model to learn adaptive
task-relevant masks based on the attention mechanism. Moreover, a
sentiment-aware masked language model is further proposed to fill in the blanks
in the masked positions by incorporating both context and sentiment polarity to
capture the multi-grained semantics comprehensively. AM-ST is thoroughly
evaluated on two popular datasets, and the experimental results demonstrate the
superiority of our proposal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Automatic Speech Recognition in an Incremental Setting. (arXiv:2302.12049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12049">
<div class="article-summary-box-inner">
<span><p>The increasing reliability of automatic speech recognition has proliferated
its everyday use. However, for research purposes, it is often unclear which
model one should choose for a task, particularly if there is a requirement for
speed as well as accuracy. In this paper, we systematically evaluate six speech
recognizers using metrics including word error rate, latency, and the number of
updates to already recognized words on English test data, as well as propose
and compare two methods for streaming audio into recognizers for incremental
recognition. We further propose Revokes per Second as a new metric for
evaluating incremental recognition and demonstrate that it provides insights
into overall model performance. We find that, generally, local recognizers are
faster and require fewer updates than cloud-based recognizers. Finally, we find
Meta's Wav2Vec model to be the fastest, and find Mozilla's DeepSpeech model to
be the most stable in its predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPINDLE: Spinning Raw Text into Lambda Terms with Graph Attention. (arXiv:2302.12050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12050">
<div class="article-summary-box-inner">
<span><p>This paper describes SPINDLE - an open source Python module implementing an
efficient and accurate parser for written Dutch that transforms raw text input
to programs for meaning composition, expressed as {\lambda} terms. The parser
integrates a number of breakthrough advances made in recent years. Its output
consists of hi-res derivations of a multimodal type-logical grammar, capturing
two orthogonal axes of syntax, namely deep function-argument structures and
dependency relations. These are produced by three interdependent systems: a
static type-checker asserting the well-formedness of grammatical analyses, a
state-of-the-art, structurally-aware supertagger based on heterogeneous graph
convolutions, and a massively parallel proof search component based on Sinkhorn
iterations. Packed in the software are also handy utilities and extras for
proof visualization and inference, intended to facilitate end-user utilization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12057">
<div class="article-summary-box-inner">
<span><p>We present ProsAudit, a benchmark in English to assess structural prosodic
knowledge in self-supervised learning (SSL) speech models. It consists of two
subtasks, their corresponding metrics, an evaluation dataset. In the
protosyntax task, the model must correctly identify strong versus weak prosodic
boundaries. In the lexical task, the model needs to correctly distinguish
between pauses inserted between words and within words. We also provide human
evaluation scores on this benchmark. We evaluated a series of SSL models and
found that they were all able to perform above chance on both tasks, even when
trained on an unseen language. However, non-native models performed
significantly worse than native ones on the lexical task, highlighting the
importance of lexical knowledge in this task. We also found a clear effect of
size with models trained on more data performing better in the two subtasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning model for Mongolian Citizens Feedback Analysis using Word Vector Embeddings. (arXiv:2302.12069v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12069">
<div class="article-summary-box-inner">
<span><p>A large amount of feedback was collected over the years. Many feedback
analysis models have been developed focusing on the English language.
Recognizing the concept of feedback is challenging and crucial in languages
which do not have applicable corpus and tools employed in Natural Language
Processing (i.e., vocabulary corpus, sentence structure rules, etc). However,
in this paper, we study a feedback classification in Mongolian language using
two different word embeddings for deep learning. We compare the results of
proposed approaches. We use feedback data in Cyrillic collected from 2012-2018.
The result indicates that word embeddings using their own dataset improve the
deep learning based proposed model with the best accuracy of 80.1% and 82.7%
for two classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12095">
<div class="article-summary-box-inner">
<span><p>ChatGPT is a recent chatbot service released by OpenAI and is receiving
increasing attention over the past few months. While evaluations of various
aspects of ChatGPT have been done, its robustness, i.e., the performance when
facing unexpected inputs, is still unclear to the public. Robustness is of
particular concern in responsible AI, especially for safety-critical
applications. In this paper, we conduct a thorough evaluation of the robustness
of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To
do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial
robustness and the Flipkart review and DDXPlus medical diagnosis datasets for
OOD evaluation. We select several popular foundation models as baselines.
Results show that ChatGPT does not show consistent advantages on adversarial
and OOD classification tasks, while performing well on translation tasks. This
suggests that adversarial and OOD robustness remains a significant threat to
foundation models. Moreover, ChatGPT shows astounding performance in
understanding dialogue-related texts and we find that it tends to provide
informal suggestions for medical tasks instead of definitive answers. Finally,
we present in-depth discussions of possible research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KHAN: Knowledge-Aware Hierarchical Attention Networks for Political Stance Prediction. (arXiv:2302.12126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12126">
<div class="article-summary-box-inner">
<span><p>The political stance prediction for news articles has been widely studied to
mitigate the echo chamber effect -- people fall into their thoughts and
reinforce their pre-existing beliefs. The previous works for the political
stance problem focus on (1) identifying political factors that could reflect
the political stance of a news article and (2) capturing those factors
effectively. Despite their empirical successes, they are not sufficiently
justified in terms of how effective their identified factors are in the
political stance prediction. Motivated by this, in this work, we conduct a user
study to investigate important factors in political stance prediction, and
observe that the context and tone of a news article (implicit) and external
knowledge for real-world entities appearing in the article (explicit) are
important in determining its political stance. Based on this observation, we
propose a novel knowledge-aware approach to political stance prediction (KHAN),
employing (1) hierarchical attention networks (HAN) to learn the relationships
among words and sentences in three different levels and (2) knowledge encoding
(KE) to incorporate external knowledge for real-world entities into the process
of political stance prediction. Also, to take into account the subtle and
important difference between opposite political stances, we build two
independent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by
ourselves and learn to fuse the different political knowledge. Through
extensive evaluations on three real-world datasets, we demonstrate the
superiority of DASH in terms of (1) accuracy, (2) efficiency, and (3)
effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Generalization Ability of Retrieval-Enhanced Transformers. (arXiv:2302.12128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12128">
<div class="article-summary-box-inner">
<span><p>Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown
that off-loading memory from trainable weights to a retrieval database can
significantly improve language modeling and match the performance of
non-retrieval models that are an order of magnitude larger in size. It has been
suggested that at least some of this performance gain is due to non-trivial
generalization based on both model weights and retrieval. In this paper, we try
to better understand the relative contributions of these two components. We
find that the performance gains from retrieval largely originate from
overlapping tokens between the database and the test data, suggesting less
non-trivial generalization than previously assumed. More generally, our results
point to the challenges of evaluating the generalization of retrieval-augmented
language models such as RETRO, as even limited token overlap may significantly
decrease test-time loss. We release our code and model at
https://github.com/TobiasNorlund/retro
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prosodic features improve sentence segmentation and parsing. (arXiv:2302.12165v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12165">
<div class="article-summary-box-inner">
<span><p>Parsing spoken dialogue presents challenges that parsing text does not,
including a lack of clear sentence boundaries. We know from previous work that
prosody helps in parsing single sentences (Tran et al. 2018), but we want to
show the effect of prosody on parsing speech that isn't segmented into
sentences. In experiments on the English Switchboard corpus, we find prosody
helps our model both with parsing and with accurately identifying sentence
boundaries. However, we find that the best-performing parser is not necessarily
the parser that produces the best sentence segmentation performance. We suggest
that the best parses instead come from modelling sentence boundaries jointly
with other constituent boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models. (arXiv:2302.12173v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12173">
<div class="article-summary-box-inner">
<span><p>We are currently witnessing dramatic advances in the capabilities of Large
Language Models (LLMs). They are already being adopted in practice and
integrated into many systems, including integrated development environments
(IDEs) and search engines. The functionalities of current LLMs can be modulated
via natural language prompts, while their exact internal functionality remains
implicit and unassessable. This property, which makes them adaptable to even
unseen tasks, might also make them susceptible to targeted adversarial
prompting. Recently, several ways to misalign LLMs using Prompt Injection (PI)
attacks have been introduced. In such attacks, an adversary can prompt the LLM
to produce malicious content or override the original instructions and the
employed filtering schemes. Recent work showed that these attacks are hard to
mitigate, as state-of-the-art LLMs are instruction-following. So far, these
attacks assumed that the adversary is directly prompting the LLM.
</p>
<p>In this work, we show that augmenting LLMs with retrieval and API calling
capabilities (so-called Application-Integrated LLMs) induces a whole new set of
attack vectors. These LLMs might process poisoned content retrieved from the
Web that contains malicious prompts pre-injected and selected by adversaries.
We demonstrate that an attacker can indirectly perform such PI attacks. Based
on this key insight, we systematically analyze the resulting threat landscape
of Application-Integrated LLMs and discuss a variety of new attack vectors. To
demonstrate the practical viability of our attacks, we implemented specific
demonstrations of the proposed attacks within synthetic applications. In
summary, our work calls for an urgent evaluation of current mitigation
techniques and an investigation of whether new techniques are needed to defend
LLMs against these threats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Scalable Nearest Neighbor Machine Translation. (arXiv:2302.12188v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12188">
<div class="article-summary-box-inner">
<span><p>$k$NN-MT is a straightforward yet powerful approach for fast domain
adaptation, which directly plugs pre-trained neural machine translation (NMT)
models with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval
to achieve domain adaptation without retraining. Despite being conceptually
attractive, $k$NN-MT is burdened with massive storage requirements and high
computational complexity since it conducts nearest neighbor searches over the
entire reference corpus. In this paper, we propose a simple and scalable
nearest neighbor machine translation framework to drastically promote the
decoding and storage efficiency of $k$NN-based models while maintaining the
translation performance. To this end, we dynamically construct an extremely
small datastore for each input via sentence-level retrieval to avoid searching
the entire datastore in vanilla $k$NN-MT, based on which we further introduce a
distance-aware adapter to adaptively incorporate the $k$NN retrieval results
into the pre-trained NMT models. Experiments on machine translation in two
general settings, static domain adaptation and online learning, demonstrate
that our proposed approach not only achieves almost 90% speed as the NMT model
without performance degradation, but also significantly reduces the storage
requirements of $k$NN-MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HL Dataset: Grounding High-Level Linguistic Concepts in Vision. (arXiv:2302.12189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12189">
<div class="article-summary-box-inner">
<span><p>Current captioning datasets, focus on object-centric captions, describing the
visible objects in the image, often ending up stating the obvious (for humans),
e.g. "people eating food in a park". Although these datasets are useful to
evaluate the ability of Vision &amp; Language models to recognize the visual
content, they lack in expressing trivial abstract concepts, e.g. "people having
a picnic". Such concepts are licensed by human's personal experience and
contribute to forming common sense assumptions. We present the High-Level
Dataset; a dataset extending 14997 images of the COCO dataset with 134973
human-annotated (high-level) abstract captions collected along three axes:
scenes, actions and rationales. We describe and release such dataset and we
show how it can be used to assess models' multimodal grounding of abstract
concepts and enrich models' visio-lingusitic representations. Moreover, we
describe potential tasks enabled by this dataset involving high- and low-level
concepts interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media. (arXiv:2302.12190v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12190">
<div class="article-summary-box-inner">
<span><p>The widespread availability of internet access and handheld devices confers
to social media a power similar to the one newspapers used to have. People seek
affordable information on social media and can reach it within seconds. Yet
this convenience comes with dangers; any user may freely post whatever they
please and the content can stay online for a long period, regardless of its
truthfulness. A need to detect untruthful information, also known as fake news,
arises. In this paper, we present an end-to-end solution that accurately
detects fake news and immunizes network nodes that spread them in real-time. To
detect fake news, we propose two new stack deep learning architectures that
utilize convolutional and bidirectional LSTM layers. To mitigate the spread of
fake news, we propose a real-time network-aware strategy that (1) constructs a
minimum-cost weighted directed spanning tree for a detected node, and (2)
immunizes nodes in that tree by scoring their harmfulness using a novel ranking
function. We demonstrate the effectiveness of our solution on five real-world
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12200">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) models capable of Continual Learning (CL) are
realistically valuable in areas where entity types continuously increase (e.g.,
personal assistants). Meanwhile the learning paradigm of NER advances to new
patterns such as the span-based methods. However, its potential to CL has not
been fully explored. In this paper, we propose SpanKL1, a simple yet effective
Span-based model with Knowledge distillation (KD) to preserve memories and
multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence
labeling approaches, the inherently independent modeling in span and entity
level with the designed coherent optimization on SpanKL promotes its learning
at each incremental step and mitigates the forgetting. Experiments on synthetic
CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly
outperforms previous SoTA in many aspects, and obtains the smallest gap from CL
to the upper bound revealing its high practiced value.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Nearest Neighbor Machine Translation. (arXiv:2302.12211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12211">
<div class="article-summary-box-inner">
<span><p>To protect user privacy and meet legal regulations, federated learning (FL)
is attracting significant attention. Training neural machine translation (NMT)
models with traditional FL algorithm (e.g., FedAvg) typically relies on
multi-round model-based interactions. However, it is impractical and
inefficient for machine translation tasks due to the vast communication
overheads and heavy synchronization. In this paper, we propose a novel
federated nearest neighbor (FedNN) machine translation framework that, instead
of multi-round model-based interactions, leverages one-round memorization-based
interaction to share knowledge across different clients to build low-overhead
privacy-preserving systems. The whole approach equips the public NMT model
trained on large-scale accessible data with a $k$-nearest-neighbor ($$kNN)
classifier and integrates the external datastore constructed by private text
data in all clients to form the final FL model. A two-phase datastore
encryption strategy is introduced to achieve privacy-preserving during this
process. Extensive experiments show that FedNN significantly reduces
computational and communication costs compared with FedAvg, while maintaining
promising performance in different FL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What makes a language easy to deep-learn?. (arXiv:2302.12239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12239">
<div class="article-summary-box-inner">
<span><p>Neural networks drive the success of natural language processing. A
fundamental property of natural languages is their compositional structure,
allowing us to describe new meanings systematically. However, neural networks
notoriously struggle with systematic generalization and do not necessarily
benefit from a compositional structure in emergent communication simulations.
Here, we test how neural networks compare to humans in learning and
generalizing a new language. We do this by closely replicating an artificial
language learning study (conducted originally with human participants) and
evaluating the memorization and generalization capabilities of deep neural
networks with respect to the degree of structure in the input language. Our
results show striking similarities between humans and deep neural networks:
More structured linguistic input leads to more systematic generalization and
better convergence between humans and neural network agents and between
different neural agents. We then replicate this structure bias found in humans
and our recurrent neural networks with a Transformer-based large language model
(GPT-3), showing a similar benefit for structured linguistic input regarding
generalization systematicity and memorization errors. These findings show that
the underlying structure of languages is crucial for systematic generalization.
Due to the correlation between community size and linguistic structure in
natural languages, our findings underscore the challenge of automated
processing of low-resource languages. Nevertheless, the similarity between
humans and machines opens new avenues for language evolution research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12246">
<div class="article-summary-box-inner">
<span><p>The increasing scale of large language models (LLMs) brings emergent
abilities to various complex tasks requiring reasoning, such as arithmetic and
commonsense reasoning. It is known that the effective design of task-specific
prompts is critical for LLMs' ability to produce high-quality answers. In
particular, an effective approach for complex question-and-answer tasks is
example-based prompting with chain-of-thought (CoT) reasoning, which
significantly improves the performance of LLMs. However, current CoT methods
rely on a fixed set of human-annotated exemplars, which are not necessarily the
most effective examples for different tasks. This paper proposes a new method,
Active-Prompt, to adapt LLMs to different tasks with task-specific example
prompts (annotated with human-designed CoT reasoning). For this purpose, we
propose a solution to the key problem of determining which questions are the
most important and helpful ones to annotate from a pool of task-specific
queries. By borrowing ideas from the related problem of uncertainty-based
active learning, we introduce several metrics to characterize the uncertainty
so as to select the most uncertain questions for annotation. Experimental
results demonstrate the superiority of our proposed method, achieving
state-of-the-art on eight complex reasoning tasks. Further analyses of
different uncertainty metrics, pool sizes, zero-shot learning, and
accuracy-uncertainty relationship demonstrate the effectiveness of our method.
Our code will be available at https://github.com/shizhediao/active-cot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying & Modeling Feature Interactions: An Information Decomposition Framework. (arXiv:2302.12247v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.12247">
<div class="article-summary-box-inner">
<span><p>The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different signals. Despite these empirical advances, there
remain fundamental research questions: how can we quantify the nature of
interactions that exist among input features? Subsequently, how can we capture
these interactions using suitable data-driven methods? To answer this question,
we propose an information-theoretic approach to quantify the degree of
redundancy, uniqueness, and synergy across input features, which we term the
PID statistics of a multimodal distribution. Using 2 newly proposed estimators
that scale to high-dimensional distributions, we demonstrate their usefulness
in quantifying the interactions within multimodal datasets, the nature of
interactions captured by multimodal models, and principled approaches for model
selection. We conduct extensive experiments on both synthetic datasets where
the PID statistics are known and on large-scale multimodal benchmarks where PID
estimation was previously impossible. Finally, to demonstrate the real-world
applicability of our approach, we present three case studies in pathology, mood
prediction, and robotic perception where our framework accurately recommends
strong multimodal models for each application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08927">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) aims to determine the logical relationship
between two sentences, such as Entailment, Contradiction, and Neutral. In
recent years, deep learning models have become a prevailing approach to NLI,
but they lack interpretability and explainability. In this work, we address the
explainability of NLI by weakly supervised logical reasoning, and propose an
Explainable Phrasal Reasoning (EPR) approach. Our model first detects phrases
as the semantic unit and aligns corresponding phrases in the two sentences.
Then, the model predicts the NLI label for the aligned phrases, and induces the
sentence label by fuzzy logic formulas. Our EPR is almost everywhere
differentiable and thus the system can be trained end to end. In this way, we
are able to provide explicit explanations of phrasal logical relationships in a
weakly supervised manner. We further show that such reasoning results help
textual explanation generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black-box Prompt Learning for Pre-trained Language Models. (arXiv:2201.08531v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08531">
<div class="article-summary-box-inner">
<span><p>The increasing scale of general-purpose Pre-trained Language Models (PLMs)
necessitates the study of more efficient adaptation across different downstream
tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL)
to resonate with pragmatic interactions between the cloud infrastructure and
edge devices. Particularly, instead of fine-tuning the model in the cloud, we
adapt PLMs by prompt learning, which efficiently optimizes only a few
parameters of the discrete prompts. Moreover, we consider the scenario that we
do not have access to the parameters and gradients of the pre-trained models,
except for its outputs given inputs. This black-box setting secures the cloud
infrastructure from potential attack and misuse to cause a single-point
failure, which is preferable to the white-box counterpart by current
infrastructures. Under this black-box constraint, we apply a variance-reduced
policy gradient algorithm to estimate the gradients of parameters in the
categorical distribution of each discrete prompt. In light of our method, the
user devices can efficiently tune their tasks by querying the PLMs bounded by a
range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the
proposed algorithm achieves significant improvement on eight benchmarks in a
cloud-device collaboration manner. Finally, we conduct in-depth case studies to
comprehensively analyze our method in terms of various data sizes, prompt
lengths, training budgets, optimization objectives, prompt transferability, and
explanations of the learned prompts. Our code will be available at
https://github.com/shizhediao/Black-Box-Prompt-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Bias in Text: Labeled Datasets and Lexicons. (arXiv:2201.08675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08675">
<div class="article-summary-box-inner">
<span><p>Language has a profound impact on our thoughts, perceptions, and conceptions
of gender roles. Gender-inclusive language is, therefore, a key tool to promote
social inclusion and contribute to achieving gender equality. Consequently,
detecting and mitigating gender bias in texts is instrumental in halting its
propagation and societal implications. However, there is a lack of gender bias
datasets and lexicons for automating the detection of gender bias using
supervised and unsupervised machine learning (ML) and natural language
processing (NLP) techniques. Therefore, the main contribution of this work is
to publicly provide labeled datasets and exhaustive lexicons by collecting,
annotating, and augmenting relevant sentences to facilitate the detection of
gender bias in English text. Towards this end, we present an updated version of
our previously proposed taxonomy by re-formalizing its structure, adding a new
bias type, and mapping each bias subtype to an appropriate detection
methodology. The released datasets and lexicons span multiple bias subtypes
including: Generic He, Generic She, Explicit Marking of Sex, and Gendered
Neologisms. We leveraged the use of word embedding models to further augment
the collected lexicons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Words are all you need? Language as an approximation for human similarity judgments. (arXiv:2206.04105v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04105">
<div class="article-summary-box-inner">
<span><p>Human similarity judgments are a powerful supervision signal for machine
learning applications based on techniques such as contrastive learning,
information retrieval, and model alignment, but classical methods for
collecting human similarity judgments are too expensive to be used at scale.
Recent methods propose using pre-trained deep neural networks (DNNs) to
approximate human similarity, but pre-trained DNNs may not be available for
certain domains (e.g., medical images, low-resource languages) and their
performance in approximating human similarity has not been extensively tested.
We conducted an evaluation of 611 pre-trained models across three domains --
images, audio, video -- and found that there is a large gap in performance
between human similarity judgments and pre-trained DNNs. To address this gap,
we propose a new class of similarity approximation methods based on language.
To collect the language data required by these new methods, we also developed
and validated a novel adaptive tag collection pipeline. We find that our
proposed language-based methods are significantly cheaper, in the number of
human judgments, than classical methods, but still improve performance over the
DNN-based methods. Finally, we also develop `stacked' methods that combine
language embeddings with DNN embeddings, and find that these consistently
provide the best approximations for human similarity across all three of our
modalities. Based on the results of this comprehensive study, we provide a
concise guide for researchers interested in collecting or approximating human
similarity data. To accompany this guide, we also release all of the similarity
and language data, a total of 206,339 human judgments, that we collected in our
experiments, along with a detailed breakdown of all modeling results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BaIT: Barometer for Information Trustworthiness. (arXiv:2206.07535v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07535">
<div class="article-summary-box-inner">
<span><p>This paper presents a new approach to the FNC-1 fake news classification task
which involves employing pre-trained encoder models from similar NLP tasks,
namely sentence similarity and natural language inference, and two neural
network architectures using this approach are proposed. Methods in data
augmentation are explored as a means of tackling class imbalance in the
dataset, employing common pre-existing methods and proposing a method for
sample generation in the under-represented class using a novel sentence
negation algorithm. Comparable overall performance with existing baselines is
achieved, while significantly increasing accuracy on an under-represented but
nonetheless important class for FNC-1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamically Retrieving Knowledge via Query Generation for Informative Dialogue Generation. (arXiv:2208.00128v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.00128">
<div class="article-summary-box-inner">
<span><p>Knowledge-driven dialog system has recently made remarkable breakthroughs.
Compared with general dialog systems, superior knowledge-driven dialog systems
can generate more informative and knowledgeable responses with pre-provided
knowledge. However, in practical applications, the dialog system cannot be
provided with corresponding knowledge in advance because it cannot know in
advance the development of the conversation. Therefore, in order to make the
knowledge dialogue system more practical, it is vital to find a way to retrieve
relevant knowledge based on the dialogue history. To solve this problem, we
design a knowledge-driven dialog system named DRKQG (Dynamically Retrieving
Knowledge via Query Generation for informative dialog response). Specifically,
the system can be divided into two modules: the query generation module and the
dialog generation module. First, a time-aware mechanism is utilized to capture
context information, and a query can be generated for retrieving knowledge
through search engine. Then, we integrate the copy mechanism and transformers,
which allows the response generation module to produce responses derived from
the context and retrieved knowledge. Experimental results at LIC2022, Language
and Intelligence Technology Competition, show that our module outperforms the
baseline model by a large margin on automatic evaluation metrics, while human
evaluation by the Baidu Linguistics team shows that our system achieves
impressive results in Factually Correct and Knowledgeable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment Analysis. (arXiv:2208.01368v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.01368">
<div class="article-summary-box-inner">
<span><p>The advancement of aspect-based sentiment analysis (ABSA) has urged the lack
of a user-friendly framework that can largely lower the difficulty of
reproducing state-of-the-art ABSA performance, especially for beginners. To
meet the demand, we present \our, a modularized framework built on PyTorch for
reproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA
subtasks, including aspect term extraction, aspect sentiment classification,
and end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates
29 models and 26 datasets. With just a few lines of code, the result of a model
on a specific dataset can be reproduced. With a modularized design, PyABSA can
also be flexiblely extended to considered models, datasets, and other related
tasks. Besides, PyABSA highlights its data augmentation and annotation
features, which significantly address data scarity. All are welcome to have a
try at \url{https://github.com/yangheng95/PyABSA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning. (arXiv:2208.14565v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.14565">
<div class="article-summary-box-inner">
<span><p>We present a bi-encoder framework for named entity recognition (NER), which
applies contrastive learning to map candidate text spans and entity types into
the same vector representation space. Prior work predominantly approaches NER
as sequence labeling or span classification. We instead frame NER as a
representation learning problem that maximizes the similarity between the
vector representations of an entity mention and its type. This makes it easy to
handle nested and flat NER alike, and can better leverage noisy
self-supervision signals. A major challenge to this bi-encoder formulation for
NER lies in separating non-entity spans from entity mentions. Instead of
explicitly labeling all non-entity spans as the same class $\texttt{Outside}$
($\texttt{O}$) as in most prior methods, we introduce a novel dynamic
thresholding loss. Experiments show that our method performs well in both
supervised and distantly supervised settings, for nested and flat NER alike,
establishing new state of the art across standard datasets in the general
domain (e.g., ACE2004, ACE2005) and high-value verticals such as biomedicine
(e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at
github.com/microsoft/binder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting. (arXiv:2210.05404v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05404">
<div class="article-summary-box-inner">
<span><p>This paper presents work on novel machine translation (MT) systems between
spoken and signed languages, where signed languages are represented in
SignWriting, a sign language writing system. Our work seeks to address the lack
of out-of-the-box support for signed languages in current MT systems and is
based on the SignBank dataset, which contains pairs of spoken language text and
SignWriting content. We introduce novel methods to parse, factorize, decode,
and evaluate SignWriting, leveraging ideas from neural factored MT. In a
bilingual setup--translating from American Sign Language to (American)
English--our method achieves over 30 BLEU, while in two multilingual
setups--translating in both directions between spoken languages and signed
languages--we achieve over 20 BLEU. We find that common MT techniques used to
improve spoken language translation similarly affect the performance of sign
language translation. These findings validate our use of an intermediate text
representation for signed languages to include them in natural language
processing research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13623">
<div class="article-summary-box-inner">
<span><p>In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Converge to the Truth: Factual Error Correction via Iterative Constrained Editing. (arXiv:2211.12130v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12130">
<div class="article-summary-box-inner">
<span><p>Given a possibly false claim sentence, how can we automatically correct it
with minimal editing? Existing methods either require a large number of pairs
of false and corrected claims for supervised training or do not handle well
errors spanning over multiple tokens within an utterance. In this paper, we
propose VENCE, a novel method for factual error correction (FEC) with minimal
edits. VENCE formulates the FEC problem as iterative sampling editing actions
with respect to a target density function. We carefully design the target
function with predicted truthfulness scores from an offline trained fact
verification model. VENCE samples the most probable editing positions based on
back-calculated gradients of the truthfulness score concerning input tokens and
the editing actions using a distantly-supervised language model (T5).
Experiments on a public dataset show that VENCE improves the well-adopted SARI
metric by 5.3 (or a relative improvement of 11.8%) over the previous best
distantly-supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and accurate factorized neural transducer for text adaption of end-to-end speech recognition models. (arXiv:2212.01992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01992">
<div class="article-summary-box-inner">
<span><p>Neural transducer is now the most popular end-to-end model for speech
recognition, due to its naturally streaming ability. However, it is challenging
to adapt it with text-only data. Factorized neural transducer (FNT) model was
proposed to mitigate this problem. The improved adaptation ability of FNT on
text-only adaptation data came at the cost of lowered accuracy compared to the
standard neural transducer model. We propose several methods to improve the
performance of the FNT model. They are: adding CTC criterion during training,
adding KL divergence loss during adaptation, using a pre-trained language model
to seed the vocabulary predictor, and an efficient adaptation approach by
interpolating the vocabulary predictor with the n-gram language model. A
combination of these approaches results in a relative word-error-rate reduction
of 9.48\% from the standard FNT model. Furthermore, n-gram interpolation with
the vocabulary predictor improves the adaptation speed hugely with satisfactory
adaptation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.03494">
<div class="article-summary-box-inner">
<span><p>Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KG-ECO: Knowledge Graph Enhanced Entity Correction for Query Rewriting. (arXiv:2302.10454v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.10454">
<div class="article-summary-box-inner">
<span><p>Query Rewriting (QR) plays a critical role in large-scale dialogue systems
for reducing frictions. When there is an entity error, it imposes extra
challenges for a dialogue system to produce satisfactory responses. In this
work, we propose KG-ECO: Knowledge Graph enhanced Entity COrrection for query
rewriting, an entity correction system with corrupt entity span detection and
entity retrieval/re-ranking functionalities. To boost the model performance, we
incorporate Knowledge Graph (KG) to provide entity structural information
(neighboring entities encoded by graph neural networks) and textual information
(KG entity descriptions encoded by RoBERTa). Experimental results show that our
approach yields a clear performance gain over two baselines: utterance level QR
and entity correction without utilizing KG information. The proposed system is
particularly effective for few-shot learning cases where target entities are
rarely seen in training or there is a KG relation between the target entity and
other contextual entities in the query.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Subword Pooling Strategy on Cross-lingual Event Detection. (arXiv:2302.11365v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.11365">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models (e.g., mBERT, XLM-RoBERTa) have
significantly advanced the state-of-the-art for zero-shot cross-lingual
information extraction. These language models ubiquitously rely on word
segmentation techniques that break a word into smaller constituent subwords.
Therefore, all word labeling tasks (e.g. named entity recognition, event
detection, etc.), necessitate a pooling strategy that takes the subword
representations as input and outputs a representation for the entire word.
Taking the task of cross-lingual event detection as a motivating example, we
show that the choice of pooling strategy can have a significant impact on the
target language performance. For example, the performance varies by up to 16
absolute $f_{1}$ points depending on the pooling strategy when training in
English and testing in Arabic on the ACE task. We carry out our analysis with
five different pooling strategies across nine languages in diverse
multi-lingual datasets. Across configurations, we find that the canonical
strategy of taking just the first subword to represent the entire word is
usually sub-optimal. On the other hand, we show that attention pooling is
robust to language and dataset variations by being either the best or close to
the optimal strategy. For reproducibility, we make our code available at
https://github.com/isi-boston/ed-pooling.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-02-24 23:12:58.195057148 UTC">2023-02-24 23:12:58 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>