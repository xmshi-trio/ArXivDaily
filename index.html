<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-11-24T01:30:00Z">11-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Is the Elephant Flying? Resolving Ambiguities in Text-to-Image Generative Models. (arXiv:2211.12503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12503">
<div class="article-summary-box-inner">
<span><p>Natural language often contains ambiguities that can lead to
misinterpretation and miscommunication. While humans can handle ambiguities
effectively by asking clarifying questions and/or relying on contextual cues
and common-sense knowledge, resolving ambiguities can be notoriously hard for
machines. In this work, we study ambiguities that arise in text-to-image
generative models. We curate a benchmark dataset covering different types of
ambiguities that occur in these systems. We then propose a framework to
mitigate ambiguities in the prompts given to the systems by soliciting
clarifications from the user. Through automatic and human evaluations, we show
the effectiveness of our framework in generating more faithful images aligned
with human intention in the presence of ambiguities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying gender bias in blockbuster movies through the lens of machine learning. (arXiv:2211.12504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12504">
<div class="article-summary-box-inner">
<span><p>The problem of gender bias is highly prevalent and well known. In this paper,
we have analysed the portrayal of gender roles in English movies, a medium that
effectively influences society in shaping people's beliefs and opinions. First,
we gathered scripts of films from different genres and derived sentiments and
emotions using natural language processing techniques. Afterwards, we converted
the scripts into embeddings, i.e. a way of representing text in the form of
vectors. With a thorough investigation, we found specific patterns in male and
female characters' personality traits in movies that align with societal
stereotypes. Furthermore, we used mathematical and machine learning techniques
and found some biases wherein men are shown to be more dominant and envious
than women, whereas women have more joyful roles in movies. In our work, we
introduce, to the best of our knowledge, a novel technique to convert dialogues
into an array of emotions by combining it with Plutchik's wheel of emotions.
Our study aims to encourage reflections on gender equality in the domain of
film and facilitate other researchers in analysing movies automatically instead
of using manual approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-Aware Datasets are Adaptive Knowledgebases for the New Normal. (arXiv:2211.12508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12508">
<div class="article-summary-box-inner">
<span><p>Recent advances in text classification and knowledge capture in language
models have relied on availability of large-scale text datasets. However,
language models are trained on static snapshots of knowledge and are limited
when that knowledge evolves. This is especially critical for misinformation
detection, where new types of misinformation continuously appear, replacing old
campaigns. We propose time-aware misinformation datasets to capture
time-critical phenomena. In this paper, we first present evidence of evolving
misinformation and show that incorporating even simple time-awareness
significantly improves classifier accuracy. Second, we present COVID-TAD, a
large-scale COVID-19 misinformation da-taset spanning 25 months. It is the
first large-scale misinformation dataset that contains multiple snapshots of a
datastream and is orders of magnitude bigger than related misinformation
datasets. We describe the collection and labeling pro-cess, as well as
preliminary experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP meets psychotherapy: Using predicted client emotions and self-reported client emotions to measure emotional coherence. (arXiv:2211.12512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12512">
<div class="article-summary-box-inner">
<span><p>Emotions are experienced and expressed through various response systems.
Coherence between emotional experience and emotional expression is considered
important to clients' well being. To date, emotional coherence (EC) has been
studied at a single time point using lab-based tasks with relatively small
datasets. No study has examined EC between the subjective experience of
emotions and emotion expression in therapy or whether this coherence is
associated with clients' well being. Natural language Processing (NLP)
approaches have been applied to identify emotions from psychotherapy dialogue,
which can be implemented to study emotional processes on a larger scale.
However, these methods have yet to be used to study coherence between emotional
experience and emotional expression over the course of therapy and whether it
relates to clients' well-being. This work presents an end-to-end approach where
we use emotion predictions from our transformer based emotion recognition model
to study emotional coherence and its diagnostic potential in psychotherapy
research. We first employ our transformer based approach on a Hebrew
psychotherapy dataset to automatically label clients' emotions at utterance
level in psychotherapy dialogues. We subsequently investigate the emotional
coherence between clients' self-reported emotional states and our model-based
emotion predictions. We also examine the association between emotional
coherence and clients' well being. Our findings indicate a significant
correlation between clients' self-reported emotions and positive and negative
emotions expressed verbally during psychotherapy sessions. Coherence in
positive emotions was also highly correlated with clients well-being. These
results illustrate how NLP can be applied to identify important emotional
processes in psychotherapy to improve diagnosis and treatment for clients
suffering from mental-health problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Agriculture : A Novel Multilevel Approach for Agricultural Risk Assessment over Unstructured Data. (arXiv:2211.12515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12515">
<div class="article-summary-box-inner">
<span><p>Detecting opportunities and threats from massive text data is a challenging
task for most. Traditionally, companies would rely mainly on structured data to
detect and predict risks, losing a huge amount of information that could be
extracted from unstructured text data. Fortunately, artificial intelligence
came to remedy this issue by innovating in data extraction and processing
techniques, allowing us to understand and make use of Natural Language data and
turning it into structures that a machine can process and extract insight from.
Uncertainty refers to a state of not knowing what will happen in the future.
This paper aims to leverage natural language processing and machine learning
techniques to model uncertainties and evaluate the risk level in each
uncertainty cluster using massive text data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12561">
<div class="article-summary-box-inner">
<span><p>Recent multimodal models such as DALL-E and CM3 have achieved remarkable
progress in text-to-image and image-to-text generation. However, these models
store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the
model parameters, requiring increasingly larger models and training data to
capture more knowledge. To integrate knowledge in a more scalable and modular
way, we propose a retrieval-augmented multimodal model, which enables a base
multimodal model (generator) to refer to relevant knowledge fetched by a
retriever from external memory (e.g., multimodal documents on the web).
Specifically, we implement a retriever using the pretrained CLIP model and a
generator using the CM3 Transformer architecture, and train this model using
the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3),
is the first multimodal model that can retrieve and generate mixtures of text
and images. We show that RA-CM3 significantly outperforms baseline multimodal
models such as DALL-E and CM3 on both image and caption generation tasks (12
FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute
for training (&lt;30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel
capabilities such as knowledge-intensive image generation and multimodal
in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the Type and Target of Offensive Social Media Posts in Marathi. (arXiv:2211.12570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12570">
<div class="article-summary-box-inner">
<span><p>The presence of offensive language on social media is very common motivating
platforms to invest in strategies to make communities safer. This includes
developing robust machine learning systems capable of recognizing offensive
content online. Apart from a few notable exceptions, most research on automatic
offensive language identification has dealt with English and a few other high
resource languages such as French, German, and Spanish. In this paper we
address this gap by tackling offensive language identification in Marathi, a
low-resource Indo-Aryan language spoken in India. We introduce the Marathi
Offensive Language Dataset v.2.0 or MOLD 2.0 and present multiple experiments
on this dataset. MOLD 2.0 is a much larger version of MOLD with expanded
annotation to the levels B (type) and C (target) of the popular OLID taxonomy.
MOLD 2.0 is the first hierarchical offensive language dataset compiled for
Marathi, thus opening new avenues for research in low-resource Indo-Aryan
languages. Finally, we also introduce SeMOLD, a larger dataset annotated
following the semi-supervised methods presented in SOLID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12588">
<div class="article-summary-box-inner">
<span><p>Recently, there has been significant progress in teaching language models to
perform step-by-step reasoning to solve complex numerical reasoning tasks.
Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these
tasks. CoT uses language models to perform both reasoning and computation in
the multi-step `thought' process. To disentangle computation from reasoning, we
propose `Program of Thoughts' (PoT), which uses language models (mainly Codex)
to express the reasoning process as a program. The computation is relegated to
an external computer, which executes the generated programs to derive the
answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,
TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)
for both few-shot and zero-shot setups. Under both few-shot and zero-shot
settings, PoT can show an average performance gain over CoT by around 12\%
across all the evaluated datasets. By combining PoT with self-consistency
decoding, we can achieve SoTA performance on all math problem datasets and
near-SoTA performance on financial datasets. All of our data and code are
released in
Github\footnote{\url{https://github.com/wenhuchen/Program-of-Thoughts}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoReply: Detecting Nonsense in Dialogue Introspectively with Discriminative Replies. (arXiv:2211.12615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12615">
<div class="article-summary-box-inner">
<span><p>Existing approaches built separate classifiers to detect nonsense in
dialogues. In this paper, we show that without external classifiers, dialogue
models can detect errors in their own messages introspectively, by calculating
the likelihood of replies that are indicative of poor messages. For example, if
an agent believes its partner is likely to respond "I don't understand" to a
candidate message, that message may not make sense, so an alternative message
should be chosen. We evaluate our approach on a dataset from the game
Diplomacy, which contains long dialogues richly grounded in the game state, on
which existing models make many errors. We first show that hand-crafted replies
can be effective for the task of detecting nonsense in applications as complex
as Diplomacy. We then design AutoReply, an algorithm to search for such
discriminative replies automatically, given a small number of annotated
dialogue examples. We find that AutoReply-generated replies outperform
handcrafted replies and perform on par with carefully fine-tuned large
supervised models. Results also show that one single reply without much
computation overheads can also detect dialogue nonsense reasonably well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Data Recasting to Enhance Tabular Reasoning. (arXiv:2211.12641v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12641">
<div class="article-summary-box-inner">
<span><p>Creating challenging tabular inference data is essential for learning complex
reasoning. Prior work has mostly relied on two data generation strategies. The
first is human annotation, which yields linguistically diverse data but is
difficult to scale. The second category for creation is synthetic generation,
which is scalable and cost effective but lacks inventiveness. In this research,
we present a framework for semi-automatically recasting existing tabular data
to make use of the benefits of both approaches. We utilize our framework to
build tabular NLI instances from five datasets that were initially intended for
tasks like table2text creation, tabular Q/A, and semantic parsing. We
demonstrate that recasted data could be used as evaluation benchmarks as well
as augmentation data to enhance performance on tabular NLI tasks. Furthermore,
we investigate the effectiveness of models trained on recasted data in the
zero-shot scenario, and analyse trends in performance across different recasted
datasets types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data. (arXiv:2211.12668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12668">
<div class="article-summary-box-inner">
<span><p>Numerical reasoning over hybrid data containing tables and long texts has
recently received research attention from the AI community. To generate an
executable reasoning program consisting of math and table operations to answer
a question, state-of-the-art methods use a retriever-generator pipeline.
However, their retrieval results are static, while different generation steps
may rely on different sentences. To attend to the retrieved information that is
relevant to each generation step, in this paper, we propose DyRRen, an extended
retriever-reranker-generator framework where each generation step is enhanced
by a dynamic reranking of retrieved sentences. It outperforms existing
baselines on the FinQA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word-Level Representation From Bytes For Language Modeling. (arXiv:2211.12677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12677">
<div class="article-summary-box-inner">
<span><p>Modern language models mostly take sub-words as input, a design that balances
the trade-off between vocabulary size, number of parameters, and performance.
However, sub-word tokenization still has disadvantages like not being robust to
noise and difficult to generalize to new languages. Also, the current trend of
scaling up models reveals that larger models require larger embeddings but that
makes parallelization hard. Previous work on image classification proves
splitting raw input into a sequence of chucks is a strong, model-agnostic
inductive bias. Based on this observation, we rethink the existing
character-aware method that takes character-level inputs but makes word-level
sequence modeling and prediction. We overhaul this method by introducing a
cross-attention network that builds word-level representation directly from
bytes, and a sub-word level prediction based on word-level hidden states to
avoid the time and space requirement of word-level prediction. With these two
improvements combined, we have a token free model with slim input embeddings
for downstream tasks. We name our method Byte2Word and perform evaluations on
language modeling and text classification. Experiments show that Byte2Word is
on par with the strong sub-word baseline BERT but only takes up 10\% of
embedding size. We further test our method on synthetic noise and cross-lingual
transfer and find it competitive to baseline methods on both settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12701">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) is an emerging learning paradigm that aims to emulate
the human capability of learning and accumulating knowledge continually without
forgetting the previously learned knowledge and also transferring the knowledge
to new tasks to learn them better. This survey presents a comprehensive review
of the recent progress of CL in the NLP field. It covers (1) all CL settings
with a taxonomy of existing techniques. Besides dealing with forgetting, it
also focuses on (2) knowledge transfer, which is of particular importance to
NLP. Both (1) and (2) are not mentioned in the existing survey. Finally, a list
of future directions is also discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Open-Domain QA Reader Utilize External Knowledge Efficiently like Humans?. (arXiv:2211.12707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12707">
<div class="article-summary-box-inner">
<span><p>Recent state-of-the-art open-domain QA models are typically based on a two
stage retriever-reader approach in which the retriever first finds the relevant
knowledge/passages and the reader then leverages that to predict the answer.
Prior work has shown that the performance of the reader usually tends to
improve with the increase in the number of these passages. Thus,
state-of-the-art models use a large number of passages (e.g. 100) for
inference. While the reader in this approach achieves high prediction
performance, its inference is computationally very expensive. We humans, on the
other hand, use a more efficient strategy while answering: firstly, if we can
confidently answer the question using our already acquired knowledge then we do
not even use the external knowledge, and in the case when we do require
external knowledge, we don't read the entire knowledge at once, instead, we
only read that much knowledge that is sufficient to find the answer. Motivated
by this procedure, we ask a research question "Can the open-domain QA reader
utilize external knowledge efficiently like humans without sacrificing the
prediction performance?"
</p>
<p>Driven by this question, we explore an approach that utilizes both
'closed-book' (leveraging knowledge already present in the model parameters)
and 'open-book' inference (leveraging external knowledge). Furthermore, instead
of using a large fixed number of passages for open-book inference, we
dynamically read the external knowledge in multiple 'knowledge iterations'.
Through comprehensive experiments on NQ and TriviaQA datasets, we demonstrate
that this dynamic reading approach improves both the 'inference efficiency' and
the 'prediction accuracy' of the reader. Comparing with the FiD reader, this
approach matches its accuracy by utilizing just 18.32% of its reader inference
cost and also outperforms it by achieving up to 55.10% accuracy on NQ Open.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding Compression for Text Classification Using Dictionary Screening. (arXiv:2211.12715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12715">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a dictionary screening method for embedding
compression in text classification tasks. The key purpose of this method is to
evaluate the importance of each keyword in the dictionary. To this end, we
first train a pre-specified recurrent neural network-based model using a full
dictionary. This leads to a benchmark model, which we then use to obtain the
predicted class probabilities for each sample in a dataset. Next, to evaluate
the impact of each keyword in affecting the predicted class probabilities, we
develop a novel method for assessing the importance of each keyword in a
dictionary. Consequently, each keyword can be screened, and only the most
important keywords are reserved. With these screened keywords, a new dictionary
with a considerably reduced size can be constructed. Accordingly, the original
text sequence can be substantially compressed. The proposed method leads to
significant reductions in terms of parameters, average text sequence, and
dictionary size. Meanwhile, the prediction power remains very competitive
compared to the benchmark model. Extensive numerical studies are presented to
demonstrate the empirical performance of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoentGen: Vision-Language Foundation Model for Chest X-ray Generation. (arXiv:2211.12737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12737">
<div class="article-summary-box-inner">
<span><p>Multimodal models trained on large natural image-text pair datasets have
exhibited astounding abilities in generating high-quality images. Medical
imaging data is fundamentally different to natural images, and the language
used to succinctly capture relevant details in medical data uses a different,
narrow but semantically rich, domain-specific vocabulary. Not surprisingly,
multi-modal models trained on natural image-text pairs do not tend to
generalize well to the medical domain. Developing generative imaging models
faithfully representing medical concepts while providing compositional
diversity could mitigate the existing paucity of high-quality, annotated
medical imaging datasets. In this work, we develop a strategy to overcome the
large natural-medical distributional shift by adapting a pre-trained latent
diffusion model on a corpus of publicly available chest x-rays (CXR) and their
corresponding radiology (text) reports. We investigate the model's ability to
generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We
assess the model outputs quantitatively using image quality metrics, and
evaluate image quality and text-image alignment by human domain experts. We
present evidence that the resulting model (RoentGen) is able to create visually
convincing, diverse synthetic CXR images, and that the output can be controlled
to a new extent by using free-form text prompts including radiology-specific
language. Fine-tuning this model on a fixed training set and using it as a data
augmentation method, we measure a 5% improvement of a classifier trained
jointly on synthetic and real images, and a 3% improvement when trained on a
larger but purely synthetic training set. Finally, we observe that this
fine-tuning distills in-domain knowledge in the text-encoder and can improve
its representation capabilities of certain diseases like pneumothorax by 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agent-Specific Deontic Modality Detection in Legal Language. (arXiv:2211.12752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12752">
<div class="article-summary-box-inner">
<span><p>Legal documents are typically long and written in legalese, which makes it
particularly difficult for laypeople to understand their rights and duties.
While natural language understanding technologies can be valuable in supporting
such understanding in the legal domain, the limited availability of datasets
annotated for deontic modalities in the legal domain, due to the cost of hiring
experts and privacy issues, is a bottleneck. To this end, we introduce,
LEXDEMOD, a corpus of English contracts annotated with deontic modality
expressed with respect to a contracting party or agent along with the modal
triggers. We benchmark this dataset on two tasks: (i) agent-specific
multi-label deontic modality classification, and (ii) agent-specific deontic
modality and trigger span detection using Transformer-based (Vaswani et al.,
2017) language models. Transfer learning experiments show that the linguistic
diversity of modal expressions in LEXDEMOD generalizes reasonably from lease to
employment and rental agreements. A small case study indicates that a model
trained on LEXDEMOD can detect red flags with high recall. We believe our work
offers a new research direction for deontic modality detection in the legal
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12764">
<div class="article-summary-box-inner">
<span><p>Many recent studies leverage the pre-trained CLIP for text-video cross-modal
retrieval by tuning the backbone with additional heavy modules, which not only
brings huge computational burdens with much more parameters, but also leads to
the knowledge forgetting from upstream models.In this work, we propose the VoP:
Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video
retrieval task. The proposed VoP is an end-to-end framework with both video &amp;
text prompts introducing, which can be regarded as a powerful baseline with
only 0.1% trainable parameters. Further, based on the spatio-temporal
characteristics of videos, we develop three novel video prompt mechanisms to
improve the performance with different scales of trainable parameters. The
basic idea of the VoP enhancement is to model the frame position, frame
context, and layer function with specific trainable prompts, respectively.
Extensive experiments show that compared to full fine-tuning, the enhanced VoP
achieves a 1.4% average R@1 gain across five text-video retrieval benchmarks
with 6x less parameter overhead. The code will be available at
https://github.com/bighuang624/VoP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling. (arXiv:2211.12781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12781">
<div class="article-summary-box-inner">
<span><p>Existing research generally treats Chinese character as a minimum unit for
representation. However, such Chinese character representation will suffer two
bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich
internal features (e.g., radicals and strokes); and 2) Parameter bottleneck,
each individual character has to be represented by a unique vector. In this
paper, we introduce a novel representation method for Chinese characters to
break the bottlenecks, namely StrokeNet, which represents a Chinese character
by a Latinized stroke sequence (e.g., "ao1 (concave)" to "ajaie" and "tu1
(convex)" to "aeaqe"). Specifically, StrokeNet maps each stroke to a specific
Latin character, thus allowing similar Chinese characters to have similar Latin
representations. With the introduction of StrokeNet to neural machine
translation (NMT), many powerful but not applicable techniques to non-Latin
languages (e.g., shared subword vocabulary learning and ciphertext-based data
augmentation) can now be perfectly implemented. Experiments on the widely-used
NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT
tasks show that StrokeNet can provide a significant performance boost over the
strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17
Chinese-English task which is better than any previously reported results
without using monolingual data. Code and scripts are freely available at
https://github.com/zjwang21/StrokeNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IMaSC -- ICFOSS Malayalam Speech Corpus. (arXiv:2211.12796v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12796">
<div class="article-summary-box-inner">
<span><p>Modern text-to-speech (TTS) systems use deep learning to synthesize speech
increasingly approaching human quality, but they require a database of high
quality audio-text sentence pairs for training. Malayalam, the official
language of the Indian state of Kerala and spoken by 35+ million people, is a
low resource language in terms of available corpora for TTS systems. In this
paper, we present IMaSC, a Malayalam text and speech corpora containing
approximately 50 hours of recorded speech. With 8 speakers and a total of
34,473 text-audio pairs, IMaSC is larger than every other publicly available
alternative. We evaluated the database by using it to train TTS models for each
speaker based on a modern deep learning architecture. Via subjective
evaluation, we show that our models perform significantly better in terms of
naturalness compared to previous studies and publicly available models, with an
average mean opinion score of 4.50, indicating that the synthesized speech is
close to human quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation. (arXiv:2211.12824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12824">
<div class="article-summary-box-inner">
<span><p>Generating a video given the first several static frames is challenging as it
anticipates reasonable future frames with temporal coherence. Besides video
prediction, the ability to rewind from the last frame or infilling between the
head and tail is also crucial, but they have rarely been explored for video
completion. Since there could be different outcomes from the hints of just a
few frames, a system that can follow natural language to perform video
completion may significantly improve controllability. Inspired by this, we
introduce a novel task, text-guided video completion (TVC), which requests the
model to generate a video from partial frames guided by an instruction. We then
propose Multimodal Masked Video Generation (MMVG) to address this TVC task.
During training, MMVG discretizes the video frames into visual tokens and masks
most of them to perform video completion from any time point. At inference
time, a single MMVG model can address all 3 cases of TVC, including video
prediction, rewind, and infilling, by applying corresponding masking
conditions. We evaluate MMVG in various video scenarios, including egocentric,
animation, and gaming. Extensive experimental results indicate that MMVG is
effective in generating high-quality visual appearances with text guidance for
TVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Socratic Subquestions for Teaching Math Word Problems. (arXiv:2211.12835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12835">
<div class="article-summary-box-inner">
<span><p>Socratic questioning is an educational method that allows students to
discover answers to complex problems by asking them a series of thoughtful
questions. Generation of didactically sound questions is challenging, requiring
understanding of the reasoning process involved in the problem. We hypothesize
that such questioning strategy can not only enhance the human performance, but
also assist the math word problem (MWP) solvers. In this work, we explore the
ability of large language models (LMs) in generating sequential questions for
guiding math word problem-solving. We propose various guided question
generation schemes based on input conditioning and reinforcement learning. On
both automatic and human quality evaluations, we find that LMs constrained with
desirable question properties generate superior questions and improve the
overall performance of a math word problem solver. We conduct a preliminary
user study to examine the potential value of such question generation models in
the education domain. Results suggest that the difficulty level of problems
plays an important role in determining whether questioning improves or hinders
human performance. We discuss the future of using such questioning strategies
in education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphWOZ: Dialogue Management with Conversational Knowledge Graphs. (arXiv:2211.12852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12852">
<div class="article-summary-box-inner">
<span><p>We present a new approach to dialogue management using conversational
knowledge graphs as core representation of the dialogue state. To this end, we
introduce a new dataset, GraphWOZ, which comprises Wizard-of-Oz dialogues in
which human participants interact with a robot acting as a receptionist. In
contrast to most existing work on dialogue management, GraphWOZ relies on a
dialogue state explicitly represented as a dynamic knowledge graph instead of a
fixed set of slots. This graph is composed of a varying number of entities
(such as individuals, places, events, utterances and mentions) and relations
between them (such as persons being part of a group or attending an event). The
graph is then regularly updated on the basis of new observations and system
actions. GraphWOZ is released along with detailed manual annotations related to
the user intents, system responses, and reference relations occurring in both
user and system turns. Based on GraphWOZ, we present experimental results for
two dialogue management tasks, namely conversational entity linking and
response ranking. For conversational entity linking, we show how to connect
utterance mentions to their corresponding entity in the knowledge graph with a
neural model relying on a combination of both string and graph-based features.
Response ranking is then performed by summarizing the relevant content of the
graph into a text, which is concatenated with the dialogue history and employed
as input to score possible responses to a given dialogue state.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning. (arXiv:2211.12878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12878">
<div class="article-summary-box-inner">
<span><p>To overcome the data sparsity issue in short text topic modeling, existing
methods commonly rely on data augmentation or the data characteristic of short
texts to introduce more word co-occurrence information. However, most of them
do not make full use of the augmented data or the data characteristic: they
insufficiently learn the relations among samples in data, leading to dissimilar
topic distributions of semantically similar text pairs. To better address data
sparsity, in this paper we propose a novel short text topic modeling framework,
Topic-Semantic Contrastive Topic Model (TSCTM). To sufficiently model the
relations among samples, we employ a new contrastive learning method with
efficient positive and negative sampling strategies based on topic semantics.
This contrastive learning method refines the representations, enriches the
learning signals, and thus mitigates the sparsity issue. Extensive experimental
results show that our TSCTM outperforms state-of-the-art baselines regardless
of the data augmentation availability, producing high-quality topics and topic
distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Conspiracy Theory Against COVID-19 Vaccines. (arXiv:2211.13003v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13003">
<div class="article-summary-box-inner">
<span><p>Since the beginning of the vaccination trial, social media has been flooded
with anti-vaccination comments and conspiracy beliefs. As the day passes, the
number of COVID- 19 cases increases, and online platforms and a few news
portals entertain sharing different conspiracy theories. The most popular
conspiracy belief was the link between the 5G network spreading COVID-19 and
the Chinese government spreading the virus as a bioweapon, which initially
created racial hatred. Although some disbelief has less impact on society,
others create massive destruction. For example, the 5G conspiracy led to the
burn of the 5G Tower, and belief in the Chinese bioweapon story promoted an
attack on the Asian-Americans. Another popular conspiracy belief was that Bill
Gates spread this Coronavirus disease (COVID-19) by launching a mass
vaccination program to track everyone. This Conspiracy belief creates distrust
issues among laypeople and creates vaccine hesitancy. This study aims to
discover the conspiracy theory against the vaccine on social platforms. We
performed a sentiment analysis on the 598 unique sample comments related to
COVID-19 vaccines. We used two different models, BERT and Perspective API, to
find out the sentiment and toxicity of the sentence toward the COVID-19
vaccine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sarcasm Detection Framework Using Emotion and Sentiment Features. (arXiv:2211.13014v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13014">
<div class="article-summary-box-inner">
<span><p>Sarcasm detection is an essential task that can help identify the actual
sentiment in user-generated data, such as discussion forums or tweets. Sarcasm
is a sophisticated form of linguistic expression because its surface meaning
usually contradicts its inner, deeper meaning. Such incongruity is the
essential component of sarcasm, however, it makes sarcasm detection quite a
challenging task. In this paper, we propose a model which incorporates emotion
and sentiment features to capture the incongruity intrinsic to sarcasm.
Moreover, we use CNN and pre-trained Transformer to capture context features.
Our approach achieved state-of-the-art results on four datasets from social
networking platforms and online media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Lifelong Language Learning. (arXiv:2211.13050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13050">
<div class="article-summary-box-inner">
<span><p>Lifelong learning aims to accumulate knowledge and alleviate catastrophic
forgetting when learning tasks sequentially. However, existing lifelong
language learning methods only focus on the supervised learning setting.
Unlabeled data, which can be easily accessed in real-world scenarios, are
underexplored. In this paper, we explore a novel setting, semi-supervised
lifelong language learning (SSLL), where a model learns sequentially arriving
language tasks with both labeled and unlabeled data. We propose an unlabeled
data enhanced lifelong learner to explore SSLL. Specially, we dedicate
task-specific modules to alleviate catastrophic forgetting and design two
modules to exploit unlabeled data: (1) a virtual supervision enhanced task
solver is constructed on a teacher-student framework to mine the underlying
knowledge from unlabeled data; and (2) a backward augmented learner is built to
encourage knowledge transfer from newly arrived unlabeled data to previous
tasks. Experimental results on various language tasks demonstrate our model's
effectiveness and superiority over competitive baselines under the new setting
SSLL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Schr\"{o}dinger's Bat: Diffusion Models Sometimes Generate Polysemous Words in Superposition. (arXiv:2211.13095v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13095">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that despite their impressive capabilities,
text-to-image diffusion models such as DALL-E 2 (Ramesh et al., 2022) can
display strange behaviours when a prompt contains a word with multiple possible
meanings, often generating images containing both senses of the word (Rassin et
al., 2022). In this work we seek to put forward a possible explanation of this
phenomenon. Using the similar Stable Diffusion model (Rombach et al., 2022), we
first show that when given an input that is the sum of encodings of two
distinct words, the model can produce an image containing both concepts
represented in the sum. We then demonstrate that the CLIP encoder used to
encode prompts (Radford et al., 2021) encodes polysemous words as a
superposition of meanings, and that using linear algebraic techniques we can
edit these representations to influence the senses represented in the generated
images. Combining these two findings, we suggest that the homonym duplication
phenomenon described by Rassin et al. (2022) is caused by diffusion models
producing images representing both of the meanings that are present in
superposition in the encoding of a polysemous word.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish. (arXiv:2211.13112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13112">
<div class="article-summary-box-inner">
<span><p>The availability of compute and data to train larger and larger language
models increases the demand for robust methods of benchmarking the true
progress of LM training. Recent years witnessed significant progress in
standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or
KILT have become de facto standard tools to compare large language models.
Following the trend to replicate GLUE for other languages, the KLEJ benchmark
has been released for Polish. In this paper, we evaluate the progress in
benchmarking for low-resourced languages. We note that only a handful of
languages have such comprehensive benchmarks. We also note the gap in the
number of tasks being evaluated by benchmarks for resource-rich English/Chinese
and the rest of the world. In this paper, we introduce LEPISZCZE (the Polish
word for glew, the Middle English predecessor of glue), a new, comprehensive
benchmark for Polish NLP with a large variety of tasks and high-quality
operationalization of the benchmark. We design LEPISZCZE with flexibility in
mind. Including new models, datasets, and tasks is as simple as possible while
still offering data versioning and model tracking. In the first run of the
benchmark, we test 13 experiments (task and dataset pairs) based on the five
most recent LMs for Polish. We use five datasets from the Polish benchmark and
add eight novel datasets. As the paper's main contribution, apart from
LEPISZCZE, we provide insights and experiences learned while creating the
benchmark for Polish as the blueprint to design similar benchmarks for other
low-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Number Theory Meets Linguistics: Modelling Noun Pluralisation Across 1497 Languages Using 2-adic Metrics. (arXiv:2211.13124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13124">
<div class="article-summary-box-inner">
<span><p>A simple machine learning model of pluralisation as a linear regression
problem minimising a p-adic metric substantially outperforms even the most
robust of Euclidean-space regressors on languages in the Indo-European,
Austronesian, Trans New-Guinea, Sino-Tibetan, Nilo-Saharan, Oto-Meanguean and
Atlantic-Congo language families. There is insufficient evidence to support
modelling distinct noun declensions as a p-adic neighbourhood even in
Indo-European languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Average Token Delay: A Latency Metric for Simultaneous Translation. (arXiv:2211.13173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13173">
<div class="article-summary-box-inner">
<span><p>Simultaneous translation is a task in which translation begins before the
speaker has finished speaking. In its evaluation, we have to consider the
latency of the translation in addition to the quality. The latency is
preferably as small as possible for users to comprehend what the speaker says
with a small delay. Existing latency metrics focus on when the translation
starts but do not consider adequately when the translation ends. This means
such metrics do not penalize the latency caused by a long translation output,
which actually delays users' comprehension. In this work, we propose a novel
latency evaluation metric called Average Token Delay (ATD) that focuses on the
end timings of partial translations in simultaneous translation. We discuss the
advantage of ATD using simulated examples and also investigate the differences
between ATD and Average Lagging with simultaneous translation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TorchScale: Transformers at Scale. (arXiv:2211.13184v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13184">
<div class="article-summary-box-inner">
<span><p>Large Transformers have achieved state-of-the-art performance across many
tasks. Most open-source libraries on scaling Transformers focus on improving
training or inference with better parallelization. In this work, we present
TorchScale, an open-source toolkit that allows researchers and developers to
scale up Transformers efficiently and effectively. TorchScale has the
implementation of several modeling techniques, which can improve modeling
generality and capability, as well as training stability and efficiency.
Experimental results on language modeling and neural machine translation
demonstrate that TorchScale can successfully scale Transformers to different
sizes without tears. The library is available at https://aka.ms/torchscale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeedBERT: Recovering Annotator Rating Distributions from an Aggregated Label. (arXiv:2211.13196v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13196">
<div class="article-summary-box-inner">
<span><p>Many machine learning tasks -- particularly those in affective computing --
are inherently subjective. When asked to classify facial expressions or to rate
an individual's attractiveness, humans may disagree with one another, and no
single answer may be objectively correct. However, machine learning datasets
commonly have just one "ground truth" label for each sample, so models trained
on these labels may not perform well on tasks that are subjective in nature.
Though allowing models to learn from the individual annotators' ratings may
help, most datasets do not provide annotator-specific labels for each sample.
To address this issue, we propose SeedBERT, a method for recovering annotator
rating distributions from a single label by inducing pre-trained models to
attend to different portions of the input. Our human evaluations indicate that
SeedBERT's attention mechanism is consistent with human sources of annotator
disagreement. Moreover, in our empirical evaluations using large language
models, SeedBERT demonstrates substantial gains in performance on downstream
subjective tasks compared both to standard deep learning models and to other
current models that account explicitly for annotator disagreement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors. (arXiv:2211.13224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.13224">
<div class="article-summary-box-inner">
<span><p>Recent diffusion-based generative models combined with vision-language models
are capable of creating realistic images from natural language prompts. While
these models are trained on large internet-scale datasets, such pre-trained
models are not directly introduced to any semantic localization or grounding.
Most current approaches for localization or grounding rely on human-annotated
localization information in the form of bounding boxes or segmentation masks.
The exceptions are a few unsupervised methods that utilize architectures or
loss functions geared towards localization, but they need to be trained
separately. In this work, we explore how off-the-shelf diffusion models,
trained with no exposure to such localization information, are capable of
grounding various semantic phrases with no segmentation-specific re-training.
An inference time optimization process is introduced, that is capable of
generating segmentation masks conditioned on natural language. We evaluate our
proposal Peekaboo for unsupervised semantic segmentation on the Pascal VOC
dataset. In addition, we evaluate for referring segmentation on the RefCOCO
dataset. In summary, we present a first zero-shot, open-vocabulary,
unsupervised (no localization information), semantic grounding technique
leveraging diffusion-based generative models with no re-training. Our code will
be released publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Executing Instructions in Situated Collaborative Interactions. (arXiv:1910.03655v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.03655">
<div class="article-summary-box-inner">
<span><p>We study a collaborative scenario where a user not only instructs a system to
complete tasks, but also acts alongside it. This allows the user to adapt to
the system abilities by changing their language or deciding to simply
accomplish some tasks themselves, and requires the system to effectively
recover from errors as the user strategically assigns it new goals. We build a
game environment to study this scenario, and learn to map user instructions to
system actions. We introduce a learning approach focused on recovery from
cascading errors between instructions, and modeling methods to explicitly
reason about instructions with multiple goals. We evaluate with a new
evaluation protocol using recorded interactions and online games with human
users, and observe how users adapt to the system abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CANDLE: Decomposing Conditional and Conjunctive Queries for Task-Oriented Dialogue Systems. (arXiv:2107.03884v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03884">
<div class="article-summary-box-inner">
<span><p>Domain-specific dialogue systems generally determine user intents by relying
on sentence level classifiers that mainly focus on single action sentences.
Such classifiers are not designed to effectively handle complex queries
composed of conditional and sequential clauses that represent multiple actions.
We attempt to decompose such queries into smaller single action subqueries that
are reasonable for intent classifiers to understand in a dialogue pipeline. We
release, CANDLE(Conditional &amp; AND type Expressions), a dataset consisting of
4282 utterances manually tagged with conditional and sequential labels, and
demonstrates this decomposition by training two baseline taggers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13532">
<div class="article-summary-box-inner">
<span><p>Prompt-based methods have been successfully applied in sentence-level
few-shot learning tasks, mostly owing to the sophisticated design of templates
and label words. However, when applied to token-level labeling tasks such as
NER, it would be time-consuming to enumerate the template queries over all
potential entity spans. In this work, we propose a more elegant method to
reformulate NER tasks as LM problems without any templates. Specifically, we
discard the template construction process while maintaining the word prediction
paradigm of pre-training models to predict a class-related pivot word (or label
word) at the entity position. Meanwhile, we also explore principled ways to
automatically search for appropriate label words that the pre-trained models
can easily adapt to. While avoiding complicated template-based process, the
proposed LM objective also reduces the gap between different objectives used in
pre-training and fine-tuning, thus it can better benefit the few-shot
performance. Experimental results demonstrate the effectiveness of the proposed
method over bert-tagger and template-based method under few-shot setting.
Moreover, the decoding speed of the proposed method is up to 1930.12 times
faster than the template-based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13696">
<div class="article-summary-box-inner">
<span><p>Compensation for channel mismatch and noise interference is essential for
robust automatic speech recognition. Enhanced speech has been introduced into
the multi-condition training of acoustic models to improve their generalization
ability. In this paper, a noise-aware training framework based on two cascaded
neural structures is proposed to jointly optimize speech enhancement and speech
recognition. The feature enhancement module is composed of a multi-task
autoencoder, where noisy speech is decomposed into clean speech and noise. By
concatenating its enhanced, noise-aware, and noisy features for each frame, the
acoustic-modeling module maps each feature-augmented frame into a triphone
state by optimizing the lattice-free maximum mutual information and cross
entropy between the predicted and actual state sequences. On top of the
factorized time delay neural network (TDNN-F) and its convolutional variant
(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error
rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared
with the best existing systems that use bigram and trigram language models for
decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction
of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based
system also outperforms the baseline CNN-TDNNF system on the AMI task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis. (arXiv:2203.16369v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16369">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a
specific aspect in the given sentence. While pre-trained language models such
as BERT have achieved great success, incorporating dynamic semantic changes
into ABSA remains challenging. To this end, in this paper, we propose to
address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method
designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we
first take the Stack-BERT layers as a primary encoder to grasp the overall
semantic of the sentence and then fine-tune it by incorporating a lightweight
Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention
to a small region of the sentences at each step and re-weigh the vitally
important words for better aspect-aware sentiment understanding. Finally,
experimental results on three benchmark datasets demonstrate the effectiveness
and the rationality of our proposed model and provide good interpretable
insights for future semantic modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Spoken Dialogue Language Modeling. (arXiv:2203.16502v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16502">
<div class="article-summary-box-inner">
<span><p>We introduce dGSLM, the first "textless" model able to generate audio samples
of naturalistic spoken dialogues. It uses recent work on unsupervised spoken
unit discovery coupled with a dual-tower transformer architecture with
cross-attention trained on 2000 hours of two-channel raw conversational audio
(Fisher dataset) without any text or labels. We show that our model is able to
generate speech, laughter and other paralinguistic signals in the two channels
simultaneously and reproduces more naturalistic and fluid turn-taking compared
to a text-based cascaded model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phylogeny-Inspired Adaptation of Multilingual Models to New Languages. (arXiv:2205.09634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09634">
<div class="article-summary-box-inner">
<span><p>Large pretrained multilingual models, trained on dozens of languages, have
delivered promising results due to cross-lingual learning capabilities on
variety of language tasks. Further adapting these models to specific languages,
especially ones unseen during pre-training, is an important goal towards
expanding the coverage of language technologies. In this study, we show how we
can use language phylogenetic information to improve cross-lingual transfer
leveraging closely related languages in a structured, linguistically-informed
manner. We perform adapter-based training on languages from diverse language
families (Germanic, Uralic, Tupian, Uto-Aztecan) and evaluate on both syntactic
and semantic tasks, obtaining more than 20% relative performance improvements
over strong commonly used baselines, especially on languages unseen during
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeT: Code Generation with Generated Tests. (arXiv:2207.10397v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10397">
<div class="article-summary-box-inner">
<span><p>The task of generating code solutions for a given programming problem can
benefit from the use of pre-trained language models such as Codex, which can
produce multiple diverse samples. However, a major challenge for this task is
to select the most appropriate solution from the multiple samples generated by
the pre-trained language models. A natural way to evaluate the quality and
correctness of a code solution is to run it against a set of test cases, but
the manual creation of such test cases is often costly and time-consuming. In
this paper, we propose a novel method, CodeT, that leverages the same
pre-trained language models to automatically generate test cases for the code
samples, thus reducing the human effort and increasing the coverage of the test
scenarios. CodeT then executes the code samples using the generated test cases,
and performs a dual execution agreement, which considers both the consistency
of the outputs against the generated test cases and the agreement of the
outputs with other code samples. We conduct comprehensive experiments on four
benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different
pre-trained language models with varying sizes and capabilities. Our results
show that CodeT can significantly improve the performance of code solution
selection over previous methods, achieving remarkable and consistent gains
across different models and benchmarks. For instance, CodeT improves the pass@1
metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%
over the code-davinci-002 model, and an absolute improvement of more than 20%
over the previous state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Coherent Narratives by Learning Dynamic and Discrete Entity States with a Contrastive Framework. (arXiv:2208.03985v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.03985">
<div class="article-summary-box-inner">
<span><p>Despite advances in generating fluent texts, existing pretraining models tend
to attach incoherent event sequences to involved entities when generating
narratives such as stories and news. We conjecture that such issues result from
representing entities as static embeddings of superficial words, while
neglecting to model their ever-changing states, i.e., the information they
carry, as the text unfolds. Therefore, we extend the Transformer model to
dynamically conduct entity state updates and sentence realization for narrative
generation. We propose a contrastive framework to learn the state
representations in a discrete space, and insert additional attention layers
into the decoder to better exploit these states. Experiments on two narrative
datasets show that our model can generate more coherent and diverse narratives
than strong baselines with the guidance of meaningful entity states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. (arXiv:2209.07858v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07858">
<div class="article-summary-box-inner">
<span><p>We describe our early efforts to red team language models in order to
simultaneously discover, measure, and attempt to reduce their potentially
harmful outputs. We make three main contributions. First, we investigate
scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B
parameters) and 4 model types: a plain language model (LM); an LM prompted to
be helpful, honest, and harmless; an LM with rejection sampling; and a model
trained to be helpful and harmless using reinforcement learning from human
feedback (RLHF). We find that the RLHF models are increasingly difficult to red
team as they scale, and we find a flat trend with scale for the other model
types. Second, we release our dataset of 38,961 red team attacks for others to
analyze and learn from. We provide our own analysis of the data and find a
variety of harmful outputs, which range from offensive language to more subtly
harmful non-violent unethical outputs. Third, we exhaustively describe our
instructions, processes, statistical methodologies, and uncertainty about red
teaming. We hope that this transparency accelerates our ability to work
together as a community in order to develop shared norms, practices, and
technical standards for how to red team language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.11416">
<div class="article-summary-box-inner">
<span><p>Finetuning language models on a collection of datasets phrased as
instructions has been shown to improve model performance and generalization to
unseen tasks. In this paper we explore instruction finetuning with a particular
focus on (1) scaling the number of tasks, (2) scaling the model size, and (3)
finetuning on chain-of-thought data. We find that instruction finetuning with
the above aspects dramatically improves performance on a variety of model
classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and
evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For
instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM
540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves
state-of-the-art performance on several benchmarks, such as 75.2% on five-shot
MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong
few-shot performance even compared to much larger models, such as PaLM 62B.
Overall, instruction finetuning is a general method for improving the
performance and usability of pretrained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation. (arXiv:2210.12409v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12409">
<div class="article-summary-box-inner">
<span><p>Variational Auto-Encoder (VAE) has been widely adopted in text generation.
Among many variants, recurrent VAE learns token-wise latent variables with each
conditioned on the preceding ones, which captures sequential variability better
in the era of RNN. However, it is unclear how to incorporate such recurrent
dynamics into the recently dominant Transformer due to its parallelism. In this
work, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE
imposes recurrence on segment-wise latent variables with arbitrarily separated
text segments and constructs the posterior distribution with residual
parameterization. Besides, we design an acceleration method by approximating
idempotent matrices, which allows parallelism while maintaining the conditional
dependence of latent variables. We demonstrate that TRACE could enhance the
entanglement of each segment and preceding latent variables and deduce a
non-zero lower bound of the KL term, providing a theoretical guarantee of
generation diversity. Experiments on two unconditional and one conditional
generation tasks show that TRACE achieves significantly improved diversity
while maintaining satisfactory generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Teacher-student Framework for Unsupervised Speech Enhancement Using Noise Remixing Training and Two-stage Inference. (arXiv:2210.15368v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15368">
<div class="article-summary-box-inner">
<span><p>The lack of clean speech is a practical challenge to the development of
speech enhancement systems, which means that the training of neural network
models must be done in an unsupervised manner, and there is an inevitable
mismatch between their training criterion and evaluation metric. In response to
this unfavorable situation, we propose a teacher-student training strategy that
does not require any subjective/objective speech quality metrics as learning
reference by improving the previously proposed noisy-target training (NyTT).
Because homogeneity between in-domain noise and extraneous noise is the key to
the effectiveness of NyTT, we train various student models by remixing the
teacher model's estimated speech and noise for clean-target training or raw
noisy speech and the teacher model's estimated noise for noisy-target training.
We use the NyTT model as the initial teacher model. Experimental results show
that our proposed method outperforms several baselines, especially with
two-stage inference, where clean speech is derived successively through the
bootstrap model and the final student model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic extraction of materials and properties from superconductors scientific literature. (arXiv:2210.15600v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15600">
<div class="article-summary-box-inner">
<span><p>The automatic extraction of materials and related properties from the
scientific literature is gaining attention in data-driven materials science
(Materials Informatics). In this paper, we discuss Grobid-superconductors, our
solution for automatically extracting superconductor material names and
respective properties from text. Built as a Grobid module, it combines machine
learning and heuristic approaches in a multi-step architecture that supports
input data as raw text or PDF documents. Using Grobid-superconductors, we built
SuperCon2, a database of 40324 materials and properties records from 37700
papers. The material (or sample) information is represented by name, chemical
formula, and material class, and is characterized by shape, doping,
substitution variables for components, and substrate as adjoined information.
The properties include the Tc superconducting critical temperature and, when
available, applied pressure with the Tc measurement method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations. (arXiv:2210.16637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.16637">
<div class="article-summary-box-inner">
<span><p>Recent work has demonstrated that pre-trained language models (PLMs) are
zero-shot learners. However, most existing zero-shot methods involve heavy
human engineering or complicated self-training pipelines, hindering their
application to new situations. In this work, we show that zero-shot text
classification can be improved simply by clustering texts in the embedding
spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian
Gaussian Mixture Model after initializing cluster positions and shapes using
class names. Despite its simplicity, this approach achieves superior or
comparable performance on both topic and sentiment classification datasets and
outperforms prior works significantly on unbalanced datasets. We further
explore the applicability of our clustering approach by evaluating it on 14
datasets with more diverse topics, text lengths, and numbers of classes. Our
approach achieves an average of 20% absolute improvement over prompt-based
zero-shot learning. Finally, we compare different PLM embedding spaces and find
that texts are well-clustered by topics even if the PLM is not explicitly
pre-trained to generate meaningful sentence embeddings. This work indicates
that PLM embeddings can categorize texts without task-specific fine-tuning,
thus providing a new way to analyze and utilize their knowledge and zero-shot
learning ability.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-11-24 23:14:32.961639762 UTC">2022-11-24 23:14:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>