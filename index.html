<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-29T01:30:00Z">05-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Talking with Machines: A Comprehensive Survey of Emergent Dialogue Systems. (arXiv:2305.16324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16324">
<div class="article-summary-box-inner">
<span><p>From the earliest experiments in the 20th century to the utilization of large
language models and transformers, dialogue systems research has continued to
evolve, playing crucial roles in numerous fields. This paper offers a
comprehensive review of these systems, tracing their historical development and
examining their fundamental operations. We analyze popular and emerging
datasets for training and survey key contributions in dialogue systems
research, including traditional systems and advanced machine learning methods.
Finally, we consider conventional and transformer-based evaluation metrics,
followed by a short discussion of prevailing challenges and future prospects in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16326">
<div class="article-summary-box-inner">
<span><p>Biomedical literature is growing rapidly, making it challenging to curate and
extract knowledge manually. Biomedical natural language processing (BioNLP)
techniques that can automatically extract information from biomedical
literature help alleviate this burden. Recently, large Language Models (LLMs),
such as GPT-3 and GPT-4, have gained significant attention for their impressive
performance. However, their effectiveness in BioNLP tasks and impact on method
development and downstream users remain understudied. This pilot study (1)
establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and
one-shot settings in eight BioNLP datasets across four applications: named
entity recognition, relation extraction, multi-label document classification,
and semantic similarity and reasoning, (2) examines the errors produced by the
LLMs and categorized the errors into three types: missingness, inconsistencies,
and unwanted artificial content, and (3) provides suggestions for using LLMs in
BioNLP applications. We make the datasets, baselines, and results publicly
available to the community via
https://github.com/qingyu-qc/gpt_bionlp_benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Composition in Visually Grounded Language Models. (arXiv:2305.16328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16328">
<div class="article-summary-box-inner">
<span><p>What is sentence meaning and its ideal representation? Much of the expressive
power of human language derives from semantic composition, the mind's ability
to represent meaning hierarchically &amp; relationally over constituents. At the
same time, much sentential meaning is outside the text and requires grounding
in sensory, motor, and experiential modalities to be adequately learned.
Although large language models display considerable compositional ability,
recent work shows that visually-grounded language models drastically fail to
represent compositional structure. In this thesis, we explore whether &amp; how
models compose visually grounded semantics, and how we might improve their
ability to do so.
</p>
<p>Specifically, we introduce 1) WinogroundVQA, a new compositional visual
question answering benchmark, 2) Syntactic Neural Module Distillation, a
measure of compositional ability in sentence embedding models, 3) Causal
Tracing for Image Captioning Models to locate neural representations vital for
vision-language composition, 4) Syntactic MeanPool to inject a compositional
inductive bias into sentence embeddings, and 5) Cross-modal Attention
Congruence Regularization, a self-supervised objective function for
vision-language relation alignment. We close by discussing connections of our
work to neuroscience, psycholinguistics, formal semantics, and philosophy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Generation with Speech Synthesis for ASR Data Augmentation. (arXiv:2305.16333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16333">
<div class="article-summary-box-inner">
<span><p>Aiming at reducing the reliance on expensive human annotations, data
synthesis for Automatic Speech Recognition (ASR) has remained an active area of
research. While prior work mainly focuses on synthetic speech generation for
ASR data augmentation, its combination with text generation methods is
considerably less explored. In this work, we explore text augmentation for ASR
using large-scale pre-trained neural networks, and systematically compare those
to traditional text augmentation methods. The generated synthetic texts are
then converted to synthetic speech using a text-to-speech (TTS) system and
added to the ASR training data. In experiments conducted on three datasets, we
find that neural models achieve 9%-15% relative WER improvement and outperform
traditional methods. We conclude that text augmentation, particularly through
modern neural approaches, is a viable tool for improving the accuracy of ASR
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities. (arXiv:2305.16334v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16334">
<div class="article-summary-box-inner">
<span><p>In most current research, large language models (LLMs) are able to perform
reasoning tasks by generating chains of thought through the guidance of
specific prompts. However, there still exists a significant discrepancy between
their capability in solving complex reasoning problems and that of humans. At
present, most approaches focus on chains of thought (COT) and tool use, without
considering the adoption and application of human cognitive frameworks. It is
well-known that when confronting complex reasoning challenges, humans typically
employ various cognitive abilities, and necessitate interaction with all
aspects of tools, knowledge, and the external environment information to
accomplish intricate tasks. This paper introduces a novel intelligent
framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive
architecture framework, and propose to simulate certain aspects of human
cognition. The framework involves approximating different cognitive modules,
including attention, memory, reasoning, learning, and corresponding scheduling
and decision-making mechanisms. Inspired by the active learning mechanism of
human beings, it proposes a learning unit to record previous mistakes and
expert opinions, and dynamically refer to them to strengthen their ability to
solve similar problems. The paper also outlines common effective reasoning
frameworks for human problem-solving and designs Chain-of-Thought (COT)
templates accordingly. A comprehensive decision-making mechanism is also
proposed to maximize model accuracy. The efficacy of OlaGPT has been
stringently evaluated on multiple reasoning datasets, and the experimental
outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks,
demonstrating its superior performance. Our implementation of OlaGPT is
available on GitHub: \url{https://github.com/oladata-team/OlaGPT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering. (arXiv:2305.16335v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16335">
<div class="article-summary-box-inner">
<span><p>Short text clustering is challenging since it takes imbalanced and noisy data
as inputs. Existing approaches cannot solve this problem well, since (1) they
are prone to obtain degenerate solutions especially on heavy imbalanced
datasets, and (2) they are vulnerable to noises. To tackle the above issues, we
propose a Robust Short Text Clustering (RSTC) model to improve robustness
against imbalanced and noisy data. RSTC includes two modules, i.e.,
pseudo-label generation module and robust representation learning module. The
former generates pseudo-labels to provide supervision for the later, which
contributes to more robust representations and correctly separated clusters. To
provide robustness against the imbalance in data, we propose self-adaptive
optimal transport in the pseudo-label generation module. To improve robustness
against the noise in data, we further introduce both class-wise and
instance-wise contrastive learning in the robust representation learning
module. Our empirical studies on eight short text clustering datasets
demonstrate that RSTC significantly outperforms the state-of-the-art models.
The code is available at: https://github.com/hmllmh/RSTC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16337">
<div class="article-summary-box-inner">
<span><p>Labels noise refers to errors in training labels caused by cheap data
annotation methods, such as web scraping or crowd-sourcing, which can be
detrimental to the performance of supervised classifiers. Several methods have
been proposed to counteract the effect of random label noise in supervised
classification, and some studies have shown that BERT is already robust against
high rates of randomly injected label noise. However, real label noise is not
random; rather, it is often correlated with input features or other
annotator-specific factors. In this paper, we evaluate BERT in the presence of
two types of realistic label noise: feature-dependent label noise, and
synthetic label noise from annotator disagreements. We show that the presence
of these types of noise significantly degrades BERT classification performance.
To improve robustness, we evaluate different types of ensembles and
noise-cleaning methods and compare their effectiveness against label noise
across different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16338">
<div class="article-summary-box-inner">
<span><p>Large language model (LLM)-based decision-making agents have shown the
ability to generalize across multiple tasks. However, their performance relies
on massive data and compute. We argue that this inefficiency stems from the
forgetting phenomenon, in which a model memorizes its behaviors in parameters
throughout training. As a result, training on a new task may deteriorate the
model's performance on previous tasks. In contrast to LLMs' implicit memory
mechanism, the human brain utilizes distributed memory storage, which helps
manage and organize multiple skills efficiently, mitigating the forgetting
phenomenon. Thus inspired, we propose an internal working memory module to
store, blend, and retrieve information for different downstream tasks.
Evaluation results show that the proposed method improves training efficiency
and generalization in both Atari games and meta-world object manipulation
tasks. Moreover, we demonstrate that memory fine-tuning further enhances the
adaptability of the proposed architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Trust GPT When Your Question Is Not In English. (arXiv:2305.16339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16339">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated exceptional natural language
understanding abilities and have excelled in a variety of natural language
processing (NLP)tasks in recent years. Despite the fact that most LLMs are
trained predominantly in English, multiple studies have demonstrated their
comparative performance in many other languages. However, fundamental questions
persist regarding how LLMs acquire their multi-lingual abilities and how
performance varies across different languages. These inquiries are crucial for
the study of LLMs since users and researchers often come from diverse language
backgrounds, potentially influencing their utilization and interpretation of
LLMs' results. In this work, we propose a systematic way of qualifying the
performance disparities of LLMs under multilingual settings. We investigate the
phenomenon of across-language generalizations in LLMs, wherein insufficient
multi-lingual training data leads to advanced multi-lingual capabilities. To
accomplish this, we employ a novel back-translation-based prompting method. The
results show that GPT exhibits highly translating-like behaviour in
multilingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16340">
<div class="article-summary-box-inner">
<span><p>Transformers have shown dominant performance across a range of domains
including language and vision. However, their computational cost grows
quadratically with the sequence length, making their usage prohibitive for
resource-constrained applications. To counter this, our approach is to divide
the whole sequence into segments. The information across segments can then be
aggregated using neurons with recurrence leveraging their inherent memory. Such
an approach leads to models with sequential processing capability at a lower
computation/memory cost. To investigate this idea, first, we examine the
effects of using local attention mechanism on the individual segments. Then we
propose a segmented recurrent transformer (SRformer) that combines segmented
attention with recurrent attention. It uses recurrent accumulate and fire (RAF)
layers to process information between consecutive segments. The loss caused by
reducing the attention window length is compensated by updating the product of
keys and values with RAF neurons' inherent recurrence. The segmented attention
and lightweight RAF gates ensure the efficiency of the proposed transformer. We
apply the proposed method to T5 and BART transformers. The modified models are
tested on summarization datasets including CNN-dailymail and XSUM. Notably,
using segmented inputs of different sizes, the proposed model achieves 4-19%
higher ROUGE1 scores than the segmented transformer baseline. Compared to full
attention, the proposed model largely reduces the complexity of cross attention
and results in around 40% reduction in computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition. (arXiv:2305.16342v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16342">
<div class="article-summary-box-inner">
<span><p>The local and global features are both essential for automatic speech
recognition (ASR). Many recent methods have verified that simply combining
local and global features can further promote ASR performance. However, these
methods pay less attention to the interaction of local and global features, and
their series architectures are rigid to reflect local and global relationships.
To address these issues, this paper proposes InterFormer for interactive local
and global features fusion to learn a better representation for ASR.
Specifically, we combine the convolution block with the transformer block in a
parallel design. Besides, we propose a bidirectional feature interaction module
(BFIM) and a selective fusion module (SFM) to implement the interaction and
fusion of local and global features, respectively. Extensive experiments on
public ASR datasets demonstrate the effectiveness of our proposed InterFormer
and its superior performance over the other Transformer and Conformer models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Distributed Automatic Domain-Specific Multi-Word Term Recognition Architecture using Spark Ecosystem. (arXiv:2305.16343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16343">
<div class="article-summary-box-inner">
<span><p>Automatic Term Recognition is used to extract domain-specific terms that
belong to a given domain. In order to be accurate, these corpus and
language-dependent methods require large volumes of textual data that need to
be processed to extract candidate terms that are afterward scored according to
a given metric. To improve text preprocessing and candidate terms extraction
and scoring, we propose a distributed Spark-based architecture to automatically
extract domain-specific terms. The main contributions are as follows: (1)
propose a novel distributed automatic domain-specific multi-word term
recognition architecture built on top of the Spark ecosystem; (2) perform an
in-depth analysis of our architecture in terms of accuracy and scalability; (3)
design an easy-to-integrate Python implementation that enables the use of Big
Data processing in fields such as Computational Linguistics and Natural
Language Processing. We prove empirically the feasibility of our architecture
by performing experiments on two real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset. (arXiv:2305.16344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16344">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) demonstrate exceptional performance in textual
understanding and tabular reasoning tasks. However, their ability to comprehend
and analyze hybrid text, containing textual and tabular data, remains
underexplored. In this research, we specialize in harnessing the potential of
LLMs to comprehend critical information from financial reports, which are
hybrid long-documents. We propose an Automated Financial Information Extraction
(AFIE) framework that enhances LLMs' ability to comprehend and extract
information from financial reports. To evaluate AFIE, we develop a Financial
Reports Numerical Extraction (FINE) dataset and conduct an extensive
experimental analysis. Our framework is effectively validated on GPT-3.5 and
GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,
compared to a naive method. These results suggest that the AFIE framework
offers accuracy for automated numerical extraction from complex, hybrid
documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexinvariant Language Models. (arXiv:2305.16349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16349">
<div class="article-summary-box-inner">
<span><p>Token embeddings, a mapping from discrete lexical symbols to continuous
vectors, are at the heart of any language model (LM). However, lexical symbol
meanings can also be determined and even redefined by their structural role in
a long context. In this paper, we ask: is it possible for a language model to
be performant without \emph{any} fixed token embeddings? Such a language model
would have to rely entirely on the co-occurence and repetition of tokens in the
context rather than the \textit{a priori} identity of any token. To answer
this, we study \textit{lexinvariant}language models that are invariant to
lexical symbols and therefore do not need fixed token embeddings in practice.
First, we prove that we can construct a lexinvariant LM to converge to the true
language model at a uniform rate that is polynomial in terms of the context
length, with a constant factor that is sublinear in the vocabulary size.
Second, to build a lexinvariant LM, we simply encode tokens using random
Gaussian vectors, such that each token maps to the same representation within
each sequence but different representations across sequences. Empirically, we
demonstrate that it can indeed attain perplexity comparable to that of a
standard language model, given a sufficiently long context. We further explore
two properties of the lexinvariant language models: First, given text generated
from a substitution cipher of English, it implicitly implements Bayesian
in-context deciphering and infers the mapping to the underlying real tokens
with high accuracy. Second, it has on average 4X better accuracy over synthetic
in-context reasoning tasks. Finally, we discuss regularizing standard language
models towards lexinvariance and potential practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion. (arXiv:2305.16353v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16353">
<div class="article-summary-box-inner">
<span><p>Audio Deepfake Detection (ADD) aims to detect the fake audio generated by
text-to-speech (TTS), voice conversion (VC) and replay, etc., which is an
emerging topic. Traditionally we take the mono signal as input and focus on
robust feature extraction and effective classifier design. However, the
dual-channel stereo information in the audio signal also includes important
cues for deepfake, which has not been studied in the prior work. In this paper,
we propose a novel ADD model, termed as M2S-ADD, that attempts to discover
audio authenticity cues during the mono-to-stereo conversion process. We first
projects the mono to a stereo signal using a pretrained stereo synthesizer,
then employs a dual-branch neural architecture to process the left and right
channel signals, respectively. In this way, we effectively reveal the artifacts
in the fake audio, thus improve the ADD performance. The experiments on the
ASVspoof2019 database show that M2S-ADD outperforms all baselines that input
mono. We release the source code at \url{https://github.com/AI-S2-Lab/M2S-ADD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PandaGPT: One Model To Instruction-Follow Them All. (arXiv:2305.16355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16355">
<div class="article-summary-box-inner">
<span><p>We present PandaGPT, an approach to emPower large lANguage moDels with visual
and Auditory instruction-following capabilities. Our pilot experiments show
that PandaGPT can perform complex tasks such as detailed image description
generation, writing stories inspired by videos, and answering questions about
audios. More interestingly, PandaGPT can take multimodal inputs simultaneously
and compose their semantics naturally. For example, PandaGPT can connect how
objects look in an image/video and how they sound in an audio. To do so,
PandaGPT combines the multimodal encoders from ImageBind and the large language
models from Vicuna. Notably, only aligned image-text pairs are required for the
training of PandaGPT. Thanks to the strong capability of ImageBind in embedding
data from different modalities into the same space, PandaGPT displays emergent,
i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g.,
video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an
initial step toward building AGI that can perceive and understand inputs in
different modalities holistically, as we humans do. Our project page is at
https://panda-gpt.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDM3: Event Detection as Multi-task Text Generation. (arXiv:2305.16357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16357">
<div class="article-summary-box-inner">
<span><p>Event detection refers to identifying event occurrences in a text and
comprises of two subtasks; event identification and classification. We present
EDM3, a novel approach for Event Detection that formulates three generative
tasks: identification, classification, and combined detection. We show that
EDM3 helps to learn transferable knowledge that can be leveraged to perform
Event Detection and its subtasks concurrently, mitigating the error propagation
inherent in pipelined approaches. Unlike previous dataset- or domain-specific
approaches, EDM3 utilizes the existing knowledge of language models, allowing
it to be trained over any classification schema. We evaluate EDM3 on multiple
event detection datasets: RAMS, WikiEvents, MAVEN, and MLEE, showing that EDM3
outperforms 1) single-task performance by 8.4% on average and 2) multi-task
performance without instructional prompts by 2.4% on average. We obtain SOTA
results on RAMS (71.3% vs. 65.1% F-1) and competitive performance on other
datasets. We analyze our approach to demonstrate its efficacy in low-resource
and multi-sentence settings. We also show the effectiveness of this approach on
non-standard event configurations such as multi-word and multi-class event
triggers. Overall, our results show that EDM3 is a promising approach for Event
Detection that has the potential for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving. (arXiv:2305.16366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16366">
<div class="article-summary-box-inner">
<span><p>Large language models~(LLMs) present an intriguing avenue of exploration in
the domain of formal theorem proving. Nonetheless, the full utilization of
these models, particularly in terms of demonstration formatting and
organization, remains an underexplored area. In an endeavor to enhance the
efficacy of LLMs, we introduce a subgoal-based demonstration learning
framework, consisting of two primary elements: Firstly, drawing upon the
insights of subgoal learning from the domains of reinforcement learning and
robotics, we propose the construction of distinct subgoals for each
demonstration example and refine these subgoals in accordance with the
pertinent theories of subgoal learning. Secondly, we build upon recent advances
in diffusion models to predict the optimal organization, simultaneously
addressing two intricate issues that persist within the domain of demonstration
organization: subset selection and order determination. Through the integration
of subgoal-based learning methodologies, we have successfully increased the
prevailing proof accuracy from 38.9\% to 44.3\% on the miniF2F benchmark.
Furthermore, the adoption of diffusion models for demonstration organization
can lead to an additional enhancement in accuracy to 45.5\%, or a $5\times$
improvement in sampling efficiency compared with the long-standing
state-of-the-art method. Our code is available at
\url{https://github.com/HKUNLP/subgoal-theorem-prover}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Role-Play with Large Language Models. (arXiv:2305.16367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16367">
<div class="article-summary-box-inner">
<span><p>As dialogue agents become increasingly human-like in their performance, it is
imperative that we develop effective ways to describe their behaviour in
high-level terms without falling into the trap of anthropomorphism. In this
paper, we foreground the concept of role-play. Casting dialogue agent behaviour
in terms of role-play allows us to draw on familiar folk psychological terms,
without ascribing human characteristics to language models they in fact lack.
Two important cases of dialogue agent behaviour are addressed this way, namely
(apparent) deception and (apparent) self-awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition. (arXiv:2305.16371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16371">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems have attained unprecedented
performance with large speech models pre-trained based on self-supervised
speech representation learning. However, these pre-trained speech models suffer
from representational bias as they tend to better represent those prominent
accents (i.e., native (L1) English accent) in the pre-training speech corpus
than less represented accents, resulting in a deteriorated performance for
non-native (L2) English accents. Although there have been some approaches to
mitigate this issue, all of these methods require updating the pre-trained
model weights. In this paper, we propose Information Theoretic Adversarial
Prompt Tuning (INTapt), which introduces prompts concatenated to the original
input that can re-modulate the attention of the pre-trained model such that the
corresponding input resembles a native (L1) English speech without updating the
backbone weights. INTapt is trained simultaneously in the following two
manners: (1) adversarial training to reduce accent feature dependence between
the original input and the prompt-concatenated input and (2) training to
minimize CTC loss for improving ASR performance to a prompt-concatenated input.
Experimental results show that INTapt improves the performance of L2 English
and increases feature similarity between L2 and L1 accents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16380">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has shown impressive performance in multiple
research domains and has become the backbone of many neural network models.
However, there is limited understanding on how it works. In particular, with a
simple predictive loss, how the representation emerges from the gradient
\emph{training dynamics} remains a mystery. In this paper, for 1-layer
transformer with one self-attention layer plus one decoder layer, we analyze
its SGD training dynamics for the task of next token prediction in a
mathematically rigorous manner. We open the black box of the dynamic process of
how the self-attention layer combines input tokens, and reveal the nature of
underlying inductive bias. More specifically, with the assumption (a) no
positional encoding, (b) long input sequence, and (c) the decoder layer learns
faster than the self-attention layer, we prove that self-attention acts as a
\emph{discriminative scanning algorithm}: starting from uniform attention, it
gradually attends more to distinct key tokens for a specific next token to be
predicted, and pays less attention to common key tokens that occur across
different next tokens. Among distinct tokens, it progressively drops attention
weights, following the order of low to high co-occurrence between the key and
the query token in the training set. Interestingly, this procedure does not
lead to winner-takes-all, but decelerates due to a \emph{phase transition} that
is controllable by the learning rates of the two layers, leaving (almost) fixed
token combination. We verify this \textbf{\emph{scan and snap}} dynamics on
synthetic and real-world data (WikiText).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16397">
<div class="article-summary-box-inner">
<span><p>Text-conditioned image generation models have recently shown immense
qualitative success using denoising diffusion processes. However, unlike
discriminative vision-and-language models, it is a non-trivial task to subject
these diffusion-based generative models to automatic fine-grained quantitative
evaluation of high-level phenomena such as compositionality. Towards this goal,
we perform two innovations. First, we transform diffusion-based models (in our
case, Stable Diffusion) for any image-text matching (ITM) task using a novel
method called DiffusionITM. Second, we introduce the Generative-Discriminative
Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language
tasks, bias evaluation and detailed analysis. We find that Stable Diffusion +
DiffusionITM is competitive on many tasks and outperforms CLIP on compositional
tasks like like CLEVR and Winoground. We further boost its compositional
performance with a transfer setup by fine-tuning on MS-COCO while retaining
generative capabilities. We also measure the stereotypical bias in diffusion
models, and find that Stable Diffusion 2.1 is, for the most part, less biased
than Stable Diffusion 1.5. Overall, our results point in an exciting direction
bringing discriminative and generative model evaluation closer. We will release
code and benchmark setup soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Attention Layers coupled with Optimal Transport Domain Adaptation methods for recognizing dementia from spontaneous speech. (arXiv:2305.16406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16406">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is
the main cause of dementia. Although many studies have been proposed targeting
at diagnosing dementia through spontaneous speech, there are still limitations.
Existing state-of-the-art approaches, which propose multimodal methods, train
separately language and acoustic models, employ majority-vote approaches, and
concatenate the representations of the different modalities either at the input
level, i.e., early fusion, or during training. Also, some of them employ
self-attention layers, which calculate the dependencies between representations
without considering the contextual information. In addition, no prior work has
taken into consideration the model calibration. To address these limitations,
we propose some new methods for detecting AD patients, which capture the intra-
and cross-modal interactions. First, we convert the audio files into log-Mel
spectrograms, their delta, and delta-delta and create in this way an image per
audio file consisting of three channels. Next, we pass each transcript and
image through BERT and DeiT models respectively. After that, context-based
self-attention layers, self-attention layers with a gate model, and optimal
transport domain adaptation methods are employed for capturing the intra- and
inter-modal interactions. Finally, we exploit two methods for fusing the self
and cross-attended features. For taking into account the model calibration, we
apply label smoothing. We use both performance and calibration metrics.
Experiments conducted on the ADReSS Challenge dataset indicate the efficacy of
our introduced approaches over existing research initiatives with our best
performing model reaching Accuracy and F1-score up to 91.25% and 91.06%
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities. (arXiv:2305.16407v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16407">
<div class="article-summary-box-inner">
<span><p>The wide accessibility of social media has provided linguistically
under-represented communities with an extraordinary opportunity to create
content in their native languages. This, however, comes with certain challenges
in script normalization, particularly where the speakers of a language in a
bilingual community rely on another script or orthography to write their native
language. This paper addresses the problem of script normalization for several
such languages that are mainly written in a Perso-Arabic script. Using
synthetic data with various levels of noise and a transformer-based model, we
demonstrate that the problem can be effectively remediated. We conduct a
small-scale evaluation of real data as well. Our experiments indicate that
script normalization is also beneficial to improve the performance of
downstream tasks such as machine translation and language identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Probing for the influence of affect and specificity on Intergroup Bias. (arXiv:2305.16409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16409">
<div class="article-summary-box-inner">
<span><p>While existing work on studying bias in NLP focues on negative or pejorative
language use, Govindarajan et al. (2023) offer a revised framing of bias in
terms of intergroup social context, and its effects on language behavior. In
this paper, we investigate if two pragmatic features (specificity and affect)
systematically vary in different intergroup contexts -- thus connecting this
new framing of bias to language output. Preliminary analysis finds modest
correlations between specificity and affect of tweets with supervised
intergroup relationship (IGR) labels. Counterfactual probing further reveals
that while neural models finetuned for predicting IGR labels reliably use
affect in classification, the model's usage of specificity is inconclusive.
Code and data can be found at: https://github.com/venkatasg/intergroup-probing
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models. (arXiv:2305.16426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16426">
<div class="article-summary-box-inner">
<span><p>Vector space models of word meaning all share the assumption that words
occurring in similar contexts have similar meanings. In such models, words that
are similar in their topical associations but differ in their logical force
tend to emerge as semantically close, creating well-known challenges for NLP
applications that involve logical reasoning. Modern pretrained language models,
such as BERT, RoBERTa and GPT-3 hold the promise of performing better on
logical tasks than classic static word embeddings. However, reports are mixed
about their success. In the current paper, we advance this discussion through a
systematic study of scalar adverbs, an under-explored class of words with
strong logical force. Using three different tasks, involving both naturalistic
social media data and constructed examples, we investigate the extent to which
BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these
common words. We ask: 1) Do the models distinguish amongst the three semantic
categories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicit
representations of full scales from maximally negative to maximally positive?
3) How do word frequency and contextual factors impact model performance? We
find that despite capturing some aspects of logical meaning, the models fall
far short of human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation for Mathematical Formulae. (arXiv:2305.16433v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16433">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of neural machine translation of mathematical formulae
between ambiguous presentation languages and unambiguous content languages.
Compared to neural machine translation on natural language, mathematical
formulae have a much smaller vocabulary and much longer sequences of symbols,
while their translation requires extreme precision to satisfy mathematical
information needs. In this work, we perform the tasks of translating from LaTeX
to Mathematica as well as from LaTeX to semantic LaTeX. While recurrent,
recursive, and transformer networks struggle with preserving all contained
information, we find that convolutional sequence-to-sequence networks achieve
95.1% and 90.7% exact matches, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text. (arXiv:2305.16444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16444">
<div class="article-summary-box-inner">
<span><p>Can language models transform inputs to protect text classifiers against
adversarial attacks? In this work, we present ATINTER, a model that intercepts
and learns to rewrite adversarial inputs to make them non-adversarial for a
downstream text classifier. Our experiments on four datasets and five attack
mechanisms reveal that ATINTER is effective at providing better adversarial
robustness than existing defense approaches, without compromising task
accuracy. For example, on sentiment classification using the SST-2 dataset, our
method improves the adversarial accuracy over the best existing defense
approach by more than 4% with a smaller decrease in task accuracy (0.5% vs
2.5%). Moreover, we show that ATINTER generalizes across multiple downstream
tasks and classifiers without having to explicitly retrain it for those
settings. Specifically, we find that when ATINTER is trained to remove
adversarial perturbations for the sentiment classification task on the SST-2
dataset, it even transfers to a semantically different task of news
classification (on AGNews) and improves the adversarial robustness by more than
10%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Effect of Influential Messages on Varying Personas. (arXiv:2305.16470v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16470">
<div class="article-summary-box-inner">
<span><p>Predicting how a user responds to news events enables important applications
such as allowing intelligent agents or content producers to estimate the effect
on different communities and revise unreleased messages to prevent unexpected
bad outcomes such as social conflict and moral injury. We present a new task,
Response Forecasting on Personas for News Media, to estimate the response a
persona (characterizing an individual or a group) might have upon seeing a news
message. Compared to the previous efforts which only predict generic comments
to news, the proposed task not only introduces personalization in the modeling
but also predicts the sentiment polarity and intensity of each response. This
enables more accurate and comprehensive inference on the mental state of the
persona. Meanwhile, the generated sentiment dimensions make the evaluation and
application more reliable. We create the first benchmark dataset, which
consists of 13,357 responses to 3,847 news headlines from Twitter. We further
evaluate the SOTA neural language models with our dataset. The empirical
results suggest that the included persona attributes are helpful for the
performance of all response dimensions. Our analysis shows that the
best-performing models are capable of predicting responses that are consistent
with the personas, and as a byproduct, the task formulation also enables many
interesting applications in the analysis of social network groups and their
opinions, such as the discovery of extreme opinion groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype-Based Interpretability for Legal Citation Prediction. (arXiv:2305.16490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16490">
<div class="article-summary-box-inner">
<span><p>Deep learning has made significant progress in the past decade, and
demonstrates potential to solve problems with extensive social impact. In
high-stakes decision making areas such as law, experts often require
interpretability for automatic systems to be utilized in practical settings. In
this work, we attempt to address these requirements applied to the important
problem of legal citation prediction (LCP). We design the task with parallels
to the thought-process of lawyers, i.e., with reference to both precedents and
legislative provisions. After initial experimental results, we refine the
target citation predictions with the feedback of legal experts. Additionally,
we introduce a prototype architecture to add interpretability, achieving strong
performance while adhering to decision parameters used by lawyers. Our study
builds on and leverages the state-of-the-art language processing models for
law, while addressing vital considerations for high-stakes tasks with practical
societal impact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks. (arXiv:2305.16503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16503">
<div class="article-summary-box-inner">
<span><p>Backdoor attacks are an insidious security threat against machine learning
models. Adversaries can manipulate the predictions of compromised models by
inserting triggers into the training phase. Various backdoor attacks have been
devised which can achieve nearly perfect attack success without affecting model
predictions for clean inputs. Means of mitigating such vulnerabilities are
underdeveloped, especially in natural language processing. To fill this gap, we
introduce IMBERT, which uses either gradients or self-attention scores derived
from victim models to self-defend against backdoor attacks at inference time.
Our empirical studies demonstrate that IMBERT can effectively identify up to
98.5% of inserted triggers. Thus, it significantly reduces the attack success
rate while attaining competitive accuracy on the clean dataset across
widespread insertion-based attacks compared to two baselines. Finally, we show
that our approach is model-agnostic, and can be easily ported to several
pre-trained transformer models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Tool Manipulation Capability of Open-source Large Language Models. (arXiv:2305.16504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16504">
<div class="article-summary-box-inner">
<span><p>Recent studies on software tool manipulation with large language models
(LLMs) mostly rely on closed model APIs. The industrial adoption of these
models is substantially constrained due to the security and robustness risks in
exposing information to closed LLM API services. In this paper, we ask can we
enhance open-source LLMs to be competitive to leading closed LLM APIs in tool
manipulation, with practical amount of human supervision. By analyzing common
tool manipulation failures, we first demonstrate that open-source LLMs may
require training with usage examples, in-context demonstration and generation
style regulation to resolve failures. These insights motivate us to revisit
classical methods in LLM literature, and demonstrate that we can adapt them as
model alignment with programmatic data generation, system prompts and
in-context demonstration retrievers to enhance open-source LLMs for tool
manipulation. To evaluate these techniques, we create the ToolBench, a tool
manipulation benchmark consisting of diverse software tools for real-world
tasks. We demonstrate that our techniques can boost leading open-source LLMs by
up to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4
out of 8 ToolBench tasks. We show that such enhancement typically requires
about one developer day to curate data for each tool, rendering a recipe with
practical amount of human supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering. (arXiv:2305.16519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16519">
<div class="article-summary-box-inner">
<span><p>Large language models are known to produce output which sounds fluent and
convincing, but is also often wrong, e.g. "unfaithful" with respect to a
rationale as retrieved from a knowledge base. In this paper, we show that
task-based systems which exhibit certain advanced linguistic dialog behaviors,
such as lexical alignment (repeating what the user said), are in fact preferred
and trusted more, whereas other phenomena, such as pronouns and ellipsis are
dis-preferred. We use open-domain question answering systems as our test-bed
for task based dialog generation and compare several open- and closed-book
models. Our results highlight the danger of systems that appear to be
trustworthy by parroting user input while providing an unfaithful response.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Agnostic Pre-training for Zero-shot Text Classification. (arXiv:2305.16521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16521">
<div class="article-summary-box-inner">
<span><p>Conventional approaches to text classification typically assume the existence
of a fixed set of predefined labels to which a given text can be classified.
However, in real-world applications, there exists an infinite label space for
describing a given text. In addition, depending on the aspect (sentiment,
topic, etc.) and domain of the text (finance, legal, etc.), the interpretation
of the label can vary greatly. This makes the task of text classification,
particularly in the zero-shot scenario, extremely challenging. In this paper,
we investigate the task of zero-shot text classification with the aim of
improving the ability of pre-trained language models (PLMs) to generalize to
both seen and unseen data across varying aspects and domains. To solve this we
introduce two new simple yet effective pre-training strategies, Implicit and
Explicit pre-training. These methods inject aspect-level understanding into the
model at train time with the goal of conditioning the model to build task-level
understanding. To evaluate this, we construct and release UTCD, a new benchmark
dataset for evaluating text classification in zero-shot settings. Experimental
results on UTCD show that our approach achieves improved zero-shot
generalization on a suite of challenging datasets across an array of zero-shot
formalizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization. (arXiv:2305.16548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16548">
<div class="article-summary-box-inner">
<span><p>A series of datasets and models have been proposed for summaries generated
for well-formatted documents such as news articles. Dialogue summaries,
however, have been under explored. In this paper, we present the first dataset
with fine-grained factual error annotations named DIASUMFACT. We define
fine-grained factual error detection as a sentence-level multi-label
classification problem, and we evaluate two state-of-the-art (SOTA) models on
our dataset. Both models yield sub-optimal results, with a macro-averaged F1
score of around 0.25 over 6 error classes. We further propose an unsupervised
model ENDERANKER via candidate ranking using pretrained encoder-decoder models.
Our model performs on par with the SOTA models while requiring fewer resources.
These observations confirm the challenges in detecting factual errors from
dialogue summaries, which call for further studies, for which our dataset and
results offer a solid foundation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction. (arXiv:2305.16559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16559">
<div class="article-summary-box-inner">
<span><p>Class-incremental learning (CIL) aims to develop a learning system that can
continually learn new classes from a data stream without forgetting previously
learned classes. When learning classes incrementally, the classifier must be
constantly updated to incorporate new classes, and the drift in decision
boundary may lead to severe forgetting. This fundamental challenge, however,
has not yet been studied extensively, especially in the setting where no
samples from old classes are stored for rehearsal. In this paper, we take a
closer look at how the drift in the classifier leads to forgetting, and
accordingly, design four simple yet (super-) effective solutions to alleviate
the classifier drift: an Individual Classifiers with Frozen Feature Extractor
(ICE) framework where we individually train a classifier for each learning
session, and its three variants ICE-PL, ICE-O, and ICE-PL&amp;O which further take
the logits of previously learned classes from old sessions or a constant logit
of an Other class as a constraint to the learning of new classifiers. Extensive
experiments and analysis on 6 class-incremental information extraction tasks
demonstrate that our solutions, especially ICE-O, consistently show significant
improvement over the previous state-of-the-art approaches with up to 44.7%
absolute F-score gain, providing a strong baseline and insights for future
research on class-incremental learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios. (arXiv:2305.16572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16572">
<div class="article-summary-box-inner">
<span><p>Current pre-trained language models have enabled remarkable improvements in
downstream tasks, but it remains difficult to distinguish effects of
statistical correlation from more systematic logical reasoning grounded on the
understanding of real world. We tease these factors apart by leveraging
counterfactual conditionals, which force language models to predict unusual
consequences based on hypothetical propositions. We introduce a set of tests
from psycholinguistic experiments, as well as larger-scale controlled datasets,
to probe counterfactual predictions from five pre-trained language models. We
find that models are consistently able to override real-world knowledge in
counterfactual scenarios, and that this effect is more robust in case of
stronger baseline world knowledge -- however, we also find that for most models
this effect appears largely to be driven by simple lexical cues. When we
mitigate effects of both world knowledge and lexical cues to test knowledge of
linguistic nuances of counterfactuals, we find that only GPT-3 shows
sensitivity to these nuances, though this sensitivity is also non-trivially
impacted by lexical associative factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases. (arXiv:2305.16577v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16577">
<div class="article-summary-box-inner">
<span><p>Through the use of first name substitution experiments, prior research has
demonstrated the tendency of social commonsense reasoning models to
systematically exhibit social biases along the dimensions of race, ethnicity,
and gender (An et al., 2023). Demographic attributes of first names, however,
are strongly correlated with corpus frequency and tokenization length, which
may influence model behavior independent of or in addition to demographic
factors. In this paper, we conduct a new series of first name substitution
experiments that measures the influence of these factors while controlling for
the others. We find that demographic attributes of a name (race, ethnicity, and
gender) and name tokenization length are both factors that systematically
affect the behavior of social commonsense reasoning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16579">
<div class="article-summary-box-inner">
<span><p>As natural language processing (NLP) has recently seen an unprecedented level
of excitement, and more people are eager to enter the field, it is unclear
whether current research reproducibility efforts are sufficient for this group
of beginners to apply the latest developments. To understand their needs, we
conducted a study with 93 students in an introductory NLP course, where
students reproduced the results of recent NLP papers. Surprisingly, we find
that their programming skill and comprehension of research papers have a
limited impact on their effort spent completing the exercise. Instead, we find
accessibility efforts by research authors to be the key to success, including
complete documentation, better coding practice, and easier access to data
files. Going forward, we recommend that NLP researchers pay close attention to
these simple aspects of open-sourcing their work, and use insights from
beginners' feedback to provide actionable ideas on how to better support them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Investigation of Noise in Morphological Inflection. (arXiv:2305.16581v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16581">
<div class="article-summary-box-inner">
<span><p>With a growing focus on morphological inflection systems for languages where
high-quality data is scarce, training data noise is a serious but so far
largely ignored concern. We aim at closing this gap by investigating the types
of noise encountered within a pipeline for truly unsupervised morphological
paradigm completion and its impact on morphological inflection systems: First,
we propose an error taxonomy and annotation pipeline for inflection training
data. Then, we compare the effect of different types of noise on multiple
state-of-the-art inflection models. Finally, we propose a novel character-level
masked language modeling (CMLM) pretraining objective and explore its impact on
the models' resistance to noise. Our experiments show that various
architectures are impacted differently by separate types of noise, but
encoder-decoders tend to be more robust to noise than models trained with a
copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models. (arXiv:2305.16582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16582">
<div class="article-summary-box-inner">
<span><p>With the widespread use of large language models (LLMs) in NLP tasks,
researchers have discovered the potential of Chain-of-thought (CoT) to assist
LLMs in accomplishing complex reasoning tasks by generating intermediate steps.
However, human thought processes are often non-linear, rather than simply
sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)
reasoning, which models human thought processes not only as a chain but also as
a graph. By representing thought units as nodes and connections between them as
edges, our approach captures the non-sequential nature of human thinking and
allows for a more realistic modeling of thought processes. Similar to
Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating
rationales first and then producing the final answer. Specifically, we employ
an additional graph-of-thoughts encoder for GoT representation learning and
fuse the GoT representation with the original input representation through a
gated fusion mechanism. We implement a GoT reasoning model on the T5
pre-trained model and evaluate its performance on a text-only reasoning task
(GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves
significant improvement over the strong CoT baseline with 3.41% and 5.08% on
the GSM8K test set with T5-base and T5-large architectures, respectively.
Additionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base
model and from 91.68% to 92.77% using the T5-large model over the
state-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have
shown that GoT achieves comparable results to Multimodal-CoT(large) with over
700M parameters, despite having fewer than 250M backbone model parameters,
demonstrating the effectiveness of GoT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation. (arXiv:2305.16585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16585">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation is a long-standing task in natural language processing
(NLP). Supervised paraphrase generation models, which rely on human-annotated
paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand,
automatically annotated paraphrase pairs (e.g., by machine back-translation),
usually suffer from the lack of syntactic diversity -- the generated paraphrase
sentences are very similar to the source sentences in terms of syntax. In this
work, we present ParaAMR, a large-scale syntactically diverse paraphrase
dataset created by abstract meaning representation back-translation. Our
quantitative analysis, qualitative examples, and human evaluation demonstrate
that the paraphrases of ParaAMR are syntactically more diverse compared to
existing large-scale paraphrase datasets while preserving good semantic
similarity. In addition, we show that ParaAMR can be used to improve on three
NLP tasks: learning sentence embeddings, syntactically controlled paraphrase
generation, and data augmentation for few-shot learning. Our results thus
showcase the potential of ParaAMR for improving various NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16597">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient tuning (PET) methods fit pre-trained language models
(PLMs) to downstream tasks by either computing a small compressed update for a
subset of model parameters, or appending and fine-tuning a small number of new
model parameters to the pre-trained network. Hand-designed PET architectures
from the literature perform well in practice, but have the potential to be
improved via automated neural architecture search (NAS). We propose an
efficient NAS method for learning PET architectures via structured and
unstructured pruning. We present experiments on GLUE demonstrating the
effectiveness of our algorithm and discuss how PET architectural design choices
affect performance in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery. (arXiv:2305.16598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16598">
<div class="article-summary-box-inner">
<span><p>Norms, which are culturally accepted guidelines for behaviours, can be
integrated into conversational models to generate utterances that are
appropriate for the socio-cultural context. Existing methods for norm
recognition tend to focus only on surface-level features of dialogues and do
not take into account the interactions within a conversation. To address this
issue, we propose NormMark, a probabilistic generative Markov model to carry
the latent features throughout a dialogue. These features are captured by
discrete and continuous latent variables conditioned on the conversation
history, and improve the model's ability in norm recognition. The model is
trainable on weakly annotated data using the variational technique. On a
dataset with limited norm annotations, we show that our approach achieves
higher F1 score, outperforming current state-of-the-art methods, including
GPT3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation. (arXiv:2305.16599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16599">
<div class="article-summary-box-inner">
<span><p>$k$-Nearest neighbor machine translation ($k$NN-MT) has attracted increasing
attention due to its ability to non-parametrically adapt to new translation
domains. By using an upstream NMT model to traverse the downstream training
corpus, it is equipped with a datastore containing vectorized key-value pairs,
which are retrieved during inference to benefit translation. However, there
often exists a significant gap between upstream and downstream domains, which
hurts the retrieval accuracy and the final translation quality. To deal with
this issue, we propose a novel approach to boost the datastore retrieval of
$k$NN-MT by reconstructing the original datastore. Concretely, we design a
reviser to revise the key representations, making them better fit for the
downstream domain. The reviser is trained using the collected
semantically-related key-queries pairs, and optimized by two proposed losses:
one is the key-queries semantic distance ensuring each revised key
representation is semantically related to its corresponding queries, and the
other is an L2-norm loss encouraging revised key representations to effectively
retain the knowledge learned by the upstream NMT model. Extensive experiments
on domain adaptation tasks demonstrate that our method can effectively boost
the datastore retrieval and translation quality of $k$NN-MT.\footnote{Our code
is available at \url{https://github.com/DeepLearnXMU/RevisedKey-knn-mt}.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model. (arXiv:2305.16617v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16617">
<div class="article-summary-box-inner">
<span><p>The detection of machine-generated text, especially from large language
models (LLMs), is crucial in preventing serious social problems resulting from
their misuse. Some methods train dedicated detectors on specific datasets but
fall short in generalizing to unseen test data, while other zero-shot ones
often yield suboptimal performance. Although the recent DetectGPT has shown
promising detection performance, it suffers from significant inefficiency
issues, as detecting a single candidate requires scoring hundreds of its
perturbations with the source LLM. This paper aims to bridge this gap.
Technically, we propose to incorporate a Bayesian surrogate model, which allows
us to select typical samples based on Bayesian uncertainty and interpolate
scores from typical samples to other ones, to improve query efficiency. Our
empirical results demonstrate that our method significantly outperforms
existing approaches under a low query budget. Notably, our method achieves
similar performance with up to 2 times fewer queries than DetectGPT and 3.7%
higher AUROC at a query number of 5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Question Generation Needs More References. (arXiv:2305.16626v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16626">
<div class="article-summary-box-inner">
<span><p>Question generation (QG) is the task of generating a valid and fluent
question based on a given context and the target answer. According to various
purposes, even given the same context, instructors can ask questions about
different concepts, and even the same concept can be written in different ways.
However, the evaluation for QG usually depends on single reference-based
similarity metrics, such as n-gram-based metric or learned metric, which is not
sufficient to fully evaluate the potential of QG methods. To this end, we
propose to paraphrase the reference question for a more robust QG evaluation.
Using large language models such as GPT-3, we created semantically and
syntactically diverse questions, then adopt the simple aggregation of the
popular evaluation metrics as the final scores. Through our experiments, we
found that using multiple (pseudo) references is more effective for QG
evaluation while showing a higher correlation with human evaluations than
evaluation with a single reference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks. (arXiv:2305.16633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16633">
<div class="article-summary-box-inner">
<span><p>Recently large language models (LLMs) like ChatGPT have shown impressive
performance on many natural language processing tasks with zero-shot. In this
paper, we investigate the effectiveness of zero-shot LLMs in the financial
domain. We compare the performance of ChatGPT along with some open-source
generative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We
address three inter-related research questions on data annotation, performance
gaps, and the feasibility of employing generative models in the finance domain.
Our findings demonstrate that ChatGPT performs well even without labeled data
but fine-tuned models generally outperform it. Our research also highlights how
annotating with generative models can be time-intensive. Our codebase is
publicly available on GitHub under CC BY-NC 4.0 license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing. (arXiv:2305.16635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16635">
<div class="article-summary-box-inner">
<span><p>It is commonly perceived that the strongest language models (LMs) rely on a
combination of massive scale, instruction data, and human feedback to perform
specialized tasks -- e.g. summarization and paraphrasing, without supervision.
In this paper, we propose that language models can learn to summarize and
paraphrase sentences, with none of these 3 factors. We present Impossible
Distillation, a framework that distills a task-specific dataset directly from
an off-the-shelf LM, even when it is impossible for the LM itself to reliably
solve the task. By training a student model on the generated dataset and
amplifying its capability through self-distillation, our method yields a
high-quality model and dataset from a low-quality teacher model, without the
need for scale or supervision. Using Impossible Distillation, we are able to
distill an order of magnitude smaller model (with only 770M parameters) that
outperforms 175B parameter GPT-3, in both quality and controllability, as
confirmed by automatic and human evaluations. Furthermore, as a useful
byproduct of our approach, we obtain DIMSUM+, a high-quality dataset with 3.4M
sentence summaries and paraphrases. Our analyses show that this dataset, as a
purely LM-generated corpus, is more diverse and more effective for
generalization to unseen domains than all human-authored datasets -- including
Gigaword with 4M samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions. (arXiv:2305.16636v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16636">
<div class="article-summary-box-inner">
<span><p>Modern machine learning relies on datasets to develop and validate research
ideas. Given the growth of publicly available data, finding the right dataset
to use is increasingly difficult. Any research question imposes explicit and
implicit constraints on how well a given dataset will enable researchers to
answer this question, such as dataset size, modality, and domain. We introduce
a new task of recommending relevant datasets given a short natural language
description of a research idea, to help people find relevant datasets for their
needs. Dataset recommendation poses unique challenges as an information
retrieval problem; datasets are hard to directly index for search and there are
no corpora readily available for this task. To operationalize this task, we
build the DataFinder Dataset which consists of a larger
automatically-constructed training set (17.5K queries) and a smaller
expert-annotated evaluation set (392 queries). Using this data, we compare
various information retrieval algorithms on our test set and present the
first-ever published system for text-based dataset recommendation using machine
learning techniques. This system, trained on the DataFinder Dataset, finds more
relevant search results than existing third-party dataset search engines. To
encourage progress on dataset recommendation, we release our dataset and models
to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Multi-task Learning for End-to-end Metaphor Detection. (arXiv:2305.16638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16638">
<div class="article-summary-box-inner">
<span><p>Metaphor detection (MD) suffers from limited training data. In this paper, we
started with a linguistic rule called Metaphor Identification Procedure and
then proposed a novel multi-task learning framework to transfer knowledge in
basic sense discrimination (BSD) to MD. BSD is constructed from word sense
disambiguation (WSD), which has copious amounts of data. We leverage
adversarial training to align the data distributions of MD and BSD in the same
feature space, so task-invariant representations can be learned. To capture
fine-grained alignment patterns, we utilize the multi-mode structures of MD and
BSD. Our method is totally end-to-end and can mitigate the data scarcity
problem in MD. Competitive results are reported on four public datasets. Our
code and datasets are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales. (arXiv:2305.16641v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16641">
<div class="article-summary-box-inner">
<span><p>Social biases and stereotypes are embedded in our culture in part through
their presence in our stories, as evidenced by the rich history of humanities
and social science literature analyzing such biases in children stories.
Because these analyses are often conducted manually and at a small scale, such
investigations can benefit from the use of more recent natural language
processing methods that examine social bias in models and data corpora. Our
work joins this interdisciplinary effort and makes a unique contribution by
taking into account the event narrative structures when analyzing the social
bias of stories. We propose a computational pipeline that automatically
extracts a story's temporal narrative verb-based event chain for each of its
characters as well as character attributes such as gender. We also present a
verb-based event annotation scheme that can facilitate bias analysis by
including categories such as those that align with traditional stereotypes.
Through a case study analyzing gender bias in fairy tales, we demonstrate that
our framework can reveal bias in not only the unigram verb-based events in
which female and male characters participate but also in the temporal narrative
order of such event participation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16646">
<div class="article-summary-box-inner">
<span><p>Large language models have shown astonishing performance on a wide range of
reasoning tasks. In this paper, we investigate whether they could reason about
real-world events and help improve the prediction accuracy of event sequence
models. We design a modeling and prediction framework where a large language
model performs abductive reasoning to assist an event sequence model: the event
model proposes predictions on future events given the past; instructed by a few
expert-annotated demonstrations, the language model learns to suggest possible
causes for each proposal; a search module finds out the previous events that
match the causes; a scoring function learns to examine whether the retrieved
events could actually cause the proposal. Through extensive experiments on two
challenging real-world datasets (Amazon Review and GDELT), we demonstrate that
our framework -- thanks to the reasoning ability of language models -- could
significantly outperform the state-of-the-art event sequence models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dramatic Conversation Disentanglement. (arXiv:2305.16648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16648">
<div class="article-summary-box-inner">
<span><p>We present a new dataset for studying conversation disentanglement in movies
and TV series. While previous work has focused on conversation disentanglement
in IRC chatroom dialogues, movies and TV shows provide a space for studying
complex pragmatic patterns of floor and topic change in face-to-face
multi-party interactions. In this work, we draw on theoretical research in
sociolinguistics, sociology, and film studies to operationalize a
conversational thread (including the notion of a floor change) in dramatic
texts, and use that definition to annotate a dataset of 10,033 dialogue turns
(comprising 2,209 threads) from 831 movies. We compare the performance of
several disentanglement models on this dramatic dataset, and apply the
best-performing model to disentangle 808 movies. We see that, contrary to
expectation, average thread lengths do not decrease significantly over the past
40 years, and characters portrayed by actors who are women, while
underrepresented, initiate more new conversational threads relative to their
speaking time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TADA: Task-Agnostic Dialect Adapters for English. (arXiv:2305.16651v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16651">
<div class="article-summary-box-inner">
<span><p>Large Language Models, the dominant starting point for Natural Language
Processing (NLP) applications, fail at a higher rate for speakers of English
dialects other than Standard American English (SAE). Prior work addresses this
using task-specific data or synthetic data augmentation, both of which require
intervention for each dialect and task pair. This poses a scalability issue
that prevents the broad adoption of robust dialectal English NLP. We introduce
a simple yet effective method for task-agnostic dialect adaptation by aligning
non-SAE dialects using adapters and composing them with task-specific adapters
from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on
4 dialectal variants of the GLUE benchmark without task-specific supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16653">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently demonstrated the potential in
acting as autonomous agents for sequential decision-making tasks. However, most
existing methods either take actions greedily without planning or rely on
static plans that are not adaptable to environmental feedback. Consequently,
the sequential decision-making performance of LLM agents degenerates with
problem complexity and plan horizons increase. We propose a closed-loop
approach, AdaPlanner, which allows the LLM agent to refine its self-generated
plan adaptively in response to environmental feedback. In AdaPlanner, the LLM
agent adaptively refines its plan from feedback with both in-plan and
out-of-plan refinement strategies. To mitigate hallucination, we develop a
code-style LLM prompt structure that facilitates plan generation across a
variety of tasks, environments, and agent capabilities. Furthermore, we propose
a skill discovery mechanism that leverages successful plans as few-shot
exemplars, enabling the agent to plan and refine with fewer task
demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments
demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and
4.11% while utilizing 2x and 600x fewer samples, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks. (arXiv:2305.16663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16663">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) tasks show promising performance in extracting
relations from two entities mentioned in sentences, given sufficient
annotations available during training. Such annotations would be
labor-intensive to obtain in practice. Existing work adopts data augmentation
techniques to generate pseudo-annotated sentences beyond limited annotations.
These techniques neither preserve the semantic consistency of the original
sentences when rule-based augmentations are adopted, nor preserve the syntax
structure of sentences when expressing relations using seq2seq models,
resulting in less diverse augmentations. In this work, we propose a dedicated
augmentation technique for relational texts, named GDA, which uses two
complementary modules to preserve both semantic consistency and syntax
structures. We adopt a generative formulation and design a multi-tasking
solution to achieve synergies. Furthermore, GDA adopts entity hints as the
prior knowledge of the generative model to augment diverse sentences.
Experimental results in three datasets under a low-resource setting showed that
GDA could bring {\em 2.0\%} F1 improvements compared with no augmentation
technique. Source code and data are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Score-balanced Loss for Multi-aspect Pronunciation Assessment. (arXiv:2305.16664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16664">
<div class="article-summary-box-inner">
<span><p>With rapid technological growth, automatic pronunciation assessment has
transitioned toward systems that evaluate pronunciation in various aspects,
such as fluency and stress. However, despite the highly imbalanced score labels
within each aspect, existing studies have rarely tackled the data imbalance
problem. In this paper, we suggest a novel loss function, score-balanced loss,
to address the problem caused by uneven data, such as bias toward the majority
scores. As a re-weighting approach, we assign higher costs when the predicted
score is of the minority class, thus, guiding the model to gain positive
feedback for sparse score prediction. Specifically, we design two weighting
factors by leveraging the concept of an effective number of samples and using
the ranks of scores. We evaluate our method on the speechocean762 dataset,
which has noticeably imbalanced scores for several aspects. Improved results
particularly on such uneven aspects prove the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Identifiers Enhanced Generative Retrieval. (arXiv:2305.16675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16675">
<div class="article-summary-box-inner">
<span><p>Instead of simply matching a query to pre-existing passages, generative
retrieval generates identifier strings of passages as the retrieval target. At
a cost, the identifier must be distinctive enough to represent a passage.
Current approaches use either a numeric ID or a text piece (such as a title or
substrings) as the identifier. However, these identifiers cannot cover a
passage's content well. As such, we are motivated to propose a new type of
identifier, synthetic identifiers, that are generated based on the content of a
passage and could integrate contextualized information that text pieces lack.
Furthermore, we simultaneously consider multiview identifiers, including
synthetic identifiers, titles, and substrings. These views of identifiers
complement each other and facilitate the holistic ranking of passages from
multiple perspectives. We conduct a series of experiments on three public
datasets, and the results indicate that our proposed approach performs the best
in generative retrieval, demonstrating its effectiveness and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies. (arXiv:2305.16697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16697">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialog (TOD) agents often ground their responses on external
knowledge bases (KBs). These KBs can be dynamic and may be updated frequently.
Existing approaches for learning TOD agents assume the KB snapshot contemporary
to each individual dialog is available during training. However, in real-world
scenarios, only the latest KB snapshot is available during training and as a
result, the train dialogs may contain facts conflicting with the latest KB.
These dialog-KB inconsistencies in the training data may potentially confuse
the TOD agent learning algorithm.
</p>
<p>In this work, we define the novel problem of learning a TOD agent with
dialog-KB inconsistencies in the training data. We propose a Dialog-KB
Arbitration Framework (DKAF) which reduces the dialog-KB inconsistencies by
predicting the contemporary KB snapshot for each train dialog. These predicted
KB snapshots are then used for training downstream TOD agents. As there are no
existing datasets with dialog-KB inconsistencies, we systematically introduce
inconsistencies in two publicly available dialog datasets. We show that TOD
agents trained with DKAF perform better than existing baselines on both these
datasets
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation. (arXiv:2305.16701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16701">
<div class="article-summary-box-inner">
<span><p>Syntactically controlled paraphrase generation requires language models to
generate paraphrases for sentences according to specific syntactic structures.
Existing fine-tuning methods for this task are costly as all the parameters of
the model need to be updated during the training process. Inspired by recent
studies on parameter-efficient learning, we propose Parse-Instructed Prefix
(PIP), a novel adaptation of prefix-tuning to tune large pre-trained language
models on syntactically controlled paraphrase generation task in a low-data
setting with significantly less training cost. We introduce two methods to
instruct a model's encoder prefix to capture syntax-related knowledge: direct
initiation (PIP-Direct) and indirect optimization (PIP-Indirect). In contrast
to traditional fine-tuning methods for this task, PIP is a compute-efficient
alternative with 10 times less learnable parameters. Compared to existing
prefix-tuning methods, PIP excels at capturing syntax control information,
achieving significantly higher performance at the same level of learnable
parameter count.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts. (arXiv:2305.16718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16718">
<div class="article-summary-box-inner">
<span><p>Although pre-trained named entity recognition (NER) models are highly
accurate on modern corpora, they underperform on historical texts due to
differences in language OCR errors. In this work, we develop a new NER corpus
of 3.6M sentences from late medieval charters written mainly in Czech, Latin,
and German.
</p>
<p>We show that we can start with a list of known historical figures and
locations and an unannotated corpus of historical texts, and use information
retrieval techniques to automatically bootstrap a NER-annotated corpus. Using
our corpus, we train a NER model that achieves entity-level Precision of
72.81-93.98% with 58.14-81.77% Recall on a manually-annotated test dataset.
Furthermore, we show that using a weighted loss function helps to combat class
imbalance in token classification tasks. To make it easy for others to
reproduce and build upon our work, we publicly release our corpus, models, and
experimental code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16724">
<div class="article-summary-box-inner">
<span><p>Existing efforts on text synthesis for code-switching mostly require training
on code-switched texts in the target language pairs, limiting the deployment of
the models to cases lacking code-switched data. In this work, we study the
problem of synthesizing code-switched texts for language pairs absent from the
training data. We introduce GLOSS, a model built on top of a pre-trained
multilingual machine translation model (PMMTM) with an additional
code-switching module. This module, either an adapter or extra prefixes, learns
code-switching patterns from code-switched data during training, while the
primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only
adjusting the code-switching module prevents our model from overfitting to the
constrained training data for code-switching. Hence, GLOSS exhibits the ability
to generalize and synthesize code-switched texts across a broader spectrum of
language pairs. Additionally, we develop a self-training algorithm on target
language pairs further to enhance the reliability of GLOSS. Automatic
evaluations on four language pairs show that GLOSS achieves at least 55%
relative BLEU and METEOR scores improvements compared to strong baselines.
Human evaluations on two language pairs further validate the success of GLOSS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank. (arXiv:2305.16726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16726">
<div class="article-summary-box-inner">
<span><p>Unsupervised sentence representation learning is one of the fundamental
problems in natural language processing with various downstream applications.
Recently, contrastive learning has been widely adopted which derives
high-quality sentence representations by pulling similar semantics closer and
pushing dissimilar ones away. However, these methods fail to capture the
fine-grained ranking information among the sentences, where each sentence is
only treated as either positive or negative. In many real-world scenarios, one
needs to distinguish and rank the sentences based on their similarities to a
query sentence, e.g., very relevant, moderate relevant, less relevant,
irrelevant, etc. In this paper, we propose a novel approach, RankCSE, for
unsupervised sentence representation learning, which incorporates ranking
consistency and ranking distillation with contrastive learning into a unified
framework. In particular, we learn semantically discriminative sentence
representations by simultaneously ensuring ranking consistency between two
representations with different dropout masks, and distilling listwise ranking
knowledge from the teacher. An extensive set of experiments are conducted on
both semantic textual similarity (STS) and transfer (TR) tasks. Experimental
results demonstrate the superior performance of our approach over several
state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis. (arXiv:2305.16731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16731">
<div class="article-summary-box-inner">
<span><p>Emotion role labeling aims at extracting who is described in text to
experience an emotion, why, and towards whom. This is often a challenging
modelling task which might be overly sophisticated if the main question to
answer is who feels which emotion. Recently, Troiano et al. (2022) proposed a
data set that focuses on assigning emotion labels and appraisal labels to
individual entities in text and Wegge et al. (2022) presented the first
modelling experiments. Their experiencer-specific emotion prediction model has,
however, only been evaluated on gold-annotated experiencers, due to the
unavailability of an automatic experiencer detection approach. We fill this gap
with the first experiments to automatically detect emotion experiencers in text
and, subsequently, assign them emotions. We show that experiencer detection in
text is a challenging task, with a precision of .82 and a recall of .56 (F1
=.66). Consequently, the performance of the experiencer-specific emotion
detection pipeline drops with these predictions in comparison to using gold
experiencer annotations. This motivates future work of jointly modelling
emotion experiencer detection and emotion/appraisal recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model. (arXiv:2305.16734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16734">
<div class="article-summary-box-inner">
<span><p>Event argument extraction (EAE) identifies event arguments and their specific
roles for a given event. Recent advancement in generation-based EAE models has
shown great performance and generalizability over classification-based models.
However, existing generation-based EAE models mostly focus on problem
re-formulation and prompt design, without incorporating additional information
that has been shown to be effective for classification-based models, such as
the abstract meaning representation (AMR) of the input passages. Incorporating
such information into generation-based models is challenging due to the
heterogeneous nature of the natural language form prevalently used in
generation-based models and the structured form of AMRs. In this work, we study
strategies to incorporate AMR into generation-based EAE models. We propose
AMPERE, which generates AMR-aware prefixes for every layer of the generation
model. Thus, the prefix introduces AMR information to the generation-based EAE
model and then improves the generation. We also introduce an adjusted copy
mechanism to AMPERE to help overcome potential noises brought by the AMR graph.
Comprehensive experiments and analyses on ACE2005 and ERE datasets show that
AMPERE can get 4% - 10% absolute F1 score improvements with reduced training
data and it is in general powerful across different training sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlignScore: Evaluating Factual Consistency with a Unified Alignment Function. (arXiv:2305.16739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16739">
<div class="article-summary-box-inner">
<span><p>Many text generation applications require the generated text to be factually
consistent with input information. Automatic evaluation of factual consistency
is challenging. Previous work has developed various metrics that often depend
on specific functions, such as natural language inference (NLI) or question
answering (QA), trained on limited data. Those metrics thus can hardly assess
diverse factual inconsistencies (e.g., contradictions, hallucinations) that
occur in varying inputs/outputs (e.g., sentences, documents) from different
tasks. In this paper, we propose AlignScore, a new holistic metric that applies
to a variety of factual inconsistency scenarios as above. AlignScore is based
on a general function of information alignment between two arbitrary text
pieces. Crucially, we develop a unified training framework of the alignment
function by integrating a large diversity of data sources, resulting in 4.7M
training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact
verification, information retrieval, semantic similarity, and summarization).
We conduct extensive experiments on large-scale benchmarks including 22
evaluation datasets, where 19 of the datasets were never seen in the alignment
training. AlignScore achieves substantial improvement over a wide range of
previous metrics. Moreover, AlignScore (355M parameters) matches or even
outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude
larger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conjunct Resolution in the Face of Verbal Omissions. (arXiv:2305.16740v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16740">
<div class="article-summary-box-inner">
<span><p>Verbal omissions are complex syntactic phenomena in VP coordination
structures. They occur when verbs and (some of) their arguments are omitted
from subsequent clauses after being explicitly stated in an initial clause.
Recovering these omitted elements is necessary for accurate interpretation of
the sentence, and while humans easily and intuitively fill in the missing
information, state-of-the-art models continue to struggle with this task.
Previous work is limited to small-scale datasets, synthetic data creation
methods, and to resolution methods in the dependency-graph level. In this work
we propose a conjunct resolution task that operates directly on the text and
makes use of a split-and-rephrase paradigm in order to recover the missing
elements in the coordination structure. To this end, we first formulate a
pragmatic framework of verbal omissions which describes the different types of
omissions, and develop an automatic scalable collection method. Based on this
method, we curate a large dataset, containing over 10K examples of
naturally-occurring verbal omissions with crowd-sourced annotations of the
resolved conjuncts. We train various neural baselines for this task, and show
that while our best method obtains decent performance, it leaves ample space
for improvement. We propose our dataset, metrics and models as a starting point
for future research on this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Fine-Tuning without Introducing New Latency. (arXiv:2305.16742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16742">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient fine-tuning (PEFT) of pre-trained language models has
recently demonstrated remarkable achievements, effectively matching the
performance of full fine-tuning while utilizing significantly fewer trainable
parameters, and consequently addressing the storage and communication
constraints. Nonetheless, various PEFT methods are limited by their inherent
characteristics. In the case of sparse fine-tuning, which involves modifying
only a small subset of the existing parameters, the selection of fine-tuned
parameters is task- and domain-specific, making it unsuitable for federated
learning. On the other hand, PEFT methods with adding new parameters typically
introduce additional inference latency. In this paper, we demonstrate the
feasibility of generating a sparse mask in a task-agnostic manner, wherein all
downstream tasks share a common mask. Our approach, which relies solely on the
magnitude information of pre-trained parameters, surpasses existing
methodologies by a significant margin when evaluated on the GLUE benchmark.
Additionally, we introduce a novel adapter technique that directly applies the
adapter to pre-trained parameters instead of the hidden representation, thereby
achieving identical inference speed to that of full fine-tuning. Through
extensive experiments, our proposed method attains a new state-of-the-art
outcome in terms of both performance and storage efficiency, storing only 0.03%
parameters of full fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automating the Analysis of Institutional Design in International Agreements. (arXiv:2305.16750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16750">
<div class="article-summary-box-inner">
<span><p>This paper explores the automatic knowledge extraction of formal
institutional design - norms, rules, and actors - from international
agreements. The focus was to analyze the relationship between the visibility
and centrality of actors in the formal institutional design in regulating
critical aspects of cultural heritage relations. The developed tool utilizes
techniques such as collecting legal documents, annotating them with
Institutional Grammar, and using graph analysis to explore the formal
institutional design. The system was tested against the 2003 UNESCO Convention
for the Safeguarding of the Intangible Cultural Heritage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can large language models generate salient negative statements?. (arXiv:2305.16755v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16755">
<div class="article-summary-box-inner">
<span><p>We examine the ability of large language models (LLMs) to generate salient
(interesting) negative statements about real-world entities; an emerging
research topic of the last few years. We probe the LLMs using zero- and k-shot
unconstrained probes, and compare with traditional methods for negation
generation, i.e., pattern-based textual extractions and knowledge-graph-based
inferences, as well as crowdsourced gold statements. We measure the correctness
and salience of the generated lists about subjects from different domains. Our
evaluation shows that guided probes do in fact improve the quality of generated
negatives, compared to the zero-shot variant. Nevertheless, using both prompts,
LLMs still struggle with the notion of factuality of negatives, frequently
generating many ambiguous statements, or statements with negative keywords but
a positive meaning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16756">
<div class="article-summary-box-inner">
<span><p>Accurate and rapid situation analysis during humanitarian crises is critical
to delivering humanitarian aid efficiently and is fundamental to humanitarian
imperatives and the Leave No One Behind (LNOB) principle. This data analysis
can highly benefit from language processing systems, e.g., by classifying the
text data according to a humanitarian ontology. However, approaching this by
simply fine-tuning a generic large language model (LLM) involves considerable
practical and ethical issues, particularly the lack of effectiveness on
data-sparse and complex subdomains, and the encoding of societal biases and
unwanted associations. In this work, we aim to provide an effective and
ethically-aware system for humanitarian data analysis. We approach this by (1)
introducing a novel architecture adjusted to the humanitarian analysis
framework, (2) creating and releasing a novel humanitarian-specific LLM called
HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our
experiments' results show the better performance of our approach on zero-shot
and full-training settings in comparison with strong baseline models, while
also revealing the existence of biases in the resulting LLMs. Utilizing a
targeted counterfactual data augmentation approach, we significantly reduce
these biases without compromising performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backpack Language Models. (arXiv:2305.16765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16765">
<div class="article-summary-box-inner">
<span><p>We present Backpacks: a new neural architecture that marries strong modeling
performance with an interface for interpretability and control. Backpacks learn
multiple non-contextual sense vectors for each word in a vocabulary, and
represent a word in a sequence as a context-dependent, non-negative linear
combination of sense vectors in this sequence. We find that, after training,
sense vectors specialize, each encoding a different aspect of a word. We can
interpret a sense vector by inspecting its (non-contextual, linear) projection
onto the output space, and intervene on these interpretable hooks to change the
model's behavior in predictable ways. We train a 170M-parameter Backpack
language model on OpenWebText, matching the loss of a GPT-2 small
(124Mparameter) Transformer. On lexical similarity evaluations, we find that
Backpack sense vectors outperform even a 6B-parameter Transformer LM's word
embeddings. Finally, we present simple algorithms that intervene on sense
vectors to perform controllable text generation and debiasing. For example, we
can edit the sense vocabulary to tend more towards a topic, or localize a
source of gender bias to a sense vector and globally suppress that sense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review. (arXiv:2305.16768v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16768">
<div class="article-summary-box-inner">
<span><p>In recent years, pre-trained Multilingual Language Models (MLLMs) have shown
a strong ability to transfer knowledge across different languages. However,
given that the aspiration for such an ability has not been explicitly
incorporated in the design of the majority of MLLMs, it is challenging to
obtain a unique and straightforward explanation for its emergence. In this
review paper, we survey literature that investigates different factors
contributing to the capacity of MLLMs to perform zero-shot cross-lingual
transfer and subsequently outline and discuss these factors in detail. To
enhance the structure of this review and to facilitate consolidation with
future studies, we identify five categories of such factors. In addition to
providing a summary of empirical evidence from past studies, we identify
consensuses among studies with consistent findings and resolve conflicts among
contradictory ones. Our work contextualizes and unifies existing research
streams which aim at explaining the cross-lingual potential of MLLMs. This
review provides, first, an aligned reference point for future research and,
second, guidance for a better-informed and more efficient way of leveraging the
cross-lingual capacity of MLLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16784">
<div class="article-summary-box-inner">
<span><p>For text summarization, the role of discourse structure is pivotal in
discerning the core content of a text. Regrettably, prior studies on
incorporating Rhetorical Structure Theory (RST) into transformer-based
summarization models only consider the nuclearity annotation, thereby
overlooking the variety of discourse relation types. This paper introduces the
'RSTformer', a novel summarization model that comprehensively incorporates both
the types and uncertainty of rhetorical relations. Our RST-attention mechanism,
rooted in document-level rhetorical structure, is an extension of the recently
devised Longformer framework. Through rigorous evaluation, the model proposed
herein exhibits significant superiority over state-of-the-art models, as
evidenced by its notable performance on several automatic metrics and human
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media. (arXiv:2305.16797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16797">
<div class="article-summary-box-inner">
<span><p>In today's fast-paced world, the rates of stress and depression present a
surge. Social media provide assistance for the early detection of mental health
conditions. Existing methods mainly introduce feature extraction approaches and
train shallow machine learning classifiers. Other researches use deep neural
networks or transformers. Despite the fact that transformer-based models
achieve noticeable improvements, they cannot often capture rich factual
knowledge. Although there have been proposed a number of studies aiming to
enhance the pretrained transformer-based models with extra information or
additional modalities, no prior work has exploited these modifications for
detecting stress and depression through social media. In addition, although the
reliability of a machine learning model's confidence in its predictions is
critical for high-risk applications, there is no prior work taken into
consideration the model calibration. To resolve the above issues, we present
the first study in the task of depression and stress detection in social media,
which injects extra linguistic information in transformer-based models, namely
BERT and MentalBERT. Specifically, the proposed approach employs a Multimodal
Adaptation Gate for creating the combined embeddings, which are given as input
to a BERT (or MentalBERT) model. For taking into account the model calibration,
we apply label smoothing. We test our proposed approaches in three publicly
available datasets and demonstrate that the integration of linguistic features
into transformer-based models presents a surge in the performance. Also, the
usage of label smoothing contributes to both the improvement of the model's
performance and the calibration of the model. We finally perform a linguistic
analysis of the posts and show differences in language between stressful and
non-stressful texts, as well as depressive and non-depressive posts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues. (arXiv:2305.16798v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16798">
<div class="article-summary-box-inner">
<span><p>User Satisfaction Modeling (USM) is one of the popular choices for
task-oriented dialogue systems evaluation, where user satisfaction typically
depends on whether the user's task goals were fulfilled by the system.
Task-oriented dialogue systems use task schema, which is a set of task
attributes, to encode the user's task goals. Existing studies on USM neglect
explicitly modeling the user's task goals fulfillment using the task schema. In
this paper, we propose SG-USM, a novel schema-guided user satisfaction modeling
framework. It explicitly models the degree to which the user's preferences
regarding the task attributes are fulfilled by the system for predicting the
user's satisfaction level. SG-USM employs a pre-trained language model for
encoding dialogue context and task attributes. Further, it employs a
fulfillment representation layer for learning how many task attributes have
been fulfilled in the dialogue, an importance predictor component for
calculating the importance of task attributes. Finally, it predicts the user
satisfaction based on task attribute fulfillment and task attribute importance.
Experimental results on benchmark datasets (i.e. MWOZ, SGD, ReDial, and JDDC)
show that SG-USM consistently outperforms competitive existing methods. Our
extensive analysis demonstrates that SG-USM can improve the interpretability of
user satisfaction modeling, has good scalability as it can effectively deal
with unseen tasks and can also effectively work in low-resource settings by
leveraging unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support. (arXiv:2305.16799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16799">
<div class="article-summary-box-inner">
<span><p>Optimizing the phrasing of argumentative text is crucial in higher education
and professional development. However, assessing whether and how the different
claims in a text should be revised is a hard task, especially for novice
writers. In this work, we explore the main challenges to identifying
argumentative claims in need of specific revisions. By learning from
collaborative editing behaviors in online debates, we seek to capture implicit
revision patterns in order to develop approaches aimed at guiding writers in
how to further improve their arguments. We systematically compare the ability
of common word embedding models to capture the differences between different
versions of the same text, and we analyze their impact on various types of
writing issues. To deal with the noisy nature of revision-based corpora, we
propose a new sampling strategy based on revision distance. Opposed to
approaches from prior work, such sampling can be done without employing
additional annotations and judgments. Moreover, we provide evidence that using
contextual information and domain knowledge can further improve prediction
results. How useful a certain type of context is, depends on the issue the
claim is suffering from, though.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-Based Sign Language Video Summarization using Curvature and Torsion. (arXiv:2305.16801v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16801">
<div class="article-summary-box-inner">
<span><p>An interesting problem in many video-based applications is the generation of
short synopses by selecting the most informative frames, a procedure which is
known as video summarization. For sign language videos the benefits of using
the $t$-parameterized counterpart of the curvature of the 2-D signer's wrist
trajectory to identify keyframes, have been recently reported in the
literature. In this paper we extend these ideas by modeling the 3-D hand motion
that is extracted from each frame of the video. To this end we propose a new
informative function based on the $t$-parameterized curvature and torsion of
the 3-D trajectory. The method to characterize video frames as keyframes
depends on whether the motion occurs in 2-D or 3-D space. Specifically, in the
case of 3-D motion we look for the maxima of the harmonic mean of the curvature
and torsion of the target's trajectory; in the planar motion case we seek for
the maxima of the trajectory's curvature. The proposed 3-D feature is
experimentally evaluated in applications of sign language videos on (1)
objective measures using ground-truth keyframe annotations, (2) human-based
evaluation of understanding, and (3) gloss classification and the results
obtained are promising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16806">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose
language models capable of addressing many natural language generation or
understanding tasks. On the task of Machine Translation (MT), multiple works
have investigated few-shot prompting mechanisms to elicit better translations
from LLMs. However, there has been relatively little investigation on how such
translations differ qualitatively from the translations generated by standard
Neural Machine Translation (NMT) models. In this work, we investigate these
differences in terms of the literalness of translations produced by the two
systems. Using literalness measures involving word alignment and monotonicity,
we find that translations out of English (E-X) from GPTs tend to be less
literal, while exhibiting similar or better scores on MT quality metrics. We
demonstrate that this finding is borne out in human evaluations as well. We
then show that these differences are especially pronounced when translating
sentences that contain idiomatic expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16809">
<div class="article-summary-box-inner">
<span><p>When caregivers ask open--ended questions to motivate dialogue with children,
it facilitates the child's reading comprehension skills.Although there is scope
for use of technological tools, referred here as "intelligent tutoring
systems", to scaffold this process, it is currently unclear whether existing
intelligent systems that generate human--language like questions is beneficial.
Additionally, training data used in the development of these automated question
generation systems is typically sourced without attention to demographics, but
people with different cultural backgrounds may ask different questions. As a
part of a broader project to design an intelligent reading support app for
Latinx children, we crowdsourced questions from Latinx caregivers and
noncaregivers as well as caregivers and noncaregivers from other demographics.
We examine variations in question--asking within this dataset mediated by
individual, cultural, and contextual factors. We then design a system that
automatically extracts templates from this data to generate open--ended
questions that are representative of those asked by Latinx caregivers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Visual Story Generation with Adaptive Context Modeling. (arXiv:2305.16811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16811">
<div class="article-summary-box-inner">
<span><p>Diffusion models developed on top of powerful text-to-image generation models
like Stable Diffusion achieve remarkable success in visual story generation.
However, the best-performing approach considers historically generated results
as flattened memory cells, ignoring the fact that not all preceding images
contribute equally to the generation of the characters and scenes at the
current stage. To address this, we present a simple method that improves the
leading system with adaptive context modeling, which is not only incorporated
in the encoder but also adopted as additional guidance in the sampling stage to
boost the global consistency of the generated story. We evaluate our model on
PororoSV and FlintstonesSV datasets and show that our approach achieves
state-of-the-art FID scores on both story visualization and continuation
scenarios. We conduct detailed model analysis and show that our model excels at
generating semantically consistent images for stories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Songs Across Borders: Singable and Controllable Neural Lyric Translation. (arXiv:2305.16816v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16816">
<div class="article-summary-box-inner">
<span><p>The development of general-domain neural machine translation (NMT) methods
has advanced significantly in recent years, but the lack of naturalness and
musical constraints in the outputs makes them unable to produce singable lyric
translations. This paper bridges the singability quality gap by formalizing
lyric translation into a constrained translation problem, converting
theoretical guidance and practical techniques from translatology literature to
prompt-driven NMT approaches, exploring better adaptation methods, and
instantiating them to an English-Chinese lyric translation system. Our model
achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and
word boundary recall. In our subjective evaluation, our model shows a 75%
relative enhancement on overall quality, compared against naive fine-tuning
(Code available at https://github.com/Sonata165/ControllableLyricTranslation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness. (arXiv:2305.16819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16819">
<div class="article-summary-box-inner">
<span><p>Conditional language models still generate unfaithful output that is not
supported by their input. These unfaithful generations jeopardize trust in
real-world applications such as summarization or human-machine interaction,
motivating a need for automatic faithfulness metrics. To implement such
metrics, NLI models seem attractive, since they solve a strongly related task
that comes with a wealth of prior research and data. But recent research
suggests that NLI models require costly additional machinery to perform
reliably across datasets, e.g., by running inference on a cartesian product of
input and generated sentences, or supporting them with a
question-generation/answering step.
</p>
<p>In this work we show that pure NLI models _can_ outperform more complex
metrics when combining task-adaptive data augmentation with robust inference
procedures. We propose: (1) Augmenting NLI training data to adapt NL inferences
to the specificities of faithfulness prediction in dialogue; (2) Making use of
both entailment and contradiction probabilities in NLI, and (3) Using
Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which
combines faithfulness datasets across diverse domains and tasks, our approach
strongly improves a vanilla NLI model and significantly outperforms previous
work, while showing favourable computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization. (arXiv:2305.16820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16820">
<div class="article-summary-box-inner">
<span><p>Domain generalization is hitherto an underexplored area applied in
abstractive summarization. Moreover, most existing works on domain
generalization have sophisticated training algorithms. In this paper, we
propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging
approach to domain generalization for abstractive summarization. Given a number
of source domains, our method first trains a prefix for each one of them. These
source prefixes generate summaries for a small number of target domain
documents. The similarity of the generated summaries to their corresponding
documents is used for calculating weights required to average source prefixes.
In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging
allows for the computationally efficient addition of new source domains. When
evaluated on four diverse summarization domains, DAPA shows comparable or
better performance against the baselines, demonstrating the effectiveness of
its prefix averaging scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring. (arXiv:2305.16826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16826">
<div class="article-summary-box-inner">
<span><p>Automated essay scoring (AES) aims to score essays written for a given
prompt, which defines the writing topic. Most existing AES systems assume to
grade essays of the same prompt as used in training and assign only a holistic
score. However, such settings conflict with real-education situations;
pre-graded essays for a particular prompt are lacking, and detailed trait
scores of sub-rubrics are required. Thus, predicting various trait scores of
unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining
challenge of AES. In this paper, we propose a robust model: prompt- and trait
relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay
representation by essay-prompt attention and utilizing the topic-coherence
feature extracted by the topic-modeling mechanism without access to labeled
data; therefore, our model considers the prompt adherence of an essay, even in
a cross-prompt setting. To facilitate multi-trait scoring, we design
trait-similarity loss that encapsulates the correlations of traits. Experiments
prove the efficacy of our model, showing state-of-the-art results for all
prompts and traits. Significant improvements in low-resource-prompt and
inferior traits further indicate our model's strength.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12227">
<div class="article-summary-box-inner">
<span><p>We explore the use of residual networks and neural attention for multiple
argument mining tasks. We propose a residual architecture that exploits
attention, multi-task learning, and makes use of ensemble, without any
assumption on document or argument structure. We present an extensive
experimental evaluation on five different corpora of user-generated comments,
scientific publications, and persuasive essays. Our results show that our
approach is a strong competitor against state-of-the-art architectures with a
higher computational footprint or corpus-specific design, representing an
interesting compromise between generality, performance accuracy and reduced
model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fact-driven Logical Reasoning for Machine Reading Comprehension. (arXiv:2105.10334v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10334">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed an increasing interest in training machines with
reasoning ability, which deeply relies on accurately and clearly presented clue
forms. The clues are usually modeled as entity-aware knowledge in existing
studies. However, those entity-aware clues are primarily focused on
commonsense, making them insufficient for tasks that require knowledge of
temporary facts or events, particularly in logical reasoning for reading
comprehension. To address this challenge, we are motivated to cover both
commonsense and temporary knowledge clues hierarchically. Specifically, we
propose a general formalism of knowledge units by extracting backbone
constituents of the sentence, such as the subject-verb-object formed ``facts''.
We then construct a supergraph on top of the fact units, allowing for the
benefit of sentence-level (relations among fact groups) and entity-level
interactions (concepts or actions inside a fact). Experimental results on
logical reasoning benchmarks and dialogue modeling datasets show that our
approach improves the baselines substantially, and it is general across
backbone models. Code is available at
\url{https://github.com/ozyyshr/FocalReasoner}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs. (arXiv:2112.08804v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08804">
<div class="article-summary-box-inner">
<span><p>We present CrossSum, a large-scale cross-lingual summarization dataset
comprising 1.68 million article-summary samples in 1,500+ language pairs. We
create CrossSum by aligning parallel articles written in different languages
via cross-lingual retrieval from a multilingual abstractive summarization
dataset and perform a controlled human evaluation to validate its quality. We
propose a multistage data sampling algorithm to effectively train a
cross-lingual summarization model capable of summarizing an article in any
target language. We also introduce LaSE, an embedding-based metric for
automatically evaluating model-generated summaries. LaSE is strongly correlated
with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of
references in the target language. Performance on ROUGE and LaSE indicate that
our proposed model consistently outperforms baseline models. To the best of our
knowledge, CrossSum is the largest cross-lingual summarization dataset and the
first ever that is not centered around English. We are releasing the dataset,
training and evaluation scripts, and models to spur future research on
cross-lingual summarization. The resources can be found at
https://github.com/csebuetnlp/CrossSum
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization. (arXiv:2203.06569v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06569">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence neural networks have recently achieved great success in
abstractive summarization, especially through fine-tuning large pre-trained
language models on the downstream dataset. These models are typically decoded
with beam search to generate a unique summary. However, the search space is
very large, and with the exposure bias, such decoding is not optimal. In this
paper, we show that it is possible to directly train a second-stage model
performing re-ranking on a set of summary candidates. Our mixture-of-experts
SummaReranker learns to select a better candidate and consistently improves the
performance of the base model. With a base PEGASUS, we push ROUGE scores by
5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34%
on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and
checkpoints will be available at https://github.com/ntunlp/SummaReranker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are We Really Making Much Progress in Text Classification? A Comparative Review. (arXiv:2204.03954v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03954">
<div class="article-summary-box-inner">
<span><p>This study reviews and compares methods for single-label and multi-label text
classification, categorized into bag-of-words, sequence-based, graph-based, and
hierarchical methods. The comparison aggregates results from the literature
over five single-label and seven multi-label datasets and complements them with
new experiments. The findings reveal that all recently proposed graph-based and
hierarchy-based methods fail to outperform pre-trained language models and
sometimes perform worse than standard machine learning methods like a
multilayer perceptron on a bag-of-words. To assess the true scientific progress
in text classification, future work should thoroughly test against strong
bag-of-words baselines and state-of-the-art pre-trained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12505">
<div class="article-summary-box-inner">
<span><p>Recent works in Event Argument Extraction (EAE) have focused on improving
model generalizability to cater to new events and domains. However, standard
benchmarking datasets like ACE and ERE cover less than 40 event types and 25
entity-centric argument roles. Limited diversity and coverage hinder these
datasets from adequately evaluating the generalizability of EAE models. In this
paper, we first contribute by creating a large and diverse EAE ontology. This
ontology is created by transforming FrameNet, a comprehensive semantic role
labeling (SRL) dataset for EAE, by exploiting the similarity between these two
tasks. Then, exhaustive human expert annotations are collected to build the
ontology, concluding with 115 events and 220 argument roles, with a significant
portion of roles not being entities. We utilize this ontology to further
introduce GENEVA, a diverse generalizability benchmarking dataset comprising
four test suites, aimed at evaluating models' ability to handle limited data
and unseen event type generalization. We benchmark six EAE models from various
families. The results show that owing to non-entity argument roles, even the
best-performing model can only achieve 39% F1 score, indicating how GENEVA
provides new challenges for generalization in EAE. Overall, our large and
diverse EAE ontology can aid in creating more comprehensive future resources,
while GENEVA is a challenging benchmarking dataset encouraging further research
for improving generalizability in EAE. The code and data can be found at
https://github.com/PlusLabNLP/GENEVA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAGPRIME: A Unified Framework for Relational Structure Extraction. (arXiv:2205.12585v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12585">
<div class="article-summary-box-inner">
<span><p>Many tasks in natural language processing require the extraction of
relationship information for a given condition, such as event argument
extraction, relation extraction, and task-oriented semantic parsing. Recent
works usually propose sophisticated models for each task independently and pay
less attention to the commonality of these tasks and to have a unified
framework for all the tasks. In this work, we propose to take a unified view of
all these tasks and introduce TAGPRIME to address relational structure
extraction problems. TAGPRIME is a sequence tagging model that appends priming
words about the information of the given condition (such as an event trigger)
to the input text. With the self-attention mechanism in pre-trained language
models, the priming words make the output contextualized representations
contain more information about the given condition, and hence become more
suitable for extracting specific relationships for the condition. Extensive
experiments and analyses on three different tasks that cover ten datasets
across five different languages demonstrate the generality and effectiveness of
TAGPRIME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors. (arXiv:2205.12854v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12854">
<div class="article-summary-box-inner">
<span><p>The propensity of abstractive summarization models to make factual errors has
been studied extensively, including design of metrics to detect factual errors
and annotation of errors in current systems' outputs. However, the
ever-evolving nature of summarization systems, metrics, and annotated
benchmarks makes factuality evaluation a moving target, and drawing clear
comparisons among metrics has become increasingly difficult. In this work, we
aggregate factuality error annotations from nine existing datasets and stratify
them according to the underlying summarization model. We compare performance of
state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on
this stratified benchmark and show that their performance varies significantly
across different types of summarization models. Critically, our analysis shows
that much of the recent improvement in the factuality detection space has been
on summaries from older (pre-Transformer) models instead of more relevant
recent summarization models. We further perform a finer-grained analysis per
error-type and find similar performance variance across error types for
different factuality metrics. Our results show that no one metric is superior
in all settings or for all error types, and we provide recommendations for best
practices given these insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">B2T Connection: Serving Stability and Performance in Deep Transformers. (arXiv:2206.00330v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00330">
<div class="article-summary-box-inner">
<span><p>From the perspective of the layer normalization (LN) positions, the
architectures of Transformers can be categorized into two types: Post-LN and
Pre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep
Transformers (e.g., those with ten or more layers), the training is often
unstable, resulting in useless models. However, Post-LN has consistently
achieved better performance than Pre-LN in relatively shallow Transformers
(e.g., those with six or fewer layers). This study first investigates the
reason for these discrepant observations empirically and theoretically and made
the following discoveries: 1, the LN in Post-LN is the main source of the
vanishing gradient problem that leads to unstable training, whereas Pre-LN
prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher
layers during the back-propagation, which may lead to effective training.
Exploiting the new findings, we propose a method that can provide both high
stability and effective training by a simple modification of Post-LN. We
conduct experiments on a wide range of text generation tasks. The experimental
results demonstrate that our method outperforms Pre-LN, and enables stable
training regardless of the shallow or deep layer settings. Our code is publicly
available at https://github.com/takase/b2t_connection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden Schema Networks. (arXiv:2207.03777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03777">
<div class="article-summary-box-inner">
<span><p>Large, pretrained language models infer powerful representations that encode
rich semantic and syntactic content, albeit implicitly. In this work we
introduce a novel neural language model that enforces, via inductive biases,
explicit relational structures which allow for compositionality onto the output
representations of pretrained language models. Specifically, the model encodes
sentences into sequences of symbols (composed representations), which
correspond to the nodes visited by biased random walkers on a global latent
graph, and infers the posterior distribution of the latter. We first
demonstrate that the model is able to uncover ground-truth graphs from
artificially generated datasets of random token sequences. Next, we leverage
pretrained BERT and GPT-2 language models as encoder and decoder, respectively,
to infer networks of symbols (schemata) from natural language datasets. Our
experiments show that (i) the inferred symbols can be interpreted as encoding
different aspects of language, as e.g. topics or sentiments, and that (ii)
GPT-like models can effectively be conditioned on symbolic representations.
Finally, we explore training autoregressive, random walk ``reasoning" models on
schema networks inferred from commonsense knowledge databases, and using the
sampled paths to enhance the performance of pretrained language models on
commonsense If-Then reasoning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.12306">
<div class="article-summary-box-inner">
<span><p>Goal-oriented generative script learning aims to generate subsequent steps
based on a goal, which is an essential task to assist robots in performing
stereotypical activities of daily life. We show that the performance of this
task can be improved if historical states are not just captured by the
linguistic instructions given to people, but are augmented with the additional
information provided by accompanying images. Therefore, we propose a new task,
Multimedia Generative Script Learning, to generate subsequent steps by tracking
historical states in both text and vision modalities, as well as presenting the
first benchmark containing 2,338 tasks and 31,496 steps with descriptive
images. We aim to generate scripts that are visual-state trackable, inductive
for unseen tasks, and diverse in their individual steps. We propose to encode
visual state changes through a multimedia selective encoder, transferring
knowledge from previously observed tasks using a retrieval-augmented decoder,
and presenting the distinct information at each step by optimizing a
diversity-oriented contrastive learning objective. We define metrics to
evaluate both generation quality and inductive quality. Experiment results
demonstrate that our approach significantly outperforms strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Environmental Claim Detection. (arXiv:2209.00507v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.00507">
<div class="article-summary-box-inner">
<span><p>To transition to a green economy, environmental claims made by companies must
be reliable, comparable, and verifiable. To analyze such claims at scale,
automated methods are needed to detect them in the first place. However, there
exist no datasets or models for this. Thus, this paper introduces the task of
environmental claim detection. To accompany the task, we release an
expert-annotated dataset and models trained on this dataset. We preview one
potential application of such models: We detect environmental claims made in
quarterly earning calls and find that the number of environmental claims has
steadily increased since the Paris Agreement in 2015.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Aligned Simple German Corpus. (arXiv:2209.01106v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.01106">
<div class="article-summary-box-inner">
<span><p>"Leichte Sprache", the German counterpart to Simple English, is a regulated
language aiming to facilitate complex written language that would otherwise
stay inaccessible to different groups of people. We present a new
sentence-aligned monolingual corpus for Simple German -- German. It contains
multiple document-aligned sources which we have aligned using automatic
sentence-alignment methods. We evaluate our alignments based on a manually
labelled subset of aligned documents. The quality of our sentence alignments,
as measured by F1-score, surpasses previous work. We publish the dataset under
CC BY-SA and the accompanying code under MIT license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Document-Level Event Argument Extraction. (arXiv:2209.02203v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.02203">
<div class="article-summary-box-inner">
<span><p>Event argument extraction (EAE) has been well studied at the sentence level
but under-explored at the document level. In this paper, we study to capture
event arguments that actually spread across sentences in documents. Prior works
usually assume full access to rich document supervision, ignoring the fact that
the available argument annotation is usually limited. To fill this gap, we
present FewDocAE, a Few-Shot Document-Level Event Argument Extraction
benchmark, based on the existing document-level event extraction dataset. We
first define the new problem and reconstruct the corpus by a novel N -Way-D-Doc
sampling instead of the traditional N -Way-K-Shot strategy. Then we adjust the
current document-level neural models into the few-shot setting to provide
baseline results under in- and cross-domain settings. Since the argument
extraction depends on the context from multiple sentences and the learning
process is limited to very few examples, we find this novel task to be very
challenging with substantively low performance. Considering FewDocAE is closely
related to practical use under low-resource regimes, we hope this benchmark
encourages more research in this direction. Our data and codes will be
available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.14389">
<div class="article-summary-box-inner">
<span><p>For most natural language processing tasks, the dominant practice is to
finetune large pretrained transformer models (e.g., BERT) using smaller
downstream datasets. Despite the success of this approach, it remains unclear
to what extent these gains are attributable to the massive background corpora
employed for pretraining versus to the pretraining objectives themselves. This
paper introduces a large-scale study of self-pretraining, where the same
(downstream) training data is used for both pretraining and finetuning. In
experiments addressing both ELECTRA and RoBERTa models and 10 distinct
downstream classification datasets, we observe that self-pretraining rivals
standard pretraining on the BookWiki corpus (despite using around
$10\times$--$500\times$ less data), outperforming the latter on $7$ and $5$
datasets, respectively. Surprisingly, these task-specific pretrained models
often perform well on other tasks, including the GLUE benchmark. Besides
classification tasks, self-pretraining also provides benefits on structured
output prediction tasks such as span based question answering and commonsense
inference, often providing more than $50\%$ of the performance boosts provided
by pretraining on the BookWiki corpus. Our results hint that in many scenarios,
performance gains attributable to pretraining are driven primarily by the
pretraining objective itself and are not always attributable to the use of
external pretraining data in massive amounts. These findings are especially
relevant in light of concerns about intellectual property and offensive content
in web-scale pretraining data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A dynamic programming algorithm for span-based nested named-entity recognition in O(n^2). (arXiv:2210.04738v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04738">
<div class="article-summary-box-inner">
<span><p>Span-based nested named-entity recognition (NER) has a cubic-time complexity
using a variant of the CYK algorithm. We show that by adding a supplementary
structural constraint on the search space, nested NER has a quadratic-time
complexity, that is the same asymptotic complexity than the non-nested case.
The proposed algorithm covers a large part of three standard English benchmarks
and delivers comparable experimental results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04982">
<div class="article-summary-box-inner">
<span><p>Generating free-text rationales is a promising step towards explainable NLP,
yet evaluating such rationales remains a challenge. Existing metrics have
mostly focused on measuring the association between the rationale and a given
label. We argue that an ideal metric should focus on the new information
uniquely provided in the rationale that is otherwise not provided in the input
or the label. We investigate this research problem from an
information-theoretic perspective using conditional V-information (Hewitt et
al., 2021). More concretely, we propose a metric called REV (Rationale
Evaluation with conditional V-information), to quantify the amount of new,
label-relevant information in a rationale beyond the information already
available in the input or the label. Experiments across four benchmarks with
reasoning tasks, including chain-of-thought, demonstrate the effectiveness of
REV in evaluating rationale-label pairs, compared to existing metrics. We
further demonstrate REV is consistent with human judgments on rationale
evaluations and provides more sensitive measurements of new information in
free-text rationales. When used alongside traditional performance metrics, REV
provides deeper insights into models' reasoning and prediction processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models Be Specific? How?. (arXiv:2210.05159v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.05159">
<div class="article-summary-box-inner">
<span><p>"He is a person", "Paris is located on the earth". Both statements are
correct but meaningless - due to lack of specificity. In this paper, we propose
to measure how specific the language of pre-trained language models (PLMs) is.
To achieve this, we introduce a novel approach to build a benchmark for
specificity testing by forming masked token prediction tasks with prompts. For
instance, given "Toronto is located in [MASK].", we want to test whether a more
specific answer will be better filled in by PLMs, e.g., Ontario instead of
Canada. From our evaluations, we show that existing PLMs have only a slight
preference for more specific answers. We identify underlying factors affecting
the specificity and design two prompt-based methods to improve the specificity.
Results show that the specificity of the models can be improved by the proposed
methods without additional training. We hope this work can bring to awareness
the notion of specificity of language models and encourage the research
community to further explore this important but understudied problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06633">
<div class="article-summary-box-inner">
<span><p>Multilingual information retrieval (IR) is challenging since annotated
training data is costly to obtain in many languages. We present an effective
method to train multilingual IR systems when only English IR training data and
some parallel corpora between English and other languages are available. We
leverage parallel and non-parallel corpora to improve the pretrained
multilingual language models' cross-lingual transfer ability. We design a
semantic contrastive loss to align representations of parallel sentences that
share the same semantics in different languages, and a new language contrastive
loss to leverage parallel sentence pairs to remove language-specific
information in sentence representations from non-parallel corpora. When trained
on English IR data with these losses and evaluated zero-shot on non-English
data, our model demonstrates significant improvement to prior work on retrieval
performance, while it requires much less computational effort. We also
demonstrate the value of our model for a practical setting when a parallel
corpus is only available for a few languages, but a lack of parallel corpora
resources persists for many other low-resource languages. Our model can work
well even with a small number of parallel sentences, and be used as an add-on
module to any backbones and other tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models. (arXiv:2210.07135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07135">
<div class="article-summary-box-inner">
<span><p>Multilingual models have been widely used for cross-lingual transfer to
low-resource languages. However, the performance on these languages is hindered
by their underrepresentation in the pretraining data. To alleviate this
problem, we propose a novel multilingual training technique based on
teacher-student knowledge distillation. In this setting, we utilize monolingual
teacher models optimized for their language. We use those teachers along with
balanced (sub-sampled) data to distill the teachers' knowledge into a single
multilingual student. Our method outperforms standard training methods in
low-resource languages and retrains performance on high-resource languages
while using the same amount of data. If applied widely, our approach can
increase the representation of low-resource languages in NLP systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07586">
<div class="article-summary-box-inner">
<span><p>Most weakly supervised named entity recognition (NER) models rely on
domain-specific dictionaries provided by experts. This approach is infeasible
in many domains where dictionaries do not exist. While a phrase retrieval model
was used to construct pseudo-dictionaries with entities retrieved from
Wikipedia automatically in a recent study, these dictionaries often have
limited coverage because the retriever is likely to retrieve popular entities
rather than rare ones. In this study, we present a novel framework, HighGEN,
that generates NER datasets with high-coverage pseudo-dictionaries.
Specifically, we create entity-rich dictionaries with a novel search method,
called phrase embedding search, which encourages the retriever to search a
space densely populated with various entities. In addition, we use a new
verification process based on the embedding distance between candidate entity
mentions and entity types to reduce the false-positive noise in weak labels
generated by high-coverage dictionaries. We demonstrate that HighGEN
outperforms the previous best model by an average F1 score of 4.7 across five
NER benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Summary Candidates Fusion. (arXiv:2210.08779v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.08779">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence deep neural models fine-tuned for abstractive
summarization can achieve great performance on datasets with enough human
annotations. Yet, it has been shown that they have not reached their full
potential, with a wide gap between the top beam search output and the oracle
beam. Recently, re-ranking methods have been proposed, to learn to select a
better summary candidate. However, such methods are limited by the summary
quality aspects captured by the first-stage candidates. To bypass this
limitation, we propose a new paradigm in second-stage abstractive summarization
called SummaFusion that fuses several summary candidates to produce a novel
abstractive second-stage summary. Our method works well on several
summarization datasets, improving both the ROUGE scores and qualitative
properties of fused summaries. It is especially good when the candidates to
fuse are worse, such as in the few-shot setup where we set a new
state-of-the-art. We will make our code and checkpoints available at
https://github.com/ntunlp/SummaFusion/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks. (arXiv:2210.10343v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.10343">
<div class="article-summary-box-inner">
<span><p>Data augmentation techniques have been used to alleviate the problem of
scarce labeled data in various NER tasks (flat, nested, and discontinuous NER
tasks). Existing augmentation techniques either manipulate the words in the
original text that break the semantic coherence of the text, or exploit
generative models that ignore preserving entities in the original text, which
impedes the use of augmentation techniques on nested and discontinuous NER
tasks. In this work, we propose a novel Entity-to-Text based data augmentation
technique named EnTDA to add, delete, replace or swap entities in the entity
list of the original texts, and adopt these augmented entity lists to generate
semantically coherent and entity preserving texts for various NER tasks.
Furthermore, we introduce a diversity beam search to increase the diversity
during the text generation process. Experiments on thirteen NER datasets across
three tasks (flat, nested, and discontinuous NER tasks) and two settings (full
data and low resource settings) show that EnTDA could bring more performance
improvements compared to the baseline augmentation techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experiencer-Specific Emotion and Appraisal Prediction. (arXiv:2210.12078v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.12078">
<div class="article-summary-box-inner">
<span><p>Emotion classification in NLP assigns emotions to texts, such as sentences or
paragraphs. With texts like "I felt guilty when he cried", focusing on the
sentence level disregards the standpoint of each participant in the situation:
the writer ("I") and the other entity ("he") could in fact have different
affective states. The emotions of different entities have been considered only
partially in emotion semantic role labeling, a task that relates semantic roles
to emotion cue words. Proposing a related task, we narrow the focus on the
experiencers of events, and assign an emotion (if any holds) to each of them.
To this end, we represent each emotion both categorically and with appraisal
variables, as a psychological access to explaining why a person develops a
particular emotion. On an event description corpus, our experiencer-aware
models of emotions and appraisals outperform the experiencer-agnostic
baselines, showing that disregarding event participants is an
oversimplification for the emotion detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions. (arXiv:2210.15456v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15456">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning simulates the human ability to make presumptions about
our physical world, and it is an essential cornerstone in building general AI
systems. We propose a new commonsense reasoning dataset based on human's
Interactive Fiction (IF) gameplay walkthroughs as human players demonstrate
plentiful and diverse commonsense reasoning. The new dataset provides a natural
mixture of various reasoning types and requires multi-hop reasoning. Moreover,
the IF game-based construction procedure requires much less human interventions
than previous ones. Different from existing benchmarks, our dataset focuses on
the assessment of functional commonsense knowledge rules rather than factual
knowledge. Hence, in order to achieve higher performance on our tasks, models
need to effectively utilize such functional knowledge to infer the outcomes of
actions, rather than relying solely on memorizing facts. Experiments show that
the introduced dataset is challenging to previous machine reading models as
well as the new large language models with a significant 20% performance gap
compared to human experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.17546">
<div class="article-summary-box-inner">
<span><p>Studying data memorization in neural language models helps us understand the
risks (e.g., to privacy or copyright) associated with models regurgitating
training data, and aids in the evaluation of potential countermeasures. Many
prior works -- and some recently deployed defenses -- focus on "verbatim
memorization", defined as a model generation that exactly matches a substring
from the training set. We argue that verbatim memorization definitions are too
restrictive and fail to capture more subtle forms of memorization.
Specifically, we design and implement an efficient defense based on Bloom
filters that perfectly prevents all verbatim memorization. And yet, we
demonstrate that this "perfect" filter does not prevent the leakage of training
data. Indeed, it is easily circumvented by plausible and minimally modified
"style-transfer" prompts -- and in some cases even the non-modified original
prompts -- to extract memorized information. For example, instructing the model
to output ALL-CAPITAL texts bypasses memorization checks based on verbatim
matching. We conclude by discussing potential alternative definitions and why
defining memorization is a difficult yet crucial open question for neural
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.01482">
<div class="article-summary-box-inner">
<span><p>Existing metrics for evaluating the quality of automatically generated
questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and
predicted questions, providing a high score when there is a considerable
lexical overlap or semantic similarity between the candidate and the reference
questions. This approach has two major shortcomings. First, we need expensive
human-provided reference questions. Second, it penalises valid questions that
may not have high lexical or semantic similarity to the reference questions. In
this paper, we propose a new metric, RQUGE, based on the answerability of the
candidate question given the context. The metric consists of a
question-answering and a span scorer modules, using pre-trained models from
existing literature, thus it can be used without any further training. We
demonstrate that RQUGE has a higher correlation with human judgment without
relying on the reference question. Additionally, RQUGE is shown to be more
robust to several adversarial corruptions. Furthermore, we illustrate that we
can significantly improve the performance of QA models on out-of-domain
datasets by fine-tuning on synthetic data generated by a question generation
model and re-ranked by RQUGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. (arXiv:2211.06993v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.06993">
<div class="article-summary-box-inner">
<span><p>Large pre-trained models have revolutionized natural language processing
(NLP) research and applications, but high training costs and limited data
resources have prevented their benefits from being shared equally amongst
speakers of all the world's languages. To address issues of cross-linguistic
access to such models and reduce energy consumption for sustainability during
large-scale model training, this study proposes an effective and
energy-efficient framework called GreenPLM that uses bilingual lexicons to
directly "translate" pre-trained language models of one language into another
at almost no additional cost. We validate this approach in 18 languages' BERT
models and show that this framework is comparable to, if not better than, other
heuristics with high training costs. In addition, given lightweight continued
pre-training on limited data where available, this framework outperforms the
original monolingual language models in six out of seven tested languages with
up to 200x less pre-training efforts. Aiming at the Leave No One Behind
Principle (LNOB), our approach manages to reduce inequalities between languages
and energy consumption greatly. We make our codes and models publicly available
here: \url{https://github.com/qcznlp/GreenPLMs}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Pronunciation Assessment with Multi-Aspect Attention. (arXiv:2211.08102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08102">
<div class="article-summary-box-inner">
<span><p>Automatic pronunciation assessment is a major component of a
computer-assisted pronunciation training system. To provide in-depth feedback,
scoring pronunciation at various levels of granularity such as phoneme, word,
and utterance, with diverse aspects such as accuracy, fluency, and
completeness, is essential. However, existing multi-aspect multi-granularity
methods simultaneously predict all aspects at all granularity levels;
therefore, they have difficulty in capturing the linguistic hierarchy of
phoneme, word, and utterance. This limitation further leads to neglecting
intimate cross-aspect relations at the same linguistic unit. In this paper, we
propose a Hierarchical Pronunciation Assessment with Multi-aspect Attention
(HiPAMA) model, which hierarchically represents the granularity levels to
directly capture their linguistic structures and introduces multi-aspect
attention that reflects associations across aspects at the same level to create
more connotative representations. By obtaining relational information from both
the granularity- and aspect-side, HiPAMA can take full advantage of multi-task
learning. Remarkable improvements in the experimental results on the
speachocean762 datasets demonstrate the robustness of HiPAMA, particularly in
the difficult-to-assess aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08794">
<div class="article-summary-box-inner">
<span><p>Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into multi-view compressed representations before
feeding them into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SongRewriter: A Chinese Song Rewriting System with Controllable Content and Rhyme Scheme. (arXiv:2211.15037v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15037">
<div class="article-summary-box-inner">
<span><p>Although lyrics generation has achieved significant progress in recent years,
it has limited practical applications because the generated lyrics cannot be
performed without composing compatible melodies. In this work, we bridge this
practical gap by proposing a song rewriting system which rewrites the lyrics of
an existing song such that the generated lyrics are compatible with the rhythm
of the existing melody and thus singable. In particular, we propose
SongRewriter,a controllable Chinese lyrics generation and editing system which
assists users without prior knowledge of melody composition. The system is
trained by a randomized multi-level masking strategy which produces a unified
model for generating entirely new lyrics or editing a few fragments. To improve
the controllabiliy of the generation process, we further incorporate a keyword
prompt to control the lexical choices of the content and propose novel decoding
constraints and a vowel modeling task to enable flexible end and internal rhyme
schemes. While prior rhyming metrics are mainly for rap lyrics, we propose
three novel rhyming evaluation metrics for song lyrics. Both automatic and
human evaluations show that the proposed model performs better than the
state-of-the-art models in both contents and rhyming quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models. (arXiv:2211.15718v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15718">
<div class="article-summary-box-inner">
<span><p>In many task settings, text classification models are likely to encounter
examples from novel classes on which they cannot predict correctly. Selective
prediction, in which models abstain on low-confidence examples, provides a
possible solution, but existing models are often overly confident on unseen
classes. To remedy this overconfidence, we introduce Contrastive
Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD
examples representative of novel classes, then trains to decrease confidence on
them. First, we generate OOD examples by prompting a large language model
twice: we prompt it to enumerate relevant novel classes, then generate examples
from each novel class matching the task format. Second, we train a classifier
with a novel contrastive objective that encourages lower confidence on
generated OOD examples than training examples. When trained with CoNAL,
classifiers improve in their ability to detect and abstain on novel class
examples over prior methods by an average of 2.3% in terms of accuracy under
the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with
no cost to in-distribution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft Alignment Objectives for Robust Adaptation of Language Generation. (arXiv:2211.16550v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.16550">
<div class="article-summary-box-inner">
<span><p>Domain adaptation allows generative language models to address specific flaws
caused by the domain shift of their application. However, the traditional
adaptation by further training on in-domain data rapidly weakens the model's
ability to generalize to other domains, making the open-ended deployments of
the adapted models prone to errors. This work introduces novel training
objectives built upon a semantic similarity of the predicted tokens to the
reference.
</p>
<p>Our results show that (1) avoiding the common assumption of a single correct
prediction by constructing the training target from tokens' semantic similarity
can mitigate catastrophic forgetting during domain adaptation, while (2)
preserving the quality of the adaptation, (3) with negligible additions to
compute costs.
</p>
<p>In the broader context, the objectives grounded in a continuous token
similarity pioneer the exploration of the middle ground between the efficient
but na\"{\i}ve exact-match token-level objectives and expressive but
computationally- and resource-intensive sequential objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01117">
<div class="article-summary-box-inner">
<span><p>The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonparametric Masked Language Modeling. (arXiv:2212.01349v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.01349">
<div class="article-summary-box-inner">
<span><p>Existing language models (LMs) predict tokens with a softmax over a finite
vocabulary, which can make it difficult to predict rare tokens or phrases. We
introduce NPM, the first nonparametric masked language model that replaces this
softmax with a nonparametric distribution over every phrase in a reference
corpus. NPM fills in the [MASK] solely from retrieving a token from a text
corpus. We show that NPM can be efficiently trained with a contrastive
objective and an in-batch approximation to full corpus retrieval. Zero-shot
evaluation on 16 tasks including classification, fact probing and question
answering demonstrates that NPM outperforms significantly larger parametric
models, either with or without a retrieve-and-generate approach. It is
particularly better at dealing with rare patterns (word senses or facts) and
predicting rare or nearly unseen words (e.g., non-Latin script). We release the
model and code at github.com/facebookresearch/NPM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models Can be Fully Zero-Shot Learners. (arXiv:2212.06950v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.06950">
<div class="article-summary-box-inner">
<span><p>How can we extend a pre-trained model to many language understanding tasks,
without labeled or additional unlabeled data? Pre-trained language models
(PLMs) have been effective for a wide range of NLP tasks. However, existing
approaches either require fine-tuning on downstream labeled datasets or
manually constructing proper prompts. In this paper, we propose nonparametric
prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike
previous methods, NPPrompt uses only pre-trained language models and does not
require any labeled data or additional raw corpus for further fine-tuning, nor
does it rely on humans to construct a comprehensive set of prompt label words.
We evaluate NPPrompt against previous major few-shot and zero-shot learning
methods on diverse NLP tasks: including text classification, text entailment,
similar text retrieval, and paraphrasing. Experimental results demonstrate that
our NPPrompt outperforms the previous best fully zero-shot method by big
margins, with absolute gains of 12.8% in accuracy on text classification and
18.9% on the GLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually-augmented pretrained language models for NLP tasks without images. (arXiv:2212.07937v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07937">
<div class="article-summary-box-inner">
<span><p>Although pre-trained language models~(PLMs) have shown impressive performance
by text-only self-supervised training, they are found lack of visual semantics
or commonsense. Existing solutions often rely on explicit images for visual
knowledge augmentation (requiring time-consuming retrieval or generation), and
they also conduct the augmentation for the whole input text, without
considering whether it is actually needed in specific inputs or tasks. To
address these issues, we propose a novel \textbf{V}isually-\textbf{A}ugmented
fine-tuning approach that can be generally applied to various PLMs or NLP
tasks, \textbf{W}ithout using any retrieved or generated \textbf{I}mages,
namely \textbf{VAWI}. Experimental results show that our approach can
consistently improve the performance of BERT, RoBERTa, BART, and T5 at
different scales, and outperform several competitive baselines on ten tasks.
Our codes and data are publicly available
at~\url{https://github.com/RUCAIBox/VAWI}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08011">
<div class="article-summary-box-inner">
<span><p>Dialect differences caused by regional, social, and economic factors cause
performance discrepancies for many groups of language technology users.
Inclusive and equitable language technology must critically be dialect
invariant, meaning that performance remains constant over dialectal shifts.
Current systems often fall short of this ideal since they are designed and
tested on a single dialect: Standard American English (SAE). We introduce a
suite of resources for evaluating and achieving English dialect invariance. The
resource is called Multi-VALUE, a controllable rule-based translation system
spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE
maps SAE to synthetic forms of each dialect. First, we use this system to
stress tests question answering, machine translation, and semantic parsing.
Stress tests reveal significant performance disparities for leading models on
non-standard dialects. Second, we use this system as a data augmentation
technique to improve the dialect robustness of existing systems. Finally, we
partner with native speakers of Chicano and Indian English to release new
gold-standard variants of the popular CoQA task. To execute the transformation
code, run model checkpoints, and download both synthetic and gold-standard
dialectal benchmark datasets, see <a href="http://value-nlp.org.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue. (arXiv:2212.08054v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08054">
<div class="article-summary-box-inner">
<span><p>Modern virtual assistants use internal semantic parsing engines to convert
user utterances to actionable commands. However, prior work has demonstrated
that semantic parsing is a difficult multilingual transfer task with low
transfer efficiency compared to other tasks. In global markets such as India
and Latin America, this is a critical issue as switching between languages is
prevalent for bilingual users. In this work we dramatically improve the
zero-shot performance of a multilingual and codeswitched semantic parsing
system using two stages of multilingual alignment. First, we show that
constrastive alignment pretraining improves both English performance and
transfer efficiency. We then introduce a constrained optimization approach for
hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned
Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and
81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing
benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. (arXiv:2212.08055v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08055">
<div class="article-summary-box-inner">
<span><p>Direct speech-to-speech translation (S2ST), in which all components can be
optimized jointly, is advantageous over cascaded approaches to achieve fast
inference with a simplified pipeline. We present a novel two-pass direct S2ST
architecture, UnitY, which first generates textual representations and predicts
discrete acoustic units subsequently. We enhance the model performance by
subword prediction in the first-pass decoder, advanced two-pass decoder
architecture design and search strategy, and better training regularization. To
leverage large amounts of unlabeled text data, we pre-train the first-pass text
decoder based on the self-supervised denoising auto-encoding task. Experimental
evaluations on benchmark datasets at various data scales demonstrate that UnitY
outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU
with 2.83x decoding speed-up. We show that the proposed methods boost the
performance even when predicting spectrogram in the second pass. However,
predicting discrete units achieves 2.51x decoding speed-up compared to that
case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers. (arXiv:2212.08700v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08700">
<div class="article-summary-box-inner">
<span><p>How well do language models deal with quantification? In this study, we focus
on 'few'-type quantifiers, as in 'few children like toys', which might pose a
particular challenge for language models because the sentence components with
out the quantifier are likely to co-occur, and 'few'-type quantifiers are rare.
We present 960 English sentence stimuli from two human neurolinguistic
experiments to 22 autoregressive transformer models of differing sizes. Not
only do all the models perform poorly on 'few'-type quantifiers, but overall
the larger the model, the worse its performance. This inverse scaling is
consistent with previous work suggesting that larger models increasingly
reflect online rather than offline human processing, and we argue that the
decreasing performance of larger models may challenge uses of language models
as the basis for natural language systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09180">
<div class="article-summary-box-inner">
<span><p>There has been great recent advancement in human-computer chat. However,
proper evaluation currently requires human judgements that produce notoriously
high-variance metrics due to their inherent subjectivity. Furthermore, there is
little standardization in the methods and labels used for evaluation, with an
overall lack of work to compare and assess the validity of various evaluation
approaches. As a consequence, existing evaluation results likely leave an
incomplete picture of the strengths and weaknesses of open-domain chatbots. We
aim towards a dimensional evaluation of human-computer chat that can reliably
measure several distinct aspects of chat quality. To this end, we present our
novel human evaluation method that quantifies the rate of several
quality-related chatbot behaviors. Our results demonstrate our method to be
more suitable for dimensional chat evaluation than alternative likert-style or
comparative methods. We then use our validated method and existing methods to
evaluate four open-domain chat models from the recent literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OASum: Large-Scale Open Domain Aspect-based Summarization. (arXiv:2212.09233v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09233">
<div class="article-summary-box-inner">
<span><p>Aspect or query-based summarization has recently caught more attention, as it
can generate differentiated summaries based on users' interests. However, the
current dataset for aspect or query-based summarization either focuses on
specific domains, contains relatively small-scale instances, or includes only a
few aspect types. Such limitations hinder further explorations in this
direction. In this work, we take advantage of crowd-sourcing knowledge on
Wikipedia.org and automatically create a high-quality, large-scale open-domain
aspect-based summarization dataset named OASum, which contains more than 3.7
million instances with around 1 million different aspects on 2 million
Wikipedia pages. We provide benchmark results on OASum and demonstrate its
ability for diverse aspect-based summarization generation. To overcome the data
scarcity problem on specific domains, we also perform zero-shot, few-shot, and
fine-tuning on seven downstream datasets. Specifically, zero/few-shot and
fine-tuning results show that the model pre-trained on our corpus demonstrates
a strong aspect or query-focused generation ability compared with the backbone
model. Our dataset and pre-trained checkpoints are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09246">
<div class="article-summary-box-inner">
<span><p>Commonsense capabilities of pre-trained language models dramatically improve
with scale, leading many to believe that scale is the only winning recipe. But
is it? Here, we investigate an alternative that a priori seems impossible: can
smaller language models (e.g., GPT-2) win over models that are orders of
magnitude larger and better (e.g., GPT-3), if powered with novel commonsense
distillation algorithms? The key intellectual challenge is to design a learning
algorithm that achieve a competitive level of commonsense acquisition, without
relying on the benefits of scale. In particular, we study generative models of
commonsense knowledge, focusing on the task of generating generics, statements
of commonsense facts about everyday concepts, e.g., birds can fly.
</p>
<p>We introduce I2D2, a novel commonsense distillation framework that loosely
follows the Symbolic Knowledge Distillation of West et al. but breaks the
dependence on the extreme-scale teacher model with two innovations: (1) the
novel adaptation of NeuroLogic Decoding to enhance the generation quality of
the weak, off-the-shelf language models, and (2) self-imitation learning to
iteratively learn from the model's own enhanced commonsense acquisition
capabilities. Empirical results suggest that scale is not the only way, as
novel algorithms can be a promising alternative. Moreover, our study leads to a
new corpus of generics, Gen-A-tomic, that is the largest and highest quality
available to date.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling. (arXiv:2212.09588v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09588">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an unsupervised query enhanced approach for
knowledge-intensive conversations, namely QKConv. There are three modules in
QKConv: a query generator, an off-the-shelf knowledge selector, and a response
generator. QKConv is optimized through joint training, which produces the
response by exploring multiple candidate queries and leveraging corresponding
selected knowledge. The joint training solely relies on the dialogue context
and target response, getting exempt from extra query annotations or knowledge
provenances. To evaluate the effectiveness of the proposed QKConv, we conduct
experiments on three representative knowledge-intensive conversation datasets:
conversational question-answering, task-oriented dialogue, and
knowledge-grounded conversation. Experimental results reveal that QKConv
performs better than all unsupervised methods across three datasets and
achieves competitive performance compared to supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Summarization Re-ranking. (arXiv:2212.09593v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09593">
<div class="article-summary-box-inner">
<span><p>With the rise of task-specific pre-training objectives, abstractive
summarization models like PEGASUS offer appealing zero-shot performance on
downstream summarization tasks. However, the performance of such unsupervised
models still lags significantly behind their supervised counterparts. Similarly
to the supervised setup, we notice a very high variance in quality among
summary candidates from these models while only one candidate is kept as the
summary output. In this paper, we propose to re-rank summary candidates in an
unsupervised manner, aiming to close the performance gap between unsupervised
and supervised models. Our approach improves the unsupervised PEGASUS by up to
7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted
summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%
from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on
a dataset, evaluating on another).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Python Code Generation by Asking Clarification Questions. (arXiv:2212.09885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09885">
<div class="article-summary-box-inner">
<span><p>Code generation from text requires understanding the user's intent from a
natural language description and generating an executable code snippet that
satisfies this intent. While recent pretrained language models demonstrate
remarkable performance for this task, these models fail when the given natural
language description is under-specified. In this work, we introduce a novel and
more realistic setup for this task. We hypothesize that the under-specification
of a natural language description can be resolved by asking clarification
questions. Therefore, we collect and introduce a new dataset named CodeClarQA
containing pairs of natural language descriptions and code with created
synthetic clarification questions and answers. The empirical results of our
evaluation of pretrained language model performance on code generation show
that clarifications result in more precisely generated code, as shown by the
substantial improvement of model performance in all evaluation metrics.
Alongside this, our task and dataset introduce new challenges to the community,
including when and what clarification questions should be asked. Our code and
dataset are available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization. (arXiv:2212.10018v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10018">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization has recently garnered significant attention due to its
wide range of applications. However, existing methods for summarizing dialogues
have limitations because they do not take into account the inherent structure
of dialogue and rely heavily on labeled data, which can lead to poor
performance in new domains. In this work, we propose DIONYSUS (dynamic input
optimization in pre-training for dialogue summarization), a pre-trained
encoder-decoder model for summarizing dialogues in any new domain. To pre-train
DIONYSUS, we create two pseudo summaries for each dialogue example: one is
produced by a fine-tuned summarization model, and the other is a collection of
dialogue turns that convey important information. We then choose one of these
pseudo summaries based on the difference in information distribution across
different types of dialogues. This selected pseudo summary serves as the
objective for pre-training DIONYSUS using a self-supervised approach on a large
dialogue corpus. Our experiments show that DIONYSUS outperforms existing
methods on six datasets, as demonstrated by its ROUGE scores in zero-shot and
few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation. (arXiv:2212.10140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10140">
<div class="article-summary-box-inner">
<span><p>One of the major challenges of machine translation (MT) is ambiguity, which
can in some cases be resolved by accompanying context such as images. However,
recent work in multimodal MT (MMT) has shown that obtaining improvements from
images is challenging, limited not only by the difficulty of building effective
cross-modal representations, but also by the lack of specific evaluation and
training data. We present a new MMT approach based on a strong text-only MT
model, which uses neural adapters, a novel guided self-attention mechanism and
which is jointly trained on both visually-conditioned masking and MMT. We also
introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation
set of ambiguous sentences and their possible translations, accompanied by
disambiguating images corresponding to each translation. Our approach obtains
competitive results compared to strong text-only models on standard
English-to-French, English-to-German and English-to-Czech benchmarks and
outperforms baselines and state-of-the-art MMT systems by a large margin on our
contrastive test set. Our code and CoMMuTE are freely available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reasoning in Large Language Models: A Survey. (arXiv:2212.10403v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10403">
<div class="article-summary-box-inner">
<span><p>Reasoning is a fundamental aspect of human intelligence that plays a crucial
role in activities such as problem solving, decision making, and critical
thinking. In recent years, large language models (LLMs) have made significant
progress in natural language processing, and there is observation that these
models may exhibit reasoning abilities when they are sufficiently large.
However, it is not yet clear to what extent LLMs are capable of reasoning. This
paper provides a comprehensive overview of the current state of knowledge on
reasoning in LLMs, including techniques for improving and eliciting reasoning
in these models, methods and benchmarks for evaluating reasoning abilities,
findings and implications of previous research in this field, and suggestions
on future directions. Our aim is to provide a detailed and up-to-date review of
this topic and stimulate meaningful discussion and future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10449">
<div class="article-summary-box-inner">
<span><p>In long document controllable summarization, where labeled data is scarce,
pretrained models struggle to adapt to the task and effectively respond to user
queries. In this paper, we introduce Socratic pretraining, a question-driven,
unsupervised pretraining objective specifically designed to improve
controllability in summarization tasks. By training a model to generate and
answer relevant questions in a given context, Socratic pretraining enables the
model to more effectively adhere to user-provided queries and identify relevant
content to be summarized. We demonstrate the effectiveness of this approach
through extensive experimentation on two summarization domains, short stories
and dialogue, and multiple control strategies: keywords, questions, and factoid
QA pairs. Our pretraining method relies only on unlabeled documents and a
question generation system and outperforms pre-finetuning approaches that use
additional supervised data. Furthermore, our results show that Socratic
pretraining cuts task-specific labeled data requirements in half, is more
faithful to user-provided queries, and achieves state-of-the-art performance on
QMSum and SQuALITY.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Instruct: Aligning Language Models with Self-Generated Instructions. (arXiv:2212.10560v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10560">
<div class="article-summary-box-inner">
<span><p>Large "instruction-tuned" language models (i.e., finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is often limited in quantity, diversity, and creativity, therefore
hindering the generality of the tuned model. We introduce Self-Instruct, a
framework for improving the instruction-following capabilities of pretrained
language models by bootstrapping off their own generations. Our pipeline
generates instructions, input, and output samples from a language model, then
filters invalid or similar ones before using them to finetune the original
model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute
improvement over the original model on Super-NaturalInstructions, on par with
the performance of InstructGPT-001, which was trained with private user data
and human annotations. For further evaluation, we curate a set of
expert-written instructions for novel tasks, and show through human evaluation
that tuning GPT3 with Self-Instruct outperforms using existing public
instruction datasets by a large margin, leaving only a 5% absolute gap behind
InstructGPT-001. Self-Instruct provides an almost annotation-free method for
aligning pre-trained language models with instructions, and we release our
large synthetic dataset to facilitate future studies on instruction tuning. Our
code and data are available at https://github.com/yizhongw/self-instruct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions. (arXiv:2212.10720v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10720">
<div class="article-summary-box-inner">
<span><p>Morality in dialogue systems has raised great attention in research recently.
A moral dialogue system aligned with users' values could enhance conversation
engagement and user connections. In this paper, we propose a framework,
MoralDial to train and evaluate moral dialogue systems. In our framework, we
first explore the communication mechanisms of morality and resolve expressed
morality into three parts, which indicate the roadmap for building a moral
dialogue system. Based on that, we design a simple yet effective method:
constructing moral discussions between simulated specific users and the
dialogue system. The constructed discussions consist of expressing, explaining,
revising, and inferring moral views in dialogue exchanges, which makes
conversational models learn morality well in a natural manner. Furthermore, we
propose a novel evaluation method under the framework. We evaluate the multiple
aspects of morality by judging the relation between dialogue responses and
human values in discussions, where the multifaceted nature of morality is
particularly considered. Automatic and manual experiments demonstrate that our
framework is promising to train and evaluate moral dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges. (arXiv:2301.10075v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10075">
<div class="article-summary-box-inner">
<span><p>Gender inclusivity in language technologies has become a prominent research
topic. In this study, we explore gender-neutral translation (GNT) as a form of
gender inclusivity and a goal to be achieved by machine translation (MT)
models, which have been found to perpetuate gender bias and discrimination.
Specifically, we focus on translation from English into Italian, a language
pair representative of salient gender-related linguistic transfer problems. To
define GNT, we review a selection of relevant institutional guidelines for
gender-inclusive language, discuss its scenarios of use, and examine the
technical challenges of performing GNT in MT, concluding with a discussion of
potential solutions to encourage advancements toward greater inclusivity in MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.13823">
<div class="article-summary-box-inner">
<span><p>We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process arbitrarily interleaved
image-and-text data, and generate text interleaved with retrieved images. Our
method leverages the abilities of language models learnt from large scale
text-only pretraining, such as in-context learning and free-form text
generation. We keep the language model frozen, and finetune input and output
linear layers to enable cross-modality interactions. This allows our model to
process arbitrarily interleaved image-and-text inputs, and generate free-form
text interleaved with retrieved images. We achieve strong zero-shot performance
on grounded tasks such as contextual image retrieval and multimodal dialogue,
and showcase compelling interactive abilities. Our approach works with any
off-the-shelf language model and paves the way towards an effective, general
solution for leveraging pretrained language models in visually grounded
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07189">
<div class="article-summary-box-inner">
<span><p>Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification, and the datasets for evaluation are
relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have corresponding KB entities
by matching them to a special NIL entity. To better utilize BERT, we propose
new techniques including NIL entity representation and classification, with
synonym enhancement. We also propose KB Pruning and Versioning strategies to
automatically construct out-of-KB datasets from common in-KB EL datasets.
Results on five datasets of clinical notes, biomedical publications, and
Wikipedia articles in various domains show the advantages of BLINKout over
existing methods to identify out-of-KB mentions for the medical ontologies,
UMLS, SNOMED CT, and the general KB, WikiData.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation. (arXiv:2303.08302v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.08302">
<div class="article-summary-box-inner">
<span><p>Post-training quantization (PTQ) has emerged as a promising technique for
mitigating memory consumption and computational costs in large language models
(LLMs). However, a systematic examination of various quantization schemes,
model families, and quantization bit precision has been absent from the
literature. In this paper, we conduct a comprehensive analysis of these factors
by investigating the effects of PTQ on weight-only, activation-only, and
weight-and-activation quantization using diverse methods such as
round-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these
methods to two distinct model families with parameters ranging from 125M to
176B. Our contributions include: (1) a sensitivity analysis revealing that
activation quantization is generally more susceptible to weight quantization,
with smaller models often outperforming larger models in terms of activation
quantization; (2) an evaluation and comparison of existing PTQ methods to
optimize model size reduction while minimizing the impact on accuracy,
revealing that none of the current methods can achieve the original model
quality for quantization with either INT4-weight or
INT4-weight-and-INT8-activation; (3) based on these insights, we propose an
optimized method called Low-Rank Compensation (LoRC), which employs low-rank
matrices to enhance model quality recovery with a minimal increase in model
size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.17651">
<div class="article-summary-box-inner">
<span><p>Like humans, large language models (LLMs) do not always generate the best
output on their first try. Motivated by how humans refine their written text,
we introduce Self-Refine, an approach for improving initial outputs from LLMs
through iterative feedback and refinement. The main idea is to generate an
initial output using an LLMs; then, the same LLMs provides feedback for its
output and uses it to refine itself, iteratively. Self-Refine does not require
any supervised training data, additional training, or reinforcement learning,
and instead uses a single LLM as the generator, refiner, and feedback provider.
We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response
generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,
and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine
are preferred by humans and automatic metrics over those generated with the
same LLM using conventional one-step generation, improving by ~20% absolute on
average in task performance. Our work demonstrates that even state-of-the-art
LLMs like GPT-4 can be further improved at test time using our simple,
standalone approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets. (arXiv:2304.00483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.00483">
<div class="article-summary-box-inner">
<span><p>Low-quality data can cause downstream problems in high-stakes applications.
Data-centric approach emphasizes on improving dataset quality to enhance model
performance. High-quality datasets are needed for general-purpose Large
Language Models (LLMs) training, as well as for domain-specific models, which
are usually small in size as it is costly to engage a large number of domain
experts for their creation. Thus, it is vital to ensure high-quality
domain-specific training data. In this paper, we propose a framework for
enhancing the data quality of original datasets. We applied the proposed
framework to four biomedical datasets and showed relative improvement of up to
33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when
using back translation to enhance the original dataset quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01528">
<div class="article-summary-box-inner">
<span><p>Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural
language interactions between players and hidden state information. Recent work
has shown that large language models (LLMs) that have access to state
information can generate higher quality game turns than LLMs that use dialog
history alone. However, previous work used game state information that was
heuristically created and was not a true gold standard game state. We present
FIREBALL, a large dataset containing nearly 25,000 unique sessions from real
D&amp;D gameplay on Discord with true game state info. We recorded game play
sessions of players who used the Avrae bot, which was developed to aid people
in playing D&amp;D online, capturing language, game commands and underlying game
state information. We demonstrate that FIREBALL can improve natural language
generation (NLG) by using Avrae state information, improving both automated
metrics and human judgments of quality. Additionally, we show that LLMs can
generate executable Avrae commands, particularly after finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02031">
<div class="article-summary-box-inner">
<span><p>Modern Natural Language Generation (NLG) models come with massive
computational and storage requirements. In this work, we study the potential of
compressing them, which is crucial for real-world applications serving millions
of users. We focus on Knowledge Distillation (KD) techniques, in which a small
student model learns to imitate a large teacher model, allowing to transfer
knowledge from the teacher to the student. In contrast to much of the previous
work, our goal is to optimize the model for a specific NLG task and a specific
dataset. Typically in real-world applications, in addition to labeled data
there is abundant unlabeled task-specific data, which is crucial for attaining
high compression rates via KD. In this work, we conduct a systematic study of
task-specific KD techniques for various NLG tasks under realistic assumptions.
We discuss the special characteristics of NLG distillation and particularly the
exposure bias problem. Following, we derive a family of Pseudo-Target (PT)
augmentation methods, substantially extending prior work on sequence-level KD.
We propose the Joint-Teaching method, which applies word-level KD to multiple
PTs generated by both the teacher and the student. Finally, we validate our
findings in an extreme setup with no labeled examples using GPT-4 as the
teacher. Our study provides practical model design observations and
demonstrates the effectiveness of PT training for task-specific KD in NLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives. (arXiv:2305.02364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02364">
<div class="article-summary-box-inner">
<span><p>Sustaining coherent and engaging narratives requires dialogue or storytelling
agents to understand how the personas of speakers or listeners ground the
narrative. Specifically, these agents must infer personas of their listeners to
produce statements that cater to their interests. They must also learn to
maintain consistent speaker personas for themselves throughout the narrative,
so that their counterparts feel involved in a realistic conversation or story.
</p>
<p>However, personas are diverse and complex: they entail large quantities of
rich interconnected world knowledge that is challenging to robustly represent
in general narrative systems (e.g., a singer is good at singing, and may have
attended conservatoire). In this work, we construct a new large-scale persona
commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona
facts. Our knowledge graph schematizes five dimensions of persona knowledge
identified in previous studies of human interactive behaviours, and distils
facts in this schema from both existing commonsense knowledge graphs and
large-scale pretrained language models. Our analysis indicates that PeaCoK
contains rich and precise world persona inferences that help downstream systems
generate more consistent and engaging narratives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03130">
<div class="article-summary-box-inner">
<span><p>The retrieval model is an indispensable component for real-world
knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As
separate retrieval skills are annotated for different datasets, recent work
focuses on customized methods, limiting the model transferability and
scalability. In this work, we propose a modular retriever where individual
modules correspond to key skills that can be reused across datasets. Our
approach supports flexible skill configurations based on the target domain to
boost performance. To mitigate task interference, we design a novel
modularization parameterization inspired by sparse Transformer. We demonstrate
that our model can benefit from self-supervised pretraining on Wikipedia and
fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our
approach outperforms recent self-supervised retrievers in zero-shot evaluations
and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA
and OTT-QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04087">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated an impressive ability to
generate codes on competitive programming tasks. However, with limited sample
numbers, LLMs still suffer from poor accuracy. Inspired by the process of human
programming, we propose a generate-and-edit approach named Self-Edit that
utilizes execution results of the generated code from LLMs to improve the code
quality on the competitive programming task. We execute the generated code on
the example test case provided in the question and wrap execution results into
a supplementary comment. Utilizing this comment as guidance, our fault-aware
code editor is employed to correct errors in the generated code. We perform
extensive evaluations across two competitive programming datasets with nine
different LLMs. Compared to directly generating from LLMs, our approach can
improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\%
on HumanEval over nine popular code generation LLMs with parameter sizes
ranging from 110M to 175B. Compared to other post-processing methods, our
method demonstrates superior accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. (arXiv:2305.04091v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04091">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently been shown to deliver impressive
performance in various NLP tasks. To tackle multi-step reasoning tasks,
few-shot chain-of-thought (CoT) prompting includes a few manually crafted
step-by-step reasoning demonstrations which enable LLMs to explicitly generate
reasoning steps and improve their reasoning task accuracy. To eliminate the
manual effort, Zero-shot-CoT concatenates the target problem statement with
"Let's think step by step" as an input prompt to LLMs. Despite the success of
Zero-shot-CoT, it still suffers from three pitfalls: calculation errors,
missing-step errors, and semantic misunderstanding errors. To address the
missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of
two components: first, devising a plan to divide the entire task into smaller
subtasks, and then carrying out the subtasks according to the plan. To address
the calculation errors and improve the quality of generated reasoning steps, we
extend PS prompting with more detailed instructions and derive PS+ prompting.
We evaluate our proposed prompting strategy on ten datasets across three
reasoning problems. The experimental results over GPT-3 show that our proposed
zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets
by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought
Prompting, and has comparable performance with 8-shot CoT prompting on the math
reasoning problem. The code can be found at
https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04990">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) are so powerful that they sometimes learn
correlations between labels and features that are irrelevant to the task,
leading to poor generalization on out-of-distribution data. We propose
explanation-based finetuning as a general approach to mitigate LLMs' reliance
on spurious correlations. Unlike standard finetuning where the model only
predicts the answer given the input, we finetune the model to additionally
generate a free-text explanation supporting its answer. To evaluate our method,
we finetune the model on artificially constructed training sets containing
different types of spurious cues, and test it on a test set without these cues.
Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably
more robust against spurious cues in terms of accuracy drop across four
classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC
(+6.5). The efficacy generalizes across multiple model families and scales,
with greater gains for larger models. Finally, our method also works well with
explanations generated by the model, implying its applicability to more
datasets without human-written explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANALOGICAL -- A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models. (arXiv:2305.05050v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05050">
<div class="article-summary-box-inner">
<span><p>Over the past decade, analogies, in the form of word-level analogies, have
played a significant role as an intrinsic measure of evaluating the quality of
word embedding methods such as word2vec. Modern large language models (LLMs),
however, are primarily evaluated on extrinsic measures based on benchmarks such
as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs
can draw analogies between long texts. In this paper, we present ANALOGICAL, a
new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of
long text with six levels of complexity -- (i) word, (ii) word vs. sentence,
(iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using
thirteen datasets and three different distance measures, we evaluate the
abilities of eight LLMs in identifying analogical pairs in the semantic vector
space. Our evaluation finds that it is increasingly challenging for LLMs to
identify analogies when going up the analogy taxonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05252">
<div class="article-summary-box-inner">
<span><p>In everyday life, humans often plan their actions by following step-by-step
instructions in the form of goal-oriented scripts. Previous work has exploited
language models (LMs) to plan for abstract goals of stereotypical activities
(e.g., "make a cake"), but leaves more specific goals with multi-facet
constraints understudied (e.g., "make a cake for diabetics"). In this paper, we
define the task of constrained language planning for the first time. We propose
an overgenerate-then-filter approach to improve large language models (LLMs) on
this task, and use it to distill a novel constrained language planning dataset,
CoScript, which consists of 55,000 scripts. Empirical results demonstrate that
our method significantly improves the constrained language planning ability of
LLMs, especially on constraint faithfulness. Furthermore, CoScript is
demonstrated to be quite effective in endowing smaller LMs with constrained
language planning ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data. (arXiv:2305.05295v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05295">
<div class="article-summary-box-inner">
<span><p>Transferring information retrieval (IR) models from a high-resource language
(typically English) to other languages in a zero-shot fashion has become a
widely adopted approach. In this work, we show that the effectiveness of
zero-shot rankers diminishes when queries and documents are present in
different languages. Motivated by this, we propose to train ranking models on
artificially code-switched data instead, which we generate by utilizing
bilingual lexicons. To this end, we experiment with lexicons induced from (1)
cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use
the mMARCO dataset to extensively evaluate reranking models on 36 language
pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual
IR (MLIR). Our results show that code-switching can yield consistent and
substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while
maintaining stable performance in MoIR. Encouragingly, the gains are especially
pronounced for distant languages (up to 2x absolute gain). We further show that
our approach is robust towards the ratio of code-switched tokens and also
extends to unseen languages. Our results demonstrate that training on
code-switched data is a cheap and effective way of generalizing zero-shot
rankers for cross-lingual and multilingual retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.05940">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) unfolds as large language models become capable of
inferring test labels conditioned on a few labeled samples without any gradient
update. ICL-enabled large language models provide a promising step forward
toward bypassing recurrent annotation costs in a low-resource setting. Yet,
only a handful of past studies have explored ICL in a cross-lingual setting, in
which the need for transferring label-knowledge from a high-resource language
to a low-resource one is immensely crucial. To bridge the gap, we provide the
first in-depth analysis of ICL for cross-lingual text classification. We find
that the prevalent mode of selecting random input-label pairs to construct the
prompt-context is severely limited in the case of cross-lingual ICL, primarily
due to the lack of alignment in the input as well as the output spaces. To
mitigate this, we propose a novel prompt construction strategy -- Cross-lingual
In-context Source-Target Alignment (X-InSTA). With an injected coherence in the
semantics of the input examples and a task-based alignment across the source
and target languages, X-InSTA is able to outperform random prompt selection by
a large margin across three different tasks using 44 different cross-lingual
pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.06595">
<div class="article-summary-box-inner">
<span><p>The analysis of consumer sentiment, as expressed through reviews, can provide
a wealth of insight regarding the quality of a product. While the study of
sentiment analysis has been widely explored in many popular languages,
relatively less attention has been given to the Bangla language, mostly due to
a lack of relevant data and cross-domain adaptability. To address this
limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews
consisting of 158,065 samples classified into three broad categories: positive,
negative, and neutral. We provide a detailed statistical analysis of the
dataset and employ a range of machine learning models to establish baselines
including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial
performance advantage of pre-trained models over models that rely on manually
crafted features, emphasizing the necessity for additional training resources
in this domain. Additionally, we conduct an in-depth error analysis by
examining sentiment unigrams, which may provide insight into common
classification errors in under-resourced languages like Bangla. Our codes and
data are publicly available at https://github.com/mohsinulkabir14/BanglaBook.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07622">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have recently received significant attention for
their exceptional capabilities. Despite extensive efforts in developing
general-purpose LLMs that can be utilized in various natural language
processing (NLP) tasks, there has been less research exploring their potential
in recommender systems. In this paper, we propose a novel framework, named
PALR, which aiming to combine user history behaviors (such as clicks,
purchases, ratings, etc.) with LLMs to generate user preferred items.
Specifically, we first use user/item interactions as guidance for candidate
retrieval. Then we adopt a LLM-based ranking model to generate recommended
items. Unlike existing approaches that typically adopt general-purpose LLMs for
zero/few-shot recommendation testing or training on small-sized language models
(with less than 1 billion parameters), which cannot fully elicit LLMs'
reasoning abilities and leverage rich item side parametric knowledge, we
fine-tune a 7 billion parameters LLM for the ranking purpose. This model takes
retrieval candidates in natural language format as input, with instruction
which explicitly asking to select results from input candidates during
inference. Our experimental results demonstrate that our solution outperforms
state-of-the-art models on various sequential recommendation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Melody-Guided Lyrics Generation. (arXiv:2305.07760v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07760">
<div class="article-summary-box-inner">
<span><p>Automatic song writing is a topic of significant practical interest. However,
its research is largely hindered by the lack of training data due to copyright
concerns and challenged by its creative nature. Most noticeably, prior works
often fall short of modeling the cross-modal correlation between melody and
lyrics due to limited parallel data, hence generating lyrics that are less
singable. Existing works also lack effective mechanisms for content control, a
much desired feature for democratizing song creation for people with limited
music background. In this work, we propose to generate pleasantly listenable
lyrics without training on melody-lyric aligned data. Instead, we design a
hierarchical lyric generation framework that disentangles training (based
purely on text) from inference (melody-guided text generation). At inference
time, we leverage the crucial alignments between melody and lyrics and compile
the given melody into constraints to guide the generation process. Evaluation
results show that our model can generate high-quality lyrics that are more
singable, intelligible, coherent, and in rhyme than strong baselines including
those supervised on parallel data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08566">
<div class="article-summary-box-inner">
<span><p>In this study, we analyze automatic evaluation metrics for Natural Language
Generation (NLG), specifically task-agnostic metrics and human-aligned metrics.
Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective
and highly adaptable to diverse NLG tasks, yet they have a weak correlation
with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation
level by incorporating desirable human-like qualities as training objective.
However, their effectiveness at discerning system-level performance and quality
of system outputs remain unclear.
</p>
<p>We present metric preference checklist as a framework to assess the
effectiveness of automatic metrics in three NLG tasks: Text Summarization,
Dialogue Response Generation, and Controlled Generation. Our proposed framework
provides access: (i) for verifying whether automatic metrics are faithful to
human preference, regardless of their correlation level to human; and (ii) for
inspecting the strengths and limitations of NLG systems via pairwise
evaluation. We show that automatic metrics provide a better guidance than human
on discriminating system-level performance in Text Summarization and Controlled
Generation tasks. We also show that multi-aspect human-aligned metric (UniEval)
is not necessarily dominant over single-aspect human-aligned metrics (CTC,
CtrlEval) and task-agnostic metrics (BLEU, BERTScore), particularly in
Controlled Generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.08711">
<div class="article-summary-box-inner">
<span><p>We present sustainAI, an intelligent, context-aware recommender system that
assists auditors and financial investors as well as the general public to
efficiently analyze companies' sustainability reports. The tool leverages an
end-to-end trainable architecture that couples a BERT-based encoding module
with a multi-label classification head to match relevant text passages from
sustainability reports to their respective law regulations from the Global
Reporting Initiative (GRI) standards. We evaluate our model on two novel German
sustainability reporting data sets and consistently achieve a significantly
higher recommendation performance compared to multiple strong baselines.
Furthermore, sustainAI is publicly available for everyone at
https://sustain.ki.nrw/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10156">
<div class="article-summary-box-inner">
<span><p>Comprehending characters' personalities is a crucial aspect of story reading.
As readers engage with a story, their understanding of a character evolves
based on new events and information; and multiple fine-grained aspects of
personalities can be perceived. This leads to a natural problem of situated and
fine-grained personality understanding. The problem has not been studied in the
NLP field, primarily due to the lack of appropriate datasets mimicking the
process of book reading. We present the first labeled dataset PersoNet for this
problem. Our novel annotation strategy involves annotating user notes from
online reading apps as a proxy for the original books. Experiments and human
studies indicate that our dataset construction is both efficient and accurate;
and our task heavily relies on long-term context to achieve accurate
predictions for both machines and humans. The dataset is available at
https://github.com/Gorov/personet_acl23.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paxion: Patching Action Knowledge in Video-Language Foundation Models. (arXiv:2305.10683v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10683">
<div class="article-summary-box-inner">
<span><p>Action knowledge involves the understanding of textual, visual, and temporal
aspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)
containing two carefully designed probing tasks: Action Antonym and Video
Reversal, which targets multimodal alignment capabilities and temporal
understanding skills of the model, respectively. Despite recent video-language
models' (VidLM) impressive performance on various benchmark tasks, our
diagnostic tasks reveal their surprising deficiency (near-random performance)
in action knowledge, suggesting that current models rely on object recognition
abilities as a shortcut for action understanding. To remedy this, we propose a
novel framework, Paxion, along with a new Discriminative Video Dynamics
Modeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher
network to encode new action knowledge and a Knowledge Fuser component to
integrate the Patcher into frozen VidLMs without compromising their existing
capabilities. Due to limitations of the widely-used Video-Text Contrastive
(VTC) loss for learning action knowledge, we introduce the DVDM objective to
train the Knowledge Patcher. DVDM forces the model to encode the correlation
between the action text and the correct ordering of video frames. Our extensive
analyses show that Paxion and DVDM together effectively fill the gap in action
knowledge understanding (~50% to 80%), while maintaining or improving
performance on a wide spectrum of both object- and action-centric downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10688">
<div class="article-summary-box-inner">
<span><p>Generative pre-trained Transformer (GPT) has demonstrates its great success
in natural language processing and related techniques have been adapted into
molecular modeling. Considering that text is the most important record for
scientific discovery, in this paper, we propose MolXPT, a unified language
model of text and molecules pre-trained on SMILES (a sequence representation of
molecules) wrapped by text. Briefly, we detect the molecule names in each
sequence and replace them to the corresponding SMILES. In this way, the SMILES
could leverage the information from surrounding text, and vice versa. The above
wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem
are all fed into a language model for pre-training. Experimental results
demonstrate that MolXPT outperforms strong baselines of molecular property
prediction on MoleculeNet, performs comparably to the best model in
text-molecule translation while using less than half of its parameters, and
enables zero-shot molecular generation without finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11029">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (DocRE) aims to infer complex semantic
relations among entities in a document. Distant supervision (DS) is able to
generate massive auto-labeled data, which can improve DocRE performance. Recent
works leverage pseudo labels generated by the pre-denoising model to reduce
noise in DS data. However, unreliable pseudo labels bring new noise, e.g.,
adding false pseudo labels and losing correct DS labels. Therefore, how to
select effective pseudo labels to denoise DS data is still a challenge in
document-level distant relation extraction. To tackle this issue, we introduce
uncertainty estimation technology to determine whether pseudo labels can be
trusted. In this work, we propose a Document-level distant Relation Extraction
framework with Uncertainty Guided label denoising, UGDRE. Specifically, we
propose a novel instance-level uncertainty estimation method, which measures
the reliability of the pseudo labels with overlapping relations. By further
considering the long-tail problem, we design dynamic uncertainty thresholds for
different types of relations to filter high-uncertainty pseudo labels. We
conduct experiments on two public datasets. Our framework outperforms strong
baselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11038">
<div class="article-summary-box-inner">
<span><p>Named entity recognition in real-world applications suffers from the
diversity of entity types, the emergence of new entity types, and the lack of
high-quality annotations. To address the above problems, this paper proposes an
in-context learning-based NER approach, which can effectively inject in-context
NER ability into PLMs and recognize entities of novel types on-the-fly using
only a few demonstrative instances. Specifically, we model PLMs as a
meta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}.
M}$, and a new entity extractor can be implicitly constructed by applying new
instruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M)
}$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will be
a new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject the
above in-context NER ability into PLMs, we propose a meta-function pre-training
algorithm, which pre-trains PLMs by comparing the (instruction,
demonstration)-initialized extractor with a surrogate golden extractor.
Experimental results on 4 few-shot NER datasets show that our method can
effectively inject in-context NER ability into PLMs and significantly
outperforms the PLMs+fine-tuning counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages. (arXiv:2305.12182v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12182">
<div class="article-summary-box-inner">
<span><p>The NLP community has mainly focused on scaling Large Language Models (LLMs)
vertically, i.e., making them better for about 100 languages. We instead scale
LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM
that covers 511 predominantly low-resource languages. An important part of this
effort is to collect and clean Glot500-c, a corpus that covers these 511
languages and allows us to train Glot500-m. We evaluate Glot500-m on five
diverse tasks across these languages. We observe large improvements for both
high-resource and low-resource languages compared to an XLM-R baseline. Our
analysis shows that no single factor explains the quality of multilingual LLM
representations. Rather, a combination of factors determines quality including
corpus size, script, "help" from related languages and the total capacity of
the model. Our work addresses an important goal of NLP research: we should not
limit NLP to a small fraction of the world's languages and instead strive to
support as many languages as possible to bring the benefits of NLP technology
to all languages and cultures. Code, data and models are available at
https://github.com/cisnlp/Glot500.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Head State Space Model for Speech Recognition. (arXiv:2305.12498v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12498">
<div class="article-summary-box-inner">
<span><p>State space models (SSMs) have recently shown promising results on
small-scale sequence and language modelling tasks, rivalling and outperforming
many attention-based approaches. In this paper, we propose a multi-head state
space (MH-SSM) architecture equipped with special gating mechanisms, where
parallel heads are taught to learn local and global temporal dynamics on
sequence data. As a drop-in replacement for multi-head attention in transformer
encoders, this new model significantly outperforms the transformer transducer
on the LibriSpeech speech recognition corpus. Furthermore, we augment the
transformer block with MH-SSMs layers, referred to as the Stateformer,
achieving state-of-the-art performance on the LibriSpeech task, with word error
rates of 1.76\%/4.37\% on the development and 1.91\%/4.36\% on the test sets
without using an external language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition. (arXiv:2305.12676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.12676">
<div class="article-summary-box-inner">
<span><p>Energy-based language models (ELMs) parameterize an unnormalized distribution
for natural sentences and are radically different from popular autoregressive
language models (ALMs). As an important application, ELMs have been
successfully used as a means for calculating sentence scores in speech
recognition, but they all use less-modern CNN or LSTM networks. The recent
progress in Transformer networks and large pretrained models such as BERT and
GPT2 opens new possibility to further advancing ELMs. In this paper, we explore
different architectures of energy functions and different training methods to
investigate the capabilities of ELMs in rescoring for speech recognition, all
using large pretrained models as backbones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gene Set Summarization using Large Language Models. (arXiv:2305.13338v2 [q-bio.GN] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13338">
<div class="article-summary-box-inner">
<span><p>Molecular biologists frequently interpret gene lists derived from
high-throughput experiments and computational analysis. This is typically done
as a statistical enrichment analysis that measures the over- or
under-representation of biological function terms associated with genes or
their properties, based on curated assertions from a knowledge base (KB) such
as the Gene Ontology (GO). Interpreting gene lists can also be framed as a
textual summarization task, enabling the use of Large Language Models (LLMs),
potentially utilizing scientific texts directly and avoiding reliance on a KB.
</p>
<p>We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language
Descriptions of Controlled Terms for Ontology Reporting), a method that uses
GPT models to perform gene set function summarization as a complement to
standard enrichment analysis. This method can use different sources of gene
functional information: (1) structured text derived from curated ontological KB
annotations, (2) ontology-free narrative gene summaries, or (3) direct model
retrieval.
</p>
<p>We demonstrate that these methods are able to generate plausible and
biologically valid summary GO term lists for gene sets. However, GPT-based
approaches are unable to deliver reliable scores or p-values and often return
terms that are not statistically significant. Crucially, these methods were
rarely able to recapitulate the most precise and informative term from standard
enrichment, likely due to an inability to generalize and reason using an
ontology. Results are highly nondeterministic, with minor variations in prompt
resulting in radically different term lists. Our results show that at this
point, LLM-based methods are unsuitable as a replacement for standard term
enrichment analysis and that manual curation of ontological assertions remains
necessary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning the Norwegian UD Treebank with Entity and Coreference Information. (arXiv:2305.13527v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13527">
<div class="article-summary-box-inner">
<span><p>This paper presents a merged collection of entity and coreference annotated
data grounded in the Universal Dependencies (UD) treebanks for the two written
forms of Norwegian: Bokm{\aa}l and Nynorsk. The aligned and converted corpora
are the Norwegian Named Entities (NorNE) and Norwegian Anaphora Resolution
Corpus (NARC). While NorNE is aligned with an older version of the treebank,
NARC is misaligned and requires extensive transformation from the original
annotations to the UD structure and CoNLL-U format. We here demonstrate the
conversion and alignment processes, along with an analysis of discovered issues
and errors in the data - some of which include data split overlaps in the
original treebank. These procedures and the developed system may prove helpful
for future corpus alignment and coreference annotation endeavors. The merged
corpora comprise the first Norwegian UD treebank enriched with named entities
and coreference information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipschitz Restraint. (arXiv:2305.13599v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13599">
<div class="article-summary-box-inner">
<span><p>A self-explaining rationalization model is generally constructed by a
cooperative game where a generator selects the most human-intelligible pieces
from the input text as rationales, followed by a predictor that makes
predictions based on the selected rationales. However, such a cooperative game
may incur the degeneration problem where the predictor overfits to the
uninformative pieces generated by a not yet well-trained generator and in turn,
leads the generator to converge to a sub-optimal model that tends to select
senseless pieces. In this paper, we theoretically bridge degeneration with the
predictor's Lipschitz continuity. Then, we empirically propose a simple but
effective method named DR, which can naturally and flexibly restrain the
Lipschitz constant of the predictor, to address the problem of degeneration.
The main idea of DR is to decouple the generator and predictor to allocate them
with asymmetric learning rates. A series of experiments conducted on two widely
used benchmarks have verified the effectiveness of the proposed method. Codes:
\href{https://github.com/jugechengzi/Rationalization-DR}{https://github.com/jugechengzi/Rationalization-DR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.13617">
<div class="article-summary-box-inner">
<span><p>Event-centric structured prediction involves predicting structured outputs of
events. In most NLP cases, event structures are complex with manifold
dependency, and it is challenging to effectively represent these complicated
structured events. To address these issues, we propose Structured Prediction
with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex
dependency among event structured components with energy-based modeling, and
represents event classes with simple but effective hyperspheres. Experiments on
two unified-annotated event datasets indicate that SPEECH is predominant in
event detection and event-relation extraction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14580">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) systems play a key role in applications
involving human-machine interactions. Despite their importance, ASR models for
the Portuguese language proposed in the last decade have limitations in
relation to the correct identification of punctuation marks in automatic
transcriptions, which hinder the use of transcriptions by other systems,
models, and even by humans. However, recently Whisper ASR was proposed by
OpenAI, a general-purpose speech recognition model that has generated great
expectations in dealing with such limitations. This chapter presents the first
study on the performance of Whisper for punctuation prediction in the
Portuguese language. We present an experimental evaluation considering both
theoretical aspects involving pausing points (comma) and complete ideas
(exclamation, question, and fullstop), as well as practical aspects involving
transcript-based topic modeling - an application dependent on punctuation marks
for promising performance. We analyzed experimental results from videos of
Museum of the Person, a virtual museum that aims to tell and preserve people's
life histories, thus discussing the pros and cons of Whisper in a real-world
scenario. Although our experiments indicate that Whisper achieves
state-of-the-art results, we conclude that some punctuation marks require
improvements, such as exclamation, semicolon and colon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification. (arXiv:2305.15282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15282">
<div class="article-summary-box-inner">
<span><p>In recent years, large language models (LLMs) have achieved strong
performance on benchmark tasks, especially in zero or few-shot settings.
However, these benchmarks often do not adequately address the challenges posed
in the real-world, such as that of hierarchical classification. In order to
address this challenge, we propose refactoring conventional tasks on
hierarchical datasets into a more indicative long-tail prediction task. We
observe LLMs are more prone to failure in these cases. To address these
limitations, we propose the use of entailment-contradiction prediction in
conjunction with LLMs, which allows for strong performance in a strict
zero-shot setting. Importantly, our method does not require any parameter
updates, a resource-intensive process and achieves strong performance across
multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15645">
<div class="article-summary-box-inner">
<span><p>In conversational search, the user's real search intent for the current turn
is dependent on the previous conversation history. It is challenging to
determine a good search query from the whole conversation context. To avoid the
expensive re-training of the query encoder, most existing methods try to learn
a rewriting model to de-contextualize the current query by mimicking the manual
query rewriting. However, manually rewritten queries are not always the best
search queries. Training a rewriting model on them would limit the model's
ability to produce good search queries. Another useful hint is the potential
answer to the question. In this paper, we propose ConvGQR, a new framework to
reformulate conversational queries based on generative pre-trained language
models (PLMs), one for query rewriting and another for generating potential
answers. By combining both, ConvGQR can produce better search queries. In
addition, to relate query reformulation to retrieval performance, we propose a
knowledge infusion mechanism to optimize both query reformulation and
retrieval. Extensive experiments on four conversational search datasets
demonstrate the effectiveness of ConvGQR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15722">
<div class="article-summary-box-inner">
<span><p>The term "Code Mixed" refers to the use of more than one language in the same
text. This phenomenon is predominantly observed on social media platforms, with
an increasing amount of adaptation as time goes on. It is critical to detect
foreign elements in a language and process them correctly, as a considerable
number of individuals are using code-mixed languages that could not be
comprehended by understanding one of those languages. In this work, we focus on
low-resource Hindi-English code-mixed language and enhancing the performance of
different code-mixed natural language processing tasks such as sentiment
analysis, emotion recognition, and hate speech identification. We perform a
comparative analysis of different Transformer-based language Models pre-trained
using unsupervised approaches. We have included the code-mixed models like
HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like
AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-English
downstream tasks. We report state-of-the-art results on respective datasets
using HingBERT-based models which are specifically pre-trained on real
code-mixed text. Our HingBERT-based models provide significant improvements
thus highlighting the poor performance of vanilla BERT models on code-mixed
text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversity-Aware Coherence Loss for Improving Neural Topic Models. (arXiv:2305.16199v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16199">
<div class="article-summary-box-inner">
<span><p>The standard approach for neural topic modeling uses a variational
autoencoder (VAE) framework that jointly minimizes the KL divergence between
the estimated posterior and prior, in addition to the reconstruction loss.
Since neural topic models are trained by recreating individual input documents,
they do not explicitly capture the coherence between topic words on the corpus
level. In this work, we propose a novel diversity-aware coherence loss that
encourages the model to learn corpus-level coherence scores while maintaining a
high diversity between topics. Experimental results on multiple datasets show
that our method significantly improves the performance of neural topic models
without requiring any pretraining or additional parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNITE: A Unified Benchmark for Text-to-SQL Evaluation. (arXiv:2305.16265v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.16265">
<div class="article-summary-box-inner">
<span><p>A practical text-to-SQL system should generalize well on a wide variety of
natural language questions, unseen database schemas, and novel SQL query
structures. To comprehensively evaluate text-to-SQL systems, we introduce a
\textbf{UNI}fied benchmark for \textbf{T}ext-to-SQL \textbf{E}valuation
(UNITE). It is composed of publicly available text-to-SQL datasets, containing
natural language questions from more than 12 domains, SQL queries from more
than 3.9K patterns, and 29K databases. Compared to the widely used Spider
benchmark \cite{yu-etal-2018-spider}, we introduce $\sim$120K additional
examples and a threefold increase in SQL patterns, such as comparative and
boolean questions. We conduct a systematic study of six state-of-the-art (SOTA)
text-to-SQL parsers on our new benchmark and show that: 1) Codex performs
surprisingly well on out-of-domain datasets; 2) specially designed decoding
methods (e.g. constrained beam search) can improve performance for both
in-domain and out-of-domain settings; 3) explicitly modeling the relationship
between questions and schemas further improves the Seq2Seq models. More
importantly, our benchmark presents key challenges towards compositional
generalization and robustness issues -- which these SOTA models cannot address
well. \footnote{Our code and data processing script will be available at
\url{https://github.com/XXXX.}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?. (arXiv:2212.10784v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10784">
<div class="article-summary-box-inner">
<span><p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of
annotations and the prevalence of instances without explicitly pre-defined
labels due to low annotation coverage. Existing approaches, which treat
biomedical RE as a multi-class classification task, often result in poor
generalization in low-resource settings and do not have the ability to make
selective prediction on unknown cases but give a guess from seen relations,
hindering the applicability of those approaches. We present NBR, which converts
biomedical RE as natural language inference formulation through indirect
supervision. By converting relations to natural language hypotheses, NBR is
capable of exploiting semantic cues to alleviate annotation scarcity. By
incorporating a ranking-based loss that implicitly calibrates abstinent
instances, NBR learns a clearer decision boundary and is instructed to abstain
on uncertain instances. Extensive experiments on three widely-used biomedical
RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in
both full-set and low-resource regimes. Our analysis demonstrates that indirect
supervision benefits biomedical RE even when a domain gap exists, and combining
NLI knowledge with biomedical knowledge leads to the best performance gains.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-29 23:10:54.875693466 UTC">2023-05-29 23:10:54 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>