<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-12-16T01:30:00Z">12-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Hope Speech Detection on Social Media Platforms. (arXiv:2212.07424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07424">
<div class="article-summary-box-inner">
<span><p>Since personal computers became widely available in the consumer market, the
amount of harmful content on the internet has significantly expanded. In simple
terms, harmful content is anything online which causes a person distress or
harm. It may include hate speech, violent content, threats, non-hope speech,
etc. The online content must be positive, uplifting and supportive. Over the
past few years, many studies have focused on solving this problem through hate
speech detection, but very few focused on identifying hope speech. This paper
discusses various machine learning approaches to identify a sentence as Hope
Speech, Non-Hope Speech, or a Neutral sentence. The dataset used in the study
contains English YouTube comments and is released as a part of the shared task
"EACL-2021: Hope Speech Detection for Equality, Diversity, and Inclusion".
Initially, the dataset obtained from the shared task had three classes: Hope
Speech, non-Hope speech, and not in English; however, upon deeper inspection,
we discovered that dataset relabeling is required. A group of undergraduates
was hired to help perform the entire dataset's relabeling task. We experimented
with conventional machine learning models (such as Na\"ive Bayes, logistic
regression and support vector machine) and pre-trained models (such as BERT) on
relabeled data. According to the experimental results, the relabeled data has
achieved a better accuracy for Hope speech identification than the original
data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments. (arXiv:2212.07425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07425">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation, propaganda, and flawed argumentation has been
amplified in the Internet era. Given the volume of data and the subtlety of
identifying violations of argumentation norms, supporting information analytics
tasks, like content moderation, with trustworthy methods that can identify
logical fallacies is essential. In this paper, we formalize prior theoretical
work on logical fallacies into a comprehensive three-stage evaluation framework
of detection, coarse-grained, and fine-grained classification. We adapt
existing evaluation datasets for each stage of the evaluation. We devise three
families of robust and explainable methods based on prototype reasoning,
instance-based reasoning, and knowledge injection. The methods are designed to
combine language models with background knowledge and explainable mechanisms.
Moreover, we address data sparsity with strategies for data augmentation and
curriculum learning. Our three-stage framework natively consolidates prior
datasets and methods from existing tasks, like propaganda detection, serving as
an overarching evaluation testbed. We extensively evaluate these methods on our
datasets, focusing on their robustness and explainability. Our results provide
insight into the strengths and weaknesses of the methods on different
components and fallacy classes, indicating that fallacy identification is a
challenging task that may require specialized forms of reasoning to capture
various classes. We share our open-source code and data on GitHub to support
further work on logical fallacy identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference. (arXiv:2212.07428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07428">
<div class="article-summary-box-inner">
<span><p>We introduce a linguistically enhanced combination of pre-training methods
for transformers. The pre-training objectives include POS-tagging, synset
prediction based on semantic knowledge graphs, and parent prediction based on
dependency parse trees. Our approach achieves competitive results on the
Natural Language Inference task, compared to the state of the art. Specifically
for smaller models, the method results in a significant performance boost,
emphasizing the fact that intelligent pre-training can make up for fewer
parameters and help building more efficient models. Combining POS-tagging and
synset prediction yields the overall best results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Multilingual Corpora for a Complex Named Entity Recognition and Classification Hierarchy using Wikipedia and DBpedia. (arXiv:2212.07429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07429">
<div class="article-summary-box-inner">
<span><p>With the ever-growing popularity of the field of NLP, the demand for datasets
in low resourced-languages follows suit. Following a previously established
framework, in this paper, we present the UNER dataset, a multilingual and
hierarchical parallel corpus annotated for named-entities. We describe in
detail the developed procedure necessary to create this type of dataset in any
language available on Wikipedia with DBpedia information. The three-step
procedure extracts entities from Wikipedia articles, links them to DBpedia, and
maps the DBpedia sets of classes to the UNER labels. This is followed by a
post-processing procedure that significantly increases the number of identified
entities in the final results. The paper concludes with a statistical and
qualitative analysis of the resulting dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Infinite Index: Information Retrieval on Generative Text-To-Image Models. (arXiv:2212.07476v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07476">
<div class="article-summary-box-inner">
<span><p>The text-to-image model Stable Diffusion has recently become very popular.
Only weeks after its open source release, millions are experimenting with image
generation. This is due to its ease of use, since all it takes is a brief
description of the desired image to "prompt" the generative model. Rarely do
the images generated for a new prompt immediately meet the user's expectations.
Usually, an iterative refinement of the prompt ("prompt engineering") is
necessary for satisfying images. As a new perspective, we recast image prompt
engineering as interactive image retrieval - on an "infinite index". Thereby, a
prompt corresponds to a query and prompt engineering to query refinement.
Selected image-prompt pairs allow direct relevance feedback, as the model can
modify an image for the refined prompt. This is a form of one-sided interactive
retrieval, where the initiative is on the user side, whereas the server side
remains stateless. In light of an extensive literature review, we develop these
parallels in detail and apply the findings to a case study of a creative search
task on such a model. We note that the uncertainty in searching an infinite
index is virtually never-ending. We also discuss future research opportunities
related to retrieval models specialized for generative models and interactive
generative image retrieval. The application of IR technology, such as query
reformulation and relevance feedback, will contribute to improved workflows
when using generative models, while the notion of an infinite index raises new
challenges in IR research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Intelligence for Health Message Generation: Theory, Method, and an Empirical Study Using Prompt Engineering. (arXiv:2212.07507v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07507">
<div class="article-summary-box-inner">
<span><p>This study introduces and examines the potential of an AI system to generate
health awareness messages. The topic of folic acid, a vitamin that is critical
during pregnancy, served as a test case. Using prompt engineering, we generated
messages that could be used to raise awareness and compared them to retweeted
human-generated messages via computational and human evaluation methods. The
system was easy to use and prolific, and computational analyses revealed that
the AI-generated messages were on par with human-generated ones in terms of
sentiment, reading ease, and semantic content. Also, the human evaluation study
showed that AI-generated messages ranked higher in message quality and clarity.
We discuss the theoretical, practical, and ethical implications of these
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language. (arXiv:2212.07525v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07525">
<div class="article-summary-box-inner">
<span><p>Current self-supervised learning algorithms are often modality-specific and
require large amounts of computational resources. To address these issues, we
increase the training efficiency of data2vec, a learning objective that
generalizes across several modalities. We do not encode masked tokens, use a
fast convolutional decoder and amortize the effort to build teacher
representations. data2vec 2.0 benefits from the rich contextualized target
representations introduced in data2vec which enable a fast self-supervised
learner. Experiments on ImageNet-1K image classification show that data2vec 2.0
matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,
on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x
less time, and on GLUE natural language understanding it matches a retrained
RoBERTa model in half the time. Trading some speed for accuracy results in
ImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relationship Between Online Harmful Behaviors and Social Network Message Writing Style. (arXiv:2212.07526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07526">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the relationship between an individual's writing
style and the risk that they will engage in online harmful behaviors (such as
cyberbullying). In particular, we consider whether measurable differences in
writing style relate to different personality types, as modeled by the Big-Five
personality traits and the Dark Triad traits, and can differentiate between
users who do or do not engage in harmful behaviors. We study messages from
nearly 2,500 users from two online communities (Twitter and Reddit) and find
that we can measure significant personality differences between regular and
harmful users from the writing style of as few as 100 tweets or 40 Reddit
posts, aggregate these values to distinguish between healthy and harmful
communities, and also use style attributes to predict which users will engage
in harmful behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07530">
<div class="article-summary-box-inner">
<span><p>Multilingual machine translation models can benefit from synergy between
different language pairs, but also suffer from interference. While there is a
growing number of sophisticated methods that aim to eliminate interference, our
understanding of interference as a phenomenon is still limited. This work
identifies the main factors that contribute to interference in multilingual
machine translation. Through systematic experimentation, we find that
interference (or synergy) are primarily determined by model size, data size,
and the proportion of each language pair within the total dataset. We observe
that substantial interference occurs mainly when the model is very small with
respect to the available training data, and that using standard transformer
configurations with less than one billion parameters largely alleviates
interference and promotes synergy. Moreover, we show that tuning the sampling
temperature to control the proportion of each language pair in the data is key
to balancing the amount of interference between low and high resource language
pairs effectively, and can lead to superior performance overall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record. (arXiv:2212.07538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07538">
<div class="article-summary-box-inner">
<span><p>Objective: Social Determinants of Health (SDOH) influence personal health
outcomes and health systems interactions. Health systems capture SDOH
information through structured data and unstructured clinical notes; however,
clinical notes often contain a more comprehensive representation of several key
SDOH. The objective of this work is to assess the SDOH information gain
achievable by extracting structured semantic representations of SDOH from the
clinical narrative and combining these extracted representations with available
structured data.
</p>
<p>Materials and Methods: We developed a natural language processing (NLP)
information extraction model for SDOH that utilizes a deep learning entity and
relation extraction architecture. In an electronic health record (EHR) case
study, we applied the SDOH extractor to a large existing clinical data set with
over 200,000 patients and 400,000 notes and compared the extracted information
with available structured data.
</p>
<p>Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the
EHR case study, we found 19\% of current tobacco users, 10\% of drug users, and
32\% of homeless patients only include documentation of these risk factors in
the clinical narrative.
</p>
<p>Conclusions: Patients who are at-risk for negative health outcomes due to
SDOH may be better served if health systems are able to identify SDOH risk
factors and associated social needs. Structured semantic representations of
text-encoded SDOH information can augment existing structured, and this more
comprehensive SDOH representation can assist health systems in identifying and
addressing social needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent Recognition and Question Answering Architecture. (arXiv:2212.07542v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07542">
<div class="article-summary-box-inner">
<span><p>As artificial intelligence (AI) becomes a prominent part of modern life, AI
literacy is becoming important for all citizens, not just those in technology
careers. Previous research in AI education materials has largely focused on the
introduction of terminology as well as AI use cases and ethics, but few allow
students to learn by creating their own machine learning models. Therefore,
there is a need for enriching AI educational tools with more adaptable and
flexible platforms for interested educators with any level of technical
experience to utilize within their teaching material. As such, we propose the
development of an open-source tool (Build-a-Bot) for students and teachers to
not only create their own transformer-based chatbots based on their own course
material, but also learn the fundamentals of AI through the model creation
process. The primary concern of this paper is the creation of an interface for
students to learn the principles of artificial intelligence by using a natural
language pipeline to train a customized model to answer questions based on
their own school curriculums. The model uses contexts given by their
instructor, such as chapters of a textbook, to answer questions and is deployed
on an interactive chatbot/voice agent. The pipeline teaches students data
collection, data augmentation, intent recognition, and question answering by
having them work through each of these processes while creating their AI agent,
diverging from previous chatbot work where students and teachers use the bots
as black-boxes with no abilities for customization or the bots lack AI
capabilities, with the majority of dialogue scripts being rule-based. In
addition, our tool is designed to make each step of this pipeline intuitive for
students at a middle-school level. Further work primarily lies in providing our
tool to schools and seeking student and teacher evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology. (arXiv:2212.07547v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07547">
<div class="article-summary-box-inner">
<span><p>We propose a fully unsupervised method to detect bias in contextualized
embeddings. The method leverages the assortative information latently encoded
by social networks and combines orthogonality regularization, structured
sparsity learning, and graph neural networks to find the embedding subspace
capturing this information. As a concrete example, we focus on the phenomenon
of ideological bias: we introduce the concept of an ideological subspace, show
how it can be found by applying our method to online discussion forums, and
present techniques to probe it. Our experiments suggest that the ideological
subspace encodes abstract evaluative semantics and reflects changes in the
political left-right spectrum during the presidency of Donald Trump.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReDDIT: Regret Detection and Domain Identification from Text. (arXiv:2212.07549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07549">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a study of regret and its expression on social
media platforms. Specifically, we present a novel dataset of Reddit texts that
have been classified into three classes: Regret by Action, Regret by Inaction,
and No Regret. We then use this dataset to investigate the language used to
express regret on Reddit and to identify the domains of text that are most
commonly associated with regret. Our findings show that Reddit users are most
likely to express regret for past actions, particularly in the domain of
relationships. We also found that deep learning models using GloVe embedding
outperformed other models in all experiments, indicating the effectiveness of
GloVe for representing the meaning and context of words in the domain of
regret. Overall, our study provides valuable insights into the nature and
prevalence of regret on social media, as well as the potential of deep learning
and word embeddings for analyzing and understanding emotional language in
online text. These findings have implications for the development of natural
language processing algorithms and the design of social media platforms that
support emotional expression and communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation. (arXiv:2212.07571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07571">
<div class="article-summary-box-inner">
<span><p>Sparsely gated Mixture of Experts (MoE) models have been shown to be a
compute-efficient method to scale model capacity for multilingual machine
translation. However, for low-resource tasks, MoE models severely over-fit. We
show effective regularization strategies, namely dropout techniques for MoE
layers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods
that prevent over-fitting and improve the performance of MoE models on
low-resource tasks without adversely affecting high-resource tasks. On a
massively multilingual machine translation benchmark, our strategies result in
about +1 chrF++ improvement in very low resource language pairs. We perform an
extensive analysis of the learned MoE routing to better understand the impact
of our regularization methods and how we can improve them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking. (arXiv:2212.07617v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07617">
<div class="article-summary-box-inner">
<span><p>Masked language modeling (MLM) has been widely used for pre-training
effective bidirectional representations, but incurs substantial training costs.
In this paper, we propose a novel concept-based curriculum masking (CCM) method
to efficiently pre-train a language model. CCM has two key differences from
existing curriculum learning approaches to effectively reflect the nature of
MLM. First, we introduce a carefully-designed linguistic difficulty criterion
that evaluates the MLM difficulty of each token. Second, we construct a
curriculum that gradually masks words related to the previously masked words by
retrieving a knowledge graph. Experimental results show that CCM significantly
improves pre-training efficiency. Specifically, the model trained with CCM
shows comparative performance with the original BERT on the General Language
Understanding Evaluation benchmark at half of the training cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07634">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models achieve superior performance, but they are
computationally expensive due to their large size. Techniques such as pruning
and knowledge distillation (KD) have been developed to reduce their size and
latency. In most structural pruning methods, the pruning units, such as
attention heads and feed-forward hidden dimensions, only span a small model
structure space and limit the structures that the pruning algorithm can
explore. In this work, we propose Gradient-based Intra-attention pruning
(GRAIN), which inspects fine intra-attention structures, and allows different
heads to have different sizes. Intra-attention pruning greatly expands the
searching space of model structures and yields highly heterogeneous structures.
We further propose structure regularization to encourage generating more
regular structures, which achieves higher speedups than heterogeneous ones. We
also integrate KD into the pruning process with a gradient separation strategy
to reduce the interference of KD with the pruning process. GRAIN is evaluated
on a variety of tasks. Results show that it notably outperforms other methods
at the same or similar model size. Even under extreme compression where only
$3\%$ weights in transformers remain, the pruned model is still competitive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Text Classification Accuracy with Intent Information. (arXiv:2212.07649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07649">
<div class="article-summary-box-inner">
<span><p>Text classification, a core component of task-oriented dialogue systems,
attracts continuous research from both the research and industry community, and
has resulted in tremendous progress. However, existing method does not consider
the use of label information, which may weaken the performance of text
classification systems in some token-aware scenarios. To address the problem,
in this paper, we introduce the use of label information as label embedding for
the task of text classification and achieve remarkable performance on benchmark
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Two Losses and Two Datasets Simultaneously to Improve TempoWiC Accuracy. (arXiv:2212.07669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07669">
<div class="article-summary-box-inner">
<span><p>WSD (Word Sense Disambiguation) is the task of identifying which sense of a
word is meant in a sentence or other segment of text. Researchers have worked
on this task (e.g. Pustejovsky, 2002) for years but it's still a challenging
one even for SOTA (state-of-the-art) LMs (language models). The new dataset,
TempoWiC introduced by Loureiro et al. (2022b) focuses on the fact that words
change over time. Their best baseline achieves 70.33% macro-F1. In this work,
we use two different losses simultaneously to train RoBERTa-based
classification models. We also improve our model by using another similar
dataset to generalize better. Our best configuration beats their best baseline
by 4.23% and reaches 74.56% macroF1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization. (arXiv:2212.07672v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07672">
<div class="article-summary-box-inner">
<span><p>The goal of multimodal abstractive summarization (MAS) is to produce a
concise summary given the multimodal data (text and vision). Existing studies
on MAS mainly focus on how to effectively use the extracted visual features,
having achieved impressive success on the high-resource English dataset.
However, less attention has been paid to the quality of the visual features to
the summary, which may limit the model performance especially in the low- and
zero-resource scenarios. In this paper, we propose to improve the summary
quality through summary-oriented visual features. To this end, we devise two
auxiliary tasks including \emph{vision to summary task} and \emph{masked image
modeling task}. Together with the main summarization task, we optimize the MAS
model via the training objectives of all these tasks. By these means, the MAS
model can be enhanced by capturing the summary-oriented visual features,
thereby yielding more accurate summaries. Experiments on 44 languages, covering
mid-high-, low-, and zero-resource scenarios, verify the effectiveness and
superiority of the proposed approach, which achieves state-of-the-art
performance under all scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers learn in-context by gradient descent. (arXiv:2212.07677v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07677">
<div class="article-summary-box-inner">
<span><p>Transformers have become the state-of-the-art neural network architecture
across numerous domains of machine learning. This is partly due to their
celebrated ability to transfer and to learn in-context based on few examples.
Nevertheless, the mechanisms by which Transformers become in-context learners
are not well understood and remain mostly an intuition. Here, we argue that
training Transformers on auto-regressive tasks can be closely related to
well-known gradient-based meta-learning formulations. We start by providing a
simple weight construction that shows the equivalence of data transformations
induced by 1) a single linear self-attention layer and by 2) gradient-descent
(GD) on a regression loss. Motivated by that construction, we show empirically
that when training self-attention-only Transformers on simple regression tasks
either the models learned by GD and Transformers show great similarity or,
remarkably, the weights found by optimization match the construction. Thus we
show how trained Transformers implement gradient descent in their forward pass.
This allows us, at least in the domain of regression problems, to
mechanistically understand the inner workings of optimized Transformers that
learn in-context. Furthermore, we identify how Transformers surpass plain
gradient descent by an iterative curvature correction and learn linear models
on deep data representations to solve non-linear regression tasks. Finally, we
discuss intriguing parallels to a mechanism identified to be crucial for
in-context learning termed induction-head (Olsson et al., 2022) and show how it
could be understood as a specific case of in-context learning by gradient
descent learning within Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-based Disentanglement with Distant Supervision. (arXiv:2212.07699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07699">
<div class="article-summary-box-inner">
<span><p>Disentangled representation learning remains challenging as ground truth
factors of variation do not naturally exist. To address this, we present
Vocabulary Disentanglement Retrieval~(VDR), a simple yet effective
retrieval-based disentanglement framework that leverages nature language as
distant supervision. Our approach is built upon the widely-used bi-encoder
architecture with disentanglement heads and is trained on data-text pairs that
are readily available on the web or in existing datasets. This makes our
approach task- and modality-agnostic with potential for a wide range of
downstream applications. We conduct experiments on 16 datasets in both
text-to-text and cross-modal scenarios and evaluate VDR in a zero-shot setting.
With the incorporation of disentanglement heads and a minor increase in
parameters, VDR achieves significant improvements over the base retriever it is
built upon, with a 9% higher on NDCG@10 scores in zero-shot text-to-text
retrieval and an average of 13% higher recall in cross-modal retrieval. In
comparison to other baselines, VDR outperforms them in most tasks, while also
improving explainability and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FreCDo: A Large Corpus for French Cross-Domain Dialect Identification. (arXiv:2212.07707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07707">
<div class="article-summary-box-inner">
<span><p>We present a novel corpus for French dialect identification comprising
413,522 French text samples collected from public news websites in Belgium,
Canada, France and Switzerland. To ensure an accurate estimation of the dialect
identification performance of models, we designed the corpus to eliminate
potential biases related to topic, writing style, and publication source. More
precisely, the training, validation and test splits are collected from
different news websites, while searching for different keywords (topics). This
leads to a French cross-domain (FreCDo) dialect identification task. We conduct
experiments with four competitive baselines, a fine-tuned CamemBERT model, an
XGBoost based on fine-tuned CamemBERT features, a Support Vector Machines (SVM)
classifier based on fine-tuned CamemBERT features, and an SVM based on word
n-grams. Aside from presenting quantitative results, we also make an analysis
of the most discriminative features learned by CamemBERT. Our corpus is
available at https://github.com/MihaelaGaman/FreCDo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRIP: Triangular Document-level Pre-training for Multilingual Language Models. (arXiv:2212.07752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07752">
<div class="article-summary-box-inner">
<span><p>Despite the current success of multilingual pre-training, most prior works
focus on leveraging monolingual data or bilingual parallel data and overlooked
the value of trilingual parallel data. This paper presents \textbf{Tri}angular
Document-level \textbf{P}re-training (\textbf{TRIP}), which is the first in the
field to extend the conventional monolingual and bilingual pre-training to a
trilingual setting by (i) \textbf{Grafting} the same documents in two languages
into one mixed document, and (ii) predicting the remaining one language as the
reference translation. Our experiments on document-level MT and cross-lingual
abstractive summarization show that TRIP brings by up to 3.65 d-BLEU points and
6.2 ROUGE-L points on three multilingual document-level machine translation
benchmarks and one cross-lingual abstractive summarization benchmark, including
multiple strong state-of-the-art (SOTA) scores. In-depth analysis indicates
that TRIP improves document-level machine translation and captures better
document contexts in at least three characteristics: (i) tense consistency,
(ii) noun consistency and (iii) conjunction presence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COLA: Improving Conversational Recommender Systems by Collaborative Augmentation. (arXiv:2212.07767v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07767">
<div class="article-summary-box-inner">
<span><p>Conversational recommender systems (CRS) aim to employ natural language
conversations to suggest suitable products to users. Understanding user
preferences for prospective items and learning efficient item representations
are crucial for CRS. Despite various attempts, earlier studies mostly learned
item representations based on individual conversations, ignoring item
popularity embodied among all others. Besides, they still need support in
efficiently capturing user preferences since the information reflected in a
single conversation is limited. Inspired by collaborative filtering, we propose
a collaborative augmentation (COLA) method to simultaneously improve both item
representation learning and user preference modeling to address these issues.
We construct an interactive user-item graph from all conversations, which
augments item representations with user-aware information, i.e., item
popularity. To improve user preference modeling, we retrieve similar
conversations from the training corpus, where the involved items and attributes
that reflect the user's potential interests are used to augment the user
representation through gate control. Extensive experiments on two benchmark
datasets demonstrate the effectiveness of our method. Our code and data are
available at https://github.com/DongdingLin/COLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLAM: Selective Clarification for Ambiguous Questions with Large Language Models. (arXiv:2212.07769v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07769">
<div class="article-summary-box-inner">
<span><p>State-of-the-art language models are often accurate on many
question-answering benchmarks with well-defined questions. Yet, in real
settings questions are often unanswerable without asking the user for
clarifying information. We show that current SotA models often do not ask the
user for clarification when presented with imprecise questions and instead
provide incorrect answers or "hallucinate". To address this, we introduce CLAM,
a framework that first uses the model to detect ambiguous questions, and if an
ambiguous question is detected, prompts the model to ask the user for
clarification. Furthermore, we show how to construct a scalable and
cost-effective automatic evaluation protocol using an oracle language model
with privileged information to provide clarifying information. We show that our
method achieves a 20.15 percentage point accuracy improvement over SotA on a
novel ambiguous question-answering answering data set derived from TriviaQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07796">
<div class="article-summary-box-inner">
<span><p>A fundamental characteristic common to both human vision and natural language
is their compositional nature. Yet, despite the performance gains contributed
by large vision and language pretraining, we find that - across 6 architectures
trained with 4 algorithms on massive datasets - they exhibit little
compositionality. To arrive at this conclusion, we introduce a new
compositionality evaluation benchmark CREPE which measures two important
aspects of compositionality identified by cognitive science literature:
systematicity and productivity. To measure systematicity, CREPE consists of
three test datasets. The three test sets are designed to test models trained on
three of the popular training datasets: CC-12M, YFCC-15M, and LAION-400M. They
contain 385K, 385K, and 373K image-text pairs and 237K, 210K, and 178K hard
negative captions. To test productivity, CREPE contains 17K image-text pairs
with nine different complexities plus 246K hard negative captions with atomic,
swapping, and negation foils. The datasets are generated by repurposing the
Visual Genome scene graphs and region descriptions and applying handcrafted
templates and GPT-3. For systematicity, we find that model performance
decreases consistently when novel compositions dominate the retrieval set, with
Recall@1 dropping by up to 8%. For productivity, models' retrieval success
decays as complexity increases, frequently nearing random chance at high
complexity. These results hold regardless of model and training dataset size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Background Knowledge for Robust Reasoning over Traffic Situations. (arXiv:2212.07798v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07798">
<div class="article-summary-box-inner">
<span><p>Understanding novel situations in the traffic domain requires an intricate
combination of domain-specific and causal commonsense knowledge. Prior work has
provided sufficient perception-based modalities for traffic monitoring, in this
paper, we focus on a complementary research aspect of Intelligent
Transportation: traffic understanding. We scope our study to text-based methods
and datasets given the abundant commonsense knowledge that can be extracted
using language models from large corpus and knowledge graphs. We adopt three
knowledge-driven approaches for zero-shot QA over traffic situations, based on
prior natural language inference methods, commonsense models with knowledge
graph self-supervision, and dense retriever-based models. We constructed two
text-based multiple-choice question answering sets: BDD-QA for evaluating
causal reasoning in the traffic domain and HDT-QA for measuring the possession
of domain knowledge akin to human driving license tests. Among the methods,
Unified-QA reaches the best performance on the BDD-QA dataset with the
adaptation of multiple formats of question answers. Language models trained
with inference information and commonsense knowledge are also good at
predicting the cause and effect in the traffic domain but perform badly at
answering human-driving QA sets. For such sets, DPR+Unified-QA performs the
best due to its efficient knowledge extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeTIm-Eval: a novel curated evaluation data set for comparing text-to-image models. (arXiv:2212.07839v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07839">
<div class="article-summary-box-inner">
<span><p>Evaluating and comparing text-to-image models is a challenging problem.
Significant advances in the field have recently been made, piquing interest of
various industrial sectors. As a consequence, a gold standard in the field
should cover a variety of tasks and application contexts. In this paper a novel
evaluation approach is experimented, on the basis of: (i) a curated data set,
made by high-quality royalty-free image-text pairs, divided into ten
categories; (ii) a quantitative metric, the CLIP-score, (iii) a human
evaluation task to distinguish, for a given text, the real and the generated
images. The proposed method has been applied to the most recent models, i.e.,
DALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early
experimental results show that the accuracy of the human judgement is fully
coherent with the CLIP-score. The dataset has been made available to the
public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers. (arXiv:2212.07841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07841">
<div class="article-summary-box-inner">
<span><p>Dense retrieval aims to map queries and passages into low-dimensional vector
space for efficient similarity measuring, showing promising effectiveness in
various large-scale retrieval tasks. Since most existing methods commonly adopt
pre-trained Transformers (e.g. BERT) for parameter initialization, some work
focuses on proposing new pre-training tasks for compressing the useful semantic
information from passages into dense vectors, achieving remarkable
performances. However, it is still challenging to effectively capture the rich
semantic information and relations about passages into the dense vectors via
one single particular pre-training task. In this work, we propose a multi-task
pre-trained model, MASTER, that unifies and integrates multiple pre-training
tasks with different learning objectives under the bottlenecked masked
autoencoder architecture. Concretely, MASTER utilizes a multi-decoder
architecture to integrate three types of pre-training tasks: corrupted passages
recovering, related passage recovering and PLMs outputs recovering. By
incorporating a shared deep encoder, we construct a representation bottleneck
in our architecture, compressing the abundant semantic information across tasks
into dense vectors. The first two types of tasks concentrate on capturing the
semantic information of passages and relationships among them within the
pre-training corpus. The third one can capture the knowledge beyond the corpus
from external PLMs (e.g. GPT-2). Extensive experiments on several large-scale
passage retrieval datasets have shown that our approach outperforms the
previous state-of-the-art dense retrieval methods. Our code and data are
publicly released in https://github.com/microsoft/SimXNS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention as a guide for Simultaneous Speech Translation. (arXiv:2212.07850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07850">
<div class="article-summary-box-inner">
<span><p>The study of the attention mechanism has sparked interest in many fields,
such as language modeling and machine translation. Although its patterns have
been exploited to perform different tasks, from neural network understanding to
textual alignment, no previous work has analysed the encoder-decoder attention
behavior in speech translation (ST) nor used it to improve ST on a specific
task. In this paper, we fill this gap by proposing an attention-based policy
(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the
existing attention relations between audio input and textual output. Its goal
is to leverage the encoder-decoder attention scores to guide inference in real
time. Results on en-&gt;{de, es} show that the EDAtt policy achieves overall
better results compared to the SimulST state of the art, especially in terms of
computational-aware latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The effects of gender bias in word embeddings on depression prediction. (arXiv:2212.07852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07852">
<div class="article-summary-box-inner">
<span><p>Word embeddings are extensively used in various NLP problems as a
state-of-the-art semantic feature vector representation. Despite their success
on various tasks and domains, they might exhibit an undesired bias for
stereotypical categories due to statistical and societal biases that exist in
the dataset they are trained on. In this study, we analyze the gender bias in
four different pre-trained word embeddings specifically for the depression
category in the mental disorder domain. We use contextual and non-contextual
embeddings that are trained on domain-independent as well as clinical
domain-specific data. We observe that embeddings carry bias for depression
towards different gender groups depending on the type of embeddings. Moreover,
we demonstrate that these undesired correlations are transferred to the
downstream task for depression phenotype recognition. We find that data
augmentation by simply swapping gender words mitigates the bias significantly
in the downstream task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effects of In-domain Corpus Size on pre-training BERT. (arXiv:2212.07914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07914">
<div class="article-summary-box-inner">
<span><p>Many prior language modeling efforts have shown that pre-training on an
in-domain corpus can significantly improve performance on downstream
domain-specific NLP tasks. However, the difficulties associated with collecting
enough in-domain data might discourage researchers from approaching this
pre-training task. In this paper, we conducted a series of experiments by
pre-training Bidirectional Encoder Representations from Transformers (BERT)
with different sizes of biomedical corpora. The results demonstrate that
pre-training on a relatively small amount of in-domain data (4GB) with limited
training steps, can lead to better performance on downstream domain-specific
NLP tasks compared with fine-tuning models pre-trained on general corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07919">
<div class="article-summary-box-inner">
<span><p>Large language models show improved downstream task performance when prompted
to generate step-by-step reasoning to justify their final answers. These
reasoning steps greatly improve model interpretability and verification, but
objectively studying their correctness (independent of the final answer) is
difficult without reliable methods for automatic evaluation. We simply do not
know how often the stated reasoning steps actually support the final end task
predictions. In this work, we present ROSCOE, a suite of interpretable,
unsupervised automatic scores that improve and extend previous text generation
evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a
typology of reasoning errors and collect synthetic and human evaluation scores
on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE
can measure semantic consistency, logicality, informativeness, fluency, and
factuality - among other traits - by leveraging properties of step-by-step
rationales. We empirically verify the strength of our metrics on five human
annotated and six programmatically perturbed diagnostics datasets - covering a
diverse set of tasks that require reasoning skills and show that ROSCOE can
consistently outperform baseline metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Learning for Joint Intent and Slot Labeling. (arXiv:2212.07922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07922">
<div class="article-summary-box-inner">
<span><p>It is expensive and difficult to obtain the large number of sentence-level
intent and token-level slot label annotations required to train neural network
(NN)-based Natural Language Understanding (NLU) components of task-oriented
dialog systems, especially for the many real world tasks that have a large and
growing number of intents and slot types. While zero shot learning approaches
that require no labeled examples -- only features and auxiliary information --
have been proposed only for slot labeling, we show that one can profitably
perform joint zero-shot intent classification and slot labeling. We demonstrate
the value of capturing dependencies between intents and slots, and between
different slots in an utterance in the zero shot setting. We describe NN
architectures that translate between word and sentence embedding spaces, and
demonstrate that these modifications are required to enable zero shot learning
for this task. We show a substantial improvement over strong baselines and
explain the intuition behind each architectural modification through
visualizations and ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Natural Language Processing to Predict Costume Core Vocabulary of Historical Artifacts. (arXiv:2212.07931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07931">
<div class="article-summary-box-inner">
<span><p>Historic dress artifacts are a valuable source for human studies. In
particular, they can provide important insights into the social aspects of
their corresponding era. These insights are commonly drawn from garment
pictures as well as the accompanying descriptions and are usually stored in a
standardized and controlled vocabulary that accurately describes garments and
costume items, called the Costume Core Vocabulary. Building an accurate Costume
Core from garment descriptions can be challenging because the historic garment
items are often donated, and the accompanying descriptions can be based on
untrained individuals and use a language common to the period of the items. In
this paper, we present an approach to use Natural Language Processing (NLP) to
map the free-form text descriptions of the historic items to that of the
controlled vocabulary provided by the Costume Core. Despite the limited
dataset, we were able to train an NLP model based on the Universal Sentence
Encoder to perform this mapping with more than 90% test accuracy for a subset
of the Costume Core vocabulary. We describe our methodology, design choices,
and development of our approach, and show the feasibility of predicting the
Costume Core for unseen descriptions. With more garment descriptions still
being curated to be used for training, we expect to have higher accuracy for
better generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually-augmented pretrained language models for NLP tasks without images. (arXiv:2212.07937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07937">
<div class="article-summary-box-inner">
<span><p>Although pre-trained language models (PLMs) have shown impressive performance
by text-only self-supervised training, they are found lack of visual semantics
or commonsense, e.g., sizes, shapes, and colors of commonplace objects.
Existing solutions often rely on explicit images for visual knowledge
augmentation (requiring time-consuming retrieval or generation), and they also
conduct the augmentation for the whole input text, without considering whether
it is actually needed in specific inputs or tasks. To address these issues, we
propose a novel visually-augmented fine-tuning approach that can be generally
applied to various PLMs or NLP tasks, without using any retrieved or generated
images, namely VAWI. Specifically, we first identify the visually-hungry words
(VH-words) from input text via a token selector, where three different methods
have been proposed, including syntax-, attention- and learning-based
strategies. Then, we adopt a fixed CLIP text encoder to generate the
visually-augmented representations of these VH-words. As it has been
pre-trained by vision-language alignment task on the large-scale corpus, it is
capable of injecting visual semantics into the aligned text representations.
Finally, the visually-augmented features will be fused and transformed into the
pre-designed visual prompts based on VH-words, which can be inserted into PLMs
to enrich the visual semantics in word representations. We conduct extensive
experiments on ten NLP tasks, i.e., GLUE benchmark, CommonsenseQA, CommonGen,
and SNLI-VE. Experimental results show that our approach can consistently
improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and
outperform several competitive baselines significantly. Our codes and data are
publicly available at~\url{https://github.com/RUCAIBox/VAWI}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RWEN-TTS: Relation-aware Word Encoding Network for Natural Text-to-Speech Synthesis. (arXiv:2212.07939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07939">
<div class="article-summary-box-inner">
<span><p>With the advent of deep learning, a huge number of text-to-speech (TTS)
models which produce human-like speech have emerged. Recently, by introducing
syntactic and semantic information w.r.t the input text, various approaches
have been proposed to enrich the naturalness and expressiveness of TTS models.
Although these strategies showed impressive results, they still have some
limitations in utilizing language information. First, most approaches only use
graph networks to utilize syntactic and semantic information without
considering linguistic features. Second, most previous works do not explicitly
consider adjacent words when encoding syntactic and semantic information, even
though it is obvious that adjacent words are usually meaningful when encoding
the current word. To address these issues, we propose Relation-aware Word
Encoding Network (RWEN), which effectively allows syntactic and semantic
information based on two modules (i.e., Semantic-level Relation Encoding and
Adjacent Word Relation Encoding). Experimental results show substantial
improvements compared to previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. (arXiv:2212.07981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07981">
<div class="article-summary-box-inner">
<span><p>Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation protocols and benchmarks for summarization either exhibit low
inter-annotator agreement or lack the scale needed to draw statistically
significant conclusions, and an in-depth analysis of human evaluation is
lacking. In this work, we address the shortcomings of existing summarization
evaluation along the following axes: 1) We propose a modified summarization
salience protocol, Atomic Content Units (ACUs), which relies on fine-grained
semantic units and allows for high inter-annotator agreement. 2) We curate the
Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation
dataset consisting of over 22k summary-level annotations over state-of-the-art
systems on three datasets. 3) We compare our ACU protocol with three other
human evaluation protocols, underscoring potential confounding factors in
evaluation setups. 4) We evaluate existing automatic metrics using the
collected human annotations across evaluation protocols and demonstrate how our
benchmark leads to more statistically stable and significant results.
Furthermore, our findings have important implications for evaluating large
language models (LLMs), as we show that LLMs adjusted by human feedback (e.g.,
GPT-3.5) may overfit unconstrained human evaluation, which is affected by the
annotators' prior, input-agnostic preferences, calling for more robust,
targeted evaluation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformers are Parameter-Efficient Audio-Visual Learners. (arXiv:2212.07983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07983">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have achieved impressive results on various
computer vision tasks in the last several years. In this work, we study the
capability of frozen ViTs, pretrained only on visual data, to generalize to
audio-visual data without finetuning any of its original parameters. To do so,
we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained
ViTs to audio-visual tasks by injecting a small number of trainable parameters
into every layer of a frozen ViT. To efficiently fuse visual and audio cues,
our LAVISH adapter uses a small set of latent tokens, which form an attention
bottleneck, thus, eliminating the quadratic cost of standard cross-attention.
Compared to the existing modality-specific audio-visual methods, our approach
achieves competitive or even better performance on various audio-visual tasks
while using fewer tunable parameters and without relying on costly audio
pretraining or external audio encoders. Our code is available at
https://genjib.github.io/project_page/LAVISH/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08011">
<div class="article-summary-box-inner">
<span><p>Dialect differences caused by regional, social, and economic barriers cause
performance discrepancies for many groups of users of language technology.
Fair, inclusive, and equitable language technology must critically be dialect
invariant, meaning that performance remains constant over dialectal shifts.
Current English systems often fall significantly short of this ideal since they
are designed and tested on a single dialect: Standard American English. We
introduce Multi-VALUE -- a suite of resources for evaluating and achieving
English dialect invariance. We build a controllable rule-based translation
system spanning 50 English dialects and a total of 189 unique linguistic
features. Our translation maps Standard American English text to synthetic form
of each dialect, which uses an upper-bound on the natural density of features
in that dialect. First, we use this system to build stress tests for question
answering, machine translation, and semantic parsing tasks. Stress tests reveal
significant performance disparities for leading models on non-standard
dialects. Second, we use this system as a data augmentation technique to
improve the dialect robustness of existing systems. Finally, we partner with
native speakers of Chicano and Indian English to release new gold-standard
variants of the popular CoQA task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models. (arXiv:2212.08037v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08037">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown impressive results across a variety
of tasks while requiring little or no direct supervision. Further, there is
mounting evidence that LLMs may have potential in information-seeking
scenarios. We believe the ability of an LLM to attribute the text that it
generates is likely to be crucial for both system developers and users in this
setting. We propose and study Attributed QA as a key first step in the
development of attributed LLMs. We develop a reproducable evaluation framework
for the task, using human annotations as a gold standard and a correlated
automatic metric that we show is suitable for development settings. We describe
and benchmark a broad set of architectures for the task. Our contributions give
some concrete answers to two key questions (How to measure attribution?, and
How well do current state-of-the-art methods perform on attribution?), and give
some hints as to how to address a third key question (How to build LLMs with
attribution?).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can REF output quality scores be assigned by AI? Experimental evidence. (arXiv:2212.08041v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08041">
<div class="article-summary-box-inner">
<span><p>This document describes strategies for using Artificial Intelligence (AI) to
predict some journal article scores in future research assessment exercises.
Five strategies have been assessed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue. (arXiv:2212.08054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08054">
<div class="article-summary-box-inner">
<span><p>Modern virtual assistants use internal semantic parsing engines to convert
user utterances to actionable commands. However, prior work has demonstrated
that semantic parsing is a difficult multilingual transfer task with low
transfer efficiency compared to other tasks. In global markets such as India
and Latin America, this is a critical issue as switching between languages is
prevalent for bilingual users. In this work we dramatically improve the
zero-shot performance of a multilingual and codeswitched semantic parsing
system using two stages of multilingual alignment. First, we show that
constrastive alignment pretraining improves both English performance and
transfer efficiency. We then introduce a constrained optimization approach for
hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned
Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and
81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing
benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. (arXiv:2212.08055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08055">
<div class="article-summary-box-inner">
<span><p>Direct speech-to-speech translation (S2ST), in which all components can be
optimized jointly, is advantageous over cascaded approaches to achieve fast
inference with a simplified pipeline. We present a novel two-pass direct S2ST
architecture, {\textit UnitY}, which first generates textual representations
and predicts discrete acoustic units subsequently. We enhance the model
performance by subword prediction in the first-pass decoder, advanced two-pass
decoder architecture design and search strategy, and better training
regularization. To leverage large amounts of unlabeled text data, we pre-train
the first-pass text decoder based on the self-supervised denoising
auto-encoding task. Experimental evaluations on benchmark datasets at various
data scales demonstrate that UnitY outperforms a single-pass speech-to-unit
translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show
that the proposed methods boost the performance even when predicting
spectrogram in the second pass. However, predicting discrete units achieves
2.51x decoding speed-up compared to that case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. (arXiv:2212.08061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08061">
<div class="article-summary-box-inner">
<span><p>Generating a chain of thought (CoT) can increase large language model (LLM)
performance on a wide range of tasks. Zero-shot CoT evaluations, however, have
been conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In
this paper, we perform a controlled evaluation of zero-shot CoT across two
sensitive domains: harmful questions and stereotype benchmarks. We find that
using zero-shot CoT reasoning in a prompt can significantly increase a model's
likelihood to produce undesirable output. Without future advances in alignment
or explicit mitigation instructions, zero-shot CoT should be avoided on tasks
where models can make inferences about marginalized groups or harmful topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Definition and a Test for Human-Level Artificial Intelligence. (arXiv:2011.09410v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09410">
<div class="article-summary-box-inner">
<span><p>Despite recent advances of AI research in many application-specific domains,
we do not know how to build a human-level artificial intelligence (HLAI). We
conjecture that learning from others' experience with the language is the
essential characteristic that distinguishes human intelligence from the rest.
Humans can update the action-value function with the verbal description as if
they experience states, actions, and corresponding rewards sequences firsthand.
In this paper, we present a classification of intelligence according to how
individual agents learn and propose a definition and a test for HLAI. The main
idea is that language acquisition without explicit rewards can be a sufficient
test for HLAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Ideological Salience and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity. (arXiv:2104.08829v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08829">
<div class="article-summary-box-inner">
<span><p>The increasing polarization of online political discourse calls for
computational tools that automatically detect and monitor ideological divides
in social media. We introduce a minimally supervised method that leverages the
network structure of online discussion forums, specifically Reddit, to detect
polarized concepts. We model polarization along the dimensions of salience and
framing, drawing upon insights from moral psychology. Our architecture combines
graph neural networks with structured sparsity learning and results in
representations for concepts and subreddits that capture temporal ideological
dynamics such as right-wing and left-wing radicalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03276">
<div class="article-summary-box-inner">
<span><p>Currently, pre-trained models can be considered the default choice for a wide
range of NLP tasks. Despite their SoTA results, there is practical evidence
that these models may require a different number of computing layers for
different input sequences, since evaluating all layers leads to overconfidence
in wrong predictions (namely overthinking). This problem can potentially be
solved by implementing adaptive computation time approaches, which were first
designed to improve inference speed. Recently proposed PonderNet may be a
promising solution for performing an early exit by treating the exit layer's
index as a latent variable. However, the originally proposed exit criterion,
relying on sampling from trained posterior distribution on the probability of
exiting from the $i$-th layer, introduces major variance in exit layer indices,
significantly reducing the resulting model's performance. In this paper, we
propose improving PonderNet with a novel deterministic Q-exit criterion and a
revisited model architecture. We adapted the proposed mechanism to ALBERT and
RoBERTa and compared it with recent methods for performing an early exit. We
observed that the proposed changes can be considered significant improvements
on the original PonderNet architecture and outperform PABEE on a wide range of
GLUE tasks. In addition, we also performed an in-depth ablation study of the
proposed architecture to further understand Lambda layers and their
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Tokenization Learning. (arXiv:2205.11443v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11443">
<div class="article-summary-box-inner">
<span><p>In the presented study, we discover that the so-called "transition freedom"
metric appears superior for unsupervised tokenization purposes in comparison to
statistical metrics such as mutual information and conditional probability,
providing F-measure scores in range from 0.71 to 1.0 across explored
multilingual corpora. We find that different languages require different
offshoots of that metric (such as derivative, variance, and "peak values") for
successful tokenization. Larger training corpora do not necessarily result in
better tokenization quality, while compressing the models by eliminating
statistically weak evidence tends to improve performance. The proposed
unsupervised tokenization technique provides quality better than or comparable
to lexicon-based ones, depending on the language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Label Errors by using Pre-Trained Language Models. (arXiv:2205.12702v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12702">
<div class="article-summary-box-inner">
<span><p>We show that large pre-trained language models are inherently highly capable
of identifying label errors in natural language datasets: simply examining
out-of-sample data points in descending order of fine-tuned task loss
significantly outperforms more complex error-detection mechanisms proposed in
previous work.
</p>
<p>To this end, we contribute a novel method for introducing realistic,
human-originated label noise into existing crowdsourced datasets such as SNLI
and TweetNLP. We show that this noise has similar properties to real,
hand-verified label errors, and is harder to detect than existing synthetic
noise, creating challenges for model robustness. We argue that human-originated
noise is a better standard for evaluation than synthetic noise.
</p>
<p>Finally, we use crowdsourced verification to evaluate the detection of real
errors on IMDB, Amazon Reviews, and Recon, and confirm that pre-trained models
perform at a 9-36% higher absolute Area Under the Precision-Recall Curve than
existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Scientific Creativity with Retrieval across Knowledge Domains. (arXiv:2206.01328v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01328">
<div class="article-summary-box-inner">
<span><p>Exposure to ideas in domains outside a scientist's own may benefit her in
reformulating existing research problems in novel ways and discovering new
application domains for existing solution ideas. While improved performance in
scholarly search engines can help scientists efficiently identify relevant
advances in domains they may already be familiar with, it may fall short of
helping them explore diverse ideas \textit{outside} such domains. In this paper
we explore the design of systems aimed at augmenting the end-user ability in
cross-domain exploration with flexible query specification. To this end, we
develop an exploratory search system in which end-users can select a portion of
text core to their interest from a paper abstract and retrieve papers that have
a high similarity to the user-selected core aspect but differ in terms of
domains. Furthermore, end-users can `zoom in' to specific domain clusters to
retrieve more papers from them and understand nuanced differences within the
clusters. Our case studies with scientists uncover opportunities and design
implications for systems aimed at facilitating cross-domain exploration and
inspiration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages. (arXiv:2208.11761v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.11761">
<div class="article-summary-box-inner">
<span><p>A cornerstone in AI research has been the creation and adoption of
standardized training and test datasets to earmark the progress of
state-of-the-art models. A particularly successful example is the GLUE dataset
for training and evaluating Natural Language Understanding (NLU) models for
English. The large body of research around self-supervised BERT-based language
models revolved around performance improvements on NLU tasks in GLUE. To
evaluate language models in other languages, several language-specific GLUE
datasets were created. The area of speech language understanding (SLU) has
followed a similar trajectory. The success of large self-supervised models such
as wav2vec2 enable creation of speech models with relatively easy to access
unlabelled data. These models can then be evaluated on SLU tasks, such as the
SUPERB benchmark. In this work, we extend this to Indic languages by releasing
the IndicSUPERB benchmark. Specifically, we make the following three
contributions. (i) We collect Kathbath containing 1,684 hours of labelled
speech data across 12 Indian languages from 1,218 contributors located in 203
districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech
tasks: Automatic Speech Recognition, Speaker Verification, Speaker
Identification (mono/multi), Language Identification, Query By Example, and
Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train
and evaluate different self-supervised models alongside a commonly used
baseline FBANK. We show that language-specific fine-tuned models are more
accurate than baseline on most of the tasks, including a large gap of 76\% for
the Language Identification task. However, for speaker identification,
self-supervised models trained on large datasets demonstrate an advantage. We
hope IndicSUPERB contributes to the progress of developing speech language
understanding models for Indian languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations. (arXiv:2209.07562v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.07562">
<div class="article-summary-box-inner">
<span><p>We present TwHIN-BERT, a multilingual language model trained on in-domain
data from the popular social network Twitter. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on a variety of multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We will freely open-source
TwHIN-BERT and our curated hashtag prediction and social engagement benchmark
datasets to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalized and Explainable Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.06282">
<div class="article-summary-box-inner">
<span><p>Long-range context modeling is crucial to both dialogue understanding and
generation. The most popular method for dialogue context representation is to
concatenate the last-$k$ previous utterances. However, this method may not be
ideal for conversations containing long-range dependencies. In this work, we
propose DialoGX, a novel encoder-decoder based framework for conversational
response generation with a generalized and explainable context representation
that can look beyond the last-$k$ utterances. Hence the method is adaptive to
conversations with long-range dependencies. The main idea of our approach is to
identify and utilize the most relevant historical utterances instead of the
last-$k$ utterances in chronological order. We study the effectiveness of our
proposed method on both dialogue generation (open-domain) and understanding
(DST) tasks. DialoGX achieves comparable performance with the state-of-the-art
models on DailyDialog dataset. We also observe performance gain in existing DST
models with our proposed context representation strategy on MultiWOZ dataset.
We justify our context representation through the lens of psycholinguistics and
show that the relevance score of previous utterances agrees well with human
cognition which makes DialoGX explainable as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning. (arXiv:2210.07792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.07792">
<div class="article-summary-box-inner">
<span><p>Controlled automated story generation seeks to generate natural language
stories satisfying constraints from natural language critiques or preferences.
Existing methods to control for story preference utilize prompt engineering
which is labor intensive and often inconsistent. They may also use
logit-manipulation methods which require annotated datasets to exist for the
desired attributes. To address these issues, we first train a contrastive
bi-encoder model to align stories with corresponding human critiques, named
CARP, building a general purpose preference model. This is subsequently used as
a reward function to fine-tune a generative language model via reinforcement
learning. However, simply fine-tuning a generative language model with a
contrastive reward model does not always reliably result in a story generation
system capable of generating stories that meet user preferences. To increase
story generation robustness we further fine-tune the contrastive reward model
using a prompt-learning technique. A human participant study is then conducted
comparing generations from our full system, ablations, and two baselines. We
show that the full fine-tuning pipeline results in a story generator preferred
over a LLM 20x as large as well as logit-based methods. This motivates the use
of contrastive learning for general purpose human preference modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous diffusion for categorical data. (arXiv:2211.15089v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15089">
<div class="article-summary-box-inner">
<span><p>Diffusion models have quickly become the go-to paradigm for generative
modelling of perceptual signals (such as images and sound) through iterative
refinement. Their success hinges on the fact that the underlying physical
phenomena are continuous. For inherently discrete and categorical data such as
language, various diffusion-inspired alternatives have been proposed. However,
the continuous nature of diffusion models conveys many benefits, and in this
work we endeavour to preserve it. We propose CDCD, a framework for modelling
categorical data with diffusion models that are continuous both in time and
input space. We demonstrate its efficacy on several language modelling tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification. (arXiv:2212.05506v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05506">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised text classification aims to train a classifier using only
class descriptions and unlabeled data. Recent research shows that
keyword-driven methods can achieve state-of-the-art performance on various
tasks. However, these methods not only rely on carefully-crafted class
descriptions to obtain class-specific keywords but also require substantial
amount of unlabeled data and takes a long time to train. This paper proposes
FastClass, an efficient weakly-supervised classification approach. It uses
dense text representation to retrieve class-relevant documents from external
unlabeled corpus and selects an optimal subset to train a classifier. Compared
to keyword-driven methods, our approach is less reliant on initial class
descriptions as it no longer needs to expand each class description into a set
of class-specific keywords. Experiments on a wide range of classification tasks
show that the proposed approach frequently outperforms keyword-driven models in
terms of classification accuracy and often enjoys orders-of-magnitude faster
training speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing a Recurrent Neural Network into Modules for Enabling Reusability and Replacement. (arXiv:2212.05970v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.05970">
<div class="article-summary-box-inner">
<span><p>Can we take a recurrent neural network (RNN) trained to translate between
languages and augment it to support a new natural language without retraining
the model from scratch? Can we fix the faulty behavior of the RNN by replacing
portions associated with the faulty behavior? Recent works on decomposing a
fully connected neural network (FCNN) and convolutional neural network (CNN)
into modules have shown the value of engineering deep models in this manner,
which is standard in traditional SE but foreign for deep learning models.
However, prior works focus on the image-based multiclass classification
problems and cannot be applied to RNN due to (a) different layer structures,
(b) loop structures, (c) different types of input-output architectures, and (d)
usage of both nonlinear and logistic activation functions. In this work, we
propose the first approach to decompose an RNN into modules. We study different
types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN
modules can be reused and replaced in various scenarios. We evaluate our
approach against 5 canonical datasets (i.e., Math QA, Brown Corpus,
Wiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.
We found that decomposing a trained model has a small cost (Accuracy: -0.6%,
BLEU score: +0.10%). Also, the decomposed modules can be reused and replaced
without needing to retrain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards mapping the contemporary art world with ArtLM: an art-specific NLP model. (arXiv:2212.07127v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.07127">
<div class="article-summary-box-inner">
<span><p>With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-12-17 23:11:28.668797344 UTC">2022-12-17 23:11:28 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>