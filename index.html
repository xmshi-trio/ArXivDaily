<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-27T01:30:00Z">01-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering. (arXiv:2301.10799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10799">
<div class="article-summary-box-inner">
<span><p>Providing explanations for visual question answering (VQA) has gained much
attention in research. However, most existing systems use separate models for
predicting answers and providing explanations. We argue that training
explanation models independently of the QA model makes the explanations less
grounded and limits performance. To address this, we propose a multitask
learning approach towards a Unified Model for more grounded and consistent
generation of both Answers and Explanations (UMAE). To achieve this, we add
artificial prompt tokens to training instances and finetune a multimodal
encoder-decoder model on various VQA tasks. In our experiments, UMAE models
surpass the prior SOTA answer accuracy on A-OKVQA by 10~15%, show competitive
results on OK-VQA, achieve new SOTA explanation scores on A-OKVQA and VCR, and
demonstrate promising out-of-domain performance on VQA-X.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the inconsistency of separable losses for structured prediction. (arXiv:2301.10810v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10810">
<div class="article-summary-box-inner">
<span><p>In this paper, we prove that separable negative log-likelihood losses for
structured prediction are not necessarily Bayes consistent, or, in other words,
minimizing these losses may not result in a model that predicts the most
probable structure in the data distribution for a given input. This fact opens
the question of whether these losses are well-adapted for structured prediction
and, if so, why.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10856">
<div class="article-summary-box-inner">
<span><p>In response to disinformation and propaganda from Russian online media
following the Russian invasion of Ukraine, Russian outlets including Russia
Today and Sputnik News were banned throughout Europe. Many of these Russian
outlets, in order to reach their audiences, began to heavily promote their
content on messaging services like Telegram. In this work, to understand this
phenomenon, we study how 16 Russian media outlets have interacted with and
utilized 732 Telegram channels throughout 2022. To do this, we utilize a
multilingual version of the foundational model MPNet to embed articles and
Telegram messages in a shared embedding space and semantically compare content.
Leveraging a parallelized version of DP-Means clustering, we perform
paragraph-level topic/narrative extraction and time-series analysis with Hawkes
Processes. With this approach, across our websites, we find between 2.3%
(ura.news) and 26.7% (ukraina.ru) of their content originated/resulted from
activity on Telegram. Finally, tracking the spread of individual narratives, we
measure the rate at which these websites and channels disseminate content
within the Russian media ecosystem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10871">
<div class="article-summary-box-inner">
<span><p>Our work advances an approach for predicting hate speech in social media,
drawing out the critical need to consider the discussions that follow a post to
successfully detect when hateful discourse may arise. Using graph transformer
networks, coupled with modelling attention and BERT-level natural language
processing, our approach can capture context and anticipate upcoming
anti-social behaviour. In this paper, we offer a detailed qualitative analysis
of this solution for hate speech detection in social networks, leading to
insights into where the method has the most impressive outcomes in comparison
with competitors and identifying scenarios where there are challenges to
achieving ideal performance. Included is an exploration of the kinds of posts
that permeate social media today, including the use of hateful images. This
suggests avenues for extending our model to be more comprehensive. A key
insight is that the focus on reasoning about the concept of context positions
us well to be able to support multi-modal analysis of online posts. We conclude
with a reflection on how the problem we are addressing relates especially well
to the theme of dynamic change, a critical concern for all AI solutions for
social impact. We also comment briefly on how mental health well-being can be
advanced with our work, through curated content attuned to the extent of hate
in posts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Break It Down: Evidence for Structural Compositionality in Neural Networks. (arXiv:2301.10884v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10884">
<div class="article-summary-box-inner">
<span><p>Many tasks can be described as compositions over subroutines. Though modern
neural networks have achieved impressive performance on both vision and
language tasks, we know little about the functions that they implement. One
possibility is that neural networks implicitly break down complex tasks into
subroutines, implement modular solutions to these subroutines, and compose them
into an overall solution to a task -- a property we term structural
compositionality. Or they may simply learn to match new inputs to memorized
representations, eliding task decomposition entirely. Here, we leverage model
pruning techniques to investigate this question in both vision and language,
across a variety of architectures, tasks, and pretraining regimens. Our results
demonstrate that models oftentimes implement solutions to subroutines via
modular subnetworks, which can be ablated while maintaining the functionality
of other subroutines. This suggests that neural networks may be able to learn
to exhibit compositionality, obviating the need for specialized symbolic
mechanisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Text-based Early Prediction by Distillation from Privileged Time-Series Text. (arXiv:2301.10887v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10887">
<div class="article-summary-box-inner">
<span><p>Modeling text-based time-series to make prediction about a future event or
outcome is an important task with a wide range of applications. The standard
approach is to train and test the model using the same input window, but this
approach neglects the data collected in longer input windows between the
prediction time and the final outcome, which are often available during
training. In this study, we propose to treat this neglected text as privileged
information available during training to enhance early prediction modeling
through knowledge distillation, presented as Learning using Privileged
tIme-sEries Text (LuPIET). We evaluate the method on clinical and social media
text, with four clinical prediction tasks based on clinical notes and two
mental health prediction tasks based on social media posts. Our results show
LuPIET is effective in enhancing text-based early predictions, though one may
need to consider choosing the appropriate text representation and windows for
privileged text to achieve optimal performance. Compared to two other methods
using transfer learning and mixed training, LuPIET offers more stable
improvements over the baseline, standard training. As far as we are concerned,
this is the first study to examine learning using privileged information for
time-series in the NLP context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Reasoning of Entities and Events in Procedural Texts. (arXiv:2301.10896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10896">
<div class="article-summary-box-inner">
<span><p>Entities and events have long been regarded as the crux of machine reasoning.
Specifically, procedural texts have received increasing attention due to the
dynamic nature of involved entities and events. Existing work has exclusively
focused on entity state tracking (e.g., the temperature of a pan) or
counterfactual event reasoning (e.g., how likely am I to burn myself by
touching the pan), while these two tasks are tightly intertwined. In this work,
we propose CREPE, the first benchmark on causal reasoning about event
plausibility based on entity states. We experiment with strong large language
models and show that most models including GPT3 perform close to chance of .30
F1, lagging far behind the human performance of .87 F1. Inspired by the finding
that structured representations such as programming languages benefits event
reasoning as a prompt to code language models such as Codex, we creatively
inject the causal relations between entities and events through intermediate
variables and boost the performance to .67 to .72 F1. Our proposed event
representation not only allows for knowledge injection, but also marks the
first successful attempt of chain-of-thought reasoning with code language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning. (arXiv:2301.10915v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10915">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) is an important step in dialogue management to
keep track of users' beliefs. Existing works fine-tune all language model (LM)
parameters to tackle the DST task, which requires significant data and
computing resources for training and hosting. The cost grows exponentially in
the real-world deployment where dozens of fine-tuned LM are used for different
domains and tasks. To reduce parameter size and better utilize cross-task
shared information, we propose to use soft prompt token embeddings to learn
task properties. Without tuning LM parameters, our method drastically reduces
the number of parameters needed to less than 0.5% of prior works while achieves
better low-resource DST performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Faces for Goal-Driven Dyadic Communication. (arXiv:2301.10939v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10939">
<div class="article-summary-box-inner">
<span><p>We introduce a video framework for modeling the association between verbal
and non-verbal communication during dyadic conversation. Given the input speech
of a speaker, our approach retrieves a video of a listener, who has facial
expressions that would be socially appropriate given the context. Our approach
further allows the listener to be conditioned on their own goals,
personalities, or backgrounds. Our approach models conversations through a
composition of large language models and vision-language models, creating
internal representations that are interpretable and controllable. To study
multimodal communication, we propose a new video dataset of unscripted
conversations covering diverse topics and demographics. Experiments and
visualizations show our approach is able to output listeners that are
significantly more socially appropriate than baselines. However, many
challenges remain, and we release our dataset publicly to spur further
progress. See our website for video results, data, and code:
https://realtalk.cs.columbia.edu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images. (arXiv:2301.10951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10951">
<div class="article-summary-box-inner">
<span><p>Deep learning models can be applied successfully in real-work problems;
however, training most of these models requires massive data. Recent methods
use language and vision, but unfortunately, they rely on datasets that are not
usually publicly available. Here we pave the way for further research in the
multimodal language-vision domain for radiology. In this paper, we train a
representation learning method that uses local and global representations of
the language and vision through an attention mechanism and based on the
publicly available Indiana University Radiology Report (IU-RR) dataset.
Furthermore, we use the learned representations to diagnose five lung
pathologies: atelectasis, cardiomegaly, edema, pleural effusion, and
consolidation. Finally, we use both supervised and zero-shot classifications to
extensively analyze the performance of the representation learning on the IU-RR
dataset. Average Area Under the Curve (AUC) is used to evaluate the accuracy of
the classifiers for classifying the five lung pathologies. The average AUC for
classifying the five lung pathologies on the IU-RR test set ranged from 0.85 to
0.87 using the different training datasets, namely CheXpert and CheXphoto.
These results compare favorably to other studies using UI-RR. Extensive
experiments confirm consistent results for classifying lung pathologies using
the multimodal global local representations of language and vision information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Dynamic Focused Topic Model. (arXiv:2301.10988v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10988">
<div class="article-summary-box-inner">
<span><p>Topic models and all their variants analyse text by learning meaningful
representations through word co-occurrences. As pointed out by Williamson et
al. (2010), such models implicitly assume that the probability of a topic to be
active and its proportion within each document are positively correlated. This
correlation can be strongly detrimental in the case of documents created over
time, simply because recent documents are likely better described by new and
hence rare topics. In this work we leverage recent advances in neural
variational inference and present an alternative neural approach to the dynamic
Focused Topic Model. Indeed, we develop a neural model for topic evolution
which exploits sequences of Bernoulli random variables in order to track the
appearances of topics, thereby decoupling their activities from their
proportions. We evaluate our model on three different datasets (the UN general
debates, the collection of NeurIPS papers, and the ACL Anthology dataset) and
show that it (i) outperforms state-of-the-art topic models in generalization
tasks and (ii) performs comparably to them on prediction tasks, while employing
roughly the same number of parameters, and converging about two times faster.
Source code to reproduce our experiments is available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11004">
<div class="article-summary-box-inner">
<span><p>Interactions among humans on social media often convey intentions behind
their actions, yielding a psychological language resource for Mental Health
Analysis (MHA) of online users. The success of Computational Intelligence
Techniques (CIT) for inferring mental illness from such social media resources
points to NLP as a lens for causal analysis and perception mining. However, we
argue that more consequential and explainable research is required for optimal
impact on clinical psychology practice and personalized mental healthcare. To
bridge this gap, we posit two significant dimensions: (1) Causal analysis to
illustrate a cause and effect relationship in the user generated text; (2)
Perception mining to infer psychological perspectives of social effects on
online users intentions. Within the scope of Natural Language Processing (NLP),
we further explore critical areas of inquiry associated with these two
dimensions, specifically through recent advancements in discourse analysis.
This position paper guides the community to explore solutions in this space and
advance the state of practice in developing conversational agents for inferring
mental health from social media. We advocate for a more explainable approach
toward modeling computational psychology problems through the lens of language
as we observe an increased number of research contributions in dataset and
problem formulation for causal relation extraction and perception enhancements
while inferring mental states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrase Acquisition from Image Captions. (arXiv:2301.11030v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11030">
<div class="article-summary-box-inner">
<span><p>We propose to use captions from the Web as a previously underutilized
resource for paraphrases (i.e., texts with the same "message") and to create
and analyze a corresponding dataset. When an image is reused on the Web, an
original caption is often assigned. We hypothesize that different captions for
the same image naturally form a set of mutual paraphrases. To demonstrate the
suitability of this idea, we analyze captions in the English Wikipedia, where
editors frequently relabel the same image for different articles. The paper
introduces the underlying mining technology and compares known paraphrase
corpora with respect to their syntactic and semantic paraphrase similarity to
our new resource. In this context, we introduce characteristic maps along the
two similarity dimensions to identify the style of paraphrases coming from
different sources. An annotation study demonstrates the high reliability of the
algorithmically determined characteristic maps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A benchmark for toxic comment classification on Civil Comments dataset. (arXiv:2301.11125v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11125">
<div class="article-summary-box-inner">
<span><p>Toxic comment detection on social media has proven to be essential for
content moderation. This paper compares a wide set of different models on a
highly skewed multi-label hate speech dataset. We consider inference time and
several metrics to measure performance and bias in our comparison. We show that
all BERTs have similar performance regardless of the size, optimizations or
language used to pre-train the models. RNNs are much faster at inference than
any of the BERT. BiLSTM remains a good compromise between performance and
inference time. RoBERTa with Focal Loss offers the best performance on biases
and AUROC. However, DistilBERT combines both good AUROC and a low inference
time. All models are affected by the bias of associating identities. BERT, RNN,
and XLNet are less sensitive than the CNN and Compact Convolutional
Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data. (arXiv:2301.11174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11174">
<div class="article-summary-box-inner">
<span><p>We present a novel data-efficient semi-supervised framework to improve the
generalization of image captioning models. Constructing a large-scale labeled
image captioning dataset is an expensive task in terms of labor, time, and
cost. In contrast to manually annotating all the training samples, separately
collecting uni-modal datasets is immensely easier, e.g., a large-scale image
dataset and a sentence dataset. We leverage such massive unpaired image and
caption data upon standard paired data by learning to associate them. To this
end, our proposed semi-supervised learning method assigns pseudo-labels to
unpaired samples in an adversarial learning fashion, where the joint
distribution of image and caption is learned. Our method trains a captioner to
learn from a paired data and to progressively associate unpaired data. This
approach shows noticeable performance improvement even in challenging scenarios
including out-of-task data (i.e., relational captioning, where the target task
is different from the unpaired data) and web-crawled data. We also show that
our proposed method is theoretically well-motivated and has a favorable global
optimal property. Our extensive and comprehensive empirical results both on (1)
image-based and (2) dense region-based captioning datasets followed by
comprehensive analysis on the scarcely-paired COCO dataset demonstrate the
consistent effectiveness of our semisupervised learning method with unpaired
data compared to competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?. (arXiv:2301.11219v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11219">
<div class="article-summary-box-inner">
<span><p>Memes can sway people's opinions over social media as they combine visual and
textual information in an easy-to-consume manner. Since memes instantly turn
viral, it becomes crucial to infer their intent and potentially associated
harmfulness to take timely measures as needed. A common problem associated with
meme comprehension lies in detecting the entities referenced and characterizing
the role of each of these entities. Here, we aim to understand whether the meme
glorifies, vilifies, or victimizes each entity it refers to. To this end, we
address the task of role identification of entities in harmful memes, i.e.,
detecting who is the 'hero', the 'villain', and the 'victim' in the meme, if
any. We utilize HVVMemes - a memes dataset on US Politics and Covid-19 memes,
released recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains
memes, entities referenced, and their associated roles: hero, villain, victim,
and other. We further design VECTOR (Visual-semantic role dEteCToR), a robust
multi-modal framework for the task, which integrates entity-based contextual
information in the multi-modal representation and compare it to several
standard unimodal (text-only or image-only) or multi-modal (image+text) models.
Our experimental results show that our proposed model achieves an improvement
of 4% over the best baseline and 1% over the best competing stand-alone
submission from the shared-task. Besides divulging an extensive experimental
setup with comparative analyses, we finally highlight the challenges
encountered in addressing the complex task of semantic role labeling within
memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Molecular Language Model as Multi-task Generator. (arXiv:2301.11259v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11259">
<div class="article-summary-box-inner">
<span><p>Molecule generation with desired properties has grown immensely in popularity
by disruptively changing the way scientists design molecular structures and
providing support for chemical and materials design. However, despite the
promising outcome, previous machine learning-based deep generative models
suffer from a reliance on complex, task-specific fine-tuning, limited
dimensional latent spaces, or the quality of expert rules. In this work, we
propose MolGen, a pre-trained molecular language model that effectively learns
and shares knowledge across multiple generation tasks and domains.
Specifically, we pre-train MolGen with the chemical language SELFIES on more
than 100 million unlabelled molecules. We further propose multi-task molecular
prefix tuning across several molecular generation tasks and different molecular
domains (synthetic &amp; natural products) with a self-feedback mechanism.
Extensive experiments show that MolGen can obtain superior performances on
well-known molecular generation benchmark datasets. The further analysis
illustrates that MolGen can accurately capture the distribution of molecules,
implicitly learn their structural characteristics, and efficiently explore the
chemical space with the guidance of multi-task molecular prefix tuning. Codes,
datasets, and the pre-trained model will be available in
https://github.com/zjunlp/MolGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BayesSpeech: A Bayesian Transformer Network for Automatic Speech Recognition. (arXiv:2301.11276v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11276">
<div class="article-summary-box-inner">
<span><p>Recent developments using End-to-End Deep Learning models have been shown to
have near or better performance than state of the art Recurrent Neural Networks
(RNNs) on Automatic Speech Recognition tasks. These models tend to be lighter
weight and require less training time than traditional RNN-based approaches.
However, these models take frequentist approach to weight training. In theory,
network weights are drawn from a latent, intractable probability distribution.
We introduce BayesSpeech for end-to-end Automatic Speech Recognition.
BayesSpeech is a Bayesian Transformer Network where these intractable
posteriors are learned through variational inference and the local
reparameterization trick without recurrence. We show how the introduction of
variance in the weights leads to faster training time and near state-of-the-art
performance on LibriSpeech-960.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Finetuning for Factual Knowledge Extraction from Language Models. (arXiv:2301.11293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11293">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) pretrained on large corpora of text from the web have
been observed to contain large amounts of various types of knowledge about the
world. This observation has led to a new and exciting paradigm in knowledge
graph construction where, instead of manual curation or text mining, one
extracts knowledge from the parameters of an LM. Recently, it has been shown
that finetuning LMs on a set of factual knowledge makes them produce better
answers to queries from a different set, thus making finetuned LMs a good
candidate for knowledge extraction and, consequently, knowledge graph
construction. In this paper, we analyze finetuned LMs for factual knowledge
extraction. We show that along with its previously known positive effects,
finetuning also leads to a (potentially harmful) phenomenon which we call
Frequency Shock, where at the test time the model over-predicts rare entities
that appear in the training set and under-predicts common entities that do not
appear in the training set enough times. We show that Frequency Shock leads to
a degradation in the predictions of the model and beyond a point, the harm from
Frequency Shock can even outweigh the positive effects of finetuning, making
finetuning harmful overall. We then consider two solutions to remedy the
identified negative effect: 1- model mixing and 2- mixture finetuning with the
LM's pre-training task. The two solutions combined lead to significant
improvements compared to vanilla finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. (arXiv:2301.11305v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11305">
<div class="article-summary-box-inner">
<span><p>The fluency and factual knowledge of large language models (LLMs) heightens
the need for corresponding systems to detect whether a piece of text is
machine-written. For example, students may use LLMs to complete written
assignments, leaving instructors unable to accurately assess student learning.
In this paper, we first demonstrate that text sampled from an LLM tends to
occupy negative curvature regions of the model's log probability function.
Leveraging this observation, we then define a new curvature-based criterion for
judging if a passage is generated from a given LLM. This approach, which we
call DetectGPT, does not require training a separate classifier, collecting a
dataset of real or generated passages, or explicitly watermarking generated
text. It uses only log probabilities computed by the model of interest and
random perturbations of the passage from another generic pre-trained language
model (e.g, T5). We find DetectGPT is more discriminative than existing
zero-shot methods for model sample detection, notably improving detection of
fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the
strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See
https://ericmitchell.ai/detectgpt for code, data, and other project
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification. (arXiv:2301.11309v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11309">
<div class="article-summary-box-inner">
<span><p>Extreme classification (XC) involves predicting over large numbers of classes
(thousands to millions), with real-world applications like news article
classification and e-commerce product tagging. The zero-shot version of this
task requires generalization to novel classes without additional supervision.
In this paper, we develop SemSup-XC, a model that achieves state-of-the-art
zero-shot and few-shot performance on three XC datasets derived from legal,
e-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically
collected semantic class descriptions to represent classes and facilitate
generalization through a novel hybrid matching module that matches input
instances to class descriptions using a combination of semantic and lexical
similarity. Trained with contrastive learning, SemSup-XC significantly
outperforms baselines and establishes state-of-the-art performance on all three
datasets considered, gaining up to 12 precision points on zero-shot and more
than 10 precision points on one-shot tests, with similar gains for recall@10.
Our ablation studies highlight the relative importance of our hybrid matching
module and automatically collected class descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization. (arXiv:2301.11312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11312">
<div class="article-summary-box-inner">
<span><p>Text Summarization is a popular task and an active area of research for the
Natural Language Processing community. By definition, it requires to account
for long input texts, a characteristic which poses computational challenges for
neural models. Moreover, real-world documents come in a variety of complex,
visually-rich, layouts. This information is of great relevance, whether to
highlight salient content or to encode long-range interactions between textual
passages. Yet, all publicly available summarization datasets only provide plain
text content. To facilitate research on how to exploit visual/layout
information to better capture long-range dependencies in summarization models,
we present LoRaLay, a collection of datasets for long-range summarization with
accompanying visual/layout information. We extend existing and popular English
datasets (arXiv and PubMed) with layout information and propose four novel
datasets -- consistently built from scholar resources -- covering French,
Spanish, Portuguese, and Korean languages. Further, we propose new baselines
merging layout-aware and long-range models -- two orthogonal approaches -- and
obtain state-of-the-art results, showing the importance of combining both lines
of research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualizing Emerging Trends in Financial News Articles. (arXiv:2301.11318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11318">
<div class="article-summary-box-inner">
<span><p>Identifying and exploring emerging trends in the news is becoming more
essential than ever with many changes occurring worldwide due to the global
health crises. However, most of the recent research has focused mainly on
detecting trends in social media, thus, benefiting from social features (e.g.
likes and retweets on Twitter) which helped the task as they can be used to
measure the engagement and diffusion rate of content. Yet, formal text data,
unlike short social media posts, comes with a longer, less restricted writing
format, and thus, more challenging. In this paper, we focus our study on
emerging trends detection in financial news articles about Microsoft, collected
before and during the start of the COVID-19 pandemic (July 2019 to July 2020).
We make the dataset accessible and propose a strong baseline (Contextual
Leap2Trend) for exploring the dynamics of similarities between pairs of
keywords based on topic modelling and term frequency. Finally, we evaluate
against a gold standard (Google Trends) and present noteworthy real-world
scenarios regarding the influence of the pandemic on Microsoft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Automated Construction of Food Composition Knowledge Base. (arXiv:2301.11322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11322">
<div class="article-summary-box-inner">
<span><p>A food composition knowledge base, which stores the essential phyto-, micro-,
and macro-nutrients of foods is useful for both research and industrial
applications. Although many existing knowledge bases attempt to curate such
information, they are often limited by time-consuming manual curation
processes. Outside of the food science domain, natural language processing
methods that utilize pre-trained language models have recently shown promising
results for extracting knowledge from unstructured text. In this work, we
propose a semi-automated framework for constructing a knowledge base of food
composition from the scientific literature available online. To this end, we
utilize a pre-trained BioBERT language model in an active learning setup that
allows the optimal use of limited training data. Our work demonstrates how
human-in-the-loop models are a step toward AI-assisted food systems that scale
well to the ever-increasing big data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marpa, A practical general parser: the recognizer. (arXiv:1910.08129v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.08129">
<div class="article-summary-box-inner">
<span><p>The Marpa recognizer is described. Marpa is a practical and fully implemented
algorithm for the recognition, parsing and evaluation of context-free grammars.
The Marpa recognizer is the first to unite the improvements to Earley's
algorithm found in Joop Leo's 1991 paper to those in Aycock and Horspool's 2002
paper. Marpa tracks the full state of the parse, at it proceeds, in a form
convenient for the application. This greatly improves error detection and
enables event-driven parsing. One such technique is "Ruby Slippers" parsing, in
which the input is altered in response to the parser's expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking. (arXiv:2201.05780v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05780">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) module is an important component for
task-oriented dialog systems to understand users' goals and needs. Collecting
dialogue state labels including slots and values can be costly, especially with
the wide application of dialogue systems in more and more new-rising domains.
In this paper, we focus on how to utilize the language understanding and
generation ability of pre-trained language models for DST. We design a dual
prompt learning framework for few-shot DST. Specifically, we consider the
learning of slot generation and value generation as dual tasks, and two prompts
are designed based on such a dual structure to incorporate task-related
knowledge of these two tasks respectively. In this way, the DST task can be
formulated as a language modeling task efficiently under few-shot settings.
Experimental results on two task-oriented dialogue datasets show that the
proposed method not only outperforms existing state-of-the-art few-shot
methods, but also can generate unseen slots. It indicates that DST-related
knowledge can be probed from PLM and utilized to address low-resource DST
efficiently with the help of prompt learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Information Seeking. (arXiv:2201.08808v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08808">
<div class="article-summary-box-inner">
<span><p>Conversational information seeking (CIS) is concerned with a sequence of
interactions between one or more users and an information system. Interactions
in CIS are primarily based on natural language dialogue, while they may include
other types of interactions, such as click, touch, and body gestures. This
monograph provides a thorough overview of CIS definitions, applications,
interactions, interfaces, design, implementation, and evaluation. This
monograph views CIS applications as including conversational search,
conversational question answering, and conversational recommendation. Our aim
is to provide an overview of past research related to CIS, introduce the
current state-of-the-art in CIS, highlight the challenges still being faced in
the community. and suggest future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALBETO and DistilBETO: Lightweight Spanish Language Models. (arXiv:2204.09145v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09145">
<div class="article-summary-box-inner">
<span><p>In recent years there have been considerable advances in pre-trained language
models, where non-English language versions have also been made available. Due
to their increasing use, many lightweight versions of these models (with
reduced parameters) have also been released to speed up training and inference
times. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for
languages other than English are still scarce. In this paper we present ALBETO
and DistilBETO, which are versions of ALBERT and DistilBERT pre-trained
exclusively on Spanish corpora. We train several versions of ALBETO ranging
from 5M to 223M parameters and one of DistilBETO with 67M parameters. We
evaluate our models in the GLUES benchmark that includes various natural
language understanding tasks in Spanish. The results show that our lightweight
models achieve competitive results to those of BETO (Spanish-BERT) despite
having fewer parameters. More specifically, our larger ALBETO model outperforms
all other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets.
However, BETO remains unbeaten for POS and NER. As a further contribution, all
models are publicly available to the community for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assistive Recipe Editing through Critiquing. (arXiv:2205.02454v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02454">
<div class="article-summary-box-inner">
<span><p>There has recently been growing interest in the automatic generation of
cooking recipes that satisfy some form of dietary restrictions, thanks in part
to the availability of online recipe data. Prior studies have used pre-trained
language models, or relied on small paired recipe data (e.g., a recipe paired
with a similar one that satisfies a dietary constraint). However, pre-trained
language models generate inconsistent or incoherent recipes, and paired
datasets are not available at scale. We address these deficiencies with
RecipeCrit, a hierarchical denoising auto-encoder that edits recipes given
ingredient-level critiques. The model is trained for recipe completion to learn
semantic relationships within recipes. Our work's main innovation is our
unsupervised critiquing module that allows users to edit recipes by interacting
with the predicted ingredients; the system iteratively rewrites recipes to
satisfy users' feedback. Experiments on the Recipe1M recipe dataset show that
our model can more effectively edit recipes compared to strong
language-modeling baselines, creating recipes that satisfy user constraints and
are more correct, serendipitous, coherent, and relevant as measured by human
judges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Explanations and Critiques in Recommendation Systems. (arXiv:2205.07268v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07268">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence and machine learning algorithms have become
ubiquitous. Although they offer a wide range of benefits, their adoption in
decision-critical fields is limited by their lack of interpretability,
particularly with textual data. Moreover, with more data available than ever
before, it has become increasingly important to explain automated predictions.
</p>
<p>Generally, users find it difficult to understand the underlying computational
processes and interact with the models, especially when the models fail to
generate the outcomes or explanations, or both, correctly. This problem
highlights the growing need for users to better understand the models' inner
workings and gain control over their actions. This dissertation focuses on two
fundamental challenges of addressing this need. The first involves explanation
generation: inferring high-quality explanations from text documents in a
scalable and data-driven manner. The second challenge consists in making
explanations actionable, and we refer to it as critiquing. This dissertation
examines two important applications in natural language processing and
recommendation tasks.
</p>
<p>Overall, we demonstrate that interpretability does not come at the cost of
reduced performance in two consequential applications. Our framework is
applicable to other fields as well. This dissertation presents an effective
means of closing the gap between promise and practice in artificial
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08657">
<div class="article-summary-box-inner">
<span><p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose Bridge-Tower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, Bridge-Tower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, Bridge-Tower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, Bridge-Tower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
\url{https://github.com/microsoft/BridgeTower}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media. (arXiv:2208.09021v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.09021">
<div class="article-summary-box-inner">
<span><p>We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an
extension of the popular Vision-and-Language Transformer (ViLT), and improves
performance on vision-and-language (VL) tasks that involve more complex text
inputs than image captions while having minimal impact on training and
inference efficiency. ViLT, importantly, enables efficient training and
inference in VL tasks, achieved by encoding images using a linear projection of
patches instead of an object detector. However, it is pretrained on captioning
datasets, where the language input is simple, literal, and descriptive,
therefore lacking linguistic diversity. So, when working with multimedia data
in the wild, such as multimodal social media data, there is a notable shift
from captioning language data, as well as diversity of tasks. We indeed find
evidence that the language capacity of ViLT is lacking. The key insight and
novelty of VAuLT is to propagate the output representations of a large language
model (LM) like BERT to the language input of ViLT. We show that joint training
of the LM and ViLT can yield relative improvements up to 20% over ViLT and
achieve state-of-the-art or comparable performance on VL tasks involving richer
language inputs and affective constructs, such as for Target-Oriented Sentiment
Classification in TWITTER-2015 and TWITTER-2017, and Sentiment Classification
in MVSA-Single and MVSA-Multiple. Our code is available at
https://github.com/gchochla/VAuLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review of Natural Language Processing in Pharmacology. (arXiv:2208.10228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2208.10228">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) is an area of artificial intelligence that
applies information technologies to process the human language, understand it
to a certain degree, and use it in various applications. This area has rapidly
developed in the last few years and now employs modern variants of deep neural
networks to extract relevant patterns from large text corpora. The main
objective of this work is to survey the recent use of NLP in the field of
pharmacology. As our work shows, NLP is a highly relevant information
extraction and processing approach for pharmacology. It has been used
extensively, from intelligent searches through thousands of medical documents
to finding traces of adversarial drug interactions in social media. We split
our coverage into five categories to survey modern NLP methodology, commonly
addressed tasks, relevant textual data, knowledge bases, and useful programming
libraries. We split each of the five categories into appropriate subcategories,
describe their main properties and ideas, and summarize them in a tabular form.
The resulting survey presents a comprehensive overview of the area, useful to
practitioners and interested observers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. (arXiv:2210.01240v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01240">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown remarkable reasoning capabilities
given chain-of-thought prompts (examples with intermediate reasoning steps).
Existing benchmarks measure reasoning ability indirectly, by evaluating
accuracy on downstream tasks such as mathematical reasoning. However, it is
unclear how these models obtain the answers and whether they rely on simple
heuristics rather than the generated chain-of-thought. To enable systematic
exploration of the reasoning ability of LLMs, we present a new synthetic
question-answering dataset called PrOntoQA, where each example is generated
from a synthetic world model represented in first-order logic. This allows us
to parse the generated chain-of-thought into symbolic proofs for formal
analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite
capable of making correct individual deduction steps, and so are generally
capable of reasoning, even in fictional contexts. However, they have difficulty
with proof planning: When multiple valid deduction steps are available, they
are not able to systematically explore the different options.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. (arXiv:2210.13382v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.13382">
<div class="article-summary-box-inner">
<span><p>Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create "latent saliency maps" that can help explain predictions
in human terms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parsel: A (De-)compositional Framework for Algorithmic Reasoning with Language Models. (arXiv:2212.10561v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10561">
<div class="article-summary-box-inner">
<span><p>Despite recent success in large language model (LLM) reasoning, LLMs struggle
with hierarchical multi-step reasoning tasks like generating complex programs.
For these tasks, humans often start with a high-level algorithmic design and
implement each part gradually. We introduce Parsel, a framework enabling
automatic implementation and validation of complex algorithms with code LLMs,
taking hierarchical function descriptions in natural language as input. We show
that Parsel can be used across domains requiring hierarchical reasoning,
including program synthesis, robotic planning, and theorem proving. We show
that LLMs generating Parsel solve more competition-level problems in the APPS
dataset, resulting in pass rates that are over 75% higher than prior results
from directly sampling AlphaCode and Codex, while often using a smaller sample
budget. We also find that LLM-generated robotic plans using Parsel as an
intermediate language are more than twice as likely to be considered accurate
than directly generated plans. Lastly, we explore how Parsel addresses LLM
limitations and discuss how Parsel may be useful for human programmers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study. (arXiv:2301.09099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.09099">
<div class="article-summary-box-inner">
<span><p>Several high-resource Text to Speech (TTS) systems currently produce natural,
well-established human-like speech. In contrast, low-resource languages,
including Arabic, have very limited TTS systems due to the lack of resources.
We propose a fully unsupervised method for building TTS, including automatic
data selection and pre-training/fine-tuning strategies for TTS training, using
broadcast news as a case study. We show how careful selection of data, yet
smaller amounts, can improve the efficiency of TTS system in generating more
natural speech than a system trained on a bigger dataset. We adopt to propose
different approaches for the: 1) data: we applied automatic annotations using
DNSMOS, automatic vowelization, and automatic speech recognition (ASR) for
fixing transcriptions' errors; 2) model: we used transfer learning from
high-resource language in TTS model and fine-tuned it with one hour broadcast
recording then we used this model to guide a FastSpeech2-based Conformer model
for duration. Our objective evaluation shows 3.9% character error rate (CER),
while the groundtruth has 1.3% CER. As for the subjective evaluation, where 1
is bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a
mean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness,
where many annotators recognized the voice of the broadcaster, which proves the
effectiveness of our proposed unsupervised method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViHOS: Hate Speech Spans Detection for Vietnamese. (arXiv:2301.10186v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10186">
<div class="article-summary-box-inner">
<span><p>The rise in hateful and offensive language directed at other users is one of
the adverse side effects of the increased use of social networking platforms.
This could make it difficult for human moderators to review tagged comments
filtered by classification systems. To help address this issue, we present the
ViHOS (Vietnamese Hate and Offensive Spans) dataset, the first human-annotated
corpus containing 26k spans on 11k comments. We also provide definitions of
hateful and offensive spans in Vietnamese comments as well as detailed
annotation guidelines. Besides, we conduct experiments with various
state-of-the-art models. Specifically, XLM-R$_{Large}$ achieved the best
F1-scores in Single span detection and All spans detection, while
PhoBERT$_{Large}$ obtained the highest in Multiple spans detection. Finally,
our error analysis demonstrates the difficulties in detecting specific types of
spans in our data for future research.
</p>
<p>Disclaimer: This paper contains real comments that could be considered
profane, offensive, or abusive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-level Relation Extraction as Semantic Segmentation. (arXiv:2106.03618v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03618">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction aims to extract relations among multiple
entity pairs from a document. Previously proposed graph-based or
transformer-based models utilize the entities independently, regardless of
global information among relational triples. This paper approaches the problem
by predicting an entity-level relation matrix to capture local and global
information, parallel to the semantic segmentation task in computer vision.
Herein, we propose a Document U-shaped Network for document-level relation
extraction. Specifically, we leverage an encoder module to capture the context
information of entities and a U-shaped segmentation module over the image-style
feature map to capture global interdependency among triples. Experimental
results show that our approach can obtain state-of-the-art performance on three
benchmark datasets DocRED, CDR, and GDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-enhanced Prompt-tuning for Few-shot Learning. (arXiv:2201.11332v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11332">
<div class="article-summary-box-inner">
<span><p>Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-29 23:12:31.314287494 UTC">2023-01-29 23:12:31 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>