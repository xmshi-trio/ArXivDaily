<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-06-12T01:30:00Z">06-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">LexGPT 0.1: pre-trained GPT-J models with Pile of Law. (arXiv:2306.05431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05431">
<div class="article-summary-box-inner">
<span><p>This research aims to build generative language models specialized for the
legal domain. The manuscript presents the development of LexGPT models based on
GPT-J models and pre-trained with Pile of Law. The foundation model built in
this manuscript is the initial step for the development of future applications
in the legal domain, such as further training with reinforcement learning from
human feedback. Another objective of this manuscript is to assist legal
professionals in utilizing language models through the ``No Code'' approach. By
fine-tuning models with specialized data and without modifying any source code,
legal professionals can create custom language models for downstream tasks with
minimum effort and technical knowledge. The downstream task in this manuscript
is to turn a LexGPT model into a classifier, although the performance is
notably lower than the state-of-the-art result. How to enhance downstream task
performance without modifying the model or its source code is a research topic
for future exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-end Speech-to-text Summarization. (arXiv:2306.05432v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05432">
<div class="article-summary-box-inner">
<span><p>Speech-to-text (S2T) summarization is a time-saving technique for filtering
and keeping up with the broadcast news uploaded online on a daily basis. The
rise of large language models from deep learning with impressive text
generation capabilities has placed the research focus on summarization systems
that produce paraphrased compact versions of the document content, also known
as abstractive summaries. End-to-end (E2E) modelling of S2T abstractive
summarization is a promising approach that offers the possibility of generating
rich latent representations that leverage non-verbal and acoustic information,
as opposed to the use of only linguistic information from automatically
generated transcripts in cascade systems. However, the few literature on E2E
modelling of this task fails on exploring different domains, namely broadcast
news, which is challenging domain where large and diversified volumes of data
are presented to the user every day. We model S2T summarization both with a
cascade and an E2E system for a corpus of broadcast news in French. Our novel
E2E model leverages external data by resorting to transfer learning from a
pre-trained T2T summarizer. Experiments show that both our cascade and E2E
abstractive summarizers are stronger than an extractive baseline. However, the
performance of the E2E model still lies behind the cascade one, which is object
of an extensive analysis that includes future directions to close that gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?. (arXiv:2306.05434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05434">
<div class="article-summary-box-inner">
<span><p>Annotating cross-document event coreference links is a time-consuming and
cognitively demanding task that can compromise annotation quality and
efficiency. To address this, we propose a model-in-the-loop annotation approach
for event coreference resolution, where a machine learning model suggests
likely corefering event pairs only. We evaluate the effectiveness of this
approach by first simulating the annotation process and then, using a novel
annotator-centric Recall-Annotation effort trade-off metric, we compare the
results of various underlying models and datasets. We finally present a method
for obtaining 97\% recall while substantially reducing the workload required by
a fully manual annotation process. Code and data can be found at
https://github.com/ahmeshaf/model_in_coref
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance. (arXiv:2306.05443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05443">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) has shown great performance on natural
language processing (NLP) in the financial domain, there are no publicly
available financial tailtored LLMs, instruction tuning datasets, and evaluation
benchmarks, which is critical for continually pushing forward the open-source
development of financial artificial intelligence (AI). This paper introduces
PIXIU, a comprehensive framework including the first financial LLM based on
fine-tuning LLaMA with instruction data, the first instruction data with 136K
data samples to support the fine-tuning, and an evaluation benchmark with 5
tasks and 9 datasets. We first construct the large-scale multi-task instruction
data considering a variety of financial tasks, financial document types, and
financial data modalities. We then propose a financial LLM called FinMA by
fine-tuning LLaMA with the constructed dataset to be able to follow
instructions for various financial tasks. To support the evaluation of
financial LLMs, we propose a standardized benchmark that covers a set of
critical financial tasks, including five financial NLP tasks and one financial
prediction task. With this benchmark, we conduct a detailed analysis of FinMA
and several existing LLMs, uncovering their strengths and weaknesses in
handling critical financial tasks. The model, datasets, benchmark, and
experimental results are open-sourced to facilitate future research in
financial AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Phrase Matching for Dysarthric Speech. (arXiv:2306.05446v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05446">
<div class="article-summary-box-inner">
<span><p>Many consumer speech recognition systems are not tuned for people with speech
disabilities, resulting in poor recognition and user experience, especially for
severe speech differences. Recent studies have emphasized interest in
personalized speech models from people with atypical speech patterns. We
propose a query-by-example-based personalized phrase recognition system that is
trained using small amounts of speech, is language agnostic, does not assume a
traditional pronunciation lexicon, and generalizes well across speech
difference severities. On an internal dataset collected from 32 people with
dysarthria, this approach works regardless of severity and shows a 60%
improvement in recall relative to a commercial speech recognition system. On
the public EasyCall dataset of dysarthric speech, our approach improves
accuracy by 30.5%. Performance degrades as the number of phrases increases, but
consistently outperforms ASR systems when trained with 50 unique phrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hexatagging: Projective Dependency Parsing as Tagging. (arXiv:2306.05477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05477">
<div class="article-summary-box-inner">
<span><p>We introduce a novel dependency parser, the hexatagger, that constructs
dependency trees by tagging the words in a sentence with elements from a finite
set of possible tags. In contrast to many approaches to dependency parsing, our
approach is fully parallelizable at training time, i.e., the structure-building
actions needed to build a dependency parse can be predicted in parallel to each
other. Additionally, exact decoding is linear in time and space complexity.
Furthermore, we derive a probabilistic dependency parser that predicts hexatags
using no more than a linear model with features from a pretrained language
model, i.e., we forsake a bespoke architecture explicitly designed for the
task. Despite the generality and simplicity of our approach, we achieve
state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test
set. Additionally, our parser's linear time complexity and parallelism
significantly improve computational efficiency, with a roughly 10-times
speed-up over previous state-of-the-art models during decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Injection attack against LLM-integrated Applications. (arXiv:2306.05499v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05499">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs), renowned for their superior proficiency in
language comprehension and generation, stimulate a vibrant ecosystem of
applications around them. However, their extensive assimilation into various
services introduces significant security risks. This study deconstructs the
complexities and implications of prompt injection attacks on actual
LLM-integrated applications. Initially, we conduct an exploratory analysis on
ten commercial applications, highlighting the constraints of current attack
strategies in practice. Prompted by these limitations, we subsequently
formulate HouYi, a novel black-box prompt injection attack technique, which
draws inspiration from traditional web injection attacks. HouYi is
compartmentalized into three crucial elements: a seamlessly-incorporated
pre-constructed prompt, an injection prompt inducing context partition, and a
malicious payload designed to fulfill the attack objectives. Leveraging HouYi,
we unveil previously unknown and severe attack outcomes, such as unrestricted
arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi
on 36 actual LLM-integrated applications and discern 31 applications
susceptible to prompt injection. 10 vendors have validated our discoveries,
including Notion, which has the potential to impact millions of users. Our
investigation illuminates both the possible risks of prompt injection attacks
and the possible tactics for mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word-Level Explanations for Analyzing Bias in Text-to-Image Models. (arXiv:2306.05500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05500">
<div class="article-summary-box-inner">
<span><p>Text-to-image models take a sentence (i.e., prompt) and generate images
associated with this input prompt. These models have created award wining-art,
videos, and even synthetic datasets. However, text-to-image (T2I) models can
generate images that underrepresent minorities based on race and sex. This
paper investigates which word in the input prompt is responsible for bias in
generated images. We introduce a method for computing scores for each word in
the prompt; these scores represent its influence on biases in the model's
output. Our method follows the principle of \emph{explaining by removing},
leveraging masked language models to calculate the influence scores. We perform
experiments on Stable Diffusion to demonstrate that our method identifies the
replication of societal stereotypes in generated images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05523">
<div class="article-summary-box-inner">
<span><p>Combating disinformation is one of the burning societal crises -- about 67%
of the American population believes that disinformation produces a lot of
uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows
that disinformation can manipulate democratic processes and public opinion,
causing disruption in the share market, panic and anxiety in society, and even
death during crises. Therefore, disinformation should be identified promptly
and, if possible, mitigated. With approximately 3.2 billion images and 720,000
hours of video shared online daily on social media platforms, scalable
detection of multimodal disinformation requires efficient fact verification.
Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR),
the research community lacks substantial effort in multimodal fact
verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3
million samples that pushes the boundaries of the domain of fact verification
via a multimodal fake news dataset, in addition to offering explainability
through the concept of 5W question-answering. Salient features of the dataset
include: (i) textual claims, (ii) ChatGPT-generated paraphrased claims, (iii)
associated images, (iv) stable diffusion-generated additional images (i.e.,
visual paraphrases), (v) pixel-level image heatmap to foster image-text
explainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news
stories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT. (arXiv:2306.05524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05524">
<div class="article-summary-box-inner">
<span><p>With ChatGPT under the spotlight, utilizing large language models (LLMs) for
academic writing has drawn a significant amount of discussions and concerns in
the community. While substantial research efforts have been stimulated for
detecting LLM-Generated Content (LLM-content), most of the attempts are still
in the early stage of exploration. In this paper, we present a holistic
investigation of detecting LLM-generate academic writing, by providing a
dataset, evidence, and algorithms, in order to inspire more community effort to
address the concern of LLM academic misuse. We first present GPABenchmark, a
benchmarking dataset of 600,000 samples of human-written, GPT-written,
GPT-completed, and GPT-polished abstracts of research papers in CS, physics,
and humanities and social sciences (HSS). We show that existing open-source and
commercial GPT detectors provide unsatisfactory performance on GPABenchmark,
especially for GPT-polished text. Moreover, through a user study of 150+
participants, we show that it is highly challenging for human users, including
experienced faculty members and researchers, to identify GPT-generated
abstracts. We then present CheckGPT, a novel LLM-content detector consisting of
a general representation module and an attentive-BiLSTM classification module,
which is accurate, transferable, and interpretable. Experimental results show
that CheckGPT achieves an average classification accuracy of 98% to 99% for the
task-specific discipline-specific detectors and the unified detectors. CheckGPT
is also highly transferable that, without tuning, it achieves ~90% accuracy in
new domains, such as news articles, while a model tuned with approximately
2,000 samples in the target domain achieves ~98% accuracy. Finally, we
demonstrate the explainability insights obtained from CheckGPT to reveal the
key behaviors of how LLM generates texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05535">
<div class="article-summary-box-inner">
<span><p>A large portion of society united around the same vision and ideas carries
enormous energy. That is precisely what political figures would like to
accumulate for their cause. With this goal in mind, they can sometimes resort
to distorting or hiding the truth, unintentionally or on purpose, which opens
the door for misinformation and disinformation. Tools for automatic detection
of check-worthy claims would be of great help to moderators of debates,
journalists, and fact-checking organizations. While previous work on detecting
check-worthy claims has focused on text, here we explore the utility of the
audio signal as an additional information source. We create a new multimodal
dataset (text and audio in English) containing 48 hours of speech. Our
evaluation results show that the audio modality together with text yields
improvements over text alone in the case of multiple speakers. Moreover, an
audio-only model could outperform a text-only one for a single speaker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization. (arXiv:2306.05537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05537">
<div class="article-summary-box-inner">
<span><p>The rapid growth of information on the Internet has led to an overwhelming
amount of opinions and comments on various activities, products, and services.
This makes it difficult and time-consuming for users to process all the
available information when making decisions. Text summarization, a Natural
Language Processing (NLP) task, has been widely explored to help users quickly
retrieve relevant information by generating short and salient content from long
or multiple documents. Recent advances in pre-trained language models, such as
ChatGPT, have demonstrated the potential of Large Language Models (LLMs) in
text generation. However, LLMs require massive amounts of data and resources
and are challenging to implement as offline applications. Furthermore, existing
text summarization approaches often lack the ``adaptive" nature required to
capture diverse aspects in opinion summarization, which is particularly
detrimental to users with specific requirements or preferences. In this paper,
we propose an Aspect-adaptive Knowledge-based Opinion Summarization model for
product reviews, which effectively captures the adaptive nature required for
opinion summarization. The model generates aspect-oriented summaries given a
set of reviews for a particular product, efficiently providing users with
useful information on specific aspects they are interested in, ensuring the
generated summaries are more personalized and informative. Extensive
experiments have been conducted using real-world datasets to evaluate the
proposed model. The results demonstrate that our model outperforms
state-of-the-art approaches and is adaptive and efficient in generating
summaries that focus on particular aspects, enabling users to make
well-informed decisions and catering to their diverse interests and
preferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instruction Tuned Models are Quick Learners. (arXiv:2306.05539v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05539">
<div class="article-summary-box-inner">
<span><p>Instruction tuning of language models has demonstrated the ability to enhance
model generalization to unseen tasks via in-context learning using a few
examples. However, typical supervised learning still requires a plethora of
downstream training data for finetuning. Often in real-world situations, there
is a scarcity of data available for finetuning, falling somewhere between few
shot inference and fully supervised finetuning. In this work, we demonstrate
the sample efficiency of instruction tuned models over various tasks by
estimating the minimal downstream training data required by them to perform
transfer learning and match the performance of state-of-the-art (SOTA)
supervised models. We conduct experiments on 119 tasks from Super Natural
Instructions (SuperNI) in both the single task learning (STL) and multi task
learning (MTL) settings. Our findings reveal that, in the STL setting,
instruction tuned models equipped with 25% of the downstream train data surpass
the SOTA performance on the downstream tasks. In the MTL setting, an
instruction tuned model trained on only 6% of downstream training data achieve
SOTA, while using 100% of the training data results in a 3.69% points
improvement (ROUGE-L 74.68) over the previous SOTA. We conduct an analysis on
T5 vs Tk-Instruct by developing several baselines to demonstrate that
instruction tuning aids in increasing both sample efficiency and transfer
learning. Additionally, we observe a consistent ~4% performance increase in
both settings when pre-finetuning is performed with instructions. Finally, we
conduct a categorical study and find that contrary to previous results, tasks
in the question rewriting and title generation categories suffer from
instruction tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text. (arXiv:2306.05540v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05540">
<div class="article-summary-box-inner">
<span><p>With the rapid progress of large language models (LLMs) and the huge amount
of text they generated, it becomes more and more impractical to manually
distinguish whether a text is machine-generated. Given the growing use of LLMs
in social media and education, it prompts us to develop methods to detect
machine-generated text, preventing malicious usage such as plagiarism,
misinformation, and propaganda. Previous work has studied several zero-shot
methods, which require no training data. These methods achieve good
performance, but there is still a lot of room for improvement. In this paper,
we introduce two novel zero-shot methods for detecting machine-generated text
by leveraging the log rank information. One is called DetectLLM-LRR, which is
fast and efficient, and the other is called DetectLLM-NPR, which is more
accurate, but slower due to the need for perturbations. Our experiments on
three datasets and seven language models show that our proposed methods improve
over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover,
DetectLLM-NPR needs fewer perturbations than previous work to achieve the same
level of performance, which makes it more practical for real-world use. We also
investigate the efficiency--performance trade-off based on users preference on
these two measures and we provide intuition for using them in practice
effectively. We release the data and the code of both methods in
https://github.com/mbzuai-nlp/DetectLLM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks. (arXiv:2306.05550v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05550">
<div class="article-summary-box-inner">
<span><p>The rapid deployment of artificial intelligence (AI) models demands a
thorough investigation of biases and risks inherent in these models to
understand their impact on individuals and society. This study extends the
focus of bias evaluation in extant work by examining bias against social
stigmas on a large scale. It focuses on 93 stigmatized groups in the United
States, including a wide range of conditions related to disease, disability,
drug use, mental illness, religion, sexuality, socioeconomic status, and other
relevant factors. We investigate bias against these groups in English
pre-trained Masked Language Models (MLMs) and their downstream sentiment
classification tasks. To evaluate the presence of bias against 93 stigmatized
conditions, we identify 29 non-stigmatized conditions to conduct a comparative
analysis. Building upon a psychology scale of social rejection, the Social
Distance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large,
BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to
analyze the predicted words from these models, with which we measure the extent
of bias against stigmatized groups. When prompts include stigmatized
conditions, the probability of MLMs predicting negative words is approximately
20 percent higher than when prompts have non-stigmatized conditions. In the
sentiment classification tasks, when sentences include stigmatized conditions
related to diseases, disability, education, and mental illness, they are more
likely to be classified as negative. We also observe a strong correlation
between bias in MLMs and their downstream sentiment classifiers (r =0.79). The
evidence indicates that MLMs and their downstream sentiment classification
tasks exhibit biases against socially stigmatized groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery. (arXiv:2306.05552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05552">
<div class="article-summary-box-inner">
<span><p>Large language models have been useful in expanding mental health care
delivery. ChatGPT, in particular, has gained popularity for its ability to
generate human-like dialogue. However, data-sensitive domains -- including but
not limited to healthcare -- face challenges in using ChatGPT due to privacy
and data-ownership concerns. To enable its utilization, we propose a text
ambiguation framework that preserves user privacy. We ground this in the task
of addressing stress prompted by user-provided texts to demonstrate the
viability and helpfulness of privacy-preserved generations. Our results suggest
that chatGPT recommendations are still able to be moderately helpful and
relevant, even when the original user text is not provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion and Sentiment Guided Paraphrasing. (arXiv:2306.05556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05556">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation, a.k.a. paraphrasing, is a common and important task in
natural language processing. Emotional paraphrasing, which changes the emotion
embodied in a piece of text while preserving its meaning, has many potential
applications, including moderating online dialogues and preventing
cyberbullying. We introduce a new task of fine-grained emotional paraphrasing
along emotion gradients, that is, altering the emotional intensities of the
paraphrases in fine-grained settings following smooth variations in affective
dimensions while preserving the meaning of the original text. We reconstruct
several widely used paraphrasing datasets by augmenting the input and target
texts with their fine-grained emotion labels. Then, we propose a framework for
emotion and sentiment guided paraphrasing by leveraging pre-trained language
models for conditioned text generation. Extensive evaluation of the fine-tuned
models suggests that including fine-grained emotion labels in the paraphrase
task significantly improves the likelihood of obtaining high-quality
paraphrases that reflect the desired emotions while achieving consistently
better scores in paraphrase metrics such as BLEU, ROUGE, and METEOR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization. (arXiv:2306.05561v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05561">
<div class="article-summary-box-inner">
<span><p>This work investigates the effectiveness of different pseudonymization
techniques, ranging from rule-based substitutions to using pre-trained Large
Language Models (LLMs), on a variety of datasets and models used for two widely
used NLP tasks: text classification and summarization. Our work provides
crucial insights into the gaps between original and anonymized data (focusing
on the pseudonymization technique) and model quality and fosters future
research into higher-quality anonymization techniques to better balance the
trade-offs between data protection and utility preservation. We make our code,
pseudonymized datasets, and downstream models publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LOST: A Mental Health Dataset of Low Self-esteem in Reddit Posts. (arXiv:2306.05596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05596">
<div class="article-summary-box-inner">
<span><p>Low self-esteem and interpersonal needs (i.e., thwarted belongingness (TB)
and perceived burdensomeness (PB)) have a major impact on depression and
suicide attempts. Individuals seek social connectedness on social media to
boost and alleviate their loneliness. Social media platforms allow people to
express their thoughts, experiences, beliefs, and emotions. Prior studies on
mental health from social media have focused on symptoms, causes, and
disorders. Whereas an initial screening of social media content for
interpersonal risk factors and low self-esteem may raise early alerts and
assign therapists to at-risk users of mental disturbance. Standardized scales
measure self-esteem and interpersonal needs from questions created using
psychological theories. In the current research, we introduce a
psychology-grounded and expertly annotated dataset, LoST: Low Self esTeem, to
study and detect low self-esteem on Reddit. Through an annotation approach
involving checks on coherence, correctness, consistency, and reliability, we
ensure gold-standard for supervised learning. We present results from different
deep language models tested using two data augmentation techniques. Our
findings suggest developing a class of language models that infuses
psychological and clinical knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Generative Approach to Product Attribute-Value Identification. (arXiv:2306.05605v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05605">
<div class="article-summary-box-inner">
<span><p>Product attribute-value identification (PAVI) has been studied to link
products on e-commerce sites with their attribute values (e.g., &lt;Material,
Cotton&gt;) using product text as clues. Technical demands from real-world
e-commerce platforms require PAVI methods to handle unseen values,
multi-attribute values, and canonicalized values, which are only partly
addressed in existing extraction- and classification-based approaches.
Motivated by this, we explore a generative approach to the PAVI task. We
finetune a pre-trained generative model, T5, to decode a set of attribute-value
pairs as a target sequence from the given product text. Since the attribute
value pairs are unordered set elements, how to linearize them will matter; we,
thus, explore methods of composing an attribute-value pair and ordering the
pairs for the task. Experimental results confirm that our generation-based
approach outperforms the existing extraction and classification-based methods
on large-scale real-world datasets meant for those methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word sense extension. (arXiv:2306.05609v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05609">
<div class="article-summary-box-inner">
<span><p>Humans often make creative use of words to express novel senses. A
long-standing effort in natural language processing has been focusing on word
sense disambiguation (WSD), but little has been explored about how the sense
inventory of a word may be extended toward novel meanings. We present a
paradigm of word sense extension (WSE) that enables words to spawn new senses
toward novel context. We develop a framework that simulates novel word sense
extension by first partitioning a polysemous word type into two pseudo-tokens
that mark its different senses, and then inferring whether the meaning of a
pseudo-token can be extended to convey the sense denoted by the token
partitioned from the same word type. Our framework combines cognitive models of
chaining with a learning scheme that transforms a language model embedding
space to support various types of word sense extension. We evaluate our
framework against several competitive baselines and show that it is superior in
predicting plausible novel senses for over 7,500 English words. Furthermore, we
show that our WSE framework improves performance over a range of
transformer-based WSD models in predicting rare word senses with few or zero
mentions in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection. (arXiv:2306.05617v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05617">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech models are a rapidly developing research topic in fake
audio detection. Many pre-trained models can serve as feature extractors,
learning richer and higher-level speech features. However,when fine-tuning
pre-trained models, there is often a challenge of excessively long training
times and high memory consumption, and complete fine-tuning is also very
expensive. To alleviate this problem, we apply low-rank adaptation(LoRA) to the
wav2vec2 model, freezing the pre-trained model weights and injecting a
trainable rank-decomposition matrix into each layer of the transformer
architecture, greatly reducing the number of trainable parameters for
downstream tasks. Compared with fine-tuning with Adam on the wav2vec2 model
containing 317M training parameters, LoRA achieved similar performance by
reducing the number of trainable parameters by 198 times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Customizing General-Purpose Foundation Models for Medical Report Generation. (arXiv:2306.05642v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05642">
<div class="article-summary-box-inner">
<span><p>Medical caption prediction which can be regarded as a task of medical report
generation (MRG), requires the automatic generation of coherent and accurate
captions for the given medical images. However, the scarcity of labelled
medical image-report pairs presents great challenges in the development of deep
and large-scale neural networks capable of harnessing the potential artificial
general intelligence power like large language models (LLMs). In this work, we
propose customizing off-the-shelf general-purpose large-scale pre-trained
models, i.e., foundation models (FMs), in computer vision and natural language
processing with a specific focus on medical report generation. Specifically,
following BLIP-2, a state-of-the-art vision-language pre-training approach, we
introduce our encoder-decoder-based MRG model. This model utilizes a
lightweight query Transformer to connect two FMs: the giant vision Transformer
EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred
to as ChatGLM-6B). Furthermore, we conduct ablative experiments on the
trainable components of the model to identify the crucial factors for effective
transfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn
medical image representations, followed by parameter-efficient training of
ChatGLM-6B to capture the writing styles of medical reports, is essential for
achieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and
the 2nd, respectively, out of 13 participating teams, based on the BERTScore
and ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction
Task competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05644">
<div class="article-summary-box-inner">
<span><p>Most existing word alignment methods rely on manual alignment datasets or
parallel corpora, which limits their usefulness. Here, to mitigate the
dependence on manual data, we broaden the source of supervision by relaxing the
requirement for correct, fully-aligned, and parallel sentences. Specifically,
we make noisy, partially aligned, and non-parallel paragraphs. We then use such
a large-scale weakly-supervised dataset for word alignment pre-training via
span prediction. Extensive experiments with various settings empirically
demonstrate that our approach, which is named WSPAlign, is an effective and
scalable way to pre-train word aligners without manual data. When fine-tuned on
standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon
the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER.
Furthermore, WSPAlign also achieves competitive performance compared with the
corresponding baselines in few-shot, zero-shot and cross-lingual tests, which
demonstrates that WSPAlign is potentially more practical for low-resource
languages than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Aware Question-Answering System for Online Mental Health Risk Assessment. (arXiv:2306.05652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05652">
<div class="article-summary-box-inner">
<span><p>Social media platforms have enabled individuals suffering from mental
illnesses to share their lived experiences and find the online support
necessary to cope. However, many users fail to receive genuine clinical
support, thus exacerbating their symptoms. Screening users based on what they
post online can aid providers in administering targeted healthcare and minimize
false positives. Pre-trained Language Models (LMs) can assess users' social
media data and classify them in terms of their mental health risk. We propose a
Question-Answering (QA) approach to assess mental health risk using the
Unified-QA model on two large mental health datasets. To protect user data, we
extend Unified-QA by anonymizing the model training process using differential
privacy. Our results demonstrate the effectiveness of modeling risk assessment
as a QA task, specifically for mental health use cases. Furthermore, the
model's performance decreases by less than 1% with the inclusion of
differential privacy. The proposed system's performance is indicative of a
promising research direction that will lead to the development of privacy-aware
diagnostic systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05659">
<div class="article-summary-box-inner">
<span><p>Prompt-based learning has been proved to be an effective way in pre-trained
language models (PLMs), especially in low-resource scenarios like few-shot
settings. However, the trustworthiness of PLMs is of paramount significance and
potential vulnerabilities have been shown in prompt-based templates that could
mislead the predictions of language models, causing serious security concerns.
In this paper, we will shed light on some vulnerabilities of PLMs, by proposing
a prompt-based adversarial attack on manual templates in black box scenarios.
First of all, we design character-level and word-level heuristic approaches to
break manual templates separately. Then we present a greedy algorithm for the
attack based on the above heuristic destructive approaches. Finally, we
evaluate our approach with the classification tasks on three variants of BERT
series models and eight datasets. And comprehensive experimental results
justify the effectiveness of our approach in terms of attack success rate and
attack speed. Further experimental studies indicate that our proposed method
also displays good capabilities in scenarios with varying shot counts, template
lengths and query counts, exhibiting good generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I run as fast as a rabbit, can you? A Multilingual Simile Dialogue Dataset. (arXiv:2306.05672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05672">
<div class="article-summary-box-inner">
<span><p>A simile is a figure of speech that compares two different things (called the
tenor and the vehicle) via shared properties. The tenor and the vehicle are
usually connected with comparator words such as "like" or "as". The simile
phenomena are unique and complex in a real-life dialogue scene where the tenor
and the vehicle can be verbal phrases or sentences, mentioned by different
speakers, exist in different sentences, or occur in reversed order. However,
the current simile research usually focuses on similes in a triplet tuple
(tenor, property, vehicle) or a single sentence where the tenor and vehicle are
usually entities or noun phrases, which could not reflect complex simile
phenomena in real scenarios. In this paper, we propose a novel and high-quality
multilingual simile dialogue (MSD) dataset to facilitate the study of complex
simile phenomena. The MSD is the largest manually annotated simile data
($\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD
data can also be used on dialogue tasks to test the ability of dialogue systems
when using similes. We design 3 simile tasks (recognition, interpretation, and
generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each
task, we provide experimental results from strong pre-trained or
state-of-the-art models. The experiments demonstrate the challenge of MSD and
we have released the data/code on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05685">
<div class="article-summary-box-inner">
<span><p>Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, such as position and verbosity biases
and limited reasoning ability, and propose solutions to migrate some of them.
We then verify the agreement between LLM judges and human preferences by
introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot
Arena, a crowdsourced battle platform. Our results reveal that strong LLM
judges like GPT-4 can match both controlled and crowdsourced human preferences
well, achieving over 80\% agreement, the same level of agreement between
humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate
human preferences, which are otherwise very expensive to obtain. Additionally,
we show our benchmark and traditional benchmarks complement each other by
evaluating several variants of LLaMA/Vicuna. We will publicly release 80
MT-bench questions, 3K expert votes, and 30K conversations with human
preferences from Chatbot Arena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech. (arXiv:2306.05709v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05709">
<div class="article-summary-box-inner">
<span><p>Effective speech emotional representations play a key role in Speech Emotion
Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional
speech samples are more difficult and expensive to acquire compared with
Neutral style speech, which causes one issue that most related works
unfortunately neglect: imbalanced datasets. Models might overfit to the
majority Neutral class and fail to produce robust and effective emotional
representations. In this paper, we propose an Emotion Extractor to address this
issue. We use augmentation approaches to train the model and enable it to
extract effective and generalizable emotional representations from imbalanced
datasets. Our empirical results show that (1) for the SER task, the proposed
Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced
datasets; (2) the produced representations from our Emotion Extractor benefit
the TTS model, and enable it to synthesize more expressive speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests. (arXiv:2306.05715v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05715">
<div class="article-summary-box-inner">
<span><p>Background and Context: Over the past year, large language models (LLMs) have
taken the world by storm. In computing education, like in other walks of life,
many opportunities and threats have emerged as a consequence.
</p>
<p>Objectives: In this article, we explore such opportunities and threats in a
specific area: responding to student programmers' help requests. More
specifically, we assess how good LLMs are at identifying issues in problematic
code that students request help on.
</p>
<p>Method: We collected a sample of help requests and code from an online
programming course. We then prompted two different LLMs (OpenAI Codex and
GPT-3.5) to identify and explain the issues in the students' code and assessed
the LLM-generated answers both quantitatively and qualitatively.
</p>
<p>Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently
find at least one actual issue in each student program (GPT-3.5 in 90% of the
cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%
of the time). False positives are common (40% chance for GPT-3.5). The advice
that the LLMs provide on the issues is often sensible. The LLMs perform better
on issues involving program logic rather than on output formatting. Model
solutions are frequently provided even when the LLM is prompted not to. LLM
responses to prompts in a non-English language are only slightly worse than
responses to English prompts.
</p>
<p>Implications: Our results continue to highlight the utility of LLMs in
programming education. At the same time, the results highlight the
unreliability of LLMs: LLMs make some of the same mistakes that students do,
perhaps especially when formatting output as required by automated assessment
systems. Our study informs teachers interested in using LLMs as well as future
efforts to customize LLMs for the needs of programming education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Opportunities for the Design of Smart Speakers. (arXiv:2306.05741v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05741">
<div class="article-summary-box-inner">
<span><p>Advances in voice technology and voice user interfaces (VUIs) -- such as
Alexa, Siri, and Google Home -- have opened up the potential for many new types
of interaction. However, despite the potential of these devices reflected by
the growing market and body of VUI research, there is a lingering sense that
the technology is still underused. In this paper, we conducted a systematic
literature review of 35 papers to identify and synthesize 127 VUI design
guidelines into five themes. Additionally, we conducted semi-structured
interviews with 15 smart speaker users to understand their use and non-use of
the technology. From the interviews, we distill four design challenges that
contribute the most to non-use. Based on their (non-)use, we identify four
opportunity spaces for designers to explore such as focusing on information
support while multitasking (cooking, driving, childcare, etc), incorporating
users' mental models for smart speakers, and integrating calm design
principles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Time-to-Event Prediction for Chronic Kidney Disease Deterioration. (arXiv:2306.05779v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05779">
<div class="article-summary-box-inner">
<span><p>Deep-learning techniques, particularly the transformer model, have shown
great potential in enhancing the prediction performance of longitudinal health
records. While previous methods have mainly focused on fixed-time risk
prediction, time-to-event prediction (also known as survival analysis) is often
more appropriate for clinical scenarios. Here, we present a novel deep-learning
architecture we named STRAFE, a generalizable survival analysis
transformer-based architecture for electronic health records. The performance
of STRAFE was evaluated using a real-world claim dataset of over 130,000
individuals with stage 3 chronic kidney disease (CKD) and was found to
outperform other time-to-event prediction algorithms in predicting the exact
time of deterioration to stage 5. Additionally, STRAFE was found to outperform
binary outcome algorithms in predicting fixed-time risk, possibly due to its
ability to train on censored data. We show that STRAFE predictions can improve
the positive predictive value of high-risk patients by 3-fold, demonstrating
possible usage to improve targeting for intervention programs. Finally, we
suggest a novel visualization approach to predictions on a per-patient basis.
In conclusion, STRAFE is a cutting-edge time-to-event prediction algorithm that
has the potential to enhance risk predictions in large claims datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05783">
<div class="article-summary-box-inner">
<span><p>New Natural Langauge Process~(NLP) benchmarks are urgently needed to align
with the rapid development of large language models (LLMs). We present Xiezhi,
the most comprehensive evaluation suite designed to assess holistic domain
knowledge. Xiezhi comprises multiple-choice questions across 516 diverse
disciplines ranging from 13 different subjects with 220,000 questions and
accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k
questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results
indicate that LLMs exceed average performance of humans in science,
engineering, agronomy, medicine, and art, but fall short in economics,
jurisprudence, pedagogy, literature, history, and management. We anticipate
Xiezhi will help analyze important strengths and shortcomings of LLMs, and the
benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causality between Sentiment and Cryptocurrency Prices. (arXiv:2306.05803v1 [q-fin.CP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05803">
<div class="article-summary-box-inner">
<span><p>This study investigates the relationship between narratives conveyed through
microblogging platforms, namely Twitter, and the value of crypto assets. Our
study provides a unique technique to build narratives about cryptocurrency by
combining topic modelling of short texts with sentiment analysis. First, we
used an unsupervised machine learning algorithm to discover the latent topics
within the massive and noisy textual data from Twitter, and then we revealed
4-5 cryptocurrency-related narratives, including financial investment,
technological advancement related to crypto, financial and political
regulations, crypto assets, and media coverage. In a number of situations, we
noticed a strong link between our narratives and crypto prices. Our work
connects the most recent innovation in economics, Narrative Economics, to a new
area of study that combines topic modelling and sentiment analysis to relate
consumer behaviour to narratives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives. (arXiv:2306.05827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05827">
<div class="article-summary-box-inner">
<span><p>With the ever-increasing utilization of natural language processing (NLP), we
started to witness over the past few years a significant transformation in our
interaction with legal texts. This technology has advanced the analysis and
enhanced the understanding of complex legal terminology and contexts. The
development of recent large language models (LLMs), particularly ChatGPT, has
also introduced a revolutionary contribution to the way that legal texts can be
processed and comprehended. In this paper, we present our work on a
cooperative-legal question-answering LLM-based chatbot, where we developed a
set of legal questions about Palestinian cooperatives, associated with their
regulations and compared the auto-generated answers by the chatbot to their
correspondences that are designed by a legal expert. To evaluate the proposed
chatbot, we have used 50 queries generated by the legal expert and compared the
answers produced by the chart to their relevance judgments. Finding
demonstrated that an overall accuracy rate of 82% has been achieved when
answering the queries, while exhibiting an F1 score equivalent to 79%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05836">
<div class="article-summary-box-inner">
<span><p>Causal inference is one of the hallmarks of human intelligence. While the
field of CausalNLP has attracted much interest in the recent years, existing
causal inference datasets in NLP primarily rely on discovering causality from
empirical knowledge (e.g., commonsense knowledge). In this work, we propose the
first benchmark dataset to test the pure causal inference skills of large
language models (LLMs). Specifically, we formulate a novel task Corr2Cause,
which takes a set of correlational statements and determines the causal
relationship between the variables. We curate a large-scale dataset of more
than 400K samples, on which we evaluate seventeen existing LLMs. Through our
experiments, we identify a key shortcoming of LLMs in terms of their causal
inference skills, and show that these models achieve almost close to random
performance on the task. This shortcoming is somewhat mitigated when we try to
re-purpose LLMs for this skill via finetuning, but we find that these models
still fail to generalize -- they can only perform causal inference in
in-distribution settings when variable names and textual expressions used in
the queries are similar to those in the training set, but fail in
out-of-distribution settings generated by perturbing these queries. Corr2Cause
is a challenging task for LLMs, and would be helpful in guiding future research
on improving LLMs' pure reasoning skills and generalizability. Our data is at
https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at
https://github.com/causalNLP/corr2cause.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement. (arXiv:2306.05861v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05861">
<div class="article-summary-box-inner">
<span><p>Current speech enhancement (SE) research has largely neglected channel
attention and spatial attention, and encoder-decoder architecture-based
networks have not adequately considered how to provide efficient inputs to the
intermediate enhancement layer. To address these issues, this paper proposes a
time-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved
densely connected blocks, dual-path modules, convolution-augmented transformers
(conformers), channel attention, and spatial attention. Compared with previous
models, our proposed model has a more efficient encoder-decoder and can learn
comprehensive features. Experimental results on the VCTK+DEMAND dataset
demonstrate that our method outperforms existing techniques in SE performance.
Furthermore, the improved densely connected block and two dimensions attention
module developed in this work are highly adaptable and easily integrated into
existing networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?. (arXiv:2306.05871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05871">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing (NLP) have led to the
development of large language models (LLMs) such as ChatGPT. This paper
proposes a methodology for developing and evaluating ChatGPT detectors for
French text, with a focus on investigating their robustness on out-of-domain
data and against common attack schemes. The proposed method involves
translating an English dataset into French and training a classifier on the
translated data. Results show that the detectors can effectively detect
ChatGPT-generated text, with a degree of robustness against basic attack
techniques in in-domain settings. However, vulnerabilities are evident in
out-of-domain contexts, highlighting the challenge of detecting adversarial
text. The study emphasizes caution when applying in-domain testing results to a
wider variety of content. We provide our translated datasets and models as
open-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems. (arXiv:2306.05882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05882">
<div class="article-summary-box-inner">
<span><p>Machine Translation (MT) continues to make significant strides in quality and
is increasingly adopted on a larger scale. Consequently, analyses have been
redirected to more nuanced aspects, intricate phenomena, as well as potential
risks that may arise from the widespread use of MT tools. Along this line, this
paper offers a meticulous assessment of three commercial MT systems - Google
Translate, DeepL, and Modern MT - with a specific focus on gender translation
and bias. For three language pairs (English/Spanish, English/Italian, and
English/French), we scrutinize the behavior of such systems at several levels
of granularity and on a variety of naturally occurring gender phenomena in
translation. Our study takes stock of the current state of online MT tools, by
revealing significant discrepancies in the gender translation of the three
systems, with each system displaying varying degrees of bias despite their
overall translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention. (arXiv:2306.05887v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05887">
<div class="article-summary-box-inner">
<span><p>We present an efficient speech separation neural network, ARFDCN, which
combines dilated convolutions, multi-scale fusion (MSF), and channel attention
to overcome the limited receptive field of convolution-based networks and the
high computational cost of transformer-based networks. The suggested network
architecture is encoder-decoder based. By using dilated convolutions with
gradually increasing dilation value to learn local and global features and
fusing them at adjacent stages, the model can learn rich feature content.
Meanwhile, by adding channel attention modules to the network, the model can
extract channel weights, learn more important features, and thus improve its
expressive power and robustness. Experimental results indicate that the model
achieves a decent balance between performance and computational efficiency,
making it a promising alternative to current mainstream models for practical
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Can Learn Exceptions to Syntactic Rules. (arXiv:2306.05969v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05969">
<div class="article-summary-box-inner">
<span><p>Artificial neural networks can generalize productively to novel contexts. Can
they also learn exceptions to those productive rules? We explore this question
using the case of restrictions on English passivization (e.g., the fact that
"The vacation lasted five days" is grammatical, but "*Five days was lasted by
the vacation" is not). We collect human acceptability judgments for passive
sentences with a range of verbs, and show that the probability distribution
defined by GPT-2, a language model, matches the human judgments with high
correlation. We also show that the relative acceptability of a verb in the
active vs. passive voice is positively correlated with the relative frequency
of its occurrence in those voices. These results provide preliminary support
for the entrenchment hypothesis, according to which learners track and uses the
distributional properties of their input to learn negative exceptions to rules.
At the same time, this hypothesis fails to explain the magnitude of
unpassivizability demonstrated by certain individual verbs, suggesting that
other cues to exceptionality are available in the linguistic input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Labeling of German Chest X-Ray Radiology Reports using Deep Learning. (arXiv:2306.05997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05997">
<div class="article-summary-box-inner">
<span><p>Radiologists are in short supply globally, and deep learning models offer a
promising solution to address this shortage as part of clinical
decision-support systems. However, training such models often requires
expensive and time-consuming manual labeling of large datasets. Automatic label
extraction from radiology reports can reduce the time required to obtain
labeled datasets, but this task is challenging due to semantically similar
words and missing annotated data. In this work, we explore the potential of
weak supervision of a deep learning-based label prediction model, using a
rule-based labeler. We propose a deep learning-based CheXpert label prediction
model, pre-trained on reports labeled by a rule-based German CheXpert model and
fine-tuned on a small dataset of manually labeled reports. Our results
demonstrate the effectiveness of our approach, which significantly outperformed
the rule-based model on all three tasks. Our findings highlight the benefits of
employing deep learning-based models even in scenarios with sparse data and the
use of the rule-based labeler as a tool for weak supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine. (arXiv:2306.06029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06029">
<div class="article-summary-box-inner">
<span><p>Providing high quality explanations for AI predictions based on machine
learning is a challenging and complex task. To work well it requires, among
other factors: selecting a proper level of generality/specificity of the
explanation; considering assumptions about the familiarity of the explanation
beneficiary with the AI task under consideration; referring to specific
elements that have contributed to the decision; making use of additional
knowledge (e.g. expert evidence) which might not be part of the prediction
process; and providing evidence supporting negative hypothesis. Finally, the
system needs to formulate the explanation in a clearly interpretable, and
possibly convincing, way. Given these considerations, ANTIDOTE fosters an
integrated vision of explainable AI, where low-level characteristics of the
deep learning process are combined with higher level schemes proper of the
human argumentation capacity. ANTIDOTE will exploit cross-disciplinary
competences in deep learning and argumentation to support a broader and
innovative view of explainable AI, where the need for high-quality explanations
for clinical cases deliberation is critical. As a first result of the project,
we publish the Antidote CasiMedicos dataset to facilitate research on
explainable AI in general, and argumentation in the medical domain in
particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinGPT: Open-Source Financial Large Language Models. (arXiv:2306.06031v1 [q-fin.ST])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06031">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have shown the potential of revolutionizing
natural language processing tasks in diverse domains, sparking great interest
in finance. Accessing high-quality financial data is the first challenge for
financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken
advantage of their unique data accumulation, such privileged access calls for
an open-source alternative to democratize Internet-scale financial data.
</p>
<p>In this paper, we present an open-source large language model, FinGPT, for
the finance sector. Unlike proprietary models, FinGPT takes a data-centric
approach, providing researchers and practitioners with accessible and
transparent resources to develop their FinLLMs. We highlight the importance of
an automatic data curation pipeline and the lightweight low-rank adaptation
technique in building FinGPT. Furthermore, we showcase several potential
applications as stepping stones for users, such as robo-advising, algorithmic
trading, and low-code development. Through collaborative efforts within the
open-source AI4Finance community, FinGPT aims to stimulate innovation,
democratize FinLLMs, and unlock new opportunities in open finance. Two
associated code repos are \url{https://github.com/AI4Finance-Foundation/FinGPT}
and \url{https://github.com/AI4Finance-Foundation/FinNLP}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assisting Language Learners: Automated Trans-Lingual Definition Generation via Contrastive Prompt Learning. (arXiv:2306.06058v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06058">
<div class="article-summary-box-inner">
<span><p>The standard definition generation task requires to automatically produce
mono-lingual definitions (e.g., English definitions for English words), but
ignores that the generated definitions may also consist of unfamiliar words for
language learners. In this work, we propose a novel task of Trans-Lingual
Definition Generation (TLDG), which aims to generate definitions in another
language, i.e., the native speaker's language. Initially, we explore the
unsupervised manner of this task and build up a simple implementation of
fine-tuning the multi-lingual machine translation model. Then, we develop two
novel methods, Prompt Combination and Contrastive Prompt Learning, for further
enhancing the quality of the generation. Our methods are evaluated against the
baseline Pipeline method in both rich- and low-resource settings, and we
empirically establish its superiority in generating higher-quality
trans-lingual definitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind2Web: Towards a Generalist Agent for the Web. (arXiv:2306.06070v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06070">
<div class="article-summary-box-inner">
<span><p>We introduce Mind2Web, the first dataset for developing and evaluating
generalist agents for the web that can follow language instructions to complete
complex tasks on any website. Existing datasets for web agents either use
simulated websites or only cover a limited set of websites and tasks, thus not
suitable for generalist web agents. With over 2,000 open-ended tasks collected
from 137 websites spanning 31 domains and crowdsourced action sequences for the
tasks, Mind2Web provides three necessary ingredients for building generalist
web agents: 1) diverse domains, websites, and tasks, 2) use of real-world
websites instead of simulated and simplified ones, and 3) a broad spectrum of
user interaction patterns. Based on Mind2Web, we conduct an initial exploration
of using large language models (LLMs) for building generalist web agents. While
the raw HTML of real-world websites are often too large to be fed to LLMs, we
show that first filtering it with a small LM significantly improves the
effectiveness and efficiency of LLMs. Our solution demonstrates a decent level
of performance, even on websites or entire domains the model has never seen
before, but there is still a substantial room to improve towards truly
generalizable agents. We open-source our dataset, model implementation, and
trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further
research on building a generalist agent for the web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06077">
<div class="article-summary-box-inner">
<span><p>Language-vision models like CLIP have made significant progress in zero-shot
vision tasks, such as zero-shot image classification (ZSIC). However,
generating specific and expressive class descriptions remains a major
challenge. Existing approaches suffer from granularity and label ambiguity
issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel
method leveraging modern language models and semantic knowledge bases to
produce visually-grounded class descriptions. We demonstrate V-GLOSS's
effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets
including ImageNet and STL-10. In addition, we introduce a silver dataset with
class descriptions generated by V-GLOSS, and show its usefulness for vision
tasks. We make available our code and dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Fairness and Robustness in End-to-End Speech Recognition through unsupervised clustering. (arXiv:2306.06083v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06083">
<div class="article-summary-box-inner">
<span><p>The challenge of fairness arises when Automatic Speech Recognition (ASR)
systems do not perform equally well for all sub-groups of the population. In
the past few years there have been many improvements in overall speech
recognition quality, but without any particular focus on advancing Equality and
Equity for all user groups for whom systems do not perform well. ASR fairness
is therefore also a robustness issue. Meanwhile, data privacy also takes
priority in production systems. In this paper, we present a privacy preserving
approach to improve fairness and robustness of end-to-end ASR without using
metadata, zip codes, or even speaker or utterance embeddings directly in
training. We extract utterance level embeddings using a speaker ID model
trained on a public dataset, which we then use in an unsupervised fashion to
create acoustic clusters. We use cluster IDs instead of speaker utterance
embeddings as extra features during model training, which shows improvements
for all demographic groups and in particular for different accents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trapping LLM Hallucinations Using Tagged Context Prompts. (arXiv:2306.06085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06085">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models (LLMs), such as ChatGPT, have led to
highly sophisticated conversation agents. However, these models suffer from
"hallucinations," where the model generates false or fabricated information.
Addressing this challenge is crucial, particularly with AI-driven platforms
being adopted across various sectors. In this paper, we propose a novel method
to recognize and flag instances when LLMs perform outside their domain
knowledge, and ensuring users receive accurate information.
</p>
<p>We find that the use of context combined with embedded tags can successfully
combat hallucinations within generative language models. To do this, we
baseline hallucination frequency in no-context prompt-response pairs using
generated URLs as easily-tested indicators of fabricated data. We observed a
significant reduction in overall hallucination when context was supplied along
with question prompts for tested generative engines. Lastly, we evaluated how
placing tags within contexts impacted model responses and were able to
eliminate hallucinations in responses with 98.88% effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing Speech Processing Pipelines for Police Accountability. (arXiv:2306.06086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06086">
<div class="article-summary-box-inner">
<span><p>Police body-worn cameras have the potential to improve accountability and
transparency in policing. Yet in practice, they result in millions of hours of
footage that is never reviewed. We investigate the potential of large
pre-trained speech models for facilitating reviews, focusing on ASR and officer
speech detection in footage from traffic stops. Our proposed pipeline includes
training data alignment and filtering, fine-tuning with resource constraints,
and combining officer speech detection with ASR for a fully automated approach.
We find that (1) fine-tuning strongly improves ASR performance on officer
speech (WER=12-13%), (2) ASR on officer speech is much more accurate than on
community member speech (WER=43.55-49.07%), (3) domain-specific tasks like
officer speech detection and diarization remain challenging. Our work offers
practical applications for reviewing body camera footage and general guidance
for adapting pre-trained speech models to noisy multi-speaker domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.06094">
<div class="article-summary-box-inner">
<span><p>Recently, large language models (LLMs) have made significant advancements in
natural language understanding and generation. However, their potential in
computer vision remains largely unexplored. In this paper, we introduce a new,
exploratory approach that enables LLMs to process images using the Scalable
Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions
of SVG representations instead of raster images, we aim to bridge the gap
between the visual and textual modalities, allowing LLMs to directly understand
and manipulate images without the need for parameterized visual components. Our
method facilitates simple image classification, generation, and in-context
learning using only LLM capabilities. We demonstrate the promise of our
approach across discriminative and generative tasks, highlighting its (i)
robustness against distribution shift, (ii) substantial improvements achieved
by tapping into the in-context learning abilities of LLMs, and (iii) image
understanding and generation capabilities with human guidance. Our code, data,
and models can be found here https://github.com/mu-cai/svg-llm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Semiotics. (arXiv:2008.10522v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.10522">
<div class="article-summary-box-inner">
<span><p>Recognizing a basic difference between the semiotics of humans and machines
presents a possibility to overcome the shortcomings of current speech assistive
devices. For the machine, the meaning of a (human) utterance is defined by its
own scope of actions. Machines, thus, do not need to understand the
conventional meaning of an utterance. Rather, they draw conversational
implicatures in the sense of (neo-)Gricean pragmatics. For speech assistive
devices, the learning of machine-specific meanings of human utterances, i.e.
the fossilization of conversational implicatures into conventionalized ones by
trial and error through lexicalization appears to be sufficient. Using the
quite trivial example of a cognitive heating device, we show that - based on
dynamic semantics - this process can be formalized as the reinforcement
learning of utterance-meaning pairs (UMP).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08079">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) has seen significant progress in recent years,
with numerous state-of-the-art (SOTA) models achieving high performance.
However, very few studies have focused on the generation of entities' context.
In this paper, we introduce CONTEXT-NER, a task that aims to generate the
relevant context for entities in a sentence, where the context is a phrase
describing the entity but not necessarily present in the sentence. To
facilitate research in this task, we also present the EDGAR10-Q dataset, which
consists of annual and quarterly reports from the top 1500 publicly traded
companies. The dataset is the largest of its kind, containing 1M sentences,
2.8M entities, and an average of 35 tokens per sentence, making it a
challenging dataset. We propose a baseline approach that combines a phrase
generation algorithm with inferencing using a 220M language model, achieving a
ROUGE-L score of 27% on the test split. Additionally, we perform a one-shot
inference with ChatGPT, which obtains a 30% ROUGE-L, highlighting the
difficulty of the dataset. We also evaluate models such as T5 and BART, which
achieve a maximum ROUGE-L of 49% after supervised finetuning on EDGAR10-Q. We
also find that T5-large, when pre-finetuned on EDGAR10-Q, achieve SOTA results
on downstream finance tasks such as Headline, FPB, and FiQA SA, outperforming
vanilla version by 10.81 points. To our surprise, this 66x smaller
pre-finetuned model also surpasses the finance-specific LLM BloombergGPT-50B by
15 points. We hope that our dataset and generated artifacts will encourage
further research in this direction, leading to the development of more
sophisticated language models for financial text analysis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving Deeper into Cross-lingual Visual Question Answering. (arXiv:2202.07630v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07630">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) is one of the crucial vision-and-language
tasks. Yet, existing VQA research has mostly focused on the English language,
due to a lack of suitable evaluation resources. Previous work on cross-lingual
VQA has reported poor zero-shot transfer performance of current multilingual
multimodal Transformers with large gaps to monolingual performance, without any
deeper analysis. In this work, we delve deeper into the different aspects of
cross-lingual VQA, aiming to understand the impact of 1) modeling methods and
choices, including architecture, inductive bias, fine-tuning; 2) learning
biases: including question types and modality biases in cross-lingual setups.
The key results of our analysis are: 1) We show that simple modifications to
the standard training setup can substantially reduce the transfer gap to
monolingual English performance, yielding +10 accuracy points over existing
methods. 2) We analyze cross-lingual VQA across different question types of
varying complexity for different multilingual multimodal Transformers, and
identify question types that are the most difficult to improve on. 3) We
provide an analysis of modality biases present in training data and models,
revealing why zero-shot performance gaps remain for certain question types and
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arithmetic-Based Pretraining -- Improving Numeracy of Pretrained Language Models. (arXiv:2205.06733v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06733">
<div class="article-summary-box-inner">
<span><p>State-of-the-art pretrained language models tend to perform below their
capabilities when applied out-of-the-box on tasks that require understanding
and working with numbers. Recent work suggests two main reasons for this: (1)
popular tokenisation algorithms have limited expressiveness for numbers, and
(2) common pretraining objectives do not target numeracy. Approaches that
address these shortcomings usually require architectural changes or pretraining
from scratch. In this paper, we propose a new extended pretraining approach
called Arithmetic-Based Pretraining that jointly addresses both in one extended
pretraining step without requiring architectural changes or pretraining from
scratch. Arithmetic-Based Pretraining combines contrastive learning to improve
the number representation, and a novel extended pretraining objective called
Inferable Number Prediction Task to improve numeracy. Our experiments show the
effectiveness of Arithmetic-Based Pretraining in three different tasks that
require improved numeracy, i.e., reading comprehension in the DROP dataset,
inference-on-tables in the InfoTabs dataset, and table-to-text generation in
the WikiBio and SciGen datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08657">
<div class="article-summary-box-inner">
<span><p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars. (arXiv:2212.09140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09140">
<div class="article-summary-box-inner">
<span><p>We study grammar induction with mildly context-sensitive grammars for
unsupervised discontinuous parsing. Using the probabilistic linear context-free
rewriting system (LCFRS) formalism, our approach fixes the rule structure in
advance and focuses on parameter learning with maximum likelihood. To reduce
the computational complexity of both parsing and parameter estimation, we
restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two)
and further discard rules that require O(n^6) time to parse, reducing inference
to O(n^5). We find that using a large number of nonterminals is beneficial and
thus make use of tensor decomposition-based rank-space dynamic programming with
an embedding-based parameterization of rule probabilities to scale up the
number of nonterminals. Experiments on German and Dutch show that our approach
is able to induce linguistically meaningful trees with continuous and
discontinuous structures
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10449">
<div class="article-summary-box-inner">
<span><p>In long document controllable summarization, where labeled data is scarce,
pretrained models struggle to adapt to the task and effectively respond to user
queries. In this paper, we introduce Socratic pretraining, a question-driven,
unsupervised pretraining objective specifically designed to improve
controllability in summarization tasks. By training a model to generate and
answer relevant questions in a given context, Socratic pretraining enables the
model to more effectively adhere to user-provided queries and identify relevant
content to be summarized. We demonstrate the effectiveness of this approach
through extensive experimentation on two summarization domains, short stories
and dialogue, and multiple control strategies: keywords, questions, and factoid
QA pairs. Our pretraining method relies only on unlabeled documents and a
question generation system and outperforms pre-finetuning approaches that use
additional supervised data. Furthermore, our results show that Socratic
pretraining cuts task-specific labeled data requirements in half, is more
faithful to user-provided queries, and achieves state-of-the-art performance on
QMSum and SQuALITY.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Domain Adaptation of Semantic Parsers. (arXiv:2212.10520v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10520">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems often assist users with personal or
confidential matters. For this reason, the developers of such a system are
generally prohibited from observing actual usage. So how can they know where
the system is failing and needs more training data or new functionality? In
this work, we study ways in which realistic user utterances can be generated
synthetically, to help increase the linguistic and functional coverage of the
system, without compromising the privacy of actual users. To this end, we
propose a two-stage Differentially Private (DP) generation method which first
generates latent semantic parses, and then generates utterances based on the
parses. Our proposed approach improves MAUVE by 2.5$\times$ and parse tree
function type overlap by 1.3$\times$ relative to current approaches for private
synthetic data generation, improving both on fluency and semantic coverage. We
further validate our approach on a realistic domain adaptation task of adding
new functionality from private user data to a semantic parser, and show overall
gains of 8.5% points in accuracy with the new feature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.11660">
<div class="article-summary-box-inner">
<span><p>As the size of the pre-trained language model (PLM) continues to increase,
numerous parameter-efficient transfer learning methods have been proposed
recently to compensate for the tremendous cost of fine-tuning. Despite the
impressive results achieved by large pre-trained language models (PLMs) and
various parameter-efficient transfer learning (PETL) methods on sundry
benchmarks, it remains unclear if they can handle inputs that have been
distributionally shifted effectively. In this study, we systematically explore
how the ability to detect out-of-distribution (OOD) changes as the size of the
PLM grows or the transfer methods are altered. Specifically, we evaluated
various PETL techniques, including fine-tuning, Adapter, LoRA, and
prefix-tuning, on three different intention classification tasks, each
utilizing various language models with different scales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04091">
<div class="article-summary-box-inner">
<span><p>While Artificial Intelligence (AI) models have achieved human or even
superhuman performance in narrowly defined applications, they still struggle to
show signs of broader and more flexible intelligence. The Abstraction and
Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how
close AI systems are to human-like cognitive abilities. Most current approaches
rely on carefully handcrafted domain-specific languages (DSLs), which are used
to brute-force solutions to the tasks present in ARC. In this work, we propose
a general framework for solving ARC based on natural language descriptions of
the tasks. While not yet beating state-of-the-art DSL models on ARC, we
demonstrate the immense potential of our approach hinted at by the ability to
solve previously unsolved tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.06245">
<div class="article-summary-box-inner">
<span><p>As large dialogue models become commonplace in practice, the problems
surrounding high compute requirements for training, inference and larger memory
footprint still persists. In this work, we present AUTODIAL, a multi-task
dialogue model that addresses the challenges of deploying dialogue model.
AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act
prediction, domain prediction, intent prediction, and dialogue state tracking.
Using classification decoders over generative decoders allows AUTODIAL to
significantly reduce memory footprint and achieve faster inference times
compared to existing generative approach namely SimpleTOD. We demonstrate that
AUTODIAL provides 3-6x speedups during inference while having 11x fewer
parameters on three dialogue tasks compared to SimpleTOD. Our results show that
extending current dialogue models to have parallel decoders can be a viable
alternative for deploying them in resource-constrained environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13835">
<div class="article-summary-box-inner">
<span><p>Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01555">
<div class="article-summary-box-inner">
<span><p>Scaling language models have revolutionized widespread NLP tasks, yet little
comprehensively explored few-shot relation extraction with large language
models. In this paper, we investigate principal methodologies, in-context
learning and data generation, for few-shot relation extraction via GPT-3.5
through exhaustive experiments. To enhance few-shot performance, we further
propose task-related instructions and schema-constrained data generation. We
observe that in-context learning can achieve performance on par with previous
prompt learning approaches, and data generation with the large language model
can boost previous solutions to obtain new state-of-the-art few-shot results on
four widely-studied relation extraction datasets. We hope our work can inspire
future research for the capabilities of large language models in few-shot
relation extraction. Code is available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Multi-bit Natural Language Watermarking through Invariant Features. (arXiv:2305.01904v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.01904">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed a proliferation of valuable original natural
language contents found in subscription-based media outlets, web novel
platforms, and outputs of large language models. However, these contents are
susceptible to illegal piracy and potential misuse without proper security
measures. This calls for a secure watermarking system to guarantee copyright
protection through leakage tracing or ownership identification. To effectively
combat piracy and protect copyrights, a multi-bit watermarking framework should
be able to embed adequate bits of information and extract the watermarks in a
robust manner despite possible corruption. In this work, we explore ways to
advance both payload and robustness by following a well-known proposition from
image watermarking and identify features in natural language that are invariant
to minor corruption. Through a systematic analysis of the possible sources of
errors, we further propose a corruption-resistant infill model. Our full method
improves upon the previous work on robustness by +16.8% point on average on
four datasets, three corruption types, and two corruption ratios. Code
available at https://github.com/bangawayoo/nlp-watermarking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.03827">
<div class="article-summary-box-inner">
<span><p>Jointly extracting entity pairs and their relations is challenging when
working on distantly-supervised data with ambiguous or noisy labels. To
mitigate such impact, we propose uncertainty-aware bootstrap learning, which is
motivated by the intuition that the higher uncertainty of an instance, the more
likely the model confidence is inconsistent with the ground truths.
Specifically, we first explore instance-level data uncertainty to create an
initial high-confident examples. Such subset serves as filtering noisy
instances and facilitating the model to converge fast at the early stage.
During bootstrap learning, we propose self-ensembling as a regularizer to
alleviate inter-model uncertainty produced by noisy labels. We further define
probability variance of joint tagging probabilities to estimate inner-model
parametric uncertainty, which is used to select and build up new reliable
training instances for the next iteration. Experimental results on two large
datasets reveal that our approach outperforms existing strong baselines and
related methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.04835">
<div class="article-summary-box-inner">
<span><p>Compositional generalization--understanding unseen combinations of seen
primitives--is an essential reasoning capability in human intelligence. The AI
community mainly studies this capability by fine-tuning neural networks on lots
of training samples, while it is still unclear whether and how in-context
learning--the prevailing few-shot paradigm based on large language
models--exhibits compositional generalization. In this paper, we present CoFe,
a test suite to investigate in-context compositional generalization. We find
that the compositional generalization performance can be easily affected by the
selection of in-context examples, thus raising the research question what the
key factors are to make good in-context examples for compositional
generalization. We study three potential factors: similarity, diversity and
complexity. Our systematic experiments indicate that in-context examples should
be structurally similar to the test case, diverse from each other, and
individually simple. Furthermore, two strong limitations are observed:
in-context compositional generalization on fictional words is much weaker than
that on commonly used ones; it is still critical that the in-context examples
should cover required linguistic structures, even though the backbone model has
been pre-trained on large corpus. We hope our analysis would facilitate the
understanding and utilization of in-context learning paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11255">
<div class="article-summary-box-inner">
<span><p>While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is open at
https://github.com/scofield7419/THOR-ISA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.11579">
<div class="article-summary-box-inner">
<span><p>Recently, speech-text pre-training methods have shown remarkable success in
many speech and natural language processing tasks. However, most previous
pre-trained models are usually tailored for one or two specific tasks, but fail
to conquer a wide range of speech-text tasks. In addition, existing speech-text
pre-training methods fail to explore the contextual information within a
dialogue to enrich utterance representations. In this paper, we propose
Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT
cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog
pre-training model. Concretely, to consider the temporality of speech modality,
we design a novel temporal position prediction task to capture the speech-text
alignment. This pre-training task aims to predict the start and end time of
each textual word in the corresponding speech waveform. In addition, to learn
the characteristics of spoken dialogs, we generalize a response selection task
from textual dialog pre-training to speech-text dialog pre-training scenarios.
Experimental results on four different downstream speech-text tasks demonstrate
the superiority of SPECTRA in learning speech-text alignment and multi-turn
dialog context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification. (arXiv:2305.15182v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15182">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification as the labels form a complex hierarchical structure.
Existing dual-encoder methods in HTC achieve weak performance gains with huge
memory overheads and their structure encoders heavily rely on domain knowledge.
Under such observation, we tend to investigate the feasibility of a
memory-friendly model with strong generalization capability that could boost
the performance of HTC without prior statistics or label semantics. In this
paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance
the text representations with only syntactic information of the label
hierarchy. Specifically, we convert the label hierarchy into an unweighted tree
structure, termed coding tree, with the guidance of structural entropy. Then we
design a structure encoder to incorporate hierarchy-aware information in the
coding tree into text representations. Besides the text encoder, HiTIN only
contains a few multi-layer perceptions and linear transformations, which
greatly saves memory. We conduct experiments on three commonly used datasets
and the results demonstrate that HiTIN could achieve better test performance
and less memory consumption than state-of-the-art (SOTA) methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning. (arXiv:2305.18170v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18170">
<div class="article-summary-box-inner">
<span><p>Chain-of-thought (CoT) prompting with large language models has proven
effective in numerous natural language processing tasks, but designing prompts
that generalize well to diverse problem types can be challenging, especially in
the context of math word problem (MWP) solving. Additionally, it is common to
have a large amount of training data that have a better diversity coverage but
CoT annotations are not available, which limits the use of supervised learning
techniques. To address these issues, we investigate two approaches to leverage
the training data in a few-shot prompting scenario: dynamic program prompting
and program distillation. Our approach is largely inspired by Gao et al.,
(2022), where they proposed to replace the CoT with the programs as the
intermediate reasoning step. Such a prompting strategy allows us to accurately
verify the answer correctness through program execution in MWP solving. Our
dynamic program prompting involves annotating the training data by sampling
correct programs from a large language model, while program distillation
involves adapting a smaller model to the program-annotated training data. Our
experiments on three standard MWP datasets demonstrate the effectiveness of
these approaches, yielding significant improvements over previous baselines for
prompting and fine-tuning. Our results suggest that leveraging a large amount
of training data can improve the generalization ability of prompts and boost
the performance of fine-tuned small models in MWP solving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements. (arXiv:2306.01985v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01985">
<div class="article-summary-box-inner">
<span><p>Warning: This paper contains content that may be offensive or upsetting.
Understanding the harms and offensiveness of statements requires reasoning
about the social and situational context in which statements are made. For
example, the utterance "your English is very good" may implicitly signal an
insult when uttered by a white man to a non-white colleague, but uttered by an
ESL teacher to their student would be interpreted as a genuine compliment. Such
contextual factors have been largely ignored by previous approaches to toxic
language detection. We introduce COBRA frames, the first context-aware
formalism for explaining the intents, reactions, and harms of offensive or
biased statements grounded in their social and situational context. We create
COBRACORPUS, a dataset of 33k potentially offensive statements paired with
machine-generated contexts and free-text explanations of offensiveness, implied
biases, speaker intents, and listener reactions. To study the contextual
dynamics of offensiveness, we train models to generate COBRA explanations, with
and without access to the context. We find that explanations by
context-agnostic models are significantly worse than by context-aware ones,
especially in situations where the context inverts the statement's
offensiveness (29% accuracy drop). Our work highlights the importance and
feasibility of contextualized NLP by modeling social factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels. (arXiv:2306.02693v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02693">
<div class="article-summary-box-inner">
<span><p>Utilizing language models (LMs) without internal access is becoming an
attractive paradigm in the field of NLP as many cutting-edge LMs are released
through APIs and boast a massive scale. The de-facto method in this type of
black-box scenario is known as prompting, which has shown progressive
performance enhancements in situations where data labels are scarce or
unavailable. Despite their efficacy, they still fall short in comparison to
fully supervised counterparts and are generally brittle to slight
modifications. In this paper, we propose Clustering-enhanced Linear
Discriminative Analysis, a novel approach that improves the text classification
accuracy with a very weak-supervision signal (i.e., name of the labels). Our
framework draws a precise decision boundary without accessing weights or
gradients of the LM model or data labels. The core ideas of CELDA are twofold:
(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and
(2) training a lightweight and robust model on the top of LM, which learns an
accurate decision boundary from an extracted noisy dataset. Throughout in-depth
investigations on various datasets, we demonstrated that CELDA reaches new
state-of-the-art in weakly-supervised text classification and narrows the gap
with a fully-supervised model. Additionally, our proposed methodology can be
applied universally to any LM and has the potential to scale to larger models,
making it a more viable option for utilizing large LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03984">
<div class="article-summary-box-inner">
<span><p>Measurement of interaction quality is a critical task for the improvement of
spoken dialog systems. Existing approaches to dialog quality estimation either
focus on evaluating the quality of individual turns, or collect dialog-level
quality measurements from end users immediately following an interaction. In
contrast to these approaches, we introduce a new dialog-level annotation
workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate
the quality of dialogs as a whole, and also label dialogs for attributes such
as goal completion and user sentiment. In this contribution, we show that: (i)
while dialog quality cannot be completely decomposed into dialog-level
attributes, there is a strong relationship between some objective dialog
attributes and judgments of dialog quality; (ii) for the task of dialog-level
quality estimation, a supervised model trained on dialog-level annotations
outperforms methods based purely on aggregating turn-level features; and (iii)
the proposed evaluation model shows better domain generalization ability
compared to the baselines. On the basis of these results, we argue that having
high-quality human-annotated data is an important component of evaluating
interaction quality for large industrial-scale voice assistant platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.04634">
<div class="article-summary-box-inner">
<span><p>As LLMs become commonplace, machine-generated text has the potential to flood
the internet with spam, social media bots, and valueless content. Watermarking
is a simple and effective strategy for mitigating such harms by enabling the
detection and documentation of LLM-generated text. Yet a crucial question
remains: How reliable is watermarking in realistic settings in the wild? There,
watermarked text may be modified to suit a user's needs, or entirely rewritten
to avoid detection.
</p>
<p>We study the robustness of watermarked text after it is re-written by humans,
paraphrased by a non-watermarked LLM, or mixed into a longer hand-written
document. We find that watermarks remain detectable even after human and
machine paraphrasing. While these attacks dilute the strength of the watermark,
paraphrases are statistically likely to leak n-grams or even longer fragments
of the original text, resulting in high-confidence detections when enough
tokens are observed. For example, after strong human paraphrasing the watermark
is detectable after observing 800 tokens on average, when setting a 1e-5 false
positive rate. We also consider a range of new detection schemes that are
sensitive to short spans of watermarked text embedded inside a large document,
and we compare the robustness of watermarking to other kinds of detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05176">
<div class="article-summary-box-inner">
<span><p>Owing to the impressive dot-product attention, the Transformers have been the
dominant architectures in various natural language processing (NLP) tasks.
Recently, the Receptance Weighted Key Value (RWKV) architecture follows a
non-transformer architecture to eliminate the drawbacks of dot-product
attention, where memory and computational complexity exhibits quadratic scaling
with sequence length. Although RWKV has exploited a linearly tensor-product
attention mechanism and achieved parallelized computations by deploying the
time-sequential mode, it fails to capture long-range dependencies because of
its limitation on looking back at previous information, compared with full
information obtained by direct interactions in the standard transformer.
Therefore, the paper devises the Retrospected Receptance Weighted Key Value
(RRWKV) architecture via incorporating the retrospecting ability into the RWKV
to effectively absorb information, which maintains memory and computational
efficiency as well.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-06-12 23:10:57.193616113 UTC">2023-06-12 23:10:57 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>