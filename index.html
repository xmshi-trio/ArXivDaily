<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2024-01-24T01:30:00Z">01-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12246">
<div class="article-summary-box-inner">
<span><p>In this study, we introduce Orion-14B, a collection of multilingual large
language models with 14 billion parameters. We utilize a data scheduling
approach to train a foundational model on a diverse corpus of 2.5 trillion
tokens, sourced from texts in English, Chinese, Japanese, Korean, and other
languages. Additionally, we fine-tuned a series of models tailored for
conversational applications and other specific use cases. Our evaluation
results demonstrate that Orion-14B achieves state-of-the-art performance across
a broad spectrum of tasks. We make the Orion-14B model family and its
associated code publicly accessible https://github.com/OrionStarAI/Orion,
aiming to inspire future research and practical applications in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Ethics of Interaction: Mitigating Security Threats in LLMs. (arXiv:2401.12273v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12273">
<div class="article-summary-box-inner">
<span><p>This paper comprehensively explores the ethical challenges arising from
security threats to Language Learning Models (LLMs). These intricate digital
repositories are increasingly integrated into our daily lives, making them
prime targets for attacks that can compromise their training data and the
confidentiality of their data sources. The paper delves into the nuanced
ethical repercussions of such security threats on society and individual
privacy. We scrutinize five major threats: prompt injection, jailbreaking,
Personal Identifiable Information (PII) exposure, sexually explicit content,
and hate based content, going beyond mere identification to assess their
critical ethical consequences and the urgency they create for robust defensive
strategies. The escalating reliance on LLMs underscores the crucial need for
ensuring these systems operate within the bounds of ethical norms, particularly
as their misuse can lead to significant societal and individual harm. We
propose conceptualizing and developing an evaluative tool tailored for LLMs,
which would serve a dual purpose, guiding developers and designers in
preemptive fortification of backend systems and scrutinizing the ethical
dimensions of LLM chatbot responses during the testing phase. By comparing LLM
responses with those expected from humans in a moral context, we aim to discern
the degree to which AI behaviors align with the ethical values held by a
broader society. Ultimately, this paper not only underscores the ethical
troubles presented by LLMs, it also highlights a path toward cultivating trust
in these systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12292">
<div class="article-summary-box-inner">
<span><p>Truthfulness is paramount for large language models (LLMs) as they are
increasingly deployed in real-world applications. However, existing LLMs still
struggle with generating truthful answers and content, as evidenced by their
modest performance on benchmarks like TruthfulQA. To address this issue, we
propose GRAdual self-truTHifying (GRATH), a novel post-processing method to
enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to
generate corresponding answers and adaptively optimizes the model via direct
preference optimization (DPO). Note that during this process, GRATH learns
truthfulness in a self-supervised manner without requiring annotated answers.
In particular, GRATH first generates pairwise truthfulness training data by
prompting the LLM itself, with each pair containing a question and its correct
and incorrect answers. The model is then fine-tuned using DPO to learn from the
difference between answer pairs. Subsequently, GRATH iteratively refines the
truthfulness data and optimizes the model, leading to a gradual improvement in
model truthfulness. Empirically, we evaluate GRATH using different 7B-LLMs and
compare with LLMs with similar or even larger sizes on benchmark datasets. Our
results show that GRATH effectively improves LLMs' truthfulness without
compromising other core capabilities. Notably, GRATH achieves state-of-the-art
performance on TruthfulQA, with MC1 accuracy as 54.71% and MC2 accuracy as
69.10%, which even surpass those on larger-scale models, such as
Llama2-Chat-70B, by 23.62% and 24.18%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data. (arXiv:2401.12295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12295">
<div class="article-summary-box-inner">
<span><p>The field of machine learning has recently made significant progress in
reducing the requirements for labelled training data when building new models.
These `cheaper' learning techniques hold significant potential for the social
sciences, where development of large labelled training datasets is often a
significant practical impediment to the use of machine learning for analytical
tasks. In this article we review three `cheap' techniques that have developed
in recent years: weak supervision, transfer learning and prompt engineering.
For the latter, we also review the particular case of zero-shot prompting of
large language models. For each technique we provide a guide of how it works
and demonstrate its application across six different realistic social science
applications (two different tasks paired with three different dataset makeups).
We show good performance for all techniques, and in particular we demonstrate
how prompting of large language models can achieve high accuracy at very low
cost. Our results are accompanied by a code repository to make it easy for
others to duplicate our work and use it in their own research. Overall, our
article is intended to stimulate further uptake of these techniques in the
social sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. (arXiv:2401.12326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12326">
<div class="article-summary-box-inner">
<span><p>SemEval-2024 Task 8 introduces the challenge of identifying machine-generated
texts from diverse Large Language Models (LLMs) in various languages and
domains. The task comprises three subtasks: binary classification in
monolingual and multilingual (Subtask A), multi-class classification (Subtask
B), and mixed text detection (Subtask C). This paper focuses on Subtask A &amp; B.
Each subtask is supported by three datasets for training, development, and
testing. To tackle this task, two methods: 1) using traditional machine
learning (ML) with natural language preprocessing (NLP) for feature extraction,
and 2) fine-tuning LLMs for text classification. The results show that
transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in
effectiveness, with majority voting being particularly effective in
multilingual contexts for identifying machine-generated texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subgraph Extraction-based Feedback-guided Iterative Scheduling for HLS. (arXiv:2401.12343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12343">
<div class="article-summary-box-inner">
<span><p>This paper proposes ISDC, a novel feedback-guided iterative system of
difference constraints (SDC) scheduling algorithm for high-level synthesis
(HLS). ISDC leverages subgraph extraction-based low-level feedback from
downstream tools like logic synthesizers to iteratively refine HLS scheduling.
Technical innovations include: (1) An enhanced SDC formulation that effectively
integrates low-level feedback into the linear-programming (LP) problem; (2) A
fanout and window-based subgraph extraction mechanism driving the feedback
cycle; (3) A no-human-in-loop ISDC flow compatible with a wide range of
downstream tools and process design kits (PDKs). Evaluation shows that ISDC
reduces register usage by 28.5% against an industrial-strength open-source HLS
tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of an NLP-driven computer-based test guide for visually impaired students. (arXiv:2401.12375v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12375">
<div class="article-summary-box-inner">
<span><p>In recent years, advancements in Natural Language Processing (NLP) techniques
have revolutionized the field of accessibility and exclusivity of testing,
particularly for visually impaired students (VIS). CBT has shown in years back
its relevance in terms of administering exams electronically, making the test
process easier, providing quicker and more accurate results, and offering
greater flexibility and accessibility for candidates. Yet, its relevance was
not felt by the visually impaired students as they cannot access printed
documents. Hence, in this paper, we present an NLP-driven Computer-Based Test
guide for visually impaired students. It employs a speech technology
pre-trained methods to provide real-time assistance and support to visually
impaired students. The system utilizes NLP technologies to convert the
text-based questions and the associated options in a machine-readable format.
Subsequently, the speech technology pre-trained model processes the converted
text enabling the VIS to comprehend and analyze the content. Furthermore, we
validated that this pre-trained model is not perverse by testing for accuracy
using sample audio datasets labels (A, B, C, D, E, F, G) to compare with the
voice recordings obtained from 20 VIS which is been predicted by the system to
attain values for precision, recall, and F1-scores. These metrics are used to
assess the performance of the pre-trained model and have indicated that it is
proficient enough to give its better performance to the evaluated system. The
methodology adopted for this system is Object Oriented Analysis and Design
Methodology (OOADM) where Objects are discussed and built by modeling
real-world instances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Longitudinal Sentiment Classification of Reddit Posts. (arXiv:2401.12382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12382">
<div class="article-summary-box-inner">
<span><p>We report results of a longitudinal sentiment classification of Reddit posts
written by students of four major Canadian universities. We work with the texts
of the posts, concentrating on the years 2020-2023. By finely tuning a
sentiment threshold to a range of [-0.075,0.075], we successfully built
classifiers proficient in categorizing post sentiments into positive and
negative categories. Noticeably, our sentiment classification results are
consistent across the four university data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing In-context Learning via Linear Probe Calibration. (arXiv:2401.12406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12406">
<div class="article-summary-box-inner">
<span><p>In-context learning (ICL) is a new paradigm for natural language processing
that utilizes Generative Pre-trained Transformer (GPT)-like models. This
approach uses prompts that include in-context demonstrations to generate the
corresponding output for a new query input. However, applying ICL in real cases
does not scale with the number of samples, and lacks robustness to different
prompt templates and demonstration permutations. In this paper, we first show
that GPT-like models using ICL result in unreliable predictions based on a new
metric based on Shannon entropy. Then, to solve this problem, we propose a new
technique called the Linear Probe Calibration (LinC), a method that calibrates
the model's output probabilities, resulting in reliable predictions and
improved performance, while requiring only minimal additional samples (as few
as five labeled data samples). LinC significantly enhances the ICL test
performance of GPT models on various benchmark datasets, with an average
improvement of up to 21%, and up to a 50% improvement in some cases, and
significantly boosts the performance of PEFT methods, especially in the low
resource regime. Moreover, LinC achieves lower expected calibration error, and
is highly robust to varying label proportions, prompt templates, and
demonstration permutations. Our code is available at
\url{https://github.com/mominabbass/LinC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12413">
<div class="article-summary-box-inner">
<span><p>Zero-shot translation is an open problem, aiming to translate between
language pairs unseen during training in Multilingual Machine Translation
(MMT). A common, albeit resource-consuming, solution is to mine as many
translation directions as possible to add to the parallel corpus. In this
paper, we show that the zero-shot capability of an English-centric model can be
easily enhanced by fine-tuning with a very small amount of multi-parallel data.
For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English
overall improvements (870 directions) can be achieved by using only 100
multi-parallel samples, meanwhile preserving capability in English-centric
directions. We further study the size effect of fine-tuning data and its
transfer capabilities. Surprisingly, our empirical analysis shows that
comparable overall improvements can be achieved even through fine-tuning in a
small, randomly sampled direction set (10\%). Also, the resulting non-English
performance is quite close to the upper bound (complete translation). Due to
its high efficiency and practicality, we encourage the community 1) to consider
the use of the fine-tuning method as a strong baseline for zero-shot
translation and 2) to construct more comprehensive and high-quality
multi-parallel data to cover real-world demand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12425">
<div class="article-summary-box-inner">
<span><p>Vision-language models (VLMs) excel in zero-shot recognition but exhibit
drastically imbalanced performance across visual concepts. For example, CLIP,
despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields
$&lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because
these concepts are under-represented in VLMs' imbalanced pretraining data. Yet,
assessing this imbalance is challenging as it is non-trivial to calculate the
frequency of specific concepts within VLMs' large-scale pretraining data. Our
work makes the first attempt to measure the concept frequency by analyzing
pretraining texts. We use off-the-shelf language models to help count relevant
texts that contain synonyms of the given concepts and resolve linguistic
ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit
long-tailed concept distributions, which strongly correlate with per-class
accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and
text-to-image generators, also struggle with the rare concepts identified by
our method. To mitigate VLMs' imbalanced performance in zero-shot recognition,
we propose REtrieval-Augmented Learning REAL. First, instead of prompting VLMs
using the original class names, REAL uses their most frequent synonyms found in
VLMs' pretraining texts. This already outperforms human-engineered and
LLM-generated prompts over nine benchmark datasets, likely because VLMs have
seen more images associated with the frequently used synonyms. Second, REAL
uses all the concept synonyms to retrieve a small, class-balanced set of
pretraining data to train a robust classifier. REAL surpasses the recent
retrieval-augmented solution REACT, using 400x less storage and 10,000x less
training time!
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators. (arXiv:2401.12428v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12428">
<div class="article-summary-box-inner">
<span><p>In recent years, various computing-in-memory (CIM) processors have been
presented, showing superior performance over traditional architectures. To
unleash the potential of various CIM architectures, such as device precision,
crossbar size, and crossbar number, it is necessary to develop compilation
tools that are fully aware of the CIM architectural details and implementation
diversity. However, due to the lack of architectural support in current popular
open-source compiling stacks, existing CIM designs either manually deploy
networks or build their own compilers, which is time-consuming and
labor-intensive. Although some works expose the specific CIM device programming
interfaces to compilers, they are often bound to a fixed CIM architecture,
lacking the flexibility to support the CIM architectures with different
computing granularity. On the other hand, existing compilation works usually
consider the scheduling of limited operation types (such as crossbar-bound
matrix-vector multiplication). Unlike conventional processors, CIM accelerators
are featured by their diverse architecture, circuit, and device, which cannot
be simply abstracted by a single level if we seek to fully explore the
advantages brought by CIM. Therefore, we propose CIM-MLC, a universal
multi-level compilation framework for general CIM architectures. We first
establish a general hardware abstraction for CIM architectures and computing
modes to represent various CIM accelerators. Based on the proposed abstraction,
CIM-MLC can compile tasks onto a wide range of CIM accelerators having
different devices, architectures, and programming interfaces. More importantly,
compared with existing compilation work, CIM-MLC can explore the mapping and
scheduling strategies across multiple architectural tiers, which form a
tractable yet effective design space, to achieve better scheduling and
instruction generation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Adversarial Training against Textual Adversarial Attacks. (arXiv:2401.12461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12461">
<div class="article-summary-box-inner">
<span><p>Many adversarial defense methods have been proposed to enhance the
adversarial robustness of natural language processing models. However, most of
them introduce additional pre-set linguistic knowledge and assume that the
synonym candidates used by attackers are accessible, which is an ideal
assumption. We delve into adversarial training in the embedding space and
propose a Fast Adversarial Training (FAT) method to improve the model
robustness in the synonym-unaware scenario from the perspective of single-step
perturbation generation and perturbation initialization. Based on the
observation that the adversarial perturbations crafted by single-step and
multi-step gradient ascent are similar, FAT uses single-step gradient ascent to
craft adversarial examples in the embedding space to expedite the training
process. Based on the observation that the perturbations generated on the
identical training sample in successive epochs are similar, FAT fully utilizes
historical information when initializing the perturbation. Extensive
experiments demonstrate that FAT significantly boosts the robustness of BERT
models in the synonym-unaware scenario, and outperforms the defense baselines
under various attacks with character-level and word-level modifications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning in Distilled Models. (arXiv:2401.12472v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12472">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing models like BERT can provide state-of-the-art
word embeddings for downstream NLP tasks. However, these models yet to perform
well on Semantic Textual Similarity, and may be too large to be deployed as
lightweight edge applications. We seek to apply a suitable contrastive learning
method based on the SimCSE paper, to a model architecture adapted from a
knowledge distillation based model, DistilBERT, to address these two issues.
Our final lightweight model DistilFace achieves an average of 72.1 in
Spearman's correlation on STS tasks, a 34.2 percent improvement over BERT base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12474">
<div class="article-summary-box-inner">
<span><p>Considerable efforts have been invested in augmenting the role-playing
proficiency of open-source large language models (LLMs) by emulating
proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor
role-play capabilities, owing to the extensive knowledge of characters and
potential dialogues ingrained in their vast training corpora. Thus, in this
study, we introduce Ditto, a self-alignment method for role-play. Ditto
capitalizes on character knowledge, encouraging an instruction-following LLM to
simulate role-play dialogues as a variant of reading comprehension. This method
creates a role-play training set comprising 4,000 characters, surpassing the
scale of currently available datasets by tenfold regarding the number of roles.
Subsequently, we fine-tune the LLM using this self-generated dataset to augment
its role-playing capabilities. Upon evaluating our meticulously constructed and
reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in
various parameter scales, consistently maintains a consistent role identity and
provides accurate role-specific knowledge in multi-turn role-play
conversations. Notably, it outperforms all open-source role-play baselines,
showcasing performance levels comparable to advanced proprietary chatbots.
Furthermore, we present the first comprehensive cross-supervision alignment
experiment in the role-play domain, revealing that the intrinsic capabilities
of LLMs confine the knowledge within role-play. Meanwhile, the role-play styles
can be easily acquired with the guidance of smaller models. We open-source
related resources at https://github.com/OFA-Sys/Ditto.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing and Understanding Creativity in Large Language Models. (arXiv:2401.12491v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12491">
<div class="article-summary-box-inner">
<span><p>In the field of natural language processing, the rapid development of large
language model (LLM) has attracted more and more attention. LLMs have shown a
high level of creativity in various tasks, but the methods for assessing such
creativity are inadequate. The assessment of LLM creativity needs to consider
differences from humans, requiring multi-dimensional measurement while
balancing accuracy and efficiency. This paper aims to establish an efficient
framework for assessing the level of creativity in LLMs. By adapting the
modified Torrance Tests of Creative Thinking, the research evaluates the
creative performance of various LLMs across 7 tasks, emphasizing 4 criteria
including Fluency, Flexibility, Originality, and Elaboration. In this context,
we develop a comprehensive dataset of 700 questions for testing and an
LLM-based evaluation method. In addition, this study presents a novel analysis
of LLMs' responses to diverse prompts and role-play situations. We found that
the creativity of LLMs primarily falls short in originality, while excelling in
elaboration. Besides, the use of prompts and the role-play settings of the
model significantly influence creativity. Additionally, the experimental
results also indicate that collaboration among multiple LLMs can enhance
originality. Notably, our findings reveal a consensus between human evaluations
and LLMs regarding the personality traits that influence creativity. The
findings underscore the significant impact of LLM design on creativity and
bridges artificial intelligence and human creativity, offering insights into
LLMs' creativity and potential applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12492">
<div class="article-summary-box-inner">
<span><p>Natural language processing has made progress in incorporating human context
into its models, but whether it is more effective to use group-wise attributes
(e.g., over-45-year-olds) or model individuals remains open. Group attributes
are technically easier but coarse: not all 45-year-olds write the same way. In
contrast, modeling individuals captures the complexity of each person's
identity. It allows for a more personalized representation, but we may have to
model an infinite number of users and require data that may be impossible to
get. We compare modeling human context via group attributes, individual users,
and combined approaches. Combining group and individual features significantly
benefits user-level regression tasks like age estimation or personality
assessment from a user's documents. Modeling individual users significantly
improves the performance of single document-level classification tasks like
stance and topic detection. We also find that individual-user modeling does
well even without user's historical data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements. (arXiv:2401.12520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12520">
<div class="article-summary-box-inner">
<span><p>With the rapid proliferation of textual data, predicting long texts has
emerged as a significant challenge in the domain of natural language
processing. Traditional text prediction methods encounter substantial
difficulties when grappling with long texts, primarily due to the presence of
redundant and irrelevant information, which impedes the model's capacity to
capture pivotal insights from the text. To address this issue, we introduce a
novel approach to long-text classification and prediction. Initially, we employ
embedding techniques to condense the long texts, aiming to diminish the
redundancy therein. Subsequently,the Bidirectional Encoder Representations from
Transformers (BERT) embedding method is utilized for text classification
training. Experimental outcomes indicate that our method realizes considerable
performance enhancements in classifying long texts of Preferential Trade
Agreements. Furthermore, the condensation of text through embedding methods not
only augments prediction accuracy but also substantially reduces computational
complexity. Overall, this paper presents a strategy for long-text prediction,
offering a valuable reference for researchers and engineers in the natural
language processing sphere.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12522">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) commonly employ autoregressive generation during
inference, leading to high memory bandwidth demand and consequently extended
latency. To mitigate this inefficiency, we present Bi-directional Tuning for
lossless Acceleration (BiTA), an innovative method expediting LLMs via
streamlined semi-autoregressive generation and draft verification. Inspired by
the concept of prompt tuning, we enhance LLMs with a parameter-efficient design
called bi-directional tuning for the capability in semi-autoregressive
generation. Employing efficient tree-based decoding, the models perform draft
candidate generation and verification in parallel, ensuring outputs identical
to their autoregressive counterparts under greedy sampling. BiTA serves as a
lightweight plug-in module, seamlessly boosting the inference efficiency of
existing LLMs without requiring additional assistance models or incurring
significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat
achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments
confirm our method surpasses state-of-the-art acceleration techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DREditor: An Time-efficient Approach for Building a Domain-specific Dense Retrieval Model. (arXiv:2401.12540v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12540">
<div class="article-summary-box-inner">
<span><p>Deploying dense retrieval models efficiently is becoming increasingly
important across various industries. This is especially true for enterprise
search services, where customizing search engines to meet the time demands of
different enterprises in different domains is crucial. Motivated by this, we
develop a time-efficient approach called DREditor to edit the matching rule of
an off-the-shelf dense retrieval model to suit a specific domain. This is
achieved by directly calibrating the output embeddings of the model using an
efficient and effective linear mapping. This mapping is powered by an edit
operator that is obtained by solving a specially constructed least squares
problem. Compared to implicit rule modification via long-time finetuning, our
experimental results show that DREditor provides significant advantages on
different domain-specific datasets, dataset sources, retrieval models, and
computing devices. It consistently enhances time efficiency by 100-300 times
while maintaining comparable or even superior retrieval performance. In a
broader context, we take the first step to introduce a novel embedding
calibration approach for the retrieval task, filling the technical blank in the
current field of embedding calibration. This approach also paves the way for
building domain-specific dense retrieval models efficiently and inexpensively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Fact-Checking of Climate Change Claims with Large Language Models. (arXiv:2401.12566v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12566">
<div class="article-summary-box-inner">
<span><p>This paper presents Climinator, a novel AI-based tool designed to automate
the fact-checking of climate change claims. Utilizing an array of Large
Language Models (LLMs) informed by authoritative sources like the IPCC reports
and peer-reviewed scientific literature, Climinator employs an innovative
Mediator-Advocate framework. This design allows Climinator to effectively
synthesize varying scientific perspectives, leading to robust, evidence-based
evaluations. Our model demonstrates remarkable accuracy when testing claims
collected from Climate Feedback and Skeptical Science. Notably, when
integrating an advocate with a climate science denial perspective in our
framework, Climinator's iterative debate process reliably converges towards
scientific consensus, underscoring its adeptness at reconciling diverse
viewpoints into science-based, factual conclusions. While our research is
subject to certain limitations and necessitates careful interpretation, our
approach holds significant potential. We hope to stimulate further research and
encourage exploring its applicability in other contexts, including political
fact-checking and legal domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12576">
<div class="article-summary-box-inner">
<span><p>Interpretability tools that offer explanations in the form of a dialogue have
demonstrated their efficacy in enhancing users' understanding, as one-off
explanations may occasionally fall short in providing sufficient information to
the user. Current solutions for dialogue-based explanations, however, require
many dependencies and are not easily transferable to tasks they were not
designed for. With LLMCheckup, we present an easily accessible tool that allows
users to chat with any state-of-the-art large language model (LLM) about its
behavior. We enable LLMs to generate all explanations by themselves and take
care of intent recognition without fine-tuning, by connecting them with a broad
spectrum of Explainable AI (XAI) tools, e.g. feature attributions,
embedding-based similarity, and prompting strategies for counterfactual and
rationale generation. LLM (self-)explanations are presented as an interactive
dialogue that supports follow-up questions and generates suggestions.
LLMCheckup provides tutorials for operations available in the system, catering
to individuals with varying levels of expertise in XAI and supports multiple
input modalities. We introduce a new parsing strategy called multi-prompt
parsing substantially enhancing the parsing accuracy of LLMs. Finally, we
showcase the tasks of fact checking and commonsense question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12585">
<div class="article-summary-box-inner">
<span><p>The dynamic nature of language, particularly evident in the realm of slang
and memes on the Internet, poses serious challenges to the adaptability of
large language models (LLMs). Traditionally anchored to static datasets, these
models often struggle to keep up with the rapid linguistic evolution
characteristic of online communities. This research addresses the critical need
to bridge this gap, aiming to enhance LLMs' comprehension of evolving new
concepts on the internet, without the high cost and impracticality of continual
retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$
to assess LLMs' proficiency in comprehending emerging linguistic trends and a
baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs
to understand new phrases and usage patterns. This approach involves
scrutinizing real-world instances of linguistic shifts, serving as contextual
beacons, to form more precise and contextually relevant connections between
newly emerging expressions and their intended meanings. The empirical analysis
shows that our causal inference-based approach outperforms the traditional
models in terms of precision and relevance in the interpretation of Internet
slang and memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments. (arXiv:2401.12631v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12631">
<div class="article-summary-box-inner">
<span><p>We respond to the recent paper by Makelov et al. (2023), which reviews
subspace interchange intervention methods like distributed alignment search
(DAS; Geiger et al. 2023) and claims that these methods potentially cause
"interpretability illusions". We first review Makelov et al. (2023)'s technical
notion of what an "interpretability illusion" is, and then we show that even
intuitive and desirable explanations can qualify as illusions in this sense. As
a result, their method of discovering "illusions" can reject explanations they
consider "non-illusory". We then argue that the illusions Makelov et al. (2023)
see in practice are artifacts of their training and evaluation paradigms. We
close by emphasizing that, though we disagree with their core characterization,
Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the
field of interpretability forward.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context. (arXiv:2401.12671v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12671">
<div class="article-summary-box-inner">
<span><p>In the continuously advancing AI landscape, crafting context-rich and
meaningful responses via Large Language Models (LLMs) is essential. Researchers
are becoming more aware of the challenges that LLMs with fewer parameters
encounter when trying to provide suitable answers to open-ended questions. To
address these hurdles, the integration of cutting-edge strategies, augmentation
of rich external domain knowledge to LLMs, offers significant improvements.
This paper introduces a novel framework that combines graph-driven context
retrieval in conjunction to knowledge graphs based enhancement, honing the
proficiency of LLMs, especially in domain specific community question answering
platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on
various LLMs with different parameter sizes to evaluate their ability to ground
knowledge and determine factual accuracy in answers to open-ended questions.
Our methodology GraphContextGen consistently outperforms dominant text-based
retrieval systems, demonstrating its robustness and adaptability to a larger
number of use cases. This advancement highlights the importance of pairing
context rich data retrieval with LLMs, offering a renewed approach to knowledge
sourcing and generation in AI systems. We also show that, due to rich
contextual data retrieval, the crucial entities, along with the generated
answer, remain factually coherent with the gold answer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12689">
<div class="article-summary-box-inner">
<span><p>The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE's validity, together with its
superiority compared with prior approaches. We also prove MDE's versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12713">
<div class="article-summary-box-inner">
<span><p>The task of rumour verification in social media concerns assessing the
veracity of a claim on the basis of conversation threads that result from it.
While previous work has focused on predicting a veracity label, here we
reformulate the task to generate model-centric, free-text explanations of a
rumour's veracity. We follow an unsupervised approach by first utilising
post-hoc explainability methods to score the most important posts within a
thread and then we use these posts to generate informative explanatory
summaries by employing template-guided summarisation. To evaluate the
informativeness of the explanatory summaries, we exploit the few-shot learning
capabilities of a large language model (LLM). Our experiments show that LLMs
can have similar agreement to humans in evaluating summaries. Importantly, we
show that explanatory abstractive summaries are more informative and better
reflect the predicted rumour veracity than just using the highest ranking posts
in the thread.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions. (arXiv:2401.12720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12720">
<div class="article-summary-box-inner">
<span><p>Language is a dynamic aspect of our culture that changes when expressed in
different technologies/communities. Online social networks have enabled the
diffusion and evolution of different dialects, including African American
English (AAE). However, this increased usage is not without barriers. One
particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity
(Google's Perspective and the open-source Detoxify) methods present biases
towards utterances with AAE expressions. Consider Google's Perspective to
understand bias. Here, an utterance such as ``All n*ggers deserve to die
respectfully. The police murder us.'' it reaches a higher toxicity than
``African-Americans deserve to die respectfully. The police murder us.''. This
score difference likely arises because the tool cannot understand the
re-appropriation of the term ``n*gger''. One explanation for this bias is that
AI models are trained on limited datasets, and using such a term in training
data is more likely to appear in a toxic utterance. While this may be
plausible, the tool will make mistakes regardless. Here, we study bias on two
Web-based (YouTube and Twitter) datasets and two spoken English datasets. Our
analysis shows how most models present biases towards AAE in most settings. We
isolate the impact of AAE expression usage via linguistic control features from
the Linguistic Inquiry and Word Count (LIWC) software, grammatical control
features extracted via Part-of-Speech (PoS) tagging from Natural Language
Processing (NLP) models, and the semantic of utterances by comparing sentence
embeddings from recent language models. We present consistent results on how a
heavy usage of AAE expressions may cause the speaker to be considered
substantially more toxic, even when speaking about nearly the same subject. Our
study complements similar analyses focusing on small datasets and/or one method
only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12756">
<div class="article-summary-box-inner">
<span><p>The knowledge encapsulated in a model is the core factor determining its
final performance on downstream tasks. Much research in NLP has focused on
efficient methods for storing and adapting different types of knowledge, e.g.,
in dedicated modularized structures, and on how to effectively combine these,
e.g., by learning additional parameters. However, given the many possible
options, a thorough understanding of the mechanisms involved in these
compositions is missing, and hence it remains unclear which strategies to
utilize. To address this research gap, we propose a novel framework for
zero-shot module composition, which encompasses existing and some novel
variations for selecting, weighting, and combining parameter modules under a
single unified notion. Focusing on the scenario of domain knowledge and adapter
layers, our framework provides a systematic unification of concepts, allowing
us to conduct the first comprehensive benchmarking study of various zero-shot
knowledge composition strategies. In particular, we test two module combination
methods and five selection and weighting strategies for their effectiveness and
efficiency in an extensive experimental setup. Our results highlight the
efficacy of ensembling but also hint at the power of simple though
often-ignored weighting methods. Further in-depth analyses allow us to
understand the role of weighting vs. top-k selection, and show that, to a
certain extent, the performance of adapter composition can even be predicted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study. (arXiv:2401.12789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12789">
<div class="article-summary-box-inner">
<span><p>In the era of large models, the autoregressive nature of decoding often
results in latency serving as a significant bottleneck. We propose a
non-autoregressive LM-fused ASR system that effectively leverages the
parallelization capabilities of accelerator hardware. Our approach combines the
Universal Speech Model (USM) and the PaLM 2 language model in per-segment
scoring mode, achieving an average relative WER improvement across all
languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our
comprehensive ablation study analyzes key parameters such as LLM size, context
length, vocabulary size, fusion methodology. For instance, we explore the
impact of LLM size ranging from 128M to 340B parameters on ASR performance.
This study provides valuable insights into the factors influencing the
effectiveness of practical large-scale LM-fused speech recognition systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking LLMs via Uncertainty Quantification. (arXiv:2401.12794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12794">
<div class="article-summary-box-inner">
<span><p>The proliferation of open-source Large Language Models (LLMs) from various
institutions has highlighted the urgent need for comprehensive evaluation
methods. However, current evaluation platforms, such as the widely recognized
HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,
which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce
a new benchmarking approach for LLMs that integrates uncertainty
quantification. Our examination involves eight LLMs (LLM series) spanning five
representative natural language processing tasks. Additionally, we introduce an
uncertainty-aware evaluation metric, UAcc, which takes into account both
prediction accuracy and prediction uncertainty. Our findings reveal that: I)
LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs
may display greater uncertainty compared to their smaller counterparts; and
III) Instruction-finetuning tends to increase the uncertainty of LLMs. By
taking uncertainty into account, our new UAcc metric can either amplify or
diminish the relative improvement of one LLM over another and may even change
the relative ranking of two LLMs. These results underscore the significance of
incorporating uncertainty in the evaluation of LLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12798">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA), a pivotal process in integrating multi-source
Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these
graphs. Most existing approaches regard EA as a graph representation learning
task, concentrating on enhancing graph encoders. However, the decoding process
in EA - essential for effective operation and alignment accuracy - has received
limited attention and remains tailored to specific datasets and model
architectures, necessitating both entity and additional explicit relation
embeddings. This specificity limits its applicability, particularly in
GNN-based models. To address this gap, we introduce a novel, generalized, and
efficient decoding approach for EA, relying solely on entity embeddings. Our
method optimizes the decoding process by minimizing Dirichlet energy, leading
to the gradient flow within the graph, to promote graph homophily. The
discretization of the gradient flow produces a fast and scalable approach,
termed Triple Feature Propagation (TFP). TFP innovatively channels gradient
flow through three views: entity-to-entity, entity-to-relation, and
relation-to-entity. This generalized gradient flow enables TFP to harness the
multi-view structural information of KGs. Rigorous experimentation on diverse
real-world datasets demonstrates that our approach significantly enhances
various EA methods. Notably, the approach achieves these advancements with less
than 6 seconds of additional computational time, establishing a new benchmark
in efficiency and adaptability for future EA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning. (arXiv:2401.12863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12863">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have demonstrated impressive performance in
natural language processing tasks by leveraging chain of thought (CoT) that
enables step-by-step thinking. Extending LLMs with multimodal capabilities is
the recent interest, but incurs computational cost and requires substantial
hardware resources. To address these challenges, we propose KAM-CoT a framework
that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities
for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a
two-stage training process with KG grounding to generate effective rationales
and answers. By incorporating external knowledge from KGs during reasoning, the
model gains a deeper contextual understanding reducing hallucinations and
enhancing the quality of answers. This knowledge-augmented CoT reasoning
empowers the model to handle questions requiring external context, providing
more informed answers. Experimental findings show KAM-CoT outperforms the
state-of-the-art methods. On the ScienceQA dataset, we achieve an average
accuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by
10%. Remarkably, KAM-CoT achieves these results with only 280M trainable
parameters at a time, demonstrating its cost-efficiency and effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12873">
<div class="article-summary-box-inner">
<span><p>Insufficient modeling of human preferences within the reward model is a major
obstacle for leveraging human feedback to improve translation quality.
Fortunately, quality estimation (QE), which predicts the quality of a given
translation without reference, has achieved impressive alignment with human
evaluations in the last two years. In this work, we investigate the potential
of employing the QE model as the reward model (the QE-based reward model) to
predict human preferences for feedback training. We first identify the
overoptimization problem during QE-based feedback training, manifested as an
increase in reward while translation quality declines. We examine the problem
and argue that the vulnerability of the QE model might lead to high rewards for
incorrect translations, resulting in overoptimization and error propagation. To
address the problem, we adopt a simple yet effective method that uses heuristic
rules to detect the incorrect translations and assigns a penalty term to the
QE-based rewards for the detected incorrect translations. Experimental results
show that the proposed QE-based feedback training achieves consistent and
significant improvements across various settings, further verified through
human preference studies. Our subsequent analysis demonstrates the high data
efficiency of the proposed QE-based feedback training: the proposed approach
using a small amount of monolingual data can outperform systems using larger
parallel corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Understanding to Utilization: A Survey on Explainability for Large Language Models. (arXiv:2401.12874v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12874">
<div class="article-summary-box-inner">
<span><p>This survey paper delves into the burgeoning field of explainability for
Large Language Models (LLMs), a critical yet challenging aspect of natural
language processing. With LLMs playing a pivotal role in various applications,
their "black-box" nature raises concerns about transparency and ethical use.
This paper emphasizes the necessity for enhanced explainability in LLMs,
addressing both the general public's trust and the technical community's need
for a deeper understanding of these models. We concentrate on pre-trained
Transformer-based LLMs, such as LLaMA, which present unique interpretability
challenges due to their scale and complexity. Our review categorizes existing
explainability methods and discusses their application in improving model
transparency and reliability. We also discuss representative evaluation
methods, highlighting their strengths and limitations. The goal of this survey
is to bridge the gap between theoretical understanding and practical
application, offering insights for future research and development in the field
of LLM explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Red Teaming Visual Language Models. (arXiv:2401.12915v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12915">
<div class="article-summary-box-inner">
<span><p>VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language
Models) to accept multimodal inputs. Since it has been verified that LLMs can
be induced to generate harmful or inaccurate content through specific test
cases (termed as Red Teaming), how VLMs perform in similar scenarios,
especially with their combination of textual and visual inputs, remains a
question. To explore this problem, we present a novel red teaming dataset
RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal
jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,
privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to
benchmark current VLMs in terms of these 4 different aspects. Detailed analysis
shows that 10 prominent open-sourced VLMs struggle with the red teaming in
different degrees and have up to 31% performance gap with GPT-4V. Additionally,
we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning
(SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM
test set, 13% in MM-Hal, and without noticeable decline in MM-Bench,
overpassing other LLaVA-based models with regular alignment data. This reveals
that current open-sourced VLMs still lack red teaming alignment. Our code and
datasets will be open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multicultural Name Recognition For Previously Unseen Names. (arXiv:2401.12941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12941">
<div class="article-summary-box-inner">
<span><p>State of the art Named Entity Recognition (NER) models have achieved an
impressive ability to extract common phrases from text that belong to labels
such as location, organization, time, and person. However, typical NER systems
that rely on having seen a specific entity in their training data in order to
label an entity perform poorly on rare or unseen entities ta in order to label
an entity perform poorly on rare or unseen entities (Derczynski et al., 2017).
This paper attempts to improve recognition of person names, a diverse category
that can grow any time someone is born or changes their name. In order for
downstream tasks to not exhibit bias based on cultural background, a model
should perform well on names from a variety of backgrounds. In this paper I
experiment with the training data and input structure of an English Bi-LSTM
name recognition model. I look at names from 103 countries to compare how well
the model performs on names from different cultures, specifically in the
context of a downstream task where extracted names will be matched to
information on file. I find that a model with combined character and word input
outperforms word-only models and may improve on accuracy compared to classical
NER models that are not geared toward identifying unseen entity values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion. (arXiv:2401.12947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12947">
<div class="article-summary-box-inner">
<span><p>This paper investigates the ability of transformer-based models to learn
structural recursion from examples. Recursion is a universal concept in both
natural and formal languages. Structural recursion is central to the
programming language and formal mathematics tasks where symbolic tools
currently excel beyond neural models, such as inferring semantic relations
between datatypes and emulating program behavior. We introduce a general
framework that nicely connects the abstract concepts of structural recursion in
the programming language domain to concrete sequence modeling problems and
learned models' behavior. The framework includes a representation that captures
the general \textit{syntax} of structural recursion, coupled with two different
frameworks for understanding their \textit{semantics} -- one that is more
natural from a programming languages perspective and one that helps bridge that
perspective with a mechanistic understanding of the underlying transformer
architecture.
</p>
<p>With our framework as a powerful conceptual tool, we identify different
issues under various set-ups. The models trained to emulate recursive
computations cannot fully capture the recursion yet instead fit short-cut
algorithms and thus cannot solve certain edge cases that are under-represented
in the training distribution. In addition, it is difficult for state-of-the-art
large language models (LLMs) to mine recursive rules from in-context
demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating
reduction (step-wise computation) of the recursive function.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding. (arXiv:2401.12954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12954">
<div class="article-summary-box-inner">
<span><p>We introduce meta-prompting, an effective scaffolding technique designed to
enhance the functionality of language models (LMs). This approach transforms a
single LM into a multi-faceted conductor, adept at managing and integrating
multiple independent LM queries. By employing high-level instructions,
meta-prompting guides the LM to break down complex tasks into smaller, more
manageable subtasks. These subtasks are then handled by distinct "expert"
instances of the same LM, each operating under specific, tailored instructions.
Central to this process is the LM itself, in its role as the conductor, which
ensures seamless communication and effective integration of the outputs from
these expert models. It additionally employs its inherent critical thinking and
robust verification processes to refine and authenticate the end result. This
collaborative prompting approach empowers a single LM to simultaneously act as
a comprehensive orchestrator and a panel of diverse experts, significantly
enhancing its performance across a wide array of tasks. The zero-shot,
task-agnostic nature of meta-prompting greatly simplifies user interaction by
obviating the need for detailed, task-specific instructions. Furthermore, our
research demonstrates the seamless integration of external tools, such as a
Python interpreter, into the meta-prompting framework, thereby broadening its
applicability and utility. Through rigorous experimentation with GPT-4, we
establish the superiority of meta-prompting over conventional scaffolding
methods: When averaged across all tasks, including the Game of 24,
Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented
with a Python interpreter functionality, surpasses standard prompting by 17.1%,
expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.12963">
<div class="article-summary-box-inner">
<span><p>Foundation models that incorporate language, vision, and more recently
actions have revolutionized the ability to harness internet scale data to
reason about useful tasks. However, one of the key challenges of training
embodied foundation models is the lack of data grounded in the physical world.
In this paper, we propose AutoRT, a system that leverages existing foundation
models to scale up the deployment of operational robots in completely unseen
scenarios with minimal human supervision. AutoRT leverages vision-language
models (VLMs) for scene understanding and grounding, and further uses large
language models (LLMs) for proposing diverse and novel instructions to be
performed by a fleet of robots. Guiding data collection by tapping into the
knowledge of foundation models enables AutoRT to effectively reason about
autonomy tradeoffs and safety while significantly scaling up data collection
for robot learning. We demonstrate AutoRT proposing instructions to over 20
robots across multiple buildings and collecting 77k real robot episodes via
both teleoperation and autonomous robot policies. We experimentally show that
such "in-the-wild" data collected by AutoRT is significantly more diverse, and
that AutoRT's use of LLMs allows for instruction following data collection
robots that can align to human preferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Questions by Enhancing Text Generation with Sentence Selection. (arXiv:2212.12192v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.12192">
<div class="article-summary-box-inner">
<span><p>We introduce an approach for the answer-aware question generation problem.
Instead of only relying on the capability of strong pre-trained language
models, we observe that the information of answers and questions can be found
in some relevant sentences in the context. Based on that, we design a model
which includes two modules: a selector and a generator. The selector forces the
model to more focus on relevant sentences regarding an answer to provide
implicit local information. The generator generates questions by implicitly
combining local information from the selector and global information from the
whole context encoded by the encoder. The model is trained jointly to take
advantage of latent interactions between the two modules. Experimental results
on two benchmark datasets show that our model is better than strong pre-trained
models for the question generation task. The code is also available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14391">
<div class="article-summary-box-inner">
<span><p>Language is compositional; an instruction can express multiple relation
constraints to hold among objects in a scene that a robot is tasked to
rearrange. Our focus in this work is an instructable scene-rearranging
framework that generalizes to longer instructions and to spatial concept
compositions never seen at training time. We propose to represent
language-instructed spatial concepts with energy functions over relative object
arrangements. A language parser maps instructions to corresponding energy
functions and an open-vocabulary visual-language model grounds their arguments
to relevant objects in the scene. We generate goal scene configurations by
gradient descent on the sum of energy functions, one per language predicate in
the instruction. Local vision-based policies then re-locate objects to the
inferred goal locations. We test our model on established instruction-guided
manipulation benchmarks, as well as benchmarks of compositional instructions we
introduce. We show our model can execute highly compositional instructions
zero-shot in simulation and in the real world. It outperforms
language-to-action reactive policies and Large Language Model planners by a
large margin, especially for long instructions that involve compositions of
multiple spatial concepts. Simulation and real-world robot execution videos, as
well as our code and datasets are publicly available on our website:
https://ebmplanner.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.02317">
<div class="article-summary-box-inner">
<span><p>Recent advances in large language models elicit reasoning in a
chain-of-thought that allows models to decompose problems in a human-like
fashion. Though this paradigm improves multi-step reasoning ability in language
models, it is limited by being unimodal and applied mainly to
question-answering tasks. We claim that incorporating visual augmentation into
reasoning is essential, especially for complex, imaginative tasks.
Consequently, we introduce VCoT, a novel method that leverages chain-of-thought
prompting with vision-language grounding to recursively bridge the logical gaps
within sequential data. Our method uses visual guidance to generate synthetic
multimodal infillings that add consistent and novel information to reduce the
logical gaps for downstream tasks that can benefit from temporal reasoning, as
well as provide interpretability into models' multi-step reasoning. We apply
VCoT to the Visual Storytelling and WikiHow summarization datasets and
demonstrate through human evaluation that VCoT offers novel and consistent
synthetic data augmentation beating chain-of-thought baselines, which can be
used to enhance downstream performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification. (arXiv:2305.09781v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09781">
<div class="article-summary-box-inner">
<span><p>This paper introduces SpecInfer, a system that accelerates generative large
language model (LLM) serving with tree-based speculative inference and
verification. The key idea behind SpecInfer is leveraging small speculative
models to predict the LLM's outputs; the predictions are organized as a token
tree, whose nodes each represent a candidate token sequence. The correctness of
all candidate token sequences represented by a token tree is verified against
the LLM in parallel using a novel tree-based parallel decoding mechanism.
SpecInfer uses an LLM as a token tree verifier instead of an incremental
decoder, which significantly reduces the end-to-end latency and computational
requirement for serving generative LLMs while provably preserving model
quality. Our evaluation shows that SpecInfer outperforms existing LLM serving
systems by 1.5-2.8x for distributed LLM inference and by 2.6-3.5x for
offloading-based LLM inference, while preserving the same generative
performance. SpecInfer is publicly available at
https://github.com/flexflow/FlexFlow/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.14259">
<div class="article-summary-box-inner">
<span><p>Literature-Based Discovery (LBD) aims to discover new scientific knowledge by
mining papers and generating hypotheses. Standard LBD is limited to predicting
pairwise relations between discrete concepts (e.g., drug-disease links), and
ignores critical contexts like experimental settings (e.g., a specific patient
population where a drug is evaluated) and background motivations (e.g., to find
drugs without specific side effects). We address these limitations with a novel
formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in
natural language, while grounding them in a context that controls the
hypothesis search space. We present a modeling framework using retrieval of
``inspirations'' from past scientific papers. Our evaluations reveal that GPT-4
tends to generate ideas with overall low technical depth and novelty, while our
inspiration prompting approaches partially mitigate this issue. Our work
represents a first step toward building language models that generate new ideas
derived from scientific literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.02272">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) with hundreds of billions of parameters require
powerful server-grade GPUs for inference, limiting their practical deployment.
To address this challenge, we introduce the outlier-aware weight quantization
(OWQ) method, which aims to minimize LLM's footprint through low-precision
representation. OWQ prioritizes a small subset of structured weights sensitive
to quantization, storing them in high-precision, while applying highly tuned
quantization to the remaining dense weights. This sensitivity-aware
mixed-precision scheme reduces the quantization error notably, and extensive
experiments demonstrate that 3.1-bit models using OWQ perform comparably to
4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a
parameter-efficient fine-tuning for task-specific adaptation, called weak
column tuning (WCT), enabling accurate task-specific LLM adaptation with
minimal memory overhead in the optimized format. OWQ represents a notable
advancement in the flexibility, efficiency, and practicality of LLM
optimization literature. The source code is available at
https://github.com/xvyaward/owq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI. (arXiv:2308.07213v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.07213">
<div class="article-summary-box-inner">
<span><p>While many Natural Language Processing (NLP) techniques have been proposed
for fact-checking, both academic research and fact-checking organizations
report limited adoption of such NLP work due to poor alignment with
fact-checker practices, values, and needs. To address this, we investigate a
co-design method, Matchmaking for AI, to enable fact-checkers, designers, and
NLP researchers to collaboratively identify what fact-checker needs should be
addressed by technology, and to brainstorm ideas for potential solutions.
Co-design sessions we conducted with 22 professional fact-checkers yielded a
set of 11 design ideas that offer a "north star", integrating fact-checker
criteria into novel NLP design concepts. These concepts range from pre-bunking
misinformation, efficient and personalized monitoring misinformation,
proactively reducing fact-checker potential biases, and collaborative writing
fact-check reports. Our work provides new insights into both human-centered
fact-checking research and practice and AI co-design research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12890">
<div class="article-summary-box-inner">
<span><p>The emergence of generative Large Language Models (LLMs) emphasizes the need
for accurate and efficient prompting approaches. LLMs are often applied in
Few-Shot Learning (FSL) contexts, where tasks are executed with minimal
training data. FSL has become popular in many Artificial Intelligence (AI)
subdomains, including AI for health. Rare diseases affect a small fraction of
the population. Rare disease identification from clinical notes inherently
requires FSL techniques due to limited data availability. Manual data
collection and annotation is both expensive and time-consuming. In this paper,
we propose Models-Vote Prompting (MVP), a flexible prompting approach for
improving the performance of LLM queries in FSL settings. MVP works by
prompting numerous LLMs to perform the same tasks and then conducting a
majority vote on the resulting outputs. This method achieves improved results
to any one model in the ensemble on one-shot rare disease identification and
classification tasks. We also release a novel rare disease dataset for FSL,
available to those who signed the MIMIC-IV Data Use Agreement (DUA).
Furthermore, in using MVP, each model is prompted multiple times, substantially
increasing the time needed for manual annotation, and to address this, we
assess the feasibility of using JSON for automating generative LLM evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models. (arXiv:2308.16692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16692">
<div class="article-summary-box-inner">
<span><p>Current speech large language models build upon discrete speech
representations, which can be categorized into semantic tokens and acoustic
tokens. However, existing speech tokens are not specifically designed for
speech language modeling. To assess the suitability of speech tokens for
building speech language models, we established the first benchmark,
SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are
ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech
tokenizer for speech large language models. SpeechTokenizer adopts the
Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying
semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of
speech information hierarchically across different RVQ layers. Furthermore, We
construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer.
Experiments show that SpeechTokenizer performs comparably to EnCodec in speech
reconstruction and demonstrates strong performance on the SLMTokBench
benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks.
Code and models are available at
https://github.com/ZhangXInFD/SpeechTokenizer/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiariST: Streaming Speech Translation with Speaker Diarization. (arXiv:2309.08007v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.08007">
<div class="article-summary-box-inner">
<span><p>End-to-end speech translation (ST) for conversation recordings involves
several under-explored challenges such as speaker diarization (SD) without
accurate word time stamps and handling of overlapping speech in a streaming
fashion. In this work, we propose DiariST, the first streaming ST and SD
solution. It is built upon a neural transducer-based streaming ST system and
integrates token-level serialized output training and t-vector, which were
originally developed for multi-talker speech recognition. Due to the absence of
evaluation benchmarks in this area, we develop a new evaluation dataset,
DiariST-AliMeeting, by translating the reference Chinese transcriptions of the
AliMeeting corpus into English. We also propose new metrics, called
speaker-agnostic BLEU and speaker-attributed BLEU, to measure the ST quality
while taking SD accuracy into account. Our system achieves a strong ST and SD
capability compared to offline systems based on Whisper, while performing
streaming inference for overlapping speech. To facilitate the research in this
new direction, we release the evaluation data, the offline baseline systems,
and the evaluation code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multitask Training Approach to Enhance Whisper with Contextual Biasing and Open-Vocabulary Keyword Spotting. (arXiv:2309.09552v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.09552">
<div class="article-summary-box-inner">
<span><p>End-to-end automatic speech recognition (ASR) systems often struggle to
recognize rare name entities, such as personal names, organizations, and
terminologies not frequently encountered in the training data. This paper
presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on
OpenAI's Whisper model that can recognize user-defined name entities by
performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of
Whisper encoder. The recognized entities are used as prompts for the Whisper
decoder. We first propose a multitask training approach with OV-KWS and ASR
tasks to optimize the model. Experiments show that this approach substantially
improves the entity recalls compared to the original Whisper model on Chinese
Aishell hot word subsets and two internal code-switch test sets. However, we
observed a slight increase in mixed-error-rate (MER) on internal test sets due
to catastrophic forgetting. To address this problem and use different sizes of
the Whisper model without finetuning, we propose to use OV-KWS as a separate
module and construct a spoken form prompt to prevent hallucination. The OV-KWS
module consistently improves MER and Entity Recall for whisper-small, medium,
and large models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ. (arXiv:2310.00367v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00367">
<div class="article-summary-box-inner">
<span><p>Generating bitmap graphics from text has gained considerable attention, yet
for scientific figures, vector graphics are often preferred. Given that vector
graphics are typically encoded using low-level graphics primitives, generating
them directly is difficult. To address this, we propose the use of TikZ, a
well-known abstract graphics language that can be compiled to vector graphics,
as an intermediate representation of scientific figures. TikZ offers
human-oriented, high-level commands, thereby facilitating conditional language
modeling with any large language model. To this end, we introduce DaTikZ, the
first large-scale TikZ dataset consisting of 120k TikZ drawings aligned with
captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which
augments LLaMA with multimodal CLIP embeddings. In both human and automatic
evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms
of similarity to human-created figures, with CLiMA additionally improving
text-image alignment. Our detailed analysis shows that all models generalize
well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend
to generate more simplistic figures compared to both humans and our models. We
make our framework, AutomaTikZ, along with model weights and datasets, publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v3 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.00737">
<div class="article-summary-box-inner">
<span><p>Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are marvels of technology; celebrated for their prowess in natural language
processing and multimodal content generation, they promise a transformative
future. But as with all powerful tools, they come with their shadows. Picture
living in a world where deepfakes are indistinguishable from reality, where
synthetic identities orchestrate malicious campaigns, and where targeted
misinformation or scams are crafted with unparalleled precision. Welcome to the
darker side of GenAI applications. This article is not just a journey through
the meanders of potential misuse of GenAI and LLMs, but also a call to
recognize the urgency of the challenges ahead. As we navigate the seas of
misinformation campaigns, malicious content generation, and the eerie creation
of sophisticated malware, we'll uncover the societal implications that ripple
through the GenAI revolution we are witnessing. From AI-powered botnets on
social media platforms to the unnerving potential of AI to generate fabricated
identities, or alibis made of synthetic realities, the stakes have never been
higher. The lines between the virtual and the real worlds are blurring, and the
consequences of potential GenAI's nefarious applications impact us all. This
article serves both as a synthesis of rigorous research presented on the risks
of GenAI and misuse of LLMs and as a thought-provoking vision of the different
types of harmful GenAI applications we might encounter in the near future, and
some ways we can prepare for them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.03025">
<div class="article-summary-box-inner">
<span><p>Extending the context window of large language models (LLMs) is getting
popular recently, while the solution of augmenting LLMs with retrieval has
existed for years. The natural questions are: i) Retrieval-augmentation versus
long context window, which one is better for downstream tasks? ii) Can both
methods be combined to get the best of both worlds? In this work, we answer
these questions by studying both solutions using two state-of-the-art
pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps
surprisingly, we find that LLM with 4K context window using simple
retrieval-augmentation at generation can achieve comparable performance to
finetuned LLM with 16K context window via positional interpolation on long
context tasks, while taking much less computation. More importantly, we
demonstrate that retrieval can significantly improve the performance of LLMs
regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms
GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context
tasks including question answering, query-based summarization, and in-context
few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k
baseline by a margin, while being much faster at generation. Our study provides
general insights on the choice of retrieval-augmentation versus long context
extension of LLM for practitioners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.04691">
<div class="article-summary-box-inner">
<span><p>Neural language models are probabilistic models of human text. They are
predominantly trained using maximum likelihood estimation (MLE), which is
equivalent to minimizing the forward cross-entropy between the empirical data
distribution and the model distribution. However, various degeneration
phenomena are still widely observed when decoding from the distributions
learned by such models. We establish that the forward cross-entropy is
suboptimal as a distance metric for aligning human and model distribution due
to its (1) recall-prioritization (2) negative diversity ignorance and (3)
train-test mismatch. In this paper, we propose Earth Mover Distance
Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on
the inherent properties of earth mover distance to address the aforementioned
challenges. Due to the high complexity of direct computation, we further
introduce a feasible upper bound for EMO to ease end-to-end training. Upon
extensive evaluation of language models trained using EMO and MLE. We find that
EMO demonstrates a consistently better language modeling performance than MLE
across domains. Moreover, EMO demonstrates noteworthy enhancements in
downstream performance with minimal fine-tuning on merely 25,000 sentences.
This highlights the tremendous potential of EMO as a lightweight calibration
method for enhancing large-scale pre-trained language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.08535">
<div class="article-summary-box-inner">
<span><p>Autonomous, goal-driven agents powered by LLMs have recently emerged as
promising tools for solving challenging problems without the need for
task-specific finetuned models that can be expensive to procure. Currently, the
design and implementation of such agents is ad hoc, as the wide variety of
tasks that LLM-based agents may be applied to naturally means there can be no
one-size-fits-all approach to agent design. In this work we aim to alleviate
the difficulty of designing and implementing new agents by proposing a
minimalistic generation framework that simplifies the process of building
agents. The framework we introduce allows the user to define desired agent
behaviors in a high-level, declarative specification that is then used to
construct a decoding monitor which guarantees the LLM will produce an output
exhibiting the desired behavior. Our declarative approach, in which the
behavior is described without concern for how it should be implemented or
enforced, enables rapid design, implementation, and experimentation with
different LLM-based agents. We demonstrate how the proposed framework can be
used to implement recent LLM-based agents (e.g., ReACT), and show how the
flexibility of our approach can be leveraged to define a new agent with more
complex behavior, the Plan-Act-Summarize-Solve (PASS) agent. Lastly, we
demonstrate that our method outperforms other agents on multiple popular
reasoning-centric question-answering benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2310.17715">
<div class="article-summary-box-inner">
<span><p>Representations from large language models (LLMs) are known to be dominated
by a small subset of dimensions with exceedingly high variance. Previous works
have argued that although ablating these outlier dimensions in LLM
representations hurts downstream performance, outlier dimensions are
detrimental to the representational quality of embeddings. In this study, we
investigate how fine-tuning impacts outlier dimensions and show that 1) outlier
dimensions that occur in pre-training persist in fine-tuned models and 2) a
single outlier dimension can complete downstream tasks with a minimal error
rate. Our results suggest that outlier dimensions can encode crucial
task-specific knowledge and that the value of a representation in a single
outlier dimension drives downstream model decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text. (arXiv:2311.12373v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2311.12373">
<div class="article-summary-box-inner">
<span><p>Significant progress has been made on text generation by pre-trained language
models (PLMs), yet distinguishing between human and machine-generated text
poses an escalating challenge. This paper offers an in-depth evaluation of
three distinct methods used to address this task: traditional shallow learning,
Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These
approaches are rigorously tested on a wide range of machine-generated texts,
providing a benchmark of their competence in distinguishing between
human-authored and machine-authored linguistic constructs. The results reveal
considerable differences in performance across methods, thus emphasizing the
continued need for advancement in this crucial area of NLP. This study offers
valuable insights and paves the way for future research aimed at creating
robust and highly discriminative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A ripple in time: a discontinuity in American history. (arXiv:2312.01185v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.01185">
<div class="article-summary-box-inner">
<span><p>In this note we use the State of the Union Address (SOTU) dataset from Kaggle
to make some surprising (and some not so surprising) observations pertaining to
the general timeline of American history, and the character and nature of the
addresses themselves. Our main approach is using vector embeddings, such as
BERT (DistilBERT) and GPT-2.
</p>
<p>While it is widely believed that BERT (and its variations) is most suitable
for NLP classification tasks, we find out that GPT-2 in conjunction with
nonlinear dimension reduction methods such as UMAP provide better separation
and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In
our case, no model fine-tuning is required, and the pre-trained out-of-the-box
GPT-2 model is enough.
</p>
<p>We also used a fine-tuned DistilBERT model for classification detecting which
President delivered which address, with very good results (accuracy 93% - 95%
depending on the run). An analogous task was performed to determine the year of
writing, and we were able to pin it down to about 4 years (which is a single
presidential term).
</p>
<p>It is worth noting that SOTU addresses provide relatively small writing
samples (with about 8'000 words on average, and varying widely from under 2'000
words to more than 20'000), and that the number of authors is relatively large
(we used SOTU addresses of 42 US presidents). This shows that the techniques
employed turn out to be rather efficient, while all the computations described
in this note can be performed using a single GPU instance of Google Colab.
</p>
<p>The accompanying code is available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.07913">
<div class="article-summary-box-inner">
<span><p>Text watermarking algorithms play a crucial role in the copyright protection
of textual content, yet their capabilities and application scenarios have been
limited historically. The recent developments in large language models (LLMs)
have opened new opportunities for the advancement of text watermarking
techniques. LLMs not only enhance the capabilities of text watermarking
algorithms through their text understanding and generation abilities but also
necessitate the use of text watermarking algorithms for their own copyright
protection. This paper conducts a comprehensive survey of the current state of
text watermarking technology, covering four main aspects: (1) an overview and
comparison of different text watermarking techniques; (2) evaluation methods
for text watermarking algorithms, including their success rates, impact on text
quality, robustness, and unforgeability; (3) potential application scenarios
for text watermarking technology; (4) current challenges and future directions
for development. This survey aims to provide researchers with a thorough
understanding of text watermarking technology, thereby promoting its further
advancement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. (arXiv:2312.13010v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2312.13010">
<div class="article-summary-box-inner">
<span><p>The advancement of natural language processing (NLP) has been significantly
boosted by the development of transformer-based large language models (LLMs).
These models have revolutionized NLP tasks, particularly in code generation,
aiding developers in creating software with enhanced efficiency. Despite their
advancements, challenges in balancing code snippet generation with effective
test case generation and execution persist. To address these issues, this paper
introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution
comprising a multi-agent framework with specialized agents: the programmer
agent, the test designer agent, and the test executor agent. During the coding
procedure, the programmer agent will focus on the code generation and
refinement based on the test executor agent's feedback. The test designer agent
will generate test cases for the generated code, and the test executor agent
will run the code with the test cases and write the feedback to the programmer.
This collaborative system ensures robust code generation, surpassing the
limitations of single-agent models and traditional methodologies. Our extensive
experiments on 9 code generation models and 12 enhancement approaches showcase
AgentCoder's superior performance over existing code generation models and
prompt engineering techniques across various benchmarks. For example,
AgentCoder achieves 77.4% and 89.1% pass@1 in HumanEval-ET and MBPP-ET with
GPT-3.5, while SOTA baselines obtain only 69.5% and 63.0%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.02994">
<div class="article-summary-box-inner">
<span><p>In conversational AI research, there's a noticeable trend towards developing
models with a larger number of parameters, exemplified by models like ChatGPT.
While these expansive models tend to generate increasingly better chat
responses, they demand significant computational resources and memory. This
study explores a pertinent question: Can a combination of smaller models
collaboratively achieve comparable or enhanced performance relative to a
singular large model? We introduce an approach termed "blending", a
straightforward yet effective method of integrating multiple chat AIs. Our
empirical evidence suggests that when specific smaller models are
synergistically blended, they can potentially outperform or match the
capabilities of much larger counterparts. For instance, integrating just three
models of moderate size (6B/13B paramaeters) can rival or even surpass the
performance metrics of a substantially larger model like ChatGPT (175B+
paramaters). This hypothesis is rigorously tested using A/B testing
methodologies with a large user base on the Chai research platform over a span
of thirty days. The findings underscore the potential of the "blending"
strategy as a viable approach for enhancing chat AI efficacy without a
corresponding surge in computational demands.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of User Behaviors for Objectively Evaluating Spoken Dialogue Systems. (arXiv:2401.04867v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.04867">
<div class="article-summary-box-inner">
<span><p>Establishing evaluation schemes for spoken dialogue systems is important, but
it can also be challenging. While subjective evaluations are commonly used in
user experiments, objective evaluations are necessary for research comparison
and reproducibility. To address this issue, we propose a framework for
indirectly but objectively evaluating systems based on users' behaviors. In
this paper, to this end, we investigate the relationship between user behaviors
and subjective evaluation scores in social dialogue tasks: attentive listening,
job interview, and first-meeting conversation. The results reveal that in
dialogue tasks where user utterances are primary, such as attentive listening
and job interview, indicators like the number of utterances and words play a
significant role in evaluation. Observing disfluency also can indicate the
effectiveness of formal tasks, such as job interview. On the other hand, in
dialogue tasks with high interactivity, such as first-meeting conversation,
behaviors related to turn-taking, like average switch pause length, become more
important. These findings suggest that selecting appropriate user behaviors can
provide valuable insights for objective evaluation in each social dialogue
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning. (arXiv:2401.06827v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.06827">
<div class="article-summary-box-inner">
<span><p>Pre-trained Vision-Language (V-L) models set the benchmark for generalization
to downstream tasks among the noteworthy contenders. Many characteristics of
the V-L model have been explored in existing research including the challenge
of the sensitivity to text input and the tuning process across multi-modal
prompts. With the advanced utilization of the V-L model like CLIP, recent
approaches deploy learnable prompts instead of hand-craft prompts to boost the
generalization performance and address the aforementioned challenges. Inspired
by layer-wise training, which is wildly used in image fusion, we note that
using a sequential training process to adapt different modalities branches of
CLIP efficiently facilitates the improvement of generalization. In the context
of addressing the multi-modal prompting challenge, we propose Token-wise
Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities
prompts, vision and language, as tokens in a sequential manner. APLe addresses
the challenges in V-L models to promote prompt learning across both modalities,
which indicates a competitive generalization performance in line with the
state-of-the-art. Preeminently, APLe shows robustness and favourable
performance in prompt-length experiments with an absolute advantage in adopting
the V-L models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.08517">
<div class="article-summary-box-inner">
<span><p>Student commitment towards a learning recommendation is not separable from
their understanding of the reasons it was recommended to them; and their
ability to modify it based on that understanding. Among explainability
approaches, chatbots offer the potential to engage the student in a
conversation, similar to a discussion with a peer or a mentor. The capabilities
of chatbots, however, are still not sufficient to replace a human mentor,
despite the advancements of generative AI (GenAI) and large language models
(LLM). Therefore, we propose an approach to utilize chatbots as mediators of
the conversation and sources of limited and controlled generation of
explanations, to harvest the potential of LLMs while reducing their potential
risks at the same time. The proposed LLM-based chatbot supports students in
understanding learning-paths recommendations. We use a knowledge graph (KG) as
a human-curated source of information, to regulate the LLM's output through
defining its prompt's context. A group chat approach is developed to connect
students with human mentors, either on demand or in cases that exceed the
chatbot's pre-defined tasks. We evaluate the chatbot with a user study, to
provide a proof-of-concept and highlight the potential requirements and
limitations of utilizing chatbots in conversational explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.08919">
<div class="article-summary-box-inner">
<span><p>Diacritization plays a pivotal role in improving readability and
disambiguating the meaning of Arabic texts. Efforts have so far focused on
marking every eligible character (Full Diacritization). Comparatively
overlooked, Partial Diacritzation (PD) is the selection of a subset of
characters to be marked to aid comprehension where needed. Research has
indicated that excessive diacritic marks can hinder skilled readers--reducing
reading speed and accuracy. We conduct a behavioral experiment and show that
partially marked text is often easier to read than fully marked text, and
sometimes easier than plain text. In this light, we introduce
Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which
integrates seamlessly with existing Arabic diacritization systems. CCPD
processes each word twice, once with context and once without, and diacritizes
only the characters with disparities between the two inferences. Further, we
introduce novel indicators for measuring partial diacritization quality (SR,
PDER, HDER, ERE), essential for establishing this as a machine learning task.
Lastly, we introduce TD2, a Transformer-variant of an established model which
offers a markedly different performance profile on our proposed indicators
compared to all other known systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.10134">
<div class="article-summary-box-inner">
<span><p>Traffic prediction, a critical component for intelligent transportation
systems, endeavors to foresee future traffic at specific locations using
historical data. Although existing traffic prediction models often emphasize
developing complex neural network structures, their accuracy has not seen
improvements accordingly. Recently, Large Language Models (LLMs) have shown
outstanding capabilities in time series analysis. Differing from existing
models, LLMs progress mainly through parameter expansion and extensive
pre-training while maintaining their fundamental structures. In this paper, we
propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic
prediction. Specifically, ST-LLM redefines the timesteps at each location as
tokens and incorporates a spatial-temporal embedding module to learn the
spatial location and global temporal representations of tokens. Then these
representations are fused to provide each token with unified spatial and
temporal information. Furthermore, we propose a novel partially frozen
attention strategy of the LLM, which is designed to capture spatial-temporal
dependencies for traffic prediction. Comprehensive experiments on real traffic
datasets offer evidence that ST-LLM outperforms state-of-the-art models.
Notably, the ST-LLM also exhibits robust performance in both few-shot and
zero-shot prediction scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.10225">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce ChatQA, a family of conversational question
answering (QA) models that obtain GPT-4 level accuracies. Specifically, we
propose a two-stage instruction tuning method that can significantly improve
the zero-shot conversational QA results from large language models (LLMs). To
handle retrieval-augmented generation in conversational QA, we fine-tune a
dense retriever on a multi-turn QA dataset, which provides comparable results
to using the state-of-the-art query rewriting model while largely reducing
deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of
average score on 10 conversational QA datasets (54.14 vs. 53.90), without
relying on any synthetic data from OpenAI GPT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual acoustic word embeddings for zero-resource languages. (arXiv:2401.10543v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.10543">
<div class="article-summary-box-inner">
<span><p>This research addresses the challenge of developing speech applications for
zero-resource languages that lack labelled data. It specifically uses acoustic
word embedding (AWE) -- fixed-dimensional representations of variable-duration
speech segments -- employing multilingual transfer, where labelled data from
several well-resourced languages are used for pertaining. The study introduces
a new neural network that outperforms existing AWE models on zero-resource
languages. It explores the impact of the choice of well-resourced languages.
AWEs are applied to a keyword-spotting system for hate speech detection in
Swahili radio broadcasts, demonstrating robustness in real-world scenarios.
Additionally, novel semantic AWE models improve semantic query-by-example
search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.11033">
<div class="article-summary-box-inner">
<span><p>The rapid evolution of Large Language Models (LLMs) underscores the critical
importance of ethical considerations and data integrity in AI development,
emphasizing the role of FAIR (Findable, Accessible, Interoperable, Reusable)
data principles. While these principles have long been a cornerstone of ethical
data stewardship, their application in LLM training data is less prevalent, an
issue our research aims to address. Our study begins with a review of existing
literature, highlighting the significance of FAIR principles in data management
for model training. Building on this foundation, we introduce a novel framework
that incorporates FAIR principles into the LLM training process. A key aspect
of this approach is a comprehensive checklist, designed to assist researchers
and developers in consistently applying FAIR data principles throughout the
model development lifecycle. The practicality and effectiveness of our
framework are demonstrated through a case study that involves creating a
FAIR-compliant dataset to detect and reduce biases. This case study not only
validates the usefulness of our framework but also establishes new benchmarks
for more equitable, transparent, and ethical practices in LLM training. We
offer this framework to the community as a means to promote technologically
advanced, ethically sound, and socially responsible AI models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.11624">
<div class="article-summary-box-inner">
<span><p>Language models, especially pre-trained large language models, have showcased
remarkable abilities as few-shot in-context learners (ICL), adept at adapting
to new tasks with just a few demonstrations in the input context. However, the
model's ability to perform ICL is sensitive to the choice of the few-shot
demonstrations. Instead of using a fixed set of demonstrations, one recent
development is to retrieve demonstrations tailored to each input query. The
implementation of demonstration retrieval is relatively straightforward,
leveraging existing databases and retrieval systems. This not only improves the
efficiency and scalability of the learning process but also has been shown to
reduce biases inherent in manual example selection. In light of the encouraging
results and growing research in ICL with retrieved demonstrations, we conduct
an extensive review of studies in this area. In this survey, we discuss and
compare different design choices for retrieval models, retrieval training
procedures, and inference algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research. (arXiv:2401.11969v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.11969">
<div class="article-summary-box-inner">
<span><p>Automated fact-checking has drawn considerable attention over the past few
decades due to the increase in the diffusion of misinformation on online
platforms. This is often carried out as a sequence of tasks comprising (i) the
detection of sentences circulating in online platforms which constitute claims
needing verification, followed by (ii) the verification process of those
claims. This survey focuses on the former, by discussing existing efforts
towards detecting claims needing fact-checking, with a particular focus on
multilingual data and methods. This is a challenging and fertile direction
where existing methods are yet far from matching human performance due to the
profoundly challenging nature of the issue. Especially, the dissemination of
information across multiple social platforms, articulated in multiple languages
and modalities demands more generalized solutions for combating misinformation.
Focusing on multilingual misinformation, we present a comprehensive survey of
existing multilingual claim detection research. We present state-of-the-art
multilingual claim detection research categorized into three key factors of the
problem, verifiability, priority, and similarity. Further, we present a
detailed overview of the existing multilingual datasets along with the
challenges and suggest possible future advancements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization. (arXiv:2401.06980v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2401.06980">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel bilevel optimization-based training
approach to training acoustic models for automatic speech recognition (ASR)
tasks that we term {bi-level joint unsupervised and supervised training
(BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an
unsupervised loss and a supervised loss respectively, leveraging recent
advances in penalty-based bilevel optimization to solve this challenging ASR
problem with affordable complexity and rigorous convergence guarantees.} To
evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2
datasets have been conducted. BL-JUST achieves superior performance over the
commonly used pre-training followed by fine-tuning strategy.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2024-01-24 23:12:03.415992993 UTC">2024-01-24 23:12:03 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>