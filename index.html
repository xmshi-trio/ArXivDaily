<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-07-14T01:30:00Z">07-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Incomplete Utterance Rewriting as Sequential Greedy Tagging. (arXiv:2307.06337v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06337">
<div class="article-summary-box-inner">
<span><p>The task of incomplete utterance rewriting has recently gotten much
attention. Previous models struggled to extract information from the dialogue
context, as evidenced by the low restoration scores. To address this issue, we
propose a novel sequence tagging-based model, which is more adept at extracting
information from context. Meanwhile, we introduce speaker-aware embedding to
model speaker variation. Experiments on multiple public datasets show that our
model achieves optimal results on all nine restoration scores while having
other metric scores comparable to previous state-of-the-art models.
Furthermore, benefitting from the model's simplicity, our approach outperforms
most previous models on inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Acquisition of Semantic Relationships between words. (arXiv:2307.06419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06419">
<div class="article-summary-box-inner">
<span><p>The study of semantic relationships has revealed a close connection between
these relationships and the morphological characteristics of a language.
Morphology, as a subfield of linguistics, investigates the internal structure
and formation of words. By delving into the relationship between semantic
relationships and language morphology, we can gain deeper insights into how the
underlying structure of words contributes to the interpretation and
comprehension of language. This paper explores the dynamic interplay between
semantic relationships and the morphological aspects of different languages, by
examining the intricate relationship between language morphology and semantic
relationships, valuable insights can be gained regarding how the structure of
words influences language comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06435">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have shown excellent generalization capabilities
that have led to the development of numerous models. These models propose
various new architectures, tweaking existing architectures with refined
training strategies, increasing context length, using high-quality training
data, and increasing training time to outperform baselines. Analyzing new
developments is crucial for identifying changes that enhance training stability
and improve generalization in LLMs. This survey paper comprehensively analyses
the LLMs architectures and their categorization, training strategies, training
datasets, and performance evaluations and discusses future research directions.
Moreover, the paper also discusses the basic building blocks and concepts
behind LLMs, followed by a complete overview of LLMs, including their important
features and functions. Finally, the paper summarizes significant findings from
LLM research and consolidates essential architectural and training strategies
for developing advanced LLMs. Given the continuous advancements in LLMs, we
intend to regularly update this paper by incorporating new sections and
featuring the latest LLM models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events. (arXiv:2307.06439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06439">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as GPT-4, have demonstrated remarkable
capabilities across a wide range of tasks, including health applications. In
this paper, we study how LLMs can be used to scale biomedical knowledge
curation. We find that while LLMs already possess decent competency in
structuring biomedical text, by distillation into a task-specific student model
through self-supervised learning, substantial gains can be attained over
out-of-box LLMs, with additional advantages such as cost, efficiency, and
white-box model access.
</p>
<p>We conduct a case study on adverse drug event (ADE) extraction, which is an
important area for improving care. On standard ADE extraction evaluation, a
GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised
state-of-the-art models without using any labeled data. Despite being over
1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by
over 6 absolute points in F1 and GPT-4 by over 5 absolute points.
</p>
<p>Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT)
and ADE extraction architecture shed light on best practice for biomedical
knowledge extraction. Similar gains were attained by distillation for other
standard biomedical knowledge extraction tasks such as gene-disease
associations and protected health information, further illustrating the promise
of this approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06440">
<div class="article-summary-box-inner">
<span><p>The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Ability of ChatGPT to Screen Articles for Systematic Reviews. (arXiv:2307.06464v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06464">
<div class="article-summary-box-inner">
<span><p>By organizing knowledge within a research field, Systematic Reviews (SR)
provide valuable leads to steer research. Evidence suggests that SRs have
become first-class artifacts in software engineering. However, the tedious
manual effort associated with the screening phase of SRs renders these studies
a costly and error-prone endeavor. While screening has traditionally been
considered not amenable to automation, the advent of generative AI-driven
chatbots, backed with large language models is set to disrupt the field. In
this report, we propose an approach to leverage these novel technological
developments for automating the screening of SRs. We assess the consistency,
classification performance, and generalizability of ChatGPT in screening
articles for SRs and compare these figures with those of traditional
classifiers used in SR automation. Our results indicate that ChatGPT is a
viable option to automate the SR processes, but requires careful considerations
from developers when integrating ChatGPT into their SR tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06483">
<div class="article-summary-box-inner">
<span><p>Automated classifiers (ACs), often built via supervised machine learning
(SML), can categorize large, statistically powerful samples of data ranging
from text to images and video, and have become widely popular measurement
devices in communication science and related fields. Despite this popularity,
even highly accurate classifiers make errors that cause misclassification bias
and misleading results in downstream analyses-unless such analyses account for
these errors. As we show in a systematic literature review of SML applications,
communication scholars largely ignore misclassification bias. In principle,
existing statistical methods can use "gold standard" validation data, such as
that created by human annotators, to correct misclassification bias and produce
consistent estimates. We introduce and test such methods, including a new
method we design and implement in the R package misclassificationmodels, via
Monte Carlo simulations designed to reveal each method's limitations, which we
also release. Based on our results, we recommend our new error correction
method as it is versatile and efficient. In sum, automated classifiers, even
those below common accuracy standards or making systematic misclassifications,
can be useful for measurement with careful study design and appropriate error
correction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agreement Tracking for Multi-Issue Negotiation Dialogues. (arXiv:2307.06524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06524">
<div class="article-summary-box-inner">
<span><p>Automated negotiation support systems aim to help human negotiators reach
more favorable outcomes in multi-issue negotiations (e.g., an employer and a
candidate negotiating over issues such as salary, hours, and promotions before
a job offer). To be successful, these systems must accurately track agreements
reached by participants in real-time. Existing approaches either focus on
task-oriented dialogues or produce unstructured outputs, rendering them
unsuitable for this objective. Our work introduces the novel task of agreement
tracking for two-party multi-issue negotiations, which requires continuous
monitoring of agreements within a structured state space. To address the
scarcity of annotated corpora with realistic multi-issue negotiation dialogues,
we use GPT-3 to build GPT-Negochat, a synthesized dataset that we make publicly
available. We present a strong initial baseline for our task by
transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus. Pre-training
T5-small and T5-base on MultiWOZ 2.4's DST task enhances results by 21% and 9%
respectively over training solely on GPT-Negochat. We validate our method's
sample-efficiency via smaller training subset experiments. By releasing
GPT-Negochat and our baseline models, we aim to encourage further research in
multi-issue negotiation dialogue agreement tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study. (arXiv:2307.06530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06530">
<div class="article-summary-box-inner">
<span><p>This paper explores the integration of Large Language Models (LLMs) into
Automatic Speech Recognition (ASR) systems to improve transcription accuracy.
The increasing sophistication of LLMs, with their in-context learning
capabilities and instruction-following behavior, has drawn significant
attention in the field of Natural Language Processing (NLP). Our primary focus
is to investigate the potential of using an LLM's in-context learning
capabilities to enhance the performance of ASR systems, which currently face
challenges such as ambient noise, speaker accents, and complex linguistic
contexts. We designed a study using the Aishell-1 and LibriSpeech datasets,
with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.
Unfortunately, our initial experiments did not yield promising results,
indicating the complexity of leveraging LLM's in-context learning for ASR
applications. Despite further exploration with varied settings and models, the
corrected sentences from the LLMs frequently resulted in higher Word Error
Rates (WER), demonstrating the limitations of LLMs in speech applications. This
paper provides a detailed overview of these experiments, their results, and
implications, establishing that using LLMs' in-context learning capabilities to
correct potential errors in speech recognition transcriptions is still a
challenging task at the current stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach. (arXiv:2307.06540v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06540">
<div class="article-summary-box-inner">
<span><p>This study addressed the complex task of sentiment analysis on a dataset of
119,988 original tweets from Weibo using a Convolutional Neural Network (CNN),
offering a new approach to Natural Language Processing (NLP). The data, sourced
from Baidu's PaddlePaddle AI platform, were meticulously preprocessed,
tokenized, and categorized based on sentiment labels. A CNN-based model was
utilized, leveraging word embeddings for feature extraction, and trained to
perform sentiment classification. The model achieved a macro-average F1-score
of approximately 0.73 on the test set, showing balanced performance across
positive, neutral, and negative sentiments. The findings underscore the
effectiveness of CNNs for sentiment analysis tasks, with implications for
practical applications in social media analysis, market research, and policy
studies. The complete experimental content and code have been made publicly
available on the Kaggle data platform for further research and development.
Future work may involve exploring different architectures, such as Recurrent
Neural Networks (RNN) or transformers, or using more complex pre-trained models
like BERT, to further improve the model's ability to understand linguistic
nuances and context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06576">
<div class="article-summary-box-inner">
<span><p>Precisely recommending candidate news articles to users has always been a
core challenge for personalized news recommendation systems. Most recent works
primarily focus on using advanced natural language processing techniques to
extract semantic information from rich textual data, employing content-based
methods derived from local historical news. However, this approach lacks a
global perspective, failing to account for users' hidden motivations and
behaviors beyond semantic information. To address this challenge, we propose a
novel model called GLORY (Global-LOcal news Recommendation sYstem), which
combines global representations learned from other users with local
representations to enhance personalized recommendation systems. We accomplish
this by constructing a Global-aware Historical News Encoder, which includes a
global news graph and employs gated graph neural networks to enrich news
representations, thereby fusing historical news representations by a historical
news aggregator. Similarly, we extend this approach to a Global Candidate News
Encoder, utilizing a global entity graph and a candidate news aggregator to
enhance candidate news representation. Evaluation results on two public news
datasets demonstrate that our method outperforms existing approaches.
Furthermore, our model offers more diverse recommendations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parmesan: mathematical concept extraction for education. (arXiv:2307.06699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06699">
<div class="article-summary-box-inner">
<span><p>Mathematics is a highly specialized domain with its own unique set of
challenges that has seen limited study in natural language processing. However,
mathematics is used in a wide variety of fields and multidisciplinary research
in many different domains often relies on an understanding of mathematical
concepts. To aid researchers coming from other fields, we develop a prototype
system for searching for and defining mathematical concepts in context,
focusing on the field of category theory. This system, Parmesan, depends on
natural language processing components including concept extraction, relation
extraction, definition extraction, and entity linking. In developing this
system, we show that existing techniques cannot be applied directly to the
category theory domain, and suggest hybrid techniques that do perform well,
though we expect the system to evolve over time. We also provide two cleaned
mathematical corpora that power the prototype system, which are based on
journal articles and wiki pages, respectively. The corpora have been annotated
with dependency trees, lemmas, and part-of-speech tags.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues. (arXiv:2307.06703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06703">
<div class="article-summary-box-inner">
<span><p>Answer selection in open-domain dialogues aims to select an accurate answer
from candidates. Recent success of answer selection models hinges on training
with large amounts of labeled data. However, collecting large-scale labeled
data is labor-intensive and time-consuming. In this paper, we introduce the
predicted intent labels to calibrate answer labels in a self-training paradigm.
Specifically, we propose the intent-calibrated self-training (ICAST) to improve
the quality of pseudo answer labels through the intent-calibrated answer
selection paradigm, in which we employ pseudo intent labels to help improve
pseudo answer labels. We carry out extensive experiments on two benchmark
datasets with open-domain dialogues. The experimental results show that ICAST
outperforms baselines consistently with 1%, 5% and 10% labeled data.
Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets,
compared with the strongest baseline with only 5% labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To share or not to share: What risks would laypeople accept to give sensitive data to differentially-private NLP systems?. (arXiv:2307.06708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06708">
<div class="article-summary-box-inner">
<span><p>Although the NLP community has adopted central differential privacy as a
go-to framework for privacy-preserving model training or data sharing, the
choice and interpretation of the key parameter, privacy budget $\varepsilon$
that governs the strength of privacy protection, remains largely arbitrary. We
argue that determining the $\varepsilon$ value should not be solely in the
hands of researchers or system developers, but must also take into account the
actual people who share their potentially sensitive data. In other words: Would
you share your instant messages for $\varepsilon$ of 10? We address this
research gap by designing, implementing, and conducting a behavioral experiment
(311 lay participants) to study the behavior of people in uncertain
decision-making situations with respect to privacy-threatening situations.
Framing the risk perception in terms of two realistic NLP scenarios and using a
vignette behavioral study help us determine what $\varepsilon$ thresholds would
lead lay people to be willing to share sensitive textual data - to our
knowledge, the first study of its kind.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06713">
<div class="article-summary-box-inner">
<span><p>A wide variety of natural language tasks are currently being addressed with
large-scale language models (LLMs). These models are usually trained with a
very large amount of unsupervised text data and adapted to perform a downstream
natural language task using methods like fine-tuning, calibration or in-context
learning. In this work, we propose an approach to adapt the prior class
distribution to perform text classification tasks without the need for labelled
samples and only few in-domain sample queries. The proposed approach treats the
LLM as a black box, adding a stage where the model posteriors are calibrated to
the task. Results show that these methods outperform the un-adapted model for
different number of training shots in the prompt and a previous approach were
calibration is performed without using any adaptation data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative. (arXiv:2307.06721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06721">
<div class="article-summary-box-inner">
<span><p>Dialog policies, which determine a system's action based on the current state
at each dialog turn, are crucial to the success of the dialog. In recent years,
reinforcement learning (RL) has emerged as a promising option for dialog policy
learning (DPL). In RL-based DPL, dialog policies are updated according to
rewards. The manual construction of fine-grained rewards, such as
state-action-based ones, to effectively guide the dialog policy is challenging
in multi-domain task-oriented dialog scenarios with numerous state-action pair
combinations. One way to estimate rewards from collected data is to train the
reward estimator and dialog policy simultaneously using adversarial learning
(AL). Although this method has demonstrated superior performance
experimentally, it is fraught with the inherent problems of AL, such as mode
collapse. This paper first identifies the role of AL in DPL through detailed
analyses of the objective functions of dialog policy and reward estimator.
Next, based on these analyses, we propose a method that eliminates AL from
reward estimation and DPL while retaining its advantages. We evaluate our
method using MultiWOZ, a multi-domain task-oriented dialog corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06775">
<div class="article-summary-box-inner">
<span><p>Over the last decade, there has been a vast increase in eating disorder
diagnoses and eating disorder-attributed deaths, reaching their zenith during
the Covid-19 pandemic. This immense growth derived in part from the stressors
of the pandemic but also from increased exposure to social media, which is rife
with content that promotes eating disorders. Such content can induce eating
disorders in viewers. This study aimed to create a multimodal deep learning
model capable of determining whether a given social media post promotes eating
disorders based on a combination of visual and textual data. A labeled dataset
of Tweets was collected from Twitter, upon which twelve deep learning models
were trained and tested. Based on model performance, the most effective deep
learning model was the multimodal fusion of the RoBERTa natural language
processing model and the MaxViT image classification model, attaining accuracy
and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
model, deployed to classify an unlabeled dataset of posts from the social media
sites Tumblr and Reddit, generated similar classifications as previous research
studies that did not employ artificial intelligence, showing that artificial
intelligence can develop insights congruent to those of researchers.
Additionally, the model was used to conduct a time-series analysis of yet
unseen Tweets from eight Twitter hashtags, uncovering that the relative
abundance of pro-eating disorder content has decreased drastically. However,
since approximately 2018, pro-eating disorder content has either stopped its
decline or risen once more in ampleness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negated Complementary Commonsense using Large Language Models. (arXiv:2307.06794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06794">
<div class="article-summary-box-inner">
<span><p>Larger language models, such as GPT-3, have shown to be excellent in many
tasks. However, we demonstrate that out-of-ordinary questions can throw the
model off guard. This work focuses on finding answers to negated complementary
questions in commonsense scenarios. We illustrate how such questions adversely
affect the model responses. We propose a model-agnostic methodology to improve
the performance in negated complementary scenarios. Our method outperforms
few-shot generation from GPT-3 (by more than 11 points) and, more importantly,
highlights the significance of studying the response of large language models
in negated complementary questions. The code, data, and experiments are
available under: https://github.com/navidre/negated_complementary_commonsense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalization for BERT-based Discriminative Speech Recognition Rescoring. (arXiv:2307.06832v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06832">
<div class="article-summary-box-inner">
<span><p>Recognition of personalized content remains a challenge in end-to-end speech
recognition. We explore three novel approaches that use personalized content in
a neural rescoring step to improve recognition: gazetteers, prompting, and a
cross-attention based encoder-decoder model. We use internal de-identified
en-US data from interactions with a virtual voice assistant supplemented with
personalized named entities to compare these approaches. On a test set with
personalized named entities, we show that each of these approaches improves
word error rate by over 10%, against a neural rescoring baseline. We also show
that on this test set, natural language prompts can improve word error rate by
7% without any training and with a marginal loss in generalization. Overall,
gazetteers were found to perform the best with a 10% improvement in word error
rate (WER), while also improving WER on a general test set by 1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Garbage in, garbage out: Zero-shot detection of crime using Large Language Models. (arXiv:2307.06844v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06844">
<div class="article-summary-box-inner">
<span><p>This paper proposes exploiting the common sense knowledge learned by large
language models to perform zero-shot reasoning about crimes given textual
descriptions of surveillance videos. We show that when video is (manually)
converted to high quality textual descriptions, large language models are
capable of detecting and classifying crimes with state-of-the-art performance
using only zero-shot reasoning. However, existing automated video-to-text
approaches are unable to generate video descriptions of sufficient quality to
support reasoning (garbage video descriptions into the large language model,
garbage out).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06857">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel approach for improving the quality and
consistency of generated outputs from large-scale pre-trained language models
(LLMs). Self-consistency has emerged as an effective approach for prompts with
fixed answers, selecting the answer with the highest number of votes. In this
paper, we introduce a generalized framework for self-consistency that extends
its applicability beyond problems that have fixed-answer answers. Through
extensive simulations, we demonstrate that our approach consistently recovers
the optimal or near-optimal generation from a set of candidates. We also
propose lightweight parameter-free similarity functions that show significant
and consistent improvements across code generation, autoformalization, and
summarization tasks, even without access to token log probabilities. Our method
incurs minimal computational overhead, requiring no auxiliary reranker models
or modifications to the existing model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06865">
<div class="article-summary-box-inner">
<span><p>The generations of large language models are commonly controlled through
prompting techniques, where a user's query to the model is prefixed with a
prompt that aims to guide the model's behaviour on the query. The prompts used
by companies to guide their models are often treated as secrets, to be hidden
from the user making the query. They have even been treated as commodities to
be bought and sold. However, there has been anecdotal evidence showing that the
prompts can be extracted by a user even when they are kept secret. In this
paper, we present a framework for systematically measuring the success of
prompt extraction attacks. In experiments with multiple sources of prompts and
multiple underlying language models, we find that simple text-based attacks can
in fact reveal prompts with high probability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering. (arXiv:2307.06869v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06869">
<div class="article-summary-box-inner">
<span><p>Existing evaluation metrics for natural language generation (NLG) tasks face
the challenges on generalization ability and interpretability. Specifically,
most of the well-performed metrics are required to train on evaluation datasets
of specific NLG tasks and evaluation dimensions, which may cause over-fitting
to task-specific datasets. Furthermore, existing metrics only provide an
evaluation score for each dimension without revealing the evidence to interpret
how this score is obtained. To deal with these challenges, we propose a simple
yet effective metric called DecompEval. This metric formulates NLG evaluation
as an instruction-style question answering task and utilizes instruction-tuned
pre-trained language models (PLMs) without training on evaluation datasets,
aiming to enhance the generalization ability. To make the evaluation process
more interpretable, we decompose our devised instruction-style question about
the quality of generated texts into the subquestions that measure the quality
of each sentence. The subquestions with their answers generated by PLMs are
then recomposed as evidence to obtain the evaluation result. Experimental
results show that DecompEval achieves state-of-the-art performance in untrained
metrics for evaluating text summarization and dialogue generation, which also
exhibits strong dimension-level / task-level generalization ability and
interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06908">
<div class="article-summary-box-inner">
<span><p>Before deploying a language model (LM) within a given domain, it is important
to measure its tendency to generate factually incorrect information in that
domain. Existing factual generation evaluation methods focus on facts sampled
from the LM itself, and thus do not control the set of evaluated facts and
might under-represent rare and unlikely facts. We propose FACTOR: Factual
Assessment via Corpus TransfORmation, a scalable approach for evaluating LM
factuality. FACTOR automatically transforms a factual corpus of interest into a
benchmark evaluating an LM's propensity to generate true facts from the corpus
vs. similar but incorrect statements. We use our framework to create two
benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores
increase with model size and improve when the LM is augmented with retrieval;
(ii) benchmark score correlates with perplexity, but the two metrics do not
always agree on model ranking; and (iii) when perplexity and benchmark score
disagree, the latter better reflects factuality in open-ended generation, as
measured by human annotators. We make our data and code publicly available in
https://github.com/AI21Labs/factor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT. (arXiv:2307.06917v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06917">
<div class="article-summary-box-inner">
<span><p>Knowledge Graphs (KG) provide us with a structured, flexible, transparent,
cross-system, and collaborative way of organizing our knowledge and data across
various domains in society and industrial as well as scientific disciplines.
KGs surpass any other form of representation in terms of effectiveness.
However, Knowledge Graph Engineering (KGE) requires in-depth experiences of
graph structures, web technologies, existing models and vocabularies, rule
sets, logic, as well as best practices. It also demands a significant amount of
work. Considering the advancements in large language models (LLMs) and their
interfaces and applications in recent years, we have conducted comprehensive
experiments with ChatGPT to explore its potential in supporting KGE. In this
paper, we present a selection of these experiments and their results to
demonstrate how ChatGPT can assist us in the development and management of KGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06924">
<div class="article-summary-box-inner">
<span><p>Persons with visual impairments (PwVI) have difficulties understanding and
navigating spaces around them. Current wayfinding technologies either focus
solely on navigation or provide limited communication about the environment.
Motivated by recent advances in visual-language grounding and semantic
navigation, we propose DRAGON, a guiding robot powered by a dialogue system and
the ability to associate the environment with natural language. By
understanding the commands from the user, DRAGON is able to guide the user to
the desired landmarks on the map, describe the environment, and answer
questions from visual observations. Through effective utilization of dialogue,
the robot can ground the user's free-form descriptions to landmarks in the
environment, and give the user semantic information through spoken language. We
conduct a user study with blindfolded participants in an everyday indoor
environment. Our results demonstrate that DRAGON is able to communicate with
the user smoothly, provide a good guiding experience, and connect users with
their surrounding environment in an intuitive manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06930">
<div class="article-summary-box-inner">
<span><p>Modular vision-language models (Vision-LLMs) align pretrained image encoders
with (pretrained) large language models (LLMs), representing a computationally
much more efficient alternative to end-to-end training of large vision-language
models from scratch, which is prohibitively expensive for most. Vision-LLMs
instead post-hoc condition LLMs to `understand' the output of an image encoder.
With the abundance of readily available high-quality English image-text data as
well as monolingual English LLMs, the research focus has been on English-only
Vision-LLMs. Multilingual vision-language models are still predominantly
obtained via expensive end-to-end pretraining, resulting in comparatively
smaller models, trained on limited multilingual image data supplemented with
text-only multilingual corpora. In this work, we present mBLIP, the first
multilingual Vision-LLM, which we obtain in a computationally efficient manner
-- on consumer hardware using only a few million training examples -- by
leveraging a pretrained multilingual LLM. To this end, we \textit{re-align} an
image encoder previously tuned to an English LLM to a new, multilingual LLM --
for this, we leverage multilingual data from a mix of vision-and-language
tasks, which we obtain by machine-translating high-quality English data to 95
languages. On the IGLUE benchmark, mBLIP yields results competitive with
state-of-the-art models. Moreover, in image captioning on XM3600, mBLIP
(zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to
these very large multilingual vision-language models trained from scratch, we
obtain mBLIP by training orders of magnitude fewer parameters on magnitudes
less data. We release our model and code at
\url{https://github.com/gregor-ge/mBLIP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06945">
<div class="article-summary-box-inner">
<span><p>We propose the In-context Autoencoder (ICAE) for context compression in a
large language model (LLM). The ICAE has two modules: a learnable encoder
adapted with LoRA from an LLM for compressing a long context into a limited
number of memory slots, and a fixed decoder which is the target LLM that can
condition on the memory slots for various purposes. We first pretrain the ICAE
using both autoencoding and language modeling objectives on massive text data,
enabling it to generate memory slots that accurately and comprehensively
represent the original context. Then, we fine-tune the pretrained ICAE on a
small amount of instruct data to enhance its interaction with various prompts
for producing desirable responses. Our experimental results demonstrate that
the ICAE learned with our proposed pretraining and fine-tuning paradigm can
effectively produce memory slots with $4\times$ context compression, which can
be well conditioned on by the target LLM to respond to various prompts. The
promising results demonstrate significant implications of the ICAE for its
novel approach to the long context problem and its potential to reduce
computation and memory overheads for LLM inference in practice, suggesting
further research effort in context management for an LLM. Our code and data
will be released shortly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation. (arXiv:2305.07457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07457">
<div class="article-summary-box-inner">
<span><p>Quality Estimation (QE) is the task of predicting the quality of Machine
Translation (MT) system output, without using any gold-standard translation
references. State-of-the-art QE models are supervised: they require
human-labeled quality of some MT system output on some datasets for training,
making them domain-dependent and MT-system-dependent. There has been research
on unsupervised QE, which requires glass-box access to the MT systems, or
parallel MT data to generate synthetic errors for training QE models. In this
paper, we present Perturbation-based QE - a word-level Quality Estimation
approach that works simply by analyzing MT system output on perturbed input
source sentences. Our approach is unsupervised, explainable, and can evaluate
any type of blackbox MT systems, including the currently prominent large
language models (LLMs) with opaque internal processes. For language directions
with no labeled QE data, our approach has similar or better performance than
the zero-shot supervised approach on the WMT21 shared task. Our approach is
better at detecting gender bias and word-sense-disambiguation errors in
translation than supervised QE, indicating its robustness to out-of-domain
usage. The performance gap is larger when detecting errors on a nontraditional
translation-prompting LLM, indicating that our approach is more generalizable
to different MT systems. We give examples demonstrating our approach's
explainability power, where it shows which input source words have influence on
a certain MT output word.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Small Language Models on PubMedQA via Generative Data Augmentation. (arXiv:2305.07804v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.07804">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made remarkable advancements in the field
of natural language processing. However, their increasing size poses challenges
in terms of computational cost. On the other hand, Small Language Models (SLMs)
are known for their efficiency, but they often struggle with limited capacity
and training data, especially in specific domains. In this paper, we introduce
a novel method aimed at improving SLMs in the medical domain using LLM-based
generative data augmentation. The objective of our approach is to develop more
efficient and capable models that are specifically tailored for specialized
applications. Through experiments conducted on the PubMedQA dataset, we
demonstrate the effectiveness of LLMs in refining and diversifying existing
question-answer pairs. This refinement process leads to improved performance in
a significantly smaller model after fine-tuning. Notably, our best SLM, with
under 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA
dataset. Our code and generated data are publicly available to facilitate
further explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.10400">
<div class="article-summary-box-inner">
<span><p>Automatically determining whether a text and a corresponding image are
semantically aligned is a significant challenge for vision-language models,
with applications in generative text-to-image and image-to-text tasks. In this
work, we study methods for automatic text-image alignment evaluation. We first
introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets
from both text-to-image and image-to-text generation tasks, with human
judgements for whether a given text-image pair is semantically aligned. We then
describe two automatic methods to determine alignment: the first involving a
pipeline based on question generation and visual question answering models, and
the second employing an end-to-end classification approach by finetuning
multimodal pretrained models. Both methods surpass prior approaches in various
text-image alignment tasks, with significant improvements in challenging cases
that involve complex composition or unnatural images. Finally, we demonstrate
how our approaches can localize specific misalignments between an image and a
given text, and how they can be used to automatically re-rank candidates in
text-to-image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.07848">
<div class="article-summary-box-inner">
<span><p>Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kosmos-2: Grounding Multimodal Large Language Models to the World. (arXiv:2306.14824v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.14824">
<div class="article-summary-box-inner">
<span><p>We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new
capabilities of perceiving object descriptions (e.g., bounding boxes) and
grounding text to the visual world. Specifically, we represent refer
expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where
object descriptions are sequences of location tokens. Together with multimodal
corpora, we construct large-scale data of grounded image-text pairs (called
GrIT) to train the model. In addition to the existing capabilities of MLLMs
(e.g., perceiving general modalities, following instructions, and performing
in-context learning), Kosmos-2 integrates the grounding capability into
downstream applications. We evaluate Kosmos-2 on a wide range of tasks,
including (i) multimodal grounding, such as referring expression comprehension,
and phrase grounding, (ii) multimodal referring, such as referring expression
generation, (iii) perception-language tasks, and (iv) language understanding
and generation. This work lays out the foundation for the development of
Embodiment AI and sheds light on the big convergence of language, multimodal
perception, action, and world modeling, which is a key step toward artificial
general intelligence. Code and pretrained models are available at
https://aka.ms/kosmos-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.00470">
<div class="article-summary-box-inner">
<span><p>Large language models(LLMS) have shown excellent text generation
capabilities,capable of generating fluent responses for many downstream tasks.
However,applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To address the above challenges,this paper
proposes PatternGPT, a pattern-driven text generation framework for large
language models. First,the framework utilizes the extraction capabilities of
large language models to generate rich and diverse patterns and later draws on
the idea of federated learning. Using multiple agents to achieve sharing to
obtain more diverse patterns. Finally, it searches for high-quality patterns
using judgment criteria and optimization algorithms and uses the searched
patterns to guide the model for generation. This framework has the advantages
of generating diversified patterns, protecting data privacy,combining external
knowledge, and improving the quality of generation, which provides an effective
method to optimize the text generation capability of large language models,and
make it better applied to the field of intelligent dialogue and content
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03109">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03875">
<div class="article-summary-box-inner">
<span><p>Supply chain operations traditionally involve a variety of complex decision
making problems. Over the last few decades, supply chains greatly benefited
from advances in computation, which allowed the transition from manual
processing to automation and cost-effective optimization. Nonetheless, business
operators still need to spend substantial efforts in explaining and
interpreting the optimization outcomes to stakeholders. Motivated by the recent
advances in Large Language Models (LLMs), we study how this disruptive
technology can help bridge the gap between supply chain automation and human
comprehension and trust thereof. We design OptiGuide -- a framework that
accepts as input queries in plain text, and outputs insights about the
underlying optimization outcomes. Our framework does not forgo the
state-of-the-art combinatorial optimization technology, but rather leverages it
to quantitatively answer what-if scenarios (e.g., how would the cost change if
we used supplier B instead of supplier A for a given demand?). Importantly, our
design does not require sending proprietary data over to LLMs, which can be a
privacy concern in some circumstances. We demonstrate the effectiveness of our
framework on a real server placement scenario within Microsoft's cloud supply
chain. Along the way, we develop a general evaluation benchmark, which can be
used to evaluate the accuracy of the LLM output in other scenarios.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-07-16 23:12:11.590255700 UTC">2023-07-16 23:12:11 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>