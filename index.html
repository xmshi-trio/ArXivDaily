<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-09-06T01:30:00Z">09-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinformation Concierge: A Proof-of-Concept with Curated Twitter Dataset on COVID-19 Vaccination. (arXiv:2309.00639v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00639">
<div class="article-summary-box-inner">
<span><p>We demonstrate the Misinformation Concierge, a proof-of-concept that provides
actionable intelligence on misinformation prevalent in social media.
Specifically, it uses language processing and machine learning tools to
identify subtopics of discourse and discern non/misleading posts; presents
statistical reports for policy-makers to understand the big picture of
prevalent misinformation in a timely manner; and recommends rebuttal messages
for specific pieces of misinformation, identified from within the corpus of
data - providing means to intervene and counter misinformation promptly. The
Misinformation Concierge proof-of-concept using a curated dataset is accessible
at: https://demo-frontend-uy34.onrender.com/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Mathematical Concepts with Large Language Models. (arXiv:2309.00642v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00642">
<div class="article-summary-box-inner">
<span><p>We extract mathematical concepts from mathematical text using generative
large language models (LLMs) like ChatGPT, contributing to the field of
automatic term extraction (ATE) and mathematical text processing, and also to
the study of LLMs themselves. Our work builds on that of others in that we aim
for automatic extraction of terms (keywords) in one mathematical field,
category theory, using as a corpus the 755 abstracts from a snapshot of the
online journal "Theory and Applications of Categories", circa 2020. Where our
study diverges from previous work is in (1) providing a more thorough analysis
of what makes mathematical term extraction a difficult problem to begin with;
(2) paying close attention to inter-annotator disagreements; (3) providing a
set of guidelines which both human and machine annotators could use to
standardize the extraction process; (4) introducing a new annotation tool to
help humans with ATE, applicable to any mathematical field and even beyond
mathematics; (5) using prompts to ChatGPT as part of the extraction process,
and proposing best practices for such prompts; and (6) raising the question of
whether ChatGPT could be used as an annotator on the same level as human
experts. Our overall findings are that the matter of mathematical ATE is an
interesting field which can benefit from participation by LLMs, but LLMs
themselves cannot at this time surpass human performance on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice. (arXiv:2309.00649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00649">
<div class="article-summary-box-inner">
<span><p>We assess the ability of GPT -- a large language model -- to serve as a
financial robo-advisor for the masses, by using a financial literacy test.
Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial
literacy test, respectively, compared to a baseline of 33%. However, ChatGPT
based on GPT-4 achieves a near-perfect 99% score, pointing to financial
literacy becoming an emergent ability of state-of-the-art models. We use the
Judge-Advisor System and a savings dilemma to illustrate how researchers might
assess advice-utilization from large language models. We also present a number
of directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taken out of context: On measuring situational awareness in LLMs. (arXiv:2309.00667v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00667">
<div class="article-summary-box-inner">
<span><p>We aim to better understand the emergence of `situational awareness' in large
language models (LLMs). A model is situationally aware if it's aware that it's
a model and can recognize whether it's currently in testing or deployment.
Today's LLMs are tested for safety and alignment before they are deployed. An
LLM could exploit situational awareness to achieve a high score on safety
tests, while taking harmful actions after deployment. Situational awareness may
emerge unexpectedly as a byproduct of model scaling. One way to better foresee
this emergence is to run scaling experiments on abilities necessary for
situational awareness. As such an ability, we propose `out-of-context
reasoning' (in contrast to in-context learning). We study out-of-context
reasoning experimentally. First, we finetune an LLM on a description of a test
while providing no examples or demonstrations. At test time, we assess whether
the model can pass the test. To our surprise, we find that LLMs succeed on this
out-of-context reasoning task. Their success is sensitive to the training setup
and only works when we apply data augmentation. For both GPT-3 and LLaMA-1,
performance improves with model size. These findings offer a foundation for
further empirical study, towards predicting and potentially controlling the
emergence of situational awareness in LLMs. Code is available at:
https://github.com/AsaCooperStickland/situational-awareness-evals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00723">
<div class="article-summary-box-inner">
<span><p>This paper studies contextual biasing with Large Language Models (LLMs),
where during second-pass rescoring additional contextual information is
provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We
propose to leverage prompts for a LLM without fine tuning during rescoring
which incorporate a biasing list and few-shot examples to serve as additional
information when calculating the score for the hypothesis. In addition to
few-shot prompt learning, we propose multi-task training of the LLM to predict
both the entity class and the next token. To improve the efficiency for
contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we
propose dynamic prompting, where we select the most likely class using the
class tag prediction, and only use entities in this class as contexts for next
token prediction. Word Error Rate (WER) evaluation is performed on i) an
internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli
dataset. Results indicate that biasing lists and few-shot examples can achieve
17.8% and 9.6% relative improvement compared to first pass ASR, and that
multi-task training and dynamic prompting can achieve 20.0% and 11.3% relative
WER improvement, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How trial-to-trial learning shapes mappings in the mental lexicon: Modelling Lexical Decision with Linear Discriminative Learning. (arXiv:2207.00430v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00430">
<div class="article-summary-box-inner">
<span><p>Trial-to-trial effects have been found in a number of studies, indicating
that processing a stimulus influences responses in subsequent trials. A special
case are priming effects which have been modelled successfully with
error-driven learning (Marsolek, 2008), implying that participants are
continuously learning during experiments. This study investigates whether
trial-to-trial learning can be detected in an unprimed lexical decision
experiment. We used the Discriminative Lexicon Model (DLM; Baayen et al.,
2019), a model of the mental lexicon with meaning representations from
distributional semantics, which models error-driven incremental learning with
the Widrow-Hoff rule. We used data from the British Lexicon Project (BLP;
Keuleers et al., 2012) and simulated the lexical decision experiment with the
DLM on a trial-by-trial basis for each subject individually. Then, reaction
times were predicted with Generalised Additive Models (GAMs), using measures
derived from the DLM simulations as predictors. We extracted measures from two
simulations per subject (one with learning updates between trials and one
without), and used them as input to two GAMs. Learning-based models showed
better model fit than the non-learning ones for the majority of subjects. Our
measures also provide insights into lexical processing and individual
differences. This demonstrates the potential of the DLM to model behavioural
data and leads to the conclusion that trial-to-trial learning can indeed be
detected in unprimed lexical decision. Our results support the possibility that
our lexical knowledge is subject to continuous changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combing for Credentials: Active Pattern Extraction from Smart Reply. (arXiv:2207.10802v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10802">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models, such as GPT\nobreakdash-2 and BERT, are
often fine-tuned to achieve state-of-the-art performance on a downstream task.
One natural example is the ``Smart Reply'' application where a pre-trained
model is tuned to provide suggested responses for a given query message. Since
the tuning data is often sensitive data such as emails or chat transcripts, it
is important to understand and mitigate the risk that the model leaks its
tuning data. We investigate potential information leakage vulnerabilities in a
typical Smart Reply pipeline. We consider a realistic setting where the
adversary can only interact with the underlying model through a front-end
interface that constrains what types of queries can be sent to the model.
Previous attacks do not work in these settings, but require the ability to send
unconstrained queries directly to the model. Even when there are no constraints
on the queries, previous attacks typically require thousands, or even millions,
of queries to extract useful information, while our attacks can extract
sensitive data in just a handful of queries. We introduce a new type of active
extraction attack that exploits canonical patterns in text containing sensitive
data. We show experimentally that it is possible for an adversary to extract
sensitive user information present in the training data, even in realistic
settings where all interactions with the model must go through a front-end that
limits the types of queries. We explore potential mitigation strategies and
demonstrate empirically how differential privacy appears to be a reasonably
effective defense mechanism to such pattern extraction attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.08142">
<div class="article-summary-box-inner">
<span><p>Mathematical notation makes up a large portion of STEM literature, yet
finding semantic representations for formulae remains a challenging problem.
Because mathematical notation is precise, and its meaning changes significantly
with small character shifts, the methods that work for natural text do not
necessarily work well for mathematical expressions. This work describes an
approach for representing mathematical expressions in a continuous vector
space. We use the encoder of a sequence-to-sequence architecture, trained on
visually different but mathematically equivalent expressions, to generate
vector representations (or embeddings). We compare this approach with a
structural approach that considers visual layout to embed an expression and
show that our proposed approach is better at capturing mathematical semantics.
Finally, to expedite future research, we publish a corpus of equivalent
transcendental and algebraic expression pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work. (arXiv:2211.10443v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.10443">
<div class="article-summary-box-inner">
<span><p>Substance use, substance use disorder, and overdoses related to substance use
are major public health problems globally and in the United States. A key
aspect of addressing these problems from a public health standpoint is improved
surveillance. Traditional surveillance systems are laggy, and social media are
potentially useful sources of timely data. However, mining knowledge from
social media is challenging, and requires the development of advanced
artificial intelligence, specifically natural language processing (NLP) and
machine learning methods. We developed a sophisticated end-to-end pipeline for
mining information about nonmedical prescription medication use from social
media, namely Twitter and Reddit. Our pipeline employs supervised machine
learning and NLP for filtering out noise and characterizing the chatter. In
this paper, we describe our end-to-end pipeline developed over four years. In
addition to describing our data mining infrastructure, we discuss existing
challenges in social media mining for toxicovigilance, and possible future
research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation. (arXiv:2212.10313v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10313">
<div class="article-summary-box-inner">
<span><p>Multimodal machine translation (MMT) aims to improve translation quality by
incorporating information from other modalities, such as vision. Previous MMT
systems mainly focus on better access and use of visual information and tend to
validate their methods on image-related datasets. These studies face two
challenges. First, they can only utilize triple data (bilingual texts with
images), which is scarce; second, current benchmarks are relatively restricted
and do not correspond to realistic scenarios. Therefore, this paper
correspondingly establishes new methods and new datasets for MMT. First, we
propose a framework 2/3-Triplet with two new approaches to enhance MMT by
utilizing large-scale non-triple data: monolingual image-text data and parallel
text-only data. Second, we construct an English-Chinese {e}-commercial
{m}ulti{m}odal {t}ranslation dataset (including training and testing), named
EMMT, where its test set is carefully selected as some words are ambiguous and
shall be translated mistakenly without the help of images. Experiments show
that our method is more suitable for real-world scenarios and can significantly
improve translation performance by using more non-triple data. In addition, our
model also rivals various SOTA models in conventional multimodal translation
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07189">
<div class="article-summary-box-inner">
<span><p>Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification, and the datasets for evaluation are
relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have corresponding KB entities
by matching them to a special NIL entity. To better utilize BERT, we propose
new techniques including NIL entity representation and classification, with
synonym enhancement. We also apply KB Pruning and Versioning strategies to
automatically construct out-of-KB datasets from common in-KB EL datasets.
Results on five datasets of clinical notes, biomedical publications, and
Wikipedia articles in various domains show the advantages of BLINKout over
existing methods to identify out-of-KB mentions for the medical ontologies,
UMLS, SNOMED CT, and the general KB, WikiData.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models. (arXiv:2302.07371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.07371">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (PLMs) harbor inherent social biases that can
result in harmful real-world implications. Such social biases are measured
through the probability values that PLMs output for different social groups and
attributes appearing in a set of test sentences. However, bias testing is
currently cumbersome since the test sentences are generated either from a
limited set of manual templates or need expensive crowd-sourcing. We instead
propose using ChatGPT for controllable generation of test sentences, given any
arbitrary user-specified combination of social groups and attributes appearing
in the test sentences. When compared to template-based methods, our approach
using ChatGPT for test sentence generation is superior in detecting social
bias, especially in challenging settings such as intersectional biases. We
present an open-source comprehensive bias testing framework (BiasTestGPT),
hosted on HuggingFace, that can be plugged into any open-source PLM for bias
testing. We provide a large diverse dataset of test sentences generated by
ChatGPT that satisfies the specified social group and attribute requirements
and matches the quality of human-generated sentences. We thus enable seamless
open-ended social bias testing of PLMs through an automatic large-scale
generation of diverse test sentences for any combination of social categories
and attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering. (arXiv:2303.12671v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12671">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) is a task that requires computers to give
correct answers for the input questions based on the images. This task can be
solved by humans with ease but is a challenge for computers. The
VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the
multilingual domain on a newly released dataset: UIT-EVJVQA, in which the
questions and answers are written in three different languages: English,
Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence
learning task, in which we integrated hints from pre-trained state-of-the-art
VQA models and image features with Convolutional Sequence-to-Sequence network
to generate the desired answers. Our results obtained up to 0.3442 by F1 score
on the public test set, 0.4210 on the private test set, and placed 3rd in the
competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission. (arXiv:2303.15049v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.15049">
<div class="article-summary-box-inner">
<span><p>We present the InterviewBot that dynamically integrates conversation history
and customized topics into a coherent embedding space to conduct 10 mins
hybrid-domain (open and closed) conversations with foreign students applying to
U.S. colleges for assessing their academic and cultural readiness. To build a
neural-based end-to-end dialogue model, 7,361 audio recordings of
human-to-human interviews are automatically transcribed, where 440 are manually
corrected for finetuning and evaluation. To overcome the input/output size
limit of a transformer-based encoder-decoder model, two new methods are
proposed, context attention and topic storing, allowing the model to make
relevant and consistent interactions. Our final model is tested both
statistically by comparing its responses to the interview data and dynamically
by inviting professional interviewers and various students to interact with it
in real-time, finding it highly satisfactory in fluency and context awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.06875">
<div class="article-summary-box-inner">
<span><p>As language models scale up, it becomes increasingly expensive to verify
research ideas because conclusions on small models do not trivially transfer to
large ones. A possible solution is to establish a generic system that directly
predicts some metrics for large models solely based on the results and
hyperparameters from small models. Existing methods based on scaling laws
require hyperparameter search on the largest models, which is impractical with
limited resources. We address this issue by presenting our discoveries
indicating that Maximal Update parametrization (Mup) enables accurate fitting
of scaling laws for hyperparameters close to common loss basins, without any
search. Thus, different models can be directly compared on large scales with
loss prediction even before the training starts. We propose a new paradigm as a
first step towards reliable academic research for any model scale without heavy
computation. Code is publicly available at
https://github.com/cofe-ai/Mu-scaling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMR4NLI: Interpretable and robust NLI measures from semantic graphs. (arXiv:2306.00936v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00936">
<div class="article-summary-box-inner">
<span><p>The task of natural language inference (NLI) asks whether a given premise
(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human
ratings of entailment, but the meaning relationships driving these ratings are
not formalized. Can the underlying sentence pair relationships be made more
explicit in an interpretable yet robust fashion? We compare semantic structures
to represent premise and hypothesis, including sets of contextualized
embeddings and semantic graphs (Abstract Meaning Representations), and measure
whether the hypothesis is a semantic substructure of the premise, utilizing
interpretable metrics. Our evaluation on three English benchmarks finds value
in both contextualized embeddings and semantic graphs; moreover, they provide
complementary signals, and can be leveraged together in a hybrid model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.09237">
<div class="article-summary-box-inner">
<span><p>Recent strides in Large Language Models (LLMs) have saturated many NLP
benchmarks (even professional domain-specific ones), emphasizing the need for
novel, more challenging novel ones to properly assess LLM capabilities. In this
paper, we introduce a novel NLP benchmark that poses challenges to current LLMs
across four key dimensions: processing long documents (up to 50K tokens),
utilizing domain specific knowledge (embodied in legal texts), multilingual
understanding (covering five languages), and multitasking (comprising legal
document to document Information Retrieval, Court View Generation, Leading
Decision Summarization, Citation Extraction, and eight challenging Text
Classification tasks). Our benchmark comprises diverse legal NLP datasets from
the Swiss legal system, allowing for a comprehensive study of the underlying
Non-English, inherently multilingual, federal legal system. Despite recent
advances, efficiently processing long documents for intense review/analysis
tasks remains an open challenge for language models. Also, comprehensive,
domain-specific benchmarks requiring high expertise to develop are rare, as are
multilingual benchmarks. This scarcity underscores our contribution's value,
considering most public models are trained predominantly on English corpora,
while other languages remain understudied, particularly for practical
domain-specific NLP tasks. Our benchmark allows for testing and advancing the
state-of-the-art LLMs. As part of our study, we evaluate several pre-trained
multilingual language models on our benchmark to establish strong baselines as
a point of reference. Despite the large size of our datasets (tens to hundreds
of thousands of examples), existing publicly available models struggle with
most tasks, even after in-domain pretraining. We publish all resources
(benchmark suite, pre-trained models, code) under a fully permissive open CC
BY-SA license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06954">
<div class="article-summary-box-inner">
<span><p>Conspiracy Theory Identication task is a new shared task proposed for the
first time at the Evalita 2023. The ACTI challenge, based exclusively on
comments published on conspiratorial channels of telegram, is divided into two
subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial
content and (ii) Conspiratorial Category Classification about specific
conspiracy theory classification. A total of fifteen teams participated in the
task for a total of 81 submissions. We illustrate the best performing
approaches were based on the utilization of large language models. We finally
draw conclusions about the utilization of these models for counteracting the
spreading of misinformation in online platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.07362">
<div class="article-summary-box-inner">
<span><p>Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers'
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Dataset Annotation Quality Management in the Wild. (arXiv:2307.08153v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08153">
<div class="article-summary-box-inner">
<span><p>Data quality is crucial for training accurate, unbiased, and trustworthy
machine learning models and their correct evaluation. Recent works, however,
have shown that even popular datasets used to train and evaluate
state-of-the-art models contain a non-negligible amount of erroneous
annotations, bias or annotation artifacts. There exist best practices and
guidelines regarding annotation projects. But to the best of our knowledge, no
large-scale analysis has been performed as of yet on how quality management is
actually conducted when creating natural language datasets and whether these
recommendations are followed. Therefore, we first survey and summarize
recommended quality management practices for dataset creation as described in
the literature and provide suggestions on how to apply them. Then, we compile a
corpus of 591 scientific publications introducing text datasets and annotate it
for quality-related aspects, such as annotator management, agreement,
adjudication or data validation. Using these annotations, we then analyze how
quality management is conducted in practice. We find that a majority of the
annotated publications apply good or very good quality management. However, we
deem the effort of 30% of the works as only subpar. Our analysis also shows
common errors, especially with using inter-annotator agreement and computing
annotation error rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11729">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have achieved human-level fluency in text
generation, making it difficult to distinguish between human-written and
LLM-generated texts. This poses a growing risk of misuse of LLMs and demands
the development of detectors to identify LLM-generated texts. However, existing
detectors lack robustness against attacks: they degrade detection accuracy by
simply paraphrasing LLM-generated texts. Furthermore, a malicious user might
attempt to deliberately evade the detectors based on detection results, but
this has not been assumed in previous studies. In this paper, we propose
OUTFOX, a framework that improves the robustness of LLM-generated-text
detectors by allowing both the detector and the attacker to consider each
other's output. In this framework, the attacker uses the detector's prediction
labels as examples for in-context learning and adversarially generates essays
that are harder to detect, while the detector uses the adversarially generated
essays as examples for in-context learning to learn to detect essays from a
strong attacker. Experiments in the domain of student essays show that the
proposed detector improves the detection performance on the attacker-generated
texts by up to +41.3 points in F1-score. Furthermore, the proposed detector
shows a state-of-the-art detection performance: up to 96.9 points in F1-score,
beating existing detectors on non-attacked texts. Finally, the proposed
attacker drastically degrades the performance of detectors by up to -57.0
points F1-score, massively outperforming the baseline paraphrasing method for
evading detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11772">
<div class="article-summary-box-inner">
<span><p>The task of entity alignment between knowledge graphs (KGs) aims to identify
every pair of entities from two different KGs that represent the same entity.
Many machine learning-based methods have been proposed for this task. However,
to our best knowledge, existing methods all require manually crafted seed
alignments, which are expensive to obtain. In this paper, we propose the first
fully automatic alignment method named AutoAlign, which does not require any
manually crafted seed alignments. Specifically, for predicate embeddings,
AutoAlign constructs a predicate-proximity-graph with the help of large
language models to automatically capture the similarity between predicates
across two KGs. For entity embeddings, AutoAlign first computes the entity
embeddings of each KG independently using TransE, and then shifts the two KGs'
entity embeddings into the same vector space by computing the similarity
between entities based on their attributes. Thus, both predicate alignment and
entity alignment can be done without manually crafted seed alignments.
AutoAlign is not only fully automatic, but also highly effective. Experiments
using real-world KGs show that AutoAlign improves the performance of entity
alignment significantly compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.00085">
<div class="article-summary-box-inner">
<span><p>Recent approaches to empathetic response generation try to incorporate
commonsense knowledge or reasoning about the causes of emotions to better
understand the user's experiences and feelings. However, these approaches
mainly focus on understanding the causalities of context from the user's
perspective, ignoring the system's perspective. In this paper, we propose a
commonsense-based causality explanation approach for diverse empathetic
response generation that considers both the user's perspective (user's desires
and reactions) and the system's perspective (system's intentions and
reactions). We enhance ChatGPT's ability to reason for the system's perspective
by integrating in-context learning with commonsense knowledge. Then, we
integrate the commonsense-based causality explanation with both ChatGPT and a
T5-based model. Experimental evaluations demonstrate that our method
outperforms other comparable methods on both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.02463">
<div class="article-summary-box-inner">
<span><p>In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM.We consider the construction of foundational models from
the perspectives of dataset construction, model design, and thorough
evaluation. Our contribution can be concluded as follows: (i), we construct a
large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D
medical scans with high-quality text descriptions or reports across various
data formats, modalities, and tasks, covering over 5000 distinct diseases. To
the best of our knowledge, this is the first large-scale, high-quality, medical
visual-language dataset, with both 2D and 3D scans; (ii ), we propose an
architecture that enables visually conditioned generative pre-training, i.e.,
allowing for integration of text input with 2D or 3D medical scans, and
generate responses for diverse radiologic tasks. The model was initially
pre-trained on MedMD and subsequently fine-tuned on the domain-specific
dataset, which is a radiologic cleaned version of MedMD, containing 3M
radiologic visual-language pairs, termed as RadMD; (iii), we propose a new
evaluation benchmark, RadBench, that comprises five tasks, including modality
recognition, disease diagnosis, visual question answering, report generation
and rationale diagnosis, aiming to comprehensively assess the capability of
foundation models in handling practical clinical problems. We conduct both
automatic and human evaluation on RadBench, in both cases, RadFM significantly
outperforms existing multi-modal foundation models. The codes, data, and model
checkpoint will all be made publicly available to promote further research and
development in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06035">
<div class="article-summary-box-inner">
<span><p>The advanced language processing abilities of large language models (LLMs)
have stimulated debate over their capacity to replicate human-like cognitive
processes. One differentiating factor between language processing in LLMs and
humans is that language input is often grounded in several perceptual
modalities, whereas most LLMs process solely text-based information. Multimodal
grounding allows humans to integrate - e.g. visual context with linguistic
information and thereby place constraints on the space of upcoming words,
reducing cognitive load and improving comprehension. Recent multimodal LLMs
(mLLMs) combine a visual-linguistic embedding space with a transformer type
attention mechanism for next-word prediction. Here we ask whether predictive
language processing based on multimodal input in mLLMs aligns with humans.
Two-hundred participants watched short audio-visual clips and estimated
predictability of an upcoming verb or noun. The same clips were processed by
the mLLM CLIP, with predictability scores based on comparing image and text
feature vectors. Eye-tracking was used to estimate what visual features
participants attended to, and CLIP's visual attention weights were recorded. We
find that alignment of predictability scores was driven by multimodality of
CLIP (no alignment for a unimodal state-of-the-art LLM) and by the attention
mechanism (no alignment when attention weights were perturbated or when the
same input was fed to a multimodal model without attention). We further find a
significant spatial overlap between CLIP's visual attention weights and human
eye-tracking data. Results suggest that comparable processes of integrating
multimodal information, guided by attention to relevant visual features,
supports predictive language processing in mLLMs and humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.06912">
<div class="article-summary-box-inner">
<span><p>Recent empirical evidence indicates that transformer based in-context
learning performs better when using a prefix language model (prefixLM), in
which in-context samples can all attend to each other, compared to causal
language models (causalLM), which use auto-regressive attention that prohibits
in-context samples to attend to future samples. While this result is intuitive,
it is not understood from a theoretical perspective. In this paper we take a
theoretical approach and analyze the convergence behavior of prefixLM and
causalLM under a certain parameter construction. Our analysis shows that both
LM types converge to their stationary points at a linear rate, but that while
prefixLM converges to the optimal solution of linear regression, causalLM
convergence dynamics follows that of an online gradient descent algorithm,
which is not guaranteed to be optimal even as the number of samples grows
infinitely. We supplement our theoretical claims with empirical experiments
over synthetic and real tasks and using various types of transformers. Our
experiments verify that causalLM consistently underperforms prefixLM in all
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning. (arXiv:2308.11148v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11148">
<div class="article-summary-box-inner">
<span><p>The automation of code review activities, a long-standing pursuit in software
engineering, has been primarily addressed by numerous domain-specific
pre-trained models. Despite their success, these models frequently demand
extensive resources for pre-training from scratch. In contrast, Large Language
Models (LLMs) provide an intriguing alternative, given their remarkable
capabilities when supplemented with domain-specific knowledge. However, their
potential for automating code review tasks remains largely unexplored.
</p>
<p>In response to this research gap, we present LLaMA-Reviewer, an innovative
framework that leverages the capabilities of LLaMA, a popular LLM, in the realm
of code review. Mindful of resource constraints, this framework employs
parameter-efficient fine-tuning (PEFT) methods, delivering high performance
while using less than 1% of trainable parameters.
</p>
<p>An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,
publicly available datasets. Notably, even with the smallest LLaMA base model
consisting of 6.7B parameters and a limited number of tuning epochs,
LLaMA-Reviewer equals the performance of existing code-review-focused models.
</p>
<p>The ablation experiments provide insights into the influence of various
fine-tuning process components, including input representation, instruction
tuning, and different PEFT methods. To foster continuous progress in this
field, the code and all PEFT-weight plugins have been made open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11592">
<div class="article-summary-box-inner">
<span><p>In the era of Large Language Models (LLMs), tremendous strides have been made
in the field of multimodal understanding. However, existing advanced algorithms
are limited to effectively utilizing the immense representation capabilities
and rich world knowledge inherent to these large pre-trained models, and the
beneficial connections among tasks within the context of text-rich scenarios
have not been sufficiently explored. In this work, we introduce UniDoc, a novel
multimodal model equipped with text detection and recognition capabilities,
which are deficient in existing approaches. Moreover, UniDoc capitalizes on the
beneficial interactions among tasks to enhance the performance of each
individual task. To implement UniDoc, we perform unified multimodal instruct
tuning on the contributed large-scale instruction following datasets.
Quantitative and qualitative experimental results show that UniDoc sets
state-of-the-art scores across multiple challenging benchmarks. To the best of
our knowledge, this is the first large multimodal model capable of simultaneous
text detection, recognition, spotting, and understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model. (arXiv:2308.11773v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.11773">
<div class="article-summary-box-inner">
<span><p>Language use has been shown to correlate with depression, but large-scale
validation is needed. Traditional methods like clinic studies are expensive.
So, natural language processing has been employed on social media to predict
depression, but limitations remain-lack of validated labels, biased user
samples, and no context. Our study identified 29 topics in 3919
smartphone-collected speech recordings from 265 participants using the Whisper
tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal
to 10 were regarded as risk topics for depression: No Expectations, Sleep,
Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic
emergence and associations with depression, we compared behavioral (from
wearables) and linguistic characteristics across identified topics. The
correlation between topic shifts and changes in depression severity over time
was also investigated, indicating the importance of longitudinally monitoring
language use. We also tested the BERTopic model on a similar smaller dataset
(356 speech recordings from 57 participants), obtaining some consistent
results. In summary, our findings demonstrate specific speech topics may
indicate depression severity. The presented data-driven workflow provides a
practical approach to collecting and analyzing large-scale speech data from
real-world settings for digital health research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.12014">
<div class="article-summary-box-inner">
<span><p>Big models, exemplified by Large Language Models (LLMs), are models typically
pre-trained on massive data and comprised of enormous parameters, which not
only obtain significantly improved performance across diverse tasks but also
present emergent capabilities absent in smaller models. However, the growing
intertwining of big models with everyday human lives poses potential risks and
might cause serious social harm. Therefore, many efforts have been made to
align LLMs with humans to make them better follow user instructions and satisfy
human preferences. Nevertheless, `what to align with' has not been fully
discussed, and inappropriate alignment goals might even backfire. In this
paper, we conduct a comprehensive survey of different alignment goals in
existing work and trace their evolution paths to help identify the most
essential goal. Particularly, we investigate related works from two
perspectives: the definition of alignment goals and alignment evaluation. Our
analysis encompasses three distinct levels of alignment goals and reveals a
goal transformation from fundamental abilities to value orientation, indicating
the potential of intrinsic human values as the alignment goal for enhanced
LLMs. Based on such results, we further discuss the challenges of achieving
such intrinsic value alignment and provide a collection of available resources
for future research on the alignment of big models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction Grammar and Language Models. (arXiv:2308.13315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13315">
<div class="article-summary-box-inner">
<span><p>Recent progress in deep learning and natural language processing has given
rise to powerful models that are primarily trained on a cloze-like task and
show some evidence of having access to substantial linguistic information,
including some constructional knowledge. This groundbreaking discovery presents
an exciting opportunity for a synergistic relationship between computational
methods and Construction Grammar research. In this chapter, we explore three
distinct approaches to the interplay between computational methods and
Construction Grammar: (i) computational methods for text analysis, (ii)
computational Construction Grammar, and (iii) deep learning models, with a
particular focus on language models. We touch upon the first two approaches as
a contextual foundation for the use of computational methods before providing
an accessible, yet comprehensive overview of deep learning models, which also
addresses reservations construction grammarians may have. Additionally, we
delve into experiments that explore the emergence of constructionally relevant
information within these models while also examining the aspects of
Construction Grammar that may pose challenges for these models. This chapter
aims to foster collaboration between researchers in the fields of natural
language processing and Construction Grammar. By doing so, we hope to pave the
way for new insights and advancements in both these fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. (arXiv:2308.13387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.13387">
<div class="article-summary-box-inner">
<span><p>With the rapid evolution of large language models (LLMs), new and
hard-to-predict harmful capabilities are emerging. This requires developers to
be able to identify risks through the evaluation of "dangerous capabilities" in
order to responsibly deploy LLMs. In this work, we collect the first
open-source dataset to evaluate safeguards in LLMs, and deploy safer
open-source LLMs at a low cost. Our dataset is curated and filtered to consist
only of instructions that responsible language models should not follow. We
annotate and assess the responses of six popular LLMs to these instructions.
Based on our annotation, we proceed to train several BERT-like classifiers, and
find that these small classifiers can achieve results that are comparable with
GPT-4 on automatic safety evaluation. Warning: this paper contains example data
that may be offensive, harmful, or biased.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15459">
<div class="article-summary-box-inner">
<span><p>Textual style transfer is the task of transforming stylistic properties of
text while preserving meaning. Target "styles" can be defined in numerous ways,
ranging from single attributes (e.g, formality) to authorship (e.g,
Shakespeare). Previous unsupervised style-transfer approaches generally rely on
significant amounts of labeled data for only a fixed set of styles or require
large language models. In contrast, we introduce a novel diffusion-based
framework for general-purpose style transfer that can be flexibly adapted to
arbitrary target styles at inference time. Our parameter-efficient approach,
ParaGuide, leverages paraphrase-conditioned diffusion models alongside
gradient-based guidance from both off-the-shelf classifiers and strong existing
style embedders to transform the style of text while preserving semantic
information. We validate the method on the Enron Email Corpus, with both human
and automatic evaluations, and find that it outperforms strong baselines on
formality, sentiment, and even authorship style transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v3 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.15906">
<div class="article-summary-box-inner">
<span><p>Our interdisciplinary study investigates how effectively U.S. laws confront
the challenges posed by Generative AI to human values. Through an analysis of
diverse hypothetical scenarios crafted during an expert workshop, we have
identified notable gaps and uncertainties within the existing legal framework
regarding the protection of fundamental values, such as privacy, autonomy,
dignity, diversity, equity, and physical/mental well-being. Constitutional and
civil rights, it appears, may not provide sufficient protection against
AI-generated discriminatory outputs. Furthermore, even if we exclude the
liability shield provided by Section 230, proving causation for defamation and
product liability claims is a challenging endeavor due to the intricate and
opaque nature of AI systems. To address the unique and unforeseeable threats
posed by Generative AI, we advocate for legal frameworks that evolve to
recognize new threats and provide proactive, auditable guidelines to industry
stakeholders. Addressing these issues requires deep interdisciplinary
collaborations to identify harms, values, and mitigation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16137">
<div class="article-summary-box-inner">
<span><p>In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex tasks, they often face the
need to conduct longer reasoning processes or understand larger contexts. In
these situations, the length generalization failure of LLMs on long sequences
becomes more prominent. Most pre-training schemes truncate training sequences
to a fixed length. LLMs often struggle to generate fluent and coherent texts,
let alone carry out downstream tasks, after longer contexts, even with relative
positional encoding designed to cope with this problem. Common solutions such
as finetuning on longer corpora often involve daunting hardware and time costs
and require careful training process design. To more efficiently leverage the
generation capacity of existing LLMs, we theoretically and empirically
investigate the main out-of-distribution (OOD) factors contributing to this
problem. Inspired by this diagnosis, we propose a simple yet effective solution
for on-the-fly length generalization, LM-Infinite. It involves only a
$\Lambda$-shaped attention mask (to avoid excessive attended tokens) and a
distance limit (to avoid unseen distances) while requiring no parameter updates
or learning. We find it applicable to a variety of LLMs using relative-position
encoding methods. LM-Infinite is computationally efficient with $O(n)$ time and
space, and demonstrates consistent text generation fluency and quality to as
long as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding
speedup. On downstream tasks such as passkey retrieval, it continues to work on
inputs much longer than training lengths where vanilla models fail immediately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16458">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models like ChatGPT have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks. Moreover, in bioinformatics, generating
functional programs poses additional notable challenges due to the amount of
domain knowledge, the need for complicated data operations, and intricate
functional dependencies between the operations. Here, we present BioCoder, a
benchmark developed to evaluate existing pre-trained models in generating
bioinformatics code. In relation to function-code generation, BioCoder covers
potential package dependencies, class declarations, and global variables. It
incorporates 1026 functions and 1243 methods in Python and Java from GitHub and
253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing
framework for evaluation, and we have applied it to evaluate many models
including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+,
InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizes
the importance of domain knowledge, pragmatic code generation, and contextual
understanding. Our dataset, benchmark, Docker images, and scripts required for
testing are all available at https://github.com/gersteinlab/biocoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Link Prediction for Wikipedia Articles as a Natural Language Inference Task. (arXiv:2308.16469v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16469">
<div class="article-summary-box-inner">
<span><p>Link prediction task is vital to automatically understanding the structure of
large knowledge bases. In this paper, we present our system to solve this task
at the Data Science and Advanced Analytics 2023 Competition "Efficient and
Effective Link Prediction" (DSAA-2023 Competition) with a corpus containing
948,233 training and 238,265 for public testing. This paper introduces an
approach to link prediction in Wikipedia articles by formulating it as a
natural language inference (NLI) task. Drawing inspiration from recent
advancements in natural language processing and understanding, we cast link
prediction as an NLI task, wherein the presence of a link between two articles
is treated as a premise, and the task is to determine whether this premise
holds based on the information presented in the articles. We implemented our
system based on the Sentence Pair Classification for Link Prediction for the
Wikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000
Macro F1-score for the public and private test sets, respectively. Our team
UIT-NLP ranked 3rd in performance on the private test set, equal to the scores
of the first and second places. Our code is publicly for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16824">
<div class="article-summary-box-inner">
<span><p>When human programmers have mastered a programming language, it would be
easier when they learn a new programming language. In this report, we focus on
exploring whether programming languages can boost each other during the
instruction fine-tuning phase of code large language models. We conduct
extensive experiments of 8 popular programming languages (Python, JavaScript,
TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that
programming languages can significantly improve each other. For example,
CodeM-Python 15B trained on Python is able to increase Java by an absolute
17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B
trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our
training data is released at https://github.com/NL2Code/CodeM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2308.16890">
<div class="article-summary-box-inner">
<span><p>Large vision-language models (LVLMs) have recently witnessed rapid
advancements, exhibiting a remarkable capacity for perceiving, understanding,
and processing visual information by connecting visual receptor with large
language models (LLMs). However, current assessments mainly focus on
recognizing and reasoning abilities, lacking direct evaluation of
conversational skills and neglecting visual storytelling abilities. In this
paper, we propose an evaluation method that uses strong LLMs as judges to
comprehensively evaluate the various abilities of LVLMs. Firstly, we construct
a comprehensive visual dialogue dataset TouchStone, consisting of open-world
images and questions, covering five major categories of abilities and 27
subtasks. This dataset not only covers fundamental recognition and
comprehension but also extends to literary creation. Secondly, by integrating
detailed image annotations we effectively transform the multimodal input
content into a form understandable by LLMs. This enables us to employ advanced
LLMs for directly evaluating the quality of the multimodal dialogue without
requiring human intervention. Through validation, we demonstrate that powerful
LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging
their textual capabilities alone, aligning with human preferences. We hope our
work can serve as a touchstone for LVLMs' evaluation and pave the way for
building stronger LVLMs. The evaluation code is available at
https://github.com/OFA-Sys/TouchStone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2309.00614">
<div class="article-summary-box-inner">
<span><p>As Large Language Models quickly become ubiquitous, it becomes critical to
understand their security vulnerabilities. Recent work shows that text
optimizers can produce jailbreaking prompts that bypass moderation and
alignment. Drawing from the rich body of work on adversarial machine learning,
we approach these attacks with three questions: What threat models are
practically useful in this domain? How do baseline defense techniques perform
in this new domain? How does LLM security differ from computer vision?
</p>
<p>We evaluate several baseline defense strategies against leading adversarial
attacks on LLMs, discussing the various settings in which each is feasible and
effective. Particularly, we look at three types of defenses: detection
(perplexity based), input preprocessing (paraphrase and retokenization), and
adversarial training. We discuss white-box and gray-box settings and discuss
the robustness-performance trade-off for each of the defenses considered. We
find that the weakness of existing discrete optimizers for text, combined with
the relatively high costs of optimization, makes standard adaptive attacks more
challenging for LLMs. Future research will be needed to uncover whether more
powerful optimizers can be developed, or whether the strength of filtering and
preprocessing defenses is greater in the LLMs domain than it has been in
computer vision.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-09-06 23:10:44.293809675 UTC">2023-09-06 23:10:44 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>