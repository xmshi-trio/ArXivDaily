<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-05-01T01:30:00Z">05-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative AI Perceptions: A Survey to Measure the Perceptions of Faculty, Staff, and Students on Generative AI Tools in Academia. (arXiv:2304.14415v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14415">
<div class="article-summary-box-inner">
<span><p>ChatGPT is a natural language processing tool that can engage in human-like
conversations and generate coherent and contextually relevant responses to
various prompts. ChatGPT is capable of understanding natural text that is input
by a user and generating appropriate responses in various forms. This tool
represents a major step in how humans are interacting with technology. This
paper specifically focuses on how ChatGPT is revolutionizing the realm of
engineering education and the relationship between technology, students, and
faculty and staff. Because this tool is quickly changing and improving with the
potential for even greater future capability, it is a critical time to collect
pertinent data. A survey was created to measure the effects of ChatGPT on
students, faculty, and staff. This survey is shared as a Texas A&amp;M University
technical report to allow other universities and entities to use this survey
and measure the effects elsewhere.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Vietnamese Legal Questions Using Deep Neural Networks with Biaffine Classifiers. (arXiv:2304.14447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14447">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose using deep neural networks to extract important
information from Vietnamese legal questions, a fundamental task towards
building a question answering system in the legal domain. Given a legal
question in natural language, the goal is to extract all the segments that
contain the needed information to answer the question. We introduce a deep
model that solves the task in three stages. First, our model leverages recent
advanced autoencoding language models to produce contextual word embeddings,
which are then combined with character-level and POS-tag information to form
word representations. Next, bidirectional long short-term memory networks are
employed to capture the relations among words and generate sentence-level
representations. At the third stage, borrowing ideas from graph-based
dependency parsing methods which provide a global view on the input sentence,
we use biaffine classifiers to estimate the probability of each pair of
start-end words to be an important segment. Experimental results on a public
Vietnamese legal dataset show that our model outperforms the previous work by a
large margin, achieving 94.79% in the F1 score. The results also prove the
effectiveness of using contextual features extracted from pre-trained language
models combined with other types of features such as character-level and
POS-tag features when training on a limited dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMC-LLaMA: Further Finetuning LLaMA on Medical Papers. (arXiv:2304.14454v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14454">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have showcased remarkable capabilities in
natural language understanding in various domains. These models can usually
behave well on daily dialog, or question answering scenarios, however, in areas
that value precision, for example, in medical applications, they often exhibit
unsatisfactory performance due to a lack of domain-specific knowledge. In this
report, we introduce PMC-LLaMA, an open-source language model that is acquired
by fine-tuning an open-source language model on a total of 4.8 million
biomedical academic papers for further injecting medical knowledge, enhancing
its capability in medical domain. Our preliminary evaluations are conducted on
three biomedical QA datasets, including PubMedQA, MedMCQA, and USMLE, showing
that the our model after finetuning, i.e., PMC-LLaMA, demonstrates better
understanding of biomedical domain-specific concepts, thus achieving high
performance on QA benchmarks. The model and codes, along with an online demo,
are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Framing the News:From Human Perception to Large Language Model Inferences. (arXiv:2304.14456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14456">
<div class="article-summary-box-inner">
<span><p>Identifying the frames of news is important to understand the articles'
vision, intention, message to be conveyed, and which aspects of the news are
emphasized. Framing is a widely studied concept in journalism, and has emerged
as a new topic in computing, with the potential to automate processes and
facilitate the work of journalism professionals. In this paper, we study this
issue with articles related to the Covid-19 anti-vaccine movement. First, to
understand the perspectives used to treat this theme, we developed a protocol
for human labeling of frames for 1786 headlines of No-Vax movement articles of
European newspapers from 5 countries. Headlines are key units in the written
press, and worth of analysis as many people only read headlines (or use them to
guide their decision for further reading.) Second, considering advances in
Natural Language Processing (NLP) with large language models, we investigated
two approaches for frame inference of news headlines: first with a GPT-3.5
fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work
contributes to the study and analysis of the performance that these models have
to facilitate journalistic tasks like classification of frames, while
understanding whether the models are able to replicate human perception in the
identification of these frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Referential Games Further the Emergence of Disentangled Representations. (arXiv:2304.14511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14511">
<div class="article-summary-box-inner">
<span><p>Natural languages are powerful tools wielded by human beings to communicate
information. Among their desirable properties, compositionality has been the
main focus in the context of referential games and variants, as it promises to
enable greater systematicity to the agents which would wield it. The concept of
disentanglement has been shown to be of paramount importance to learned
representations that generalise well in deep learning, and is thought to be a
necessary condition to enable systematicity. Thus, this paper investigates how
do compositionality at the level of the emerging languages, disentanglement at
the level of the learned representations, and systematicity relate to each
other in the context of visual referential games. Firstly, we find that visual
referential games that are based on the Obverter architecture outperforms
state-of-the-art unsupervised learning approach in terms of many major
disentanglement metrics. Secondly, we expand the previously proposed Positional
Disentanglement (PosDis) metric for compositionality to (re-)incorporate some
concerns pertaining to informativeness and completeness features found in the
Mutual Information Gap (MIG) disentanglement metric it stems from. This
extension allows for further discrimination between the different kind of
compositional languages that emerge in the context of Obverter-based
referential games, in a way that neither the referential game accuracy nor
previous metrics were able to capture. Finally we investigate whether the
resulting (emergent) systematicity, as measured by zero-shot compositional
learning tests, correlates with any of the disentanglement and compositionality
metrics proposed so far. Throughout the training process, statically
significant correlation coefficients can be found both positive and negative
depending on the moment of the measure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Shared Speech-Text Representations. (arXiv:2304.14514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14514">
<div class="article-summary-box-inner">
<span><p>Recently, a number of approaches to train speech models by incorpo-rating
text into end-to-end models have been developed, with Mae-stro advancing
state-of-the-art automatic speech recognition (ASR)and Speech Translation (ST)
performance. In this paper, we expandour understanding of the resulting shared
speech-text representationswith two types of analyses. First we examine the
limits of speech-free domain adaptation, finding that a corpus-specific
duration modelfor speech-text alignment is the most important component for
learn-ing a shared speech-text representation. Second, we inspect the
sim-ilarities between activations of unimodal (speech or text) encodersas
compared to the activations of a shared encoder. We find that theshared encoder
learns a more compact and overlapping speech-textrepresentation than the
uni-modal encoders. We hypothesize that thispartially explains the
effectiveness of the Maestro shared speech-textrepresentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14522">
<div class="article-summary-box-inner">
<span><p>Dense retrieval models use bi-encoder network architectures for learning
query and document representations. These representations are often in the form
of a vector representation and their similarities are often computed using the
dot product function. In this paper, we propose a new representation learning
framework for dense retrieval. Instead of learning a vector for each query and
document, our framework learns a multivariate distribution and uses negative
multivariate KL divergence to compute the similarity between distributions. For
simplicity and efficiency reasons, we assume that the distributions are
multivariate normals and then train large language models to produce mean and
variance vectors for these distributions. We provide a theoretical foundation
for the proposed framework and show that it can be seamlessly integrated into
the existing approximate nearest neighbor algorithms to perform retrieval
efficiently. We conduct an extensive suite of experiments on a wide range of
datasets, and demonstrate significant improvements compared to competitive
dense retrieval models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14535">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) has recently become an important challenge
when using deep learning (DL). It requires large-scale training datasets and
high computational and storage resources. Moreover, DL techniques and machine
learning (ML) approaches in general, hypothesize that training and testing data
come from the same domain, with the same input feature space and data
distribution characteristics. This assumption, however, is not applicable in
some real-world artificial intelligence (AI) applications. Moreover, there are
situations where gathering real data is challenging, expensive, or rarely
occurring, which can not meet the data requirements of DL models. deep transfer
learning (DTL) has been introduced to overcome these issues, which helps
develop high-performing models using real datasets that are small or slightly
different but related to the training data. This paper presents a comprehensive
survey of DTL-based ASR frameworks to shed light on the latest developments and
helps academics and professionals understand current challenges. Specifically,
after presenting the DTL background, a well-designed taxonomy is adopted to
inform the state-of-the-art. A critical analysis is then conducted to identify
the limitations and advantages of each framework. Moving on, a comparative
study is introduced to highlight the current challenges before deriving
opportunities for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse over Discourse: The Need for an Expanded Pragmatic Focus in Conversational AI. (arXiv:2304.14543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14543">
<div class="article-summary-box-inner">
<span><p>The summarization of conversation, that is, discourse over discourse,
elevates pragmatic considerations as a pervasive limitation of both
summarization and other applications of contemporary conversational AI.
Building on impressive progress in both semantics and syntax, pragmatics
concerns meaning in the practical sense. In this paper, we discuss several
challenges in both summarization of conversations and other conversational AI
applications, drawing on relevant theoretical work. We illustrate the
importance of pragmatics with so-called star sentences, syntactically
acceptable propositions that are pragmatically inappropriate in conversation or
its summary. Because the baseline for quality of AI is indistinguishability
from human behavior, we draw heavily on the psycho-linguistics literature, and
label our complaints as "Turing Test Triggers" (TTTs). We discuss implications
for the design and evaluation of conversation summarization methods and
conversational AI applications like voice assistants and chatbots
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Appropriateness is all you need!. (arXiv:2304.14553v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14553">
<div class="article-summary-box-inner">
<span><p>The strive to make AI applications "safe" has led to the development of
safety-measures as the main or even sole normative requirement of their
permissible use. Similar can be attested to the latest version of chatbots,
such as chatGPT. In this view, if they are "safe", they are supposed to be
permissible to deploy. This approach, which we call "safety-normativity", is
rather limited in solving the emerging issues that chatGPT and other chatbots
have caused thus far. In answering this limitation, in this paper we argue for
limiting chatbots in the range of topics they can chat about according to the
normative concept of appropriateness. We argue that rather than looking for
"safety" in a chatbot's utterances to determine what they may and may not say,
we ought to assess those utterances according to three forms of
appropriateness: technical-discursive, social, and moral. We then spell out
what requirements for chatbots follow from these forms of appropriateness to
avoid the limits of previous accounts: positionality, acceptability, and value
alignment (PAVA). With these in mind, we may be able to determine what a
chatbot may and may not say. Lastly, one initial suggestion is to use challenge
sets, specifically designed for appropriateness, as a validation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Knowledge Graph Entity Alignment with Graph Augmentation. (arXiv:2304.14585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14585">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA) which links equivalent entities across different
knowledge graphs (KGs) plays a crucial role in knowledge fusion. In recent
years, graph neural networks (GNNs) have been successfully applied in many
embedding-based EA methods. However, existing GNN-based methods either suffer
from the structural heterogeneity issue that especially appears in the real KG
distributions or ignore the heterogeneous representation learning for unseen
(unlabeled) entities, which would lead the model to overfit on few alignment
seeds (i.e., training data) and thus cause unsatisfactory alignment
performance. To enhance the EA ability, we propose GAEA, a novel EA approach
based on graph augmentation. In this model, we design a simple Entity-Relation
(ER) Encoder to generate latent representations for entities via jointly
modeling comprehensive structural information and rich relation semantics.
Moreover, we use graph augmentation to create two graph views for margin-based
alignment learning and contrastive entity representation learning, thus
mitigating structural heterogeneity and further improving the model's alignment
performance. Extensive experiments conducted on benchmark datasets demonstrate
the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A logical word embedding for learning grammar. (arXiv:2304.14590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14590">
<div class="article-summary-box-inner">
<span><p>We introduce the logical grammar emdebbing (LGE), a model inspired by
pregroup grammars and categorial grammars to enable unsupervised inference of
lexical categories and syntactic rules from a corpus of text. LGE produces
comprehensible output summarizing its inferences, has a completely transparent
process for producing novel sentences, and can learn from as few as a hundred
sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Antisemitic Messages? A Guide to High-Quality Annotation and a Labeled Dataset of Tweets. (arXiv:2304.14599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14599">
<div class="article-summary-box-inner">
<span><p>One of the major challenges in automatic hate speech detection is the lack of
datasets that cover a wide range of biased and unbiased messages and that are
consistently labeled. We propose a labeling procedure that addresses some of
the common weaknesses of labeled datasets. We focus on antisemitic speech on
Twitter and create a labeled dataset of 6,941 tweets that cover a wide range of
topics common in conversations about Jews, Israel, and antisemitism between
January 2019 and December 2021 by drawing from representative samples with
relevant keywords. Our annotation process aims to strictly apply a commonly
used definition of antisemitism by forcing annotators to specify which part of
the definition applies, and by giving them the option to personally disagree
with the definition on a case-by-case basis. Labeling tweets that call out
antisemitism, report antisemitism, or are otherwise related to antisemitism
(such as the Holocaust) but are not actually antisemitic can help reduce false
positives in automated detection. The dataset includes 1,250 tweets (18%) that
are antisemitic according to the International Holocaust Remembrance Alliance
(IHRA) definition of antisemitism. It is important to note, however, that the
dataset is not comprehensive. Many topics are still not covered, and it only
includes tweets collected from Twitter between January 2019 and December 2021.
Additionally, the dataset only includes tweets that were written in English.
Despite these limitations, we hope that this is a meaningful contribution to
improving the automated detection of antisemitic speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CED: Catalog Extraction from Documents. (arXiv:2304.14662v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14662">
<div class="article-summary-box-inner">
<span><p>Sentence-by-sentence information extraction from long documents is an
exhausting and error-prone task. As the indicator of document skeleton,
catalogs naturally chunk documents into segments and provide informative
cascade semantics, which can help to reduce the search space. Despite their
usefulness, catalogs are hard to be extracted without the assist from external
knowledge. For documents that adhere to a specific template, regular
expressions are practical to extract catalogs. However, handcrafted heuristics
are not applicable when processing documents from different sources with
diverse formats. To address this problem, we build a large manually annotated
corpus, which is the first dataset for the Catalog Extraction from Documents
(CED) task. Based on this corpus, we propose a transition-based framework for
parsing documents into catalog trees. The experimental results demonstrate that
our proposed method outperforms baseline systems and shows a good ability to
transfer. We believe the CED task could fill the gap between raw text segments
and information extraction tasks on extremely long documents. Data and code are
available at \url{https://github.com/Spico197/CatalogExtraction}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14721">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel framework that combines large language
models (LLMs), digital twins and industrial automation system to enable
intelligent planning and control of production processes. Our approach involves
developing a digital twin system that contains descriptive information about
the production and retrofitting the automation system to offer unified
interfaces of fine-granular functionalities or skills executable by automation
components or modules. Subsequently, LLM-Agents are designed to interpret
descriptive information in the digital twins and control the physical system
through RESTful interfaces. These LLM-Agents serve as intelligent agents within
an automation system, enabling autonomous planning and control of flexible
production. Given a task instruction as input, the LLM-agents orchestrate a
sequence of atomic functionalities and skills to accomplish the task. We
demonstrate how our implemented prototype can handle un-predefined tasks, plan
a production process, and execute the operations. This research highlights the
potential of integrating LLMs into industrial automation systems for more
agile, flexible, and adaptive production processes, while also underscoring the
critical insights and limitations for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14732">
<div class="article-summary-box-inner">
<span><p>With the wide application of Large Language Models (LLMs) such as ChatGPT,
how to make the contents generated by LLM accurate and credible becomes very
important, especially in complex knowledge-intensive tasks. In this paper, we
propose a novel framework called Search-in-the-Chain (SearChain) to improve the
accuracy, credibility and traceability of LLM-generated content for multi-hop
question answering, which is a typical complex knowledge-intensive task.
SearChain is a framework that deeply integrates LLM and information retrieval
(IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition
of the multi-hop question. Each node of the chain is a query-answer pair
consisting of an IR-oriented query and the answer generated by LLM for this
query. IR verifies, completes, and traces the information of each node of the
chain, so as to guide LLM to construct the correct chain-of-query, and finally
answer the multi-hop question. SearChain makes LLM change from trying to give a
answer to trying to construct the chain-of-query when faced with the multi-hop
question, which can stimulate the knowledge-reasoning ability and provides the
interface for IR to be deeply involved in reasoning process of LLM. IR
interacts with each node of chain-of-query of LLM. It verifies the information
of the node and provides the unknown knowledge to LLM, which ensures the
accuracy of the whole chain in the process of LLM generating the answer.
Besides, the contents returned by LLM to the user include not only the final
answer but also the reasoning process for the question, that is, the
chain-of-query and the supporting documents retrieved by IR for each node of
the chain, which improves the credibility and traceability of the contents
generated by LLM. Experimental results show SearChain outperforms related
baselines on four multi-hop question-answering datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics. (arXiv:2304.14738v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14738">
<div class="article-summary-box-inner">
<span><p>Self-training based semi-supervised learning algorithms have enabled the
learning of highly accurate deep neural networks, using only a fraction of
labeled data. However, the majority of work on self-training has focused on the
objective of improving accuracy, whereas practical machine learning systems can
have complex goals (e.g. maximizing the minimum of recall across classes, etc.)
that are non-decomposable in nature. In this work, we introduce the
Cost-Sensitive Self-Training (CSST) framework which generalizes the
self-training-based methods for optimizing non-decomposable metrics. We prove
that our framework can better optimize the desired non-decomposable metric
utilizing unlabeled data, under similar data distribution assumptions made for
the analysis of self-training. Using the proposed CSST framework, we obtain
practical self-training methods (for both vision and NLP tasks) for optimizing
different non-decomposable metrics using deep neural networks. Our results
demonstrate that CSST achieves an improvement over the state-of-the-art in
majority of the cases across datasets and objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain. (arXiv:2304.14745v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14745">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach to learn domain-specific plausible materials for
components in the vehicle repair domain by probing Pretrained Language Models
(PLMs) in a cloze task style setting to overcome the lack of annotated
datasets. We devise a new method to aggregate salient predictions from a set of
cloze query templates and show that domain-adaptation using either a small,
high-quality or a customized Wikipedia corpus boosts performance. When
exploring resource-lean alternatives, we find a distilled PLM clearly
outperforming a classic pattern-based algorithm. Further, given that 98% of our
domain-specific components are multiword expressions, we successfully exploit
the compositionality assumption as a way to address data sparsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlowTransformer: A Transformer Framework for Flow-based Network Intrusion Detection Systems. (arXiv:2304.14746v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14746">
<div class="article-summary-box-inner">
<span><p>This paper presents the FlowTransformer framework, a novel approach for
implementing transformer-based Network Intrusion Detection Systems (NIDSs).
FlowTransformer leverages the strengths of transformer models in identifying
the long-term behaviour and characteristics of networks, which are often
overlooked by most existing NIDSs. By capturing these complex patterns in
network traffic, FlowTransformer offers a flexible and efficient tool for
researchers and practitioners in the cybersecurity community who are seeking to
implement NIDSs using transformer-based models. FlowTransformer allows the
direct substitution of various transformer components, including the input
encoding, transformer, classification head, and the evaluation of these across
any flow-based network dataset. To demonstrate the effectiveness and efficiency
of the FlowTransformer framework, we utilise it to provide an extensive
evaluation of various common transformer architectures, such as GPT 2.0 and
BERT, on three commonly used public NIDS benchmark datasets. We provide results
for accuracy, model size and speed. A key finding of our evaluation is that the
choice of classification head has the most significant impact on the model
performance. Surprisingly, Global Average Pooling, which is commonly used in
text classification, performs very poorly in the context of NIDS. In addition,
we show that model size can be reduced by over 50\%, and inference and training
times improved, with no loss of accuracy, by making specific choices of input
encoding and classification head instead of other commonly used alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dissecting Recall of Factual Associations in Auto-Regressive Language Models. (arXiv:2304.14767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14767">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) are known to capture factual
knowledge in their parameters. While previous work looked into where factual
associations are stored, only little is known about how they are retrieved
internally during inference. We investigate this question through the lens of
information flow. Given a subject-relation query, we study how the model
aggregates information about the subject and relation to predict the correct
attribute. With interventions on attention edges, we first identify two
critical points where information propagates to the prediction: one from the
relation positions followed by another from the subject positions. Next, by
analyzing the information at these points, we unveil a three-step internal
mechanism for attribute extraction. First, the representation at the
last-subject position goes through an enrichment process, driven by the early
MLP sublayers, to encode many subject-related attributes. Second, information
from the relation propagates to the prediction. Third, the prediction
representation "queries" the enriched subject to extract the attribute. Perhaps
surprisingly, this extraction is typically done via attention heads, which
often encode subject-attribute mappings in their parameters. Overall, our
findings introduce a comprehensive view of how factual associations are stored
and extracted internally in LMs, facilitating future research on knowledge
localization and editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction. (arXiv:2304.14770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14770">
<div class="article-summary-box-inner">
<span><p>Universal Information Extraction (UIE) is an area of interest due to the
challenges posed by varying targets, heterogeneous structures, and
demand-specific schemas. However, previous works have only achieved limited
success by unifying a few tasks, such as Named Entity Recognition (NER) and
Relation Extraction (RE), which fall short of being authentic UIE models
particularly when extracting other general schemas such as quadruples and
quintuples. Additionally, these models used an implicit structural schema
instructor, which could lead to incorrect links between types, hindering the
model's generalization and performance in low-resource scenarios. In this
paper, we redefine the authentic UIE with a formal formulation that encompasses
almost all extraction schemas. To the best of our knowledge, we are the first
to introduce UIE for any kind of schemas. In addition, we propose RexUIE, which
is a Recursive Method with Explicit Schema Instructor for UIE. To avoid
interference between different types, we reset the position ids and attention
mask matrices. RexUIE shows strong performance under both full-shot and
few-shot settings and achieves State-of-the-Art results on the tasks of
extracting complex schemas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training and Evaluation of a Multilingual Tokenizer for GPT-SW3. (arXiv:2304.14780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14780">
<div class="article-summary-box-inner">
<span><p>This paper provides a detailed discussion of the multilingual tokenizer used
for GPT-SW3. It was trained on the Nordic Pile using the SentencePiece library
and the BPE algorithm. We outline the tokenizer's most important features and
share details on its learned vocabulary. In addition, we systematically analyze
the properties and evaluate the performance of the tokenizer with regard to the
different languages present in the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14796">
<div class="article-summary-box-inner">
<span><p>Dense vector representations for textual data are crucial in modern NLP. Word
embeddings and sentence embeddings estimated from raw texts are key in
achieving state-of-the-art results in various tasks requiring semantic
understanding. However, obtaining embeddings at the document level is
challenging due to computational requirements and lack of appropriate data.
Instead, most approaches fall back on computing document embeddings based on
sentence representations. Although there exist architectures and models to
encode documents fully, they are in general limited to English and few other
high-resourced languages. In this work, we provide a systematic comparison of
methods to produce document-level representations from sentences based on
LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare
input token number truncation, sentence averaging as well as some simple
windowing and in some cases new augmented and learnable approaches, on 3 multi-
and cross-lingual tasks in 8 languages belonging to 3 different language
families. Our task-based extrinsic evaluations show that, independently of the
language, a clever combination of sentence embeddings is usually better than
encoding the full document as a single unit, even when this is possible. We
demonstrate that while a simple sentence average results in a strong baseline
for classification tasks, more complex combinations are necessary for semantic
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14802">
<div class="article-summary-box-inner">
<span><p>Transformer networks have become the preferred architecture for many tasks
due to their state-of-the-art performance. However, the optimal way to
implement residual connections in Transformer, which are essential for
effective training, is still debated. Two widely used variants are the
Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN)
Transformers, which apply layer normalization after each residual block's
output or before each residual block's input, respectively. While both variants
enjoy their advantages, they also suffer from severe limitations: Post-LN
causes gradient vanishing issue that hinders training deep Transformers, and
Pre-LN causes representation collapse issue that limits model capacity. In this
paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN
(PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits
their advantages while avoids their limitations. We conduct both theoretical
analyses and empirical experiments to verify the effectiveness of ResiDual.
Theoretically, we prove that ResiDual has a lower bound on the gradient to
avoid the vanishing issue due to the residual connection from Pre-LN. Moreover,
ResiDual also has diverse model representations to avoid the collapse issue due
to the residual connection from Post-LN. Empirically, ResiDual outperforms both
Post-LN and Pre-LN on several machine translation benchmarks across different
network depths and data sizes. Thanks to the good theoretical and empirical
performance, ResiDual Transformer can serve as a foundation architecture for
different AI models (e.g., large language models). Our code is available at
https://github.com/microsoft/ResiDual.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2023 Task 11: Learning With Disagreements (LeWiDi). (arXiv:2304.14803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14803">
<div class="article-summary-box-inner">
<span><p>NLP datasets annotated with human judgments are rife with disagreements
between the judges. This is especially true for tasks depending on subjective
judgments such as sentiment analysis or offensive language detection.
Particularly in these latter cases, the NLP community has come to realize that
the approach of 'reconciling' these different subjective interpretations is
inappropriate. Many NLP researchers have therefore concluded that rather than
eliminating disagreements from annotated corpora, we should preserve
them-indeed, some argue that corpora should aim to preserve all annotator
judgments. But this approach to corpus creation for NLP has not yet been widely
accepted. The objective of the LeWiDi series of shared tasks is to promote this
approach to developing NLP models by providing a unified framework for training
and evaluating with such datasets. We report on the second LeWiDi shared task,
which differs from the first edition in three crucial respects: (i) it focuses
entirely on NLP, instead of both NLP and computer vision tasks in its first
edition; (ii) it focuses on subjective tasks, instead of covering different
types of disagreements-as training with aggregated labels for subjective NLP
tasks is a particularly obvious misrepresentation of the data; and (iii) for
the evaluation, we concentrate on soft approaches to evaluation. This second
edition of LeWiDi attracted a wide array of participants resulting in 13 shared
task submission papers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14827">
<div class="article-summary-box-inner">
<span><p>This paper aims to quantitatively evaluate the performance of ChatGPT, an
interactive large language model, on inter-sentential relations such as
temporal relations, causal relations, and discourse relations. Given ChatGPT's
promising performance across various tasks, we conduct extensive evaluations on
the whole test sets of 13 datasets, including temporal and causal relations,
PDTB2.0-based and dialogue-based discourse relations, and downstream
applications on discourse understanding. To achieve reliable results, we adopt
three tailored prompt templates for each task, including the zero-shot prompt
template, zero-shot prompt engineering (PE) template, and in-context learning
(ICL) prompt template, to establish the initial baseline scores for all popular
sentence-pair relation classification tasks for the first time. We find that
ChatGPT exhibits strong performance in detecting and reasoning about causal
relations, while it may not be proficient in identifying the temporal order
between two events. It can recognize most discourse relations with existing
explicit discourse connectives, but the implicit discourse relation still
remains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue
discourse parsing task that requires structural understanding in a dialogue
before being aware of the discourse relation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HQP: A Human-Annotated Dataset for Detecting Online Propaganda. (arXiv:2304.14931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14931">
<div class="article-summary-box-inner">
<span><p>Online propaganda poses a severe threat to the integrity of societies.
However, existing datasets for detecting online propaganda have a key
limitation: they were annotated using weak labels that can be noisy and even
incorrect. To address this limitation, our work makes the following
contributions: (1) We present \dataset: a novel dataset (N=30,000) for
detecting online propaganda with high-quality labels. To the best of our
knowledge, \dataset is the first dataset for detecting online propaganda that
was created through human annotation. (2) We show empirically that
state-of-the-art language models fail in detecting online propaganda when
trained with weak labels (AUC: 64.03). In contrast, state-of-the-art language
models can accurately detect online propaganda when trained with our
high-quality labels (AUC: 92.25), which is an improvement of ~44%. (3) To
address the cost of labeling, we extend our work to few-shot learning.
Specifically, we show that prompt-based learning using a small sample of
high-quality labels can still achieve a reasonable performance (AUC: 80.27).
Finally, we discuss implications for the NLP community to balance the cost and
quality of labeling. Crucially, our work highlights the importance of
high-quality labels for sensitive NLP tasks such as propaganda detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14933">
<div class="article-summary-box-inner">
<span><p>Model merging (e.g., via interpolation or task arithmetic) fuses multiple
models trained on different tasks to generate a multi-task solution. The
technique has been proven successful in previous studies, where the models are
trained on similar tasks and with the same initialization. In this paper, we
expand on this concept to a multimodal setup by merging transformers trained on
different modalities. Furthermore, we conduct our study for a novel goal where
we can merge vision, language, and cross-modal transformers of a
modality-specific architecture to create a parameter-efficient
modality-agnostic architecture. Through comprehensive experiments, we
systematically investigate the key factors impacting model performance after
merging, including initialization, merging mechanisms, and model architectures.
Our analysis leads to an effective training recipe for matching the performance
of the modality-agnostic baseline (i.e. pre-trained from scratch) via model
merging. Our code is available at: https://github.com/ylsung/vl-merging
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Redundancy and Biases in Public Document Information Extraction Benchmarks. (arXiv:2304.14936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14936">
<div class="article-summary-box-inner">
<span><p>Advances in the Visually-rich Document Understanding (VrDU) field and
particularly the Key-Information Extraction (KIE) task are marked with the
emergence of efficient Transformer-based approaches such as the LayoutLM
models. Despite the good performance of KIE models when fine-tuned on public
benchmarks, they still struggle to generalize on complex real-life use-cases
lacking sufficient document annotations. Our research highlighted that KIE
standard benchmarks such as SROIE and FUNSD contain significant similarity
between training and testing documents and can be adjusted to better evaluate
the generalization of models. In this work, we designed experiments to quantify
the information redundancy in public benchmarks, revealing a 75% template
replication in SROIE official test set and 16% in FUNSD. We also proposed
resampling strategies to provide benchmarks more representative of the
generalization ability of models. We showed that models not suited for document
analysis struggle on the adjusted splits dropping on average 10,5% F1 score on
SROIE and 3.5% on FUNSD compared to multi-modal models dropping only 7,5% F1 on
SROIE and 0.5% F1 on FUNSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data. (arXiv:2304.14953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14953">
<div class="article-summary-box-inner">
<span><p>In recent years, the field of document understanding has progressed a lot. A
significant part of this progress has been possible thanks to the use of
language models pretrained on large amounts of documents. However, pretraining
corpora used in the domain of document understanding are single domain,
monolingual, or nonpublic. Our goal in this paper is to propose an efficient
pipeline for creating a big-scale, diverse, multilingual corpus of PDF files
from all over the Internet using Common Crawl, as PDF files are the most
canonical types of documents as considered in document understanding. We
analysed extensively all of the steps of the pipeline and proposed a solution
which is a trade-off between data quality and processing time. We also share a
CCpdf corpus in a form or an index of PDF files along with a script for
downloading them, which produces a collection useful for language model
pretraining. The dataset and tools published with this paper offer researchers
the opportunity to develop even better multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14986">
<div class="article-summary-box-inner">
<span><p>When applied to Image-to-text models, interpretability methods often provide
token-by-token explanations namely, they compute a visual explanation for each
token of the generated sequence. Those explanations are expensive to compute
and unable to comprehensively explain the model's output. Therefore, these
models often require some sort of approximation that eventually leads to
misleading explanations. We develop a framework based on SHAP, that allows for
generating comprehensive, meaningful explanations leveraging the meaning
representation of the output sequence as a whole. Moreover, by exploiting
semantic priors in the visual backbone, we extract an arbitrary number of
features that allows the efficient computation of Shapley values on large-scale
models, generating at the same time highly meaningful visual explanations. We
demonstrate that our method generates semantically more expressive explanations
than traditional methods at a lower compute cost and that it can be generalized
over other explainability methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs. (arXiv:2304.14999v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14999">
<div class="article-summary-box-inner">
<span><p>As foundation models continue to exponentially scale in size, efficient
methods of adaptation become increasingly critical. Parameter-efficient
fine-tuning (PEFT), a recent class of techniques that require only modifying a
small percentage of the model parameters, is currently the most popular method
for adapting large language models (LLMs). Several PEFT techniques have
recently been proposed with varying tradeoffs. We provide a comprehensive and
uniform benchmark of various PEFT techniques across a representative LLM, the
FLAN-T5 model, and evaluate model performance across different data scales of
classification and generation datasets. Based on this, we provide a framework
for choosing the optimal fine-tuning techniques given the task type and data
availability. Contrary to popular belief, we also empirically prove that PEFT
techniques converge slower than full tuning in low data scenarios, and posit
the amount of data required for PEFT methods to both perform well and converge
efficiently. Lastly, we further optimize these PEFT techniques by selectively
choosing which parts of the model to train, and find that these techniques can
be applied with significantly fewer parameters while maintaining and even
improving performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.15010">
<div class="article-summary-box-inner">
<span><p>How to efficiently transform large language models (LLMs) into instruction
followers is recently a popular research direction, while training LLM for
multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter
demonstrates the potential to handle visual inputs with LLMs, it still cannot
generalize well to open-ended visual instructions and lags behind GPT-4. In
this paper, we present LLaMA-Adapter V2, a parameter-efficient visual
instruction model. Specifically, we first augment LLaMA-Adapter by unlocking
more learnable parameters (e.g., norm, bias and scale), which distribute the
instruction-following ability across the entire LLaMA model besides adapters.
Secondly, we propose an early fusion strategy to feed visual tokens only into
the early LLM layers, contributing to better visual knowledge incorporation.
Thirdly, a joint training paradigm of image-text pairs and
instruction-following data is introduced by optimizing disjoint groups of
learnable parameters. This strategy effectively alleviates the interference
between the two tasks of image-text alignment and instruction following and
achieves strong multi-modal reasoning with only a small-scale image-text and
instruction dataset. During inference, we incorporate additional expert models
(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image
understanding capability without incurring training costs. Compared to the
original LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal
instructions by merely introducing 14M parameters over LLaMA. The newly
designed framework also exhibits stronger language-only instruction-following
capabilities and even excels in chat interactions. Our code and models are
available at https://github.com/ZrrSkywalker/LLaMA-Adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data. (arXiv:2209.15329v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.15329">
<div class="article-summary-box-inner">
<span><p>How to boost speech pre-training with textual data is an unsolved problem due
to the fact that speech and text are very different modalities with distinct
characteristics. In this paper, we propose a cross-modal Speech and Language
Model (SpeechLM) to explicitly align speech and text pre-training with a
pre-defined unified discrete representation. Specifically, we introduce two
alternative discrete tokenizers to bridge the speech and text modalities,
including phoneme-unit and hidden-unit tokenizers, which can be trained using a
small amount of paired speech-text data. Based on the trained tokenizers, we
convert the unlabeled speech and text data into tokens of phoneme units or
hidden units. The pre-training objective is designed to unify the speech and
the text into the same discrete semantic space with a unified Transformer
network. Leveraging only 10K text sentences, our SpeechLM gets a 16\% relative
WER reduction over the best base model performance (from 6.8 to 5.7) on the
public LibriSpeech ASR benchmark. Moreover, SpeechLM with fewer parameters even
outperforms previous SOTA models on CoVoST-2 speech translation tasks. We also
evaluate our SpeechLM on various spoken language processing tasks under the
universal representation evaluation framework SUPERB, demonstrating significant
improvements on content-related tasks. Our code and models are available at
https://aka.ms/SpeechLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks. (arXiv:2210.04476v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.04476">
<div class="article-summary-box-inner">
<span><p>Demonstrations and natural language instructions are two common ways to
specify and teach robots novel tasks. However, for many complex tasks, a
demonstration or language instruction alone contains ambiguities, preventing
tasks from being specified clearly. In such cases, a combination of both a
demonstration and an instruction more concisely and effectively conveys the
task to the robot than either modality alone. To instantiate this problem
setting, we train a single multi-task policy on a few hundred challenging
robotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task
Conditioning), a method for conditioning a robotic policy on task embeddings
comprised of two components: a visual demonstration and a language instruction.
By allowing these two modalities to mutually disambiguate and clarify each
other during novel task specification, DeL-TaCo (1) substantially decreases the
teacher effort needed to specify a new task and (2) achieves better
generalization performance on novel objects and instructions over previous
task-conditioning methods. To our knowledge, this is the first work to show
that simultaneously conditioning a multi-task robotic manipulation policy on
both demonstration and language embeddings improves sample efficiency and
generalization over conditioning on either modality alone. See additional
materials at https://deltaco-robot.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Severity Classification of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15387">
<div class="article-summary-box-inner">
<span><p>Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08330">
<div class="article-summary-box-inner">
<span><p>Attention-based neural networks, such as Transformers, have become ubiquitous
in numerous applications, including computer vision, natural language
processing, and time-series analysis. In all kinds of attention networks, the
attention maps are crucial as they encode semantic dependencies between input
tokens. However, most existing attention networks perform modeling or reasoning
based on representations , wherein the attention maps of different layers are
learned separately without explicit interactions. In this paper, we propose a
novel and generic evolving attention mechanism, which directly models the
evolution of inter-token relationships through a chain of residual
convolutional modules. The major motivations are twofold. On the one hand, the
attention maps in different layers share transferable knowledge, thus adding a
residual connection can facilitate the information flow of inter-token
relationships across layers. On the other hand, there is naturally an
evolutionary trend among attention maps at different abstraction levels, so it
is beneficial to exploit a dedicated convolution-based module to capture this
process. Equipped with the proposed mechanism, the convolution-enhanced
evolving attention networks achieve superior performance in various
applications, including time-series representation, natural language
understanding, machine translation, and image classification. Especially on
time-series representation tasks, Evolving Attention-enhanced Dilated
Convolutional (EA-DC-) Transformer outperforms state-of-the-art models
significantly, achieving an average of 17% improvement compared to the best
SOTA. To the best of our knowledge, this is the first work that explicitly
models the layer-wise evolution of attention maps. Our implementation is
available at https://github.com/pkuyym/EvolvingAttention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models. (arXiv:2302.04662v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2302.04662">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs), such as Codex, hold great promise in enhancing
programming education by automatically generating feedback for students. We
investigate using LLMs to generate feedback for fixing syntax errors in Python
programs, a key scenario in introductory programming. More concretely, given a
student's buggy program, our goal is to generate feedback comprising a fixed
program along with a natural language explanation describing the errors/fixes,
inspired by how a human tutor would give feedback. While using LLMs is
promising, the critical challenge is to ensure high precision in the generated
feedback, which is imperative before deploying such technology in classrooms.
The main research question we study is: Can we develop LLMs-based feedback
generation techniques with a tunable precision parameter, giving educators
quality control over the feedback that students receive? To this end, we
introduce PyFiXV, our technique to generate high-precision feedback powered by
Codex. The key idea behind PyFiXV is to use a novel run-time validation
mechanism to decide whether the generated feedback is suitable for sharing with
the student; notably, this validation mechanism also provides a precision knob
to educators. We perform an extensive evaluation using two real-world datasets
of Python programs with syntax errors and show the efficacy of PyFiXV in
generating high-precision feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05351">
<div class="article-summary-box-inner">
<span><p>Recently, large language models (LLMs) like ChatGPT have demonstrated
remarkable performance across a variety of natural language processing tasks.
However, their effectiveness in the financial domain, specifically in
predicting stock market movements, remains to be explored. In this paper, we
conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal
stock movement prediction, on three tweets and historical stock price datasets.
Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited
success in predicting stock movements, as it underperforms not only
state-of-the-art methods but also traditional methods like linear regression
using price features. Despite the potential of Chain-of-Thought prompting
strategies and the inclusion of tweets, ChatGPT's performance remains subpar.
Furthermore, we observe limitations in its explainability and stability,
suggesting the need for more specialized training or fine-tuning. This research
provides insights into ChatGPT's capabilities and serves as a foundation for
future work aimed at improving financial market analysis and prediction by
leveraging social media sentiment and historical stock data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">chatClimate: Grounding Conversational AI in Climate Science. (arXiv:2304.05510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05510">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) have made significant progress in recent years,
achieving remarkable results in question-answering tasks (QA). However, they
still face two major challenges: hallucination and outdated information after
the training phase. These challenges take center stage in critical domains like
climate change, where obtaining accurate and up-to-date information from
reliable sources in a limited time is essential and difficult. To overcome
these barriers, one potential solution is to provide LLMs with access to
external, scientifically accurate, and robust sources (long-term memory) to
continuously update their knowledge and prevent the propagation of inaccurate,
incorrect, or outdated information. In this study, we enhanced GPT-4 by
integrating the information from the Sixth Assessment Report of the
Intergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable
source in this domain. We present our conversational AI prototype, available at
www.chatclimate.ai and demonstrate its ability to answer challenging questions
accurately in three different QA scenarios: asking from 1) GPT-4, 2)
chatClimate, and 3) hybrid chatClimate. The answers and their sources were
evaluated by our team of IPCC authors, who used their expert knowledge to score
the accuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation
showed that the hybrid chatClimate provided more accurate answers, highlighting
the effectiveness of our solution. This approach can be easily scaled for
chatbots in specific domains, enabling the delivery of reliable and accurate
information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human. (arXiv:2304.07849v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.07849">
<div class="article-summary-box-inner">
<span><p>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for
digital human applications that instruction finetunes on a wide range of
dialogue tasks in a unified internet-augmented format. Different from other
open-domain dialogue models that focus on large-scale pre-training and scaling
up model size or dialogue corpus, we aim to build a powerful and practical
dialogue system for digital human with diverse skills and good multi-task
generalization by internet-augmented instruction tuning. To this end, we first
conduct large-scale pre-training on both common document corpus and dialogue
data with curriculum learning, so as to inject various world knowledge and
dialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue
tasks spanning diverse features of knowledge, personality, multi-turn memory,
and empathy, on which we further instruction tune \modelname via unified
natural language instruction templates. External knowledge from an internet
search is also used during instruction finetuning for alleviating the problem
of knowledge hallucinations. We show that \modelname outperforms
state-of-the-art Chinese dialogue systems on both automatic and human
evaluation, and demonstrates strong multi-task generalization on a variety of
text understanding and generation tasks. In addition, we deploy \modelname to
real-world applications such as Smart Speaker and Instant Message applications
with fast inference. Our models and code will be made publicly available on
ModelScope~\footnote{\small{https://modelscope.cn/models/damo/ChatPLUG-3.7B}}
and Github~\footnote{\small{https://github.com/X-PLUG/ChatPLUG}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.09349">
<div class="article-summary-box-inner">
<span><p>Embodied AI focuses on the study and development of intelligent systems that
possess a physical or virtual embodiment (i.e. robots) and are able to
dynamically interact with their environment. Memory and control are the two
essential parts of an embodied system and usually require separate frameworks
to model each of them. In this paper, we propose a novel and generalizable
framework called LLM-Brain: using Large-scale Language Model as a robotic brain
to unify egocentric memory and control. The LLM-Brain framework integrates
multiple multimodal language models for robotic tasks, utilizing a zero-shot
learning approach. All components within LLM-Brain communicate using natural
language in closed-loop multi-round dialogues that encompass perception,
planning, control, and memory. The core of the system is an embodied LLM to
maintain egocentric memory and control the robot. We demonstrate LLM-Brain by
examining two downstream tasks: active exploration and embodied question
answering. The active exploration tasks require the robot to extensively
explore an unknown environment within a limited number of actions. Meanwhile,
the embodied question answering tasks necessitate that the robot answers
questions based on observations acquired during prior explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.10637">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a core natural language processing task in
which pre-trained language models have shown remarkable performance. However,
standard benchmarks like CoNLL 2003 do not address many of the challenges that
deployed NER systems face, such as having to classify emerging or complex
entities in a fine-grained way. In this paper we present a novel NER cascade
approach comprising three steps: first, identifying candidate entities in the
input sentence; second, linking the each candidate to an existing knowledge
base; third, predicting the fine-grained category for each entity candidate. We
empirically demonstrate the significance of external knowledge bases in
accurately classifying fine-grained and emerging entities. Our system exhibits
robust performance in the MultiCoNER2 shared task, even in the low-resource
language setting where we leverage knowledge bases of high-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. (arXiv:2304.13559v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13559">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class
of database systems that can seamlessly query text and tables using SQL. To
enable seamless querying of textual data using SQL in an MMDB, we propose to
extend relational databases with so-called multi-modal operators (MMOps) which
are based on the advances of recent large language models such as GPT-3. The
main idea of MMOps is that they allow text collections to be treated as tables
without the need to manually transform the data. As we show in our evaluation,
our MMDB prototype can not only outperform state-of-the-art approaches such as
text-to-table in terms of accuracy and performance but it also requires
significantly less training data to fine-tune the model for an unseen text
collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13714">
<div class="article-summary-box-inner">
<span><p>Despite growing interest in using large language models (LLMs) in healthcare,
current explorations do not assess the real-world utility and safety of LLMs in
clinical settings. Our objective was to determine whether two LLMs can serve
information needs submitted by physicians as questions to an informatics
consultation service in a safe and concordant manner. Sixty six questions from
an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple
prompts. 12 physicians assessed the LLM responses' possibility of patient harm
and concordance with existing reports from an informatics consultation service.
Physician assessments were summarized based on majority vote. For no questions
did a majority of physicians deem either LLM response as harmful. For GPT-3.5,
responses to 8 questions were concordant with the informatics consult report,
20 discordant, and 9 were unable to be assessed. There were 29 responses with
no majority on "Agree", "Disagree", and "Unable to assess". For GPT-4,
responses to 13 questions were concordant, 15 discordant, and 3 were unable to
be assessed. There were 35 responses with no majority. Responses from both LLMs
were largely devoid of overt harm, but less than 20% of the responses agreed
with an answer from an informatics consultation service, responses contained
hallucinated references, and physicians were divided on what constitutes harm.
These results suggest that while general purpose LLMs are able to provide safe
and credible responses, they often do not meet the specific information need of
a given question. A definitive evaluation of the usefulness of LLMs in
healthcare settings will likely require additional research on prompt
engineering, calibration, and custom-tailoring of general purpose models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Data Augmentation for Context-Dependent Text-to-SQL. (arXiv:2304.13902v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.13902">
<div class="article-summary-box-inner">
<span><p>The limited scale of annotated data constraints existing context-dependent
text-to-SQL models because of the complexity of labeling. The data augmentation
method is a commonly used method to solve this problem. However, the data
generated by current augmentation methods often lack diversity. In this paper,
we introduce ConDA, which generates interactive questions and corresponding SQL
results. We designed the SQL dialogue state to enhance the data diversity
through the state transition. Meanwhile, we also present a filter method to
ensure the data quality by a grounding model. Additionally, we utilize a
grounding model to identify and filter low-quality questions that mismatch the
state information. Experimental results on the SParC and CoSQL datasets show
that ConDA boosts the baseline model to achieve an average improvement of
$3.3\%$ on complex questions. Moreover, we analyze the augmented data, which
reveals that the data generated by ConDA are of high quality in both SQL
template hardness and types, turns, and question consistency.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-05-01 23:12:52.813236711 UTC">2023-05-01 23:12:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>