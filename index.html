<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-08-01T01:30:00Z">08-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI. (arXiv:2307.15715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15715">
<div class="article-summary-box-inner">
<span><p>Primary care professionals struggle to keep up to date with the latest
scientific literature critical in guiding evidence-based practice related to
their daily work. To help solve the above-mentioned problem, we employed
generative artificial intelligence techniques based on large-scale language
models to summarize abstracts of scientific papers. Our objective is to
investigate the potential of generative artificial intelligence in diminishing
the cognitive load experienced by practitioners, thus exploring its ability to
alleviate mental effort and burden. The study participants were provided with
two use cases related to preventive care and behavior change, simulating a
search for new scientific literature. The study included 113 university
students from Slovenia and the United States randomized into three distinct
study groups. The first group was assigned to the full abstracts. The second
group was assigned to the short abstracts generated by AI. The third group had
the option to select a full abstract in addition to the AI-generated short
summary. Each use case study included ten retrieved abstracts. Our research
demonstrates that the use of generative AI for literature review is efficient
and effective. The time needed to answer questions related to the content of
abstracts was significantly lower in groups two and three compared to the first
group using full abstracts. The results, however, also show significantly lower
accuracy in extracted knowledge in cases where full abstract was not available.
Such a disruptive technology could significantly reduce the time required for
healthcare professionals to keep up with the most recent scientific literature;
nevertheless, further developments are needed to help them comprehend the
knowledge accurately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Large Language Models for Natural Interface to Pharmacology Databases. (arXiv:2307.15717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15717">
<div class="article-summary-box-inner">
<span><p>The drug development process necessitates that pharmacologists undertake
various tasks, such as reviewing literature, formulating hypotheses, designing
experiments, and interpreting results. Each stage requires accessing and
querying vast amounts of information. In this abstract, we introduce a Large
Language Model (LLM)-based Natural Language Interface designed to interact with
structured information stored in databases. Our experiments demonstrate the
feasibility and effectiveness of the proposed framework. This framework can
generalize to query a wide range of pharmaceutical data and knowledge bases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering. (arXiv:2307.15745v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15745">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) has the potential to make the Internet more
accessible in an interactive way, allowing people who cannot see images to ask
questions about them. However, multiple studies have shown that people who are
blind or have low-vision prefer image explanations that incorporate the context
in which an image appears, yet current VQA datasets focus on images in
isolation. We argue that VQA models will not fully succeed at meeting people's
needs unless they take context into account. To further motivate and analyze
the distinction between different contexts, we introduce Context-VQA, a VQA
dataset that pairs images with contexts, specifically types of websites (e.g.,
a shopping website). We find that the types of questions vary systematically
across contexts. For example, images presented in a travel context garner 2
times more "Where?" questions, and images on social media and news garner 2.8
and 1.8 times more "Who?" questions than the average. We also find that context
effects are especially important when participants can't see the image. These
results demonstrate that context affects the types of questions asked and that
VQA models should be context-sensitive to better meet people's needs,
especially in accessibility settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection. (arXiv:2307.15752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15752">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a method for resume rating using Latent Dirichlet
Allocation (LDA) and entity detection with SpaCy. The proposed method first
extracts relevant entities such as education, experience, and skills from the
resume using SpaCy's Named Entity Recognition (NER). The LDA model then uses
these entities to rate the resume by assigning topic probabilities to each
entity. Furthermore, we conduct a detailed analysis of the entity detection
using SpaCy's NER and report its evaluation metrics. Using LDA, our proposed
system breaks down resumes into latent topics and extracts meaningful semantic
representations. With a vision to define our resume score to be more
content-driven rather than a structure and keyword match driven, our model has
achieved 77% accuracy with respect to only skills in consideration and an
overall 82% accuracy with all attributes in consideration. (like college name,
work experience, degree and skills)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons in Reproducibility: Insights from NLP Studies in Materials Science. (arXiv:2307.15759v1 [physics.chem-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15759">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP), a cornerstone field within artificial
intelligence, has been increasingly utilized in the field of materials science
literature. Our study conducts a reproducibility analysis of two pioneering
works within this domain: "Machine-learned and codified synthesis parameters of
oxide materials" by Kim et al., and "Unsupervised word embeddings capture
latent knowledge from materials science literature" by Tshitoyan et al. We aim
to comprehend these studies from a reproducibility perspective, acknowledging
their significant influence on the field of materials informatics, rather than
critiquing them. Our study indicates that both papers offered thorough
workflows, tidy and well-documented codebases, and clear guidance for model
evaluation. This makes it easier to replicate their results successfully and
partially reproduce their findings. In doing so, they set commendable standards
for future materials science publications to aspire to. However, our analysis
also highlights areas for improvement such as to provide access to training
data where copyright restrictions permit, more transparency on model
architecture and the training process, and specifications of software
dependency versions. We also cross-compare the word embedding models between
papers, and find that some key differences in reproducibility and
cross-compatibility are attributable to design choices outside the bounds of
the models themselves. In summary, our study appreciates the benchmark set by
these seminal papers while advocating for further enhancements in research
reproducibility practices in the field of NLP for materials science. This
balance of understanding and continuous improvement will ultimately propel the
intersecting domains of NLP and materials science literature into a future of
exciting discoveries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15770">
<div class="article-summary-box-inner">
<span><p>In the face of climate change, are companies really taking substantial steps
toward more sustainable operations? A comprehensive answer lies in the dense,
information-rich landscape of corporate sustainability reports. However, the
sheer volume and complexity of these reports make human analysis very costly.
Therefore, only a few entities worldwide have the resources to analyze these
reports at scale, which leads to a lack of transparency in sustainability
reporting. Empowering stakeholders with LLM-based automatic analysis tools can
be a promising way to democratize sustainability report analysis. However,
developing such tools is challenging due to (1) the hallucination of LLMs and
(2) the inefficiency of bringing domain experts into the AI development loop.
In this paper, we ChatReport, a novel LLM-based system to automate the analysis
of corporate sustainability reports, addressing existing challenges by (1)
making the answers traceable to reduce the harm of hallucination and (2)
actively involving domain experts in the development loop. We make our
methodology, annotated datasets, and generated analyses of 1015 reports
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Hydra Effect: Emergent Self-repair in Language Model Computations. (arXiv:2307.15771v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15771">
<div class="article-summary-box-inner">
<span><p>We investigate the internal structure of language model computations using
causal analysis and demonstrate two motifs: (1) a form of adaptive computation
where ablations of one attention layer of a language model cause another layer
to compensate (which we term the Hydra effect) and (2) a counterbalancing
function of late MLP layers that act to downregulate the maximum-likelihood
token. Our ablation studies demonstrate that language model layers are
typically relatively loosely coupled (ablations to one layer only affect a
small number of downstream layers). Surprisingly, these effects occur even in
language models trained without any form of dropout. We analyse these effects
in the context of factual recall and consider their implications for
circuit-level attribution in language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation. (arXiv:2307.15776v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15776">
<div class="article-summary-box-inner">
<span><p>Injecting textual information into knowledge graph (KG) entity
representations has been a worthwhile expedition in terms of improving
performance in KG oriented tasks within the NLP community. External knowledge
often adopted to enhance KG embeddings ranges from semantically rich lexical
dependency parsed features to a set of relevant key words to entire text
descriptions supplied from an external corpus such as wikipedia and many more.
Despite the gains this innovation (Text-enhanced KG embeddings) has made, the
proposal in this work suggests that it can be improved even further. Instead of
using a single text description (which would not sufficiently represent an
entity because of the inherent lexical ambiguity of text), we propose a
multi-task framework that jointly selects a set of text descriptions relevant
to KG entities as well as align or augment KG embeddings with text
descriptions. Different from prior work that plugs formal entity descriptions
declared in knowledge bases, this framework leverages a retriever model to
selectively identify richer or highly relevant text descriptions to use in
augmenting entities. Furthermore, the framework treats the number of
descriptions to use in augmentation process as a parameter, which allows the
flexibility of enumerating across several numbers before identifying an
appropriate number. Experiment results for Link Prediction demonstrate a 5.5%
and 3.5% percentage increase in the Mean Reciprocal Rank (MRR) and Hits@10
scores respectively, in comparison to text-enhanced knowledge graph
augmentation methods using traditional CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15780">
<div class="article-summary-box-inner">
<span><p>We investigate various prompting strategies for enhancing personalized
content recommendation performance with large language models (LLMs) through
input augmentation. Our proposed approach, termed LLM-Rec, encompasses four
distinct prompting strategies: (1) basic prompting, (2) recommendation-driven
prompting, (3) engagement-guided prompting, and (4) recommendation-driven +
engagement-guided prompting. Our empirical experiments show that combining the
original content description with the augmented input text generated by LLM
using these prompting strategies leads to improved recommendation performance.
This finding highlights the importance of incorporating diverse prompts and
input augmentation techniques to enhance the recommendation capabilities with
large language models for personalized content recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. (arXiv:2307.15818v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15818">
<div class="article-summary-box-inner">
<span><p>We study how vision-language models trained on Internet-scale data can be
incorporated directly into end-to-end robotic control to boost generalization
and enable emergent semantic reasoning. Our goal is to enable a single
end-to-end trained model to both learn to map robot observations to actions and
enjoy the benefits of large-scale pretraining on language and vision-language
data from the web. To this end, we propose to co-fine-tune state-of-the-art
vision-language models on both robotic trajectory data and Internet-scale
vision-language tasks, such as visual question answering. In contrast to other
approaches, we propose a simple, general recipe to achieve this goal: in order
to fit both natural language responses and robotic actions into the same
format, we express the actions as text tokens and incorporate them directly
into the training set of the model in the same way as natural language tokens.
We refer to such category of models as vision-language-action models (VLA) and
instantiate an example of such a model, which we call RT-2. Our extensive
evaluation (6k evaluation trials) shows that our approach leads to performant
robotic policies and enables RT-2 to obtain a range of emergent capabilities
from Internet-scale training. This includes significantly improved
generalization to novel objects, the ability to interpret commands not present
in the robot training data (such as placing an object onto a particular number
or icon), and the ability to perform rudimentary reasoning in response to user
commands (such as picking up the smallest or largest object, or the one closest
to another object). We further show that incorporating chain of thought
reasoning allows RT-2 to perform multi-stage semantic reasoning, for example
figuring out which object to pick up for use as an improvised hammer (a rock),
or which type of drink is best suited for someone who is tired (an energy
drink).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Shaping: Empowering Agents through NPC Interaction. (arXiv:2307.15833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15833">
<div class="article-summary-box-inner">
<span><p>One major challenge in reinforcement learning (RL) is the large amount of
steps for the RL agent needs to converge in the training process and learn the
optimal policy, especially in text-based game environments where the action
space is extensive. However, non-player characters (NPCs) sometimes hold some
key information about the game, which can potentially help to train RL agents
faster. Thus, this paper explores how to interact and converse with NPC agents
to get the key information using large language models (LLMs), as well as
incorporate this information to speed up RL agent's training using knowledge
graphs (KGs) and Story Shaping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ATESA-B{\AE}RT: A Heterogeneous Ensemble Learning Model for Aspect-Based Sentiment Analysis. (arXiv:2307.15920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15920">
<div class="article-summary-box-inner">
<span><p>The increasing volume of online reviews has made possible the development of
sentiment analysis models for determining the opinion of customers regarding
different products and services. Until now, sentiment analysis has proven to be
an effective tool for determining the overall polarity of reviews. To improve
the granularity at the aspect level for a better understanding of the service
or product, the task of aspect-based sentiment analysis aims to first identify
aspects and then determine the user's opinion about them. The complexity of
this task lies in the fact that the same review can present multiple aspects,
each with its own polarity. Current solutions have poor performance on such
data. We address this problem by proposing ATESA-B{\AE}RT, a heterogeneous
ensemble learning model for Aspect-Based Sentiment Analysis. Firstly, we divide
our problem into two sub-tasks, i.e., Aspect Term Extraction and Aspect Term
Sentiment Analysis. Secondly, we use the \textit{argmax} multi-class
classification on six transformers-based learners for each sub-task. Initial
experiments on two datasets prove that ATESA-B{\AE}RT outperforms current
state-of-the-art solutions while solving the many aspects problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning. (arXiv:2307.15933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15933">
<div class="article-summary-box-inner">
<span><p>Large-scale language models such as DNABert and LOGO aim to learn optimal
gene representations and are trained on the entire Human Reference Genome.
However, standard tokenization schemes involve a simple sliding window of
tokens like k-mers that do not leverage any gene-based semantics and thus may
lead to (trivial) masking of easily predictable sequences and subsequently
inefficient Masked Language Modeling (MLM) training. Therefore, we propose a
novel masking algorithm, GeneMask, for MLM training of gene sequences, where we
randomly identify positions in a gene sequence as mask centers and locally
select the span around the mask center with the highest Normalized Pointwise
Mutual Information (NPMI) to mask. We observe that in the absence of
human-understandable semantics in the genomics domain (in contrast, semantic
units like words and phrases are inherently available in NLP), GeneMask-based
models substantially outperform the SOTA models (DNABert and LOGO) over four
benchmark gene sequence classification datasets in five few-shot settings (10
to 1000-shot). More significantly, the GeneMask-based DNABert model is trained
for less than one-tenth of the number of epochs of the original SOTA model. We
also observe a strong correlation between top-ranked PMI tokens and conserved
DNA sequence motifs, which may indicate the incorporation of latent genomic
information. The codes (including trained models) and datasets are made
publicly available at https://github.com/roysoumya/GeneMask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15936">
<div class="article-summary-box-inner">
<span><p>A major driver of AI products today is the fact that new skills emerge in
language models when their parameter set and training corpora are scaled up.
This phenomenon is poorly understood, and a mechanistic explanation via
mathematical analysis of gradient-based training seems difficult. The current
paper takes a different approach, analysing emergence using the famous (and
empirical) Scaling Laws of LLMs and a simple statistical framework.
Contributions include: (a) A statistical framework that relates cross-entropy
loss of LLMs to competence on the basic skills that underlie language tasks.
(b) Mathematical analysis showing that the Scaling Laws imply a strong form of
inductive bias that allows the pre-trained model to learn very efficiently. We
informally call this {\em slingshot generalization} since naively viewed it
appears to give competence levels at skills that violate usual generalization
theory. (c) A key example of slingshot generalization, that competence at
executing tasks involving $k$-tuples of skills emerges essentially at the same
scaling and same rate as competence on the elementary skills themselves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Codable Text Watermarking for Large Language Models. (arXiv:2307.15992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15992">
<div class="article-summary-box-inner">
<span><p>As large language models (LLMs) generate texts with increasing fluency and
realism, there is a growing need to identify the source of texts to prevent the
abuse of LLMs. Text watermarking techniques have proven reliable in
distinguishing whether a text is generated by LLMs by injecting hidden patterns
into the generated texts. However, we argue that existing watermarking methods
for LLMs are encoding-inefficient (only contain one bit of information -
whether it is generated from an LLM or not) and cannot flexibly meet the
diverse information encoding needs (such as encoding model version, generation
time, user id, etc.) in different LLMs application scenarios. In this work, we
conduct the first systematic study on the topic of Codable Text Watermarking
for LLMs (CTWL) that allows text watermarks to carry more customizable
information. First of all, we study the taxonomy of LLM watermarking technology
and give a mathematical formulation for CTWL. Additionally, we provide a
comprehensive evaluation system for CTWL: (1) watermarking success rate, (2)
robustness against various corruptions, (3) coding rate of payload information,
(4) encoding and decoding efficiency, (5) impacts on the quality of the
generated text. To meet the requirements of these non-Pareto-improving metrics,
we devise a CTWL method named Balance-Marking, based on the motivation of
ensuring that available and unavailable vocabularies for encoding information
have approximately equivalent probabilities. Compared to the random vocabulary
partitioning extended from the existing work, a probability-balanced vocabulary
partition can significantly improve the quality of the generated text.
Extensive experimental results have shown that our method outperforms a direct
baseline under comprehensive evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoCar: A Relationship Network-based Evaluation Method to Large Language Models. (arXiv:2307.15997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15997">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have received increasing attention. However, due
to the complexity of its capabilities, how to rationally evaluate the
capabilities of LLMs is still a task to be solved. We propose the RoCar method,
which utilizes the defined basic schemas to randomly construct a task graph and
generates natural language evaluation tasks based on the task graph to evaluate
the reasoning and memory abilities of LLMs respectively. Due to the very large
randomness of the task construction process, it is possible to ensure that none
of the LLMs to be tested has directly learned the evaluation tasks,
guaranteeing the fairness of the evaluation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations. (arXiv:2307.16013v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16013">
<div class="article-summary-box-inner">
<span><p>Data visualization (DV) has become the prevailing tool in the market due to
its effectiveness into illustrating insights in vast amounts of data. To lower
the barrier of using DVs, automatic DV tasks, such as natural language question
(NLQ) to visualization translation (formally called text-to-vis), have been
investigated in the research community. However, text-to-vis assumes the NLQ to
be well-organized and expressed in a single sentence. However, in real-world
settings, complex DV is needed through consecutive exchanges between the DV
system and the users. In this paper, we propose a new task named CoVis, short
for Conversational text-to-Visualization, aiming at constructing DVs through a
series of interactions between users and the system. Since it is the task which
has not been studied in the literature, we first build a benchmark dataset
named Dial-NVBench, including dialogue sessions with a sequence of queries from
a user and responses from the system. Then, we propose a multi-modal neural
network named MMCoVisNet to answer these DV-related queries. In particular,
MMCoVisNet first fully understands the dialogue context and determines the
corresponding responses. Then, it uses adaptive decoders to provide the
appropriate replies: (i) a straightforward text decoder is used to produce
general responses, (ii) an SQL-form decoder is applied to synthesize data
querying responses, and (iii) a DV-form decoder tries to construct the
appropriate DVs. We comparatively evaluate MMCoVisNet with other baselines over
our proposed benchmark dataset. Experimental results validate that MMCoVisNet
performs better than existing baselines and achieves a state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16039">
<div class="article-summary-box-inner">
<span><p>A key technology for the development of large language models (LLMs) involves
instruction tuning that helps align the models' responses with human
expectations to realize impressive learning abilities. Two major approaches for
instruction tuning characterize supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), which are currently applied to produce the
best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for
research and development efforts, various instruction-tuned open-source LLMs
have also been introduced recently, e.g., Alpaca, Vicuna, to name a few.
However, existing open-source LLMs have only been instruction-tuned for English
and a few popular languages, thus hindering their impacts and accessibility to
many other languages in the world. Among a few very recent work to explore
instruction tuning for LLMs in multiple languages, SFT has been used as the
only approach to instruction-tune LLMs for multiple languages. This has left a
significant gap for fine-tuned LLMs based on RLHF in diverse languages and
raised important questions on how RLHF can boost the performance of
multilingual instruction tuning. To overcome this issue, we present Okapi, the
first system with instruction-tuned LLMs based on RLHF for multiple languages.
Okapi introduces instruction and response-ranked data in 26 diverse languages
to facilitate the experiments and development of future multilingual LLM
research. We also present benchmark datasets to enable the evaluation of
generative LLMs in multiple languages. Our experiments demonstrate the
advantages of RLHF for multilingual instruction over SFT for different base
models and datasets. Our framework and resources are released at
\url{https://github.com/nlp-uoregon/Okapi}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Extraction of the Romanian Academic Word List: Data and Methods. (arXiv:2307.16045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16045">
<div class="article-summary-box-inner">
<span><p>This paper presents the methodology and data used for the automatic
extraction of the Romanian Academic Word List (Ro-AWL). Academic Word Lists are
useful in both L2 and L1 teaching contexts. For the Romanian language, no such
resource exists so far. Ro-AWL has been generated by combining methods from
corpus and computational linguistics with L2 academic writing approaches. We
use two types of data: (a) existing data, such as the Romanian Frequency List
based on the ROMBAC corpus, and (b) self-compiled data, such as the expert
academic writing corpus EXPRES. For constructing the academic word list, we
follow the methodology for building the Academic Vocabulary List for the
English language. The distribution of Ro-AWL features (general distribution,
POS distribution) into four disciplinary datasets is in line with previous
research. Ro-AWL is freely available and can be used for teaching, research and
NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">\`{I}r\`{o}y\`{i}nSpeech: A multi-purpose Yor\`{u}b\'{a} Speech Corpus. (arXiv:2307.16071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16071">
<div class="article-summary-box-inner">
<span><p>We introduce the \`{I}r\`{o}y\`{i}nSpeech corpus -- a new dataset influenced
by a desire to increase the amount of high quality, freely available,
contemporary Yor\`{u}b\'{a} speech. We release a multi-purpose dataset that can
be used for both TTS and ASR tasks. We curated text sentences from the news and
creative writing domains under an open license i.e., CC-BY-4.0 and had multiple
speakers record each sentence. We provide 5000 of our utterances to the Common
Voice platform to crowdsource transcriptions online. The dataset has 38.5 hours
of data in total, recorded by 80 volunteers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System. (arXiv:2307.16081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16081">
<div class="article-summary-box-inner">
<span><p>We introduce TacoBot, a user-centered task-oriented digital assistant
designed to guide users through complex real-world tasks with multiple steps.
Covering a wide range of cooking and how-to tasks, we aim to deliver a
collaborative and engaging dialogue experience. Equipped with language
understanding, dialogue management, and response generation components
supported by a robust search engine, TacoBot ensures efficient task assistance.
To enhance the dialogue experience, we explore a series of data augmentation
strategies using LLMs to train advanced neural models continuously. TacoBot
builds upon our successful participation in the inaugural Alexa Prize TaskBot
Challenge, where our team secured third place among ten competing teams. We
offer TacoBot as an open-source framework that serves as a practical example
for deploying task-oriented dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16082">
<div class="article-summary-box-inner">
<span><p>Social platforms have emerged as a crucial platform for disseminating and
discussing information about real-life events, which offers an excellent
opportunity for early detection of newsworthy events. However, most existing
approaches for event detection solely exploit keyword burstiness or network
structures to detect hot events. Thus, they often fail to identify emerging
social events before reaching a trending state regarding the challenging nature
of events and social data. Social data, e.g., tweets, is characterized by
misspellings, incompleteness, ambiguity, and irregular language, as well as
variation in aspects of opinions. Moreover, learning the evolving
characteristics of the events utilizing limited contextual knowledge is almost
infeasible for machine learning models. To address these problems, in this
paper, we propose a framework that exploits the lexical, semantic, and
contextual representations of streaming social data. In particular, we leverage
contextual knowledge to detect semantically related tweets in their earliest
emergence and enhance the quality of produced clusters. We next produce a
cluster chains for each event to show the evolving variation of the event
through time. We conducted extensive experiments to evaluate our framework,
validating the effectiveness of the proposed framework in detecting and
distinguishing social events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16125">
<div class="article-summary-box-inner">
<span><p>Based on powerful Large Language Models (LLMs), recent generative Multimodal
Large Language Models (MLLMs) have gained prominence as a pivotal research
area, exhibiting remarkable capability for both comprehension and generation.
In this work, we address the evaluation of generative comprehension in MLLMs as
a preliminary step towards a comprehensive assessment of generative models, by
introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple
choice questions with accurate human annotations (x 6 larger than existing
benchmarks), which spans 12 evaluation dimensions including the comprehension
of both the image and video modality. We develop an advanced pipeline for
generating multiple-choice questions that target specific evaluation
dimensions, integrating both automatic filtering and manual verification
processes. Multiple-choice questions with groundtruth options derived from
human annotation enables an objective and efficient assessment of model
performance, eliminating the need for human or GPT intervention during
evaluation. We further evaluate the performance of 18 models across all 12
dimensions, covering both the spatial and temporal understanding. By revealing
the limitations of existing MLLMs through evaluation results, we aim for
SEED-Bench to provide insights for motivating future research. We will launch
and consistently maintain a leaderboard to provide a platform for the community
to assess and investigate model capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination. (arXiv:2307.16139v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16139">
<div class="article-summary-box-inner">
<span><p>In modern dialogue systems, the use of Large Language Models (LLMs) has grown
exponentially due to their capacity to generate diverse, relevant, and creative
responses. Despite their strengths, striking a balance between the LLMs'
creativity and their faithfulness to external knowledge remains a key
challenge. This paper presents an innovative user-controllable mechanism that
modulates the balance between an LLM's imaginative capabilities and its
adherence to factual information. Our approach incorporates a numerical tag
during the fine-tuning phase of the LLM's training, representing the degree of
faithfulness to the reference knowledge in the generated responses. This degree
is computed through an automated process that measures lexical overlap using
ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's
self-evaluation score. During model inference, users can manipulate this
numerical tag, thus controlling the degree of the LLM's reliance on external
knowledge. We conduct extensive experiments across various scenarios,
demonstrating the adaptability of our method and its efficacy in ensuring the
quality and accuracy of the LLM's responses. The results highlight the
potential of our approach to enhance the versatility of LLMs while maintaining
a balance between creativity and hallucination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models. (arXiv:2307.16180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16180">
<div class="article-summary-box-inner">
<span><p>The field of large language models (LLMs) has made significant progress, and
their knowledge storage capacity is approaching that of human beings.
Furthermore, advanced techniques, such as prompt learning and reinforcement
learning, are being employed to address ethical concerns and hallucination
problems associated with LLMs, bringing them closer to aligning with human
values. This situation naturally raises the question of whether LLMs with
human-like abilities possess a human-like personality? In this paper, we aim to
investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a
widespread human personality assessment tool, as an evaluation metric for LLMs.
Specifically, extensive experiments will be conducted to explore: 1) the
personality types of different LLMs, 2) the possibility of changing the
personality types by prompt engineering, and 3) How does the training dataset
affect the model's personality. Although the MBTI is not a rigorous assessment,
it can still reflect the similarity between LLMs and human personality. In
practice, the MBTI has the potential to serve as a rough indicator. Our codes
are available at
https://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving TTS for Shanghainese: Addressing Tone Sandhi via Word Segmentation. (arXiv:2307.16199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16199">
<div class="article-summary-box-inner">
<span><p>Tone is a crucial component of the prosody of Shanghainese, a Wu Chinese
variety spoken primarily in urban Shanghai. Tone sandhi, which applies to all
multi-syllabic words in Shanghainese, then, is key to natural-sounding speech.
Unfortunately, recent work on Shanghainese TTS (text-to-speech) such as Apple's
VoiceOver has shown poor performance with tone sandhi, especially LD
(left-dominant sandhi). Here I show that word segmentation during text
preprocessing can improve the quality of tone sandhi production in TTS models.
Syllables within the same word are annotated with a special symbol, which
serves as a proxy for prosodic information of the domain of LD. Contrary to the
common practice of using prosodic annotation mainly for static pauses, this
paper demonstrates that prosodic annotation can also be applied to dynamic
tonal phenomena. I anticipate this project to be a starting point for bringing
formal linguistic accounts of Shanghainese into computational projects. Too
long have we been using the Mandarin models to approximate Shanghainese, but it
is a different language with its own linguistic features, and its digitisation
and revitalisation should be treated as such.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16200">
<div class="article-summary-box-inner">
<span><p>This paper focuses on term-status pair extraction from medical dialogues
(MD-TSPE), which is essential in diagnosis dialogue systems and the automatic
scribe of electronic medical records (EMRs). In the past few years, works on
MD-TSPE have attracted increasing research attention, especially after the
remarkable progress made by generative methods. However, these generative
methods output a whole sequence consisting of term-status pairs in one stage
and ignore integrating prior knowledge, which demands a deeper understanding to
model the relationship between terms and infer the status of each term. This
paper presents a knowledge-enhanced two-stage generative framework (KTGF) to
address the above challenges. Using task-specific prompts, we employ a single
model to complete the MD-TSPE through two phases in a unified generative form:
we generate all terms the first and then generate the status of each generated
term. In this way, the relationship between terms can be learned more
effectively from the sequence containing only terms in the first phase, and our
designed knowledge-enhanced prompt in the second phase can leverage the
category and status candidates of the generated term for status generation.
Furthermore, our proposed special status ``not mentioned" makes more terms
available and enriches the training data in the second phase, which is critical
in the low-resource setting. The experiments on the Chunyu and CMDD datasets
show that the proposed method achieves superior results compared to the
state-of-the-art models in the full training and low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks. (arXiv:2307.16208v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16208">
<div class="article-summary-box-inner">
<span><p>One of the key AI tools for textual corpora exploration is natural language
question-answering (QA). Unlike keyword-based search engines, QA algorithms
receive and process natural language questions and produce precise answers to
these questions, rather than long lists of documents that need to be manually
scanned by the users. State-of-the-art QA algorithms based on DNNs were
successfully employed in various domains. However, QA in the genealogical
domain is still underexplored, while researchers in this field (and other
fields in humanities and social sciences) can highly benefit from the ability
to ask questions in natural language, receive concrete answers and gain
insights hidden within large corpora. While some research has been recently
conducted for factual QA in the genealogical domain, to the best of our
knowledge, there is no previous research on the more challenging task of
numerical aggregation QA (i.e., answering questions combining aggregation
functions, e.g., count, average, max). Numerical aggregation QA is critical for
distant reading and analysis for researchers (and the general public)
interested in investigating cultural heritage domains. Therefore, in this
study, we present a new end-to-end methodology for numerical aggregation QA for
genealogical trees that includes: 1) an automatic method for training dataset
generation; 2) a transformer-based table selection method, and 3) an optimized
transformer-based numerical aggregation QA model. The findings indicate that
the proposed architecture, GLOBE, outperforms the state-of-the-art models and
pipelines by achieving 87% accuracy for this task compared to only 21% by
current state-of-the-art models. This study may have practical implications for
genealogical information centers and museums, making genealogical data research
easy and scalable for experts as well as the general public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16213">
<div class="article-summary-box-inner">
<span><p>Over the past few decades, large archives of paper-based historical
documents, such as books and newspapers, have been digitized using the Optical
Character Recognition (OCR) technology. Unfortunately, this broadly used
technology is error-prone, especially when an OCRed document was written
hundreds of years ago. Neural networks have shown great success in solving
various text processing tasks, including OCR post-correction. The main
disadvantage of using neural networks for historical corpora is the lack of
sufficiently large training datasets they require to learn from, especially for
morphologically-rich languages like Hebrew. Moreover, it is not clear what are
the optimal structure and values of hyperparameters (predefined parameters) of
neural networks for OCR error correction in Hebrew due to its unique features.
Furthermore, languages change across genres and periods. These changes may
affect the accuracy of OCR post-correction neural network models. To overcome
these challenges, we developed a new multi-phase method for generating
artificial training datasets with OCR errors and hyperparameters optimization
for building an effective neural network for OCR post-correction in Hebrew.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs. (arXiv:2307.16214v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16214">
<div class="article-summary-box-inner">
<span><p>With the rising popularity of user-generated genealogical family trees, new
genealogical information systems have been developed. State-of-the-art natural
question answering algorithms use deep neural network (DNN) architecture based
on self-attention networks. However, some of these models use sequence-based
inputs and are not suitable to work with graph-based structure, while
graph-based DNN models rely on high levels of comprehensiveness of knowledge
graphs that is nonexistent in the genealogical domain. Moreover, these
supervised DNN models require training datasets that are absent in the
genealogical domain. This study proposes an end-to-end approach for question
answering using genealogical family trees by: 1) representing genealogical data
as knowledge graphs, 2) converting them to texts, 3) combining them with
unstructured texts, and 4) training a trans-former-based question answering
model. To evaluate the need for a dedicated approach, a comparison between the
fine-tuned model (Uncle-BERT) trained on the auto-generated genealogical
dataset and state-of-the-art question-answering models was per-formed. The
findings indicate that there are significant differences between answering
genealogical questions and open-domain questions. Moreover, the proposed
methodology reduces complexity while increasing accuracy and may have practical
implications for genealogical research and real-world projects, making
genealogical data accessible to experts as well as the general public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science. (arXiv:2307.16217v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16217">
<div class="article-summary-box-inner">
<span><p>Combining computational technologies and humanities is an ongoing effort
aimed at making resources such as texts, images, audio, video, and other
artifacts digitally available, searchable, and analyzable. In recent years,
deep neural networks (DNN) dominate the field of automatic text analysis and
natural language processing (NLP), in some cases presenting a super-human
performance. DNNs are the state-of-the-art machine learning algorithms solving
many NLP tasks that are relevant for Digital Humanities (DH) research, such as
spell checking, language detection, entity extraction, author detection,
question answering, and other tasks. These supervised algorithms learn patterns
from a large number of "right" and "wrong" examples and apply them to new
examples. However, using DNNs for analyzing the text resources in DH research
presents two main challenges: (un)availability of training data and a need for
domain adaptation. This paper explores these challenges by analyzing multiple
use-cases of DH studies in recent literature and their possible solutions and
lays out a practical decision model for DH experts for when and how to choose
the appropriate deep learning approaches for their research. Moreover, in this
paper, we aim to raise awareness of the benefits of utilizing deep learning
models in the DH community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16220">
<div class="article-summary-box-inner">
<span><p>Over the past few decades, large archives of paper-based documents such as
books and newspapers have been digitized using Optical Character Recognition.
This technology is error-prone, especially for historical documents. To correct
OCR errors, post-processing algorithms have been proposed based on natural
language analysis and machine learning techniques such as neural networks.
Neural network's disadvantage is the vast amount of manually labeled data
required for training, which is often unavailable. This paper proposes an
innovative method for training a light-weight neural network for Hebrew OCR
post-correction using significantly less manually created data. The main
research goal is to develop a method for automatically generating language and
task-specific training data to improve the neural network results for OCR
post-correction, and to investigate which type of dataset is the most effective
for OCR post-correction of historical documents. To this end, a series of
experiments using several datasets was conducted. The evaluation corpus was
based on Hebrew newspapers from the JPress project. An analysis of historical
OCRed newspapers was done to learn common language and corpus-specific OCR
errors. We found that training the network using the proposed method is more
effective than using randomly generated errors. The results also show that the
performance of the neural network for OCR post-correction strongly depends on
the genre and area of the training data. Moreover, neural networks that were
trained with the proposed method outperform other state-of-the-art neural
networks for OCR post-correction and complex spellcheckers. These results may
have practical implications for many digital humanities projects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16230">
<div class="article-summary-box-inner">
<span><p>Recently, text watermarking algorithms for large language models (LLMs) have
been mitigating the potential harms of text generated by the LLMs, including
fake news and copyright issues. However, the watermark detection of current
text algorithms requires the key from the generation process, making them
susceptible to breaches and counterfeiting. In this work, we propose the first
private watermarking algorithm, which extends the current text watermarking
algorithms by using two different neural networks respectively for watermark
generation and detection, rather than using the same key at both stages.
Meanwhile, part of the parameters of the watermark generation and detection
networks are shared, which makes the detection network achieve a high accuracy
very efficiently. Experiments show that our algorithm ensures high detection
accuracy with minimal impact on generation and detection speed, due to the
small parameter size of both networks. Additionally, our subsequent analysis
demonstrates the difficulty of reverting the watermark generation rules from
the detection network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances in Hierarchical Multi-label Text Classification: A Survey. (arXiv:2307.16265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16265">
<div class="article-summary-box-inner">
<span><p>Hierarchical multi-label text classification aims to classify the input text
into multiple labels, among which the labels are structured and hierarchical.
It is a vital task in many real world applications, e.g. scientific literature
archiving. In this paper, we survey the recent progress of hierarchical
multi-label text classification, including the open sourced data sets, the main
methods, evaluation metrics, learning strategies and the current challenges. A
few future research directions are also listed for community to further improve
this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mispronunciation detection using self-supervised speech representations. (arXiv:2307.16324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16324">
<div class="article-summary-box-inner">
<span><p>In recent years, self-supervised learning (SSL) models have produced
promising results in a variety of speech-processing tasks, especially in
contexts of data scarcity. In this paper, we study the use of SSL models for
the task of mispronunciation detection for second language learners. We compare
two downstream approaches: 1) training the model for phone recognition (PR)
using native English data, and 2) training a model directly for the target task
using non-native English data. We compare the performance of these two
approaches for various SSL representations as well as a representation
extracted from a traditional DNN-based speech recognition model. We evaluate
the models on L2Arctic and EpaDB, two datasets of non-native speech annotated
with pronunciation labels at the phone level. Overall, we find that using a
downstream model trained for the target task gives the best performance and
that most upstream models perform similarly for the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distractor generation for multiple-choice questions with predictive prompting and large language models. (arXiv:2307.16338v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.16338">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable
performance across various tasks and have garnered significant attention from
both researchers and practitioners. However, in an educational context, we
still observe a performance gap in generating distractors -- i.e., plausible
yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In
this study, we propose a strategy for guiding LLMs such as ChatGPT, in
generating relevant distractors by prompting them with question items
automatically retrieved from a question bank as well-chosen in-context
examples. We evaluate our LLM-based solutions using a quantitative assessment
on an existing test set, as well as through quality annotations by human
experts, i.e., teachers. We found that on average 53% of the generated
distractors presented to the teachers were rated as high-quality, i.e.,
suitable for immediate use as is, outperforming the state-of-the-art model. We
also show the gains of our approach 1 in generating high-quality distractors by
comparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with
static examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Query-Focused Summarization as A Knowledge-Intensive Task: A Pilot Study. (arXiv:2112.07536v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07536">
<div class="article-summary-box-inner">
<span><p>Query-focused summarization (QFS) requires generating a summary given a query
using a set of relevant documents. However, such relevant documents should be
annotated manually and thus are not readily available in realistic scenarios.
To address this limitation, we tackle the QFS task as a knowledge-intensive
(KI) task without access to any relevant documents. Instead, we assume that
these documents are present in a large-scale knowledge corpus and should be
retrieved first. To explore this new setting, we build a new dataset (KI-QFS)
by adapting existing QFS datasets. In this dataset, answering the query
requires document retrieval from a knowledge corpus. We construct three
different knowledge corpora, and we further provide relevance annotations to
enable retrieval evaluation. Finally, we benchmark the dataset with
state-of-the-art QFS models and retrieval-enhanced models. The experimental
results demonstrate that QFS models perform significantly worse on KI-QFS
compared to the original QFS task, indicating that the knowledge-intensive
setting is much more challenging and offers substantial room for improvement.
We believe that our investigation will inspire further research into addressing
QFS in more realistic scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05575">
<div class="article-summary-box-inner">
<span><p>Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they typically struggle to reason rare or emerging unseen entities. In this
paper, we propose kNN-KGE, a new knowledge graph embedding approach with
pre-trained language models, by linearly interpolating its entity distribution
with k-nearest neighbors. We compute the nearest neighbors based on the
distance in the entity embedding space from the knowledge store. Our approach
can allow rare or emerging entities to be memorized explicitly rather than
implicitly in model parameters. Experimental results demonstrate that our
approach can improve inductive and transductive link prediction results and
yield better performance for low-resource settings with only a few triples,
which might be easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Transliteration Help Multilingual Language Modeling?. (arXiv:2201.12501v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12501">
<div class="article-summary-box-inner">
<span><p>Script diversity presents a challenge to Multilingual Language Models (MLLM)
by reducing lexical overlap among closely related languages. Therefore,
transliterating closely related languages that use different writing scripts to
a common script may improve the downstream task performance of MLLMs. We
empirically measure the effect of transliteration on MLLMs in this context. We
specifically focus on the Indic languages, which have the highest script
diversity in the world, and we evaluate our models on the IndicGLUE benchmark.
We perform the Mann-Whitney U test to rigorously verify whether the effect of
transliteration is significant or not. We find that transliteration benefits
the low-resource languages without negatively affecting the comparatively
high-resource languages. We also measure the cross-lingual representation
similarity of the models using centered kernel alignment on parallel sentences
from the FLORES-101 dataset. We find that for parallel sentences across
different languages, the transliteration-based model learns sentence
representations that are more similar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing for the Usage of Grammatical Number. (arXiv:2204.08831v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08831">
<div class="article-summary-box-inner">
<span><p>A central quest of probing is to uncover how pre-trained models encode a
linguistic property within their representations. An encoding, however, might
be spurious-i.e., the model might not rely on it when making predictions. In
this paper, we try to find encodings that the model actually uses, introducing
a usage-based probing setup. We first choose a behavioral task which cannot be
solved without using the linguistic property. Then, we attempt to remove the
property by intervening on the model's representations. We contend that, if an
encoding is used by the model, its removal should harm the performance on the
chosen behavioral task. As a case study, we focus on how BERT encodes
grammatical number, and on how it uses this encoding to solve the number
agreement task. Experimentally, we find that BERT relies on a linear encoding
of grammatical number to produce the correct behavioral output. We also find
that BERT uses a separate encoding of grammatical number for nouns and verbs.
Finally, we identify in which layers information about grammatical number is
transferred from a noun to its head verb.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Cross-Domain Speech Recognition with Self-Supervision. (arXiv:2206.09783v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09783">
<div class="article-summary-box-inner">
<span><p>The cross-domain performance of automatic speech recognition (ASR) could be
severely hampered due to the mismatch between training and testing
distributions. Since the target domain usually lacks labeled data, and domain
shifts exist at acoustic and linguistic levels, it is challenging to perform
unsupervised domain adaptation (UDA) for ASR. Previous work has shown that
self-supervised learning (SSL) or pseudo-labeling (PL) is effective in UDA by
exploiting the self-supervisions of unlabeled data. However, these
self-supervisions also face performance degradation in mismatched domain
distributions, which previous work fails to address. This work presents a
systematic UDA framework to fully utilize the unlabeled data with
self-supervision in the pre-training and fine-tuning paradigm. On the one hand,
we apply continued pre-training and data replay techniques to mitigate the
domain mismatch of the SSL pre-trained model. On the other hand, we propose a
domain-adaptive fine-tuning approach based on the PL technique with three
unique modifications: Firstly, we design a dual-branch PL method to decrease
the sensitivity to the erroneous pseudo-labels; Secondly, we devise an
uncertainty-aware confidence filtering strategy to improve pseudo-label
correctness; Thirdly, we introduce a two-step PL approach to incorporate target
domain linguistic knowledge, thus generating more accurate target domain
pseudo-labels. Experimental results on various cross-domain scenarios
demonstrate that the proposed approach effectively boosts the cross-domain
performance and significantly outperforms previous approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Isotropic Representation Can Improve Dense Retrieval. (arXiv:2209.00218v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2209.00218">
<div class="article-summary-box-inner">
<span><p>The recent advancement in language representation modeling has broadly
affected the design of dense retrieval models. In particular, many of the
high-performing dense retrieval models evaluate representations of query and
document using BERT, and subsequently apply a cosine-similarity based scoring
to determine the relevance. BERT representations, however, are known to follow
an anisotropic distribution of a narrow cone shape and such an anisotropic
distribution can be undesirable for the cosine-similarity based scoring. In
this work, we first show that BERT-based DR also follows an anisotropic
distribution. To cope with the problem, we introduce unsupervised
post-processing methods of Normalizing Flow and whitening, and develop
token-wise method in addition to the sequence-wise method for applying the
post-processing methods to the representations of dense retrieval models. We
show that the proposed methods can effectively enhance the representations to
be isotropic, then we perform experiments with ColBERT and RepBERT to show that
the performance (NDCG at 10) of document re-ranking can be improved by
5.17\%$\sim$8.09\% for ColBERT and 6.88\%$\sim$22.81\% for RepBERT. To examine
the potential of isotropic representation for improving the robustness of DR
models, we investigate out-of-distribution tasks where the test dataset differs
from the training dataset. The results show that isotropic representation can
achieve a generally improved performance. For instance, when training dataset
is MS-MARCO and test dataset is Robust04, isotropy post-processing can improve
the baseline performance by up to 24.98\%. Furthermore, we show that an
isotropic model trained with an out-of-distribution dataset can even outperform
a baseline model trained with the in-distribution dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks. (arXiv:2211.12402v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.12402">
<div class="article-summary-box-inner">
<span><p>Vision language pre-training aims to learn alignments between vision and
language from a large amount of data. Most existing methods only learn
image-text alignments. Some others utilize pre-trained object detectors to
leverage vision language alignments at the object level. In this paper, we
propose to learn multi-grained vision language alignments by a unified
pre-training framework that learns multi-grained aligning and multi-grained
localization simultaneously. Based on it, we present X$^2$-VLM, an all-in-one
model with a flexible modular architecture, in which we further unify
image-text pre-training and video-text pre-training in one model. X$^2$-VLM is
able to learn unlimited visual concepts associated with diverse text
descriptions. Experiment results show that X$^2$-VLM performs the best on base
and large scale for both image-text and video-text tasks, making a good
trade-off between performance and model scale. Moreover, we show that the
modular design of X$^2$-VLM results in high transferability for it to be
utilized in any language or domain. For example, by simply replacing the text
encoder with XLM-R, X$^2$-VLM outperforms state-of-the-art multilingual
multi-modal pre-trained models without any multilingual pre-training. The code
and pre-trained models are available at https://github.com/zengyan-97/X2-VLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sources of Noise in Dialogue and How to Deal with Them. (arXiv:2212.02745v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.02745">
<div class="article-summary-box-inner">
<span><p>Training dialogue systems often entails dealing with noisy training examples
and unexpected user inputs. Despite their prevalence, there currently lacks an
accurate survey of dialogue noise, nor is there a clear sense of the impact of
each noise type on task performance. This paper addresses this gap by first
constructing a taxonomy of noise encountered by dialogue systems. In addition,
we run a series of experiments to show how different models behave when
subjected to varying levels of noise and types of noise. Our results reveal
that models are quite robust to label errors commonly tackled by existing
denoising algorithms, but that performance suffers from dialogue-specific
noise. Driven by these observations, we design a data cleaning algorithm
specialized for conversational settings and apply it as a proof-of-concept for
targeted dialogue denoising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09180">
<div class="article-summary-box-inner">
<span><p>Despite tremendous advancements in dialogue systems, stable evaluation still
requires human judgments producing notoriously high-variance metrics due to
their inherent subjectivity. Moreover, methods and labels in dialogue
evaluation are not fully standardized, especially for open-domain chats, with a
lack of work to compare and assess the validity of those approaches. The use of
inconsistent evaluation can misinform the performance of a dialogue system,
which becomes a major hurdle to enhance it. Thus, a dimensional evaluation of
chat-oriented open-domain dialogue systems that reliably measures several
aspects of dialogue capabilities is desired. This paper presents a novel human
evaluation method to estimate the rates of many dialogue system behaviors. Our
method is used to evaluate four state-of-the-art open-domain dialogue systems
and compared with existing approaches. The analysis demonstrates that our
behavior method is more suitable than alternative Likert-style or comparative
approaches for dimensional evaluation of these systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog. (arXiv:2212.10008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.10008">
<div class="article-summary-box-inner">
<span><p>Many efforts have been made to construct dialog systems for different types
of conversations, such as task-oriented dialog (TOD) and open-domain dialog
(ODD). To better mimic human-level conversations that usually fuse various
dialog modes, it is essential to build a system that can effectively handle
both TOD and ODD and access different knowledge sources. To address the lack of
available data for the fused task, we propose a framework for automatically
generating dialogues that combine knowledge-grounded ODDs and TODs in various
settings. Additionally, we introduce a unified model PivotBot that is capable
of appropriately adopting TOD and ODD modes and accessing different knowledge
sources in order to effectively tackle the fused task. Evaluation results
demonstrate the superior ability of the proposed model to switch seamlessly
between TOD and ODD tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.14454">
<div class="article-summary-box-inner">
<span><p>Multi-modal entity alignment (MMEA) aims to discover identical entities
across different knowledge graphs (KGs) whose entities are associated with
relevant images. However, current MMEA algorithms rely on KG-level modality
fusion strategies for multi-modal entity representation, which ignores the
variations of modality preferences of different entities, thus compromising
robustness against noise in modalities such as blurry images and relations.
This paper introduces MEAformer, a multi-modal entity alignment transformer
approach for meta modality hybrid, which dynamically predicts the mutual
correlation coefficients among modalities for more fine-grained entity-level
modality fusion and alignment. Experimental results demonstrate that our model
not only achieves SOTA performance in multiple training scenarios, including
supervised, unsupervised, iterative, and low-resource settings, but also has a
limited number of parameters, efficient runtime, and interpretability. Our code
is available at https://github.com/zjukg/MEAformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-Based Analysis of Language Models. (arXiv:2303.00333v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.00333">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of large pretrained language models (LMs) on a
variety of prompting tasks, these models can be alarmingly brittle to small
changes in inputs or application contexts. To better understand such behavior
and motivate the design of more robust LMs, we propose a general experimental
framework, CALM (Competence-based Analysis of Language Models), where targeted
causal interventions are utilized to damage an LM's internal representation of
various linguistic properties in order to evaluate its use of each
representation in performing a given task. We implement these interventions as
gradient-based adversarial attacks, which (in contrast to prior causal probing
methodologies) are able to target arbitrarily-encoded representations of
relational properties, and carry out a case study of this approach to analyze
how BERT-like LMs use representations of several relational properties in
performing associated relation prompting tasks. We find that, while the
representations LMs leverage in performing each task are highly entangled, they
may be meaningfully interpreted in terms of the tasks where they are most
utilized; and more broadly, that CALM enables an expanded scope of inquiry in
LM analysis that may be useful in predicting and explaining weaknesses of
existing LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Architecture for Out of Domain Intent Detection and Intent Discovery. (arXiv:2303.04134v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.04134">
<div class="article-summary-box-inner">
<span><p>Intent Detection is one of the tasks of the Natural Language Understanding
(NLU) unit in task-oriented dialogue systems. Out of Scope (OOS) and Out of
Domain (OOD) inputs may run these systems into a problem. On the other side, a
labeled dataset is needed to train a model for Intent Detection in
task-oriented dialogue systems. The creation of a labeled dataset is
time-consuming and needs human resources. The purpose of this article is to
address mentioned problems. The task of identifying OOD/OOS inputs is named
OOD/OOS Intent Detection. Also, discovering new intents and pseudo-labeling of
OOD inputs is well known by Intent Discovery. In OOD intent detection part, we
make use of a Variational Autoencoder to distinguish between known and unknown
intents independent of input data distribution. After that, an unsupervised
clustering method is used to discover different unknown intents underlying
OOD/OOS inputs. We also apply a non-linear dimensionality reduction on OOD/OOS
representations to make distances between representations more meaning full for
clustering. Our results show that the proposed model for both OOD/OOS Intent
Detection and Intent Discovery achieves great results and passes baselines in
English and Persian languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.10464">
<div class="article-summary-box-inner">
<span><p>The pre-training and fine-tuning paradigm has contributed to a number of
breakthroughs in Natural Language Processing (NLP). Instead of directly
training on a downstream task, language models are first pre-trained on large
datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then
fine-tuned on task-specific data (e.g., natural language generation, text
summarization, etc.). Scaling the model and dataset size has helped improve the
performance of LLMs, but unfortunately, this also lead to highly prohibitive
computational costs. Pre-training LLMs often require orders of magnitude more
FLOPs than fine-tuning and the model capacity often remains the same between
the two phases. To achieve training efficiency w.r.t training FLOPs, we propose
to decouple the model capacity between the two phases and introduce Sparse
Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits
of using unstructured weight sparsity to train only a subset of weights during
pre-training (Sparse Pre-training) and then recover the representational
capacity by allowing the zeroed weights to learn (Dense Fine-tuning). We
demonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3
XL model resulting in a 2.5x reduction in pre-training FLOPs, without a
significant loss in accuracy on the downstream tasks relative to the dense
baseline. By rigorously evaluating multiple downstream tasks, we also establish
a relationship between sparsity, task complexity and dataset size. Our work
presents a promising direction to train large GPT models at a fraction of the
training FLOPs using weight sparsity, while retaining the benefits of
pre-trained textual representations for downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model. (arXiv:2304.03086v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.03086">
<div class="article-summary-box-inner">
<span><p>The ChatGPT, a lite and conversational variant of Generative Pretrained
Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large
Language Models (LLMs) with billions of parameters. LLMs have stirred up much
interest among researchers and practitioners in their impressive skills in
natural language processing tasks, which profoundly impact various fields. This
paper mainly discusses the future applications of LLMs in dentistry. We
introduce two primary LLM deployment methods in dentistry, including automated
dental diagnosis and cross-modal dental diagnosis, and examine their potential
applications. Especially, equipped with a cross-modal encoder, a single LLM can
manage multi-source data and conduct advanced natural language reasoning to
perform complex clinical operations. We also present cases to demonstrate the
potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical
application. While LLMs offer significant potential benefits, the challenges,
such as data privacy, data quality, and model bias, need further study.
Overall, LLMs have the potential to revolutionize dental diagnosis and
treatment, which indicates a promising avenue for clinical application and
research in dentistry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.05368">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have made significant progress in various
domains, including healthcare. However, the specialized nature of clinical
language understanding tasks presents unique challenges and limitations that
warrant further investigation. In this study, we conduct a comprehensive
evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within
the realm of clinical language understanding tasks. These tasks span a diverse
range, including named entity recognition, relation extraction, natural
language inference, semantic textual similarity, document classification, and
question-answering. We also introduce a novel prompting strategy,
self-questioning prompting (SQP), tailored to enhance LLMs' performance by
eliciting informative questions and answers pertinent to the clinical scenarios
at hand. Our evaluation underscores the significance of task-specific learning
strategies and prompting techniques for improving LLMs' effectiveness in
healthcare-related tasks. Additionally, our in-depth error analysis on the
challenging relation extraction task offers valuable insights into error
distribution and potential avenues for improvement using SQP. Our study sheds
light on the practical implications of employing LLMs in the specialized domain
of healthcare, serving as a foundation for future research and the development
of potential applications in healthcare settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2304.14535">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) has recently become an important challenge
when using deep learning (DL). It requires large-scale training datasets and
high computational and storage resources. Moreover, DL techniques and machine
learning (ML) approaches in general, hypothesize that training and testing data
come from the same domain, with the same input feature space and data
distribution characteristics. This assumption, however, is not applicable in
some real-world artificial intelligence (AI) applications. Moreover, there are
situations where gathering real data is challenging, expensive, or rarely
occurring, which can not meet the data requirements of DL models. deep transfer
learning (DTL) has been introduced to overcome these issues, which helps
develop high-performing models using real datasets that are small or slightly
different but related to the training data. This paper presents a comprehensive
survey of DTL-based ASR frameworks to shed light on the latest developments and
helps academics and professionals understand current challenges. Specifically,
after presenting the DTL background, a well-designed taxonomy is adopted to
inform the state-of-the-art. A critical analysis is then conducted to identify
the limitations and advantages of each framework. Moving on, a comparative
study is introduced to highlight the current challenges before deriving
opportunities for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing and Interpreting Causal Knowledge Graphs from News. (arXiv:2305.09359v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.09359">
<div class="article-summary-box-inner">
<span><p>Many financial jobs rely on news to learn about causal events in the past and
present, to make informed decisions and predictions about the future. With the
ever-increasing amount of news available online, there is a need to automate
the extraction of causal events from unstructured texts. In this work, we
propose a methodology to construct causal knowledge graphs (KGs) from news
using two steps: (1) Extraction of Causal Relations, and (2) Argument
Clustering and Representation into KG. We aim to build graphs that emphasize on
recall, precision and interpretability. For extraction, although many earlier
works already construct causal KGs from text, most adopt rudimentary
pattern-based methods. We close this gap by using the latest BERT-based
extraction models alongside pattern-based ones. As a result, we achieved a high
recall, while still maintaining a high precision. For clustering, we utilized a
topic modelling approach to cluster our arguments, so as to increase the
connectivity of our graph. As a result, instead of 15,686 disconnected
subgraphs, we were able to obtain 1 connected graph that enables users to infer
more causal relationships from. Our final KG effectively captures and conveys
causal relationships, validated through experiments, multiple use cases and
user feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v4 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.15299">
<div class="article-summary-box-inner">
<span><p>Large language models of artificial intelligence (AI), such as ChatGPT, find
remarkable but controversial applicability in science and research. This paper
reviews epistemological challenges, ethical and integrity risks in science
conduct in the advent of generative AI. This is with the aim to lay new timely
foundations for a high-quality research ethics review. The role of AI language
models as a research instrument and subject is scrutinized along with ethical
implications for scientists, participants and reviewers. New emerging practices
for research ethics review are discussed, concluding with ten recommendations
that shape a response for a more responsible research conduct in the era of AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective. (arXiv:2305.17760v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.17760">
<div class="article-summary-box-inner">
<span><p>How do language models "think"? This paper formulates a probabilistic
cognitive model called the bounded pragmatic speaker, which can characterize
the operation of different variations of language models. Specifically, we
demonstrate that large language models fine-tuned with reinforcement learning
from human feedback (Ouyang et al., 2022) embody a model of thought that
conceptually resembles a fast-and-slow model (Kahneman, 2011), which
psychologists have attributed to humans. We discuss the limitations of
reinforcement learning from human feedback as a fast-and-slow model of thought
and propose avenues for expanding this framework. In essence, our research
highlights the value of adopting a cognitive probabilistic modeling approach to
gain insights into the comprehension, evaluation, and advancement of language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18339">
<div class="article-summary-box-inner">
<span><p>With the widespread use of large artificial intelligence (AI) models such as
ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is
leading a paradigm shift in content creation and knowledge representation. AIGC
uses generative large AI algorithms to assist or replace humans in creating
massive, high-quality, and human-like content at a faster pace and lower cost,
based on user-provided prompts. Despite the recent significant progress in
AIGC, security, privacy, ethical, and legal challenges still need to be
addressed. This paper presents an in-depth survey of working principles,
security and privacy threats, state-of-the-art solutions, and future challenges
of the AIGC paradigm. Specifically, we first explore the enabling technologies,
general architecture of AIGC, and discuss its working modes and key
characteristics. Then, we investigate the taxonomy of security and privacy
threats to AIGC and highlight the ethical and societal implications of GPT and
AIGC technologies. Furthermore, we review the state-of-the-art AIGC
watermarking approaches for regulatable AIGC paradigms regarding the AIGC model
and its produced content. Finally, we identify future challenges and open
research directions related to AIGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forgotten Knowledge: Examining the Citational Amnesia in NLP. (arXiv:2305.18554v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2305.18554">
<div class="article-summary-box-inner">
<span><p>Citing papers is the primary method through which modern scientific writing
discusses and builds on past work. Collectively, citing a diverse set of papers
(in time and area of study) is an indicator of how widely the community is
reading. Yet, there is little work looking at broad temporal patterns of
citation. This work systematically and empirically examines: How far back in
time do we tend to go to cite papers? How has that changed over time, and what
factors correlate with this citational attention/amnesia? We chose NLP as our
domain of interest and analyzed approximately 71.5K papers to show and quantify
several key trends in citation. Notably, around 62% of cited papers are from
the immediate five years prior to publication, whereas only about 17% are more
than ten years old. Furthermore, we show that the median age and age diversity
of cited papers were steadily increasing from 1990 to 2014, but since then, the
trend has reversed, and current NLP papers have an all-time low temporal
citation diversity. Finally, we show that unlike the 1990s, the highly cited
papers in the last decade were also papers with the least citation diversity,
likely contributing to the intense (and arguably harmful) recency focus. Code,
data, and a demo are available on the project homepage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Column Type Annotation using ChatGPT. (arXiv:2306.00745v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.00745">
<div class="article-summary-box-inner">
<span><p>Column type annotation is the task of annotating the columns of a relational
table with the semantic type of the values contained in each column. Column
type annotation is an important pre-processing step for data search and data
integration in the context of data lakes. State-of-the-art column type
annotation methods either rely on matching table columns to properties of a
knowledge graph or fine-tune pre-trained language models such as BERT for
column type annotation. In this work, we take a different approach and explore
using ChatGPT for column type annotation. We evaluate different prompt designs
in zero- and few-shot settings and experiment with providing task definitions
and detailed instructions to the model. We further implement a two-step table
annotation pipeline which first determines the class of the entities described
in the table and depending on this class asks ChatGPT to annotate columns using
only the relevant subset of the overall vocabulary. Using instructions as well
as the two-step pipeline, ChatGPT reaches F1 scores of over 85% in zero- and
one-shot setups. To reach a similar F1 score a RoBERTa model needs to be
fine-tuned with 356 examples. This comparison shows that ChatGPT is able
deliver competitive results for the column type annotation task given no or
only a minimal amount of task-specific demonstrations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment analysis. (arXiv:2306.01312v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.01312">
<div class="article-summary-box-inner">
<span><p>Multimodal Sentiment Analysis (MSA) has been a popular topic in natural
language processing nowadays, at both sentence and aspect level. However, the
existing approaches almost require large-size labeled datasets, which bring
about large consumption of time and resources. Therefore, it is practical to
explore the method for few-shot sentiment analysis in cross-modalities.
Previous works generally execute on textual modality, using the prompt-based
methods, mainly two types: hand-crafted prompts and learnable prompts. The
existing approach in few-shot multi-modality sentiment analysis task has
utilized both methods, separately. We further design a hybrid pattern that can
combine one or more fixed hand-crafted prompts and learnable prompts and
utilize the attention mechanisms to optimize the prompt encoder. The
experiments on both sentence-level and aspect-level datasets prove that we get
a significant outperformance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text. (arXiv:2306.03557v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.03557">
<div class="article-summary-box-inner">
<span><p>Automatic Arabic diacritization is useful in many applications, ranging from
reading support for language learners to accurate pronunciation predictor for
downstream tasks like speech synthesis. While most of the previous works
focused on models that operate on raw non-diacritized text, production systems
can gain accuracy by first letting humans partly annotate ambiguous words. In
this paper, we propose 2SDiac, a multi-source model that can effectively
support optional diacritics in input to inform all predictions. We also
introduce Guided Learning, a training scheme to leverage given diacritics in
input with different levels of random masking. We show that the provided hints
during test affect more output positions than those annotated. Moreover,
experiments on two common benchmarks show that our approach i) greatly
outperforms the baseline also when evaluated on non-diacritized text; and ii)
achieves state-of-the-art results while reducing the parameter count by over
60%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?. (arXiv:2307.02469v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.02469">
<div class="article-summary-box-inner">
<span><p>Recent advancements in Large Language Models (LLMs) such as GPT4 have
displayed exceptional multi-modal capabilities in following open-ended
instructions given images. However, the performance of these models heavily
relies on design choices such as network structures, training data, and
training strategies, and these choices have not been extensively discussed in
the literature, making it difficult to quantify progress in this field. To
address this issue, this paper presents a systematic and comprehensive study,
quantitatively and qualitatively, on training such models. We implement over 20
variants with controlled settings. Concretely, for network structures, we
compare different LLM backbones and model designs. For training data, we
investigate the impact of data and sampling strategies. For instructions, we
explore the influence of diversified prompts on the instruction-following
ability of the trained models. For benchmarks, we contribute the first, to our
best knowledge, comprehensive evaluation set including both image and video
tasks through crowd-sourcing. Based on our findings, we present Lynx, which
performs the most accurate multi-modal understanding while keeping the best
multi-modal generation ability compared to existing open-sourced GPT4-style
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lost in the Middle: How Language Models Use Long Contexts. (arXiv:2307.03172v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.03172">
<div class="article-summary-box-inner">
<span><p>While recent language models have the ability to take long contexts as input,
relatively little is known about how well they use longer context. We analyze
language model performance on two tasks that require identifying relevant
information within their input contexts: multi-document question answering and
key-value retrieval. We find that performance is often highest when relevant
information occurs at the beginning or end of the input context, and
significantly degrades when models must access relevant information in the
middle of long contexts. Furthermore, performance substantially decreases as
the input context grows longer, even for explicitly long-context models. Our
analysis provides a better understanding of how language models use their input
context and provides new evaluation protocols for future long-context models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Lexical Diversity in Texts: The Twofold Length Problem. (arXiv:2307.04626v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.04626">
<div class="article-summary-box-inner">
<span><p>The impact of text length on the estimation of lexical diversity has captured
the attention of the scientific community for more than a century. Numerous
indices have been proposed, and many studies have been conducted to evaluate
them, but the problem remains. This methodological review provides a critical
analysis not only of the most commonly used indices in language learning
studies, but also of the length problem itself, as well as of the methodology
for evaluating the proposed solutions. The analysis of three datasets of
English language-learners' texts revealed that indices that reduce all texts to
the same length using a probabilistic or an algorithmic approach solve the
length dependency problem; however, all these indices failed to address the
second problem, which is their sensitivity to the parameter that determines the
length to which the texts are reduced. The paper concludes with recommendations
for optimizing lexical diversity analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06060">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach for interpreting deep embeddings in the context
of patient clustering. We evaluate our approach on a dataset of participants
with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful
insights into disease progression patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.06576">
<div class="article-summary-box-inner">
<span><p>Precisely recommending candidate news articles to users has always been a
core challenge for personalized news recommendation systems. Most recent works
primarily focus on using advanced natural language processing techniques to
extract semantic information from rich textual data, employing content-based
methods derived from local historical news. However, this approach lacks a
global perspective, failing to account for users' hidden motivations and
behaviors beyond semantic information. To address this challenge, we propose a
novel model called GLORY (Global-LOcal news Recommendation sYstem), which
combines global representations learned from other users with local
representations to enhance personalized recommendation systems. We accomplish
this by constructing a Global-aware Historical News Encoder, which includes a
global news graph and employs gated graph neural networks to enrich news
representations, thereby fusing historical news representations by a historical
news aggregator. Similarly, we extend this approach to a Global Candidate News
Encoder, utilizing a global entity graph and a candidate news aggregator to
enhance candidate news representation. Evaluation results on two public news
datasets demonstrate that our method outperforms existing approaches.
Furthermore, our model offers more diverse recommendations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT is Good but Bing Chat is Better for Vietnamese Students. (arXiv:2307.08272v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.08272">
<div class="article-summary-box-inner">
<span><p>This study examines the efficacy of two SOTA large language models (LLMs),
namely ChatGPT and Microsoft Bing Chat (BingChat), in catering to the needs of
Vietnamese students. Although ChatGPT exhibits proficiency in multiple
disciplines, Bing Chat emerges as the more advantageous option. We conduct a
comparative analysis of their academic achievements in various disciplines,
encompassing mathematics, literature, English language, physics, chemistry,
biology, history, geography, and civic education. The results of our study
suggest that BingChat demonstrates superior performance compared to ChatGPT
across a wide range of subjects, with the exception of literature, where
ChatGPT exhibits better performance. Additionally, BingChat utilizes the more
advanced GPT-4 technology in contrast to ChatGPT, which is built upon GPT-3.5.
This allows BingChat to improve to comprehension, reasoning and generation of
creative and informative text. Moreover, the fact that BingChat is accessible
in Vietnam and its integration of hyperlinks and citations within responses
serve to reinforce its superiority. In our analysis, it is evident that while
ChatGPT exhibits praiseworthy qualities, BingChat presents a more apdated
solutions for Vietnamese students.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Eval: Instituting Standardized Evaluation for Long Context Language Models. (arXiv:2307.11088v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11088">
<div class="article-summary-box-inner">
<span><p>Recently, there has been growing interest in extending the context length of
instruction-following models in order to effectively process single-turn long
input (e.g. summarizing a paper) and conversations with more extensive
histories. While proprietary models such as GPT-4 and Claude have shown
significant strides in handling extremely lengthy input, open-sourced models
are still in the early stages of experimentation. It also remains unclear
whether extending the context can offer substantial gains over traditional
methods such as retrieval, and to what extent it improves upon their regular
counterparts in practical downstream tasks. To address this challenge, we
propose instituting standardized evaluation for long context language models.
Concretely, we develop L-Eval which contains 411 long documents and over 2,000
human-labeled query-response pairs encompassing areas such as law, finance,
school lectures, lengthy conversations, news, long-form novels, and meetings.
L-Eval also adopts diverse evaluation methods and instruction styles, enabling
a more reliable assessment of Long Context Language Models (LCLMs). Our
findings indicate that while open-source models typically lag behind commercial
models, they still exhibit impressive performance compared with their regular
versions. LLaMA2-13B achieves the best results on both open-ended tasks (win
\textbf{42}\% vs turbo-16k-0613) and closed-ended tasks with only 4k context
length. We release our new evaluation suite, code, and all generation results
including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at
{\url{https://github.com/OpenLMLab/LEval}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.11788">
<div class="article-summary-box-inner">
<span><p>As an application domain where the slightest qualitative improvements can
yield immense value, finance is a promising candidate for early quantum
advantage. Focusing on the rapidly advancing field of Quantum Natural Language
Processing (QNLP), we explore the practical applicability of the two central
approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the
problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data
generation approach, we conduct a case study with more than 1000 realistic
sentences and find that QLSTMs can be trained substantially faster than
DisCoCat while also achieving close to classical results for their available
software implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XDLM: Cross-lingual Diffusion Language Model for Machine Translation. (arXiv:2307.13560v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.13560">
<div class="article-summary-box-inner">
<span><p>Recently, diffusion models have excelled in image generation tasks and have
also been applied to neural language processing (NLP) for controllable text
generation. However, the application of diffusion models in a cross-lingual
setting is less unexplored. Additionally, while pretraining with diffusion
models has been studied within a single language, the potential of
cross-lingual pretraining remains understudied. To address these gaps, we
propose XDLM, a novel Cross-lingual diffusion model for machine translation,
consisting of pretraining and fine-tuning stages. In the pretraining stage, we
propose TLDM, a new training objective for mastering the mapping between
different languages; in the fine-tuning stage, we build up the translation
system based on the pretrained model. We evaluate the result on several machine
translation benchmarks and outperformed both diffusion and Transformer
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15002">
<div class="article-summary-box-inner">
<span><p>The effectiveness of compression distance in KNN-based text classification
('gzip') has recently garnered lots of attention. In this note, we show that
simpler means can also be effective, and compression may not be needed. Indeed,
a 'bag-of-words' matching can achieve similar or better results, and is more
efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Geometric Notion of Causal Probing. (arXiv:2307.15054v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.15054">
<div class="article-summary-box-inner">
<span><p>Large language models rely on real-valued representations of text to make
their predictions. These representations contain information learned from the
data that the model has trained on, including knowledge of linguistic
properties and forms of demographic bias, e.g., based on gender. A growing body
of work has considered removing information about concepts such as these using
orthogonal projections onto subspaces of the representation space. We
contribute to this body of work by proposing a formal definition of
$\textit{intrinsic}$ information in a subspace of a language model's
representation space. We propose a counterfactual approach that avoids the
failure mode of spurious correlations (Kumar et al., 2022) by treating
components in the subspace and its orthogonal complement independently. We show
that our counterfactual notion of information in a subspace is optimized by a
$\textit{causal}$ concept subspace. Furthermore, this intervention allows us to
attempt concept controlled generation by manipulating the value of the
conceptual component of a representation. Empirically, we find that R-LACE
(Ravfogel et al., 2022) returns a one-dimensional subspace containing roughly
half of total concept information under our framework. Our causal controlled
intervention shows that, for at least one model, the subspace returned by
R-LACE can be used to manipulate the concept value of the generated word with
precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hexatagging: Projective Dependency Parsing as Tagging. (arXiv:2306.05477v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2306.05477">
<div class="article-summary-box-inner">
<span><p>We introduce a novel dependency parser, the hexatagger, that constructs
dependency trees by tagging the words in a sentence with elements from a finite
set of possible tags. In contrast to many approaches to dependency parsing, our
approach is fully parallelizable at training time, i.e., the structure-building
actions needed to build a dependency parse can be predicted in parallel to each
other. Additionally, exact decoding is linear in time and space complexity.
Furthermore, we derive a probabilistic dependency parser that predicts hexatags
using no more than a linear model with features from a pretrained language
model, i.e., we forsake a bespoke architecture explicitly designed for the
task. Despite the generality and simplicity of our approach, we achieve
state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test
set. Additionally, our parser's linear time complexity and parallelism
significantly improve computational efficiency, with a roughly 10-times
speed-up over previous state-of-the-art models during decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2307.14430">
<div class="article-summary-box-inner">
<span><p>The quality of training data impacts the performance of pre-trained large
language models (LMs). Given a fixed budget of tokens, we study how to best
select data that leads to good downstream model performance across tasks. We
develop a new framework based on a simple hypothesis: just as humans acquire
interdependent skills in a deliberate order, language models also follow a
natural order when learning a set of skills from their training data. If such
an order exists, it can be utilized for improved understanding of LMs and for
data-efficient training. Using this intuition, our framework formalizes the
notion of a skill and of an ordered set of skills in terms of the associated
data. First, using both synthetic and real data, we demonstrate that these
ordered skill sets exist, and that their existence enables more advanced skills
to be learned with less data when we train on their prerequisite skills.
Second, using our proposed framework, we introduce an online data sampling
algorithm, Skill-It, over mixtures of skills for both continual pre-training
and fine-tuning regimes, where the objective is to efficiently learn multiple
skills in the former and an individual skill in the latter. On the LEGO
synthetic in the continual pre-training setting, Skill-It obtains 36.5 points
higher accuracy than random sampling. On the Natural Instructions dataset in
the fine-tuning setting, Skill-It reduces the validation loss on the target
skill by 13.6% versus training on data associated with the target skill itself.
We apply our skills framework on the recent RedPajama dataset to continually
pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation
Harness with 1B tokens than the baseline approach of sampling uniformly over
data sources with 3B tokens.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-08-01 23:11:00.134707419 UTC">2023-08-01 23:11:00 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>