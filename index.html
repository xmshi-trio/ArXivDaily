<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-01-10T01:30:00Z">01-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Witscript 3: A Hybrid AI System for Improvising Jokes in a Conversation. (arXiv:2301.02695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02695">
<div class="article-summary-box-inner">
<span><p>Previous papers presented Witscript and Witscript 2, AI systems for
improvising jokes in a conversation. Witscript generates jokes that rely on
wordplay, whereas the jokes generated by Witscript 2 rely on common sense. This
paper extends that earlier work by presenting Witscript 3, which generates joke
candidates using three joke production mechanisms and then selects the best
candidate to output. Like Witscript and Witscript 2, Witscript 3 is based on
humor algorithms created by an expert comedy writer. Human evaluators judged
Witscript 3's responses to input sentences to be jokes 44% of the time. This is
evidence that Witscript 3 represents another step toward giving a chatbot a
humanlike sense of humor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facilitating Contrastive Learning of Discourse Relational Senses by Exploiting the Hierarchy of Sense Relations. (arXiv:2301.02724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02724">
<div class="article-summary-box-inner">
<span><p>Implicit discourse relation recognition is a challenging task that involves
identifying the sense or senses that hold between two adjacent spans of text,
in the absence of an explicit connective between them. In both PDTB-2 and
PDTB-3, discourse relational senses are organized into a three-level hierarchy
ranging from four broad top-level senses, to more specific senses below them.
Most previous work on implicit discourse relation recognition have used the
sense hierarchy simply to indicate what sense labels were available. Here we do
more -- incorporating the sense hierarchy into the recognition process itself
and using it to select the negative examples used in contrastive learning. With
no additional effort, the approach achieves state-of-the-art performance on the
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v1 [q-bio.BM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02748">
<div class="article-summary-box-inner">
<span><p>Protein language models (LMs) have been successful in sequence, structural
and functional predictions. However, currently, protein LMs are limited to
encoder- or decoder-only architectures for single sequences while many
biological contexts involve protein-protein interactions. Here, we introduce
pAbT5, which models antibody chain pairing as forward- and back-translations
using a T5-based architecture. We show that pAbT5 accurately reflects chain
pairing through sequence generation and mispairing as unsupervised and
supervised classifications. Our protein LM generates variable-length sequences
and its next-word prediction probability agrees with position-specific scoring
matrix from sequence alignment. Like other works in protein LM, pAbT5 performs
state-of-the-art unsupervised prediction on experimental measurements. To the
best of our knowledge, pAbT5 is the first encoder-decoder protein LM for
protein-protein interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Parallel Corpus and Training Translation Models Between Luganda and English. (arXiv:2301.02773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02773">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) has achieved great successes with large
datasets, so NMT is more premised on high-resource languages. This continuously
underpins the low resource languages such as Luganda due to the lack of
high-quality parallel corpora, so even 'Google translate' does not serve
Luganda at the time of this writing. In this paper, we build a parallel corpus
with 41,070 pairwise sentences for Luganda and English which is based on three
different open-sourced corpora. Then, we train NMT models with hyper-parameter
search on the dataset. Experiments gave us a BLEU score of 21.28 from Luganda
to English and 17.47 from English to Luganda. Some translation examples show
high quality of the translation. We believe that our model is the first
Luganda-English NMT model. The bilingual dataset we built will be available to
the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Story Generation Based on Emotion and Keywords. (arXiv:2301.02777v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02777">
<div class="article-summary-box-inner">
<span><p>Automated visual story generation aims to produce stories with corresponding
illustrations that exhibit coherence, progression, and adherence to characters'
emotional development. This work proposes a story generation pipeline to
co-create visual stories with the users. The pipeline allows the user to
control events and emotions on the generated content. The pipeline includes two
parts: narrative and image generation. For narrative generation, the system
generates the next sentence using user-specified keywords and emotion labels.
For image generation, diffusion models are used to create a visually appealing
image corresponding to each generated sentence. Further, object recognition is
applied to the generated images to allow objects in these images to be
mentioned in future story development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic-style-aware Neural Networks for Fake News Detection. (arXiv:2301.02792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02792">
<div class="article-summary-box-inner">
<span><p>We propose the hierarchical recursive neural network (HERO) to predict fake
news by learning its linguistic style, which is distinguishable from the truth,
as psychological theories reveal. We first generate the hierarchical linguistic
tree of news documents; by doing so, we translate each news document's
linguistic style into its writer's usage of words and how these words are
recursively structured as phrases, sentences, paragraphs, and, ultimately, the
document. By integrating the hierarchical linguistic tree with the neural
network, the proposed method learns and classifies the representation of news
documents by capturing their locally sequential and globally recursive
structures that are linguistically meaningful. It is the first work offering
the hierarchical linguistic tree and the neural network preserving the tree
information to our best knowledge. Experimental results based on public
real-world datasets demonstrate the proposed method's effectiveness, which can
outperform state-of-the-art techniques in classifying short and long news
documents. We also examine the differential linguistic style of fake news and
the truth and observe some patterns of fake news. The code and data have been
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RLAS-BIABC: A Reinforcement Learning-Based Answer Selection Using the BERT Model Boosted by an Improved ABC Algorithm. (arXiv:2301.02807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02807">
<div class="article-summary-box-inner">
<span><p>Answer selection (AS) is a critical subtask of the open-domain question
answering (QA) problem. The present paper proposes a method called RLAS-BIABC
for AS, which is established on attention mechanism-based long short-term
memory (LSTM) and the bidirectional encoder representations from transformers
(BERT) word embedding, enriched by an improved artificial bee colony (ABC)
algorithm for pretraining and a reinforcement learning-based algorithm for
training backpropagation (BP) algorithm. BERT can be comprised in downstream
work and fine-tuned as a united task-specific architecture, and the pretrained
BERT model can grab different linguistic effects. Existing algorithms typically
train the AS model with positive-negative pairs for a two-class classifier. A
positive pair contains a question and a genuine answer, while a negative one
includes a question and a fake answer. The output should be one for positive
and zero for negative pairs. Typically, negative pairs are more than positive,
leading to an imbalanced classification that drastically reduces system
performance. To deal with it, we define classification as a sequential
decision-making process in which the agent takes a sample at each step and
classifies it. For each classification operation, the agent receives a reward,
in which the prize of the majority class is less than the reward of the
minority class. Ultimately, the agent finds the optimal value for the policy
weights. We initialize the policy weights with the improved ABC algorithm. The
initial value technique can prevent problems such as getting stuck in the local
optimum. Although ABC serves well in most tasks, there is still a weakness in
the ABC algorithm that disregards the fitness of related pairs of individuals
in discovering a neighboring food source position.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Brain-inspired Memory Transformation based Differentiable Neural Computer for Reasoning-based Question Answering. (arXiv:2301.02809v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02809">
<div class="article-summary-box-inner">
<span><p>Reasoning and question answering as a basic cognitive function for humans, is
nevertheless a great challenge for current artificial intelligence. Although
the Differentiable Neural Computer (DNC) model could solve such problems to a
certain extent, the development is still limited by its high algorithm
complexity, slow convergence speed, and poor test robustness. Inspired by the
learning and memory mechanism of the brain, this paper proposed a Memory
Transformation based Differentiable Neural Computer (MT-DNC) model. MT-DNC
incorporates working memory and long-term memory into DNC, and realizes the
autonomous transformation of acquired experience between working memory and
long-term memory, thereby helping to effectively extract acquired knowledge to
improve reasoning ability. Experimental results on bAbI question answering task
demonstrated that our proposed method achieves superior performance and faster
convergence speed compared to other existing DNN and DNC models. Ablation
studies also indicated that the memory transformation from working memory to
long-term memory plays essential role in improving the robustness and stability
of reasoning. This work explores how brain-inspired memory transformation can
be integrated and applied to complex intelligent dialogue and reasoning
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why do Nearest Neighbor Language Models Work?. (arXiv:2301.02828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02828">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) compute the probability of a text by sequentially
computing a representation of an already-seen context and using this
representation to predict the next word. Currently, most LMs calculate these
representations through a neural network consuming the immediate previous
context. However recently, retrieval-augmented LMs have shown to improve over
standard neural LMs, by accessing information retrieved from a large datastore,
in addition to their standard, parametric, next-word prediction. In this paper,
we set out to understand why retrieval-augmented language models, and
specifically why k-nearest neighbor language models (kNN-LMs) perform better
than standard parametric LMs, even when the k-nearest neighbor component
retrieves examples from the same training set that the LM was originally
trained on. To this end, we perform a careful analysis of the various
dimensions over which kNN-LM diverges from standard LMs, and investigate these
dimensions one by one. Empirically, we identify three main reasons why kNN-LM
performs better than standard LMs: using a different input representation for
predicting the next tokens, approximate kNN search, and the importance of
softmax temperature for the kNN distribution. Further, we incorporate these
insights into the model architecture or the training procedure of the standard
parametric LM, improving its results without the need for an explicit retrieval
component. The code is available at https://github.com/frankxu2004/knnlm-why.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeeChain: A Speech Toolkit for Large-Scale Machine Speech Chain. (arXiv:2301.02966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02966">
<div class="article-summary-box-inner">
<span><p>This paper introduces SpeeChain, an open-source Pytorch-based toolkit
designed to develop the machine speech chain for large-scale use. This first
release focuses on the TTS-to-ASR chain, a core component of the machine speech
chain, that refers to the TTS data augmentation by unspoken text for ASR. To
build an efficient pipeline for the large-scale TTS-to-ASR chain, we implement
easy-to-use multi-GPU batch-level model inference, multi-dataloader batch
generation, and on-the-fly data selection techniques. In this paper, we first
explain the overall procedure of the TTS-to-ASR chain and the difficulties of
each step. Then, we present a detailed ablation study on different types of
unlabeled data, data filtering thresholds, batch composition, and
real-synthetic data ratios. Our experimental results on train_clean_460 of
LibriSpeech demonstrate that our TTS-to-ASR chain can significantly improve WER
in a semi-supervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traditional Readability Formulas Compared for English. (arXiv:2301.02975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02975">
<div class="article-summary-box-inner">
<span><p>Traditional English readability formulas, or equations, were largely
developed in the 20th century. Nonetheless, many researchers still rely on them
for various NLP applications. Such a phenomenon is presumably due to the
convenience and straightforwardness of readability formulas. In this work, we
contribute to the NLP community by 1. introducing New English Readability
Formula (NERF), 2. recalibrating the coefficients of old readability formulas
(Flesch-Kincaid Grade Level, Fog Index, SMOG Index, Coleman-Liau Index, and
Automated Readability Index), 3. evaluating the readability formulas, for use
in text simplification studies and medical texts, and 4. developing a
Python-based program for the wide application to various NLP projects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers. (arXiv:2301.02998v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.02998">
<div class="article-summary-box-inner">
<span><p>We carried out a reproducibility study of InPars recipe for unsupervised
training of neural rankers. As a by-product of this study, we developed a
simple-yet-effective modification of InPars, which we called InPars-light.
Unlike InPars, InPars-light uses only a freely available language model BLOOM
and 7x-100x smaller ranking models. On all five English retrieval collections
(used in the original InPars study) we obtained substantial (7-30%) and
statistically significant improvements over BM25 in nDCG or MRR using only a
30M parameter six-layer MiniLM ranker. In contrast, in the InPars study only a
100x larger MonoT5-3B model consistently outperformed BM25, whereas their
smaller MonoT5-220M model (which is still 7x larger than our MiniLM ranker),
outperformed BM25 only on MS MARCO and TREC DL 2020. In a purely unsupervised
setting, our 435M parameter DeBERTA v3 ranker was roughly at par with the 7x
larger MonoT5-3B: In fact, on three out of five datasets, it slightly
outperformed MonoT5-3B. Finally, these good results were achieved by re-ranking
only 100 candidate documents compared to 1000 used in InPars. We believe that
InPars-light is the first truly cost-effective prompt-based unsupervised recipe
to train and deploy neural ranking models that outperform BM25.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Representational Geometry of Acoustic Word Embeddings. (arXiv:2301.03012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03012">
<div class="article-summary-box-inner">
<span><p>Acoustic word embeddings (AWEs) are vector representations such that
different acoustic exemplars of the same word are projected nearby in the
embedding space. In addition to their use in speech technology applications
such as spoken term discovery and keyword spotting, AWE models have been
adopted as models of spoken-word processing in several cognitively motivated
studies and have been shown to exhibit human-like performance in some auditory
processing tasks. Nevertheless, the representational geometry of AWEs remains
an under-explored topic that has not been studied in the literature. In this
paper, we take a closer analytical look at AWEs learned from English speech and
study how the choice of the learning objective and the architecture shapes
their representational profile. To this end, we employ a set of analytic
techniques from machine learning and neuroscience in three different analyses:
embedding space uniformity, word discriminability, and representational
consistency. Our main findings highlight the prominent role of the learning
objective on shaping the representation profile compared to the model
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method. (arXiv:2301.03029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03029">
<div class="article-summary-box-inner">
<span><p>Topic Modelling (TM) is from the research branches of natural language
understanding (NLU) and natural language processing (NLP) that is to facilitate
insightful analysis from large documents and datasets, such as a summarisation
of main topics and the topic changes. This kind of discovery is getting more
popular in real-life applications due to its impact on big data analytics. In
this study, from the social-media and healthcare domain, we apply popular
Latent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish
newspaper articles about Coronavirus. We describe the corpus we created
including 6515 articles, methods applied, and statistics on topic changes over
approximately 1 year and two months period of time from 17th January 2020 to
13th March 2021. We hope this work can be an asset for grounding applications
of topic modelling and can be inspiring for similar case studies in an era with
pandemics, to support socio-economic impact research as well as clinical and
healthcare analytics. Our data is openly available at https://github.
com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation (LDA); Topic
Modelling; Coronavirus; Pandemics; Natural Language Understanding
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The State of Human-centered NLP Technology for Fact-checking. (arXiv:2301.03056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03056">
<div class="article-summary-box-inner">
<span><p>Misinformation threatens modern society by promoting distrust in science,
changing narratives in public health, heightening social polarization, and
disrupting democratic elections and financial markets, among a myriad of other
societal harms. To address this, a growing cadre of professional fact-checkers
and journalists provide high-quality investigations into purported facts.
However, these largely manual efforts have struggled to match the enormous
scale of the problem. In response, a growing body of Natural Language
Processing (NLP) technologies have been proposed for more scalable
fact-checking. Despite tremendous growth in such research, however, practical
adoption of NLP technologies for fact-checking still remains in its infancy
today.
</p>
<p>In this work, we review the capabilities and limitations of the current NLP
technologies for fact-checking. Our particular focus is to further chart the
design space for how these technologies can be harnessed and refined in order
to better meet the needs of human fact-checkers. To do so, we review key
aspects of NLP-based fact-checking: task formulation, dataset construction,
modeling, and human-centered strategies, such as explainable models and
human-in-the-loop approaches. Next, we review the efficacy of applying
NLP-based fact-checking tools to assist human fact-checkers. We recommend that
future research include collaboration with fact-checker stakeholders early on
in NLP research, as well as incorporation of human-centered design practices in
model development, in order to further guide technology development for human
use and practical adoption. Finally, we advocate for more research on benchmark
development supporting extrinsic evaluation of human-centered fact-checking
technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEGAnno: Exploratory Labeling for NLP in Computational Notebooks. (arXiv:2301.03095v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03095">
<div class="article-summary-box-inner">
<span><p>We present MEGAnno, a novel exploratory annotation framework designed for NLP
researchers and practitioners. Unlike existing labeling tools that focus on
data labeling only, our framework aims to support a broader, iterative ML
workflow including data exploration and model development. With MEGAnno's API,
users can programmatically explore the data through sophisticated search and
automated suggestion functions and incrementally update task schema as their
project evolve. Combined with our widget, the users can interactively sort,
filter, and assign labels to multiple items simultaneously in the same notebook
where the rest of the NLP project resides. We demonstrate MEGAnno's flexible,
exploratory, efficient, and seamless labeling experience through a sentiment
analysis use case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of German Drama Texts Using Fine Tuned GPT-2 Models. (arXiv:2301.03119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03119">
<div class="article-summary-box-inner">
<span><p>This study is devoted to the automatic generation of German drama texts. We
suggest an approach consisting of two key steps: fine-tuning a GPT-2 model (the
outline model) to generate outlines of scenes based on keywords and fine-tuning
a second model (the generation model) to generate scenes from the scene
outline. The input for the neural model comprises two datasets: the German
Drama Corpus (GerDraCor) and German Text Archive (Deutsches Textarchiv or DTA).
In order to estimate the effectiveness of the proposed method, our models are
compared with baseline GPT-2 models. Our models perform well according to
automatic quantitative evaluation, but, conversely, manual qualitative analysis
reveals a poor quality of generated texts. This may be due to the quality of
the dataset or training inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logically at Factify 2023: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture. (arXiv:2301.03127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03127">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the Logically submissions to De-Factify 2 challenge
(DE-FACTIFY 2023) on the task 1 of Multi-Modal Fact Checking. We describes our
submissions to this challenge including explored evidence retrieval and
selection techniques, pre-trained cross-modal and unimodal models, and a
cross-modal veracity model based on the well established Transformer Encoder
(TE) architecture which is heavily relies on the concept of self-attention.
Exploratory analysis is also conducted on this Factify 2 data set that uncovers
the salient multi-modal patterns and hypothesis motivating the architecture
proposed in this work. A series of preliminary experiments were done to
investigate and benchmarking different pre-trained embedding models, evidence
retrieval settings and thresholds. The final system, a standard two-stage
evidence based veracity detection system, yields weighted avg. 0.79 on both val
set and final blind test set on the task 1, which achieves 3rd place with a
small margin to the top performing system on the leaderboard among 9
participants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Removing Non-Stationary Knowledge From Pre-Trained Language Models for Entity-Level Sentiment Classification in Finance. (arXiv:2301.03136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03136">
<div class="article-summary-box-inner">
<span><p>Extraction of sentiment signals from news text, stock message boards, and
business reports, for stock movement prediction, has been a rising field of
interest in finance. Building upon past literature, the most recent works
attempt to better capture sentiment from sentences with complex syntactic
structures by introducing aspect-level sentiment classification (ASC). Despite
the growing interest, however, fine-grained sentiment analysis has not been
fully explored in non-English literature due to the shortage of annotated
finance-specific data. Accordingly, it is necessary for non-English languages
to leverage datasets and pre-trained language models (PLM) of different
domains, languages, and tasks to best their performance. To facilitate
finance-specific ASC research in the Korean language, we build KorFinASC, a
Korean aspect-level sentiment classification dataset for finance consisting of
12,613 human-annotated samples, and explore methods of intermediate transfer
learning. Our experiments indicate that past research has been ignorant towards
the potentially wrong knowledge of financial entities encoded during the
training phase, which has overestimated the predictive power of PLMs. In our
work, we use the term "non-stationary knowledge'' to refer to information that
was previously correct but is likely to change, and present "TGT-Masking'', a
novel masking pattern to restrict PLMs from speculating knowledge of the kind.
Finally, through a series of transfer learning with TGT-Masking applied we
improve 22.63% of classification accuracy compared to standalone models on
KorFinASC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Automated Machine Translation to Educational Video Courses. (arXiv:2301.03141v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03141">
<div class="article-summary-box-inner">
<span><p>We studied the capability of automated machine translation in the online
video education space by automatically translating Khan Academy videos with
state of the art translation models and applying Text-to-Speech synthesis to
build engaging videos in target languages. We also analyzed and established a
reliable translation confidence estimator based on round-trip translations in
order to efficiently manage translation quality and reduce human translation
effort. Finally, we developed a deployable system to deliver translated videos
to end users and collect user corrections for iterative improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Algorithms for Depression Detection and Their Comparison. (arXiv:2301.03222v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03222">
<div class="article-summary-box-inner">
<span><p>Textual emotional intelligence is playing a ubiquitously important role in
leveraging human emotions on social media platforms. Social media platforms are
privileged with emotional content and are leveraged for various purposes like
opinion mining, emotion mining, and sentiment analysis. This data analysis is
also levered for the prevention of online bullying, suicide prevention, and
depression detection among social media users. In this article, we have
designed an automatic depression detection of online social media users by
analyzing their social media behavior. The designed depression detection
classification can be effectively used to mine user's social media interactions
and one can determine whether a social media user is suffering from depression
or not. The underlying classifier is made using state-of-art technology in
emotional artificial intelligence which includes LSTM (Long Short Term Memory)
and other machine learning classifiers. The highest accuracy of the classifier
is around 70% of LSTM and for SVM the highest accuracy is 81.79%. We trained
the classifier on the datasets that are widely used in literature for emotion
mining tasks. A confusion matrix of results is also given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Fake Review Detection Using Supervised Machine Learning And BERT Model. (arXiv:2301.03225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03225">
<div class="article-summary-box-inner">
<span><p>Online shopping stores have grown steadily over the past few years. Due to
the massive growth of these businesses, the detection of fake reviews has
attracted attention. Fake reviews are seriously trying to mislead customers and
thereby undermine the honesty and authenticity of online shopping environments.
So far, various fake review classifiers have been proposed that take into
account the actual content of the review. To improve the accuracies of existing
fake review classification or detection approaches, we propose to use BERT
(Bidirectional Encoder Representation from Transformers) model to extract word
embeddings from texts (i.e. reviews). Word embeddings are obtained in various
basic methods such as SVM (Support vector machine), Random Forests, Naive
Bayes, and others. The confusion matrix method was also taken into account to
evaluate and graphically represent the results. The results indicate that the
SVM classifiers outperform the others in terms of accuracy and f1-score with an
accuracy of 87.81%, which is 7.6% higher than the classifier used in the
previous study [5].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAQA: A Multimodal QA Benchmark for Negation. (arXiv:2301.03238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03238">
<div class="article-summary-box-inner">
<span><p>Multimodal learning can benefit from the representation power of pretrained
Large Language Models (LLMs). However, state-of-the-art transformer based LLMs
often ignore negations in natural language and there is no existing benchmark
to quantitatively evaluate whether multimodal transformers inherit this
weakness. In this study, we present a new multimodal question answering (QA)
benchmark adapted from labeled music videos in AudioSet (Gemmeke et al., 2017)
with the goal of systematically evaluating if multimodal transformers can
perform complex reasoning to recognize new concepts as negation of previously
learned concepts. We show that with standard fine-tuning approach multimodal
transformers are still incapable of correctly interpreting negation
irrespective of model size. However, our experiments demonstrate that
augmenting the original training task distributions with negated QA examples
allow the model to reliably reason with negation. To do this, we describe a
novel data generation procedure that prompts the 540B-parameter PaLM model to
automatically generate negated QA examples as compositions of easily accessible
video tags. The generated examples contain more natural linguistic patterns and
the gains compared to template-based task augmentation approach are
significant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Abstractive Text Summarization. (arXiv:2301.03252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03252">
<div class="article-summary-box-inner">
<span><p>Construction of human-curated annotated datasets for abstractive text
summarization (ATS) is very time-consuming and expensive because creating each
instance requires a human annotator to read a long document and compose a
shorter summary that would preserve the key information relayed by the original
document. Active Learning (AL) is a technique developed to reduce the amount of
annotation required to achieve a certain level of machine learning model
performance. In information extraction and text classification, AL can reduce
the amount of labor up to multiple times. Despite its potential for aiding
expensive annotation, as far as we know, there were no effective AL query
strategies for ATS. This stems from the fact that many AL strategies rely on
uncertainty estimation, while as we show in our work, uncertain instances are
usually noisy, and selecting them can degrade the model performance compared to
passive annotation. We address this problem by proposing the first effective
query strategy for AL in ATS based on diversity principles. We show that given
a certain annotation budget, using our strategy in AL annotation helps to
improve the model performance in terms of ROUGE and consistency scores.
Additionally, we analyze the effect of self-learning and show that it can
further increase the performance of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Information Extraction as Unified Semantic Matching. (arXiv:2301.03282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03282">
<div class="article-summary-box-inner">
<span><p>The challenge of information extraction (IE) lies in the diversity of label
schemas and the heterogeneity of structures. Traditional methods require
task-specific model design and rely heavily on expensive supervision, making
them difficult to generalize to new schemas. In this paper, we decouple IE into
two basic abilities, structuring and conceptualizing, which are shared by
different tasks and schemas. Based on this paradigm, we propose to universally
model various IE tasks with Unified Semantic Matching (USM) framework, which
introduces three unified token linking operations to model the abilities of
structuring and conceptualizing. In this way, USM can jointly encode schema and
input text, uniformly extract substructures in parallel, and controllably
decode target structures on demand. Empirical evaluation on 4 IE tasks shows
that the proposed method achieves state-of-the-art performance under the
supervised experiments and shows strong generalization ability in zero/few-shot
transfer settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FullStop:Punctuation and Segmentation Prediction for Dutch with Transformers. (arXiv:2301.03319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03319">
<div class="article-summary-box-inner">
<span><p>When applying automated speech recognition (ASR) for Belgian Dutch (Van Dyck
et al. 2021), the output consists of an unsegmented stream of words, without
any punctuation. A next step is to perform segmentation and insert punctuation,
making the ASR output more readable and easy to manually correct. As far as we
know there is no publicly available punctuation insertion system for Dutch that
functions at a usable level. The model we present here is an extension of the
models of Guhr et al. (2021) for Dutch and is made publicly available. We
trained a sequence classification model, based on the Dutch language model
RobBERT (Delobelle et al. 2020). For every word in the input sequence, the
models predicts a punctuation marker that follows the word. We have also
extended a multilingual model, for cases where the language is unknown or where
code switching applies. When performing the task of segmentation, the
application of the best models onto out of domain test data, a sliding window
of 200 words of the ASR output stream is sent to the classifier, and
segmentation is applied when the system predicts a segmenting punctuation sign
with a ratio above threshold. Results show to be much better than a machine
translation baseline approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Multimodal Representation for Language Understanding. (arXiv:2301.03344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03344">
<div class="article-summary-box-inner">
<span><p>Representation learning is the foundation of natural language processing
(NLP). This work presents new methods to employ visual information as assistant
signals to general NLP tasks. For each sentence, we first retrieve a flexible
number of images either from a light topic-image lookup table extracted over
the existing sentence-image pairs or a shared cross-modal embedding space that
is pre-trained on out-of-shelf text-image pairs. Then, the text and images are
encoded by a Transformer encoder and convolutional neural network,
respectively. The two sequences of representations are further fused by an
attention layer for the interaction of the two modalities. In this study, the
retrieval process is controllable and flexible. The universal visual
representation overcomes the lack of large-scale bilingual sentence-image
pairs. Our method can be easily applied to text-only tasks without manually
annotated multimodal parallel corpora. We apply the proposed method to a wide
range of natural language generation and understanding tasks, including neural
machine translation, natural language inference, and semantic similarity.
Experimental results show that our method is generally effective for different
tasks and languages. Analysis indicates that the visual signals enrich textual
representations of content words, provide fine-grained grounding information
about the relationship between concepts and events, and potentially conduce to
disambiguation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Bidirectional Action-Language Translation with Limited Supervision and Incongruent Extra Input. (arXiv:2301.03353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03353">
<div class="article-summary-box-inner">
<span><p>Human infant learning happens during exploration of the environment, by
interaction with objects, and by listening to and repeating utterances
casually, which is analogous to unsupervised learning. Only occasionally, a
learning infant would receive a matching verbal description of an action it is
committing, which is similar to supervised learning. Such a learning mechanism
can be mimicked with deep learning. We model this weakly supervised learning
paradigm using our Paired Gated Autoencoders (PGAE) model, which combines an
action and a language autoencoder. After observing a performance drop when
reducing the proportion of supervised training, we introduce the Paired
Transformed Autoencoders (PTAE) model, using Transformer-based crossmodal
attention. PTAE achieves significantly higher accuracy in language-to-action
and action-to-language translations, particularly in realistic but difficult
cases when only few supervised training samples are available. We also test
whether the trained model behaves realistically with conflicting multimodal
input. In accordance with the concept of incongruence in psychology, conflict
deteriorates the model output. Conflicting action input has a more severe
impact than conflicting language input, and more conflicting features lead to
larger interference. PTAE can be trained on mostly unlabelled data where
labeled data is scarce, and it behaves plausibly when tested with incongruent
input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets. (arXiv:2301.03373v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03373">
<div class="article-summary-box-inner">
<span><p>The research applies AI-driven code assistants to analyze a selection of
influential computer code that has shaped modern technology, including email,
internet browsing, robotics, and malicious software. The original contribution
of this study was to examine half of the most significant code advances in the
last 50 years and, in some cases, to provide notable improvements in clarity or
performance. The AI-driven code assistant could provide insights into
obfuscated code or software lacking explanatory commentary in all cases
examined. We generated additional sample problems based on bug corrections and
code optimizations requiring much deeper reasoning than a traditional Google
search might provide. Future work focuses on adding automated documentation and
code commentary and translating select large code bases into more modern
versions with multiple new application programming interfaces (APIs) and
chained multi-tasks. The AI-driven code assistant offers a valuable tool for
software engineering, particularly in its ability to provide human-level
expertise and assist in refactoring legacy code or simplifying the explanation
or functionality of high-value repositories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI2: The next leap toward native language based and explainable machine learning framework. (arXiv:2301.03391v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03391">
<div class="article-summary-box-inner">
<span><p>The machine learning frameworks flourished in the last decades, allowing
artificial intelligence to get out of academic circles to be applied to
enterprise domains. This field has significantly advanced, but there is still
some meaningful improvement to reach the subsequent expectations. The proposed
framework, named AI$^{2}$, uses a natural language interface that allows a
non-specialist to benefit from machine learning algorithms without necessarily
knowing how to program with a programming language. The primary contribution of
the AI$^{2}$ framework allows a user to call the machine learning algorithms in
English, making its interface usage easier. The second contribution is
greenhouse gas (GHG) awareness. It has some strategies to evaluate the GHG
generated by the algorithm to be called and to propose alternatives to find a
solution without executing the energy-intensive algorithm. Another contribution
is a preprocessing module that helps to describe and to load data properly.
Using an English text-based chatbot, this module guides the user to define
every dataset so that it can be described, normalized, loaded and divided
appropriately. The last contribution of this paper is about explainability. For
decades, the scientific community has known that machine learning algorithms
imply the famous black-box problem. Traditional machine learning methods
convert an input into an output without being able to justify this result. The
proposed framework explains the algorithm's process with the proper texts,
graphics and tables. The results, declined in five cases, present usage
applications from the user's English command to the explained output.
Ultimately, the AI$^{2}$ framework represents the next leap toward native
language-based, human-oriented concerns about machine learning framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03403">
<div class="article-summary-box-inner">
<span><p>We provide a literature review about Automatic Text Summarization (ATS)
systems. We consider a citation-based approach. We start with some popular and
well-known papers that we have in hand about each topic we want to cover and we
have tracked the "backward citations" (papers that are cited by the set of
papers we knew beforehand) and the "forward citations" (newer papers that cite
the set of papers we knew beforehand). In order to organize the different
methods, we present the diverse approaches to ATS guided by the mechanisms they
use to generate a summary. Besides presenting the methods, we also present an
extensive review of the datasets available for summarization tasks and the
methods used to evaluate the quality of the summaries. Finally, we present an
empirical exploration of these methods using the CNN Corpus dataset that
provides golden summaries for extractive and abstractive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization. (arXiv:2301.03416v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03416">
<div class="article-summary-box-inner">
<span><p>Task-agnostic knowledge distillation attempts to address the problem of
deploying large pretrained language model in resource-constrained scenarios by
compressing a large pretrained model called teacher into a smaller one called
student such that the student can be directly finetuned on downstream tasks and
retains comparable performance. However, we empirically find that there is a
generalization gap between the student and the teacher in existing methods. In
this work, we show that we can leverage multi-task learning in task-agnostic
distillation to advance the generalization of the resulted student. In
particular, we propose Multi-task Infused Task-agnostic Knowledge Distillation
(MITKD). We first enhance the teacher by multi-task training it on multiple
downstream tasks and then perform distillation to produce the student.
Experimental results demonstrate that our method yields a student with much
better generalization, significantly outperforms existing baselines, and
establishes a new state-of-the-art result on in-domain, out-domain, and
low-resource datasets in the setting of task-agnostic distillation. Moreover,
our method even exceeds an 8x larger BERT$_{\text{Base}}$ on SQuAD and four
GLUE tasks. In addition, by combining ERNIE 3.0, our method achieves
state-of-the-art results on 10 Chinese datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Standardization of Arabic Dialects for Machine Translation. (arXiv:2301.03447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03447">
<div class="article-summary-box-inner">
<span><p>Based on an annotated multimedia corpus, television series Mar{\=a}y{\=a}
2013, we dig into the question of ''automatic standardization'' of Arabic
dialects for machine translation. Here we distinguish between rule-based
machine translation and statistical machine translation. Machine translation
from Arabic most of the time takes standard or modern Arabic as the source
language and produces quite satisfactory translations thanks to the
availability of the translation memories necessary for training the models. The
case is different for the translation of Arabic dialects. The productions are
much less efficient. In our research we try to apply machine translation
methods to a dialect/standard (or modern) Arabic pair to automatically produce
a standard Arabic text from a dialect input, a process we call ''automatic
standardization''. we opt here for the application of ''statistical models''
because ''automatic standardization'' based on rules is more hard with the lack
of ''diglossic'' dictionaries on the one hand and the difficulty of creating
linguistic rules for each dialect on the other. Carrying out this research
could then lead to combining ''automatic standardization'' software and
automatic translation software so that we take the output of the first software
and introduce it as input into the second one to obtain at the end a quality
machine translation. This approach may also have educational applications such
as the development of applications to help understand different Arabic dialects
by transforming dialectal texts into standard Arabic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Healthcare Procurement Data Using Text Mining and Natural Language Processing -- Reflection From An Industrial Project. (arXiv:2301.03458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03458">
<div class="article-summary-box-inner">
<span><p>While text mining and NLP research has been established for decades, there
remain gaps in the literature that reports the use of these techniques in
building real-world applications. For example, they typically look at single
and sometimes simplified tasks, and do not discuss in-depth data heterogeneity
and inconsistency that is common in real-world problems or their implication on
the development of their methods. Also, few prior work has focused on the
healthcare domain. In this work, we describe an industry project that developed
text mining and NLP solutions to mine millions of heterogeneous, multilingual
procurement documents in the healthcare sector. We extract structured
procurement contract data that is used to power a platform for dynamically
assessing supplier risks. Our work makes unique contributions in a number of
ways. First, we deal with highly heterogeneous, multilingual data and we
document our approach to tackle these challenges. This is mainly based on a
method that effectively uses domain knowledge and generalises to multiple text
mining and NLP tasks and languages. Second, applying this method to mine
millions of procurement documents, we develop the first structured procurement
contract database that will help facilitate the tendering process. Second,
Finally, we discuss lessons learned for practical text mining/NLP development,
and make recommendations for future research and practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Contextual Relatedness to Identify Suicide Documentation in Clinical Notes through Zero Shot Learning. (arXiv:2301.03531v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03531">
<div class="article-summary-box-inner">
<span><p>Identifying suicidality including suicidal ideation, attempts, and risk
factors in electronic health record data in clinical notes is difficult. A
major difficulty is the lack of training samples given the small number of true
positive instances among the increasingly large number of patients being
screened. This paper describes a novel methodology that identifies suicidality
in clinical notes by addressing this data sparsity issue through zero-shot
learning. U.S. Veterans Affairs clinical notes served as data. The training
dataset label was determined using diagnostic codes of suicide attempt and
self-harm. A base string associated with the target label of suicidality was
used to provide auxiliary information by narrowing the positive training cases
to those containing the base string. A deep neural network was trained by
mapping the training documents contents to a semantic space. For comparison, we
trained another deep neural network using the identical training dataset labels
and bag-of-words features. The zero shot learning model outperformed the
baseline model in terms of AUC, sensitivity, specificity, and positive
predictive value at multiple probability thresholds. In applying a 0.90
probability threshold, the methodology identified notes not associated with a
relevant ICD 10 CM code that documented suicidality, with 94 percent accuracy.
This new method can effectively identify suicidality without requiring manual
annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Me Intrigued: Quantifying Usage of Colors in Fiction. (arXiv:2301.03559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.03559">
<div class="article-summary-box-inner">
<span><p>We present preliminary results in quantitative analyses of color usage in
selected authors' works from LitBank. Using Glasgow Norms, human ratings on
5000+ words, we measure attributes of nouns dependent on color terms. Early
results demonstrate a significant increase in noun concreteness over time. We
also propose future research directions for computational literary color
analytics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Converse Attention Knowledge Transfer for Low-Resource Named Entity Recognition. (arXiv:1906.01183v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.01183">
<div class="article-summary-box-inner">
<span><p>In recent years, great success has been achieved in many tasks of natural
language processing (NLP), e.g., named entity recognition (NER), especially in
the high-resource language, i.e., English, thanks in part to the considerable
amount of labeled resources. However, most low-resource languages do not have
such an abundance of labeled data as high-resource English, leading to poor
performance of NER in these low-resource languages. Inspired by knowledge
transfer, we propose Converse Attention Network, or CAN in short, to improve
the performance of NER in low-resource languages by leveraging the knowledge
learned in pretrained high-resource English models. CAN first translates
low-resource languages into high-resource English using an attention based
translation module. In the process of translation, CAN obtain the attention
matrices that align the two languages. Furthermore, CAN use the attention
matrices to align the high-resource semantic features from a pretrained
high-resource English model with the low-resource semantic features. As a
result, CAN obtains aligned high-resource semantic features to enrich the
representations of low-resource languages. Experiments on four low-resource NER
datasets show that CAN achieves consistent and significant performance
improvements, which indicates the effectiveness of CAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning. (arXiv:2003.05162v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.05162">
<div class="article-summary-box-inner">
<span><p>Captioning is a crucial and challenging task for video understanding. In
videos that involve active agents such as humans, the agent's actions can bring
about myriad changes in the scene. Observable changes such as movements,
manipulations, and transformations of the objects in the scene, are reflected
in conventional video captioning. Unlike images, actions in videos are also
inherently linked to social aspects such as intentions (why the action is
taking place), effects (what changes due to the action), and attributes that
describe the agent. Thus for video understanding, such as when captioning
videos or when answering questions about videos, one must have an understanding
of these commonsense aspects. We present the first work on generating
commonsense captions directly from videos, to describe latent aspects such as
intentions, effects, and attributes. We present a new dataset
"Video-to-Commonsense (V2C)" that contains $\sim9k$ videos of human agents
performing various actions, annotated with 3 types of commonsense descriptions.
Additionally we explore the use of open-ended video-based commonsense question
answering (V2C-QA) as a way to enrich our captions. Both the generation task
and the QA task can be used to enrich video captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InsertGNN: Can Graph Neural Networks Outperform Humans in TOEFL Sentence Insertion Problem?. (arXiv:2103.15066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15066">
<div class="article-summary-box-inner">
<span><p>Sentence insertion is an interesting NLP problem but received insufficient
attention. Existing approaches in sentence ordering, text coherence, and
question answering are neither suitable nor good enough at solving it. To
bridge this gap, we propose InsertGNN, a simple yet effective model that
represents the problem as a graph and adopts a hierarchical graph neural
network (GNN) to learn the connection between sentences. We evaluate our method
in our newly collected TOEFL dataset and further verify its effectiveness on
the larger arXiv dataset using cross-domain learning. Extensive experiments
demonstrate that InsertGNN outperforms all baselines by a large margin with an
accuracy of 70\%, rivaling the average human test scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Generative Language Models for Data Augmentation in Few-Shot Text Classification. (arXiv:2111.09064v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09064">
<div class="article-summary-box-inner">
<span><p>Data augmentation techniques are widely used for enhancing the performance of
machine learning models by tackling class imbalance issues and data sparsity.
State-of-the-art generative language models have been shown to provide
significant gains across different NLP tasks. However, their applicability to
data augmentation for text classification tasks in few-shot settings have not
been fully explored, especially for specialised domains. In this paper, we
leverage GPT-2 (Radford A et al, 2019) for generating artificial training
instances in order to improve classification performance. Our aim is to analyse
the impact the selection process of seed training examples have over the
quality of GPT-generated samples and consequently the classifier performance.
We perform experiments with several seed selection strategies that, among
others, exploit class hierarchical structures and domain expert selection. Our
results show that fine-tuning GPT-2 in a handful of label instances leads to
consistent classification improvements and outperform competitive baselines.
Finally, we show that guiding this process through domain expert selection can
lead to further improvements, which opens up interesting research avenues for
combining generative models and active learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\mathcal{Y}$-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning. (arXiv:2202.09817v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09817">
<div class="article-summary-box-inner">
<span><p>With the success of large-scale pre-trained models (PTMs), how efficiently
adapting PTMs to downstream tasks has attracted tremendous attention,
especially for PTMs with billions of parameters. Although some
parameter-efficient tuning paradigms have been proposed to address this
problem, they still require large resources to compute the gradients in the
training phase. In this paper, we propose $\mathcal{Y}$-Tuning, an efficient
yet effective paradigm to adapt frozen large-scale PTMs to specific downstream
tasks. $\mathcal{Y}$-tuning learns dense representations for labels
$\mathcal{Y}$ defined in a given task and aligns them to fixed feature
representation. Without tuning the features of input text and model parameters,
$\mathcal{Y}$-tuning is both parameter-efficient and training-efficient. For
$\text{DeBERTa}_\text{XXL}$ with 1.6 billion parameters, $\mathcal{Y}$-tuning
achieves performance more than $96\%$ of full fine-tuning on GLUE Benchmark
with only $2\%$ tunable parameters and much fewer training costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Segmentation on Discovered Phone Units with Dynamic Programming and Self-Supervised Scoring. (arXiv:2202.11929v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11929">
<div class="article-summary-box-inner">
<span><p>Recent work on unsupervised speech segmentation has used self-supervised
models with phone and word segmentation modules that are trained jointly. This
paper instead revisits an older approach to word segmentation: bottom-up
phone-like unit discovery is performed first, and symbolic word segmentation is
then performed on top of the discovered units (without influencing the lower
level). To do this, I propose a new unit discovery model, a new symbolic word
segmentation model, and then chain the two models to segment speech. Both
models use dynamic programming to minimize segment costs from a self-supervised
network with an additional duration penalty that encourages longer units.
Concretely, for acoustic unit discovery, duration-penalized dynamic programming
(DPDP) is used with a contrastive predictive coding model as the scoring
network. For word segmentation, DPDP is applied with an autoencoding recurrent
neural as the scoring network. The two models are chained in order to segment
speech. This approach gives comparable word segmentation results to
state-of-the-art joint self-supervised segmentation models on an English
benchmark. On French, Mandarin, German and Wolof data, it outperforms previous
systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP
system segments shorter filler words well, but longer words might require some
external top-down signal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment Analysis. (arXiv:2204.00791v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00791">
<div class="article-summary-box-inner">
<span><p>As an extensive research in the field of natural language processing (NLP),
aspect-based sentiment analysis (ABSA) is the task of predicting the sentiment
expressed in a text relative to the corresponding aspect. Unfortunately, most
languages lack sufficient annotation resources, thus more and more recent
researchers focus on cross-lingual aspect-based sentiment analysis (XABSA).
However, most recent researches only concentrate on cross-lingual data
alignment instead of model alignment. To this end, we propose a novel
framework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based
Sentiment Analysis. Based on contrastive learning, we close the distance
between samples with the same label in different semantic spaces, thus
achieving a convergence of semantic spaces of different languages.
Specifically, we design two contrastive strategies, token level contrastive
learning of token embeddings (TL-CTE) and sentiment level contrastive learning
of token embeddings (SL-CTE), to regularize the semantic space of source and
target language to be more uniform. Since our framework can receive datasets in
multiple languages during training, our framework can be adapted not only for
XABSA task but also for multilingual aspect-based sentiment analysis (MABSA).
To further improve the performance of our model, we perform knowledge
distillation technology leveraging data from unlabeled target language. In the
distillation XABSA task, we further explore the comparative effectiveness of
different data (source dataset, translated dataset, and code-switched dataset).
The results demonstrate that the proposed method has a certain improvement in
the three tasks of XABSA, distillation XABSA and MABSA. For reproducibility,
our code for this paper is available at https://github.com/GKLMIP/CL-XABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical N-Gram Framework for Zero-Shot Link Prediction. (arXiv:2204.10293v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10293">
<div class="article-summary-box-inner">
<span><p>Due to the incompleteness of knowledge graphs (KGs), zero-shot link
prediction (ZSLP) which aims to predict unobserved relations in KGs has
attracted recent interest from researchers. A common solution is to use textual
features of relations (e.g., surface name or textual descriptions) as auxiliary
information to bridge the gap between seen and unseen relations. Current
approaches learn an embedding for each word token in the text. These methods
lack robustness as they suffer from the out-of-vocabulary (OOV) problem.
Meanwhile, models built on character n-grams have the capability of generating
expressive representations for OOV words. Thus, in this paper, we propose a
Hierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which
considers the dependencies among character n-grams of the relation surface name
for ZSLP. Our approach works by first constructing a hierarchical n-gram graph
on the surface name to model the organizational structure of n-grams that leads
to the surface name. A GramTransformer, based on the Transformer is then
presented to model the hierarchical n-gram graph to construct the relation
embedding for ZSLP. Experimental results show the proposed HNZSLP achieved
state-of-the-art performance on two ZSLP datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Foundation Models Help Us Achieve Perfect Secrecy?. (arXiv:2205.13722v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13722">
<div class="article-summary-box-inner">
<span><p>A key promise of machine learning is the ability to assist users with
personal tasks. Because the personal context required to make accurate
predictions is often sensitive, we require systems that protect privacy. A gold
standard privacy-preserving system will satisfy perfect secrecy, meaning that
interactions with the system provably reveal no private information. However,
privacy and quality appear to be in tension in existing systems for personal
tasks. Neural models typically require copious amounts of training to perform
well, while individual users typically hold a limited scale of data, so
federated learning (FL) systems propose to learn from the aggregate data of
multiple users. FL does not provide perfect secrecy, but rather practitioners
apply statistical notions of privacy -- i.e., the probability of learning
private information about a user should be reasonably low. The strength of the
privacy guarantee is governed by privacy parameters. Numerous privacy attacks
have been demonstrated on FL systems and it can be challenging to reason about
the appropriate privacy parameters for a privacy-sensitive use case. Therefore
our work proposes a simple baseline for FL, which both provides the stronger
perfect secrecy guarantee and does not require setting any privacy parameters.
We initiate the study of when and where an emerging tool in ML -- the
in-context learning abilities of recent pretrained models -- can be an
effective baseline alongside FL. We find in-context learning is competitive
with strong FL baselines on 6 of 7 popular benchmarks from the privacy
literature and a real-world case study, which is disjoint from the pretraining
data. We release our code here: https://github.com/simran-arora/focus
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">State-of-the-art generalisation research in NLP: A taxonomy and review. (arXiv:2210.03050v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.03050">
<div class="article-summary-box-inner">
<span><p>The ability to generalise well is one of the primary desiderata of natural
language processing (NLP). Yet, what 'good generalisation' entails and how it
should be evaluated is not well understood, nor are there any evaluation
standards for generalisation. In this paper, we lay the groundwork to address
both of these issues. We present a taxonomy for characterising and
understanding generalisation research in NLP. Our taxonomy is based on an
extensive literature review of generalisation research, and contains five axes
along which studies can differ: their main motivation, the type of
generalisation they investigate, the type of data shift they consider, the
source of this data shift, and the locus of the shift within the modelling
pipeline. We use our taxonomy to classify over 400 papers that test
generalisation, for a total of more than 600 individual experiments.
Considering the results of this review, we present an in-depth analysis that
maps out the current state of generalisation research in NLP, and we make
recommendations for which areas might deserve attention in the future. Along
with this paper, we release a webpage where the results of our review can be
dynamically explored, and which we intend to update as new NLP generalisation
studies are published. With this work, we aim to take steps towards making
state-of-the-art generalisation testing the new status quo in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieving Users' Opinions on Social Media with Multimodal Aspect-Based Sentiment Analysis. (arXiv:2210.15377v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.15377">
<div class="article-summary-box-inner">
<span><p>People post their opinions and experiences on social media, yielding rich
databases of end-users' sentiments. This paper shows to what extent machine
learning can analyze and structure these databases. An automated data analysis
pipeline is deployed to provide insights into user-generated content for
researchers in other domains. First, the domain expert can select an image and
a term of interest. Then, the pipeline uses image retrieval to find all images
showing similar content and applies aspect-based sentiment analysis to outline
users' opinions about the selected term. As part of an interdisciplinary
project between architecture and computer science researchers, an empirical
study of Hamburg's Elbphilharmonie was conveyed. Therefore, we selected 300
thousand posts with the hashtag \enquote{\texttt{hamburg}} from the platform
Flickr. Image retrieval methods generated a subset of slightly more than 1.5
thousand images displaying the Elbphilharmonie. We found that these posts
mainly convey a neutral or positive sentiment towards it. With this pipeline,
we suggest a new semantic computing method that offers novel insights into
end-users opinions, e.g., for architecture domain experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-HindBERT and DevBERT: Pre-Trained BERT Transformer models for Devanagari based Hindi and Marathi Languages. (arXiv:2211.11418v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.11418">
<div class="article-summary-box-inner">
<span><p>The monolingual Hindi BERT models currently available on the model hub do not
perform better than the multi-lingual models on downstream tasks. We present
L3Cube-HindBERT, a Hindi BERT model pre-trained on Hindi monolingual corpus.
Further, since Indic languages, Hindi and Marathi share the Devanagari script,
we train a single model for both languages. We release DevBERT, a Devanagari
BERT model trained on both Marathi and Hindi monolingual datasets. We evaluate
these models on downstream Hindi and Marathi text classification and named
entity recognition tasks. The HindBERT and DevBERT-based models show
significant improvements over multi-lingual MuRIL, IndicBERT, and XLM-R. Based
on these observations we also release monolingual BERT models for other Indic
languages Kannada, Telugu, Malayalam, Tamil, Gujarati, Assamese, Odia, Bengali,
and Punjabi. These models are shared at https://huggingface.co/l3cube-pune .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CultureBERT: Fine-Tuning Transformer-Based Language Models for Corporate Culture. (arXiv:2212.00509v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.00509">
<div class="article-summary-box-inner">
<span><p>This paper introduces supervised machine learning to the literature measuring
corporate culture from text documents. We compile a unique data set of employee
reviews that were labeled by human evaluators with respect to the information
the reviews reveal about the firms' corporate culture. Using this data set, we
fine-tune state-of-the-art transformer-based language models to perform the
same classification task. In out-of-sample predictions, our language models
classify 16 to 28 percent points more of employee reviews in line with human
evaluators than traditional approaches of text classification. We make our
models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding. (arXiv:2301.00876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.00876">
<div class="article-summary-box-inner">
<span><p>Reading comprehension of legal text can be a particularly challenging task
due to the length and complexity of legal clauses and a shortage of
expert-annotated datasets. To address this challenge, we introduce the Merger
Agreement Understanding Dataset (MAUD), an expert-annotated reading
comprehension dataset based on the American Bar Association's 2021 Public
Target Deal Points Study, with over 39,000 examples and over 47,000 total
annotations. Our fine-tuned Transformer baselines show promising results, with
models performing well above random on most questions. However, on a large
subset of questions, there is still room for significant improvement. As the
only expert-annotated merger agreement dataset, MAUD is valuable as a benchmark
for both the legal profession and the NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.01181">
<div class="article-summary-box-inner">
<span><p>We demonstrate a proof-of-concept of a large language model conducting
corporate lobbying related activities. An autoregressive large language model
(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are
relevant to specific public companies and provides explanations and confidence
levels. For the bills the model deems as relevant, the model drafts a letter to
the sponsor of the bill in an attempt to persuade the congressperson to make
changes to the proposed legislation. We use hundreds of novel ground-truth
labels of the relevance of a bill to a company to benchmark the performance of
the model, which outperforms the baseline of predicting the most common outcome
of irrelevance. We also benchmark the performance of the previous OpenAI GPT-3
model (text-davinci-002), which was the state-of-the-art model on many academic
natural language tasks until text-davinci-003 was recently released. The
performance of text-davinci-002 is worse than simply always predicting that a
bill is irrelevant to a company. These results suggest that, as large language
models continue to exhibit improved core natural language understanding
capabilities, performance on corporate lobbying related tasks will continue to
improve. If AI begins to influence law in a manner that is not a direct
extension of human intentions, this threatens the critical role that law as
information could play in aligning AI with humans. This paper explores how this
is increasingly a possibility. Initially, AI is being used to simply augment
human lobbyists. However, there may be a slow creep of less and less human
oversight over automated assessments of policy ideas and the written
communication to regulatory agencies and Congressional staffers. The core
question raised is where to draw the line between human-driven and AI-driven
policy influence.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-01-10 23:12:45.224747664 UTC">2023-01-10 23:12:45 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>