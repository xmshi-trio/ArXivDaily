<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2023-03-27T01:30:00Z">03-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability. (arXiv:2303.13547v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13547">
<div class="article-summary-box-inner">
<span><p>This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL
ability. Given the recent emergence of large-scale conversational language
model ChatGPT and its impressive capabilities in both conversational abilities
and code generation, we sought to evaluate its Text-to-SQL performance. We
conducted experiments on 12 benchmark datasets with different languages,
settings, or scenarios, and the results demonstrate that ChatGPT has strong
text-to-SQL abilities. Although there is still a gap from the current
state-of-the-art (SOTA) model performance, considering that the experiment was
conducted in a zero-shot scenario, ChatGPT's performance is still impressive.
Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms
the SOTA model that requires fine-tuning on the Spider dataset by 4.1\%,
demonstrating its potential for use in practical applications. To support
further research in related fields, we have made the data generated by ChatGPT
publicly available at https://github.com/THU-BPM/chatgpt-sql.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh. (arXiv:2303.13549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13549">
<div class="article-summary-box-inner">
<span><p>The Berber, or Amazigh language family is a low-resource North African
vernacular language spoken by the indigenous Berber ethnic group. It has its
own unique alphabet called Tifinagh used across Berber communities in Morocco,
Algeria, and others. The Afroasiatic language Berber is spoken by 14 million
people, yet lacks adequate representation in education, research, web
applications etc. For instance, there is no option of translation to or from
Amazigh / Berber on Google Translate, which hosts over 100 languages today.
Consequently, we do not find specialized educational apps, L2 (2nd language
learner) acquisition, automated language translation, and remote-access
facilities enabled in Berber. Motivated by this background, we propose a
supervised approach called DaToBS for Detection and Transcription of Berber
Signs. The DaToBS approach entails the automatic recognition and transcription
of Tifinagh characters from signs in photographs of natural environments. This
is achieved by self-creating a corpus of 1862 pre-processed character images;
curating the corpus with human-guided annotation; and feeding it into an OCR
model via the deployment of CNN for deep learning based on computer vision
models. We deploy computer vision modeling (rather than language models)
because there are pictorial symbols in this alphabet, this deployment being a
novel aspect of our work. The DaToBS experimentation and analyses yield over 92
percent accuracy in our research. To the best of our knowledge, ours is among
the first few works in the automated transcription of Berber signs from
roadside images with deep learning, yielding high accuracy. This can pave the
way for developing pedagogical applications in the Berber language, thereby
addressing an important goal of outreach to underrepresented communities via AI
in education.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Unsupervised Speech Recognition with Diffusion GANs. (arXiv:2303.13559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13559">
<div class="article-summary-box-inner">
<span><p>We enhance the vanilla adversarial training method for unsupervised Automatic
Speech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance
noises of various intensities to the generator's output and unlabeled reference
text which are sampled from pretrained phoneme language models with a length
constraint, (2) asks diffusion timestep-dependent discriminators to separate
them, and (3) back-propagates the gradients to update the generator.
Word/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for
test-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our
enhancement strategies work effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13570">
<div class="article-summary-box-inner">
<span><p>This study presents a novel model for invertible sentence embeddings using a
residual recurrent network trained on an unsupervised encoding task. Rather
than the probabilistic outputs common to neural machine translation models, our
approach employs a regression-based output layer to reconstruct the input
sequence's word vectors. The model achieves high accuracy and fast training
with the ADAM optimizer, a significant finding given that RNNs typically
require memory units, such as LSTMs, or second-order optimization methods. We
incorporate residual connections and introduce a "match drop" technique, where
gradients are calculated only for incorrect words. Our approach demonstrates
potential for various natural language processing applications, particularly in
neural network-based systems that require high-quality sentence embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13592">
<div class="article-summary-box-inner">
<span><p>While code-mixing is a common linguistic practice in many parts of the world,
collecting high-quality and low-cost code-mixed data remains a challenge for
natural language processing (NLP) research. The proliferation of Large Language
Models (LLMs) in recent times compels one to ask: can these systems be used for
data generation? In this article, we explore prompting LLMs in a zero-shot
manner to create code-mixed data for five languages in South East Asia (SEA) --
Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language
Singlish. We find that ChatGPT shows the most potential, capable of producing
code-mixed text 68% of the time when the term "code-mixing" is explicitly
defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in
generating Singlish texts are noteworthy, averaging a 96% success rate across a
variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT,
however, is dampened by word choice errors that lead to semantic inaccuracies.
Other multilingual models such as BLOOMZ and Flan-T5-XXL are unable to produce
code-mixed texts altogether. By highlighting the limited promises of LLMs in a
specific form of low-resource data generation, we call for a measured approach
when applying similar techniques to other data-scarce NLP contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Authorship attribution for Differences between Literary Texts by Bilingual Russian-French and Non-Bilingual French Authors. (arXiv:2303.13622v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13622">
<div class="article-summary-box-inner">
<span><p>Do bilingual Russian-French authors of the end of the twentieth century such
as Andre\"i Makine, Val\'ery Afanassiev, Vladimir F\'edorovski, Iegor Gran,
Luba Jurgenson have common stylistic traits in the novels they wrote in French?
Can we distinguish between them and non-bilingual French writers' texts? Is the
phenomenon of interference observable in French texts of Russian authors? This
paper applies authorship attribution methods including Support Vector Machine
(SVM), $K$-Nearest Neighbors (KNN), Ridge classification, and Neural Network to
answer these questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13631">
<div class="article-summary-box-inner">
<span><p>Words in a natural language not only transmit information but also evolve
with the development of civilization and human migration. The same is true for
music. To understand the complex structure behind the music, we introduced an
algorithm called the Essential Element Network (EEN) to encode the audio into
text. The network is obtained by calculating the correlations between scales,
time, and volume. Optimizing EEN to generate Zipfs law for the frequency and
rank of the clustering coefficient enables us to generate and regard the
semantic relationships as words. We map these encoded words into the
scale-temporal space, which helps us organize systematically the syntax in the
deep structure of music. Our algorithm provides precise descriptions of the
complex network behind the music, as opposed to the black-box nature of other
deep learning approaches. As a result, the experience and properties
accumulated through these processes can offer not only a new approach to the
applications of Natural Language Processing (NLP) but also an easier and more
objective way to analyze the evolution and development of music.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark. (arXiv:2303.13648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13648">
<div class="article-summary-box-inner">
<span><p>ChatGPT is a cutting-edge artificial intelligence language model developed by
OpenAI, which has attracted a lot of attention due to its surprisingly strong
ability in answering follow-up questions. In this report, we aim to evaluate
ChatGPT on the Grammatical Error Correction(GEC) task, and compare it with
commercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g.,
GECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT
performs not as well as those baselines in terms of the automatic evaluation
metrics (e.g., $F_{0.5}$ score), particularly on long sentences. We inspect the
outputs and find that ChatGPT goes beyond one-by-one corrections. Specifically,
it prefers to change the surface expression of certain phrases or sentence
structure while maintaining grammatical correctness. Human evaluation
quantitatively confirms this and suggests that ChatGPT produces less
under-correction or mis-correction issues but more over-corrections. These
results demonstrate that ChatGPT is severely under-estimated by the automatic
evaluation metrics and could be a promising tool for GEC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mordecai 3: A Neural Geoparser and Event Geocoder. (arXiv:2303.13675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13675">
<div class="article-summary-box-inner">
<span><p>Mordecai3 is a new end-to-end text geoparser and event geolocation system.
The system performs toponym resolution using a new neural ranking model to
resolve a place name extracted from a document to its entry in the Geonames
gazetteer. It also performs event geocoding, the process of linking events
reported in text with the place names where they are reported to occur, using
an off-the-shelf question-answering model. The toponym resolution model is
trained on a diverse set of existing training data, along with several thousand
newly annotated examples. The paper describes the model, its training process,
and performance comparisons with existing geoparsers. The system is available
as an open source Python library, Mordecai 3, and replaces an earlier
geoparser, Mordecai v2, one of the most widely used text geoparsers (Halterman
2017).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation. (arXiv:2303.13716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13716">
<div class="article-summary-box-inner">
<span><p>Compositional generalization benchmarks seek to assess whether models can
accurately compute meanings for novel sentences, but operationalize this in
terms of logical form (LF) prediction. This raises the concern that
semantically irrelevant details of the chosen LFs could shape model
performance. We argue that this concern is realized for the COGS benchmark (Kim
and Linzen, 2020). COGS poses generalization splits that appear impossible for
present-day models, which could be taken as an indictment of those models.
However, we show that the negative results trace to incidental features of COGS
LFs. Converting these LFs to semantically equivalent ones and factoring out
capabilities unrelated to semantic interpretation, we find that even baseline
models get traction. A recent variable-free translation of COGS LFs suggests
similar conclusions, but we observe this format is not semantically equivalent;
it is incapable of accurately representing some COGS meanings. These findings
inform our proposal for ReCOGS, a modified version of COGS that comes closer to
assessing the target semantic capabilities while remaining very challenging.
Overall, our results reaffirm the importance of compositional generalization
and careful benchmark task design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural language processing to automatically extract the presence and severity of esophagitis in notes of patients undergoing radiotherapy. (arXiv:2303.13722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13722">
<div class="article-summary-box-inner">
<span><p>Radiotherapy (RT) toxicities can impair survival and quality-of-life, yet
remain under-studied. Real-world evidence holds potential to improve our
understanding of toxicities, but toxicity information is often only in clinical
notes. We developed natural language processing (NLP) models to identify the
presence and severity of esophagitis from notes of patients treated with
thoracic RT. We fine-tuned statistical and pre-trained BERT-based models for
three esophagitis classification tasks: Task 1) presence of esophagitis, Task
2) severe esophagitis or not, and Task 3) no esophagitis vs. grade 1 vs. grade
2-3. Transferability was tested on 345 notes from patients with esophageal
cancer undergoing RT.
</p>
<p>Fine-tuning PubmedBERT yielded the best performance. The best macro-F1 was
0.92, 0.82, and 0.74 for Task 1, 2, and 3, respectively. Selecting the most
informative note sections during fine-tuning improved macro-F1 by over 2% for
all tasks. Silver-labeled data improved the macro-F1 by over 3% across all
tasks. For the esophageal cancer notes, the best macro-F1 was 0.73, 0.74, and
0.65 for Task 1, 2, and 3, respectively, without additional fine-tuning.
</p>
<p>To our knowledge, this is the first effort to automatically extract
esophagitis toxicity severity according to CTCAE guidelines from clinic notes.
The promising performance provides proof-of-concept for NLP-based automated
detailed toxicity monitoring in expanded domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13780">
<div class="article-summary-box-inner">
<span><p>ChatGPT shows remarkable capabilities for machine translation (MT). Several
prior studies have shown that it achieves comparable results to commercial
systems for high-resource languages, but lags behind in complex tasks, e.g,
low-resource and distant-language-pairs translation. However, they usually
adopt simple prompts which can not fully elicit the capability of ChatGPT. In
this report, we aim to further mine ChatGPT's translation ability by revisiting
several aspects: temperature, task information, and domain information, and
correspondingly propose two (simple but effective) prompts: Task-Specific
Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The
performance of ChatGPT depends largely on temperature, and a lower temperature
usually can achieve better performance; 2) Emphasizing the task information
further improves ChatGPT's performance, particularly in complex MT tasks; 3)
Introducing domain information can elicit ChatGPT's generalization ability and
improve its performance in the specific domain; 4) ChatGPT tends to generate
hallucinations for non-English-centric MT tasks, which can be partially
addressed by our proposed prompts but still need to be highlighted for the
MT/NLP community. We also explore the effects of advanced in-context learning
strategies and find a (negative but interesting) observation: the powerful
chain-of-thought prompt leads to word-by-word translation behavior, thus
bringing significant translation degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function. (arXiv:2303.13797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13797">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialog systems enable users to accomplish tasks using natural
language. State-of-the-art systems respond to users in the same way regardless
of their personalities, although personalizing dialogues can lead to higher
levels of adoption and better user experiences. Building personalized dialog
systems is an important, yet challenging endeavor and only a handful of works
took on the challenge. Most existing works rely on supervised learning
approaches and require laborious and expensive labeled training data for each
user profile. Additionally, collecting and labeling data for each user profile
is virtually impossible. In this work, we propose a novel framework, P-ToD, to
personalize task-oriented dialog systems capable of adapting to a wide range of
user profiles in an unsupervised fashion using a zero-shot generalizable reward
function. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three
phases. Phase one performs task-specific training. Phase two kicks off
unsupervised personalization by leveraging the proximal policy optimization
algorithm that performs policy gradients guided by the zero-shot generalizable
reward function. Our novel reward function can quantify the quality of the
generated responses even for unseen profiles. The optional final phase
fine-tunes the personalized model using a few labeled training examples. We
conduct extensive experimental analysis using the personalized bAbI dialogue
benchmark for five tasks and up to 180 diverse user profiles. The experimental
results demonstrate that P-ToD, even when it had access to zero labeled
examples, outperforms state-of-the-art supervised personalization models and
achieves competitive performance on BLEU and ROUGE metrics when compared to a
strong fully-supervised GPT-2 baseline
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Open-domain Slot Filling via Self-supervised Co-training. (arXiv:2303.13801v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13801">
<div class="article-summary-box-inner">
<span><p>Slot filling is one of the critical tasks in modern conversational systems.
The majority of existing literature employs supervised learning methods, which
require labeled training data for each new domain. Zero-shot learning and weak
supervision approaches, among others, have shown promise as alternatives to
manual labeling. Nonetheless, these learning paradigms are significantly
inferior to supervised learning approaches in terms of performance. To minimize
this performance gap and demonstrate the possibility of open-domain slot
filling, we propose a Self-supervised Co-training framework, called SCot, that
requires zero in-domain manually labeled training examples and works in three
phases. Phase one acquires two sets of complementary pseudo labels
automatically. Phase two leverages the power of the pre-trained language model
BERT, by adapting it for the slot filling task using these sets of pseudo
labels. In phase three, we introduce a self-supervised cotraining mechanism,
where both models automatically select highconfidence soft labels to further
improve the performance of the other in an iterative fashion. Our thorough
evaluations show that SCot outperforms state-of-the-art models by 45.57% and
37.56% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed
framework SCot achieves comparable performance when compared to
state-of-the-art fully supervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13809">
<div class="article-summary-box-inner">
<span><p>Generative large language models (LLMs), e.g., ChatGPT, have demonstrated
remarkable proficiency across several NLP tasks such as machine translation,
question answering, text summarization, and natural language understanding.
Recent research has shown that utilizing ChatGPT for assessing the quality of
machine translation (MT) achieves state-of-the-art performance at the system
level but performs poorly at the segment level. To further improve the
performance of LLMs on MT quality assessment, we conducted an investigation
into several prompting methods. Our results indicate that by combining
Chain-of-Thoughts and Error Analysis, a new prompting method called
\textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can
\textit{generate human-like MT evaluations at both the system and segment
level}. Additionally, we discovered some limitations of ChatGPT as an MT
evaluator, such as unstable scoring and biases when provided with multiple
translations in a single query. Our findings aim to provide a preliminary
experience for appropriately evaluating translation quality on ChatGPT while
offering a variety of tricks in designing prompts for in-context learning. We
anticipate that this report will shed new light on advancing the field of
translation evaluation with LLMs by enhancing both the accuracy and reliability
of metrics. The project can be found in
\url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference. (arXiv:2303.13824v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13824">
<div class="article-summary-box-inner">
<span><p>In-Context Learning (ICL), which formulates target tasks as prompt completion
conditioned on in-context demonstrations, has become the prevailing utilization
of LLMs. In this paper, we first disclose an actual predicament for this
typical usage that it can not scale up with training data due to context length
restriction. Besides, existing works have shown that ICL also suffers from
various biases and requires delicate calibration treatment. To address both
challenges, we advocate a simple and effective solution, $k$NN Prompting, which
first queries LLM with training data for distributed representations, then
predicts test instances by simply referring to nearest neighbors. We conduct
comprehensive experiments to demonstrate its two-fold superiority: 1)
Calibration-Free: $k$NN Prompting does not directly align LLM output
distribution with task-specific label space, instead leverages such
distribution to align test and training instances. It significantly outperforms
state-of-the-art calibration-based methods under comparable few-shot scenario.
2) Beyond-Context: $k$NN Prompting can further scale up effectively with as
many training data as are available, continually bringing substantial
improvements. The scaling trend holds across 10 orders of magnitude ranging
from 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B
to 30B. It successfully bridges data scaling into model scaling, and brings new
potentials for the gradient-free paradigm of LLM deployment. Code is publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRDoc: Dataset and Baseline Method Toward Hierarchical Reconstruction of Document Structures. (arXiv:2303.13839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13839">
<div class="article-summary-box-inner">
<span><p>The problem of document structure reconstruction refers to converting digital
or scanned documents into corresponding semantic structures. Most existing
works mainly focus on splitting the boundary of each element in a single
document page, neglecting the reconstruction of semantic structure in
multi-page documents. This paper introduces hierarchical reconstruction of
document structures as a novel task suitable for NLP and CV fields. To better
evaluate the system performance on the new task, we built a large-scale dataset
named HRDoc, which consists of 2,500 multi-page documents with nearly 2 million
semantic units. Every document in HRDoc has line-level annotations including
categories and relations obtained from rule-based extractors and human
annotators. Moreover, we proposed an encoder-decoder-based hierarchical
document structure parsing system (DSPS) to tackle this problem. By adopting a
multi-modal bidirectional encoder and a structure-aware GRU decoder with
soft-mask operation, the DSPS model surpass the baseline method by a large
margin. All scripts and datasets will be made publicly available at
https://github.com/jfma-USTC/HRDoc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the ICASSP 2023 General Meeting Understanding and Generation Challenge (MUG). (arXiv:2303.13932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13932">
<div class="article-summary-box-inner">
<span><p>ICASSP2023 General Meeting Understanding and Generation Challenge (MUG)
focuses on prompting a wide range of spoken language processing (SLP) research
on meeting transcripts, as SLP applications are critical to improve users'
efficiency in grasping important information in meetings. MUG includes five
tracks, including topic segmentation, topic-level and session-level extractive
summarization, topic title generation, keyphrase extraction, and action item
detection. To facilitate MUG, we construct and release a large-scale meeting
dataset, the AliMeeting4MUG Corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUG: A General Meeting Understanding and Generation Benchmark. (arXiv:2303.13939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13939">
<div class="article-summary-box-inner">
<span><p>Listening to long video/audio recordings from video conferencing and online
courses for acquiring information is extremely inefficient. Even after ASR
systems transcribe recordings into long-form spoken language documents, reading
ASR transcripts only partly speeds up seeking information. It has been observed
that a range of NLP applications, such as keyphrase extraction, topic
segmentation, and summarization, significantly improve users' efficiency in
grasping important information. The meeting scenario is among the most valuable
scenarios for deploying these spoken language processing (SLP) capabilities.
However, the lack of large-scale public meeting datasets annotated for these
SLP tasks severely hinders their advancement. To prompt SLP advancement, we
establish a large-scale general Meeting Understanding and Generation Benchmark
(MUG) to benchmark the performance of a wide range of SLP tasks, including
topic segmentation, topic-level and session-level extractive summarization and
topic title generation, keyphrase extraction, and action item detection. To
facilitate the MUG benchmark, we construct and release a large-scale meeting
dataset for comprehensive long-form SLP development, the AliMeeting4MUG Corpus,
which consists of 654 recorded Mandarin meeting sessions with diverse topic
coverage, with manual annotations for SLP tasks on manual transcripts of
meeting recordings. To the best of our knowledge, the AliMeeting4MUG Corpus is
so far the largest meeting corpus in scale and facilitates most SLP tasks. In
this paper, we provide a detailed introduction of this corpus, SLP tasks and
evaluation methods, baseline systems and their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13988">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called "machine psychology". The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrase Detection: Human vs. Machine Content. (arXiv:2303.13989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13989">
<div class="article-summary-box-inner">
<span><p>The growing prominence of large language models, such as GPT-4 and ChatGPT,
has led to increased concerns over academic integrity due to the potential for
machine-generated content and paraphrasing. Although studies have explored the
detection of human- and machine-paraphrased content, the comparison between
these types of content remains underexplored. In this paper, we conduct a
comprehensive analysis of various datasets commonly employed for paraphrase
detection tasks and evaluate an array of detection methods. Our findings
highlight the strengths and limitations of different detection methods in terms
of performance on individual datasets, revealing a lack of suitable
machine-generated datasets that can be aligned with human expectations. Our
main finding is that human-authored paraphrases exceed machine-generated ones
in terms of difficulty, diversity, and similarity implying that automatically
generated texts are not yet on par with human-level performance. Transformers
emerged as the most effective method across datasets with TF-IDF excelling on
semantically diverse corpora. Additionally, we identify four datasets as the
most diverse and challenging for paraphrase detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization. (arXiv:2303.14011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14011">
<div class="article-summary-box-inner">
<span><p>Neural abstractive summarization has been widely studied and achieved great
success with large-scale corpora. However, the considerable cost of annotating
data motivates the need for learning strategies under low-resource settings. In
this paper, we investigate the problems of learning summarizers with only few
examples and propose corresponding methods for improvements. First, typical
transfer learning methods are prone to be affected by data properties and
learning objectives in the pretext tasks. Therefore, based on pretrained
language models, we further present a meta learning framework to transfer
few-shot learning processes from source corpora to the target corpus. Second,
previous methods learn from training examples without decomposing the content
and preference. The generated summaries could therefore be constrained by the
preference bias in the training set, especially under low-resource settings. As
such, we propose decomposing the contents and preferences during learning
through the parameter modulation, which enables control over preferences during
inference. Third, given a target application, specifying required preferences
could be non-trivial because the preferences may be difficult to derive through
observations. Therefore, we propose a novel decoding method to automatically
estimate suitable preferences and generate corresponding summary candidates
from the few training examples. Extensive experiments demonstrate that our
methods achieve state-of-the-art performance on six diverse corpora with
30.11%/33.95%/27.51% and 26.74%/31.14%/24.48% average improvements on
ROUGE-1/2/L under 10- and 100-example settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14070">
<div class="article-summary-box-inner">
<span><p>Recent large language models (LLMs) in the general domain, such as ChatGPT,
have shown remarkable success in following instructions and producing
human-like responses. However, such language models have not been learned
individually and carefully for the medical domain, resulting in poor diagnostic
accuracy and inability to give correct recommendations for medical diagnosis,
medications, etc. To address this issue, we collected more than 700 diseases
and their corresponding symptoms, recommended medications, and required medical
tests, and then generated 5K doctor-patient conversations. By fine-tuning
models of doctor-patient conversations, these models emerge with great
potential to understand patients' needs, provide informed advice, and offer
valuable assistance in a variety of medical-related fields. The integration of
these advanced language models into healthcare can revolutionize the way
healthcare professionals and patients communicate, ultimately improving the
overall quality of care and patient outcomes. In addition, we will open all
source code, datasets and model weights to advance the further development of
dialogue models in the medical field. In addition, the training data, code, and
weights of this project are available at:
https://github.com/Kent0n-Li/ChatDoctor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14116">
<div class="article-summary-box-inner">
<span><p>With the dramatic advances in deep learning technology, machine learning
research is focusing on improving the interpretability of model predictions as
well as prediction performance in both basic and applied research. While deep
learning models have much higher prediction performance than traditional
machine learning models, the specific prediction process is still difficult to
interpret and/or explain. This is known as the black-boxing of machine learning
models and is recognized as a particularly important problem in a wide range of
research fields, including manufacturing, commerce, robotics, and other
industries where the use of such technology has become commonplace, as well as
the medical field, where mistakes are not tolerated. This bulletin is based on
the summary of the author's dissertation. The research summarized in the
dissertation focuses on the attention mechanism, which has been the focus of
much attention in recent years, and discusses its potential for both basic
research in terms of improving prediction performance and interpretability, and
applied research in terms of evaluating it for real-world applications using
large data sets beyond the laboratory environment. The dissertation also
concludes with a summary of the implications of these findings for subsequent
research and future prospects in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The crime of being poor. (arXiv:2303.14128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14128">
<div class="article-summary-box-inner">
<span><p>The criminalization of poverty has been widely denounced as a collective bias
against the most vulnerable. NGOs and international organizations claim that
the poor are blamed for their situation, are more often associated with
criminal offenses than the wealthy strata of society and even incur criminal
offenses simply as a result of being poor. While no evidence has been found in
the literature that correlates poverty and overall criminality rates, this
paper offers evidence of a collective belief that associates both concepts.
This brief report measures the societal bias that correlates criminality with
the poor, as compared to the rich, by using Natural Language Processing (NLP)
techniques in Twitter. The paper quantifies the level of crime-poverty bias in
a panel of eight different English-speaking countries. The regional differences
in the association between crime and poverty cannot be justified based on
different levels of inequality or unemployment, which the literature correlates
to property crimes. The variation in the observed rates of crime-poverty bias
for different geographic locations could be influenced by cultural factors and
the tendency to overestimate the equality of opportunities and social mobility
in specific countries. These results have consequences for policy-making and
open a new path of research for poverty mitigation with the focus not only on
the poor but on society as a whole. Acting on the collective bias against the
poor would facilitate the approval of poverty reduction policies, as well as
the restoration of the dignity of the persons affected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Expert Language Models with Unsupervised Domain Discovery. (arXiv:2303.14177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.14177">
<div class="article-summary-box-inner">
<span><p>Large language models are typically trained densely: all parameters are
updated with respect to all inputs. This requires synchronization of billions
of parameters across thousands of GPUs. We introduce a simple but effective
method to asynchronously train large, sparse language models on arbitrary text
corpora. Our method clusters a corpus into sets of related documents, trains a
separate expert language model on each cluster, and combines them in a sparse
ensemble for inference. This approach generalizes embarrassingly parallel
training by automatically discovering the domains for each expert, and
eliminates nearly all the communication overhead of existing sparse language
models. Our technique outperforms dense baselines on multiple corpora and
few-shot tasks, and our analysis shows that specializing experts to meaningful
clusters is key to these gains. Performance also improves with the number of
experts and size of training data, suggesting this is a highly efficient and
accessible approach to training large language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09543">
<div class="article-summary-box-inner">
<span><p>This paper presents a new pre-trained language model, DeBERTaV3, which
improves the original DeBERTa model by replacing mask language modeling (MLM)
with replaced token detection (RTD), a more sample-efficient pre-training task.
Our analysis shows that vanilla embedding sharing in ELECTRA hurts training
efficiency and model performance. This is because the training losses of the
discriminator and the generator pull token embeddings in different directions,
creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled
embedding sharing method that avoids the tug-of-war dynamics, improving both
training efficiency and the quality of the pre-trained model. We have
pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its
exceptional performance on a wide range of downstream natural language
understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an
example, the DeBERTaV3 Large model achieves a 91.37% average score, which is
1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art
(SOTA) among the models with a similar structure. Furthermore, we have
pre-trained a multi-lingual model mDeBERTa and observed a larger improvement
over strong baselines compared to English models. For example, the mDeBERTa
Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%
improvement over XLM-R Base, creating a new SOTA on this benchmark. We have
made our pre-trained models and inference code publicly available at
https://github.com/microsoft/DeBERTa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions. (arXiv:2205.11658v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11658">
<div class="article-summary-box-inner">
<span><p>Generics express generalizations about the world (e.g., birds can fly) that
are not universally true (e.g., newborn birds and penguins cannot fly).
Commonsense knowledge bases, used extensively in NLP, encode some generic
knowledge but rarely enumerate such exceptions and knowing when a generic
statement holds or does not hold true is crucial for developing a comprehensive
understanding of generics. We present a novel framework informed by linguistic
theory to generate exemplars -- specific cases when a generic holds true or
false. We generate ~19k exemplars for ~650 generics and show that our framework
outperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis
highlights the importance of linguistic theory-based controllability for
generating exemplars, the insufficiency of knowledge bases as a source of
exemplars, and the challenges exemplars pose for the task of natural language
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions. (arXiv:2207.14251v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14251">
<div class="article-summary-box-inner">
<span><p>Large amounts of training data are one of the major reasons for the high
performance of state-of-the-art NLP models. But what exactly in the training
data causes a model to make a certain prediction? We seek to answer this
question by providing a language for describing how training data influences
predictions, through a causal framework. Importantly, our framework bypasses
the need to retrain expensive models and allows us to estimate causal effects
based on observational data alone. Addressing the problem of extracting factual
knowledge from pretrained language models (PLMs), we focus on simple data
statistics such as co-occurrence counts and show that these statistics do
influence the predictions of PLMs, suggesting that such models rely on shallow
heuristics. Our causal framework and our results demonstrate the importance of
studying datasets and the benefits of causality for understanding NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2210.01936">
<div class="article-summary-box-inner">
<span><p>Despite the success of large vision and language models (VLMs) in many
downstream applications, it is unclear how well they encode compositional
information. Here, we create the Attribution, Relation, and Order (ARO)
benchmark to systematically evaluate the ability of VLMs to understand
different types of relationships, attributes, and order. ARO consists of Visual
Genome Attribution, to test the understanding of objects' properties; Visual
Genome Relation, to test for relational understanding; and COCO &amp;
Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude
larger than previous benchmarks of compositionality, with more than 50,000 test
cases. We show where state-of-the-art VLMs have poor relational understanding,
can blunder when linking objects to their attributes, and demonstrate a severe
lack of order sensitivity. VLMs are predominantly trained and evaluated on
large datasets with rich compositional structure in the images and captions.
Yet, training on these datasets has not been enough to address the lack of
compositional understanding, and evaluating on these datasets has failed to
surface this deficiency. To understand why these limitations emerge and are not
represented in the standard tests, we zoom into the evaluation and training
procedures. We demonstrate that it is possible to perform well on retrieval
over existing datasets without using the composition and order information.
Given that contrastive pretraining optimizes for retrieval on datasets with
similar shortcuts, we hypothesize that this can explain why the models do not
need to learn to represent compositional information. This finding suggests a
natural solution: composition-aware hard negative mining. We show that a
simple-to-implement modification of contrastive learning significantly improves
the performance on tasks requiring understanding of order and compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mathematically Modeling the Lexicon Entropy of Emergent Language. (arXiv:2211.15783v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2211.15783">
<div class="article-summary-box-inner">
<span><p>We formulate a stochastic process, FiLex, as a mathematical model of lexicon
entropy in deep learning-based emergent language systems. Defining a model
mathematically allows it to generate clear predictions which can be directly
and decisively tested. We empirically verify across four different environments
that FiLex predicts the correct correlation between hyperparameters (training
steps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax
temperature) and the emergent language's entropy in 20 out of 20
environment-hyperparameter combinations. Furthermore, our experiments reveal
that different environments show diverse relationships between their
hyperparameters and entropy which demonstrates the need for a model which can
make well-defined predictions at a precise level of granularity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.04634">
<div class="article-summary-box-inner">
<span><p>Storytelling and narrative are fundamental to human experience, intertwined
with our social and cultural engagement. As such, researchers have long
attempted to create systems that can generate stories automatically. In recent
years, powered by deep learning and massive data resources, automatic story
generation has shown significant advances. However, considerable challenges,
like the need for global coherence in generated stories, still hamper
generative models from reaching the same storytelling ability as human
narrators. To tackle these challenges, many studies seek to inject structured
knowledge into the generation process, which is referred to as structured
knowledge-enhanced story generation. Incorporating external knowledge can
enhance the logical coherence among story events, achieve better knowledge
grounding, and alleviate over-generalization and repetition problems in
stories. This survey provides the latest and comprehensive review of this
research field: (i) we present a systematical taxonomy regarding how existing
methods integrate structured knowledge into story generation; (ii) we summarize
involved story corpora, structured knowledge datasets, and evaluation metrics;
(iii) we give multidimensional insights into the challenges of
knowledge-enhanced story generation and cast light on promising directions for
future study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.08620">
<div class="article-summary-box-inner">
<span><p>We present POTATO, the Portable text annotation tool, a free, fully
open-sourced annotation system that 1) supports labeling many types of text and
multimodal data; 2) offers easy-to-configure features to maximize the
productivity of both deployers and annotators (convenient templates for common
ML/NLP tasks, active learning, keypress shortcuts, keyword highlights,
tooltips); and 3) supports a high degree of customization (editable UI,
inserting pre-screening questions, attention and qualification tests).
Experiments over two annotation tasks suggest that POTATO improves labeling
speed through its specially-designed productivity features, especially for long
documents and complex tasks. POTATO is available at
https://github.com/davidjurgens/potato and will continue to be updated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2212.09196">
<div class="article-summary-box-inner">
<span><p>The recent advent of large language models has reinvigorated debate over
whether human cognitive capacities might emerge in such generic models given
sufficient training data. Of particular interest is the ability of these models
to reason about novel problems zero-shot, without any direct training. In human
cognition, this capacity is closely tied to an ability to reason by analogy.
Here, we performed a direct comparison between human reasoners and a large
language model (the text-davinci-003 variant of GPT-3) on a range of analogical
tasks, including a novel text-based matrix reasoning task closely modeled on
Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly
strong capacity for abstract pattern induction, matching or even surpassing
human capabilities in most settings. Our results indicate that large language
models such as GPT-3 have acquired an emergent ability to find zero-shot
solutions to a broad range of analogy problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10410">
<div class="article-summary-box-inner">
<span><p>Cross-domain NER is a challenging task to address the low-resource problem in
practical scenarios. Previous typical solutions mainly obtain a NER model by
pre-trained language models (PLMs) with data from a rich-resource domain and
adapt it to the target domain. Owing to the mismatch issue among entity types
in different domains, previous approaches normally tune all parameters of PLMs,
ending up with an entirely new NER model for each domain. Moreover, current
models only focus on leveraging knowledge in one general source domain while
failing to successfully transfer knowledge from multiple sources to the target.
To address these issues, we introduce Collaborative Domain-Prefix Tuning for
cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,
we present text-to-text generation grounding domain-related instructors to
transfer knowledge to new domain NER tasks without structural modifications. We
utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate
the potential of PLMs to handle NER tasks across various domains. Experimental
results on the Cross-NER benchmark show that the proposed approach has flexible
transfer ability and performs better on both one-source and multiple-source
cross-domain NER tasks. Codes will be available in
https://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives. (arXiv:2301.10761v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2301.10761">
<div class="article-summary-box-inner">
<span><p>Disfluencies (i.e. interruptions in the regular flow of speech), are
ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that
occur the most frequently compared to other kinds of disfluencies. Yet, to the
best of our knowledge, there isn't a resource that brings together the research
perspectives influencing Spoken Language Understanding (SLU) on these speech
events. This aim of this article is to survey a breadth of perspectives in a
holistic way; i.e. from considering underlying (psycho)linguistic theory, to
their annotation and consideration in Automatic Speech Recognition (ASR) and
SLU systems, to lastly, their study from a generation standpoint. This article
aims to present the perspectives in an approachable way to the SLU and
Conversational AI community, and discuss moving forward, what we believe are
the trends and challenges in each area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.12712">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2303.13284">
<div class="article-summary-box-inner">
<span><p>In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2023-03-27 23:12:27.279344184 UTC">2023-03-27 23:12:27 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>